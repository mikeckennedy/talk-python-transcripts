WEBVTT

00:00:00.001 --> 00:00:04.560
Hey, Chang, welcome to Talk Python To Me.

00:00:04.560 --> 00:00:06.120
Hey, how are you?

00:00:06.120 --> 00:00:07.680
I'm excited to be here.

00:00:07.680 --> 00:00:09.240
I'm excited to have you here.

00:00:09.240 --> 00:00:17.400
We're going to talk about Lance DB, multi dimensional data, all sorts of fun,

00:00:17.400 --> 00:00:19.560
multimodal data, all sorts of fun things.

00:00:19.560 --> 00:00:27.360
So really cool database kind of in the same category, SQLite, DuckDB.

00:00:27.360 --> 00:00:32.600
People want a quick mental model, but different kind of data, right?

00:00:32.600 --> 00:00:34.520
Yeah, absolutely.

00:00:34.520 --> 00:00:42.600
So I think we specialize in multimodal data or AI data.

00:00:42.600 --> 00:00:49.640
And what we think about as included in that is the tabular data that we have from previous

00:00:49.640 --> 00:00:57.120
generations of data engineering, data science, but also embedding vectors, images, videos,

00:00:57.120 --> 00:01:06.360
PDFs, basically anything that don't really fit neatly into data frames or Excel sheets.

00:01:06.360 --> 00:01:08.280
Yeah, less tabular data.

00:01:08.280 --> 00:01:09.280
That's right.

00:01:09.280 --> 00:01:10.280
It's not good.

00:01:10.280 --> 00:01:12.000
It's not good tabular than it.

00:01:12.000 --> 00:01:13.360
You need something else probably.

00:01:13.360 --> 00:01:14.360
Yep.

00:01:15.360 --> 00:01:16.360
Awesome.

00:01:16.360 --> 00:01:22.600
Pretty interesting since I was one of the earliest co-contributors to Pandas.

00:01:22.600 --> 00:01:26.600
So I spent years of my life working on data frames and big data.

00:01:26.600 --> 00:01:28.840
You've given up on squares and rectangles.

00:01:28.840 --> 00:01:29.840
Oh my gosh.

00:01:29.840 --> 00:01:30.840
Well, never.

00:01:30.840 --> 00:01:34.840
But I think there's room to build something more here.

00:01:34.840 --> 00:01:37.760
Yeah, I hear you.

00:01:37.760 --> 00:01:39.760
And also, I agree.

00:01:39.760 --> 00:01:42.920
It's going to be super fun to dive into all of that stuff together.

00:01:42.920 --> 00:01:45.680
Before we do, though, let's hear your story.

00:01:45.680 --> 00:01:46.760
How do you get into programming?

00:01:46.760 --> 00:01:51.260
I know this is written in Rust, but also has some programming APIs.

00:01:51.260 --> 00:01:54.060
How much Python are you doing these days versus other languages?

00:01:54.920 --> 00:02:02.400
Yeah, well, the core of Lant's format and LantCV are in Rust.

00:02:02.400 --> 00:02:08.540
And then there are Python APIs and wrappers around that, as well as TypeScript.

00:02:08.540 --> 00:02:15.760
And the team spends a significant amount of time thinking about what the Python user experience

00:02:15.760 --> 00:02:22.840
looks like and how best to combine the performance that you get from Rust and the user views you

00:02:22.840 --> 00:02:23.560
get from Python.

00:02:23.560 --> 00:02:30.200
And then also making it more as extensible as possible so that even though it's a Python

00:02:30.200 --> 00:02:36.920
library, if you want to add features, for example, you don't have to learn Rust just to be a

00:02:36.920 --> 00:02:39.040
contributor to Lant or LantCV.

00:02:39.040 --> 00:02:47.640
You know, I think Python has benefited a lot from the various Rust-based projects out there,

00:02:47.640 --> 00:02:48.220
right?

00:02:48.220 --> 00:02:51.780
Obviously, we've got uv and Ruff.

00:02:51.780 --> 00:02:55.520
We've got Pydantic, a bunch of other things that make it faster.

00:02:55.520 --> 00:03:01.260
But there has been some pushback that if everything gets done in Rust, then it's hard for Python

00:03:01.260 --> 00:03:02.260
people to contribute.

00:03:02.260 --> 00:03:08.000
I don't necessarily subscribe to that because previously, the answer was when things are too

00:03:08.000 --> 00:03:09.180
slow, we write them in C.

00:03:10.420 --> 00:03:15.980
Maybe it's just traditional Python people also know C, but traditional Python people don't also know Rust.

00:03:15.980 --> 00:03:20.280
I think that's honestly the crux of the issue or the crux of the pushback.

00:03:20.280 --> 00:03:26.640
So I really like to hear that, you know, the extensibility points and you've thought through

00:03:26.640 --> 00:03:29.980
how Python people can extend it and contribute and stuff.

00:03:30.840 --> 00:03:40.160
Yeah, I've spent a lot of time writing like Cython and C to make Python code go faster back in the day.

00:03:40.160 --> 00:03:42.020
I think, yeah, I think you're right.

00:03:42.020 --> 00:03:44.980
A lot of the points are the same for Rust.

00:03:44.980 --> 00:03:48.220
And actually, Rust is a lot safer, actually.

00:03:48.580 --> 00:03:55.920
So when we originally started Lance in the very beginning in C++ and we spent months, we spent

00:03:55.920 --> 00:03:59.080
months writing C++ code for a new format.

00:03:59.080 --> 00:04:07.500
And I think it was the it was Christmas break of 2022 that that's what I call our Rust pill

00:04:07.500 --> 00:04:10.500
moment was when we decided to switch over.

00:04:10.500 --> 00:04:17.120
And I think we ended up, you know, we took about three weeks and we rewrote roughly four or five

00:04:17.120 --> 00:04:18.660
months of C++ code there.

00:04:18.660 --> 00:04:24.700
Overall, I think we actually ended up getting better performance for the most part.

00:04:24.700 --> 00:04:32.680
And I think the biggest thing was just us having the confidence to move forward very quickly

00:04:32.680 --> 00:04:39.100
without that, like, you know, in the back of our mind, like, where's the next sexual coming from?

00:04:39.100 --> 00:04:40.020
Yeah.

00:04:40.020 --> 00:04:40.700
Yeah.

00:04:40.700 --> 00:04:42.060
Oh, absolutely.

00:04:42.060 --> 00:04:43.840
Or the next security vulnerability.

00:04:43.840 --> 00:04:44.780
Exactly.

00:04:44.780 --> 00:04:48.740
Did an S print F, not at one of the safe variants or whatever.

00:04:48.740 --> 00:04:50.200
It's been a long time since I've done C++.

00:04:50.200 --> 00:04:53.400
But tell me about that process.

00:04:53.400 --> 00:04:59.900
I don't know how many people are working on the project at this time and how much experience

00:04:59.900 --> 00:05:00.900
you all had with Rust.

00:05:00.900 --> 00:05:04.080
Like, why did you make that choice?

00:05:04.080 --> 00:05:04.880
And how did it go?

00:05:04.880 --> 00:05:06.160
Oh, yeah.

00:05:06.740 --> 00:05:09.100
You know, I'm a Rust neophyte.

00:05:09.100 --> 00:05:14.880
Like, before that Christmas of 2022, I've never written a line of Rust.

00:05:14.880 --> 00:05:21.240
Most of our team are also new to Rust.

00:05:21.240 --> 00:05:24.460
Or they're new when they join LanceDB.

00:05:25.120 --> 00:05:31.240
And I think what helps is that most of them were also already proficient in C++.

00:05:31.240 --> 00:05:31.840
Right?

00:05:31.840 --> 00:05:39.180
So the sort of core thinking around algorithms and design is all the same, performance and all

00:05:39.180 --> 00:05:39.480
that.

00:05:39.960 --> 00:05:44.720
And the Rust language makes it just easier to learn.

00:05:44.720 --> 00:05:45.500
You're more productive.

00:05:45.500 --> 00:05:48.520
And it's just safer.

00:05:49.060 --> 00:05:54.960
And I think one of the surprising benefits that we saw that I don't see people talking about

00:05:54.960 --> 00:06:02.200
a lot, and maybe that's just us not being very good at CMake, is that productivity piece.

00:06:02.200 --> 00:06:09.440
When we were writing lots of C++ code, we spent tons of time just wrestling with CMake to make

00:06:09.440 --> 00:06:10.540
the builds work.

00:06:10.540 --> 00:06:17.240
And once we moved to Rust, Cargo just made that so easy.

00:06:17.240 --> 00:06:19.220
Basically, we spent zero time wrestling with that.

00:06:19.220 --> 00:06:19.940
Wow.

00:06:19.940 --> 00:06:20.880
That's pretty wild.

00:06:20.880 --> 00:06:25.700
So you all were pretty new to Rust.

00:06:25.700 --> 00:06:36.420
How much do you think that LLMs, ChatGPT, Mistral, Llama, all these things, the availability

00:06:36.420 --> 00:06:40.120
just go, hey, chat thing, how do you do this in Rust?

00:06:40.120 --> 00:06:41.020
Here it is in C++.

00:06:41.020 --> 00:06:43.800
Did you all find that beneficial?

00:06:43.800 --> 00:06:44.820
Did you all do this?

00:06:44.820 --> 00:06:46.080
Sometimes.

00:06:46.580 --> 00:06:56.060
So we use a lot of coding aid, so from Copilot to Cursor, Continue, Zed.

00:06:56.060 --> 00:06:58.960
We try out lots of different tools as well.

00:06:58.960 --> 00:07:05.840
I think certainly the quality of the tooling matters.

00:07:05.840 --> 00:07:10.900
But at the core of it, there's that model performance.

00:07:10.900 --> 00:07:19.860
And so the models today performs pretty well on Python and TypeScript, but on Rust, much

00:07:19.860 --> 00:07:20.400
less so.

00:07:20.400 --> 00:07:21.580
Oh, interesting.

00:07:22.260 --> 00:07:22.760
Yeah.

00:07:22.760 --> 00:07:22.760
Yeah.

00:07:22.760 --> 00:07:32.580
And so we always joke about, God grant me the confidence of ChatGPT coming up with random

00:07:32.580 --> 00:07:34.340
Rust syntax.

00:07:34.340 --> 00:07:35.600
Yeah.

00:07:35.600 --> 00:07:36.960
I've had that as well.

00:07:36.960 --> 00:07:40.660
I asked for some, how to do this in Python, because I was feeling lazy or something.

00:07:40.660 --> 00:07:42.140
I just needed it for an example real quick.

00:07:42.140 --> 00:07:44.040
And it said, you use this.

00:07:44.040 --> 00:07:45.560
It looked completely plausible.

00:07:45.720 --> 00:07:49.200
It was converting time zones and date times.

00:07:49.200 --> 00:07:51.440
And just the time zone part of it was driving me nuts.

00:07:51.440 --> 00:07:53.200
I'm like, all right, Chat, let's do this.

00:07:53.200 --> 00:07:55.340
And it just made up stuff.

00:07:55.340 --> 00:07:59.100
So, oh, you're going to use this function or this property based on this.

00:07:59.100 --> 00:08:00.560
No, that doesn't exist.

00:08:00.560 --> 00:08:01.540
I'm like, that doesn't exist.

00:08:01.540 --> 00:08:01.920
Try again.

00:08:01.920 --> 00:08:02.760
Oh, I'm so sorry.

00:08:02.760 --> 00:08:03.780
Let me try another one.

00:08:04.700 --> 00:08:04.900
Yeah.

00:08:04.900 --> 00:08:14.320
What's interesting is I think that in Python, if you're looking at really well-known libraries

00:08:14.320 --> 00:08:17.260
out there, it generally does much better.

00:08:17.260 --> 00:08:19.500
But I've had it.

00:08:19.500 --> 00:08:26.120
But even for long-known libraries that don't have as much training corpus or attention focus

00:08:26.120 --> 00:08:29.360
on it, it tends to hallucinate a lot.

00:08:29.360 --> 00:08:39.020
Even for something like Apache Arrow, like PyArrow kind of API, it's the standard for in-memory

00:08:39.020 --> 00:08:40.200
data.

00:08:40.200 --> 00:08:43.840
But the ChatGPD still makes up APIs for that.

00:08:43.840 --> 00:08:51.940
And also, I think if you were looking at in terms of the effect on the developer community,

00:08:51.940 --> 00:08:58.120
so there are lots of times where I've seen either like Hacker News posts or comments on

00:08:58.120 --> 00:09:03.740
Hacker News posts, where there's a choice between two similar libraries.

00:09:03.740 --> 00:09:11.560
And then I think the commenters are like, well, I'm choosing a because ChatGPT or Copilot

00:09:11.560 --> 00:09:13.400
generates better completions for that one.

00:09:13.400 --> 00:09:14.280
How interesting.

00:09:14.280 --> 00:09:15.320
Which is really interesting.

00:09:15.320 --> 00:09:16.140
I've never thought about that.

00:09:16.140 --> 00:09:16.860
But yeah.

00:09:16.860 --> 00:09:18.540
Wow.

00:09:18.540 --> 00:09:28.400
You know, the Python, the PSF and JetBrains Python developer survey that came out, one of the

00:09:28.400 --> 00:09:33.440
really interesting stats is 50% of the people who filled out that survey said they've been

00:09:33.440 --> 00:09:36.420
doing professional software development for less than two years.

00:09:36.420 --> 00:09:38.280
Oh, wow.

00:09:38.280 --> 00:09:39.460
That is surprising.

00:09:39.520 --> 00:09:43.200
50% of all Python developers, less than two years.

00:09:43.200 --> 00:09:50.020
And I think there's got to be a strong pull for, yeah, I could go research that.

00:09:50.020 --> 00:09:52.020
Let me just ask Chat first and see what it says.

00:09:52.020 --> 00:09:52.840
Right.

00:09:52.840 --> 00:09:58.080
Or chat, ask Copilot or whatever tools they're using, you know.

00:09:58.080 --> 00:09:59.300
Yep.

00:09:59.300 --> 00:09:59.780
Yep.

00:10:00.140 --> 00:10:00.980
Actually, yeah.

00:10:00.980 --> 00:10:06.160
I mean, I think that's, you know, now that I think about it, yeah, I do feel like that's

00:10:06.160 --> 00:10:09.080
probably, that shouldn't be surprising.

00:10:09.080 --> 00:10:18.000
And also what's interesting, I think I've noticed kind of the same thing with AI versus, you know,

00:10:18.000 --> 00:10:19.720
previous generations of machine learning.

00:10:19.720 --> 00:10:22.540
In previous generations, it was all Python.

00:10:22.540 --> 00:10:28.580
With AI, there's huge TypeScript community that has formed around AI.

00:10:28.840 --> 00:10:33.740
And then also just up in previous generations of machine learning, you had to have years

00:10:33.740 --> 00:10:41.320
of training to learn math and stats and data analytics and data science before you're a

00:10:41.320 --> 00:10:43.400
productive machine learning engineer.

00:10:43.400 --> 00:10:50.540
And I think today it's, with AI, it's definitely not the case that you could quickly, through

00:10:50.540 --> 00:10:57.540
some experimentation and self-learning to become a fairly proficient, I guess, AI engineer,

00:10:57.540 --> 00:11:01.020
which is also a new term that's been coined in the last two years.

00:11:01.020 --> 00:11:02.320
Amazing.

00:11:02.320 --> 00:11:06.980
It's going to be really interesting to see how this affects the industry.

00:11:06.980 --> 00:11:13.520
On one hand, it's going to supercharge people getting, going faster, make them, help them become

00:11:13.520 --> 00:11:15.100
unstuck if they're stuck on a problem.

00:11:15.100 --> 00:11:21.660
But it could also end up hollowing out deep knowledge, maybe, in the long term.

00:11:21.740 --> 00:11:22.740
I'm not entirely sure.

00:11:22.740 --> 00:11:24.440
Yeah.

00:11:24.440 --> 00:11:24.960
We'll see.

00:11:24.960 --> 00:11:26.340
Yeah.

00:11:26.340 --> 00:11:27.940
You know, that is very interesting.

00:11:27.940 --> 00:11:35.820
I do think that, so what we're seeing is, you know, there's a lot of focus around sort

00:11:35.820 --> 00:11:42.720
of model capabilities and then maybe like RAG and, you know, how to marry context and knowledge

00:11:42.720 --> 00:11:46.900
bases to the model through, you know, vector search and vector databases.

00:11:48.040 --> 00:11:56.080
But what we're seeing a lot is more traditional and large enterprises are now starting to adopt

00:11:56.080 --> 00:11:57.180
AI in wholesale.

00:11:57.180 --> 00:12:03.560
And the way they're thinking about it is, you know, let's build an excellence center around

00:12:03.560 --> 00:12:10.340
AI internally and make AI applications easily accessible to the rest of the company across

00:12:10.340 --> 00:12:14.420
many different groups so that you don't have to worry too much about AI infrastructure where the

00:12:14.420 --> 00:12:22.000
data is stored, how that interfaces with a model, and maybe just think about, you know, more at

00:12:22.000 --> 00:12:25.620
the business level and the user level, how you want these things to work.

00:12:25.620 --> 00:12:33.160
So I think that deep knowledge around AI, statistics, machine learning will probably start to

00:12:33.160 --> 00:12:37.400
coalesce around these centers of innovation within these large enterprises.

00:12:38.100 --> 00:12:44.700
And of course, within, you know, AI native startups, they, you know, they all have to have much

00:12:44.700 --> 00:12:50.680
deeper knowledge in advance of the rest of the market to stay ahead of the curve.

00:12:50.680 --> 00:12:51.760
Yeah.

00:12:51.760 --> 00:12:54.200
It's interesting.

00:12:54.200 --> 00:12:58.980
And, you know, I started to see these bots, we'll get to the database in just a second.

00:12:58.980 --> 00:13:05.380
I started to see these bots as kind of almost replacing search, you know, I find myself, maybe I'll

00:13:05.380 --> 00:13:06.020
just skip search.

00:13:06.100 --> 00:13:10.800
Maybe we'll just ask this thing because I have to, you know, I got a good shot of a good answer

00:13:10.800 --> 00:13:11.900
straight away rather than.

00:13:11.900 --> 00:13:13.100
Right.

00:13:13.100 --> 00:13:13.560
Right.

00:13:13.860 --> 00:13:15.760
And that kind of ties into this enterprise thing.

00:13:15.760 --> 00:13:18.940
Like, you know, searching Google is a little bit tricky sometimes.

00:13:18.940 --> 00:13:20.540
I use cat or whatever you're using.

00:13:20.540 --> 00:13:21.520
Yeah.

00:13:21.520 --> 00:13:26.860
It's tricky because there's SEO tricks going on.

00:13:26.860 --> 00:13:28.140
There's ads.

00:13:28.140 --> 00:13:30.560
There's a lot of stuff where you're like, is this really relevant?

00:13:30.560 --> 00:13:34.540
And I think that's probably even worse than trying to search within your enterprise.

00:13:34.660 --> 00:13:36.520
Like, do we have documents on this or whatever?

00:13:36.520 --> 00:13:40.520
It's probably just like, here's a 175 email email thread.

00:13:40.520 --> 00:13:42.260
Like, no, no thanks.

00:13:42.260 --> 00:13:43.100
This isn't going to help.

00:13:43.100 --> 00:13:44.180
Right.

00:13:44.180 --> 00:13:44.520
Right.

00:13:44.520 --> 00:13:46.480
Let's talk.

00:13:46.480 --> 00:13:50.080
Let's talk LanceDB and multimodal data.

00:13:50.080 --> 00:13:51.820
And I guess maybe that's the place.

00:13:51.900 --> 00:13:57.300
Before we get into the details of Lance, what is multimodal data anyway?

00:13:57.300 --> 00:13:58.820
Yeah, absolutely.

00:13:58.820 --> 00:14:04.240
So like I was saying earlier, I think with AI data, you know, vectors and embedding vectors

00:14:04.240 --> 00:14:06.660
is just scratching the surface.

00:14:06.660 --> 00:14:13.960
One of the most powerful things about AI is that it makes it a lot easier to interact with

00:14:13.960 --> 00:14:17.780
the multimodal or unstructured data that we have.

00:14:18.120 --> 00:14:23.300
And so, you know, if you're thinking about, you know, images, videos, audio forms, 3D point

00:14:23.300 --> 00:14:29.240
clouds, there's a lot that's happening in, you know, generation in things like autonomous

00:14:29.240 --> 00:14:29.740
vehicles.

00:14:29.740 --> 00:14:36.680
But even for traditional enterprises, right, you have just oodles of like PDF documents or

00:14:36.680 --> 00:14:37.480
slide decks.

00:14:37.480 --> 00:14:47.880
And there's lots of use cases for a tool that can help those users extract the extract

00:14:47.880 --> 00:14:53.760
insights and then possibly train models on top of those on top of that kind of data.

00:14:53.760 --> 00:15:02.120
And so, you know, I think if you look at if you look at data by by volume, right, you

00:15:02.120 --> 00:15:07.120
know, if you look at your average like TPCH data table, it's like what, like 150 bytes or

00:15:07.120 --> 00:15:08.240
something like that per row.

00:15:08.240 --> 00:15:13.500
And then embeddings, you know, if you just look at the previous generation of open AI,

00:15:13.620 --> 00:15:16.520
it's like, you know, 25 times that.

00:15:16.520 --> 00:15:21.620
And then if you look at images and videos, you're quickly getting to these tables where

00:15:21.620 --> 00:15:24.640
even if you have the same number of rows, the data is just huge.

00:15:24.640 --> 00:15:28.320
So it's not username address.

00:15:28.320 --> 00:15:29.140
Right.

00:15:29.140 --> 00:15:30.840
A few other things are so simple.

00:15:30.840 --> 00:15:31.100
Yeah.

00:15:31.500 --> 00:15:31.800
Yeah.

00:15:31.800 --> 00:15:40.680
So when we talk to a lot of our customers who are trying to build this new what they call

00:15:40.680 --> 00:15:45.200
unstructured data lake, the expectation, they already coming in with expectations, you know,

00:15:45.200 --> 00:15:51.440
okay, this in terms of data volume, this is going to this is going to trounce the existing

00:15:51.440 --> 00:15:53.320
previous generations of data lakes.

00:15:53.780 --> 00:15:54.140
Yeah.

00:15:54.140 --> 00:15:57.780
And then not only that data before, but now we've got big data.

00:15:57.780 --> 00:15:58.120
Right.

00:15:58.120 --> 00:15:58.620
Right.

00:15:58.620 --> 00:15:59.140
Yeah.

00:15:59.140 --> 00:16:00.120
This is even bigger.

00:16:00.120 --> 00:16:05.720
And and it's not only that each row is bigger, but, you know, previous generations of tooling,

00:16:05.720 --> 00:16:12.020
you have, you know, you like users or humans manually generating one data point at a time.

00:16:12.020 --> 00:16:17.300
And now, you know, AI is generating its own training data at thousands of tokens per second.

00:16:17.300 --> 00:16:17.700
Right.

00:16:17.780 --> 00:16:21.860
You know, when you're when you're doing completions, that completion itself becomes

00:16:21.860 --> 00:16:22.460
training data.

00:16:22.460 --> 00:16:29.640
And so the volume at which the data is increasing is also growing at very, very rapidly.

00:16:29.640 --> 00:16:31.400
Yeah.

00:16:31.400 --> 00:16:38.900
If anyone wants to get a sense for just how intensely the world is focused on this, they

00:16:38.900 --> 00:16:44.620
just reopened Three Mile Island, the nuclear power plant, purely to plug in a single Azure

00:16:44.620 --> 00:16:45.800
data center for AI.

00:16:45.800 --> 00:16:50.040
So, yeah, I think we're putting a lot of energy into generating data.

00:16:50.040 --> 00:16:51.080
What would you say?

00:16:51.080 --> 00:16:52.180
Yeah.

00:16:52.180 --> 00:16:53.820
Well, I'm glad.

00:16:53.820 --> 00:16:59.500
Well, I'm glad this data center, even if something goes wrong, the consequences will be much less dire.

00:16:59.500 --> 00:17:01.220
Yes.

00:17:01.220 --> 00:17:08.620
Honestly, I think nuclear is it's worth considering if if you rather than coal or whatever.

00:17:08.620 --> 00:17:10.240
But still, that's a whole different discussion.

00:17:10.240 --> 00:17:11.780
We don't need to go down that hole right now.

00:17:11.780 --> 00:17:12.060
Maybe.

00:17:12.060 --> 00:17:13.300
Maybe later at the end.

00:17:13.300 --> 00:17:13.740
Who knows?

00:17:13.740 --> 00:17:14.440
I mean, yeah.

00:17:14.440 --> 00:17:16.820
But yeah, there's a crazy amount of-

00:17:16.820 --> 00:17:19.020
Maybe you can turn this show into talk politics instead.

00:17:19.020 --> 00:17:19.940
No.

00:17:19.940 --> 00:17:20.940
No, no, no.

00:17:20.940 --> 00:17:21.440
Please no.

00:17:21.440 --> 00:17:21.560
No?

00:17:21.560 --> 00:17:22.600
It's too early.

00:17:23.120 --> 00:17:27.500
So let's talk about something.

00:17:27.500 --> 00:17:27.500
Yeah.

00:17:27.500 --> 00:17:36.020
Maybe dissect kind of like what the H2 or the one-sentence elevator pitch here for LanceDB is.

00:17:36.020 --> 00:17:40.420
So I'll read it out and we'll take it apart and you make sense of it for all of us.

00:17:40.420 --> 00:17:55.800
LanceDB is a developer-friendly open source database for AI from hyper scalable vector search and advanced retrieval for RAG to streaming data interactively exploring large-scale AI sets.

00:17:55.800 --> 00:17:59.680
So that's the best foundation for your AI application.

00:17:59.680 --> 00:17:59.900
Yeah.

00:17:59.900 --> 00:18:04.940
So open source for starters, right?

00:18:04.940 --> 00:18:06.180
Go over here to GitHub.

00:18:06.180 --> 00:18:09.960
You can see just under 10,000 stars, which is, you know, congratulations.

00:18:10.180 --> 00:18:11.200
That's excellent.

00:18:11.200 --> 00:18:13.620
Thank you.

00:18:13.620 --> 00:18:14.560
Actually-

00:18:14.560 --> 00:18:15.560
So we're adding-

00:18:15.560 --> 00:18:17.260
Across the different bits.

00:18:17.260 --> 00:18:17.620
But yeah.

00:18:17.620 --> 00:18:19.220
So really cool.

00:18:19.220 --> 00:18:21.600
And it's under the Apache 2 license.

00:18:21.600 --> 00:18:25.880
That's pretty open for people to do most things what they want, right?

00:18:25.880 --> 00:18:30.560
Maybe not build a closed source business on top of, but-

00:18:30.560 --> 00:18:31.900
Right, right.

00:18:31.900 --> 00:18:32.400
Yeah.

00:18:32.400 --> 00:18:42.560
So I think if the way we're thinking about it is like there's a base layer of how do you work with AI data?

00:18:42.560 --> 00:18:50.420
And we've made Lance format, which is open source and it's a columnar format that's optimized for AI.

00:18:50.420 --> 00:19:14.320
And on top of that, we're building the different workloads that allow user, our users and customers to, you know, search and search, retrieve, run analytical queries, run training workloads across the board for all of their AI needs across the enterprise.

00:19:15.020 --> 00:19:25.100
So the repo that you're looking at, which is Lance CB open source, that's one of the tools that we built on top of the Lance format.

00:19:25.100 --> 00:19:44.760
And the idea is when you, a super common workload is you want to start experimenting with building RAG or agents with memory, semantic search, AI enabled semantic search, like image, text image and video search and things like that.

00:19:44.760 --> 00:19:49.940
So Lance CB is sort of the easiest way for you to get started and prototype.

00:19:49.940 --> 00:19:57.640
So like you said in the beginning, the mental model would be something like equivalent to like a SQLite or DuckDB, right?

00:19:57.640 --> 00:20:00.460
So the data is just a file.

00:20:00.460 --> 00:20:02.840
There's no service to manage.

00:20:02.840 --> 00:20:05.620
There's nothing to sort of connect to.

00:20:05.620 --> 00:20:07.060
It's totally open.

00:20:07.060 --> 00:20:09.940
So you can look at it from with other tools that you have.

00:20:09.940 --> 00:20:12.040
And then the actual database.

00:20:12.040 --> 00:20:12.440
Totally.

00:20:12.440 --> 00:20:13.620
Yeah.

00:20:13.620 --> 00:20:15.000
I love this idea.

00:20:15.000 --> 00:20:37.160
I believe I think things like SQLite and Lance CB, I think this, I just have a file and it, there's a really smart engine on top of that file instead of a complex server architecture to run and manage and security and firewalls and all of that.

00:20:38.120 --> 00:20:39.700
Migrations, all these things.

00:20:39.700 --> 00:20:45.960
It's really pretty positive, I think, to just say, you can just have a file.

00:20:45.960 --> 00:20:47.800
You can just run it.

00:20:47.800 --> 00:20:49.700
When your app is running, it's part of your app.

00:20:49.700 --> 00:20:54.720
And when your app shuts down, it saves the data and it shuts down with it, right?

00:20:54.720 --> 00:20:55.660
Totally.

00:20:56.520 --> 00:20:56.780
Yeah.

00:20:56.780 --> 00:21:07.640
And one of the interesting things we've, a couple of interesting things about the, you know, it's just a file is I think we took that a little bit further than that.

00:21:07.640 --> 00:21:18.020
One is I think that we wanted the file to be open and so that you can inspect it, you can work with it using your other tools, right?

00:21:18.020 --> 00:21:26.080
So when you're building that AI application, one is if you, you have more than just that search and the vector lookup.

00:21:26.080 --> 00:21:29.060
There's, you know, you might want to run SQL to look at metadata.

00:21:29.060 --> 00:21:36.680
You might have, you know, tensors that will upload to PyTorch training workloads and things like that.

00:21:37.320 --> 00:21:43.380
And then we wanted that layer, so we wanted that file to be accessible by all your familiar tools, right?

00:21:43.380 --> 00:21:48.040
From like pandas and polars, you know, new data frame engines like DAF, for example.

00:21:48.040 --> 00:21:52.680
And even, you know, distributed engines like Spark and Ray.

00:21:52.680 --> 00:22:01.180
So that was one big thing that we focused on with Lance Format is making it plug in into the ecosystem.

00:22:02.060 --> 00:22:11.660
So that, you know, for folks who are coming with a Python background and a data background, working with AI would feel familiar, right?

00:22:11.660 --> 00:22:12.540
Right.

00:22:12.540 --> 00:22:17.620
A lot of people know how to do SQL to ask questions about their data, even if they're not programmers, right?

00:22:17.620 --> 00:22:23.900
Maybe a business analyst is like, well, I can write a query, maybe connect that result to some graph or whatever, right?

00:22:23.900 --> 00:22:25.300
Yeah, exactly.

00:22:25.300 --> 00:22:41.700
And this is why I think this was the primary reason why we chose Apache Arrow as the standard interface and the standard type system so that you can, it's easy to plug in all these other tools on top.

00:22:41.700 --> 00:22:43.180
Excellent.

00:22:43.180 --> 00:22:44.260
Yeah.

00:22:44.260 --> 00:22:49.700
So a good question to our audience out there from Carol is, how's the integration with Narwhals?

00:22:49.700 --> 00:22:55.020
You know, I am not familiar enough with Narwhals.

00:22:55.020 --> 00:23:02.200
So Narwhals is a layer that will let you work with multiple data frame libraries.

00:23:02.200 --> 00:23:16.060
So if you wanted to work with, say, pandas and polars, you can use this and it will adapt, depending on what the original data source is, it'll adapt everything to a subset of polars commands.

00:23:16.060 --> 00:23:21.860
So maybe the question is, how does it work with polars or pandas, right?

00:23:21.860 --> 00:23:26.040
So maybe if it does with one or the other, then there might be a way to connect it?

00:23:26.040 --> 00:23:26.980
Yeah.

00:23:26.980 --> 00:23:33.160
So there's, so basically the core interface input-output is Arrow.

00:23:33.160 --> 00:23:41.260
And then with LAN-CB, there's, on the output end, you can convert results to pandas data frames or polars data frames.

00:23:42.040 --> 00:23:52.380
And then on the input end to, I think natively, we can take pandas data frames as batches of input, but we convert that to arrow tables.

00:23:52.380 --> 00:23:58.720
And so for others, you know, it's like polars, for example, it's really easy to convert polars to like an arrow table.

00:23:59.060 --> 00:24:05.480
So a lot of that, even if it's not already automatic, it's like, you know, dot two arrow, one command away.

00:24:05.480 --> 00:24:06.420
Yeah.

00:24:06.420 --> 00:24:06.980
Oh, awesome.

00:24:06.980 --> 00:24:07.660
Yeah.

00:24:07.660 --> 00:24:10.260
Carol added, given it works with the arrow, it probably just works.

00:24:10.260 --> 00:24:10.460
Yeah.

00:24:10.460 --> 00:24:10.920
Yeah.

00:24:10.920 --> 00:24:11.260
Nice.

00:24:11.260 --> 00:24:12.260
Yeah.

00:24:12.260 --> 00:24:12.800
Awesome.

00:24:12.800 --> 00:24:14.920
Okay.

00:24:14.920 --> 00:24:15.560
Yeah.

00:24:15.560 --> 00:24:26.200
So the other thing too, is when I think, you know, when we talk about a file, we don't just mean your local file system, but also object store.

00:24:26.200 --> 00:24:31.100
So if you throw the data set on S3, you can just query it directly.

00:24:31.100 --> 00:24:36.880
Now, you know, you'll hit additional latency because object store, but it works.

00:24:36.880 --> 00:24:48.260
So a lot of our users see that as a bridge into production so that, you know, they use LANCB open source to do the prototyping MVP.

00:24:48.260 --> 00:25:01.580
And then, you know, they, if their use cases fit before they need like a distributed system, it's easy for them to just say, hey, the embedded library lives in my application code.

00:25:01.580 --> 00:25:06.800
And then the, I have a shared data layer in S3 or maybe an EFS or something like that.

00:25:06.800 --> 00:25:14.560
And you can run, you know, distributed queries very easily still without having to manage your own additional systems.

00:25:14.560 --> 00:25:15.460
Okay.

00:25:15.460 --> 00:25:26.620
Just use a really scalable storage layer like S3 or probably anything that has an S3 compatible API, you know, like.

00:25:26.620 --> 00:25:32.600
Yeah, anything that's like a POSIX compliant interface or an S3 compliant API.

00:25:32.600 --> 00:25:33.380
Okay.

00:25:33.380 --> 00:25:36.000
And, you know, this is where also the REST community.

00:25:36.000 --> 00:25:37.240
Or something like that, right?

00:25:37.240 --> 00:25:37.720
Yeah, exactly.

00:25:37.720 --> 00:25:38.160
Yeah.

00:25:38.160 --> 00:25:45.320
So, yeah, I think MinIO published their own integration with LANCB as an example as well.

00:25:45.320 --> 00:25:45.860
Oh, did they?

00:25:45.860 --> 00:25:46.520
Okay.

00:25:46.520 --> 00:25:46.780
Yeah.

00:25:46.780 --> 00:25:48.140
That's cool.

00:25:48.140 --> 00:25:52.640
Maybe tell people real quick what MinIO is.

00:25:52.640 --> 00:25:54.760
It sounds like you're familiar with it or I can.

00:25:54.760 --> 00:25:56.160
Yeah, go ahead.

00:25:56.260 --> 00:25:56.580
Is this?

00:25:56.580 --> 00:26:02.800
Yeah, so MinIO is like, I'm not sure if I got the right website pulled up, but it looks like it.

00:26:02.800 --> 00:26:11.640
So, MinIO is a self-hosted, high-end, sort of pretty complete version of what you would think of as S3.

00:26:11.640 --> 00:26:18.780
But if you don't want to use S3, you want to host on your own infrastructure with your own storage layer and things like that.

00:26:18.780 --> 00:26:22.260
But you still want to talk S3 APIs and security and all that stuff.

00:26:22.260 --> 00:26:23.660
Yeah, that's what MinIO is.

00:26:23.660 --> 00:26:24.400
Yeah.

00:26:24.400 --> 00:26:25.320
Pretty neat.

00:26:25.440 --> 00:26:26.100
Yep, totally.

00:26:26.100 --> 00:26:27.140
Yeah.

00:26:27.140 --> 00:26:27.740
Yeah.

00:26:27.740 --> 00:26:33.780
And so, yeah, so we really took It's Just a File really, really far.

00:26:34.520 --> 00:26:41.860
And I think the results are, the results end up speaking for itself.

00:26:41.860 --> 00:26:43.960
We're seeing really good results.

00:26:43.960 --> 00:26:54.540
And I think it really matches with the way that a lot of our users think about existing tooling and how to add AI to their existing business and applications.

00:26:54.540 --> 00:26:56.160
Cool.

00:26:56.160 --> 00:27:02.540
So, we might be getting just a little bit ahead of ourselves, but we'll backtrack and talk APIs and how it works and stuff.

00:27:02.740 --> 00:27:11.340
But what's the, you know, if it's just a file, which you said is obviously like a pretty broad statement by that, the way you all have implemented it.

00:27:11.340 --> 00:27:11.480
Right.

00:27:11.480 --> 00:27:15.220
But what's the go to production story, right?

00:27:15.220 --> 00:27:16.400
Yep.

00:27:16.400 --> 00:27:17.040
Absolutely.

00:27:17.040 --> 00:27:21.700
If you're using SQLite and that's your middle model, you're like, well, you probably should just switch to Postgres, right?

00:27:21.700 --> 00:27:23.360
But it doesn't sound like that's your answer.

00:27:23.360 --> 00:27:25.920
It sounds like you've thought it through on how to run it at larger scale.

00:27:25.920 --> 00:27:27.380
Right, right.

00:27:27.380 --> 00:27:36.020
So, in terms of large scale production, there's a number of different workloads for both online and offline.

00:27:36.020 --> 00:27:42.840
So, this is our commercial offering, which is calling LanceDB Enterprise.

00:27:45.320 --> 00:28:00.080
And essentially, it's one distributed system that gives you a huge scale, low latency, high throughput, a system that can be backed by the same LANs data.

00:28:00.080 --> 00:28:02.640
It's just a file in S3.

00:28:02.880 --> 00:28:20.500
So, the trick is to create that distributed system in a way that allows you to enjoy the cost efficiency of ObjectStore and the scalability of it while being very, very fast and very performant.

00:28:20.500 --> 00:28:49.000
And so, that's the production story for our customers who are putting AI applications into production where they need high throughput or they need very large index sizes or just lots of, to manage lots of vectors and query a small portion of it without the same sort of budget constraints

00:28:49.000 --> 00:28:55.320
as like you would with OpenSearch or other tools that doesn't give you that compute storage separation.

00:28:55.320 --> 00:28:56.400
Right.

00:28:56.400 --> 00:28:57.560
Oh, very cool.

00:28:57.560 --> 00:29:05.340
So, it says it has GPU support for building vector indexes, which is pretty awesome.

00:29:05.340 --> 00:29:05.640
Yeah.

00:29:05.640 --> 00:29:12.680
Databases, indexes are, they're like the magic speed dust you can sprinkle on them, obviously.

00:29:12.680 --> 00:29:14.040
Right, right.

00:29:14.040 --> 00:29:17.160
What's the story with indexing and this GPU support?

00:29:18.180 --> 00:29:18.440
Yeah.

00:29:18.440 --> 00:29:18.500
Yeah.

00:29:18.500 --> 00:29:26.760
So, I think, you know, with traditional database indices, it's not very computationally intensive.

00:29:26.760 --> 00:29:31.620
Unfortunately, for vector indices and indices, they're quite so.

00:29:31.620 --> 00:29:39.980
So, I think, you know, with Rust, you can make CPU-based indexing for vector data pretty efficient.

00:29:40.180 --> 00:29:52.480
So, if you're looking at, you know, hundreds of thousands or even up to like maybe, you know, 30 or 50 million vectors, you know, CPU indexing is pretty good and can get pretty fast.

00:29:52.480 --> 00:30:03.760
But for our customers that have, you know, 15 billion vectors in one table that they need to index one index, that's, you know, that's going to take days.

00:30:04.000 --> 00:30:04.520
Right.

00:30:04.520 --> 00:30:10.120
So, with GPU support, you know, we cut that down by more than like 15, 20x.

00:30:10.120 --> 00:30:22.680
So, then it becomes like a something that they can do repeatedly and have a, you know, acceptable feedback loop in terms of, you know, maybe adding new data, refreshing that index.

00:30:23.160 --> 00:30:32.080
And I think a lot of it, this is also what I love about the composable data ecosystem.

00:30:32.080 --> 00:30:42.000
And we talk about that, but there's also, I think, there's also an extension, which I'd like to add is like that composable AI data ecosystem.

00:30:42.000 --> 00:30:50.780
So, we talked a little bit about the benefits of like having arrow, which is just, it means if we make the input output APIs arrow, it just works with the rest of the ecosystem.

00:30:51.540 --> 00:31:02.600
And so, and this is one of the points where we get additional benefits, where with GPU support, it's easy through that arrow interface to actually talk to PyTorch.

00:31:02.600 --> 00:31:10.660
So, we can use a lot of the GPU tooling in PyTorch to build a lot of our accelerated vector indexing tools.

00:31:10.660 --> 00:31:21.080
So, you know, like, so that way it saves us lots of time messing with CUDA interfaces and things like that that are not quite mature.

00:31:21.080 --> 00:31:23.700
And so, you know, like, I think it's a lot of people that are in the Rust ecosystem.

00:31:23.700 --> 00:31:23.940
Yeah.

00:31:23.940 --> 00:31:24.660
I see.

00:31:24.660 --> 00:31:25.660
Yeah.

00:31:25.660 --> 00:31:28.040
And you just, it's something you don't have to build.

00:31:28.040 --> 00:31:31.240
It just gets better on its own and you just get to upgrade it.

00:31:31.240 --> 00:31:31.620
Exactly.

00:31:31.620 --> 00:31:32.480
Exactly.

00:31:32.480 --> 00:31:32.820
Yeah.

00:31:32.820 --> 00:31:40.040
That's part of the magic of package managers and package repositories and stuff.

00:31:40.040 --> 00:31:40.600
It's really cool.

00:31:42.820 --> 00:31:45.180
So, what is the workflow?

00:31:45.180 --> 00:31:51.220
Like, take me through the journey that somebody might go on.

00:31:51.220 --> 00:31:52.240
They have a bunch of data.

00:31:52.240 --> 00:32:03.720
They want to put it into the database and they want to put the resulting thing into production so that they could use it behind an API or something that they're implementing for their app or something like that.

00:32:04.460 --> 00:32:11.060
Is there a big training block of time that you do and then you move the resulting data somewhere?

00:32:11.060 --> 00:32:16.680
Or do you just, do you put it in production and start adding to it over and just keep adding to it over time?

00:32:16.680 --> 00:32:17.520
What's the workflow?

00:32:18.380 --> 00:32:18.560
Yeah.

00:32:18.560 --> 00:32:27.080
So, you, so with the, the Lance, the, with the Lance CBA enterprise, like the system that's running, you can just keep adding data to it.

00:32:27.080 --> 00:32:36.560
The indexing and all of that is automatic once you've configured it properly, which is just, you know, here's the schema and like create indices on, on these columns, right?

00:32:36.560 --> 00:32:40.060
Much like what you would do with a, like a Postgres table.

00:32:40.940 --> 00:32:45.620
And then once you have that set up and as you add data, the indexing is automatic.

00:32:45.620 --> 00:32:59.680
And then, you know, for a small, for small amounts of data, you know, our, our users typically will say like have Python dictionaries, JSON, or, you know, pandas data frames.

00:32:59.680 --> 00:33:05.780
They can add those to, to the database through the API for really large scale data.

00:33:05.780 --> 00:33:13.140
So if you're working with terabytes or even petabytes of data, you don't want to be shoving that through the API, like a thousand rows at a time.

00:33:13.140 --> 00:33:15.580
So this is where the open data layer comes in.

00:33:15.580 --> 00:33:29.220
So if you have a large data set and you have, you know, whether it's Spark or Ray, you can use those large distributed systems to write data directly to S3 in the, in Lance open source format.

00:33:29.520 --> 00:33:37.500
And then the system actually picks it up from, from object store and takes care of the indexing and compaction and all of that.

00:33:37.500 --> 00:33:42.280
So it's, it's sort of like, when we're talking about adding data to it, it's, it's, it's both.

00:33:42.280 --> 00:33:43.460
Okay.

00:33:43.460 --> 00:33:44.120
Yeah.

00:33:44.120 --> 00:33:44.840
It sounds really cool.

00:33:44.840 --> 00:33:53.620
So I'm talking to you on a Mac mini M2 pro with maxed out Ram and four terabytes of space or something.

00:33:53.620 --> 00:33:56.500
Can I do interesting stuff on my computer?

00:33:56.500 --> 00:33:58.720
Is it, is it too small?

00:33:58.720 --> 00:34:01.000
Oh, it's not too small at all.

00:34:01.000 --> 00:34:07.120
Um, I think one of the things about land format is it's all desk, right?

00:34:07.120 --> 00:34:17.100
So, most of the magic that we talked about with, with land CB, you know, at least open source is, is lives in that open source format layer.

00:34:17.100 --> 00:34:26.220
And that format is three things in one, one, there's the, the columnar format, the file format, and then there's a table format and then there's the indexing.

00:34:26.220 --> 00:34:30.840
So the indexing is what speeds up the NN vector lookups.

00:34:30.840 --> 00:34:45.860
And typically the, we've made the index, disk space so that, you know, if you, if you're looking up a few partitions in the, in the index, it's only loading those, those indices or those partitions.

00:34:46.120 --> 00:34:49.500
And so the RAM requirement is actually quite small.

00:34:49.500 --> 00:35:01.580
Um, and so, you know, if you, the, the, on, on your systems, probably the more limiting factor is going to be like the, the disk size rather than how much memory you have.

00:35:01.580 --> 00:35:02.460
Yeah.

00:35:02.460 --> 00:35:02.740
Okay.

00:35:02.740 --> 00:35:04.420
A couple of terabytes.

00:35:04.520 --> 00:35:05.720
I should be able to put some data on that.

00:35:05.720 --> 00:35:06.340
Yeah, exactly.

00:35:06.340 --> 00:35:07.860
A couple of free terabytes that is.

00:35:07.860 --> 00:35:09.940
All right.

00:35:09.940 --> 00:35:19.920
Let's talk about the architecture a little bit, maybe a look inside, like give us a, you've talked about some of the pieces and formats, but yeah, give us a sense here.

00:35:19.920 --> 00:35:21.120
Yep.

00:35:21.600 --> 00:35:36.500
Um, and so this is, this is sort of an architecture that includes more than just plants, but it's more about, you know, how our customers are thinking about building, their own multimodal data lake.

00:35:36.500 --> 00:35:39.880
So we call this like vector data lake or unstructured data lake.

00:35:39.880 --> 00:35:45.900
And there's no, there's no standardized terminology yet, but, this is what I was mentioning before.

00:35:45.900 --> 00:35:47.020
Nice.

00:35:47.020 --> 00:35:47.480
Yeah.

00:35:47.480 --> 00:35:48.860
I'll just add for the listeners.

00:35:48.860 --> 00:35:53.240
I'll, I'll put a link to a diagram that talks about this a little bit.

00:35:53.240 --> 00:35:54.360
People can check out if they want.

00:35:54.360 --> 00:35:55.800
Yeah.

00:35:55.800 --> 00:36:11.700
So, so the idea is, from, from these companies perspective, they now have, on the one hand, they have lots of data that they can now take advantage of because of new AI models and applications.

00:36:11.840 --> 00:36:22.000
And on the, on the, on the other hand is there's lots of, business units across that enterprise that, wants to experiment with AI or add AI applications.

00:36:22.000 --> 00:36:31.680
Uh, whether it's, you know, agents or, you know, internal tooling or, you know, customer success, you know, video and image search.

00:36:31.680 --> 00:36:45.940
And lots of different use cases where, there's a lot of, there's a lot of like, Hey, can you sprinkle some AI magic, magic dust on my, you know, existing business and maybe look for that 10 X in, in terms of ROI.

00:36:45.940 --> 00:36:48.620
Uh, and so we have all this SharePoint data.

00:36:48.740 --> 00:36:49.600
Can you help me out?

00:36:49.600 --> 00:36:51.440
Exactly.

00:36:51.440 --> 00:36:56.020
Um, that that's literally a conversation.

00:36:56.020 --> 00:36:57.920
I'm sure that it is.

00:36:57.920 --> 00:37:02.220
Cause SharePoint is bad and you don't want anything to do with it, but people keep putting stuff into it.

00:37:02.220 --> 00:37:02.740
I don't know.

00:37:02.740 --> 00:37:03.660
Yeah.

00:37:03.660 --> 00:37:04.020
Yeah.

00:37:04.020 --> 00:37:05.280
Um, yeah.

00:37:05.280 --> 00:37:11.900
And so, so it's, it's not just about the AI capabilities, but like, how do you build this base layer?

00:37:12.220 --> 00:37:12.540
Right.

00:37:12.540 --> 00:37:20.000
Um, and, make it a lot easier to access for the, the, the rest of the folks in the company.

00:37:20.000 --> 00:37:24.600
And, you know, most of these companies aren't starting from scratch.

00:37:24.600 --> 00:37:32.920
They already have made significant investments into the data science and analytical tooling, training for their data scientists, analysts.

00:37:32.920 --> 00:37:39.060
And they also made significant investments into their existing data lake for large scale data processing.

00:37:39.060 --> 00:37:39.520
Right.

00:37:39.520 --> 00:37:44.680
So, you don't want to make them have to throw all that away.

00:37:44.680 --> 00:37:52.020
And instead, you know, that, that open data layer is super important to plug into the rest of the, their existing ecosystem.

00:37:52.020 --> 00:38:00.440
And with Lance, unlike, you know, Parquet or JSON or web dataset, the, all of their multi-bono data can live in one place.

00:38:00.440 --> 00:38:04.740
Uh, so that they can do search and retrieval on the vectors.

00:38:04.740 --> 00:38:07.160
They can run a SQL.

00:38:07.460 --> 00:38:14.700
They can do, do training and data processing workloads, all sort of in one engine and one, piece of data.

00:38:14.700 --> 00:38:17.040
So it makes just, it just makes things a lot simpler.

00:38:17.040 --> 00:38:25.220
Um, it allows us to add a lot more performance optimizations and then it saves because of the, the size issue.

00:38:25.220 --> 00:38:36.720
It also saves these enterprises a lot in terms of cost, so that they don't have to keep making different copies of different parts of the data for different parts of that workload.

00:38:36.720 --> 00:38:37.200
Right.

00:38:37.200 --> 00:38:38.900
This thing needs that format of the data.

00:38:38.900 --> 00:38:40.620
That thing needs that format of the data.

00:38:40.620 --> 00:38:41.520
Right.

00:38:41.520 --> 00:38:41.940
Exactly.

00:38:41.940 --> 00:38:42.340
Yeah.

00:38:42.420 --> 00:38:48.040
And, or things like, so I'll give you like two, two examples that really stuck out to me.

00:38:48.040 --> 00:38:59.400
Um, so one, I was speaking with one user recently, and they have, you know, they're doing EDA and DuckDB.

00:38:59.400 --> 00:38:59.800
Right.

00:38:59.800 --> 00:39:00.760
So they have lots of metadata.

00:39:00.760 --> 00:39:02.120
They're doing EDA and DuckDB.

00:39:02.120 --> 00:39:03.660
But the downstream.

00:39:03.660 --> 00:39:04.860
Quick, acronym police.

00:39:04.860 --> 00:39:05.060
Sorry.

00:39:05.060 --> 00:39:05.540
What's EDA?

00:39:05.540 --> 00:39:07.680
Oh, sorry about that.

00:39:07.680 --> 00:39:09.220
Uh, exploratory data analysis.

00:39:09.220 --> 00:39:09.920
Okay.

00:39:09.920 --> 00:39:10.360
Got it.

00:39:10.360 --> 00:39:11.040
Yep.

00:39:11.040 --> 00:39:11.460
Sorry.

00:39:11.460 --> 00:39:11.800
Carry on.

00:39:11.800 --> 00:39:12.800
Um, right.

00:39:12.800 --> 00:39:23.040
So they're running SQL in DuckDB, but then they need to fetch, you know, there's like, 10 columns of images and, and like 10 columns of multi vectors.

00:39:23.040 --> 00:39:33.320
And so when they, at the, at the end of where they, they finish running that join and filter, it takes just ages.

00:39:33.320 --> 00:39:35.320
And so they're running that join and they're running that join.

00:39:35.320 --> 00:39:36.320
And so they're running that join.

00:41:42.320 --> 00:41:44.320
And so they're running that join.

00:41:44.320 --> 00:41:45.320
And so they're running that join.

00:42:04.320 --> 00:42:11.320
So let's talk about-- let's keep it in mind that this is an audio-only format primarily.

00:42:11.320 --> 00:42:14.320
Let's talk through what it's like to write a little bit of code,

00:42:14.320 --> 00:42:17.320
you know, connecting to the database, doing some queries, or adding some data.

00:42:17.320 --> 00:42:19.320
You guys have a quick start on here.

00:42:19.320 --> 00:42:23.320
Maybe you could just talk us through real quickly on how to get started in Python with it.

00:42:23.320 --> 00:42:24.320
Yeah.

00:42:24.320 --> 00:42:33.320
So overall, the goal for the quick start is, you know, you have a package that you install in seconds.

00:42:33.320 --> 00:42:48.320
And then, you know, depending on how quickly you're running through the quick start, that, you know, between like a minute to five minutes, you've got a quote unquote vector database running, embedded vector database running, where you can start building applications on top, right?

00:42:48.320 --> 00:42:51.320
So the first step is very familiar.

00:42:51.320 --> 00:42:53.320
You just run pip install lanceddb.

00:42:53.320 --> 00:43:00.320
And that pulls in, you know, lanced format and installs our Rust packages.

00:43:00.320 --> 00:43:03.320
And then you can start connecting to it, right?

00:43:03.320 --> 00:43:11.320
So the default is you just give it a local file path, and you say lanceddb.connect.

00:43:11.320 --> 00:43:21.320
And then that gives you a database connection, which-- and there's two flavors of that-- a synchronous for a client and an asynchronous client.

00:43:21.320 --> 00:43:22.320
Oh, I love it.

00:43:22.320 --> 00:43:23.320
I love it.

00:43:23.320 --> 00:43:24.320
Yeah.

00:43:24.320 --> 00:43:25.320
That's very cool.

00:43:25.320 --> 00:43:28.320
So the async is basically just connect_async.

00:43:28.320 --> 00:43:33.320
And roughly, the interfaces are roughly equivalent.

00:43:33.320 --> 00:43:42.320
There's some disparities which we're addressing now, but the main difference is you just sprinkle some await in places.

00:43:42.320 --> 00:43:51.320
The async and await keywords make concurrent programming so much easier than threads and callbacks and all that business.

00:43:51.320 --> 00:43:52.320
For sure.

00:43:52.320 --> 00:43:53.320
Yeah.

00:43:53.320 --> 00:43:59.320
So the connect bit is if people are familiar with SQLite, you just-- here's a URL to a file.

00:43:59.320 --> 00:44:01.320
That's pretty much-- you give it that.

00:44:01.320 --> 00:44:07.320
And then you have-- what's your naming convention on the async API?

00:44:07.320 --> 00:44:10.320
Are all the functions ending in _async?

00:44:10.320 --> 00:44:12.320
Or do you create like an async client?

00:44:12.320 --> 00:44:13.320
Or how do you differ?

00:44:13.320 --> 00:44:15.320
Yeah, we create-- yeah, that's a good question.

00:44:15.320 --> 00:44:16.320
Yeah.

00:44:16.320 --> 00:44:17.320
So we create an async client.

00:44:17.320 --> 00:44:21.320
The method names are the same between the two.

00:44:21.320 --> 00:44:28.320
And it's really just the-- it's really just that initial connect call that's different.

00:44:28.320 --> 00:44:29.320
So--

00:44:29.320 --> 00:44:30.320
I see.

00:44:30.320 --> 00:44:33.320
But that's kind of the factory method for the client, either sync or async, that comes back.

00:44:33.320 --> 00:44:34.320
And then you--

00:44:34.320 --> 00:44:35.320
Right.

00:44:35.320 --> 00:44:36.320
--you know what you got.

00:44:36.320 --> 00:44:37.320
Right.

00:44:37.320 --> 00:44:38.320
So--

00:44:38.320 --> 00:44:39.320
Nice.

00:44:39.320 --> 00:44:43.320
The second example, we see-- we see-- we see that in action.

00:44:43.320 --> 00:44:44.320
Uh-huh.

00:44:44.320 --> 00:44:51.320
So first step is let's create a table and let's add some-- some simple data to it.

00:44:51.320 --> 00:44:58.320
So, you know, let's-- let's say we have two fields, an item, and a price, right?

00:44:58.320 --> 00:45:00.320
And then an embedding vector.

00:45:00.320 --> 00:45:06.320
Here, for the example, we're just going to use, you know, two-- two digits for that vector.

00:45:06.320 --> 00:45:12.320
In practice, of course, it's going to be like 1,500 or, you know, 3,000 or something like that.

00:45:12.320 --> 00:45:13.320
Yeah, yeah.

00:45:13.320 --> 00:45:15.320
That's how you print it in your code.

00:45:15.320 --> 00:45:16.320
Right, right.

00:45:16.320 --> 00:45:17.320
Yeah.

00:45:17.320 --> 00:45:20.320
So here, we just have, you know, essentially digits of pi here.

00:45:20.320 --> 00:45:26.320
And then you can see whether you have the async connection or the sync connection.

00:45:26.320 --> 00:45:29.320
Both you call create table.

00:45:29.320 --> 00:45:32.320
Uh, you add it, you give it the table name.

00:45:32.320 --> 00:45:38.320
And then, optionally, you can give it data to initialize it.

00:45:38.320 --> 00:45:40.320
And there's some--

00:45:40.320 --> 00:45:42.320
Um, oh, yeah.

00:45:42.320 --> 00:45:51.320
So if you have data initially, the schema is inferred from the-- the-- the data that you provide it when you call create table.

00:45:51.320 --> 00:46:04.320
Um, if you go to the next example, you can also initialize an empty table that has-- that has-- with just a schema.

00:46:04.320 --> 00:46:10.320
So here in this quick start, it's-- you can have a arrow schema.

00:46:10.320 --> 00:46:18.320
Uh, and basically, this is the same schema as the data before, but you create an empty table, and then you can add data to it later.

00:46:18.320 --> 00:46:29.320
Uh, what's-- I-- I think I should update this, but I think what's more convenient is that we've, as you see in the-- the box below, you can actually-- we've added pedantic support.

00:46:29.320 --> 00:46:29.320
So--

00:46:29.320 --> 00:46:30.320
Beautiful.

00:46:30.320 --> 00:46:33.320
Um, there's a translation layer between pedantic and-- and arrow schemas.

00:46:33.320 --> 00:46:46.320
And so it's-- I think for a lot of our Python users, it's much easier to think in terms of pedantic objects as the data model, rather than manually dealing with the pyro API to create a schema.

00:46:46.320 --> 00:46:56.320
Like, you know, like, how do I, you know, we saw lots of, issues where users like, well, I-- I, how do I create a fixed size list?

00:46:56.320 --> 00:46:58.320
What is a fixed size list?

00:46:58.320 --> 00:46:59.320
Mm-hmm.

00:46:59.320 --> 00:47:03.320
And, you know, like, why does my vector has to be-- have to be that?

00:47:03.320 --> 00:47:08.320
Um, you know, is it float-- should I do float 32 or 64?

00:47:08.320 --> 00:47:15.320
There's lots of stuff that, is just much easier to-- to think of in terms of Python types and Python objects.

00:47:15.320 --> 00:47:19.320
I love the Python-- the pydantic integration there.

00:47:19.320 --> 00:47:20.320
That's-- that's super cool.

00:47:20.320 --> 00:47:27.320
The data layer used for my course's website and the podcast website and stuff is all based on Beanie and Mongo.

00:47:27.320 --> 00:47:44.320
And Mongo, which is async-- basically you're writing async queries against pydantic models, which is, you know, it's a real-- got the validation, but you're not writing directly to the database and just random dictionaries and who knows if it stays consistent.

00:47:44.320 --> 00:47:47.320
So speaking of which, you have a schema here.

00:47:47.320 --> 00:47:51.320
Is this-- how hard-- how strictly is this enforced, right?

00:47:51.320 --> 00:47:54.320
Is this Postgres level or is this MongoDB level?

00:47:54.320 --> 00:47:55.320
Like, uh--

00:47:55.320 --> 00:47:56.320
Well, this is--

00:47:56.320 --> 00:47:57.320
Probably should be this.

00:47:57.320 --> 00:47:58.320
Yeah, this is--

00:47:58.320 --> 00:47:59.320
Right.

00:47:59.320 --> 00:48:00.320
This is fairly strictly enforced.

00:48:00.320 --> 00:48:01.320
So think--

00:48:01.320 --> 00:48:02.320
Okay.

00:48:02.320 --> 00:48:05.320
Think like arrow tables and, you know, writing to-- writing arrow tables.

00:48:05.320 --> 00:48:14.320
I think there's some-- there's a little bit of magic-- there's a little bit of, like, give around, like, nullability.

00:48:14.320 --> 00:48:23.320
Because a lot of times if you're providing data, the-- the-- the types can be inferred, which is-- it's easier to do casting.

00:48:23.320 --> 00:48:34.320
And then, if you're inferring, like, nullable versus nullable-- non-nullable, that can get you in trouble, when-- when you're inserting data.

00:48:34.320 --> 00:48:43.320
So, for example, if you have a-- if you have a non-nullable column that you've declared in the schema, but new data coming in,

00:48:43.320 --> 00:48:53.320
sometimes that translation layer into arrow just automatically turns it into a nullable, even though you-- you have-- you didn't give it any nulls in the-- in the data.

00:48:53.320 --> 00:48:54.320
Right.

00:48:54.320 --> 00:48:56.320
And then, when you insert it, it'll produce an error.

00:48:56.320 --> 00:48:57.320
So--

00:48:57.320 --> 00:49:04.320
Do you respect things like, optional-- optional typing in Pydantic models to control nullability?

00:49:04.320 --> 00:49:07.320
Like, optional float versus float being not nullable?

00:49:07.320 --> 00:49:08.320
Yeah.

00:49:08.320 --> 00:49:09.320
Yep.

00:49:09.320 --> 00:49:10.320
So we do a bunch of that as well.

00:49:10.320 --> 00:49:15.320
Um, so in terms of Pydantic integration, you know, it's another Rust project.

00:49:15.320 --> 00:49:19.320
You know, we've-- we've loved working with it, for a long time.

00:49:19.320 --> 00:49:32.320
Um, if any listeners are-- are out there that's interested in just messing with, like, Pydantic and Arrow, so that-- that translation layer, I think, we-- we'd-- we'd love to get some help as well.

00:49:32.320 --> 00:49:40.320
So for the-- for some of the more, like, complicated nested types out there, so where that's, like, lists of lists or--

00:49:40.320 --> 00:49:47.320
lists of fixed-- fixed with lists and dictionaries and-- and that kind of thing, that-- the translation layer is-- is incomplete.

00:49:47.320 --> 00:49:56.320
Um, but we know a couple of other companies and tools in the ecosystem who've also built their own kind of translation layer.

00:49:56.320 --> 00:50:09.320
So it would be really interesting, I think, if there was a community member who can lead, say, like, let's create a-- let's-- let's get a bunch of projects together and let's create a standardized translation layer.

00:50:09.320 --> 00:50:11.320
Uh, I-- I think that would be--

00:50:11.320 --> 00:50:13.320
Not everyone just doing their own copy, right?

00:50:13.320 --> 00:50:14.320
Right, right, right.

00:50:14.320 --> 00:50:20.320
And then maybe, like, either Pydantic or maybe Arrow, one of the two projects can own that translation layer.

00:50:20.320 --> 00:50:25.320
Uh, I think that would be-- really great for the-- for the ecosystem.

00:50:25.320 --> 00:50:26.320
Yeah.

00:50:26.320 --> 00:50:28.320
That sounds like a fantastic idea.

00:50:28.320 --> 00:50:35.320
And let's maybe wrap up our little code sample here with-- talking about querying the database or doing a search.

00:50:35.320 --> 00:50:36.320
Yep.

00:50:36.320 --> 00:50:42.320
Because it's fun-- I know it is fun to put data into a database and define schemas, but the actual purpose is to ask questions, right?

00:50:42.320 --> 00:50:44.320
That's right, that's right.

00:50:44.320 --> 00:50:56.320
Um, yeah, and so we-- we wanted to make it so that it's really familiar for people who've, you know, worked with databases and data frame engines.

00:50:56.320 --> 00:51:05.320
So, the-- the main workload for-- excuse me-- the main workload for, Lansi B open source is-- is that search API.

00:51:05.320 --> 00:51:21.320
So, well, when you, you can say table.search and you can, pass in the vector-- the query vector, and then you can call dot limit to say how many results we want.

00:51:21.320 --> 00:51:29.320
And then a dot two underscore, you know, blah, blah, blah, to, determine what format you want the results back.

00:51:29.320 --> 00:51:38.320
So you have two pandas here in the, in the example, but you can convert it to pollers, or you can just get it back as a-- as a list.

00:51:38.320 --> 00:51:39.320
Awesome.

00:51:39.320 --> 00:51:40.320
Yeah, that's really cool.

00:51:40.320 --> 00:51:41.320
I love it.

00:51:41.320 --> 00:51:43.320
And then-- oh, and also a couple of other things.

00:51:43.320 --> 00:51:50.320
Uh, so here you can also do-- if you have a Pydantic model that you use as the table schema, you can also say two Pydantic models.

00:51:50.320 --> 00:51:57.320
Also say two pedantic and pass in the model, and it'll automatically return a list of, pedantic objects back-- back to you.

00:51:57.320 --> 00:51:58.320
Oh, that's cool.

00:51:58.320 --> 00:52:08.320
This is particularly useful for, like multi-step, like, agent workflows where you want that structured data and, it's easy to connect it to the rest of your--

00:52:08.320 --> 00:52:09.320
Right.

00:52:09.320 --> 00:52:16.320
You would just maybe want to take your pedantic and say, turn that into JSON and hand it over to the next agent or something like that.

00:52:16.320 --> 00:52:17.320
Exactly.

00:52:17.320 --> 00:52:18.320
Yep.

00:52:18.320 --> 00:52:19.320
Nice.

00:52:19.320 --> 00:52:27.320
So in-- in addition to, creating-- if we-- if we sort of move forward in that example a little bit.

00:52:27.320 --> 00:52:28.320
Yeah.

00:52:28.320 --> 00:52:41.320
Um, in addition to, setting up the schema, there's also the, embedding API that's really interesting.

00:52:41.320 --> 00:52:50.320
So that when you create the schema-- so here's an example where we can create the schema using, pedantic.

00:52:50.320 --> 00:52:59.320
So in this block, I've declared the class words, which is a LANS model.

00:52:59.320 --> 00:53:01.320
And the LANS model is just a pedantic base model that knows how to convert itself to-- to Aero.

00:53:01.320 --> 00:53:02.320
Right.

00:53:02.320 --> 00:53:05.320
Um, and then I have two fields in here, text and vector.

00:53:05.320 --> 00:53:15.320
And what I-- what we-- we have in LANS CB is an embedding registry where you can-- here I declared a, function--

00:53:15.320 --> 00:53:18.320
of, embedding function called func.

00:53:18.320 --> 00:53:22.320
And basically I call get registry dot get open AI dot create.

00:53:22.320 --> 00:53:24.320
And I give it the name of the embedding.

00:53:24.320 --> 00:53:30.320
Here we're using ADA2, but, you know, new-- new models have been released since.

00:53:30.320 --> 00:53:38.320
And I can use the pedantic annotations to-- to say, hey, the text field is the source field for that function.

00:53:38.320 --> 00:53:41.320
So it's text string equals func dot source field.

00:53:41.320 --> 00:53:45.320
Um, and then the function itself knows how many dimensions it is.

00:53:45.320 --> 00:53:50.320
So I don't need to think about, like, how many dimensions this-- the-- the vector-- the embedding has.

00:53:50.320 --> 00:53:57.320
Um, and then once I've-- once I've declared this schema, I can call the familiar dot create table workflow.

00:53:57.320 --> 00:54:02.320
Um, and then I can call table dot add to add data to it.

00:54:02.320 --> 00:54:10.320
And here, because I've declared the embedding function in the schema, I don't actually have to generate the embeddings myself.

00:54:10.320 --> 00:54:15.320
So in this example, I've-- I'm only passing in the-- the text field as input.

00:54:15.320 --> 00:54:16.320
Oh, that's really cool.

00:54:16.320 --> 00:54:27.320
And LANCB, yeah, just takes care of calling the open AI API, on your behalf, and then adding the vectors before, you know, adding the whole batch to the table.

00:54:27.320 --> 00:54:33.320
So open AI was-- was our first one, but there's dozens of compatible embedding models.

00:54:33.320 --> 00:54:40.320
So pretty much anything you can pull off hugging face, you know, like, and then there's lots of--

00:54:40.320 --> 00:54:43.320
um, other vendors like Coher, for example.

00:54:43.320 --> 00:54:50.320
Um, if you were running open source like Olama, there's also an-- a-- a-- integration for that.

00:54:50.320 --> 00:54:51.320
Nice.

00:54:51.320 --> 00:54:55.320
You can use some of the on machine ones potentially as well.

00:54:55.320 --> 00:54:56.320
Exactly, yeah.

00:54:56.320 --> 00:55:02.320
So a lot of the-- the ones you can pull off hugging face can just run, locally.

00:55:02.320 --> 00:55:03.320
Yeah.

00:55:03.320 --> 00:55:04.320
And it exposes the options.

00:55:04.320 --> 00:55:13.320
So if you do have, you know, for-- for your, Mac Mini or, or even your MacBook laptop, there are options.

00:55:13.320 --> 00:55:19.320
There are lots of hugging face models where you can specify MPS, to actually make it run a little bit faster.

00:55:19.320 --> 00:55:20.320
Oh, nice.

00:55:20.320 --> 00:55:24.320
That uses the neural processing units or something instead of CPU or GPU?

00:55:24.320 --> 00:55:25.320
Yeah.

00:55:25.320 --> 00:55:26.320
Okay.

00:55:26.320 --> 00:55:27.320
Yeah.

00:55:27.320 --> 00:55:28.320
Okay. That's cool. I didn't know that.

00:55:28.320 --> 00:55:29.320
Very nice.

00:55:29.320 --> 00:55:39.320
And it looks like-- I did want to ask you, sort of what's the integration with things like OpenAI or Gemini or other things?

00:55:39.320 --> 00:55:45.320
It looks like a lot of Lance DB is kind of for you building your app that is self-contained.

00:55:45.320 --> 00:55:49.320
But also, you know, here's integration with some of the OpenAI API stuff.

00:55:49.320 --> 00:55:58.320
What's-- how much should I-- how much do you depend on using other external AI systems versus just your own?

00:55:58.320 --> 00:55:59.320
Right.

00:55:59.320 --> 00:56:08.320
So I think typically the-- the embedding model is you-- you-- you either bring your own run locally or you call a--

00:56:08.320 --> 00:56:10.320
third-party API.

00:56:10.320 --> 00:56:12.320
And then the--

00:56:12.320 --> 00:56:16.320
and then the actual completion, right?

00:56:16.320 --> 00:56:19.320
That's-- that's outside of the scope of-- of Lance DB.

00:56:19.320 --> 00:56:26.320
So the-- the prompt engineering and the calling the completion model once you have the-- the context retrieved,

00:56:26.320 --> 00:56:31.320
that's-- that's not part of the Lance DB API.

00:56:31.320 --> 00:56:36.320
That said, so we'll-- we integrate with, let's say, like, a lane chain or a llama index.

00:56:36.320 --> 00:56:47.320
So if you're comfortable with that layer of sort of AI orchestration or RAG orchestration, you can use their APIs and plug in Lance DB into that.

00:56:47.320 --> 00:56:50.320
So that-- that takes care of a lot of that.

00:56:50.320 --> 00:56:53.320
The other parts of the-- the AI workflow.

00:56:53.320 --> 00:57:07.320
And then if the-- and then if you're in the-- let's say, like, AWS ecosystem, and you're familiar with, you know, Bedrock and things like that, there are-- there are a few--

00:57:07.320 --> 00:57:19.320
AWS folks who've built a complete serverless Rack stack where they're using the Bedrock APIs for embedding creation and then for completion.

00:57:19.320 --> 00:57:28.320
But then they have Lance data that's sitting on S3 and they're running Lance DB open source in an AWS Lambda function.

00:57:28.320 --> 00:57:40.320
So that you can essentially have-- you don't have to manage any servers and just be calling serverless functions to build your Rack application.

00:57:40.320 --> 00:57:41.320
Sounds super cool.

00:57:41.320 --> 00:57:43.320
All right, we're getting short-- short on time.

00:57:43.320 --> 00:57:46.320
Let's wrap it up with a couple of things here.

00:57:46.320 --> 00:57:50.320
First of all, we have the open source Lance DB.

00:57:50.320 --> 00:57:53.320
And then you talked about the enterprise stuff as well.

00:57:53.320 --> 00:57:55.320
Let's just talk business on a while really quick.

00:57:55.320 --> 00:58:10.320
I think it's interesting-- always interesting to see companies that have maybe open core or some kind of open source foundation and how you guys are making this work for both contributing to open source but also eating.

00:58:10.320 --> 00:58:11.320
Yep.

00:58:11.320 --> 00:58:13.320
What's the-- what's the story here?

00:58:13.320 --> 00:58:14.320
Right, right, right.

00:58:14.320 --> 00:58:22.320
So, yeah, I mean, it's-- it's always-- I think it's always a very complicated topic.

00:58:22.320 --> 00:58:30.320
I think the way that we think about it is your-- your journey building-- building AI.

00:58:30.320 --> 00:58:43.320
So, when you're just starting out and you're doing that prototype and that MVP, a lot of times you don't even know the-- the value of the-- the thing that you want to build or whether it works.

00:58:43.320 --> 00:58:57.320
So, why would you want to, like, go through the hassle of managing complicated infrastructure or pay a third party vendor to just have some small amounts of data, right?

00:58:57.320 --> 00:59:04.320
So-- so we wanted to make Lance DB open source super easy for you to just get started.

00:59:04.320 --> 00:59:05.320
to just get started.

00:59:05.320 --> 00:59:10.320
And also just bridge you into-- into production in that early stage.

00:59:10.320 --> 00:59:25.320
The-- the cloud and enterprise offerings are essentially for when you go into production and you want to have a super scalable and highly performant vector database where you-- you still don't have to worry about the infrastructure, but--

00:59:25.320 --> 00:59:28.320
you have lots more challenging systems needs.

00:59:28.320 --> 00:59:33.320
And so Lance DB enterprise-- Lance DB cloud is-- it's a hosted serverless offering.

00:59:33.320 --> 00:59:45.320
So for, you know, small teams that have production needs, but maybe don't have, you know, billions of vectors and-- or, or, you know, really challenging security requirements and things like that.

00:59:45.320 --> 00:59:54.320
And Lance DB enterprises, you know, we really have, OK, I need, like, 1000 to 10,000 queries per second.

00:59:54.320 --> 01:00:00.320
I have just a-- a-- a-- a ridiculous amount of data to catch myself before I curse.

01:00:00.320 --> 01:00:01.320
And--

01:00:02.320 --> 01:00:03.320
On-prem.

01:00:03.320 --> 01:00:04.320
Yeah.

01:00:04.320 --> 01:00:08.320
And-- and I need to-- I need my data to live on-- on-prem.

01:00:08.320 --> 01:00:14.320
And so the enterprise comes in-- in two different packages.

01:00:14.320 --> 01:00:21.320
One is a-- a managed offering where you can bring your own bucket.

01:00:21.320 --> 01:00:24.320
And then we just run the compute for you and give you a private link.

01:00:24.320 --> 01:00:28.320
Or it's-- it can be fully-- we call it BYOC.

01:00:28.320 --> 01:00:31.320
So it runs within the customer account.

01:00:31.320 --> 01:00:35.320
So nothing leaves the premises except for basic telemetry data.

01:00:35.320 --> 01:00:36.320
Yeah.

01:00:36.320 --> 01:00:37.320
Nice.

01:00:37.320 --> 01:00:37.320
Awesome.

01:00:37.320 --> 01:00:38.320
All right.

01:00:38.320 --> 01:00:44.320
And then, you know, quickly close it out with, I guess, what's next?

01:00:44.320 --> 01:00:46.320
Like, where are things going here?

01:00:46.320 --> 01:00:50.320
I see Lance DB Cloud is in private beta, early access mode.

01:00:50.320 --> 01:00:52.320
You know, what are you-- where are you all going?

01:00:52.320 --> 01:00:53.320
Yep.

01:00:53.320 --> 01:01:00.320
So, I think there's a couple of really exciting, things.

01:01:00.320 --> 01:01:07.320
So first-- first on the open source, we are-- you know, our vision for-- for Lance is,

01:01:07.320 --> 01:01:14.320
is-- is to make it the standard for working with AI, right?

01:01:14.320 --> 01:01:24.320
So, I think we already have lots of folks who are depending on Lance, with, image data, video data, large-scale training.

01:01:24.320 --> 01:01:27.320
Um, and we'll continue to-- to make that better.

01:01:27.320 --> 01:01:29.320
And then add additional encoding.

01:01:29.320 --> 01:01:39.320
So for, folks that-- that not just have those, but, you know, compressing text data and the metadata that they have to make it as efficient as possible.

01:01:40.320 --> 01:01:51.320
Um, and I think there we've-- you know, between Lance and Lance EB, I think we just-- we just broke through 2.2 million monthly downloads.

01:01:51.320 --> 01:01:53.320
Uh, so we're really excited about that and--

01:01:53.320 --> 01:01:54.320
Awesome.

01:01:54.320 --> 01:01:56.320
--and that community uptake as-- as well.

01:01:56.320 --> 01:02:06.320
And, you know, we're getting lots of community collaborators and contributors, and we're looking to grow that-- grow that, community.

01:02:06.320 --> 01:02:18.320
So on the open source side, it's, you know, better APIs, more automation around the data management, like compaction, and then, just better encodings for the non-large blob types, right?

01:02:18.320 --> 01:02:23.320
Smaller strings, numerical data, and things like that.

01:02:23.320 --> 01:02:36.320
And then for, Lance EB Enterprise, you know, a lot of times, if you're, if you have to, look at the-- the whole search engine, right?

01:02:36.320 --> 01:02:47.320
So right now, our customers still have to do the chunking and embedding themselves, but we're basically looking to make it much easier for those types of workloads, not just in terms of product, but also in terms of product.

01:02:47.320 --> 01:02:53.320
Um, to also save them cost as well in terms of embedding.

01:02:53.320 --> 01:03:00.320
So a lot of times what we find is our customers will update a table with a new version that's like 80% the same.

01:03:00.320 --> 01:03:16.320
And so, so if we have the right APIs and manage the embedding calls for them in the, we can essentially save 80% of that cost in terms of embedding APIs without complicated sort of,

01:03:16.320 --> 01:03:21.320
um, like query cache or embedding cache that has more complexity, right?

01:03:21.320 --> 01:03:28.320
So, and then just deepening the-- so complete, more complete search engine for AI.

01:03:28.320 --> 01:03:43.320
And then on the offline side, complete, complete features at scale around, building that training, pre-processing and, and exploratory data analysis workflow for, the-- for those types of customers.

01:03:43.320 --> 01:03:44.320
Awesome.

01:03:44.320 --> 01:03:50.320
Well, it looks like a really cool product and set of services and nice work.

01:03:50.320 --> 01:03:51.320
Thank you.

01:03:51.320 --> 01:03:52.320
Yeah.

01:03:52.320 --> 01:03:53.320
You got it.

01:03:53.320 --> 01:03:54.320
Thanks for being here.

01:03:54.320 --> 01:03:55.320
You know, final parting thoughts.

01:03:55.320 --> 01:03:56.320
People want to get started.

01:03:56.320 --> 01:03:57.320
Lance DB, what do you tell them?

01:03:57.320 --> 01:04:00.320
Oh, come to our, come to our discord.

01:04:00.320 --> 01:04:09.320
And, I think I, I forgot if you have the, if I give you the, the,

01:04:09.320 --> 01:04:13.320
I don't know if I have the link, but give it to me and I'll put it in the show notes for people later.

01:04:13.320 --> 01:04:14.320
Yeah, sounds good.

01:04:14.320 --> 01:04:16.320
Uh, so come to our discord.

01:04:16.320 --> 01:04:22.320
Uh, we're all in there and it's, it's pretty active and we respond, fairly quickly.

01:04:22.320 --> 01:04:27.320
So, and it's, there's no, there's no, there's not a lot of noise.

01:04:27.320 --> 01:04:34.320
So it's mostly sort of practical topics, debugging and talking about new features.

01:04:34.320 --> 01:04:38.320
Um, maybe having a little bit of fun with, with new examples and things like that.

01:04:38.320 --> 01:04:39.320
Awesome.

01:04:39.320 --> 01:04:39.320
All right.

01:04:39.320 --> 01:04:40.320
Well, thanks for being here.

01:04:40.320 --> 01:04:41.320
See you later.

01:04:41.320 --> 01:04:42.320
Thank you.

01:04:42.320 --> 01:04:42.880
Thank you.

01:04:42.880 --> 01:04:46.000
Thank you.

