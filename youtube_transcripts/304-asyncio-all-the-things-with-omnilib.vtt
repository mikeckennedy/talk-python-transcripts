WEBVTT

00:00:00.000 --> 00:00:06.200
>> Hello out there on the live stream. Thank you everyone for joining us. It's great to


00:00:06.200 --> 00:00:10.760
have you here. If you have questions, please put them in the comments and we'll try to


00:00:10.760 --> 00:00:16.240
make them part of the show. So without further ado, we'll get started on the podcast. My


00:00:16.240 --> 00:00:20.240
guest is John Reese. John, welcome to Talk Python to Me.


00:00:20.240 --> 00:00:22.280
>> Howdy. It's good to be here.


00:00:22.280 --> 00:00:26.920
>> Yeah, it's great to have you here as well. It's going to be a lot of fun to talk to you


00:00:26.920 --> 00:00:33.760
about async stuff. I think we both share a lot of admiration and love for asyncioing


00:00:33.760 --> 00:00:34.760
all the things.


00:00:34.760 --> 00:00:43.560
>> I definitely do. It's one of those cases where the things that it enables is so different


00:00:43.560 --> 00:00:48.800
and you have to think about everything so differently when you're using asyncio that


00:00:48.800 --> 00:00:55.640
it's a nice challenge, but also has potentially really high payoff if it's done well.


00:00:55.640 --> 00:01:00.480
>> Yeah, it has a huge payoff. I think that it's been a little bit of a mixed bag in the


00:01:00.480 --> 00:01:05.080
terms of the reception that people have had. I know there have been a couple of folks who've


00:01:05.080 --> 00:01:10.720
written articles like, "Well, I tried it. It wasn't that great." I've had examples where


00:01:10.720 --> 00:01:14.760
I'm doing something like web scraping or actually got a message from somebody who listened.


00:01:14.760 --> 00:01:18.480
I think maybe they were listening to Python Bytes, my other podcast. But anyway, I got


00:01:18.480 --> 00:01:22.880
a message from a listener after we covered some cool asyncio things and web scraping.


00:01:22.880 --> 00:01:28.720
They had to download a bunch of stuff. It takes a day. Literally, it takes all day or


00:01:28.720 --> 00:01:32.120
something. It was really crazy. Then they said, "Well, now I'm using async and now my


00:01:32.120 --> 00:01:36.560
computer runs out of memory and crashes. It's getting it so fast." That's a large difference


00:01:36.560 --> 00:01:42.560
right there. There's certainly a category of things where it's amazing.


00:01:42.560 --> 00:01:51.880
>> Yeah, I think the case we've seen it most useful for is definitely doing those sorts


00:01:51.880 --> 00:01:58.400
of concurrent web requests. Internally, it's also extraordinarily useful in monitoring


00:01:58.400 --> 00:02:04.680
situations where you want to be able to talk to a whole bunch of servers as fast as possible.


00:02:04.680 --> 00:02:07.900
Maybe the amount of stuff that comes back from it is not as important as being able


00:02:07.900 --> 00:02:15.520
to just talk to them repeatedly. But you're right. There's definitely a lot of cases where


00:02:15.520 --> 00:02:21.080
people are not necessarily using it correctly or they're hoping to add a little bit of async


00:02:21.080 --> 00:02:25.680
into an existing thing. That doesn't always work as well as just building something that's


00:02:25.680 --> 00:02:26.960
async from the start.


00:02:26.960 --> 00:02:31.720
>> Yeah, and there's more frameworks these days that are welcoming of async from the


00:02:31.720 --> 00:02:32.720
start, I guess.


00:02:32.720 --> 00:02:33.720
>> Yeah.


00:02:33.720 --> 00:02:36.520
>> We're going to talk about that. But before we get too far down the main topic, let's


00:02:36.520 --> 00:02:39.160
just start with a little bit of background on you. How did you get into programming in


00:02:39.160 --> 00:02:40.160
Python?


00:02:40.160 --> 00:02:47.360
>> Sure. My first interaction with a computer was when I was maybe five or six years old.


00:02:47.360 --> 00:02:55.680
My parents had a TI-99-4A, which is like the knockoff Commodore attached to the television.


00:02:55.680 --> 00:02:58.400
And my biggest memory was that.


00:02:58.400 --> 00:03:03.000
>> I think back to that. How could you have legible text on the CRT?


00:03:03.000 --> 00:03:04.960
>> It was pretty bad.


00:03:04.960 --> 00:03:07.440
>> It's bad, right?


00:03:07.440 --> 00:03:12.440
>> My biggest memory of it is really just every time we would try to play a game and


00:03:12.440 --> 00:03:16.880
the cartridge or tape or whatever wouldn't work correctly, it would just dump you at


00:03:16.880 --> 00:03:22.480
a basic prompt where it's just expecting you to start typing some programming in. And nobody


00:03:22.480 --> 00:03:27.640
in my family had a manual or knew anything about programming at the time. So it's like


00:03:27.640 --> 00:03:31.880
there was like, I think maybe we figured out that you could print something to the screen,


00:03:31.880 --> 00:03:40.600
but nothing beyond that. And it wasn't until we ended up getting a DOS computer a few years


00:03:40.600 --> 00:03:48.160
later that I really started to actually do some quote unquote real programming where


00:03:48.160 --> 00:03:56.800
we were writing like batch scripts to do menus for like deciding what program to run or things


00:03:56.800 --> 00:04:01.400
like autoexec bat on a floppy disk in order to boot into a game.


00:04:01.400 --> 00:04:05.960
>> I was just thinking of all the autoexec bat stuff that we had to do. Like, oh, you


00:04:05.960 --> 00:04:09.720
want to play Doom, but you don't have enough high memory, whatever the heck that was. And


00:04:09.720 --> 00:04:13.640
so you've got to rearrange where the drivers are. I mean, what's what a weird way to just


00:04:13.640 --> 00:04:16.520
go, I want to play a game. So I've got to rework where my drivers are.


00:04:16.520 --> 00:04:21.760
>> Make sure you don't load your mouse driver when you're booting into this one game that


00:04:21.760 --> 00:04:27.200
doesn't need the mouse because otherwise you run out of memory. Yeah, it was kind of crazy.


00:04:27.200 --> 00:04:32.360
And my biggest memory of programming there was, you know, there was QBasic on it. And


00:04:32.360 --> 00:04:37.120
it came with this gorilla game where you just like throw bandanas at another gorilla from


00:04:37.120 --> 00:04:43.520
like some sort of like city skyline. And I had again, not really


00:04:43.520 --> 00:04:47.280
>> King Kong knockoff, Donkey Kong knockoff type thing.


00:04:47.280 --> 00:04:53.840
>> Yeah, exactly. And I would struggle to figure out how that was actually doing anything.


00:04:53.840 --> 00:04:57.480
And it was like, I'd try to poke at it and figure it out as I went. Didn't really do


00:04:57.480 --> 00:05:04.320
that much. But it was actually my first opportunity for quote unquote, open source projects, because


00:05:04.320 --> 00:05:09.360
there's a video game that I really, really liked called NASCAR racing. And one of the


00:05:09.360 --> 00:05:16.680
things that I learned was you is a burgeoning part of the internet, for me, at least was


00:05:16.680 --> 00:05:21.600
people would host these mods for the game on like GeoCities or whatever. And so these


00:05:21.600 --> 00:05:26.460
these would change like the models for the cars or the wheels or add tracks or textures


00:05:26.460 --> 00:05:31.240
or whatever. And I actually wrote a batch script that would let you like at the time


00:05:31.240 --> 00:05:35.000
that you wanted to play the game, pick which of the ones you had enabled, because you couldn't


00:05:35.000 --> 00:05:39.480
have them all enabled. So it would like is basically just a batch script that would go


00:05:39.480 --> 00:05:44.520
and like copy a bunch of files around from one place to another. And then when you're


00:05:44.520 --> 00:05:49.060
done with the menus or whatever, then it would launch the game. And I remember posting that


00:05:49.060 --> 00:05:55.160
on GeoCities and, you know, having the silly little like JavaScript counter or whatever


00:05:55.160 --> 00:06:00.920
it was, you know, take up to like a couple hundred page views of people downloading just


00:06:00.920 --> 00:06:07.380
this script to to switch mods in and out. And so that was like the first real taste


00:06:07.380 --> 00:06:13.320
of open source programming or open source projects that I had. But that actually like


00:06:13.320 --> 00:06:19.160
led into the way that I really learned programming, which was, I wanted to have my own website


00:06:19.160 --> 00:06:25.320
that was more dynamic than what GeoCities had. And so I ended up basically picking up


00:06:25.320 --> 00:06:32.440
Perl and eventually PHP to write your web pages that I hosted on my own machine at home


00:06:32.440 --> 00:06:33.440
from like IIS and...


00:06:33.440 --> 00:06:39.240
How did you get... what did you use like DynDNS or something like that?


00:06:39.240 --> 00:06:45.400
Yes, exactly. DynDNS. It was it was the jankiest setup, but it at least worked and I could


00:06:45.400 --> 00:06:53.080
impress my friends. And it wasn't until I got to college and I was working on my first


00:06:53.080 --> 00:07:00.480
internship where the main project I was working on was essentially improving an open source


00:07:00.480 --> 00:07:06.800
bug tracker written in PHP in order to make it do the things that my company wanted to


00:07:06.800 --> 00:07:12.240
be able to do in it. So like adding a plug in system and things like that. And in the


00:07:12.240 --> 00:07:17.480
process of that, I eventually became a maintainer of the project and they had a bunch of Python


00:07:17.480 --> 00:07:23.600
scripts for managing releases, like doing things like creating the release tarballs,


00:07:23.600 --> 00:07:28.560
running other sort of like linter type things over the code base. That was my very first


00:07:28.560 --> 00:07:34.600
taste of Python. And I hated it because it was just like I couldn't get past the concept


00:07:34.600 --> 00:07:41.640
of like you're forcing me to do white space. Like how barbaric is this? But it actually


00:07:41.640 --> 00:07:47.200
didn't take long before I realized that that actually makes the code more readable. It's


00:07:47.200 --> 00:07:51.600
like you can literally pick up anybody else's Python script and it looks almost exactly


00:07:51.600 --> 00:07:53.680
like how you would have done it yourself.


00:07:53.680 --> 00:08:00.160
Yeah. And you got a lot of the PEP 8 rules and tools that automatically re format stuff


00:08:00.160 --> 00:08:06.600
into that. So it's very likely you've got black and PyCharms reformatting and whatnot.


00:08:06.600 --> 00:08:11.400
This was all before that. So I think this was when like Python 2.6 was the latest. This


00:08:11.400 --> 00:08:16.040
was quite a while ago, but right before the big diversion where there was. Yeah, yeah,


00:08:16.040 --> 00:08:20.320
exactly. Like I had no idea what Python three was until like three, two or three, three


00:08:20.320 --> 00:08:24.960
came out because it was just sequestered in this world of writing scripts for whatever


00:08:24.960 --> 00:08:31.960
version of Python was on my Linux box at the time. Right, right, right. Yeah. You know,


00:08:31.960 --> 00:08:37.200
I suspect in the early days, probably the editors were not as friendly or accommodating.


00:08:37.200 --> 00:08:41.760
Right. Like now if you work with PyCharm or VS Code or something, you just write code


00:08:41.760 --> 00:08:46.360
and it automatically does the formatting and the juggling and whatnot. You know, once you


00:08:46.360 --> 00:08:49.480
get used to it, you don't really think that much about it. It just magically happens as


00:08:49.480 --> 00:08:55.760
you work on code. Yeah. I'm wanting to say at the time I was just doing something stupid


00:08:55.760 --> 00:09:02.880
like notepad plus plus or, you know, one of the other like really generic free text editors


00:09:02.880 --> 00:09:10.520
like notepad, but it was a clip. It might've been a clips. Yeah. Was it maybe PIDev? I


00:09:10.520 --> 00:09:18.320
don't think I ever use a Python specific editor. Like I think I've tried PyCharm exactly once


00:09:18.320 --> 00:09:23.920
and I do just enough stuff that's not Python that I don't want to deal with a, an ID or


00:09:23.920 --> 00:09:30.520
editor that's not generalized. Right. Sure. Makes sense. Speaking of stuff you work on,


00:09:30.520 --> 00:09:36.440
what do you do day to day? What kind of stuff do you do? Sure. So I'm a production engineer


00:09:36.440 --> 00:09:44.240
at Facebook on our internal Python foundation team. And so most of what I do there is, you


00:09:44.240 --> 00:09:51.440
know, building infrastructure or developer tools primarily enabling engineers, data scientists,


00:09:51.440 --> 00:09:58.720
and AI or ML researchers to, you know, do what they do in Python every day. So some


00:09:58.720 --> 00:10:04.880
of that is like building out the system that allows us to integrate open source third party


00:10:04.880 --> 00:10:11.480
packages into the Facebook repository. Some of that is literally developing new open source


00:10:11.480 --> 00:10:20.760
tools for developers to use. So a while back I built a tool called, called Bowler that


00:10:20.760 --> 00:10:26.600
is basically a refactoring tool for Python. And it's based off of lib two to three that's


00:10:26.600 --> 00:10:34.240
in open source Python and essentially gives you a way to make safe code modifications


00:10:34.240 --> 00:10:40.600
rather than using regular expressions, which are terrible. Yeah. And like the AST or something


00:10:40.600 --> 00:10:46.400
like that. Yeah, exactly. Yeah. Okay. And it's like the benefit of lib CST is that


00:10:46.400 --> 00:10:52.680
it takes in the concrete syntax tree. So it keeps track of all the white space comments


00:10:52.680 --> 00:11:00.160
and everything else. So that if you modify the tree in, in lib two to three, it will


00:11:00.160 --> 00:11:05.800
then allow you to write that back out exactly the way the file came in. Whereas the AST


00:11:05.800 --> 00:11:09.040
module would have thrown all of that, you know, metadata away.


00:11:09.040 --> 00:11:12.080
Right. Formatting spaces, whatever, it doesn't care.


00:11:12.080 --> 00:11:20.840
Yeah. And one of the newer projects I've worked on is called usort. And it's like micro sort.


00:11:20.840 --> 00:11:26.560
Essentially it's a replacement that we're using internally for isort because isort has


00:11:26.560 --> 00:11:32.520
some potentially destructive behaviors in its default configuration. And our goal was


00:11:32.520 --> 00:11:41.600
essentially to get import sorting done in a way that does not require adding comment


00:11:41.600 --> 00:11:43.640
directives all over the place. Right.


00:11:43.640 --> 00:11:49.520
So the obvious example of that would be if you import some module and then you


00:11:49.520 --> 00:11:54.760
won't need to call a function out of it, like maybe that function will modify the import


00:11:54.760 --> 00:12:00.760
semantics or add in a special import hook or things like that. Or turn off network accesses


00:12:00.760 --> 00:12:08.680
like the two main use cases we see. And then you go and import more stuff after that. With


00:12:08.680 --> 00:12:12.720
isort, it would try to move all those imports above the function call that blocks the network


00:12:12.720 --> 00:12:13.720
access.


00:12:13.720 --> 00:12:16.640
I see. Yeah, yeah, yeah. And you want that to happen first and then it can go crazy.


00:12:16.640 --> 00:12:21.760
And you can't just put a skip directive on that function call because that just means


00:12:21.760 --> 00:12:27.040
isort won't try to sort that one, but it'll sort everything else around it as well. And


00:12:27.040 --> 00:12:31.480
so what we ended up seeing was a lot of developers doing things like isort skip file and it just


00:12:31.480 --> 00:12:37.640
turn off import sorting altogether. So one of the things of usort is like first do no


00:12:37.640 --> 00:12:46.640
harm. So it's, it's trying its best to make sure that these common use cases are just


00:12:46.640 --> 00:12:54.140
treated normally and correctly from the start. So in, in most cases, it's a much safer version


00:12:54.140 --> 00:12:58.680
of isort. It's not complete. It's not a 100% replacement, but it's the thing we've been


00:12:58.680 --> 00:13:04.000
using internally. And it's one of the cases where I'm, I'm proud of the way that we are


00:13:04.000 --> 00:13:07.400
helping to build better tools for the ecosystem.


00:13:07.400 --> 00:13:11.280
Yeah, this is really neat. I never really thought about that problem. One thing that


00:13:11.280 --> 00:13:17.560
does drive me crazy is sometimes I'll, I'll need to change the Python path so that future


00:13:17.560 --> 00:13:21.440
imports regardless of your working directory behave the same if you don't have a package


00:13:21.440 --> 00:13:23.440
or something like that. Right. Something simple.


00:13:23.440 --> 00:13:26.880
Yeah. That's super common in the AI and ML type of workflows.


00:13:26.880 --> 00:13:31.640
Yeah. Yeah. And I get all these warnings like you should not have code that is before an


00:13:31.640 --> 00:13:35.400
import like, well, but this one is about making the import work. If I don't put this, it's


00:13:35.400 --> 00:13:39.200
going to crash for some people if they run it weirdly and stuff like that. Right. So


00:13:39.200 --> 00:13:45.960
yeah, that's so interesting. Yeah. Very, very cool project. Nice. All right. So yeah, let's


00:13:45.960 --> 00:13:53.200
dive into async. Huh? Sure. Yeah. So maybe a little bit of history, you know, Python


00:13:53.200 --> 00:13:57.320
is hard to talk about asynchronous programming in Python without touching on the GIL global


00:13:57.320 --> 00:14:02.200
interpreter lock normally spoken as a bad word, but it's not necessarily bad. It has


00:14:02.200 --> 00:14:08.360
a purpose. It just, its purpose is somewhat counter to making asynchronous code run really


00:14:08.360 --> 00:14:13.880
quick and in parallel. I mean, it's, it's one of those things where if, if you imagined


00:14:13.880 --> 00:14:19.320
what Python would be without the global interpreter lock you end up having to do a lot more work


00:14:19.320 --> 00:14:25.000
to make sure that let's say if you had multi-threading multi-threaded stuff going on, you'd have


00:14:25.000 --> 00:14:28.720
to do a lot more work to make sure that they're not clobbering some shared data. Like you


00:14:28.720 --> 00:14:32.400
look at the way that you have to have, have to have synchronizations and everything else


00:14:32.400 --> 00:14:38.200
in Java or C++. Yeah. We don't generally need that in Python because the GIL prevents


00:14:38.200 --> 00:14:44.280
a lot of that bad behavior. And you know, the current efforts to kind of remove


00:14:44.280 --> 00:14:50.480
the GIL that have been ongoing for the past eight to 10 years in every single case, once


00:14:50.480 --> 00:14:54.160
you remove that GIL and add a whole bunch of other locks, the whole system's actually


00:14:54.160 --> 00:14:59.280
slower. So this is one of those things where it's like, it does cause problems, but it


00:14:59.280 --> 00:15:03.120
also enables Python to be a lot faster than it would be otherwise.


00:15:03.120 --> 00:15:07.920
And probably simpler. Yeah. Yeah. So the global interpreter lock, when I first


00:15:07.920 --> 00:15:11.280
heard about it, I thought of it as a threading thing and it sort of is, but you know, it's


00:15:11.280 --> 00:15:17.560
primarily says let's create a system so that we don't have to do locks as we increment


00:15:17.560 --> 00:15:21.440
and decrement the ref count on variables. So basically all the memory management can


00:15:21.440 --> 00:15:27.320
happen without the overhead of taking a lock, releasing a lock, all that kind of weirdness.


00:15:27.320 --> 00:15:32.800
Yeah. Excuse me. Yeah. So yeah, no. So we've got like a bunch of early attempts. I mean,


00:15:32.800 --> 00:15:37.920
we've got threading and multiprocessing have been around for a while. There's Jeevan, Tornado,


00:15:37.920 --> 00:15:45.120
but then around, I guess, was it Python three, four, we got asyncio, which is a little bit


00:15:45.120 --> 00:15:49.520
of a different flavor than, you know, like the computational threading or the computational


00:15:49.520 --> 00:15:55.520
multiprocessing side of async. Yeah. It's actually an interesting kind of throwback


00:15:55.520 --> 00:16:01.800
to the way that computing happened in like the eighties and early nineties where like


00:16:01.800 --> 00:16:08.920
windows 3.1 or classic macOS, essentially you can, you know, run your program or your


00:16:08.920 --> 00:16:16.200
process and you actually have to cooperatively give up control of the CPU in order for another


00:16:16.200 --> 00:16:23.960
program to work. So there'd be a lot of cases where like you're, if, if you had a bad behaving


00:16:23.960 --> 00:16:29.040
program, you'd end up not being able to do multitasking in, you know, these old operating


00:16:29.040 --> 00:16:34.000
systems because it was all cooperative in the case of asyncio. It's essentially taking


00:16:34.000 --> 00:16:41.040
that mechanism where you don't need to do a lot of context switching in threads or in


00:16:41.040 --> 00:16:47.040
processes. And you're essentially letting a bunch of functions cooperatively coexist


00:16:47.040 --> 00:16:53.160
and essentially say like when, when your function gets to a point where it's doing a network


00:16:53.160 --> 00:16:58.640
request and it's waiting on that network request, your function then, you know, will nicely


00:16:58.640 --> 00:17:03.320
hand over control back to the asyncio framework. At which point the framework and event loop


00:17:03.320 --> 00:17:09.280
can go find the next task to work on. That's, you know, that's not blocked on something.


00:17:09.280 --> 00:17:14.360
Right. Yeah. And it's very often doesn't involve threads at all or, you know, the one main


00:17:14.360 --> 00:17:20.240
thread, right? Like, so it's not a way to create threading. It's a way to allow stuff


00:17:20.240 --> 00:17:24.480
to happen while you're otherwise waiting. Yeah. In the best case, you only ever have


00:17:24.480 --> 00:17:30.080
the one thread. And now in reality it doesn't work like that because a lot of our, you know,


00:17:30.080 --> 00:17:34.680
modern computing infrastructure is not built in an async way. So like if you look at file


00:17:34.680 --> 00:17:41.440
access, there's basically no real way to do that asynchronously without threads. But in


00:17:41.440 --> 00:17:46.400
the best case, like network requests and so forth, if you have the appropriate hooks from


00:17:46.400 --> 00:17:49.960
the operating system, then that can all be completely in one thread. And that means you


00:17:49.960 --> 00:17:56.400
have a lot less overhead from the actual runtime and process from the operating system because


00:17:56.400 --> 00:18:01.800
you're not having to constantly throw a whole bunch of memory onto a stack and then pull


00:18:01.800 --> 00:18:06.600
off memory from another stack and try to figure out where you were, you know, when something


00:18:06.600 --> 00:18:10.480
interrupted you in the middle of, you know, 50 different operations.


00:18:10.480 --> 00:18:15.880
Right. If it's started swapping out the memory it's touching that might, you know, swap out


00:18:15.880 --> 00:18:21.000
what's in the L1, L2, L3 caches, which can have huge performance impacts. And it's just


00:18:21.000 --> 00:18:25.280
constantly cycling back and forth out of control a lot of times, right?


00:18:25.280 --> 00:18:30.640
Yeah. So in a lot of our testing internally, when I was working on things that would talk


00:18:30.640 --> 00:18:35.440
to lots and lots of servers, it's like we would hit a point where somewhere between


00:18:35.440 --> 00:18:43.120
64 and 128 threads would actually start to see less performance overall because it just


00:18:43.120 --> 00:18:50.080
spends all of its time trying to context switch between all of these threads because you're


00:18:50.080 --> 00:18:55.560
interrupting these threads, you know, at an arbitrary point in time because the runtime


00:18:55.560 --> 00:19:00.080
is trying to make sure that all the threads are serviced equally. But in reality, like


00:19:00.080 --> 00:19:08.320
half of these threads don't need to be given the context right now. So by, you know, doing


00:19:08.320 --> 00:19:12.560
those sort of interrupts in context, which is when the runtime wants to rather than when


00:19:12.560 --> 00:19:17.720
the functions or requests are wanting to, you end up with a lot of suboptimal behavior.


00:19:17.720 --> 00:19:25.760
Yeah, interesting. Yeah. And also things like locks, mutexes and stuff don't work in this


00:19:25.760 --> 00:19:30.560
world because it's about, you know, what thread has access. Well, it's all the codes on one


00:19:30.560 --> 00:19:36.640
thread. So to me, the real Zen of async I/O, at least for many really solid use cases,


00:19:36.640 --> 00:19:41.120
kind of like we touched on, is it's all about scaling when you're waiting. Like if I'm waiting


00:19:41.120 --> 00:19:46.800
on something else, it's like completely free to just go do it. If I'm calling microservices,


00:19:46.800 --> 00:19:54.160
external APIs, if I'm downloading something or uploading a file or talking to a database


00:19:54.160 --> 00:20:01.200
or even maybe accessing a file with something like AIO files. Yeah. Yeah. Yeah. There's


00:20:01.200 --> 00:20:10.120
a cool place called AI Awesome Async I/O by TMLFreer, which is pretty cool. Have you seen


00:20:10.120 --> 00:20:17.600
this place? I have looked at it in the past. I end up spending so much time looking at


00:20:17.600 --> 00:20:22.600
and building things. It's like I haven't actually gotten a lot of opportunity to use a bunch


00:20:22.600 --> 00:20:29.520
of these. Most of my time, I'm actually not working that high enough on the stack to make


00:20:29.520 --> 00:20:33.840
use of them. Right. Right. Right. These are more a lot of more frameworks. You do have


00:20:33.840 --> 00:20:37.800
some other neat things in there as well, like async SSH. I hadn't heard of that one. But


00:20:37.800 --> 00:20:43.200
anyway, I'll put that in the show notes. That's got, I don't know, 50, 60 libraries and packages


00:20:43.200 --> 00:20:48.520
for solving different problems with async I/O, which is pretty cool. Yeah. Whenever


00:20:48.520 --> 00:20:51.880
I talk about async I/O, one of the things I love to give a shout out to is this thing


00:20:51.880 --> 00:20:56.680
called unsync. Have you heard of unsync? I had not heard about it until I looked at


00:20:56.680 --> 00:21:03.820
the show notes. But it sounds a lot like some of the some of the things that I've seen people


00:21:03.820 --> 00:21:10.160
implement a lot of different cases. It's a very filling a common sort of use case where


00:21:10.160 --> 00:21:15.880
you have, like I was saying earlier, where people want to mix async I/O into an existing


00:21:15.880 --> 00:21:21.560
synchronous application. You do have to be very careful about how you do that, especially


00:21:21.560 --> 00:21:28.520
vice versa or or a lot of the stumbling blocks we've seen tend to be cases where you


00:21:28.520 --> 00:21:33.360
have synchronous code that calls some async code that then wants to call some synchronous


00:21:33.360 --> 00:21:38.120
code but on like another thread so that it's not blocked by it. And you actually end up


00:21:38.120 --> 00:21:44.280
getting this like in out in out sort of thing where you have like nested layers of async


00:21:44.280 --> 00:21:48.760
I/O. I'm not sure how much this may or may not solve that.


00:21:48.760 --> 00:21:53.960
I think this actually helps some with that as well. Basically, the idea is there's two


00:21:53.960 --> 00:21:57.440
main things that it solves that I think it's really neat. One, it's like a unifying layer


00:21:57.440 --> 00:22:03.720
across multi processing, multi threading and straight async I/O. Right. So you put a decorator


00:22:03.720 --> 00:22:09.000
onto a function. If the function is an async function, it runs it on async I/O. If it's


00:22:09.000 --> 00:22:13.200
a regular function, it runs it on a thread. And if you say it's a regular function, but


00:22:13.200 --> 00:22:16.880
it's computational, it'll run it on and multi processing. But it gives you basically an


00:22:16.880 --> 00:22:22.320
async I/O async and await API for it. It figures out how to run the loop. Anyway, it's pretty


00:22:22.320 --> 00:22:26.200
cool. Not what we're here to talk about, but it's definitely worth checking out while we're


00:22:26.200 --> 00:22:27.200
on the subject.


00:22:27.200 --> 00:22:33.280
So ultimately, it gives you just a future that you can then either await or ask for


00:22:33.280 --> 00:22:38.840
the result from. Right. Yeah, exactly. Exactly. And the result you instead of saying, you've


00:22:38.840 --> 00:22:42.040
got to wait till it's finished before you get the result, you just go give me the result.


00:22:42.040 --> 00:22:47.200
And if it needs to, it'll just block. So it's a nice way to sort of cap the async I/O. You


00:22:47.200 --> 00:22:51.680
know, like one of the challenges of async I/O is, well, five levels down the call stack,


00:22:51.680 --> 00:22:55.080
this thing wants to be async. So the next thing's async. So the next thing I'll say


00:22:55.080 --> 00:22:59.480
sync and like all of a sudden everything's async, right. And so it was something like


00:22:59.480 --> 00:23:02.480
this. I mean, you could do it yourself as well. You can like just go create an event


00:23:02.480 --> 00:23:07.760
loop, run it. And at this level, we're not going to be async above it, but we're coordinating


00:23:07.760 --> 00:23:12.520
stuff below using async I/O. And here's where it stops.


00:23:12.520 --> 00:23:19.800
Yeah, it sounds like a nicer version of what I see dozens of when you have lots and lots


00:23:19.800 --> 00:23:24.200
of engineers that, you know, aren't actually working on the same code base together, but


00:23:24.200 --> 00:23:28.160
they're all in the same repository. And we end up seeing these cases where everybody


00:23:28.160 --> 00:23:32.560
has solved the same use case. I do think this would be useful. And I'm actually planning


00:23:32.560 --> 00:23:34.240
on sharing it with more people.


00:23:34.240 --> 00:23:39.720
Yeah, yeah. Check it out. It's like a subtotal, I think, 126 lines of Python in one file,


00:23:39.720 --> 00:23:48.440
but it's really cool, this unifying API. All right. So I guess that probably brings us


00:23:48.440 --> 00:23:53.120
to OmniLib. You want to talk about that for a little bit? So this is what I thought would


00:23:53.120 --> 00:23:56.960
be fun to have you on the show to really focus on is like async I/O, but then also you've


00:23:56.960 --> 00:24:05.600
created this thing called OmniLib, the OmniLib project that has, solves, what's it, four


00:24:05.600 --> 00:24:09.840
problems, four different problems with async I/O. And obviously you can combine them together,


00:24:09.840 --> 00:24:12.840
I would expect.


00:24:12.840 --> 00:24:20.480
Yeah. The origins of this really is like I had built the, like AIO SQLite was the first


00:24:20.480 --> 00:24:27.160
thing that I wrote that was an async framework. And then I'd built a couple more. And at one


00:24:27.160 --> 00:24:30.760
point I realized these projects are actually getting really popular and people are using


00:24:30.760 --> 00:24:37.840
it, but they're just like one of the hundred things that are on my GitHub profile and graveyard.


00:24:37.840 --> 00:24:42.440
So I really felt like they needed to have their own separate place for like, these are


00:24:42.440 --> 00:24:49.320
the projects that I'm actually proud of. And I thought that was actually a good opportunity


00:24:49.320 --> 00:25:00.080
to be able to make a dedicated like project or organization for it. And essentially say


00:25:00.080 --> 00:25:06.280
that everything under this I'm guaranteeing is going to be developed under a very inclusive


00:25:06.280 --> 00:25:13.000
code of conduct that I personally believe in and want to try and also at the same time


00:25:13.000 --> 00:25:19.280
make it more welcoming and supportive of other contributors, especially newcomers or other


00:25:19.280 --> 00:25:26.200
otherwise marginalized developers in the ecosystem and try to be as friendly as possible with


00:25:26.200 --> 00:25:30.440
it. And it's like, this is something that I tried to do beforehand and just never really


00:25:30.440 --> 00:25:36.280
formalized it on any of my projects other than like, here's a code of conduct file in


00:25:36.280 --> 00:25:42.800
the repository. But this is like really one of the first times where I wanted to put all


00:25:42.800 --> 00:25:47.760
these together and make sure that these are really like, this is going to be whether or


00:25:47.760 --> 00:25:52.920
not enough people make it a community. I want it to be welcoming from the outset.


00:25:52.920 --> 00:25:57.680
Right. That's really cool. And you created your own special GitHub organization that


00:25:57.680 --> 00:26:01.920
you put in all under stuff like that. So it's kind of the things that are graduated from


00:26:01.920 --> 00:26:03.960
your personal projects. Is that the story?


00:26:03.960 --> 00:26:09.880
Yeah. And kind of the threshold I tried to follow is like, if this is worth making a


00:26:09.880 --> 00:26:14.840
Sphinx documentation site for then it's worth putting on, you know, OmniLab projects. So


00:26:14.840 --> 00:26:22.280
they're not all asyncio. That just happens to be where a lot of my interests and utility


00:26:22.280 --> 00:26:27.560
stands at. So that's, you know, that's what most of them are, or at least the most popular


00:26:27.560 --> 00:26:33.480
ones. But there are other projects that I have also in the back burner that will probably


00:26:33.480 --> 00:26:41.360
end up there that maybe not as useful libraries or whatever, but either way, like I said earlier,


00:26:41.360 --> 00:26:42.920
these are the ones that I'm at least proud of.


00:26:42.920 --> 00:26:48.400
Nice. That's cool. So you talked about the being there to support people who are getting


00:26:48.400 --> 00:26:53.640
into open source and whatnot and having that code of conduct. What other than that, is


00:26:53.640 --> 00:26:58.640
there like a mission behind this? Like I want to make this category of tools or solve these


00:26:58.640 --> 00:27:04.240
types of problems or is it just these are the things that you graduated?


00:27:04.240 --> 00:27:10.480
It's something I've tried to think about. I'm not 100% certain. I would like it to have


00:27:10.480 --> 00:27:17.000
maybe more of a mission, but at the same time, it's like, especially from things I've had


00:27:17.000 --> 00:27:20.860
to deal with, I work, it's like, I don't want this to be a dumping ground of stuff either.


00:27:20.860 --> 00:27:25.200
Like I want this specifically, like in the opening statement, I want it to be a group


00:27:25.200 --> 00:27:31.240
of high quality projects that are following this code of conduct. So from that perspective,


00:27:31.240 --> 00:27:37.920
it's like at the moment, it's like my personal interests are always in building things where


00:27:37.920 --> 00:27:44.220
I find gaps in availability from other libraries. So that's probably the closest to a mission


00:27:44.220 --> 00:27:51.960
of what belongs here is just things that haven't been made yet. But either way, I just want


00:27:51.960 --> 00:27:57.880
to have that dedication to the statement of like, I want these to be high quality. I want


00:27:57.880 --> 00:28:05.600
them to be tested. I want them to have continuous integration and testing and well documented


00:28:05.600 --> 00:28:12.280
and so forth. Yeah, super cool. All right. So there's four main projects here on the


00:28:12.280 --> 00:28:17.760
homepage. You do have the attribution one, but that's, that's just kind of a bookkeeping


00:28:17.760 --> 00:28:24.520
helper tool. Exactly. So you've got, well, let's, let's talk about the things that maybe


00:28:24.520 --> 00:28:32.040
they're the AIO extension of. So we, in Python, we have iter tools, right? Which is like tools


00:28:32.040 --> 00:28:39.840
for easily creating generators and such out of collections and whatnot. So you have AIO


00:28:39.840 --> 00:28:43.680
iter tools, which is awesome. And then we have multiprocessing, which is a way around


00:28:43.680 --> 00:28:47.320
the gills. Like here's a function and some data, go run that in a sub process and then


00:28:47.320 --> 00:28:51.560
give me the answer. And because it's a sub process, it has its own sub GIL or its own


00:28:51.560 --> 00:28:57.760
separate GIL. So it's all good. So you have AIO multiprocess, which is cool. And then,


00:28:57.760 --> 00:29:02.560
you know, one of the most widely used databases is SQLite already built into Python, which


00:29:02.560 --> 00:29:08.360
is super cool. And so you have AIO SQLite and then sort of extending that, that's like


00:29:08.360 --> 00:29:14.360
a raw SQLite, you know, raw SQL library, that's asyncio. Then you have AQL, which is more


00:29:14.360 --> 00:29:22.320
ORM like, I'm not sure it's a hundred percent ORM, you could categorize it for us, but yeah.


00:29:22.320 --> 00:29:31.080
I've definitely used like in quotes, in scare quotes, ORM Lite, because I want it to be


00:29:31.080 --> 00:29:39.440
able to essentially be a combination of like well-typed table definitions that you can


00:29:39.440 --> 00:29:48.160
then use to generate queries against the database. As of right now, it's more like writing a,


00:29:48.160 --> 00:29:54.720
like a DSL that lets you write a backend agnostic SQL statement.


00:29:54.720 --> 00:30:01.000
Right. Okay. Yeah, DSL domain specific language for people who aren't entirely sure.


00:30:01.000 --> 00:30:05.920
So really it's essentially just stringing together a whole bunch of method calls on


00:30:05.920 --> 00:30:13.720
a table object in order to get a SQL query out of it. The end goal is to be able to have


00:30:13.720 --> 00:30:19.600
that actually be a full end to end thing where you've defined your tables and you get objects


00:30:19.600 --> 00:30:24.360
back from it. And then you can like call something on the objects to get them to update themselves


00:30:24.360 --> 00:30:33.180
back into a database. But I've been very hesitant to pick an API on it for how to actually get


00:30:33.180 --> 00:30:40.320
all that done because the, trying to do that in an async fashion is actually really difficult


00:30:40.320 --> 00:30:48.520
to do it right. And separately, like trying to do asyncio and have everything well-typed


00:30:48.520 --> 00:30:53.640
is, you know, it's like two competing problems that have to be solved.


00:30:53.640 --> 00:31:02.360
Yeah. I just recently started playing with SQL alchemies 2.0, 1.4 beta API where they're


00:31:02.360 --> 00:31:08.680
doing the async stuff and it's quite different than the traditional SQL alchemies. So yeah,


00:31:08.680 --> 00:31:11.960
you can see the challenges there.


00:31:11.960 --> 00:31:18.240
And it's also a case where it's like having something to generate the queries to me is


00:31:18.240 --> 00:31:22.640
more important than having the thing that will actually go run the query. Especially


00:31:22.640 --> 00:31:28.480
for a lot of internal use cases. We really just want something that will generate the


00:31:28.480 --> 00:31:33.360
query or we already have a system that will talk to the database once you give it a query


00:31:33.360 --> 00:31:39.840
and parameters. It's the piece of actually saying, defining what your table hierarchy


00:31:39.840 --> 00:31:46.740
or structure is, and then being able to run stuff to get the actual SQL query out of it,


00:31:46.740 --> 00:31:53.880
but have that work for both SQLite and MySQL or Postgres or whatever other backend you're


00:31:53.880 --> 00:32:01.400
using. Having it be able to use the same code and generate the correct query based off of


00:32:01.400 --> 00:32:04.360
which database you're talking to is the important part.


00:32:04.360 --> 00:32:08.160
Yeah, cool. Well, there's probably a right order to dive into these, but since we're


00:32:08.160 --> 00:32:14.640
already talking about the AQL one a lot, maybe give us an example of what you can do with


00:32:14.640 --> 00:32:19.560
it. Maybe talk us through, it's hard to talk about code on air, but just give us a sense


00:32:19.560 --> 00:32:23.600
of what kind of code you write and what kind of things it does for us.


00:32:23.600 --> 00:32:31.280
So this is heavily built around the idea of using data classes. In this case, it's specifically


00:32:31.280 --> 00:32:36.120
uses adders simply because that's what I was more familiar with at the time that I started


00:32:36.120 --> 00:32:42.720
building this. But essentially you create a class with essentially all of your columns


00:32:42.720 --> 00:32:46.800
specified on that class with the name and the type.


00:32:46.800 --> 00:32:49.560
Not like SQL, but not like super heavy.


00:32:49.560 --> 00:32:50.560
Or Django style.


00:32:50.560 --> 00:32:51.560
Yeah, exactly.


00:32:51.560 --> 00:32:58.200
Like native types, like id colon int, name colon str, not sa.column, column.sa.string


00:32:58.200 --> 00:32:59.200
and so on, right?


00:32:59.200 --> 00:33:05.800
Exactly. Like I want this to look as close to a normal data class definition as possible.


00:33:05.800 --> 00:33:11.120
And essentially be able to decorate that and you get a special object back that when you


00:33:11.120 --> 00:33:17.080
use methods on it, like in this case, the example is you're creating a contact. So you


00:33:17.080 --> 00:33:24.280
list the integer ID, the name of it and the email. And whether those are like, whatever


00:33:24.280 --> 00:33:29.480
the primary key doesn't really matter in this case, whether the ID ends up getting auto


00:33:29.480 --> 00:33:33.600
incremented again, doesn't really matter. What we're really worried about is generating


00:33:33.600 --> 00:33:34.920
the actual queries.


00:33:34.920 --> 00:33:39.760
And you're assuming like somebody's created the table, it's already got a primary key


00:33:39.760 --> 00:33:43.760
for ID, it's auto incrementing or something like that. And you just want to talk to the


00:33:43.760 --> 00:33:44.760
thing.


00:33:44.760 --> 00:33:52.040
Yeah. And so essentially, you take this contact class that you've created, and you can call


00:33:52.040 --> 00:34:00.920
a select method on it that will then, you know, you can add an aware method to decide


00:34:00.920 --> 00:34:05.720
which contacts you want to select. There's other methods for, you know, changing the


00:34:05.720 --> 00:34:11.680
order or limits. Or furthermore, if you wanted to do joins or other sorts of things, like


00:34:11.680 --> 00:34:19.840
it kind of expects that you know what general SQL syntax looks like. Because you string


00:34:19.840 --> 00:34:25.800
together a bunch of stuff kind of in the same order that you would with a with a SQL query.


00:34:25.800 --> 00:34:30.360
But the difference is that, in this case, like when you're doing the where clause, rather


00:34:30.360 --> 00:34:36.360
than having to do an arbitrary string that says, you know, column name, like, and then


00:34:36.360 --> 00:34:44.060
some string literal. In this case, you're saying like, where contact dot email dot like,


00:34:44.060 --> 00:34:49.940
and then passing the thing that you want to check against. And the other alternative is


00:34:49.940 --> 00:34:54.220
you could if you wanted to look for a specific one, you could say like, where contact dot


00:34:54.220 --> 00:35:00.540
email equals equals, and then the value you're looking for. And so you're you're kind of


00:35:00.540 --> 00:35:09.340
using or abusing Python's expression syntaxes to essentially build up your query, definitely


00:35:09.340 --> 00:35:17.740
using domain specific language in this case. And but essentially, having the fluent API,


00:35:17.740 --> 00:35:23.260
you once you string all this together, you have this query object, which you can then,


00:35:23.260 --> 00:35:29.460
you know, pass to the appropriate engine to get an actual finalized SQL query and the


00:35:29.460 --> 00:35:37.580
parameters that would get passed if you were doing a prepared query. But you could also


00:35:37.580 --> 00:35:43.220
potentially like in the future, the goal was you would also be able to make manage your


00:35:43.220 --> 00:35:51.900
connection with aq L, and basically be able to tell it to run this query on that connection.


00:35:51.900 --> 00:35:57.460
And regardless, you'd be able to do this the same with SQL lite or MySQL or whatever. And


00:35:57.460 --> 00:36:04.220
the library is the part that handles deciding, you know, what specific part of you know,


00:36:04.220 --> 00:36:10.540
the incompatible SQL languages that they all use will will actually be available.


00:36:10.540 --> 00:36:17.540
Yeah, like, for example, MySQL uses question mark for the parameters. Yeah, SQL Server


00:36:17.540 --> 00:36:22.580
uses, I think, at parameter name, there's like, they're all have their own little style.


00:36:22.580 --> 00:36:29.380
It's not the same, right? Yeah. And it's some of that is kind of moot because of the fact


00:36:29.380 --> 00:36:38.660
that the most of the engine libraries that we use commonly in Python, like AI, MySQL,


00:36:38.660 --> 00:36:47.460
or, or SQL lite or whatever, they're already kind of unified around the there's there's


00:36:47.460 --> 00:36:53.900
a specific PEP that defines what the database interface is going to look like a DB API to


00:36:53.900 --> 00:36:59.700
or whatever it is that so some of that work has already been done by the peps and by the


00:36:59.700 --> 00:37:07.340
actual database engines. But there's a lot of cases where it's a little bit more subtle.


00:37:07.340 --> 00:37:18.780
Like the semantics, especially around using a like expression. MySQL does a case insensitive


00:37:18.780 --> 00:37:27.740
matching by default, but SQL lite doesn't. So AQL tries to kind of like unify those where


00:37:27.740 --> 00:37:32.240
possible. But also there's there's cases, especially when you're getting into joins


00:37:32.240 --> 00:37:38.340
or group buys, things like that, where the actual specific syntax being used will


00:37:38.340 --> 00:37:44.500
start to vary between the different backends. And that's where we, we've had more issues,


00:37:44.500 --> 00:37:49.020
like especially the whole point of SQL lite for a lot of people is as a drop in replacement


00:37:49.020 --> 00:37:54.140
to MySQL when you're running your unit tests. And so you want your code to be able to do


00:37:54.140 --> 00:37:58.900
the same thing regardless of what database engine it's connected to. And this is one


00:37:58.900 --> 00:37:59.900
way to do that.


00:37:59.900 --> 00:38:04.500
Okay, that's cool. Yeah, with SQL lite, you can say, the database lives in your colon


00:38:04.500 --> 00:38:09.300
memory. Yeah, exactly. And then you can just tear it up for your unit tests, and then it


00:38:09.300 --> 00:38:17.140
just goes away. It's nice. Yeah. So maybe that brings us to the next one, the AIO SQL


00:38:17.140 --> 00:38:22.020
lite. Sure. This one, this one is all about asyncio, you can see from the example here,


00:38:22.020 --> 00:38:24.060
you want to tell us about that?


00:38:24.060 --> 00:38:30.100
Yeah. So this was, again, born out of, you know, a need for using SQL lite, especially


00:38:30.100 --> 00:38:37.580
in in testing frameworks, and so forth to replace MySQL. And essentially, what I was


00:38:37.580 --> 00:38:48.700
doing was taking the normal SQL lite API from Python and essentially saying, like,


00:38:48.700 --> 00:38:55.620
how would this look in an asyncio world? Like if we were re implementing SQL lite from


00:38:55.620 --> 00:39:02.540
the ground up in an asyncio world, how can we do that? And essentially, so in this case,


00:39:02.540 --> 00:39:13.100
we're heavily using async context managers, and awaitables in order to actually run the


00:39:13.100 --> 00:39:20.460
database connection to SQL lite on a separate thread, and provide as much of an async interface


00:39:20.460 --> 00:39:27.560
to that as possible. So when you connect to a SQL lite, it spawns a background thread


00:39:27.560 --> 00:39:34.260
that actually uses the standard SQL lite library to connect to your database. And then it has


00:39:34.260 --> 00:39:41.700
methods on that thread object that allow you to actually make calls into that database.


00:39:41.700 --> 00:39:48.860
And those are essentially proxied through futures. So if you want to execute a query,


00:39:48.860 --> 00:39:58.960
when you await that query execution, it will basically cue the function call on the other


00:39:58.960 --> 00:40:06.600
thread and basically tell it here's the future to set when the result is ready. So once the


00:40:06.600 --> 00:40:12.200
SQL lite execution or cursor or whatever has actually completed doing what it's supposed


00:40:12.200 --> 00:40:17.840
to do on that background thread, it then goes back to the original threads event loop and


00:40:17.840 --> 00:40:24.200
says, you know, set this future to two finished. And so that allows the thing that's originally


00:40:24.200 --> 00:40:28.600
awaiting it to actually come back and do something with the result.


00:40:28.600 --> 00:40:34.080
Yeah, it sounds a little tricky, but also super helpful. And people might be thinking,


00:40:34.080 --> 00:40:37.920
didn't we just talk about the GIL and how threading doesn't really add much, but when


00:40:37.920 --> 00:40:42.400
you're talking over the network, or you're talking to other things, a lot of times, the


00:40:42.400 --> 00:40:45.920
gill can be released while you're waiting on the internal SQL lite or something like


00:40:45.920 --> 00:40:49.920
that, right? Yeah, so the internal SQL lite library


00:40:49.920 --> 00:40:56.320
on its own will release the GIL when when it's calling into the underlying SQL lite


00:40:56.320 --> 00:41:00.200
C library. And that's where it's waiting. So that's good.


00:41:00.200 --> 00:41:06.000
Yeah. And I mean, the other the other side of this is that it's one thread. I'm not really


00:41:06.000 --> 00:41:11.840
aware of anybody who's opening, you know, hundreds of simultaneous connections to a


00:41:11.840 --> 00:41:19.200
SQL, SQL lite database, the way that people expect to do with, say, like, a O HTTP, or


00:41:19.200 --> 00:41:25.160
things like that. So while it is, you know, potentially less efficient, if you wanted


00:41:25.160 --> 00:41:32.280
to do a whole bunch of parallel SQL lite connections, the problem really is that SQL


00:41:32.280 --> 00:41:38.360
lite itself is not thread safe. So it has to have a dedicated thread for each connection.


00:41:38.360 --> 00:41:46.920
Otherwise, you risk corruption of the backing database, which sounds not good. Right? Yeah,


00:41:46.920 --> 00:41:52.320
it's like, basically, you end up either where, where two threads clobber each other, or more


00:41:52.320 --> 00:41:58.840
specifically, what SQL lite says is, if you absolutely try to talk to a connection from


00:41:58.840 --> 00:42:03.480
a different thread, the Python module will complain unless you've specifically told it


00:42:03.480 --> 00:42:07.680
No, please don't complain. I know it's unsafe, at which point SQL lite will be really upset


00:42:07.680 --> 00:42:13.880
if you try to do a right, or modification to that database. So there are layers of protections


00:42:13.880 --> 00:42:18.280
against that. But it is one of the underlying limitations that we have to deal with us in


00:42:18.280 --> 00:42:23.720
this case. So if you wanted to have simultaneous connections to the same database, you'd you


00:42:23.720 --> 00:42:28.360
really have to spin up multiple threads in order to make that happen safely.


00:42:28.360 --> 00:42:32.960
You can always do some kind of thread pool type thing, like, we're only going to allow


00:42:32.960 --> 00:42:38.280
eight connections at a time. And you're just going to block until one of those, yeah, it's


00:42:38.280 --> 00:42:40.120
finished or whatever, right?


00:42:40.120 --> 00:42:46.240
It's, it's definitely a tricky thing. So like, the expected use case with AIO SQL lite


00:42:46.240 --> 00:42:52.520
is that you'll share the database connection between multiple workers. So you'll like,


00:42:52.520 --> 00:42:58.120
in in the piece of your application that starts up, it would make the connection to the database,


00:42:58.120 --> 00:43:04.040
and store that somewhere, and then essentially pass that around. And so AIO SQL lite is basically


00:43:04.040 --> 00:43:10.280
expecting to use a queue system to say, whoever gets the query first, is the one that you


00:43:10.280 --> 00:43:14.800
know, gets to run it first. And whoever, whoever asked for the query second, you know, is the


00:43:14.800 --> 00:43:18.600
second one to get it. So you're still doing it all on one thread. And it's slightly less


00:43:18.600 --> 00:43:23.400
performant that way, but it's at least safe, right? And still asynchronous, at least.


00:43:23.400 --> 00:43:28.680
Yeah, that's good. Very nice. And one of the things that looking at your example here,


00:43:28.680 --> 00:43:34.640
which I'll link in the show notes, of course, is Python has a lot of interesting constructs


00:43:34.640 --> 00:43:39.440
around async and await. You know, a lot of languages, you know, you think C# or


00:43:39.440 --> 00:43:44.680
JavaScript or whatever, it's kind of async function, await function calls are good. But


00:43:44.680 --> 00:43:50.960
you know, we've got async with async for a lot of interesting extensions to working with


00:43:50.960 --> 00:43:52.480
async and other constructs.


00:43:52.480 --> 00:43:57.800
Yeah, it actually makes it really nice in some ways. And essentially, these are just


00:43:57.800 --> 00:44:03.400
syntactic wrappers around a whole bunch of magic methods on objects, right?


00:44:03.400 --> 00:44:09.280
Await thing, enter, do your thing, right? Then await exit, right?


00:44:09.280 --> 00:44:15.080
And so like, the nice part is that for some amount of extra work in the library, setting


00:44:15.080 --> 00:44:20.080
up all those magic methods everywhere and deciding, you know, the right way to use them.


00:44:20.080 --> 00:44:25.520
The benefit at the end is that you have this very simple syntax for asynchronously iterating


00:44:25.520 --> 00:44:33.680
over the results of a cursor. And in that case, you don't have to care that after, you


00:44:33.680 --> 00:44:39.720
know, 64 elements of iteration, you've exhausted the local cache and now SQLite has to go back


00:44:39.720 --> 00:44:45.280
and fetch the next batch of 64 items. In that case, it's like, that's transparent to your


00:44:45.280 --> 00:44:51.600
application. And that's where the coroutine that's iterating over that cursor would then


00:44:51.600 --> 00:45:01.440
hand back its control of the event loop. And the next coroutine in waiting essentially


00:45:01.440 --> 00:45:04.320
is able to then, you know, wake up and go do its own thing too.


00:45:04.320 --> 00:45:09.320
Right? Yeah. Oh, how cool. I didn't even really think of it that way. That's neat. All right.


00:45:09.320 --> 00:45:12.560
Maybe next one to touch on would be AIO multiprocess.


00:45:12.560 --> 00:45:18.080
Sure. Oh, wow. I see it just now crossed 1000 stars today. Or recently.


00:45:18.080 --> 00:45:22.040
Oh, yeah, yeah, it did. Yeah, very recently. That's awesome.


00:45:22.040 --> 00:45:30.920
That's my real pride and joy here is getting all those stars. So sorry. Anyways. So there's


00:45:30.920 --> 00:45:37.100
like, there's this interesting dichotomy setup between threading and multiprocessing in Python.


00:45:37.100 --> 00:45:43.340
So with multi threading, you're able to, you know, interleave execution. So with the gill,


00:45:43.340 --> 00:45:48.780
it means that only one thread can actually be modifying Python objects or running Python


00:45:48.780 --> 00:45:57.460
code at any given time. So you're essentially limited to one core of your CPU.


00:45:57.460 --> 00:46:03.300
That's a big limitation, right? Right, right, exactly. Like, I see servers on a regular


00:46:03.300 --> 00:46:10.620
basis that are like 64 to 100 cores. So only using one of them is basically a non starter.


00:46:10.620 --> 00:46:17.660
You get a lot of people with pitchforks saying, why aren't we using rust? And so essentially,


00:46:17.660 --> 00:46:22.980
what the alternative of this multiprocessing, where you're spinning up an individual process,


00:46:22.980 --> 00:46:30.460
each has its own GIL. This does allow you for CPU intensive things to basically use


00:46:30.460 --> 00:46:35.020
all of the available cores on your system. So if you're crunching a whole bunch of numbers


00:46:35.020 --> 00:46:40.260
with NumPy or something like that, you could use multiprocessing and saturate all of your


00:46:40.260 --> 00:46:48.940
cores with no problem. In this case, essentially what happens is it spawns a sub a child process


00:46:48.940 --> 00:46:56.980
or forks the child process on Linux. And then it uses the pickle module in order to send


00:46:56.980 --> 00:47:03.460
data back and forth between the two. And this is great. And it's really transparent. So


00:47:03.460 --> 00:47:10.980
it's super easy to write code for multiprocessing and make use of that. But the issue becomes


00:47:10.980 --> 00:47:17.620
if you have a whole bunch of really small things, you start to have a big overhead with


00:47:17.620 --> 00:47:21.340
with the pickling of the data back and forth. But even worse than that,


00:47:21.340 --> 00:47:24.100
back and forth is like really challenging, right?


00:47:24.100 --> 00:47:28.380
Yeah. So like, if you're pickling a whole bunch of smaller objects, you actually end


00:47:28.380 --> 00:47:32.860
up with a whole bunch of overhead from the pickle module where you're serializing and


00:47:32.860 --> 00:47:37.900
deserializing and creating a bunch of objects and, you know, synchronizing them across those


00:47:37.900 --> 00:47:44.860
processes. But the real problem is when you start to want to do things like network requests


00:47:44.860 --> 00:47:52.780
that are IO bound. So in an individual process, like with multi threading, you could probably


00:47:52.780 --> 00:47:58.220
do 60 to 100 simultaneous network requests.


00:47:58.220 --> 00:48:01.020
But you guys maybe have more than 60 servers or something.


00:48:01.020 --> 00:48:04.980
Sure. But like, if you're trying to do this with multiprocessing instead, where you have


00:48:04.980 --> 00:48:09.940
a like a process pool, and you give it a whole bunch of stuff to work on, each process is


00:48:09.940 --> 00:48:16.780
only going to work on one request at a time. And so so you might spin up a process, and


00:48:16.780 --> 00:48:21.060
it waits for a couple seconds while it's doing that network request. And then and then it


00:48:21.060 --> 00:48:24.980
sends it back and you haven't really gained anything. So if you actually really want to


00:48:24.980 --> 00:48:29.340
saturate all your cores, now you need a whole bunch more processes. And that then has the


00:48:29.340 --> 00:48:35.980
problem of a lot of memory overhead. Because even if you're using copy on write semantics


00:48:35.980 --> 00:48:40.700
with forking, the problem is that like Python goes and touches all the ref counts on everything


00:48:40.700 --> 00:48:45.500
and immediately, you know, removes any benefit of copy on write forked processes.


00:48:45.500 --> 00:48:49.660
Right, which might do like the shared memory, right? So if I create 10 of these things,


00:48:49.660 --> 00:48:54.100
like 95% of the memory just might be one copy. But if you start touching ref counts and all


00:48:54.100 --> 00:48:58.460
sorts of stuff, you know, Instagram went so far as to disable the garbage collector to


00:48:58.460 --> 00:49:01.420
write event that kind of stuff. It was insane. Yeah.


00:49:01.420 --> 00:49:07.340
So it turns out that if you fork a process, as soon as you get into that new process,


00:49:07.340 --> 00:49:14.020
Python touches like 60 to 70% of the objects in its in the pool of memory, which basically


00:49:14.020 --> 00:49:20.940
means it now has to actually copy all of the memory from all of those objects. And so you


00:49:20.940 --> 00:49:24.660
don't actually get to share that much memory between the child and the parent process in


00:49:24.660 --> 00:49:32.680
the first place. So if you, you know, try to spin up, you know, a thousand processes


00:49:32.680 --> 00:49:42.200
in order to saturate 64 cores, you are wasting a lot a lot of memory. So that's where I kind


00:49:42.200 --> 00:49:48.560
of built this piece of AIO multiprocess, where essentially what it's doing is it's spinning


00:49:48.560 --> 00:49:56.300
up a process pool, and it only spins up one per core. And then on each child process,


00:49:56.300 --> 00:50:04.340
it then also spins up an asyncio event loop. And, and rather than giving a normal synchronous


00:50:04.340 --> 00:50:08.860
function as the thing that you're mapping to a whole bunch of data points, you give


00:50:08.860 --> 00:50:15.900
a coroutine. And in this case, what what AIO multiprocess is capable of doing is essentially


00:50:15.900 --> 00:50:22.820
keeping track of how many in-flight coroutines each child process is executing. And essentially


00:50:22.820 --> 00:50:30.940
being able to say that, like, if you wanted to have 32 in-flight coroutines per process,


00:50:30.940 --> 00:50:37.620
and you had 32 processes, then of course, you have whatever 32 times 32 is, I can't


00:50:37.620 --> 00:50:45.980
do that in my head, because I'm terrible at math. Essentially, you get, you know, the


00:50:45.980 --> 00:50:52.340
cross product of those two numbers. And that's the number of actual concurrent things that


00:50:52.340 --> 00:50:54.420
you can do on AIO multiprocess.


00:50:54.420 --> 00:51:01.740
And it's like, instead of creating a whole bunch of one off run this, this thing with


00:51:01.740 --> 00:51:06.980
this, these inputs over there, you say, well, let's create a chunk, like, let's go 32 here,


00:51:06.980 --> 00:51:12.140
32 there, and run them, but do that in an async way. So you're scaling the weight times.


00:51:12.140 --> 00:51:13.140
Yeah, exactly.


00:51:13.140 --> 00:51:16.780
Right, because you're probably doing network stuff.


00:51:16.780 --> 00:51:24.620
Yeah. And, and the benefit of this is essentially like you, you're scaling the benefits


00:51:24.620 --> 00:51:32.140
of asyncio with the benefits of multiprocessing. So for math, it's easier for me to to figure


00:51:32.140 --> 00:51:40.300
out. In reality, what we've seen is that you can generally do somewhere around 256 concurrent


00:51:40.300 --> 00:51:47.900
network requests on asyncio on a single process before you really start to overload the event


00:51:47.900 --> 00:51:50.740
loop. And the thing that a lot of people don't


00:51:50.740 --> 00:51:56.380
Have you looked at the other event loop implementations like uv loop or any of those alternate event


00:51:56.380 --> 00:51:57.620
loop functions?


00:51:57.620 --> 00:52:04.460
So uv loop can make things faster. But the things that it makes faster are the parts


00:52:04.460 --> 00:52:11.420
that process like network request headers. The the real problem at the end of the day


00:52:11.420 --> 00:52:16.860
is that the way that the asyncio framework and event loops work is that for each task


00:52:16.860 --> 00:52:21.500
that you give them, it basically adds it to a round robin queue of all of the things that


00:52:21.500 --> 00:52:27.740
it has to work on. So at the end of the day, if you want to run 1000 concurrent tasks,


00:52:27.740 --> 00:52:32.940
that's 1000 things that it has to go through an order before it gets to any one task. And


00:52:32.940 --> 00:52:37.060
so on asking, Are you done? Are you done? Yeah, something like that, basically. And


00:52:37.060 --> 00:52:42.540
if you're doing anything with the result of that network request before you actually return


00:52:42.540 --> 00:52:49.060
the the real result from your co routine, then you're almost certainly going to be starving


00:52:49.060 --> 00:52:53.780
the event loop of act or starving other co routines on the same event loop of processing


00:52:53.780 --> 00:52:58.720
power. And so what we've seen actually, is you end up with cases where you technically


00:52:58.720 --> 00:53:05.720
time out the request, because it's taken too long for Python or asyncio to get back to


00:53:05.720 --> 00:53:10.540
the network request before it hits like a TCP interrupt or something like that. Right,


00:53:10.540 --> 00:53:11.540
right.


00:53:11.540 --> 00:53:14.500
Oh, that's interesting. Yeah. So this way, you could say like, well, throw 10 processes


00:53:14.500 --> 00:53:21.780
or 20 processes at it and make that and so if you are willing to run like 256 network


00:53:21.780 --> 00:53:28.820
requests per process, and you have 10 processes, or 10 cores, then suddenly you can run you


00:53:28.820 --> 00:53:36.060
know, 2500 network requests simultaneously from asyncio and Python. And at that point,


00:53:36.060 --> 00:53:42.380
you're probably saturating your network connection unless you're talking to mostly local hosts.


00:53:42.380 --> 00:53:45.300
But Facebook, when you're talking about a monitoring system, that's actually what you're


00:53:45.300 --> 00:53:50.620
doing is you're almost certainly talking to things that have super low latency to talk


00:53:50.620 --> 00:53:57.540
to and super high bandwidth. And so this was essentially the answer to that is like, run


00:53:57.540 --> 00:54:03.940
asyncio event loops on a whole bunch of child processes, and then do a bunch of really,


00:54:03.940 --> 00:54:10.500
like smart things to balance the load of the tasks that you're trying to run across all


00:54:10.500 --> 00:54:15.300
of those different processes in order to try and make them, you know, execute as quickly


00:54:15.300 --> 00:54:21.620
as possible. And then also, whenever possible, try to reduce the amount of times that you're


00:54:21.620 --> 00:54:28.300
serializing things back and forth. So one of the other common things that having more


00:54:28.300 --> 00:54:36.220
processes enables you to do is actually do some of the work to process filter aggregate


00:54:36.220 --> 00:54:42.300
that data in those child processes, rather than pickling all the data back to the parent


00:54:42.300 --> 00:54:45.340
process and then you know, dealing with it and aggregating it there.


00:54:45.340 --> 00:54:48.780
Right, you've already got a big scale out for CPU cores.


00:54:48.780 --> 00:54:54.360
So it kind of gives like a local version of MapReduce, where essentially you're mapping


00:54:54.360 --> 00:54:58.660
work across all these child processes. And then inside each batch or whatever, you're


00:54:58.660 --> 00:55:04.420
accurate, you're, you're aggregating that data in into the result that you then send


00:55:04.420 --> 00:55:08.700
back up to the parent process, which can then process and aggregate that data further.


00:55:08.700 --> 00:55:15.660
Yeah, super cool. You gave a talk on this at PyCon in Cleveland, one of the last real


00:55:15.660 --> 00:55:21.420
actual in person PyCon. Yeah. Also, the first first one I've ever attended. And the first


00:55:21.420 --> 00:55:27.100
one that I've ever given a talk at, or first time I was a good one that we're gonna talk.


00:55:27.100 --> 00:55:33.980
Yeah. And yeah, the room was absolutely massive and terrifying. And I don't know how I managed


00:55:33.980 --> 00:55:34.980
to do it all.


00:55:34.980 --> 00:55:39.820
Yeah, it's just kind of block it out, block it out. But no, it's all good. Cool. Yeah.


00:55:39.820 --> 00:55:44.060
So I'll link to that as well. People can check that out. And it really focuses on this AIO


00:55:44.060 --> 00:55:52.180
multi processing part, right? Yeah. Nice. All right. Last of the AIO things at OmniLib


00:55:52.180 --> 00:55:55.020
is AIO IterTools.


00:55:55.020 --> 00:56:01.740
Yeah. So you kind of hinted on this before, like IterTools is mostly a bunch of helpers


00:56:01.740 --> 00:56:11.700
that let you, you know, process lists of things or iterables in nicer ways. And AIO IterTools


00:56:11.700 --> 00:56:18.460
is just basically taking the built in functions like iterating, getting the next thing from


00:56:18.460 --> 00:56:24.600
from an iterable or mapping, or chaining between multiple iterables or whatever, and essentially


00:56:24.600 --> 00:56:32.660
bringing that into an async first world. So all of the functions in AIO IterTools will


00:56:32.660 --> 00:56:39.960
accept both like normal standard iterators or lists or whatever, as well as async iterables


00:56:39.960 --> 00:56:46.860
or generators or whatever. And essentially, it up converts everything to an async iterable,


00:56:46.860 --> 00:56:53.300
and then gives you more async iterable interfaces to work on these.


00:56:53.300 --> 00:56:58.380
So I know how to create a generator with like yield. So I have a function, it does a thing,


00:56:58.380 --> 00:57:01.820
and then it goes through some processing, it says yield an item, like here's one of


00:57:01.820 --> 00:57:05.620
the things in the list. That's already really good because it does like lazy loading, but


00:57:05.620 --> 00:57:13.380
it doesn't scale the waiting time, right? It just waits. So for the async...


00:57:13.380 --> 00:57:14.380
So it's tricky in this case.


00:57:14.380 --> 00:57:17.460
What's the difference there?


00:57:17.460 --> 00:57:23.260
In this case, if you just call the function async def, and then have a yield.


00:57:23.260 --> 00:57:27.020
And then you have a yield statement in it, it creates an async generator, which is just


00:57:27.020 --> 00:57:35.140
an async iterable object that similar to how when you call a coroutine, it's an object,


00:57:35.140 --> 00:57:40.460
but it doesn't actually run until you await it. With an async generator, calling it creates


00:57:40.460 --> 00:57:42.500
the generator object, but you don't actually...


00:57:42.500 --> 00:57:45.300
Then the async part is done, right? At that point?


00:57:45.300 --> 00:57:49.860
Well, it's like it still doesn't even start running it until you actually start to use


00:57:49.860 --> 00:57:56.620
the async for or some other async iteration to then iterate over it.


00:57:56.620 --> 00:58:02.500
So if you're using the async iterator, you still get the lazy loading of everything like


00:58:02.500 --> 00:58:08.180
with a normal generator, but you also have the potential for your thing to be interrupted.


00:58:08.180 --> 00:58:13.540
So the common use case here or the expected use case would be if you're doing something


00:58:13.540 --> 00:58:19.600
like talking to a whole bunch of network hosts, and you want to return the results as they


00:58:19.600 --> 00:58:25.180
come in as an async iterable, then you could use something like AIO iter tools to then


00:58:25.180 --> 00:58:32.180
do things like batch up those results or run another coroutine across every result as it


00:58:32.180 --> 00:58:35.620
comes in, things like that.


00:58:35.620 --> 00:58:41.060
And the other added benefit in here is that there's also a concurrency limited version


00:58:41.060 --> 00:58:45.740
of gather. So as I said earlier, when you have a whole bunch of tasks, you're actually


00:58:45.740 --> 00:58:49.860
making the event loop do a whole bunch more work. One of the common things I've seen is


00:58:49.860 --> 00:58:59.340
that people will spawn 5000 tasks, and each task, or they'll all have some semaphore that


00:58:59.340 --> 00:59:04.700
limits how many of them can execute at once. But you still have 5000 tasks that the event


00:59:04.700 --> 00:59:09.780
loop is trying to service. And so you're you're giving a whole bunch of overhead every time


00:59:09.780 --> 00:59:13.940
it wants to switch between things. It's got to potentially go through up to 5000 of them


00:59:13.940 --> 00:59:20.100
before it gets to one that it can actually service. So the concurrency limited version


00:59:20.100 --> 00:59:26.100
of gather that AIO iter tools has let you specify some limit, like only run 64 things


00:59:26.100 --> 00:59:34.020
at a time. And so it will, you know, try to fetch the first 64 things of all of the coroutines


00:59:34.020 --> 00:59:39.500
or awaitables that you give it. And, and it will start to yield those values as they come


00:59:39.500 --> 00:59:44.220
in. But essentially, it's making sure that the event loop would never see more than 64


00:59:44.220 --> 00:59:49.220
active tasks at a time, at least from that specific memory, they don't get really get


00:59:49.220 --> 00:59:57.780
thrown into the running task. So one of the challenges or criticisms almost I've seen


00:59:57.780 --> 01:00:04.540
around asyncio is that it, it doesn't allow for any back pressure or whatever, right?


01:00:04.540 --> 01:00:08.580
Like, if I'm talking to a database, it used to be that the web front end would have like


01:00:08.580 --> 01:00:12.540
some kind of performance limit, it could only go so hard against the database. But if you


01:00:12.540 --> 01:00:17.820
do just await it, like all the traffic just piles in until it potentially can't take it


01:00:17.820 --> 01:00:22.620
anymore. And it sounds like this has some mechanisms to address that kind of generally


01:00:22.620 --> 01:00:26.660
speaking, that's at least the general intent of it is to be able to use this concurrency


01:00:26.660 --> 01:00:35.220
limit to try and prevent overloading either the event loop or your network or whatever.


01:00:35.220 --> 01:00:40.620
So even if you have 5000 items, by setting the limit to 64, you know that you're only


01:00:40.620 --> 01:00:46.100
going to be doing that many at a time. And, and then you can combine that that concurrency


01:00:46.100 --> 01:00:52.740
limited gather with something like, like, the result of that is its own, you know, async


01:00:52.740 --> 01:00:57.320
iterable. And then you could also combine that with things like chain or other things


01:00:57.320 --> 01:01:04.420
in order to mix that in with the rest of the like, inner tools, functional lifestyle, if


01:01:04.420 --> 01:01:05.420
you will.


01:01:05.420 --> 01:01:06.420
Yeah.


01:01:06.420 --> 01:01:11.860
Super cool. I can imagine that these might find some way to work together, you might


01:01:11.860 --> 01:01:19.940
have some asyncio, or say, sorry, AIO, inner tools thing that then you feed off to AIO


01:01:19.940 --> 01:01:21.860
multiprocessing or something like that.


01:01:21.860 --> 01:01:27.100
Yeah, exactly. These are definitely a whole bunch of tools that I've put together in various


01:01:27.100 --> 01:01:28.100
different use cases.


01:01:28.100 --> 01:01:34.300
Yeah, very neat. All right, well, we're getting quite near the end of the show. I think we've


01:01:34.300 --> 01:01:40.100
talked about a lot about these very, very cool libraries. So before we get out of here,


01:01:40.100 --> 01:01:43.580
though, we touched on this at the beginning, but I'll ask you this as one of the two main


01:01:43.580 --> 01:01:46.820
questions at the end of the show, if you're going to write some Python code, what editor


01:01:46.820 --> 01:01:49.920
do you use?


01:01:49.920 --> 01:01:56.620
So the snarky answer is anything with a Vim emulation mode. That was the thing that


01:01:56.620 --> 01:02:04.620
I learned in college. And I specifically avoided answering that, like, answering that earlier


01:02:04.620 --> 01:02:08.820
when we were talking about it. But that's what I learned when I was writing a whole


01:02:08.820 --> 01:02:14.300
bunch of PHP code. And that's what I used for four years. And then eventually, I found


01:02:14.300 --> 01:02:20.660
sublime text, and I really liked that. But it kind of seemed dead in the water. Adam


01:02:20.660 --> 01:02:27.660
came out, but Adam was slow. And so these days, I'm using VS Code, primarily because


01:02:27.660 --> 01:02:33.360
it has excellent Python integration, but also because it has a lot of like Facebook builds


01:02:33.360 --> 01:02:39.020
a lot of the things that we used to have on top of Adam called new Clyde, which especially


01:02:39.020 --> 01:02:44.780
were a lot of like remote editing tools. Okay, we've, we've rebuilt a lot of those on top


01:02:44.780 --> 01:02:49.740
of VS Code, because VS Code is faster, nicer, you know, has better ongoing support from


01:02:49.740 --> 01:02:51.020
the community and so forth.


01:02:51.020 --> 01:02:55.340
VS Code seems like the natural successor to Adam.


01:02:55.340 --> 01:03:01.020
Yeah. And like I said before, it's like, I had tried pi charm at one point, but it's


01:03:01.020 --> 01:03:05.740
it's one of those cases where I touch just enough stuff that's not Python that I really


01:03:05.740 --> 01:03:11.680
want my tools to work and function the same way regardless. And so VS Code has the better


01:03:11.680 --> 01:03:16.820
sort of like broader language support, where it's like, there's some days where I just


01:03:16.820 --> 01:03:21.100
have to write a bash script, and I want it to be able to do nice things for bash or,


01:03:21.100 --> 01:03:25.580
or, you know, I use it as a markdown editor, and it has a markdown preview, things like


01:03:25.580 --> 01:03:26.580
that.


01:03:26.580 --> 01:03:30.660
Yeah. All right, cool. Sounds good. And then notable pipey package. I mean, I guess we


01:03:30.660 --> 01:03:36.140
spent a lot of time on four of them, right? Five also talks about Microsoft, you sort.


01:03:36.140 --> 01:03:43.860
Yeah. So the joke answer is, I have a package called AIO Seinfeld. That's built on top of


01:03:43.860 --> 01:03:52.700
a SQLite. And essentially, you give it a database of Seinfeld scripts. And you you can search


01:03:52.700 --> 01:03:59.140
for things by actor or by keyword of what they're saying. And it will essentially give


01:03:59.140 --> 01:04:05.940
you back some elements of dialogue from from a script that contains your search query.


01:04:05.940 --> 01:04:11.780
And this is powering a site I have called Seinfeld quote.com, which is basically just


01:04:11.780 --> 01:04:20.260
a really old bootstrap template that that lets you search for pieces of Seinfeld quotes.


01:04:20.260 --> 01:04:28.340
I also implemented a chat bot and discord for some of my friends that also uses this.


01:04:28.340 --> 01:04:31.780
The more serious answer would be the other one that we didn't talk about from OmniLib,


01:04:31.780 --> 01:04:38.860
which is attribution, which is essentially a, a quick program to automate the generation


01:04:38.860 --> 01:04:45.100
of change logs, and to automate the process of cutting a release for a project. And so


01:04:45.100 --> 01:04:50.260
I use this on all of the OmniLib projects. And essentially, I type one command attribution


01:04:50.260 --> 01:04:58.380
release, I'm sorry, attribution tag, and then a version number. And it will drop a dunder


01:04:58.380 --> 01:05:07.260
version dot pi in the project directory, it will create a get tag. And let's it lets you


01:05:07.260 --> 01:05:12.940
then type in what you want the release notes to be. It's assuming you know, markdown format.


01:05:12.940 --> 01:05:19.060
And then once it's made that tag, then it regenerates the change log for that, for that


01:05:19.060 --> 01:05:24.380
tag and retags it appropriately. And so you get this really nice thing where the actual


01:05:24.380 --> 01:05:29.900
tag of the project has both the updated change log, and the appropriate version number


01:05:29.900 --> 01:05:35.480
file. So you only ever type the version in once, you only ever type the release notes


01:05:35.480 --> 01:05:40.620
in once. And it gives you, you know, as much help and automation around that as possible.


01:05:40.620 --> 01:05:45.620
Oh, yeah. Okay, very cool. That's a good one. All right, final call to action, people are


01:05:45.620 --> 01:05:51.100
excited about asyncio, maybe some of the stuff at OmniLib, they want to get started,


01:05:51.100 --> 01:05:53.300
what do you tell them?


01:05:53.300 --> 01:05:59.060
If they want to get started on the projects, going to OmniLib.dev is the easiest way to


01:05:59.060 --> 01:06:07.100
find the ones that are currently hosted on the project. We're always welcoming of


01:06:07.100 --> 01:06:12.020
code review from the community. So even if you're, you know, not a maintainer, if you


01:06:12.020 --> 01:06:17.360
are interested in reviewing pull requests, and giving feedback on things, always welcoming


01:06:17.360 --> 01:06:21.960
of that there's never enough time in my own personal life to, to review everything or


01:06:21.960 --> 01:06:27.760
respond to everything. Otherwise, if there are things in these projects that you are


01:06:27.760 --> 01:06:33.340
interested in adding, like new features or fixing bugs or whatever, either open an issue


01:06:33.340 --> 01:06:39.020
or just create a pull request. And I am more than happy to, you know, engage in design


01:06:39.020 --> 01:06:45.860
decisions or discussions or whatever. Make sure that ideally, if you open an issue first,


01:06:45.860 --> 01:06:49.360
make sure you're not wasting your time on a pull request that's going in the wrong direction.


01:06:49.360 --> 01:06:52.900
Right, right. Otherwise, people might have this idea, but you're like, this is really


01:06:52.900 --> 01:06:57.700
inconsistent where this project is meant to go or so usually, even if it's perfect, you


01:06:57.700 --> 01:07:04.100
can't accept it, right? So good. Right. So if it's just like a bug fix or something,


01:07:04.100 --> 01:07:08.180
then you know, probably just worth creating a pull request. And you know, I'm not going


01:07:08.180 --> 01:07:19.780
to bite your head off. But otherwise, the only other thing I would say is that LGBTQ


01:07:19.780 --> 01:07:24.660
things are very personal to me. And so I would ask that if you're in a position to do so


01:07:24.660 --> 01:07:31.060
that you please donate to an LGBTQ charity that will help in the community. There's two


01:07:31.060 --> 01:07:39.060
that I really like. One is called Power On. And that's a charity that donates technology


01:07:39.060 --> 01:07:49.740
to LGBTQ youth. They're either homeless or disadvantaged. And they have that at poweronLGBT.org.


01:07:49.740 --> 01:07:54.220
And then the other one is the Trevor Project, which is crisis intervention and a suicide


01:07:54.220 --> 01:08:01.180
hotline for LGBTQ youth. And that's at the Trevor project.org. Awesome. Those are just


01:08:01.180 --> 01:08:06.380
two examples, but there are plenty. Worst case, just donate to a food bank near you.


01:08:06.380 --> 01:08:12.940
Cool. Yeah, that's a great advice. Great call to action. Seems like your projects


01:08:12.940 --> 01:08:17.260
are also really open to new contributors, people getting into open source. So participating


01:08:17.260 --> 01:08:23.580
in that way seems like a great, great thing. Fantastic. All right, John. Well, thank you


01:08:23.580 --> 01:08:27.420
so much for being on Talk Python. It's been great to have you here. Thank you for having


01:08:27.420 --> 01:08:30.820
me so much. I really appreciate it. Yeah, it's been fun to talk about all the async


01:08:30.820 --> 01:08:37.900
stuff. Now, before I close the live stream, quick question. Suyash asks, can I give a


01:08:37.900 --> 01:08:43.020
roadmap for the courses that would be super helpful for beginners? Suyash, if you're still


01:08:43.020 --> 01:08:47.020
here, can you tell me if you mean like what courses I recommend for beginners or what


01:08:47.020 --> 01:08:51.980
courses are coming out that might be relevant to beginners? Let me, if you're still here,


01:08:51.980 --> 01:08:57.940
give me a sense of like structuring that. Otherwise, I'll answer the first part that


01:08:57.940 --> 01:09:02.700
came to mind anyways, like what we got coming up. We actually have a modern Python projects


01:09:02.700 --> 01:09:06.940
course coming up that talks about a lot of the tools that we touched on here and things


01:09:06.940 --> 01:09:12.540
like poetry, setting up continuous integration, working with VS Code, a lot of tools around


01:09:12.540 --> 01:09:16.100
just like making modern projects and something like attribution seems like it would fit really


01:09:16.100 --> 01:09:23.900
well in there as well. Then we have a second FastAPI course coming. But yeah. All right,


01:09:23.900 --> 01:09:28.100
well, I'm not seeing anything, but there's the answer anyway. Suyash, if you come back


01:09:28.100 --> 01:09:30.980
and watch it. All right. Thanks again, John. And thanks everyone for watching.


01:09:30.980 --> 01:09:39.380
[BLANK_AUDIO]

