WEBVTT

00:00:00.000 --> 00:00:04.880
Owen, Gregory, welcome to Talk Python To Me.


00:00:04.880 --> 00:00:05.880
>> Hi, Michael.


00:00:05.880 --> 00:00:07.080
It's great to be on the show.


00:00:07.080 --> 00:00:08.560
Thank you for inviting us today.


00:00:08.560 --> 00:00:09.560
>> Hi there.


00:00:09.560 --> 00:00:10.560
Thanks for having us.


00:00:10.560 --> 00:00:11.560
>> Yeah.


00:00:11.560 --> 00:00:13.720
Really great to have you both on the show.


00:00:13.720 --> 00:00:17.520
It's going to be a bit of a flaky episode, though, wouldn't you say?


00:00:17.520 --> 00:00:19.840
>> It's definitely going to be flaky.


00:00:19.840 --> 00:00:20.840
>> Very flaky.


00:00:20.840 --> 00:00:25.640
Looking forward to talking about flaky tests and what we can do about them.


00:00:25.640 --> 00:00:31.720
It's one of these realities of writing lots of unit tests for real-world systems, right?


00:00:31.720 --> 00:00:34.940
They end up in weird places.


00:00:34.940 --> 00:00:39.780
So for better or for worse, I've implemented a lot of programs in Python, and many of them


00:00:39.780 --> 00:00:43.140
have test suites with flaky test cases inside of them.


00:00:43.140 --> 00:00:50.100
So I have to confess to everyone, I myself have written programs with many flaky tests.


00:00:50.100 --> 00:00:51.100
As have I.


00:00:51.100 --> 00:00:52.100
As have I.


00:00:52.100 --> 00:00:58.820
All right, before we get into the show itself, maybe just a little bit of background on you


00:00:58.820 --> 00:00:59.820
two.


00:00:59.820 --> 00:01:00.820
Gregory, you want to go first?


00:01:00.820 --> 00:01:04.460
Just a quick introduction about who you are, how you got into programming Python?


00:01:04.460 --> 00:01:05.460
Sure.


00:01:05.460 --> 00:01:10.660
My name is Gregory Kapammer, and I'm a faculty member in the Department of Computer Science


00:01:10.660 --> 00:01:12.700
at Allegheny College.


00:01:12.700 --> 00:01:17.820
I've actually been programming in Python since I took an AI course in graduate school years


00:01:17.820 --> 00:01:25.620
ago and we had to implement all of our algorithms in Python. I stopped using Python for a short


00:01:25.620 --> 00:01:30.340
period of time and then picked it back up again once I learned about pytest because


00:01:30.340 --> 00:01:36.020
I found it to be such an awesome test automation framework and I've been programming in Python


00:01:36.020 --> 00:01:37.500
regularly since then.


00:01:37.500 --> 00:01:43.840
>> Yeah, that's cool. What's pretty interesting is people who don't even necessarily do Python


00:01:43.840 --> 00:01:46.700
sometimes use Python to write the tests.


00:01:46.700 --> 00:01:51.800
Yeah, absolutely. I have to say I've used a bunch of different test automation frameworks


00:01:51.800 --> 00:01:56.920
and pytest is by far and away my favorite framework out of them all.


00:01:56.920 --> 00:02:00.160
Yeah, absolutely. Owen, hello.


00:02:00.160 --> 00:02:06.840
So, hi. I'm a PhD student at the University of Sheffield. I'm actually coming right to


00:02:06.840 --> 00:02:11.040
the end of my time as a PhD student.


00:02:11.040 --> 00:02:13.600
So that's quite a journey, isn't it?


00:02:13.600 --> 00:02:16.400
It is quite a journey, yeah.


00:02:16.400 --> 00:02:22.600
And throughout my whole PhD, my main topic has been flaky tests.


00:02:22.600 --> 00:02:29.700
So before I even started my PhD, I had Python experience from just odd undergraduate projects


00:02:29.700 --> 00:02:32.320
that had to be done in Python.


00:02:32.320 --> 00:02:38.960
But for all of my research, I've thought it very important to use real software, real


00:02:38.960 --> 00:02:44.900
tests written by real people to identify flaky tests, find out what causes them, that kind


00:02:44.900 --> 00:02:47.180
of thing.


00:02:47.180 --> 00:02:51.280
And all of my sort of practical work has been done with Python.


00:02:51.280 --> 00:03:00.160
So for example, I've written pytest plugins to help detect flaky tests.


00:03:00.160 --> 00:03:05.040
And as Greg said, I think pytest is a great framework.


00:03:05.040 --> 00:03:06.500
It's very extensible.


00:03:06.500 --> 00:03:10.240
It's very great for writing plugins, very good API.


00:03:10.240 --> 00:03:15.060
And yeah, I've used lots of different types


00:03:15.060 --> 00:03:18.800
of Python projects as subjects in experiments.


00:03:18.800 --> 00:03:23.080
So I've seen quite a wide array of different types


00:03:23.080 --> 00:03:24.580
of software written in Python.


00:03:24.580 --> 00:03:29.020
- Have you studied a lot of other people's software,


00:03:29.020 --> 00:03:31.900
interviewed different people to see


00:03:31.900 --> 00:03:34.920
how they're encountering flaky tests?


00:03:34.920 --> 00:03:38.120
So I've not interviewed anyone exactly,


00:03:38.120 --> 00:03:42.560
but I did do a questionnaire that I sent out on Twitter,


00:03:42.560 --> 00:03:44.400
LinkedIn, that kind of thing,


00:03:44.400 --> 00:03:48.060
where we just wanted to get as many different kinds of developers


00:03:48.060 --> 00:03:50.560
talking about flaky tests.


00:03:50.560 --> 00:03:53.840
So we asked them questions like, first of all, what is a flaky test?


00:03:53.840 --> 00:03:56.480
People have slightly different definitions.


00:03:56.480 --> 00:04:00.540
And then I went into, what do you think causes them?


00:04:00.540 --> 00:04:04.100
What impacts do they have on you and your sort of professional workflow?


00:04:04.100 --> 00:04:09.460
And then we talked a little bit about what people do about them as well.


00:04:09.460 --> 00:04:11.540
Interesting.


00:04:11.540 --> 00:04:14.620
Well, maybe that's a good place to start.


00:04:14.620 --> 00:04:18.180
And either of you just jump in now as you see fit.


00:04:18.180 --> 00:04:21.500
So let's just start with, you know, what is a flaky test?


00:04:21.500 --> 00:04:26.980
I mean, we all know that having unit tests and broader tests, you know,


00:04:26.980 --> 00:04:29.080
integration tests and so on.


00:04:29.080 --> 00:04:32.560
For our code, it's generally a good thing.


00:04:32.560 --> 00:04:33.980
I guess if you write them poorly enough,


00:04:33.980 --> 00:04:35.520
it's not a good thing.


00:04:35.520 --> 00:04:38.780
But mostly it's recommended advice


00:04:38.780 --> 00:04:42.200
and there's a spectrum of how strongly it's recommended.


00:04:42.200 --> 00:04:44.700
Is it extreme programming TDD recommended


00:04:44.700 --> 00:04:46.860
or is it you have to have some tests


00:04:46.860 --> 00:04:48.800
to submit a PR level of recommended?


00:04:48.800 --> 00:04:51.280
But we think it's great.


00:04:51.280 --> 00:04:54.520
But there are these negative aspects


00:04:54.520 --> 00:04:57.480
of having tests as well, right?


00:04:57.480 --> 00:04:58.960
It's easy to sell them as a positive,


00:04:58.960 --> 00:05:01.000
but they become a maintenance burden.


00:05:01.000 --> 00:05:02.960
They become duplicate code.


00:05:02.960 --> 00:05:06.120
And the flakiness, I think,


00:05:06.120 --> 00:05:09.220
is a particularly challenging part of it.


00:05:09.220 --> 00:05:12.320
So let's start there.


00:05:12.320 --> 00:05:14.400
What's a flaky test for you all?


00:05:14.400 --> 00:05:16.040
- So I'll start off and then Owen,


00:05:16.040 --> 00:05:19.320
if you'd like to add more details, that would be awesome.


00:05:19.320 --> 00:05:22.000
I would say that a flaky test is a test case


00:05:22.000 --> 00:05:26.640
that passes or fails in a non-deterministic fashion,


00:05:26.640 --> 00:05:30.160
even when you're not changing the source code


00:05:30.160 --> 00:05:34.200
of the program or the test suite for the program.


00:05:34.200 --> 00:05:39.320
So this is a situation where the test case may sometimes pass,


00:05:39.320 --> 00:05:40.820
then it may fail,


00:05:40.820 --> 00:05:43.020
and then it may start to pass again,


00:05:43.020 --> 00:05:45.120
even though as a developer,


00:05:45.120 --> 00:05:47.760
you're not making changes to the program under


00:05:47.760 --> 00:05:51.760
test or the source code of the test suite itself.


00:05:51.760 --> 00:05:56.120
>> Yeah, that's, that is tricky, right?


00:05:56.120 --> 00:06:02.260
Because just one day the tests start failing, we didn't, we didn't change anything.


00:06:02.260 --> 00:06:03.260
Nothing has changed here.


00:06:03.260 --> 00:06:04.260
Why?


00:06:04.260 --> 00:06:06.120
How could this have anything to do with it, right?


00:06:06.120 --> 00:06:08.060
And oh, and so, okay.


00:06:08.060 --> 00:06:12.480
>> The only other thing I was going to add is that flaky test cases could manifest themselves


00:06:12.480 --> 00:06:17.560
on a developer workstation, or they could also manifest when you're running them in


00:06:17.560 --> 00:06:20.160
in a continuous integration environment as well.


00:06:20.160 --> 00:06:23.160
- Yeah, for sure.


00:06:23.160 --> 00:06:24.920
- So to just sort of build on what Greg said there


00:06:24.920 --> 00:06:25.740
a little bit.


00:06:25.740 --> 00:06:29.120
So one interesting thing we found from that developer survey.


00:06:29.120 --> 00:06:31.080
So the definition that Greg gave just then


00:06:31.080 --> 00:06:34.240
was pretty much the definition we proposed


00:06:34.240 --> 00:06:36.360
to the respondents of the survey.


00:06:36.360 --> 00:06:38.160
And then we asked them, do you agree?


00:06:38.160 --> 00:06:42.060
If not, what's your definition?


00:06:42.060 --> 00:06:44.560
Most people agreed, but some people said,


00:06:44.560 --> 00:06:47.060
well, it doesn't just depend on the source code


00:06:47.060 --> 00:06:48.660
of the program and the code of the test,


00:06:48.660 --> 00:06:50.780
but also the execution environment.


00:06:50.780 --> 00:06:55.780
So you can have, like you can take one piece of software


00:06:55.780 --> 00:06:58.440
and its associated test suite,


00:06:58.440 --> 00:07:00.500
run it on one computer it passes,


00:07:00.500 --> 00:07:01.540
run it on another system,


00:07:01.540 --> 00:07:02.940
and then for whatever reason it doesn't.


00:07:02.940 --> 00:07:05.020
So nothing's changed at all,


00:07:05.020 --> 00:07:06.580
except for the execution environment.


00:07:06.580 --> 00:07:10.780
And that's quite prevalent when you're talking about CI,


00:07:10.780 --> 00:07:14.580
'cause most of the time these are running on cloud machines,


00:07:14.580 --> 00:07:18.780
which may not be all exactly the same in spec.


00:07:18.780 --> 00:07:23.240
So from the perspective of the developer,


00:07:23.240 --> 00:07:25.440
it looked as if a test could just fail,


00:07:25.440 --> 00:07:27.420
but maybe it failed because it was running


00:07:27.420 --> 00:07:29.920
on a slightly different machine that time it was run.


00:07:29.920 --> 00:07:31.840
- I never really thought of that.


00:07:31.840 --> 00:07:33.640
Obviously the different environments, right?


00:07:33.640 --> 00:07:37.120
Like a CI machine is very different than my MacBook.


00:07:37.120 --> 00:07:41.600
So clearly it could be the case that a test passes


00:07:41.600 --> 00:07:45.680
on my machine, not in CI or vice versa.


00:07:45.680 --> 00:07:47.360
But I hadn't really thought about,


00:07:47.360 --> 00:07:50.680
well, this time it got an AMD cloud processor


00:07:50.680 --> 00:07:52.260
and it was already under heavy load


00:07:52.260 --> 00:07:53.940
and so the timing changed.


00:07:53.940 --> 00:07:59.120
Versus the other time it was on a premium Intel thing


00:07:59.120 --> 00:08:01.360
in the cloud that had no other thing going on


00:08:01.360 --> 00:08:03.360
so it behaved differently.


00:08:03.360 --> 00:08:04.920
It's pretty wild.


00:08:04.920 --> 00:08:06.600
- Yeah, your point is a good one, Michael.


00:08:06.600 --> 00:08:10.920
It's actually often the case that the speed of the CPU


00:08:10.920 --> 00:08:13.760
or the amount of memory or the amount of disk


00:08:13.760 --> 00:08:17.280
on the testing workstation can make a big difference


00:08:17.280 --> 00:08:19.920
when it comes to manifesting a flaky tests.


00:08:19.920 --> 00:08:24.520
- Yeah, I guess that makes a lot of sense,


00:08:24.520 --> 00:08:26.200
especially if it's a race condition, right?


00:08:26.200 --> 00:08:27.980
If you're having some sort of parallelism,


00:08:27.980 --> 00:08:32.560
then that could really come into memory as well, maybe.


00:08:32.560 --> 00:08:34.800
Right, maybe you run out of memory


00:08:34.800 --> 00:08:36.560
and get an out of memory exception.


00:08:38.820 --> 00:08:41.780
So when we're talking about flaky tests,


00:08:41.780 --> 00:08:43.960
one thing that came to mind for me,


00:08:43.960 --> 00:08:46.860
and I wanna bring it up at the start here


00:08:46.860 --> 00:08:50.520
'cause I'm wondering if this classifies as flaky for you,


00:08:50.520 --> 00:08:54.380
if this is some other kind of not great test,


00:08:54.380 --> 00:08:57.260
is has to do with science, right?


00:08:57.260 --> 00:09:00.900
So scientific type of computing, mathematical stuff.


00:09:00.900 --> 00:09:04.260
Obviously you shouldn't say,


00:09:04.260 --> 00:09:05.700
I've got some floating point number,


00:09:05.700 --> 00:09:09.940
equal equal some other long precise,


00:09:09.940 --> 00:09:11.860
here's my definition of the square root of two


00:09:11.860 --> 00:09:14.700
as a float equal equal that, right?


00:09:14.700 --> 00:09:16.020
That might be too precise.


00:09:16.020 --> 00:09:17.940
But what I'm thinking about is,


00:09:17.940 --> 00:09:21.540
if you make the smallest amount of change to some algorithm


00:09:21.540 --> 00:09:23.100
or the way something works,


00:09:23.100 --> 00:09:28.180
it could change some,


00:09:28.180 --> 00:09:29.620
like maybe you're trying to say,


00:09:29.620 --> 00:09:31.820
do we get a curve that looks like this?


00:09:31.820 --> 00:09:36.820
or do we match some kind of criteria on statistics?


00:09:36.820 --> 00:09:39.220
It could change just a little bit,


00:09:39.220 --> 00:09:42.740
but the way that you're testing for it to be a match,


00:09:42.740 --> 00:09:46.260
it changes enough in that regard,


00:09:46.260 --> 00:09:49.300
even though effectively it kind of means the same thing.


00:09:49.300 --> 00:09:50.220
Do you know what I'm asking?


00:09:50.220 --> 00:09:52.900
Does that count as a flaky test for you?


00:09:52.900 --> 00:09:55.100
- So I think, so what you're talking about


00:09:55.100 --> 00:09:58.260
is a very specific category of flaky test.


00:09:58.260 --> 00:10:00.580
So I would call that a flaky test.


00:10:00.580 --> 00:10:04.260
So yeah, so when you're dealing with programs like, for example,


00:10:04.260 --> 00:10:09.460
various machine learning packages, you'll see a lot of test cases that will say,


00:10:09.460 --> 00:10:13.780
"Assert X is equal to this within a certain tolerance range," or something,


00:10:13.780 --> 00:10:16.260
or is approximately equal to something.


00:10:16.260 --> 00:10:24.500
So with these kinds of tests, they are kind of inherently flaky,


00:10:24.500 --> 00:10:29.300
but there is a trade-off between how strong you want the test to be,


00:10:29.300 --> 00:10:34.300
i.e. how narrow you want that acceptable band to be


00:10:34.300 --> 00:10:38.540
versus how flaky you want it to be.


00:10:38.540 --> 00:10:40.540
Sorry, how flaky you don't want it to be.


00:10:40.540 --> 00:10:43.040
So the stronger you make the test,


00:10:43.040 --> 00:10:46.780
the more flaky it's likely to be because that band is narrower.


00:10:46.780 --> 00:10:48.780
But if you just increase that tolerance,


00:10:48.780 --> 00:10:50.780
then, yeah, it won't be as flaky anymore,


00:10:50.780 --> 00:10:53.280
but then maybe it won't catch as many bugs anymore


00:10:53.280 --> 00:10:56.280
because it's too relaxed of a test.


00:10:56.280 --> 00:10:57.780
Sure.


00:10:57.780 --> 00:11:00.940
And unfortunately, if you're going to reduce a test case


00:11:00.940 --> 00:11:02.260
to either a pass or a fail,


00:11:02.260 --> 00:11:04.100
which is something that we have to do,


00:11:04.100 --> 00:11:05.700
there's no real way around that.


00:11:05.700 --> 00:11:10.380
There has been done work where people have tried


00:11:10.380 --> 00:11:14.940
to calculate, oops, sorry, my, there we go.


00:11:14.940 --> 00:11:16.340
It's on a motion sensor.


00:11:16.340 --> 00:11:19.620
- Wave at the light, it's so dark for you, yes.


00:11:19.620 --> 00:11:22.300
- Well, I was like, yeah, so there is work people have done


00:11:22.300 --> 00:11:26.140
where they try to calculate what is the best


00:11:26.140 --> 00:11:30.700
tolerance threshold in test cases, but there's no kind of silver bullet solution for that


00:11:30.700 --> 00:11:31.700
kind of flakiness.


00:11:31.700 --> 00:11:35.100
Yeah, it's tricky, isn't it?


00:11:35.100 --> 00:11:36.100
Yeah.


00:11:36.100 --> 00:11:37.100
Yeah.


00:11:37.100 --> 00:11:40.820
Pardvan, out in the audience, has an interesting one.


00:11:40.820 --> 00:11:44.460
Maybe probably lands a little more into the realm directly of the kind of stuff that you're


00:11:44.460 --> 00:11:45.460
talking about.


00:11:45.460 --> 00:11:53.660
He says, "At one of my jobs, we were testing chained celery tasks that become flaky sometimes.


00:11:53.660 --> 00:11:58.940
once one Celery task fails for some reason, the chain task could fail as well.


00:11:58.940 --> 00:12:03.980
Those kind of external systems are probably at the heart of a lot of this, right?


00:12:03.980 --> 00:12:07.620
Yeah, I think it is often at the heart of it.


00:12:07.620 --> 00:12:13.740
Whether you're using Celery or some other work processing queue, or alternatively, if


00:12:13.740 --> 00:12:19.220
you're interacting with a document database or a relational database, in all of those


00:12:19.220 --> 00:12:25.660
occasions when you interact with some storage or processing system in your environment,


00:12:25.660 --> 00:12:29.300
you may not have control over that part of your environment.


00:12:29.300 --> 00:12:33.760
And then once again, that's another way in which flakiness can creep into the testing


00:12:33.760 --> 00:12:34.760
process.


00:12:34.760 --> 00:12:35.760
>> Right.


00:12:35.760 --> 00:12:43.020
And you maybe don't care about how the Celery server is doing, but at the same time, you


00:12:43.020 --> 00:12:46.660
need it, you maybe you haven't mocked that part out, you need to somehow, are you doing


00:12:46.660 --> 00:12:48.220
an end-to-end test or something?


00:12:48.220 --> 00:12:52.060
it becomes part of the reliability of your system.


00:12:52.060 --> 00:12:54.580
Even though maybe that's like a Q&A celery server,


00:12:54.580 --> 00:12:56.900
not the production server, right?


00:12:56.900 --> 00:12:58.600
- Yeah, and in fact, you brought up another


00:12:58.600 --> 00:13:01.300
really good point when it comes to mocking.


00:13:01.300 --> 00:13:04.540
There are many circumstances in which I've had to use


00:13:04.540 --> 00:13:08.580
one of the various mocking features that Python provides


00:13:08.580 --> 00:13:12.820
in order to stand up my own version of an external service.


00:13:12.820 --> 00:13:15.380
However, that is a trade-off,


00:13:15.380 --> 00:13:17.640
like Owen mentioned previously,


00:13:17.640 --> 00:13:21.720
because now my test case may be less flaky


00:13:21.720 --> 00:13:24.520
and yet it's also less realistic


00:13:24.520 --> 00:13:26.360
and so therefore may not be able


00:13:26.360 --> 00:13:28.600
to catch certain types of bugs.


00:13:28.600 --> 00:13:32.280
So now we've seen another example of the trade-off


00:13:32.280 --> 00:13:36.240
associated with making a test less flaky


00:13:36.240 --> 00:13:39.620
but perhaps also making it less realistic.


00:13:39.620 --> 00:13:42.440
- Yeah, I'm starting to get a sense


00:13:42.440 --> 00:13:47.440
that there's probably a precision versus stability


00:13:47.440 --> 00:13:50.940
trade-off that's always at play here.


00:13:50.940 --> 00:13:55.640
Yeah, obviously in the extreme end of the spectrum,


00:13:55.640 --> 00:14:00.340
you can make any test non-flaky by just deleting it, right?


00:14:00.340 --> 00:14:03.340
That way, put an ignore attribute on it.


00:14:03.340 --> 00:14:04.340
Exactly.


00:14:04.340 --> 00:14:07.840
So you've got to be careful that you're not optimizing your tests


00:14:07.840 --> 00:14:13.140
just for passing, which if you're trying to get a PR through,


00:14:13.140 --> 00:14:18.140
then that is a trap you might fall into.


00:14:18.140 --> 00:14:21.660
- Yeah, yeah, absolutely.


00:14:21.660 --> 00:14:24.260
So you talked about that survey a little bit before.


00:14:24.260 --> 00:14:25.940
Maybe you wanna talk about some more of the things


00:14:25.940 --> 00:14:29.220
you learned from there, like what were


00:14:29.220 --> 00:14:30.720
some of the responses there?


00:14:30.720 --> 00:14:34.260
- So we got some really interesting ones, actually.


00:14:34.260 --> 00:14:38.380
If you wanna find the paper yourself,


00:14:38.380 --> 00:14:40.220
or if anyone's listening wants to find the paper,


00:14:40.220 --> 00:14:45.820
If you just Google surveying the developer experience of flaky tests,


00:14:45.820 --> 00:14:48.560
you should be able to find it.


00:14:48.560 --> 00:14:54.060
As I said, we also asked developers what they thought


00:14:54.060 --> 00:14:56.600
were the most common causes of flaky tests.


00:14:56.600 --> 00:15:06.740
And the cause that got the most votes was set up and tear down.


00:15:07.480 --> 00:15:15.800
So what I mean by that is flakiness being caused by a test case that either doesn't


00:15:15.800 --> 00:15:22.600
fully set up its executing environment or alternatively doesn't clean up after itself.


00:15:22.600 --> 00:15:26.660
And if it doesn't clean up after itself correctly, it could leave some global state behind that


00:15:26.660 --> 00:15:31.960
could then impact a later test case that's executed, if that makes sense.


00:15:31.960 --> 00:15:34.160
Yeah, absolutely.


00:15:34.160 --> 00:15:38.280
The cleanup one is especially tricky, right?


00:15:38.280 --> 00:15:41.920
We kind of know about setup because you're like, "Oh, well, we have to do this in order


00:15:41.920 --> 00:15:46.320
for this file to exist," or whatever.


00:15:46.320 --> 00:15:51.080
But the teardown part, that becomes really tricky because it could have knock-on effects


00:15:51.080 --> 00:15:58.120
for tests that either pass and they shouldn't or don't pass because it didn't start fresh,


00:15:58.120 --> 00:15:59.120
right?


00:15:59.120 --> 00:16:03.960
Well, this kind of leads us into a whole other type of flaky test called a test order dependent


00:16:03.960 --> 00:16:08.720
So when you have a test case that doesn't clean up after itself properly,


00:16:08.720 --> 00:16:15.220
then that can potentially mean that later tests that perhaps are targeting similar parts of the program


00:16:15.220 --> 00:16:17.460
where there might be some state involved,


00:16:17.460 --> 00:16:23.640
they might fail when they should pass or alternatively pass when they should fail


00:16:23.640 --> 00:16:27.140
just because the assumptions that were there when the developer wrote that test


00:16:27.140 --> 00:16:30.480
aren't being met anymore because something's changed by another test.


00:16:30.480 --> 00:16:37.480
So what that means is that if you just take an arbitrary test suite and randomize the order, shuffle it,


00:16:37.480 --> 00:16:42.480
for any large test suite, I can almost guarantee that some tests are going to fail and all you've done is change the order.


00:16:42.480 --> 00:16:47.480
And they're failing because somewhere a test isn't cleaning up after itself properly.


00:16:47.480 --> 00:16:52.480
Yeah, or cleaning up can mean different things, right?


00:16:52.480 --> 00:16:59.480
Cleaning up can mean we didn't change, we didn't take away that file we created or put back the file we deleted.


00:16:59.480 --> 00:17:01.800
the file we deleted as part of this test scenario


00:17:01.800 --> 00:17:02.640
we're working with.


00:17:02.640 --> 00:17:05.360
But it could also be, we're testing by talking


00:17:05.360 --> 00:17:07.440
to a database and we made an insert to it


00:17:07.440 --> 00:17:09.280
and didn't roll that back.


00:17:09.280 --> 00:17:13.520
Or maybe the most subtle, I guess there's two more levels


00:17:13.520 --> 00:17:16.720
here, one, you could have changed in memory state.


00:17:16.720 --> 00:17:17.560
- Yeah. - Right?


00:17:17.560 --> 00:17:19.360
You could have, like there's a shared variable


00:17:19.360 --> 00:17:21.280
which is probably the most common reason.


00:17:21.280 --> 00:17:24.260
Like some shared state of the process just isn't


00:17:24.260 --> 00:17:27.600
in its starting or expected position.


00:17:27.600 --> 00:17:29.280
But the fourth one, when I said, ah, there's three,


00:17:29.280 --> 00:17:31.120
but actually I think there's more,


00:17:31.120 --> 00:17:33.120
is you mocked out something.


00:17:33.120 --> 00:17:36.840
Like I've mocked out what datetime.now means.


00:17:36.840 --> 00:17:38.320
I forgot to put it back.


00:17:38.320 --> 00:17:40.840
So time has stopped or something like that, right?


00:17:40.840 --> 00:17:44.480
- Yeah, those are all really good examples


00:17:44.480 --> 00:17:48.280
of the flakiness that can appear when you have shared state


00:17:48.280 --> 00:17:52.160
and building on what both you and Owen just said,


00:17:52.160 --> 00:17:54.800
again, I think there's another trade-off here.


00:17:54.800 --> 00:17:58.920
One of the trade-offs is connected to the efficiency


00:17:58.920 --> 00:18:02.020
of the testing process versus the flakiness


00:18:02.020 --> 00:18:03.700
of the testing process.


00:18:03.700 --> 00:18:07.100
So if you do a really good job at clearing out state


00:18:07.100 --> 00:18:10.300
from your database or resetting state in the memory


00:18:10.300 --> 00:18:14.320
of your process, that may take a longer time,


00:18:14.320 --> 00:18:16.840
but potentially reduce the amount of flakiness


00:18:16.840 --> 00:18:18.940
that manifests in your tests.


00:18:18.940 --> 00:18:21.560
And then additionally, it's worth noting that


00:18:21.560 --> 00:18:25.140
when you have test suites with really good setup


00:18:25.140 --> 00:18:28.540
and really good tear down and cleaning mechanisms,


00:18:28.540 --> 00:18:31.380
Those are also more time consuming for us to write


00:18:31.380 --> 00:18:35.080
as developers, which may mean we're spending a lot of time


00:18:35.080 --> 00:18:38.940
investing in our test suite and perhaps slightly less time


00:18:38.940 --> 00:18:41.660
actually adding new features to our program.


00:18:41.660 --> 00:18:43.860
And so there's trade-offs both in terms of


00:18:43.860 --> 00:18:46.560
developer productivity and the efficiency


00:18:46.560 --> 00:18:47.960
of the testing process.


00:18:47.960 --> 00:18:53.500
- Yeah, those both matter.


00:18:53.500 --> 00:18:56.180
Which one matters more to you


00:18:56.180 --> 00:18:58.340
probably depends on your situation, right?


00:18:58.340 --> 00:19:02.100
If you're a small team and you need to move quick,


00:19:02.100 --> 00:19:05.500
the developer overhead is probably a serious hassle.


00:19:05.500 --> 00:19:10.420
But if you're a large team and you have 100,000 tests,


00:19:10.420 --> 00:19:14.180
you want to get answers today and not tomorrow from your test suite.


00:19:14.180 --> 00:19:18.060
The speed of execution probably matters more at that point.


00:19:18.060 --> 00:19:20.820
>> Yeah, I think that's absolutely the case.


00:19:20.820 --> 00:19:23.660
So there have been some situations where I have


00:19:23.660 --> 00:19:27.580
certain test cases that take a really long time to run.


00:19:27.580 --> 00:19:30.820
And so in pytest, I might set a marker


00:19:30.820 --> 00:19:35.020
and only run those test cases at certain points of time


00:19:35.020 --> 00:19:38.040
during development on my laptop,


00:19:38.040 --> 00:19:40.820
and then always run them inside of CI.


00:19:40.820 --> 00:19:42.900
And the nice thing about removing


00:19:42.900 --> 00:19:44.440
those long running test cases


00:19:44.440 --> 00:19:47.000
is that it can make the testing process faster,


00:19:47.000 --> 00:19:50.760
and I don't have to do my rigorous cleaning approach


00:19:50.760 --> 00:19:53.180
except when I am running them in CI.


00:19:53.180 --> 00:19:55.980
- Yeah, that's an interesting idea.


00:19:55.980 --> 00:20:00.140
maybe giving them tags and then coming up with a category of speed.


00:20:00.140 --> 00:20:04.740
I mean, I know I've heard of people doing like marking a test as slow,


00:20:04.740 --> 00:20:09.860
a set of tests as slow, but maybe, maybe that's not fine grained enough.


00:20:09.860 --> 00:20:14.740
Maybe doing something like, you know, fast, less than a second,


00:20:14.740 --> 00:20:16.620
less than five seconds, less than 10 seconds.


00:20:16.620 --> 00:20:20.220
So I'm willing to run all the ones that run in three seconds or less, you know,


00:20:20.220 --> 00:20:21.940
but not more than that. Right.


00:20:21.940 --> 00:20:24.700
So you could kind of scale it up more than just fast and slow.


00:20:25.740 --> 00:20:27.740
So on the topic of markers,


00:20:27.740 --> 00:20:31.980
there's several plugins for pytest


00:20:31.980 --> 00:20:34.380
that enable you to mark a test as flaky.


00:20:34.380 --> 00:20:37.420
-Okay. -Basically, what that then means


00:20:37.420 --> 00:20:41.720
is that if it fails, it'll retry it some number of times.


00:20:41.720 --> 00:20:44.560
And then if it passes at least once,


00:20:44.560 --> 00:20:46.560
it will call the whole thing a pass.


00:20:46.560 --> 00:20:50.740
So while that means that,


00:20:50.740 --> 00:20:53.440
yeah, you can just make your test suite pass,


00:20:54.940 --> 00:20:59.940
So for example, in the survey, we had one respondent tell us,


00:20:59.940 --> 00:21:03.540
"Sometimes a flaky test isn't always a bad thing


00:21:03.540 --> 00:21:07.040
because sometimes the fact that a test is non-deterministic


00:21:07.040 --> 00:21:12.540
is showing that part of the software is non-deterministic when it shouldn't be."


00:21:12.540 --> 00:21:18.140
So if you were to follow this methodology of just rerunning all your flaky tests


00:21:18.140 --> 00:21:23.140
and ignoring them, if they pass at least once, then you'd miss out on that


00:21:23.140 --> 00:21:25.940
because you wouldn't be notified that that test failed.


00:21:25.940 --> 00:21:29.020
- Yeah, that's a good point.


00:21:29.020 --> 00:21:34.020
Maybe it's highlighting a weakness in your infrastructure,


00:21:34.020 --> 00:21:36.020
your DevOps story.


00:21:36.020 --> 00:21:39.020
And you could say, well, that's out of my control.


00:21:39.020 --> 00:21:39.860
It's not my problem.


00:21:39.860 --> 00:21:41.780
Or you could say, actually folks, look,


00:21:41.780 --> 00:21:43.540
this is pointing out,


00:21:43.540 --> 00:21:48.300
this is the least stable pillar of our uptime


00:21:48.300 --> 00:21:49.580
for our app, right?


00:21:49.580 --> 00:21:50.540
- Yeah.


00:21:50.540 --> 00:21:51.680
- Yeah, that's a good point.


00:21:51.680 --> 00:21:54.160
The other thing, since we're talking about randomness,


00:21:54.160 --> 00:21:56.280
that's important to discuss


00:21:56.280 --> 00:21:59.100
is the use of property-based testing tools.


00:21:59.100 --> 00:22:02.380
So for example, I frequently use hypothesis


00:22:02.380 --> 00:22:06.440
in order to automatically generate inputs


00:22:06.440 --> 00:22:09.920
and then send them into my function under test.


00:22:09.920 --> 00:22:14.920
And there may be cases where hypothesis reveals a bug,


00:22:14.920 --> 00:22:18.600
and that could in fact actually be a bug in my program,


00:22:18.600 --> 00:22:21.520
even though I've run exactly that same test case


00:22:21.520 --> 00:22:25.720
frequently in the past, and it just happens to be the case


00:22:25.720 --> 00:22:29.280
that in that run, when I was using a hypothesis


00:22:29.280 --> 00:22:32.600
property-based test, it was able to find


00:22:32.600 --> 00:22:33.940
a potential problem.


00:22:33.940 --> 00:22:37.040
So in that situation, even though that test didn't fail


00:22:37.040 --> 00:22:41.320
the last three times, this could still be a silver lining


00:22:41.320 --> 00:22:44.620
to suggest that there is a problem with my program


00:22:44.620 --> 00:22:47.080
and I need to resolve it because hypothesis


00:22:47.080 --> 00:22:49.820
has randomly generated an input


00:22:49.820 --> 00:22:51.420
that I haven't seen previously.


00:22:51.420 --> 00:22:54.260
- Yeah, the hypothesis story is interesting.


00:22:54.260 --> 00:22:55.860
I was thinking about that as well


00:22:55.860 --> 00:23:00.180
after reading some of the work that you're doing here.


00:23:00.180 --> 00:23:04.500
Thinking things like hypothesis and parameterized testing


00:23:04.500 --> 00:23:06.860
and those sorts of things where they just,


00:23:06.860 --> 00:23:09.020
they naturally take some test scenario


00:23:09.020 --> 00:23:11.500
and run it over and over with a bunch of different inputs.


00:23:11.500 --> 00:23:14.980
Probably uncovers this better than one-off tests, I imagine.


00:23:14.980 --> 00:23:18.460
- And hypothesis also has a mode


00:23:18.460 --> 00:23:22.700
that lets you run a long running fuzz testing campaign.


00:23:22.700 --> 00:23:25.880
And in those situations, it doesn't constrain its execution


00:23:25.880 --> 00:23:28.140
by a specific period of time.


00:23:28.140 --> 00:23:31.180
And I have found that when I let a fuzzing campaign go


00:23:31.180 --> 00:23:32.820
for a given function,


00:23:32.820 --> 00:23:36.200
maybe I've described its inputs with a JSON schema,


00:23:36.200 --> 00:23:40.000
and I'm using the hypothesis JSON schema plugin,


00:23:40.000 --> 00:23:43.020
I might not find a bug for a long period of time.


00:23:43.020 --> 00:23:45.220
And then suddenly a bug will crop up.


00:23:45.220 --> 00:23:47.900
And I often use that as an opportunity


00:23:47.900 --> 00:23:50.540
to rethink my assumptions about the function


00:23:50.540 --> 00:23:51.420
that I'm testing.


00:23:51.420 --> 00:23:55.060
- So you said fuzzing.


00:23:55.060 --> 00:23:56.900
Tell people out there, give people a definition


00:23:56.900 --> 00:23:59.100
that they're familiar with fuzzing.


00:23:59.100 --> 00:24:03.460
- So when I think of fuzzing, I think of it as a process


00:24:03.460 --> 00:24:06.980
where you're randomly generating inputs


00:24:06.980 --> 00:24:10.260
that you're going to send to the function under test,


00:24:10.260 --> 00:24:11.940
and you're frequently doing that


00:24:11.940 --> 00:24:14.240
in what I would call a campaign,


00:24:14.240 --> 00:24:17.800
which means that the input generation process


00:24:17.800 --> 00:24:22.280
is going to range perhaps for an extended period of time.


00:24:22.280 --> 00:24:23.720
And you may have different goals


00:24:23.720 --> 00:24:25.720
during that fuzzing campaign,


00:24:25.720 --> 00:24:28.940
like covering more of the program under test,


00:24:28.940 --> 00:24:33.480
or attempting to realize as many crash-inducing inputs


00:24:33.480 --> 00:24:34.580
as is possible.


00:24:34.580 --> 00:24:38.840
- Yeah, that's such a cool idea.


00:24:38.840 --> 00:24:42.000
And that's sort of how hypothesis works.


00:24:42.000 --> 00:24:45.720
Although I don't know it's really meant for fuzzing


00:24:45.720 --> 00:24:50.520
in the sense of just we're going to hit it with a whole bunch of stuff at extreme scale


00:24:50.520 --> 00:24:51.980
until it breaks.


00:24:51.980 --> 00:24:56.280
But it certainly is meant to run it a lot of times with different inputs.


00:24:56.280 --> 00:25:03.440
So yeah, it's sort of the effect, if not necessarily the intent of it.


00:25:03.440 --> 00:25:07.760
Here's another one from the audience that also is one that I hadn't thought about from


00:25:07.760 --> 00:25:08.760
Marwan.


00:25:08.760 --> 00:25:12.940
It says, "Sometimes flakiness shows up as conflicts to access a shared resource when


00:25:12.940 --> 00:25:17.940
and more than one CI pipeline is running for the same code.


00:25:17.940 --> 00:25:19.300
That's pretty wild.


00:25:19.300 --> 00:25:20.300
I hadn't really thought about that, right?


00:25:20.300 --> 00:25:23.460
But if you have a lock on something.


00:25:23.460 --> 00:25:25.580
- Resource availability in general


00:25:25.580 --> 00:25:27.660
is quite a common cause of flakiness.


00:25:27.660 --> 00:25:31.820
So that might be, so this resource could be a file system


00:25:31.820 --> 00:25:34.080
or a database or anything really.


00:25:34.080 --> 00:25:38.980
Or even something, even if we're not talking about CI,


00:25:38.980 --> 00:25:41.380
just on a local machine, you've got, like you said,


00:25:41.380 --> 00:25:44.420
locks and things that aren't supposed to be shared


00:25:44.420 --> 00:25:47.220
between multiple processes or whatever.


00:25:47.220 --> 00:25:49.740
So yeah, that is a relatively common cause


00:25:49.740 --> 00:25:53.020
that I've seen in the programs that I've tested.


00:25:53.020 --> 00:25:54.080
- Sure.


00:25:54.080 --> 00:25:56.980
Yeah, the thing about that that stands out to me is


00:25:56.980 --> 00:26:01.020
you might have an assumption that only one process


00:26:01.020 --> 00:26:05.720
of your app is ever gonna be running on a server at a time.


00:26:05.720 --> 00:26:10.820
And yet somehow, because there were multiple Git commits


00:26:10.820 --> 00:26:12.820
that you weren't even aware of,


00:26:12.820 --> 00:26:14.500
now they're running kind of in parallel.


00:26:14.500 --> 00:26:18.440
So you're in this situation that you never saw coming


00:26:18.440 --> 00:26:21.900
because you just don't run your app that way.


00:26:21.900 --> 00:26:22.980
You know what I mean?


00:26:22.980 --> 00:26:26.220
- Yeah, this is actually a really good example


00:26:26.220 --> 00:26:29.680
of when a flaky test is again a silver lining


00:26:29.680 --> 00:26:32.940
because it forces you to question your assumptions


00:26:32.940 --> 00:26:37.060
about how your program will run and when it will run


00:26:37.060 --> 00:26:41.060
and how much of resources your program is going to consume.


00:26:41.060 --> 00:26:44.780
So in the situation when I never thought about my program


00:26:44.780 --> 00:26:47.700
running in multiple instances at the same time,


00:26:47.700 --> 00:26:49.660
if my tests become flaky,


00:26:49.660 --> 00:26:53.140
that may actually open up a whole new opportunity


00:26:53.140 --> 00:26:55.660
for me to refactor and improve my program.


00:26:55.660 --> 00:26:58.780
- Yeah, yeah, that's right.


00:26:58.780 --> 00:26:59.980
You're like, wait a minute,


00:26:59.980 --> 00:27:01.500
I didn't realize this was a problem,


00:27:01.500 --> 00:27:03.360
but yes, maybe it is.


00:27:03.360 --> 00:27:06.500
Let's see.


00:27:06.500 --> 00:27:10.220
So let's talk a little bit about some articles


00:27:10.220 --> 00:27:14.540
that a couple of the big tech companies wrote here.


00:27:14.540 --> 00:27:18.100
So both Google and Spotify talked about


00:27:18.100 --> 00:27:22.220
how they're using, how they're experiencing flaky tests


00:27:22.220 --> 00:27:26.940
and what they're doing to either reduce them


00:27:26.940 --> 00:27:28.760
or maybe as you put it Gregory,


00:27:28.760 --> 00:27:31.940
some of the silver linings that they're finding in it


00:27:31.940 --> 00:27:33.420
and some of the tools that are building


00:27:33.420 --> 00:27:34.940
to help deal with it.


00:27:34.940 --> 00:27:37.400
So over at Google, they say they're running, obviously,


00:27:37.400 --> 00:27:39.280
a large set of tests.


00:27:39.280 --> 00:27:42.160
I could probably-- like, that is a massive understatement,


00:27:42.160 --> 00:27:42.720
I imagine.


00:27:42.720 --> 00:27:45.680
But it says they see a continual rate


00:27:45.680 --> 00:27:51.960
of flakiness of about 1.5% on all test cases, which for them,


00:27:51.960 --> 00:27:56.840
I imagine, is a lot of tests.


00:27:56.840 --> 00:28:01.360
And so you want to talk a little bit about this, either you


00:28:01.360 --> 00:28:05.720
you guys and maybe some of the mitigation strategies they have?


00:28:05.720 --> 00:28:12.560
Well, so from a developer perspective,


00:28:12.560 --> 00:28:17.880
so this article and others as well point out an interesting side of flakiness


00:28:17.880 --> 00:28:21.280
that when you're talking from a purely technical perspective,


00:28:21.280 --> 00:28:22.680
you don't really consider.


00:28:22.680 --> 00:28:27.720
And that's the psychological impact of them.


00:28:27.720 --> 00:28:30.240
So it's a little bit like the boy who cried wolf.


00:28:30.240 --> 00:28:33.380
So if you have a test case that's known to be flaky,


00:28:33.380 --> 00:28:39.680
you might be tempted to just put some marker on it


00:28:39.680 --> 00:28:43.280
that says ignore it or it's an expected fail or whatever.


00:28:43.280 --> 00:28:47.420
But then suppose it fails for real sometime


00:28:47.420 --> 00:28:50.620
and you're ignoring it or you have it quarantined or something,


00:28:50.620 --> 00:28:54.460
then that means you're missing out on real bugs.


00:28:54.460 --> 00:28:58.660
So as well as just being a hindrance to CI


00:28:58.660 --> 00:29:01.460
and that kind of thing.


00:29:01.460 --> 00:29:07.980
It can almost make a developer team lose the discipline to properly investigate every test


00:29:07.980 --> 00:29:08.980
failure.


00:29:08.980 --> 00:29:13.100
If they've got a test suite that's known to be full of flaky tests, then naturally you're


00:29:13.100 --> 00:29:16.300
not going to trust it as much.


00:29:16.300 --> 00:29:21.500
So that's probably one of the biggest problems that flaky tests cause, in my opinion.


00:29:21.500 --> 00:29:26.500
I think the mental aspect of it, how much do I trust it?


00:29:26.500 --> 00:29:29.160
Do I have faith in our test suite?


00:29:29.160 --> 00:29:34.160
Do I have faith in the continuous deployment capabilities


00:29:34.160 --> 00:29:37.320
of our pipelines and things like that?


00:29:37.320 --> 00:29:40.260
I think that's pretty serious.


00:29:40.260 --> 00:29:42.280
There's already a bit of a challenge, I think,


00:29:42.280 --> 00:29:45.380
on teams to have complete buy-in


00:29:45.380 --> 00:29:50.280
on making sure the software is self-evaluating.


00:29:50.280 --> 00:29:52.920
Some people will check in code that breaks the build,


00:29:52.920 --> 00:29:54.780
but they're kind of like, "YOLO, whatever."


00:29:54.780 --> 00:29:57.740
Other people really, somehow, they really want the build


00:29:57.740 --> 00:30:00.780
to work, so it's their job to kind of chase that person down


00:30:00.780 --> 00:30:01.660
and make them fix it.


00:30:01.660 --> 00:30:03.500
And it's always kind of a bit of a struggle.


00:30:03.500 --> 00:30:08.140
But that's when the tests are awesome, right?


00:30:08.140 --> 00:30:10.500
It just changes to the code, makes it--


00:30:10.500 --> 00:30:12.500
sort of adds these breaking builds.


00:30:12.500 --> 00:30:17.140
But if the code is flaky, all of a sudden,


00:30:17.140 --> 00:30:19.260
you can start to see CI as an annoyance,


00:30:19.260 --> 00:30:20.900
because it tells you something's wrong.


00:30:20.900 --> 00:30:22.400
You're like, I know nothing's wrong.


00:30:22.400 --> 00:30:24.700
It's just-- it'll go away.


00:30:24.700 --> 00:30:34.700
So maybe speak to the psychological bit of how flakiness can maybe degrade people's caring about tests at all.


00:30:34.700 --> 00:30:44.700
I would say that overall developers have the risk of losing a confidence in two types of correctness.


00:30:44.700 --> 00:30:52.700
First of all, flaky test cases may cause developers to begin to mistrust and lose confidence in the test suite.


00:30:52.700 --> 00:30:59.700
suite, then they also may lose confidence in the overall correctness of their program.


00:30:59.700 --> 00:31:06.380
And that then may cause them to stop running test cases, which then reduces test quality


00:31:06.380 --> 00:31:11.060
and maybe even also reduces the quality of the program under test as well.


00:31:11.060 --> 00:31:18.380
So I think regrettably, it's a negative reinforcing cycle where you start to mistrust your tests


00:31:18.380 --> 00:31:19.880
so you don't run them.


00:31:19.880 --> 00:31:21.820
But then you start to lose confidence


00:31:21.820 --> 00:31:24.060
in the correctness of your program.


00:31:24.060 --> 00:31:26.340
And now you're not sure what to do


00:31:26.340 --> 00:31:29.540
because tests are failing for spurious reasons.


00:31:29.540 --> 00:31:32.380
You disable them, but then as Owen mentioned previously,


00:31:32.380 --> 00:31:34.820
you lose the opportunity to get the feedback


00:31:34.820 --> 00:31:35.860
from those tests.


00:31:35.860 --> 00:31:39.680
- Right, it goes both ways, right?


00:31:39.680 --> 00:31:43.820
You don't feel like if it says it's broken,


00:31:43.820 --> 00:31:45.060
it provides you much value,


00:31:45.060 --> 00:31:47.820
'cause it might report broken even though it's working.


00:31:47.820 --> 00:31:53.360
But on the flip side, if you were doing, you know, continuous deployment, and by that I


00:31:53.360 --> 00:31:57.820
mean I check into a branch, that branch noticed the change, automatically it rolls out the


00:31:57.820 --> 00:31:59.820
new version, right?


00:31:59.820 --> 00:32:04.620
Maybe you merge over to a production branch and it just, it takes off.


00:32:04.620 --> 00:32:10.600
The gate to making that not go to production is the CI system that's going to say whether


00:32:10.600 --> 00:32:12.500
or not the tests pass.


00:32:12.500 --> 00:32:19.700
But if the test pass in, maybe they shouldn't have because you've got this flakiness.


00:32:19.700 --> 00:32:23.100
Well, that's also not good.


00:32:23.100 --> 00:32:24.100
>> Yeah.


00:32:24.100 --> 00:32:28.500
So that's a situation when you could have just deployed software that wasn't working.


00:32:28.500 --> 00:32:33.500
And then the flip side of that is you have a flaky build and you want to be able to release


00:32:33.500 --> 00:32:39.280
quickly but because test cases are failing, you don't release your system.


00:32:39.280 --> 00:32:45.040
And so it can really be a hindrance to being able to quickly push your work to production


00:32:45.040 --> 00:32:50.780
because you frequently have flaky test cases that are causing you to limit the velocity


00:32:50.780 --> 00:32:52.600
of your development process.


00:32:52.600 --> 00:32:54.240
Yeah, absolutely.


00:32:54.240 --> 00:32:59.000
Owen, you got thoughts on that?


00:32:59.000 --> 00:33:01.240
I think Greg's pretty much covered that pretty well.


00:33:01.240 --> 00:33:02.800
Yeah, yeah.


00:33:02.800 --> 00:33:03.800
Excellent.


00:33:03.800 --> 00:33:12.800
The two things, two takeaways from the Google article is one, they talked about mitigation strategies.


00:33:12.800 --> 00:33:16.800
They said they have a tool that monitors the flakiness of tests.


00:33:16.800 --> 00:33:20.800
And if the flakiness is too high, it automatically quarantines the test.


00:33:20.800 --> 00:33:24.800
Takes it out of the critical path, takes it out of CI.


00:33:24.800 --> 00:33:29.800
Maybe somebody notices like, hey, there's a new flaky test.


00:33:29.800 --> 00:33:32.800
We need to go find the root cause of that.


00:33:32.800 --> 00:33:34.680
- That's a pretty interesting idea, isn't it?


00:33:34.680 --> 00:33:39.400
Some sort of automation or maybe not quite totally automatic


00:33:39.400 --> 00:33:42.280
but some kind of tool that you can run that'll say,


00:33:42.280 --> 00:33:45.160
this thing has reached a point where maybe


00:33:45.160 --> 00:33:48.480
its value in the test suite is degraded


00:33:48.480 --> 00:33:50.800
because it's so flaky that we need to either fix it


00:33:50.800 --> 00:33:51.720
or just delete it.


00:33:51.720 --> 00:33:56.000
- I think the problem with quarantining tests is


00:33:56.000 --> 00:33:59.000
it only works if the development team


00:33:59.840 --> 00:34:02.900
is serious about investigating them.


00:34:02.900 --> 00:34:05.080
Otherwise, what could end up happening is


00:34:05.080 --> 00:34:08.520
quarantining becomes effectively equivalent to just deleting it.


00:34:08.520 --> 00:34:10.780
If they all end up in a special flaky bucket


00:34:10.780 --> 00:34:12.480
and no one looks at them again,


00:34:12.480 --> 00:34:15.680
then the whole point of the process is kind of moot, really.


00:34:15.680 --> 00:34:19.760
So doing something like that, I think, can be really useful


00:34:19.760 --> 00:34:24.700
if the developers are willing to actually investigate


00:34:24.700 --> 00:34:26.700
why these tests are flaky.


00:34:26.700 --> 00:34:28.500
Yeah, that's true.


00:34:28.500 --> 00:34:32.180
If it becomes just a black box, basically a trash can for tests,


00:34:32.180 --> 00:34:34.100
then what's the point, right?


00:34:34.100 --> 00:34:35.740
Exactly, yeah.


00:34:35.740 --> 00:34:38.400
It kind of goes back to my talking about there's some people on the team


00:34:38.400 --> 00:34:42.900
that really care about the build and continuous integration and all this,


00:34:42.900 --> 00:34:44.580
and other people just don't.


00:34:44.580 --> 00:34:50.020
So it does come back to the team mentality and people really


00:34:50.020 --> 00:34:51.180
caring about these things.


00:34:51.180 --> 00:34:57.620
But it's a cool idea, at least in the optimistic point of view,


00:34:57.620 --> 00:35:01.000
where assuming everyone wants to make sure


00:35:01.000 --> 00:35:03.100
these keep working and someone's gonna pay attention


00:35:03.100 --> 00:35:04.820
to this and so on.


00:35:04.820 --> 00:35:05.860
- Yeah, it's a difficult one,


00:35:05.860 --> 00:35:08.580
'cause I mean, sometimes you can just write a bad test


00:35:08.580 --> 00:35:11.860
and that test is flaky purely because it's a bad test.


00:35:11.860 --> 00:35:14.700
But other times you can write a good test


00:35:14.700 --> 00:35:18.180
that's flaky because there's a problem.


00:35:18.180 --> 00:35:21.140
Like I said before, we had one developer say


00:35:21.140 --> 00:35:22.840
that sometimes a flaky test implies


00:35:22.840 --> 00:35:26.020
that a part of the program they thought was deterministic


00:35:26.020 --> 00:35:27.200
was actually non-deterministic.


00:35:27.200 --> 00:35:32.200
So you're potentially throwing away useful information


00:35:32.200 --> 00:35:35.560
as well as potentially throwing away


00:35:35.560 --> 00:35:37.680
just poorly written tests.


00:35:37.680 --> 00:35:38.920
- Right, right.


00:35:38.920 --> 00:35:41.600
- And it's hard to distinguish between those two.


00:35:41.600 --> 00:35:42.440
- I'm sure that it is.


00:35:42.440 --> 00:35:45.600
Yeah, I mean, identifying these,


00:35:45.600 --> 00:35:47.320
maybe not quarantine them,


00:35:47.320 --> 00:35:50.440
but identifying them is pretty valuable, I would think.


00:35:50.440 --> 00:35:51.280
- Yeah.


00:35:51.280 --> 00:35:53.000
- And then you can see what lessons come from that, right?


00:35:53.000 --> 00:35:54.520
What you do once you've identified it,


00:35:54.520 --> 00:35:56.980
I think that is up for debate, right?


00:35:56.980 --> 00:35:57.820
- Yeah.


00:35:57.820 --> 00:36:02.100
- Okay, the other one is Test Flakiness,


00:36:02.100 --> 00:36:04.780
Methods for Identifying and Dealing with Flaky Tests


00:36:04.780 --> 00:36:08.820
by Jason Palmer from Spotify, which is also cool.


00:36:08.820 --> 00:36:11.060
This one has pictures, which is fun.


00:36:11.060 --> 00:36:14.340
They've got like a graphical analysis of their tests


00:36:14.340 --> 00:36:17.580
and the flakiness of it and so on.


00:36:17.580 --> 00:36:22.580
So they came up with a thing called Flaky Bot.


00:36:23.620 --> 00:36:28.620
And it's a GitHub integration, GitHub bot


00:36:28.620 --> 00:36:35.220
that they can make it run, and they can ask it


00:36:35.220 --> 00:36:38.700
to exercise the test really quickly and see if it's flaky.


00:36:38.700 --> 00:36:41.140
And I got the sense that it does that by just running


00:36:41.140 --> 00:36:43.660
it a bunch of different times with different delays


00:36:43.660 --> 00:36:45.440
and seeing if it always passes


00:36:45.440 --> 00:36:48.300
or if it potentially sometimes passes or fails.


00:36:48.300 --> 00:36:52.500
- So I think broadly, one of the things


00:36:52.500 --> 00:36:55.220
that is mentioned in this article,


00:36:55.220 --> 00:36:58.180
and something that's done by a number of pytest plugins


00:36:58.180 --> 00:37:01.460
as well, is rerunning the test suite.


00:37:01.460 --> 00:37:05.460
And so you could imagine rerunning each test case


00:37:05.460 --> 00:37:07.060
in isolation.


00:37:07.060 --> 00:37:11.400
You could also imagine picking a group of test cases


00:37:11.400 --> 00:37:14.900
and then rerunning the test cases in that group,


00:37:14.900 --> 00:37:19.180
either in a random order or in certain fixed orders.


00:37:19.180 --> 00:37:25.340
So rerunning is often a very helpful way for us to detect flaky test cases,


00:37:25.340 --> 00:37:30.660
whether we rerun the whole test suite, whether we run test cases individually,


00:37:30.660 --> 00:37:34.020
or whether we run test cases in groups.


00:37:34.020 --> 00:37:39.140
Obviously, one of the clear downsides associated with rerunning a test suite


00:37:39.140 --> 00:37:43.300
is the execution time associated with the rerunning process.


00:37:43.300 --> 00:37:46.420
- Right, yeah.


00:37:46.420 --> 00:37:51.580
The more you run it, the more likely you're able to detect flakiness.


00:37:51.580 --> 00:37:58.120
If it's only a little bit flaky, but at the same time, the longer that goes, the longer


00:37:58.120 --> 00:37:59.620
it takes, that's also a problem.


00:37:59.620 --> 00:38:05.420
There's another thing in here that I thought was pretty interesting, but I'm struggling


00:38:05.420 --> 00:38:08.140
to find it in this article for the second.


00:38:08.140 --> 00:38:10.140
Integration, no?


00:38:10.140 --> 00:38:11.140
I thought...


00:38:11.140 --> 00:38:14.940
Oh, end to end, maybe?


00:38:14.940 --> 00:38:15.940
Yeah.


00:38:15.940 --> 00:38:45.580
So in the Spotify article, they say that their assessment that end to end tests are flaky by nature, right fewer of them. So I get the sense I don't I get the sense maybe all sort of feel this way as well. But I don't necessarily agree with that. I think end to end tests, if they are flaky, that's telling you something about your program.


00:38:45.660 --> 00:38:50.060
it might not be really precisely narrowing in on it, but it's telling you something about


00:38:50.060 --> 00:38:55.100
your program if you can write end-to-end tests that are flaky. What do you think?


00:38:55.100 --> 00:38:58.540
I think with end-to-end tests, I mean,


00:38:58.540 --> 00:39:07.100
sort of saying they're flaky by nature, maybe a little strong, but they're certainly more


00:39:07.100 --> 00:39:13.420
susceptible to flakiness purely because there's a hell of a lot more going on. So I think,


00:39:14.300 --> 00:39:18.180
When we talk about this flakiness and precision trade-off,


00:39:18.180 --> 00:39:19.380
I think with end-to-end tests,


00:39:19.380 --> 00:39:21.700
you should be a little bit more forgiving with flakiness,


00:39:21.700 --> 00:39:24.380
purely because there's more going on.


00:39:24.380 --> 00:39:29.100
For example, for a unit test,


00:39:29.100 --> 00:39:31.060
you shouldn't really accept any flakiness because that's


00:39:31.060 --> 00:39:35.460
a very focused test case.


00:39:35.460 --> 00:39:38.820
Those are my thoughts on that.


00:39:38.820 --> 00:39:42.300
>> Okay. I would agree with what Owen said.


00:39:42.300 --> 00:39:47.900
I still think there is quite a bit of value in end-to-end or integration testing,


00:39:47.900 --> 00:39:54.100
because from my perspective, it's increasing the realism of the testing process.


00:39:54.100 --> 00:39:59.300
So I still write end-to-end test cases if I'm building a web API,


00:39:59.300 --> 00:40:02.400
or even if I'm building a command line application.


00:40:02.400 --> 00:40:06.500
But I think I have to be willing to tolerate a little bit more flakiness,


00:40:06.500 --> 00:40:12.100
and perhaps even be creative with the various strategies that I adopt


00:40:12.100 --> 00:40:13.860
when I do rerunning.


00:40:13.860 --> 00:40:17.180
Maybe I need to run some of my integration tests


00:40:17.180 --> 00:40:19.820
with really good setup and teardown


00:40:19.820 --> 00:40:22.580
to avoid pollution between test cases.


00:40:22.580 --> 00:40:25.340
Or maybe certain integration test cases


00:40:25.340 --> 00:40:28.380
have to be run completely in isolation


00:40:28.380 --> 00:40:30.820
and they can't be run while any other part


00:40:30.820 --> 00:40:32.860
of the program is being used.


00:40:32.860 --> 00:40:35.860
So in those cases, maybe my integration tests


00:40:35.860 --> 00:40:39.300
are run less frequently, but I still keep them


00:40:39.300 --> 00:40:41.500
as a part of my pytest test suite.


00:40:41.500 --> 00:40:44.620
- Yeah, yeah, interesting, both of you.


00:40:44.620 --> 00:40:47.580
For me, one of the things I do that I think


00:40:47.580 --> 00:40:50.060
is really valuable is over at Talk Python,


00:40:50.060 --> 00:40:52.540
we have the courses and the web app


00:40:52.540 --> 00:40:55.060
that serves up the courses and lets people buy them


00:40:55.060 --> 00:40:55.900
and all that sort of stuff.


00:40:55.900 --> 00:40:58.420
It's like 20,000 lines of Python, maybe more.


00:40:58.420 --> 00:41:00.020
These days, I haven't measured it for a long time,


00:41:00.020 --> 00:41:02.540
but it's a non-trivial amount.


00:41:02.540 --> 00:41:07.540
And it's got a site map of all the pages on there.


00:41:07.540 --> 00:41:09.240
And one of the things I do for the test


00:41:09.240 --> 00:41:14.000
is just go and find every, you know, pull the sitemap,


00:41:14.000 --> 00:41:16.160
look at every page on the site and just request it


00:41:16.160 --> 00:41:21.160
and make sure it doesn't 500 out or 404 or things like that.


00:41:21.160 --> 00:41:24.500
And it just, they all work, right?


00:41:24.500 --> 00:41:27.800
Now there's like 6,000 links in the sitemap.


00:41:27.800 --> 00:41:32.600
So it says, well, these 500 are all really the same thing


00:41:32.600 --> 00:41:34.440
with just different data behind it.


00:41:34.440 --> 00:41:35.760
So just pick one of those.


00:41:35.760 --> 00:41:37.880
There's a way to kind of winnow it down to, you know,


00:41:37.880 --> 00:41:40.960
20 requests and not 6,000.


00:41:40.960 --> 00:41:47.640
But that kind of stuff, there should be no time


00:41:47.640 --> 00:41:51.760
where there is a 404 on your site.


00:41:51.760 --> 00:41:55.800
It's not an inherent flakiness of testing


00:41:55.800 --> 00:41:56.880
that there's not a 404.


00:41:56.880 --> 00:41:58.360
There should not be a 404.


00:41:58.360 --> 00:42:00.480
Same thing, there should not be a 500,


00:42:00.480 --> 00:42:03.360
my website, my server crashed.


00:42:03.360 --> 00:42:05.680
It should never crash.


00:42:05.680 --> 00:42:07.360
And so those types of integration tests,


00:42:07.360 --> 00:42:10.840
I think they still add a lot of value, right?


00:42:10.840 --> 00:42:12.400
'Cause you could miss something like,


00:42:12.400 --> 00:42:15.600
well, I checked the database models, they were fine.


00:42:15.600 --> 00:42:16.600
I checked the code that works


00:42:16.600 --> 00:42:17.760
with the database models are fine,


00:42:17.760 --> 00:42:21.140
but the HTML assumed there was a field


00:42:21.140 --> 00:42:23.140
in the database model we passed to it.


00:42:23.140 --> 00:42:25.000
That wasn't, you know, there is value


00:42:25.000 --> 00:42:26.360
in these sort of holistic,


00:42:26.360 --> 00:42:30.440
like does it still click together story, I think.


00:42:30.440 --> 00:42:33.000
- No, I think you're making a really good point, Michael.


00:42:33.000 --> 00:42:35.760
And so what I often do, like say for example,


00:42:35.760 --> 00:42:39.680
I'm adding a new feature or I'm adding a bug fix.


00:42:39.680 --> 00:42:42.520
What I'm going to regularly do to be confident


00:42:42.520 --> 00:42:47.520
in my changes to the system is run my integration tests,


00:42:47.520 --> 00:42:49.700
maybe every once in a while,


00:42:49.700 --> 00:42:52.400
and run my unit tests very frequently


00:42:52.400 --> 00:42:54.820
during the refactoring process.


00:42:54.820 --> 00:42:56.400
I can't run them all the time


00:42:56.400 --> 00:42:59.560
because they regrettably take too long to run,


00:42:59.560 --> 00:43:03.760
but I can run my integration test cases very frequently


00:43:03.760 --> 00:43:06.320
when I'm adding a bug fix or adding a new feature.


00:43:06.320 --> 00:43:08.120
And then every once in a while,


00:43:08.120 --> 00:43:10.960
do the kind of smoke tests that you mentioned


00:43:10.960 --> 00:43:13.560
and then the integration or end-to-end testing


00:43:13.560 --> 00:43:14.900
that we've been discussing,


00:43:14.900 --> 00:43:18.680
so that ultimately I have rapid feedback


00:43:18.680 --> 00:43:20.360
that gives me confidence.


00:43:20.360 --> 00:43:23.240
And then additionally, I have longer running tests


00:43:23.240 --> 00:43:26.320
that further give me confidence that my program is working.


00:43:26.320 --> 00:43:29.280
- Yeah, yeah, that's a really good point.


00:43:29.280 --> 00:43:31.700
And maybe you don't even run all your unit tests


00:43:31.700 --> 00:43:32.840
as you're writing that feature.


00:43:32.840 --> 00:43:35.900
Maybe you run a group of them that you think are related.


00:43:35.900 --> 00:43:38.320
- Yeah, so you're bringing up something


00:43:38.320 --> 00:43:41.440
that I really love about the Python ecosystem,


00:43:41.440 --> 00:43:44.560
which is the awesome coverage.py


00:43:44.560 --> 00:43:47.260
and the pytest-cov plugin.


00:43:47.260 --> 00:43:49.640
Those plugins are really good.


00:43:49.640 --> 00:43:51.920
And what's awesome about coverage.py


00:43:51.920 --> 00:43:56.280
is that it can track code coverage on a per test case basis.


00:43:56.280 --> 00:43:59.000
So one of the things that I will often do


00:43:59.000 --> 00:44:03.080
is look at what test cases cover what part of the system.


00:44:03.080 --> 00:44:06.380
And as you mentioned, I'll only run those test cases


00:44:06.380 --> 00:44:09.540
that are covering the parts of the system that I'm changing


00:44:09.540 --> 00:44:12.440
because that helps me to get very rapid feedback


00:44:12.440 --> 00:44:15.900
from my unit test cases while I'm adding a new feature


00:44:15.900 --> 00:44:18.460
to a certain part of my program.


00:44:18.460 --> 00:44:20.480
- Yeah, I didn't realize that


00:44:20.480 --> 00:44:24.800
Coverage.py would tell you that in reverse.


00:44:24.800 --> 00:44:27.980
Like for this part of your program,


00:44:27.980 --> 00:44:30.420
these are the five tests.


00:44:30.420 --> 00:44:32.260
That's really cool.


00:44:32.260 --> 00:44:34.020
- Yeah, so I really like that feature.


00:44:34.020 --> 00:44:37.740
I think it was released in coverage pi 5.0,


00:44:37.740 --> 00:44:41.220
and I've been using it since the feature was available.


00:44:41.220 --> 00:44:43.800
It's incredibly helpful because of the fact


00:44:43.800 --> 00:44:48.340
that you can look at specific statements in your code


00:44:48.340 --> 00:44:51.860
and then find out which test cases cover those statements


00:44:51.860 --> 00:44:55.520
and then choose to rerun those specific tests


00:44:55.520 --> 00:44:58.800
when you're repeatedly running your test suite.


00:44:58.800 --> 00:45:01.600
And I call that test suite reduction,


00:45:01.600 --> 00:45:04.200
or coverage-based test suite reduction.


00:45:04.200 --> 00:45:09.200
And having what CoveragePi calls the context of coverage


00:45:09.200 --> 00:45:12.360
is, in my experience, very, very helpful.


00:45:12.360 --> 00:45:14.920
- Yeah, the bigger your test suite is,


00:45:14.920 --> 00:45:16.200
the more helpful it is, right?


00:45:16.200 --> 00:45:17.880
- Mm-hmm, absolutely.


00:45:17.880 --> 00:45:18.720
- Yeah.


00:45:18.720 --> 00:45:21.960
Oh, and did you find that a lot of people


00:45:21.960 --> 00:45:29.880
using those kinds of tools to sort of limit the amount of tests they got to run?


00:45:29.880 --> 00:45:37.600
I mean, with the sort of programs I was working with, so for the purposes of my experiments,


00:45:37.600 --> 00:45:42.160
I was running the whole test suite in its entirety, multiple times to find flaky tests,


00:45:42.160 --> 00:45:48.720
but I did see evidence of that kind of thing being set up. So I think it is fairly well


00:45:48.720 --> 00:45:54.560
as opted, once again, it's a lot more relevant to very large projects as opposed to small


00:45:54.560 --> 00:45:59.280
projects where if it only takes you 10 seconds to run the whole test suite, obviously there's


00:45:59.280 --> 00:46:00.640
not a lot of point in doing--


00:46:00.640 --> 00:46:01.640
Just let it run.


00:46:01.640 --> 00:46:02.640
Yeah.


00:46:02.640 --> 00:46:03.640
But when you've got...


00:46:03.640 --> 00:46:12.260
I've dealt with test suites that take best part of six hours to run end to end.


00:46:12.260 --> 00:46:18.960
So having some kind of test selection, test reduction there is essential really.


00:46:18.960 --> 00:46:23.280
In the winter it's nice because then your computer can spend six hours heating the room.


00:46:23.280 --> 00:46:24.280
Yeah.


00:46:24.280 --> 00:46:28.760
A little less stress on the house heater or office heater.


00:46:28.760 --> 00:46:35.640
Seriously, there's, what do you think about the systems like the IDEs that have their


00:46:35.640 --> 00:46:41.040
extensions are built right in, where they just constantly run the tests as you make


00:46:41.040 --> 00:46:42.040
changes?


00:46:42.040 --> 00:46:44.740
the files have changed, so we're rerunning the tests.


00:46:44.740 --> 00:46:49.680
- So I don't use that in an IDE,


00:46:49.680 --> 00:46:52.280
but I do have something like that set up


00:46:52.280 --> 00:46:55.180
that runs in a terminal window,


00:46:55.180 --> 00:46:59.120
and I found continuous testing to be quite helpful.


00:46:59.120 --> 00:47:02.440
It's really helpful in cases where maybe I forget


00:47:02.440 --> 00:47:05.740
to run my test suite while I'm refactoring my program,


00:47:05.740 --> 00:47:08.920
and it can give me immediate feedback.


00:47:08.920 --> 00:47:11.840
To go back to a comment that I made previously,


00:47:11.840 --> 00:47:15.160
You can also use different pytest plugins


00:47:15.160 --> 00:47:17.520
or use something like Hypothesis


00:47:17.520 --> 00:47:19.400
so that you can run your test suite


00:47:19.400 --> 00:47:22.560
with random inputs on a regular basis.


00:47:22.560 --> 00:47:25.280
And I have found that's another good way for me


00:47:25.280 --> 00:47:29.020
to be able to, without having to think too hard,


00:47:29.020 --> 00:47:32.020
find potential bugs in the functions that I'm testing.


00:47:32.020 --> 00:47:34.160
- Okay, interesting.


00:47:34.160 --> 00:47:38.600
Let's talk about some of the tools.


00:47:38.600 --> 00:47:41.000
So you all highlighted a couple of tools


00:47:41.000 --> 00:47:46.000
that people can use to help find flaky tests.


00:47:46.000 --> 00:47:48.840
So over at Datadog,


00:47:48.840 --> 00:47:51.480
they've got one for flaky test management.


00:47:51.480 --> 00:47:53.140
You wanna tell people about that?


00:47:53.140 --> 00:47:57.240
- So many of the tools that are provided


00:47:57.240 --> 00:48:00.720
by companies like Datadog are offering you


00:48:00.720 --> 00:48:03.780
some type of dashboard that will help you


00:48:03.780 --> 00:48:07.160
to better understand the characteristics


00:48:07.160 --> 00:48:08.400
of your test suite.


00:48:08.400 --> 00:48:11.600
so you can see what are the test cases


00:48:11.600 --> 00:48:14.020
that tend to be the most flaky.


00:48:14.020 --> 00:48:16.320
I think oftentimes it's hard for us


00:48:16.320 --> 00:48:20.080
to get a big picture view of our test suite


00:48:20.080 --> 00:48:23.200
and to understand what is and is not flaky.


00:48:23.200 --> 00:48:26.520
And so therefore having a flaky test management dashboard


00:48:26.520 --> 00:48:31.520
like Datadog provides can often give me the observability


00:48:31.520 --> 00:48:34.340
or the visibility that I might miss otherwise.


00:48:34.340 --> 00:48:37.160
- Yeah, that's super cool.


00:48:37.160 --> 00:48:40.880
And let's see, there's, I don't know,


00:48:40.880 --> 00:48:42.960
that's not the one I want to pull up.


00:48:42.960 --> 00:48:45.980
Also at Cypress has flaky test management.


00:48:45.980 --> 00:48:50.040
- So this is a really interesting approach


00:48:50.040 --> 00:48:53.960
because I normally use Cypress when I'm testing websites.


00:48:53.960 --> 00:48:55.560
And in my experience, when I--


00:48:55.560 --> 00:48:56.400
- What is Cypress?


00:48:56.400 --> 00:48:57.220
I'm not familiar with that.


00:48:57.220 --> 00:48:58.680
Maybe people aren't as well.


00:48:58.680 --> 00:48:59.840
Give us a quick intro first.


00:48:59.840 --> 00:49:01.120
- Sure, I'd love to do so.


00:49:01.120 --> 00:49:05.520
So Cypress is a tool that helps you to do testing


00:49:05.520 --> 00:49:08.280
of your web user interfaces.


00:49:08.280 --> 00:49:10.560
So if you have a web application


00:49:10.560 --> 00:49:13.920
and you want to be able to test the input to a form


00:49:13.920 --> 00:49:16.660
or you want to be able to test certain workflows


00:49:16.660 --> 00:49:18.780
through your web application,


00:49:18.780 --> 00:49:23.060
you can use Cypress and you write your test cases.


00:49:23.060 --> 00:49:27.500
Essentially, it's as if Cypress is running its own Chrome


00:49:27.500 --> 00:49:29.300
and it can control your test suite,


00:49:29.300 --> 00:49:31.200
it can run your test cases.


00:49:31.200 --> 00:49:34.700
When things fail, it can actually give you snapshots


00:49:34.700 --> 00:49:39.520
of what failed, it can tell you about the browser version


00:49:39.520 --> 00:49:44.160
that it was using, or maybe the mobile ready viewport


00:49:44.160 --> 00:49:46.500
that it was currently running it at.


00:49:46.500 --> 00:49:49.060
And again, the nice thing about things


00:49:49.060 --> 00:49:52.960
like what Cypress provides is that it can give you


00:49:52.960 --> 00:49:56.280
some kind of flaky test case analytics,


00:49:56.280 --> 00:49:57.840
which can show you which are failing


00:49:57.840 --> 00:49:59.000
and which are passing.


00:49:59.000 --> 00:50:01.120
And it can also say, hey, these are the ones


00:50:01.120 --> 00:50:04.120
that are flaky, and then break it out


00:50:04.120 --> 00:50:06.200
in terms of which ones are the most flaky


00:50:06.200 --> 00:50:08.280
versus the least flaky.


00:50:08.280 --> 00:50:13.080
Again, primarily in the context of testing web interfaces


00:50:13.080 --> 00:50:15.200
or web applications.


00:50:15.200 --> 00:50:17.880
- Sounds a little bit like Selenium or Playwright,


00:50:17.880 --> 00:50:19.360
which are both nice.


00:50:19.360 --> 00:50:20.320
- It is.


00:50:20.320 --> 00:50:24.120
So I have to say I've had the most flaky tests


00:50:24.120 --> 00:50:26.960
for my Selenium test cases.


00:50:26.960 --> 00:50:31.280
But when I switched to either Cypress or Playwright,


00:50:31.280 --> 00:50:33.400
Playwright as well has a way


00:50:33.400 --> 00:50:37.000
so that you don't have to do these baked in weights


00:50:37.000 --> 00:50:39.200
inside of your test case,


00:50:39.200 --> 00:50:41.640
which is one of the sources of flakiness


00:50:41.640 --> 00:50:43.840
that Owen and I have found in a number


00:50:43.840 --> 00:50:45.640
of real world programs.


00:50:45.640 --> 00:50:48.600
- I'd say that's almost one of the most common actually.


00:50:48.600 --> 00:50:49.640
- Hmm.


00:50:49.640 --> 00:50:50.480
Okay, Owen.


00:50:50.480 --> 00:50:53.600
So Gregory points out that it could be


00:50:53.600 --> 00:50:58.240
not exactly there's something wrong with your program


00:50:58.240 --> 00:51:01.120
or your code or the infrastructure your code depends upon,


00:51:01.120 --> 00:51:05.960
but maybe almost a flaky test framework itself,


00:51:05.960 --> 00:51:08.460
a flaky test runner scenario


00:51:08.460 --> 00:51:12.420
where the flakiness is not even necessarily,


00:51:12.420 --> 00:51:16.840
it's in the observation, not in the execution.


00:51:16.840 --> 00:51:18.380
That's pretty interesting.


00:51:18.380 --> 00:51:20.920
- Well, yeah, the classic formula for something like that


00:51:20.920 --> 00:51:22.560
is a test case that says,


00:51:22.560 --> 00:51:25.840
launch something asynchronously,


00:51:25.840 --> 00:51:28.420
wait one second, check something.


00:51:28.420 --> 00:51:32.820
So yeah, you might think that that one second is enough,


00:51:32.820 --> 00:51:35.580
but if there's a time when for whatever reason


00:51:35.580 --> 00:51:37.380
there's some background work going on


00:51:37.380 --> 00:51:39.700
or it takes a little longer than that, then...


00:51:39.700 --> 00:51:43.700
- Then all of a sudden, you have a half second.


00:51:43.700 --> 00:51:46.540
Yeah, yeah, okay.


00:51:46.540 --> 00:51:47.380
Yeah, for sure.


00:51:47.380 --> 00:51:49.380
Any of those things where you have to start something


00:51:49.380 --> 00:51:52.900
and then wait for it to, something to happen remotely,


00:51:52.900 --> 00:51:55.060
that's gotta be pretty sketchy.


00:51:55.060 --> 00:51:57.380
- So the, I mean, the usual approach


00:51:57.380 --> 00:51:59.060
is to sort of have an explicit wait.


00:51:59.060 --> 00:52:01.020
So you'll sort of say,


00:52:01.020 --> 00:52:03.920
I'm actually going to wait until this is completed,


00:52:03.920 --> 00:52:04.960
whatever that means.


00:52:04.960 --> 00:52:08.740
But then you run into a situation where,


00:52:08.740 --> 00:52:10.340
well, what if for whatever reason,


00:52:10.340 --> 00:52:12.860
this asynchronous thing you're interacting with


00:52:12.860 --> 00:52:14.720
is timed out or frozen,


00:52:14.720 --> 00:52:15.940
then you're going to end up with a test


00:52:15.940 --> 00:52:17.380
that's waiting forever.


00:52:17.380 --> 00:52:20.340
So you have to have some kind of upper limit


00:52:20.340 --> 00:52:21.740
to how long you'll wait for.


00:52:21.740 --> 00:52:24.780
- Otherwise you may wait forever, yeah.


00:52:24.780 --> 00:52:25.700
- Exactly. - This test case


00:52:25.700 --> 00:52:26.900
is real slow.


00:52:26.900 --> 00:52:29.660
So once again, there's no kind of silver bullet solution,


00:52:29.660 --> 00:52:30.780
really.


00:52:30.780 --> 00:52:32.060
It's just trade-offs again.


00:52:32.060 --> 00:52:32.900
- Yeah, yeah, yeah.


00:52:32.900 --> 00:52:35.040
What do you all think about things like,


00:52:35.040 --> 00:52:37.820
one sec,


00:52:37.820 --> 00:52:41.560
things like tenacity,


00:52:41.560 --> 00:52:46.540
where you can go and put a decorator onto a function


00:52:46.540 --> 00:52:49.860
and just say, "Retry this with some scenario."


00:52:49.860 --> 00:52:55.300
Or another one that I just learned about is Hennex stamina,


00:52:55.300 --> 00:52:56.500
which is cool as well.


00:52:56.500 --> 00:52:59.180
you can say, put a decorator and say,


00:52:59.180 --> 00:53:02.860
retry certain number of attempts with, you know,


00:53:02.860 --> 00:53:05.620
like some kind of back off, an exponential back off


00:53:05.620 --> 00:53:10.120
where you give it a certain amount of time.


00:53:10.120 --> 00:53:13.900
Like for flaky tests, you see it making sense to say,


00:53:13.900 --> 00:53:16.780
well, maybe call the functions this way


00:53:16.780 --> 00:53:18.340
in some of your test cases.


00:53:18.340 --> 00:53:20.440
- So I've never actually seen either of these plugins,


00:53:20.440 --> 00:53:22.240
but they do look quite interesting.


00:53:22.240 --> 00:53:24.620
- Yeah.


00:53:25.700 --> 00:53:29.500
I haven't used Tenacity either, but I was aware of it.


00:53:29.500 --> 00:53:31.900
And I think you could imagine using Tenacity


00:53:31.900 --> 00:53:34.380
in two distinct locations.


00:53:34.380 --> 00:53:38.380
Maybe you want to put some of these Tenacity annotations


00:53:38.380 --> 00:53:41.380
on your multi-threaded code, and then


00:53:41.380 --> 00:53:45.220
let the test case call those annotated functions.


00:53:45.220 --> 00:53:46.020
Yes, exactly.


00:53:46.020 --> 00:53:47.740
Don't put them in your production code.


00:53:47.740 --> 00:53:48.900
Don't put them on your test.


00:53:48.900 --> 00:53:51.100
But just have an intermediate one


00:53:51.100 --> 00:53:53.900
that you can control the back off and retry count.


00:53:53.900 --> 00:53:54.620
Exactly.


00:53:54.620 --> 00:53:55.700
- Exactly.


00:53:55.700 --> 00:53:58.100
And another thing that I think is worth pointing out


00:53:58.100 --> 00:54:01.000
since we were previously discussing web testing


00:54:01.000 --> 00:54:05.620
is that there is a way in which it can be a problem


00:54:05.620 --> 00:54:07.620
with your testing framework,


00:54:07.620 --> 00:54:09.700
like you previously mentioned, Michael.


00:54:09.700 --> 00:54:12.040
So for example, Playwright does have


00:54:12.040 --> 00:54:15.180
a really nice auto-weighting feature.


00:54:15.180 --> 00:54:18.580
And so when I'm testing a web application,


00:54:18.580 --> 00:54:22.000
I can use Playwright's auto-weight feature,


00:54:22.000 --> 00:54:25.400
And that will help me to avoid baking in


00:54:25.400 --> 00:54:28.600
hard-coded weights inside my test


00:54:28.600 --> 00:54:32.480
because the actual testing framework itself


00:54:32.480 --> 00:54:35.120
has a way to do auto-weighting.


00:54:35.120 --> 00:54:37.960
- So when you say auto-weighting,


00:54:37.960 --> 00:54:39.720
you can say things like,


00:54:39.720 --> 00:54:41.800
request this page,


00:54:41.800 --> 00:54:42.920
find this field,


00:54:42.920 --> 00:54:44.220
put my email address into it,


00:54:44.220 --> 00:54:45.320
click this button,


00:54:45.320 --> 00:54:47.280
test that this thing is,


00:54:47.280 --> 00:54:49.280
now the page has this thing in it.


00:54:49.280 --> 00:54:52.860
but obviously servers don't instantly respond to that,


00:54:52.860 --> 00:54:54.740
right, so you've got to have some sorts of delays.


00:54:54.740 --> 00:54:55.980
So you're talking about the system


00:54:55.980 --> 00:54:58.180
can kind of track that automatically.


00:54:58.180 --> 00:55:00.200
- Yeah, so Playwright can actually do


00:55:00.200 --> 00:55:01.580
some of that on its own.


00:55:01.580 --> 00:55:05.760
So for example, if you're looking for a certain element


00:55:05.760 --> 00:55:08.460
in the webpage to be available,


00:55:08.460 --> 00:55:11.260
Playwright has a way that will allow you to ensure


00:55:11.260 --> 00:55:14.420
that the element is actually attached to the DOM,


00:55:14.420 --> 00:55:18.420
that it's actually visible, that it hasn't moved around,


00:55:18.420 --> 00:55:21.700
or that it's not being animated in some way.


00:55:21.700 --> 00:55:24.280
And all of those things are actually part


00:55:24.280 --> 00:55:26.260
of the testing framework,


00:55:26.260 --> 00:55:28.140
which makes it incredibly helpful


00:55:28.140 --> 00:55:31.980
because then I don't have to actually implement all of that


00:55:31.980 --> 00:55:34.180
when I'm writing my test cases.


00:55:34.180 --> 00:55:35.240
- Yeah, fantastic.


00:55:35.240 --> 00:55:38.420
We're getting a little short on time here.


00:55:38.420 --> 00:55:40.340
Let's round, I want to round it out


00:55:40.340 --> 00:55:43.140
with a little bit of a survey,


00:55:43.140 --> 00:55:45.580
if I find the right thing,


00:55:46.380 --> 00:55:50.740
of some, you all mentioned some of the Pytest plugins


00:55:50.740 --> 00:55:52.460
that might be relevant here.


00:55:52.460 --> 00:55:56.040
So I'm pulling up awesome Pytest, which I'll link to,


00:55:56.040 --> 00:55:58.500
just an awesome list of Pytest things.


00:55:58.500 --> 00:56:03.500
But you've got things like in here, like Pytest randomly,


00:56:03.500 --> 00:56:06.380
which lets you randomly order tests and set a seed


00:56:06.380 --> 00:56:08.980
and those kinds of things and a bunch of other stuff.


00:56:08.980 --> 00:56:10.340
Do you want to pull out some of these


00:56:10.340 --> 00:56:11.940
you maybe think are relevant


00:56:11.940 --> 00:56:14.940
or see if they're at least in your list, the ones you like?


00:56:14.940 --> 00:56:21.260
I've used randomly before. Like how I said earlier, this could be a great way of


00:56:21.260 --> 00:56:28.220
finding those tests that, not necessarily the ones that don't clean up after themselves properly,


00:56:28.220 --> 00:56:33.980
but it will certainly show you tests that are potentially impacted by other tests not cleaning


00:56:33.980 --> 00:56:38.700
up after themselves. I think if you take almost any large test suite and apply randomly to it,


00:56:38.700 --> 00:56:43.260
the chances are you are probably going to see some failed tests that weren't failing before


00:56:43.260 --> 00:56:44.700
before you shuffle the order.


00:56:44.700 --> 00:56:47.740
So I think that's quite an interesting plugin


00:56:47.740 --> 00:56:50.540
and you can use to quickly assess


00:56:50.540 --> 00:56:53.380
if you've got all the dependent tests in your test suite.


00:56:53.380 --> 00:56:57.320
- Speaking from experience,


00:56:57.320 --> 00:56:59.700
the one additional point that I would add


00:56:59.700 --> 00:57:02.580
is that when I use pytest randomly,


00:57:02.580 --> 00:57:06.180
I try to make sure I integrate it early


00:57:06.180 --> 00:57:09.540
into the lifetime of my development process.


00:57:09.540 --> 00:57:13.180
So instead of writing 947 test cases


00:57:13.180 --> 00:57:15.980
and then trying to use pytest randomly,


00:57:15.980 --> 00:57:19.380
I tried to always make sure that pytest randomly


00:57:19.380 --> 00:57:22.740
is running in GitHub Actions very early


00:57:22.740 --> 00:57:25.000
in the development of my application


00:57:25.000 --> 00:57:27.620
so that when I only have 40 or 50 test cases,


00:57:27.620 --> 00:57:30.920
I can immediately find those dependent tests


00:57:30.920 --> 00:57:32.300
that could have flakiness


00:57:32.300 --> 00:57:35.340
and then begin to be more proactive


00:57:35.340 --> 00:57:38.380
when it comes to avoiding flakiness very early


00:57:38.380 --> 00:57:40.580
when I'm launching a new product.


00:57:40.580 --> 00:57:41.420
- Yeah, that makes sense.


00:57:41.420 --> 00:57:47.580
my one of my follow-up questions was going to be, would you all recommend just de facto


00:57:47.580 --> 00:57:53.060
installing that into and turning that on at least unless you have a reason to disable


00:57:53.060 --> 00:57:54.540
it for new projects?


00:57:54.540 --> 00:57:56.660
Yeah, I find it very helpful.


00:57:56.660 --> 00:58:00.580
It's one of the things that I frequently add to a new project.


00:58:00.580 --> 00:58:06.340
So when I'm I use Poetry for a lot of my package management and I have various templates that


00:58:06.340 --> 00:58:12.540
I use and I often add pytest randomly right away as one of my dev dependencies and then


00:58:12.540 --> 00:58:17.460
make sure I'm always running my test suite in a random order when I'm running it in GitHub


00:58:17.460 --> 00:58:18.460
Actions.


00:58:18.460 --> 00:58:23.620
So I can't remember who I heard this all for, where I read it exactly, but I have heard


00:58:23.620 --> 00:58:30.140
that at Google and other companies as well, running the tests in a random order is actually


00:58:30.140 --> 00:58:32.700
standard practice.


00:58:32.700 --> 00:58:34.200
And the reason, like Greg just said,


00:58:34.200 --> 00:58:36.480
so when you start on a new project,


00:58:36.480 --> 00:58:40.860
you're starting with this sort of shuffled order


00:58:40.860 --> 00:58:41.780
test running.


00:58:41.780 --> 00:58:45.280
And I suppose it's kind of like a technical debt then.


00:58:45.280 --> 00:58:47.200
You're kind of paying it off early


00:58:47.200 --> 00:58:49.400
rather than writing a bunch of tests


00:58:49.400 --> 00:58:51.740
and then having a big fixing effort


00:58:51.740 --> 00:58:53.200
when you realize there's a big problem


00:58:53.200 --> 00:58:55.000
with a whole bunch of them.


00:58:55.000 --> 00:58:58.740
- I feel like it's a little similar to linting.


00:58:58.740 --> 00:59:00.700
And you have things that go through and tell you


00:59:00.700 --> 00:59:06.300
the recommended issues or issues we found we recommend fixing for your code.


00:59:06.300 --> 00:59:12.340
If you apply that retroactively, like rough or whatever, if you apply that to your project


00:59:12.340 --> 00:59:16.220
after it's huge, you'll get thousands of lines and nobody wants to spend the next two weeks


00:59:16.220 --> 00:59:17.220
fixing them.


00:59:17.220 --> 00:59:20.860
But if you just run that as you develop, you go, there's two little things we got to fix,


00:59:20.860 --> 00:59:21.860
no big deal.


00:59:21.860 --> 00:59:26.180
See, it sounds similar to that effect.


00:59:26.180 --> 00:59:27.180
I agree.


00:59:27.180 --> 00:59:28.180
Yeah.


00:59:28.180 --> 00:59:32.180
Here's another one that's interesting.


00:59:32.180 --> 00:59:36.180
pytest.socket to disable socket calls during tests.


00:59:36.180 --> 00:59:40.180
So you heard me talk about requesting every page


00:59:40.180 --> 00:59:44.180
on the sitemap. So I'm not necessarily suggesting that you would


00:59:44.180 --> 00:59:48.180
want to just do this in general. But one of the areas that seems to me that could be


00:59:48.180 --> 00:59:52.180
result in flakiness for a set of tests is like


00:59:52.180 --> 00:59:56.180
I depend on an external system. Like, oh, I thought we were mocking out the database, but I'm


00:59:56.180 --> 01:00:00.140
talking to the database or oh I thought we were talking mocking out the API call


01:00:00.140 --> 01:00:04.820
but we're talking to it you could probably turn that on for a moment see


01:00:04.820 --> 01:00:08.540
which test fails and just go well these three were not supposed to be talking


01:00:08.540 --> 01:00:12.340
over the network but somehow they fail when we don't let them talk to the


01:00:12.340 --> 01:00:16.740
network so that might be worth looking into what do you think about that that's


01:00:16.740 --> 01:00:20.180
a good point I haven't tried that tool but the way that you've explained it


01:00:20.180 --> 01:00:25.420
makes it really clear that there would be a lot of utility to using it yeah


01:00:25.420 --> 01:00:30.420
Let's see, there's probably a couple other ones in here.


01:00:30.420 --> 01:00:33.180
There was one right back here.


01:00:33.180 --> 01:00:36.820
It was called a pytest Picked.


01:00:36.820 --> 01:00:38.540
I don't really know how I feel about this.


01:00:38.540 --> 01:00:45.140
I don't know if it's precise enough, but you were talking, Greg, about winnowing down the


01:00:45.140 --> 01:00:47.060
set of tests you were running using coverage.


01:00:47.060 --> 01:00:53.340
This one says it runs tests related to changes detected by version control in just unstaged


01:00:53.340 --> 01:00:54.340
files.


01:00:54.340 --> 01:01:01.060
I feel like this is a really cool idea, but it does it in the wrong order.


01:01:01.060 --> 01:01:05.060
I feel like it's looking at just the test files that are not changed or that are not


01:01:05.060 --> 01:01:06.980
committed and rerunning those.


01:01:06.980 --> 01:01:15.380
But you should look at the code covered, the changes, the unstaged production files, and


01:01:15.380 --> 01:01:20.180
then use code covers to figure out which tests need to be rerun.


01:01:20.180 --> 01:01:25.800
It's really cool to use the idea of having the source control tell you what the changes


01:01:25.800 --> 01:01:28.760
are since your last commit.


01:01:28.760 --> 01:01:32.340
But then it's just applying it to the test files, I think.


01:01:32.340 --> 01:01:37.180
But if it could say, well, now we use coverage to figure out these tests, that would be awesome.


01:01:37.180 --> 01:01:40.460
Yeah, and I regret that I can't remember the name of it.


01:01:40.460 --> 01:01:46.320
There is a tool that does a type of test suite reduction as a pytest plugin.


01:01:46.320 --> 01:01:50.560
And maybe I'll look it up after the show and we can include it in the show notes.


01:01:50.560 --> 01:01:55.720
Of course, the thing that you've got to be careful about is that there could be dependencies


01:01:55.720 --> 01:02:02.220
between program components that are not evidenced in the source code or the coverage relationship,


01:02:02.220 --> 01:02:05.300
but maybe by access to external resources.


01:02:05.300 --> 01:02:09.320
And so in those cases, the selection process may not work as intended.


01:02:09.320 --> 01:02:10.320
Right.


01:02:10.320 --> 01:02:12.360
This thing changed something in the database.


01:02:12.360 --> 01:02:17.840
other part of the code read it and that makes it crash, but you didn't actually touch that


01:02:17.840 --> 01:02:19.640
code over there, something like that.


01:02:19.640 --> 01:02:20.640
Yeah, absolutely.


01:02:20.640 --> 01:02:21.640
Yeah, interesting.


01:02:21.640 --> 01:02:27.120
Maybe one more, I don't know too much about this, but Bill points out, says, "I remember


01:02:27.120 --> 01:02:33.480
Anthony (so Teely) had worked on a tool to detect test pollution, which is a kind of


01:02:33.480 --> 01:02:34.480
related topic."


01:02:34.480 --> 01:02:40.760
And let me see, I had that pulled up.


01:02:40.760 --> 01:02:41.760
Hold on.


01:02:41.760 --> 01:02:42.760
Pi PI.


01:02:42.760 --> 01:02:51.760
It's detect test pollution, I believe.


01:02:51.760 --> 01:03:02.520
It says a test pollution is where a test fails due to the side effects of some other test


01:03:02.520 --> 01:03:05.480
in the suite.


01:03:05.480 --> 01:03:06.480
That's pretty interesting.


01:03:06.480 --> 01:03:10.360
So maybe that's where something for people to look at.


01:03:10.360 --> 01:03:11.360
Have you heard of this?


01:03:11.360 --> 01:03:13.640
I haven't heard of this before, so I can't speak too much.


01:03:13.640 --> 01:03:23.880
I've not heard of this specific tool, but I've seen it done in Java as a Java project.


01:03:23.880 --> 01:03:27.600
And yeah, you can do it fairly successfully, depending on...


01:03:27.600 --> 01:03:29.000
And you can go quite deep with it as well.


01:03:29.000 --> 01:03:33.400
I mean, it's hard to see exactly how this will work just based on the description, but


01:03:33.400 --> 01:03:34.400
I mean...


01:03:34.400 --> 01:03:35.400
Yeah.


01:03:35.400 --> 01:03:38.040
example where it had a global variable.


01:03:38.040 --> 01:03:47.480
But I mean, obviously that's quite a trivial example, but I mean, you can get state pollution


01:03:47.480 --> 01:03:52.200
in ways you really wouldn't expect it.


01:03:52.200 --> 01:03:59.240
So for example, I've seen a test where the two tests that were dependent on each other


01:03:59.240 --> 01:04:04.240
were individual parameterizations of the same test.


01:04:04.240 --> 01:04:06.640
And there was a dependency


01:04:06.640 --> 01:04:11.640
because in the parameterization decorator,


01:04:11.640 --> 01:04:16.040
someone had used a list object as an argument.


01:04:16.040 --> 01:04:19.720
So then, and then in the test, they've modified that list,


01:04:19.720 --> 01:04:23.240
but then the list isn't then recreated for the next test.


01:04:23.240 --> 01:04:25.320
So then the next test gets,


01:04:25.320 --> 01:04:26.680
but that's not a global variable.


01:04:26.680 --> 01:04:31.680
that's just sort of created when that file is executed.


01:04:31.680 --> 01:04:33.680
- A weird Python default value behavior, yeah.


01:04:33.680 --> 01:04:35.560
- So I mean, yeah, it's quite,


01:04:35.560 --> 01:04:38.320
I've seen people complain about that quite a lot.


01:04:38.320 --> 01:04:42.400
So like when you have a function


01:04:42.400 --> 01:04:44.280
and you put a list or a mutable object


01:04:44.280 --> 01:04:46.360
as like a default argument,


01:04:46.360 --> 01:04:48.960
that's quite a common gotcha.


01:04:48.960 --> 01:04:51.320
So it's a similar kind of thing.


01:04:51.320 --> 01:04:52.800
- Yeah, and it's, if you run tools,


01:04:52.800 --> 01:04:53.800
like I talked about Linting,


01:04:53.800 --> 01:04:56.040
if you run tools like Ruff or others


01:04:56.040 --> 01:04:58.840
that will flakate those types of things.


01:04:58.840 --> 01:05:00.840
Many of them will warn you, like,


01:05:00.840 --> 01:05:02.880
no, this is a bad idea, don't do that.


01:05:02.880 --> 01:05:11.040
So maybe running, I would imagine running tools


01:05:11.040 --> 01:05:13.880
like Ruff and other linters that detect these issues


01:05:13.880 --> 01:05:16.440
might actually reduce the test flakiness


01:05:16.440 --> 01:05:19.640
by finding some of these anti-patterns


01:05:19.640 --> 01:05:21.240
that you maybe didn't catch.


01:05:21.240 --> 01:05:24.360
- Yeah, it may well do, yeah.


01:05:24.360 --> 01:05:29.400
And I think another thing that's important to note when we're talking about a linter like RUF


01:05:29.400 --> 01:05:35.640
is that it's so fast to run that there's not really a big cost from a developer's perspective.


01:05:35.640 --> 01:05:37.000
Yeah, it's nearly instant.


01:05:37.000 --> 01:05:43.800
Yeah, again, integrate it early, use it regularly, have it in your IDE, use it in CI,


01:05:43.800 --> 01:05:49.240
and it's one of those things where it might help you to avoid certain coding practices


01:05:49.240 --> 01:05:52.840
that would ultimately lead to test flakiness creeping into your system.


01:05:53.560 --> 01:05:55.260
- Yeah, really good advice.


01:05:55.260 --> 01:05:57.060
It is so fast.


01:05:57.060 --> 01:06:00.100
I ran it against, like I said, 20,000 lines of Python,


01:06:00.100 --> 01:06:03.760
and it went, it looked like it didn't even do anything.


01:06:03.760 --> 01:06:05.360
I thought, oh, I didn't do it right


01:06:05.360 --> 01:06:06.760
because nothing happened.


01:06:06.760 --> 01:06:09.300
No, but it's so quick.


01:06:09.300 --> 01:06:10.940
And you can put it, there's plugins


01:06:10.940 --> 01:06:13.680
for both PyCharm and VS Code that'll just run it.


01:06:13.680 --> 01:06:16.620
And PyCharm even integrates it into its code fixes


01:06:16.620 --> 01:06:18.260
and all of its behaviors


01:06:18.260 --> 01:06:20.700
and just reformat code options and stuff.


01:06:20.700 --> 01:06:21.800
It's really good.


01:06:21.800 --> 01:06:23.800
Yeah, I've been using Ruff recently,


01:06:23.800 --> 01:06:25.520
and I really like it as well.


01:06:25.520 --> 01:06:27.360
Along with the point that you mentioned,


01:06:27.360 --> 01:06:29.400
I like the fact that you can configure it


01:06:29.400 --> 01:06:31.800
through the PyProject TOMO file, which


01:06:31.800 --> 01:06:35.760
is where I'm already putting all of my other configurations.


01:06:35.760 --> 01:06:40.340
And then it also essentially can serve as a language server


01:06:40.340 --> 01:06:42.320
protocol implementation.


01:06:42.320 --> 01:06:45.080
So even if you don't use the two text editors


01:06:45.080 --> 01:06:46.800
that you mentioned, you can still


01:06:46.800 --> 01:06:49.560
get all of the code actions and fixes.


01:06:49.560 --> 01:06:52.680
And because it's so fast, it's really easy to use it


01:06:52.680 --> 01:06:54.960
even on big code basis.


01:06:54.960 --> 01:06:56.260
- Yeah, awesome.


01:06:56.260 --> 01:06:58.760
Okay, one more really quick,


01:06:58.760 --> 01:07:00.560
'cause I think this is a good suggestion.


01:07:00.560 --> 01:07:03.440
And this goes back to how I talked about,


01:07:03.440 --> 01:07:04.680
you know, like the call is coming


01:07:04.680 --> 01:07:07.200
from inside the house type of problem,


01:07:07.200 --> 01:07:10.560
in that the error could actually be


01:07:10.560 --> 01:07:12.880
with the test framework and the test code,


01:07:12.880 --> 01:07:15.600
not just, not actually your code.


01:07:15.600 --> 01:07:19.280
So Marwan points out that scoping fixtures incorrectly


01:07:19.280 --> 01:07:21.360
could be another source of flakiness.


01:07:21.360 --> 01:07:24.700
So the fixture could say, create a generator


01:07:24.700 --> 01:07:25.740
and pass it over to you,


01:07:25.740 --> 01:07:27.720
but you could say this is a class-based one


01:07:27.720 --> 01:07:31.080
instead of a test instance one,


01:07:31.080 --> 01:07:33.160
and then you get different results


01:07:33.160 --> 01:07:35.700
depending on the order and all these kinds of things.


01:07:35.700 --> 01:07:37.820
That's really interesting, I think.


01:07:37.820 --> 01:07:39.740
- I would agree.


01:07:39.740 --> 01:07:41.160
I think that's a really good point.


01:07:41.160 --> 01:07:43.200
The other thing that I sometimes need


01:07:43.200 --> 01:07:44.920
to be very careful about


01:07:44.920 --> 01:07:49.320
is having auto use test fixtures inside of my code,


01:07:49.320 --> 01:07:52.240
because then those might be applied everywhere,


01:07:52.240 --> 01:07:56.160
along with other decorators that are fixtures


01:07:56.160 --> 01:07:58.000
that are just applied selectively.


01:07:58.000 --> 01:08:00.840
And then I might get a kind of non-determinism


01:08:00.840 --> 01:08:04.160
in my testing process, just because of the way


01:08:04.160 --> 01:08:06.360
that various text fixtures are applied


01:08:06.360 --> 01:08:08.260
or the order in which they're applied.


01:08:08.260 --> 01:08:11.200
- Yeah, absolutely.


01:08:11.200 --> 01:08:13.880
All right, Owen, last word on this.


01:08:13.880 --> 01:08:16.880
The other pytest plugin that might be interesting


01:08:16.880 --> 01:08:21.080
is pytest xdist, or running these distributed.


01:08:21.080 --> 01:08:24.720
What do you think of, how's that help or hurt us here?


01:08:24.720 --> 01:08:29.560
- So, I mean, running your tests in parallel


01:08:29.560 --> 01:08:33.760
can be obviously a great way to expose


01:08:33.760 --> 01:08:37.760
concurrency-related flakiness, because as you said before,


01:08:37.760 --> 01:08:39.920
when you're writing the test,


01:08:39.920 --> 01:08:41.640
you might be writing it under the assumption


01:08:41.640 --> 01:08:44.740
that you're the only one accessing certain resources


01:08:44.740 --> 01:08:46.740
or running it at a certain time.


01:08:46.740 --> 01:08:50.880
Another thing that something like this can do as well


01:08:50.880 --> 01:08:55.080
is it can also expose order-dependent tests.


01:08:55.080 --> 01:09:00.120
Because, so the way it will work is it will create,


01:09:00.120 --> 01:09:04.260
say you're wanting to run eight tests at a time,


01:09:04.260 --> 01:09:09.840
this plugin will then create eight separate processes.


01:09:09.840 --> 01:09:14.480
But within those processes, each one has its own independent Python interpreter.


01:09:14.480 --> 01:09:18.000
So they're running independently of each other.


01:09:18.000 --> 01:09:26.240
But then you could also, by doing that, expose a test case that was expecting a previous test to run,


01:09:26.240 --> 01:09:29.200
but now isn't because it's running in a different process.


01:09:29.200 --> 01:09:33.280
And that test could then go on to fail.


01:09:33.280 --> 01:09:39.120
So that would then be another issue of inadequate setup from that test.


01:09:39.120 --> 01:09:39.960
- Yeah.


01:09:39.960 --> 01:09:44.680
This is something I should probably be running more of


01:09:44.680 --> 01:09:47.160
as well, like, why not?


01:09:47.160 --> 01:09:49.060
I have 10 cores on this computer.


01:09:49.060 --> 01:09:52.640
I've been running, why don't I just have my test run faster?


01:09:52.640 --> 01:09:54.060
It probably not 10 times faster,


01:09:54.060 --> 01:09:58.680
but it could do more than just running one thread in serial.


01:09:58.680 --> 01:10:00.400
- You could do, yeah.


01:10:00.400 --> 01:10:01.480
- But certainly run them in parallel


01:10:01.480 --> 01:10:04.560
would certainly pull up some of those ordering issues


01:10:04.560 --> 01:10:07.360
as well as resource contention issues.


01:10:07.360 --> 01:10:12.560
Yeah, so as well as providing a speed up, it's also great because it might expose


01:10:12.560 --> 01:10:14.160
some problems in your test suite as well.


01:10:14.160 --> 01:10:16.000
Yeah, absolutely.


01:10:16.000 --> 01:10:21.160
All right, guys, I think we're going to have to leave it there for the time we got,


01:10:21.160 --> 01:10:23.200
but excellent stuff.


01:10:23.200 --> 01:10:26.000
And there's a lot of detail here, isn't there?


01:10:26.000 --> 01:10:27.720
As you dig into it.


01:10:27.720 --> 01:10:33.560
Yeah, I think flaky tests are something that all of us as developers have encountered.


01:10:34.040 --> 01:10:37.680
We recognize that they limit us as developers,


01:10:37.680 --> 01:10:40.400
but also there's something that if we can automatically


01:10:40.400 --> 01:10:43.800
detect them or mitigate them in some way,


01:10:43.800 --> 01:10:47.000
we can remove that hassle from developers.


01:10:47.000 --> 01:10:48.640
So I think what we would like to do


01:10:48.640 --> 01:10:50.960
both as researchers and developers


01:10:50.960 --> 01:10:54.360
is allow people who write pytest test suites


01:10:54.360 --> 01:10:58.780
to be more productive and to write better tests


01:10:58.780 --> 01:10:59.960
that are less flaky.


01:10:59.960 --> 01:11:02.600
- Excellent.


01:11:02.600 --> 01:11:07.360
All right, before we wrap up the show, I'll just ask you one quick question I usually


01:11:07.360 --> 01:11:15.200
do at the end, and that is, if you've got a flaky test-related project on PyPI, some


01:11:15.200 --> 01:11:18.880
library, some package you want to recommend to people, or it could be something other


01:11:18.880 --> 01:11:23.240
than flaky-related, but something you want to recommend, some package you've come across


01:11:23.240 --> 01:11:27.280
lately.


01:11:27.280 --> 01:11:31.840
- So I was actually going to recommend something that's not connected to flaky test cases.


01:11:31.840 --> 01:11:33.680
- Go for it.


01:11:33.680 --> 01:11:36.420
- So a lot of the work that I do involves


01:11:36.420 --> 01:11:40.600
various types of processing of the abstract syntax tree


01:11:40.600 --> 01:11:42.480
of a Python program.


01:11:42.480 --> 01:11:45.840
And so I thought I might first call out the AST package


01:11:45.840 --> 01:11:49.120
that's actually a part of Python,


01:11:49.120 --> 01:11:53.100
which is built in and an incredibly useful tool,


01:11:53.100 --> 01:11:57.080
which isn't available in a lot of programming languages.


01:11:57.080 --> 01:12:00.080
The other two packages which are on PyPI,


01:12:00.080 --> 01:12:04.180
which I'll share more about is number one, libcst,


01:12:04.180 --> 01:12:05.660
which implements something


01:12:05.660 --> 01:12:08.860
that's called a concrete syntax tree.


01:12:08.860 --> 01:12:10.720
And it's a super useful tool


01:12:10.720 --> 01:12:14.820
when you want to be able to make changes to Python code


01:12:14.820 --> 01:12:17.780
or detect patterns in Python code.


01:12:17.780 --> 01:12:21.140
And you want to be able to fully preserve things


01:12:21.140 --> 01:12:23.580
like the blank space in the code


01:12:23.580 --> 01:12:25.460
and the comment strings in the code


01:12:25.460 --> 01:12:26.760
and things of that nature.


01:12:28.100 --> 01:12:32.100
Libcst is actually the foundation for another tool


01:12:32.100 --> 01:12:34.060
which is called Fixit.


01:12:34.060 --> 01:12:37.560
And Fixit is a little bit like Ruff,


01:12:37.560 --> 01:12:40.460
except that it allows you to very easily


01:12:40.460 --> 01:12:42.700
write your own linting rules.


01:12:42.700 --> 01:12:46.100
And then finally, the last thing that I would share


01:12:46.100 --> 01:12:47.900
on this same theme, Michael,


01:12:47.900 --> 01:12:51.060
is that there's a really fun to use tool


01:12:51.060 --> 01:12:55.580
by someone who is a core member of the Django project.


01:12:55.580 --> 01:13:00.580
And it's called PyAST Grip.


01:13:00.580 --> 01:13:04.460
And it actually lets you write XPath expressions.


01:13:04.460 --> 01:13:07.500
And then you can use those XPath expressions


01:13:07.500 --> 01:13:11.580
to essentially query the abstract syntax tree


01:13:11.580 --> 01:13:12.920
of your Python program.


01:13:12.920 --> 01:13:15.340
- Incredible, okay.


01:13:15.340 --> 01:13:19.860
Yeah, looks like, now you can.


01:13:19.860 --> 01:13:24.060
I guess syntax trees are a little bit like XML,


01:13:24.060 --> 01:13:24.900
aren't they?


01:13:24.900 --> 01:13:31.940
And if anybody has to do work where they're building an automated refactoring tool, or


01:13:31.940 --> 01:13:37.500
they're building a new linting tool, or various types of program analysis tools, the packages


01:13:37.500 --> 01:13:39.580
that I've mentioned might be very helpful.


01:13:39.580 --> 01:13:40.580
Cool.


01:13:40.580 --> 01:13:41.580
Thank you.


01:13:41.580 --> 01:13:42.980
That was a bunch of good ones.


01:13:42.980 --> 01:13:45.260
Owen, you got anything you want to give a shout out to?


01:13:45.260 --> 01:13:52.220
Well, it's actually a bit spooky because I was also about to recommend libcst as well.


01:13:52.220 --> 01:13:57.220
- So one small library I've used a few times is Radon.


01:13:57.220 --> 01:14:02.860
So that's R-A-D-O-N, I believe it's spelled.


01:14:02.860 --> 01:14:06.700
So this will basically calculate


01:14:06.700 --> 01:14:08.740
a load of code metrics for you.


01:14:08.740 --> 01:14:09.780
- Nice, okay.


01:14:09.780 --> 01:14:11.740
- So these are from relatively simple things


01:14:11.740 --> 01:14:15.660
like number of lines while taking into account


01:14:15.660 --> 01:14:17.940
comments and that kind of things,


01:14:17.940 --> 01:14:19.300
to more complex metrics.


01:14:19.300 --> 01:14:21.700
So there's this maintainability index,


01:14:21.700 --> 01:14:23.180
which is basically like a weighted sum


01:14:23.180 --> 01:14:24.500
of a bunch of other code metrics.


01:14:24.500 --> 01:14:25.580
- I really like that one.


01:14:25.580 --> 01:14:27.380
Yeah, it's like, it combines and says,


01:14:27.380 --> 01:14:29.380
well, cyclical complexity is this,


01:14:29.380 --> 01:14:30.860
line length is that, function length,


01:14:30.860 --> 01:14:32.980
like all that kind of stuff, right?


01:14:32.980 --> 01:14:35.780
- And I've actually found sort of empirically,


01:14:35.780 --> 01:14:39.260
there is, appears to be some correlation in some cases


01:14:39.260 --> 01:14:43.700
to between having a high, sorry,


01:14:43.700 --> 01:14:45.220
or having a poor maintainability


01:14:45.220 --> 01:14:47.340
to having very complex code.


01:14:47.340 --> 01:14:49.060
Having very complex test case code


01:14:49.060 --> 01:14:51.020
in that test case actually being flaky.


01:14:51.020 --> 01:15:01.020
which is interesting. Yeah, I can believe it. Okay. Well, that's also a cool I hadn't heard a rate on. That's neat.


01:15:01.020 --> 01:15:08.020
Alright guys, thank you for being on the show. It's been a super interesting conversation.


01:15:08.020 --> 01:15:16.020
Final call to action. People either have flaky tests and want to get out of them or they want to avoid having them in the first place.


01:15:16.020 --> 01:15:18.180
What do you tell them? What are your parting thoughts?


01:15:18.180 --> 01:15:24.420
So my quick parting thought is as follows. We'll have some links in the show notes to


01:15:24.420 --> 01:15:30.820
various papers and tools that Owen and our colleagues and I have developed, and we hope


01:15:30.820 --> 01:15:36.500
that people will try them out. It would also be awesome if people can get in contact with us and


01:15:36.500 --> 01:15:42.580
share some of their flaky test case war stories. We would love to learn from you and partner with


01:15:42.580 --> 01:15:46.460
you to help you solve some of the flaky test case challenges that you have.


01:15:46.460 --> 01:15:48.380
Owen, what else do you want to add?


01:15:48.380 --> 01:15:51.500
I think that's pretty much it for me.


01:15:51.500 --> 01:15:57.540
I'd say probably most important thing to do would just be just stick with testing.


01:15:57.540 --> 01:16:02.140
Don't let flaky tests put you off test driven development or anything like that.


01:16:02.140 --> 01:16:05.180
because it's better than not testing.


01:16:05.180 --> 01:16:07.500
Yeah, indeed.


01:16:07.500 --> 01:16:07.860
All right.


01:16:07.860 --> 01:16:08.540
Well, thanks guys.


01:16:08.540 --> 01:16:09.260
Thanks for being on the show.


01:16:09.260 --> 01:16:10.380
Thank you.

