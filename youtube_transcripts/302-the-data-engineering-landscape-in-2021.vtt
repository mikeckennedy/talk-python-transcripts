WEBVTT

00:00:00.000 --> 00:00:05.000
(silence)


00:00:05.000 --> 00:00:09.000
The Talk Python to Me podcast live stream will be starting in just a moment.


00:00:09.000 --> 00:00:14.000
Thank you so much for joining us today. At Talk Python, our goal is to bring Python stories to life.


00:00:14.000 --> 00:00:17.000
We have another great one queued up for you today.


00:00:17.000 --> 00:00:22.000
Please put feedback and questions in the chat, and when possible, we'll feature your comment on the show.


00:00:22.000 --> 00:00:26.000
I'm your host, Michael Kennedy. My guest and I will be on the screen shortly.


00:00:26.000 --> 00:00:29.000
(silence)


00:00:29.000 --> 00:00:32.000
Hello, everyone. Thanks for joining us on the live show.


00:00:32.000 --> 00:00:36.000
And like I said, put some thoughts and ideas in the comments.


00:00:36.000 --> 00:00:40.000
Tobias Macy, my guest here, and I, we will try to make that part of the show.


00:00:40.000 --> 00:00:43.000
So without further ado, Tobias, ready to kick it off?


00:00:43.000 --> 00:00:46.000
Yeah, sounds good. Thanks for having me on, Mike.


00:00:46.000 --> 00:00:49.000
Yeah. Great to have you here.


00:00:49.000 --> 00:00:55.000
It's good to have you back. I was recently looking at my podcast page here,


00:00:55.000 --> 00:00:59.000
and it says you were on the show 68, which is a lot of fun.


00:00:59.000 --> 00:01:04.000
That was when Chris Patti was with you as well, around podcasting it.


00:01:04.000 --> 00:01:07.000
But boy, that was 2016.


00:01:07.000 --> 00:01:09.000
Yeah, it's been a while.


00:01:09.000 --> 00:01:12.000
We've been at this a while. I mean, ironically, we started within a week of each other.


00:01:12.000 --> 00:01:15.000
But yeah, we're still going, both of us.


00:01:15.000 --> 00:01:21.000
Yeah, it's definitely been a fun journey and a lot of great sort of unexpected benefits


00:01:21.000 --> 00:01:24.000
and great people that I've been able to meet as a result of it.


00:01:24.000 --> 00:01:27.000
So definitely glad to be on the journey with you.


00:01:27.000 --> 00:01:32.000
Yeah, same here. Podcasting opened doors like nothing else. It's crazy.


00:01:32.000 --> 00:01:35.000
People who wouldn't normally want to talk to you are like, "Hey, you want to be on the show?"


00:01:35.000 --> 00:01:38.000
Yeah, let's spend an hour together all of a sudden, right? It's fantastic.


00:01:38.000 --> 00:01:42.000
So what's new since 2016? What have you been up to?


00:01:42.000 --> 00:01:45.000
So definitely a number of things.


00:01:45.000 --> 00:01:49.000
I mean, one being that I actually ended up going solo as the host.


00:01:49.000 --> 00:01:53.000
So I've been running the Podcast in It show by myself.


00:01:53.000 --> 00:01:58.000
I don't remember exactly when it happened, but I think probably sometime around 2017.


00:01:58.000 --> 00:02:02.000
I know around the same time that I was on your show, you were on mine.


00:02:02.000 --> 00:02:04.000
So we kind of flip-flopped.


00:02:04.000 --> 00:02:10.000
And then you've been on the show again since then, talking about your experience working with MongoDB and Python.


00:02:10.000 --> 00:02:14.000
Beyond that, I also ended up starting a second podcast.


00:02:14.000 --> 00:02:18.000
So I've got Podcast in It, which focuses on Python and its community.


00:02:18.000 --> 00:02:24.000
So a lot of stuff about DevOps, data science, machine learning, web development, you name it.


00:02:24.000 --> 00:02:27.000
Anything that people are doing with Python, I've had them on.


00:02:27.000 --> 00:02:31.000
But I've also started a second show focused on data engineering.


00:02:31.000 --> 00:02:36.000
So going beyond just the constraints of Python into a separate niche.


00:02:36.000 --> 00:02:41.000
So more languages, but more tightly focused problem domain.


00:02:41.000 --> 00:02:45.000
And so I've been enjoying learning a lot more about the area of data engineering.


00:02:45.000 --> 00:02:52.000
And so it's actually been a good companion to the two where there's a lot of data science that happens in Python.


00:02:52.000 --> 00:02:55.000
So I'm able to cover that side of things on Podcast.init.


00:02:55.000 --> 00:03:01.000
And then data engineering is all of the prep work that makes data scientists' lives easier.


00:03:01.000 --> 00:03:06.000
And so just learning a lot about the technologies and challenges that happen on that side of things.


00:03:06.000 --> 00:03:07.000
Yeah, that's super cool.


00:03:07.000 --> 00:03:12.000
And to be honest, one of the reasons I invited you on the show is because I know people talk about data engineering.


00:03:12.000 --> 00:03:19.000
And I'm like, I know there's neat tools and they feel like they come out of the data science space, but not exactly.


00:03:19.000 --> 00:03:23.000
And so I'm really looking forward to learning about them along with everyone else listening.


00:03:23.000 --> 00:03:24.000
So it's going to be a lot of fun.


00:03:24.000 --> 00:03:25.000
Absolutely.


00:03:25.000 --> 00:03:26.000
Yeah.


00:03:26.000 --> 00:03:31.000
Before we dive into that, you just let people maybe know, what do you do in day to day these days?


00:03:31.000 --> 00:03:34.000
Are you doing consulting or you got a full time job?


00:03:34.000 --> 00:03:35.000
What's the plan?


00:03:35.000 --> 00:03:36.000
Yes.


00:03:36.000 --> 00:03:38.000
Yes to all of that.


00:03:39.000 --> 00:03:44.000
So, yeah, I mean, I run the podcast as a side just sort of hobby.


00:03:44.000 --> 00:03:54.000
And for my day to day, I actually work full time at MIT in the open learning department and help run the platform engineering and data engineering team there.


00:03:54.000 --> 00:04:02.000
So responsible for making sure that all the cloud environments are set up and secured and servers are up and running and applications stay available.


00:04:02.000 --> 00:04:18.000
And working through building out a data platform to provide a lot of means for analytics and gaining insights into the learning habits and the behaviors that global learners have and how they interact with all of the different platforms that we run.


00:04:18.000 --> 00:04:20.000
That's fantastic.


00:04:20.000 --> 00:04:22.000
Yeah, it's definitely a great place to work.


00:04:22.000 --> 00:04:24.000
I've been happy to be there for a number of years now.


00:04:24.000 --> 00:04:26.000
And then, you know, I run the podcast.


00:04:26.000 --> 00:04:28.000
So those go out every week.


00:04:28.000 --> 00:04:32.000
So a lot of stuff that happens behind the scenes there.


00:04:32.000 --> 00:04:42.000
And then I also do some consulting where lately it's been more of the advisory type where it used to be I'd be hands on keyboard, but I've been able to level up beyond that.


00:04:42.000 --> 00:04:48.000
And so I've been working with a couple of venture capital firms to help them understand the data ecosystem.


00:04:48.000 --> 00:04:51.000
So data engineering, data science.


00:04:51.000 --> 00:05:15.000
I've also worked a little bit with a couple of businesses just helping them understand sort of what are the challenges and what's the potential in the data marketplace and data ecosystem to be able to go beyond just having an application and then being able to use the information and the data that they gather from that to be able to build more interesting insights into their business, but also products for their customers.


00:05:15.000 --> 00:05:17.000
Oh, yeah, that sounds really fun.


00:05:17.000 --> 00:05:19.000
MIT sounds amazing.


00:05:19.000 --> 00:05:31.000
And then those advisory roles are really neat because you kind of get a take, especially as a podcaster, you get this broad view because you talk to so many people and, you know, they've got different situations and different contexts.


00:05:31.000 --> 00:05:34.000
And so you can say, all right, look, here's kind of what I see.


00:05:34.000 --> 00:05:35.000
You seem to fit into this realm.


00:05:35.000 --> 00:05:38.000
And so this might be the right path.


00:05:38.000 --> 00:05:39.000
Absolutely.


00:05:39.000 --> 00:05:44.000
Yeah, I mean, it's the data ecosystem in particular is very fast moving.


00:05:44.000 --> 00:05:49.000
It's definitely very difficult to be able to keep abreast with all the different aspects of it.


00:05:49.000 --> 00:06:18.000
And so because that's kind of my job as a podcaster, I'm able to dig deep into various areas of it and be able to learn from and take advantage of all of the expertise and insight that these various, you know, innovators and leaders in the space have and kind of synthesize that because I'm talking to people across the storage layer to the data processing layer and orchestration and analytics and, you know, machine learning and operationalizing all of that.


00:06:18.000 --> 00:06:30.000
Whereas if I were one of the people who's deep in the trenches, it's you get a very detailed but narrow view, whereas I've got a very shallow and broad view across the whole ecosystem.


00:06:30.000 --> 00:06:33.000
Which is the perfect match for the high level view, right?


00:06:33.000 --> 00:06:34.000
Exactly.


00:06:34.000 --> 00:06:35.000
Nice.


00:06:35.000 --> 00:06:38.000
All right, well, let's jump into our main topic.


00:06:38.000 --> 00:06:44.000
And we touched on it a little bit, but, you know, I know what data science is, I think.


00:06:44.000 --> 00:06:55.000
And there's a really interesting interview I did with Emily and Jacqueline, I don't remember both their last names, recently, but about building a career in data science.


00:06:55.000 --> 00:07:05.000
And they talked about basically three areas of data science that you might be in, like production and machine learning versus like making predictions and so on.


00:07:05.000 --> 00:07:14.000
And data engineering, it feels like it's kind of in that data science realm, but it's not exactly that, like it could kind of be databases and other stuff too, right?


00:07:14.000 --> 00:07:20.000
Like, what is this data engineering thing, maybe compare and contrast against data science?


00:07:20.000 --> 00:07:22.000
People probably know that pretty well.


00:07:22.000 --> 00:07:30.000
Yeah, so it's one of those kind of all encompassing terms that, you know, the role depends on the organization that you're in.


00:07:30.000 --> 00:07:38.000
So in some places, the data engineer might just be the person who, you know, used to be the DBA or the database administrator.


00:07:38.000 --> 00:07:42.000
And other places, they might be responsible for cloud infrastructure.


00:07:42.000 --> 00:07:46.000
And another place, they might be responsible for maintaining streaming systems.


00:07:46.000 --> 00:08:00.000
There are one way that I've seen it broken down as kind of two sort of broad classifications of data engineering is there's the SQL focused data engineer, where they might have a background as a database administrator.


00:08:00.000 --> 00:08:14.000
And so they do a lot of work in managing the data warehouse, they work with SQL oriented tools, where there are a lot of them coming out now where you can actually use SQL for being able to pull data from source systems into the data warehouse and then provide,


00:08:14.000 --> 00:08:20.000
you know, build transformations to provide to analysts and data scientists.


00:08:20.000 --> 00:08:35.000
And then there is the more engineering oriented data engineer, which is somebody who writes a lot of software, they're building complex infrastructure and architectures using things like Kafka or Flink or Spark.


00:08:35.000 --> 00:08:42.000
They're working with the database, they're working with data orchestration tools like Airflow or Dagster or Prefect.


00:08:42.000 --> 00:08:44.000
They might be using Dask.


00:08:44.000 --> 00:08:50.000
And so they're much more focused on actually writing software and delivering code as the output of their efforts.


00:08:50.000 --> 00:09:09.000
But a shared context across whatever, however you define data engineering, the shared aspect of it is that they're all working to bring data from multiple locations into a place that is accessible for various end users,


00:09:09.000 --> 00:09:14.000
might be analysts or data scientists or the business intelligence tools.


00:09:14.000 --> 00:09:27.000
And they're tasked with making sure that those workflows are repeatable and maintainable and that the data is clean and organized so that it's useful because, you know, everybody knows the whole garbage in garbage out principle.


00:09:27.000 --> 00:09:38.000
If you're a data scientist and you don't have all the context of where the data is coming from, you just have a small, narrow scope of what you need to work with.


00:09:38.000 --> 00:09:41.000
You're kind of struggling with that garbage in garbage out principle.


00:09:41.000 --> 00:09:46.000
And so the data engineer's job is to get rid of all the garbage and give you something clean that you can work from.


00:09:46.000 --> 00:09:50.000
I think that's really a tricky problem in the data science side of things.


00:09:50.000 --> 00:09:58.000
You know, you take your data, you run it through a model or through some analysis graphing layer and it gives you a picture.


00:09:58.000 --> 00:10:00.000
And you're like, well, that's the answer.


00:10:00.000 --> 00:10:01.000
Maybe, maybe it is.


00:10:01.000 --> 00:10:03.000
Did you give it the right input?


00:10:03.000 --> 00:10:05.000
Did you train the models in the right data?


00:10:05.000 --> 00:10:07.000
Who knows? Right?


00:10:07.000 --> 00:10:08.000
Yeah.


00:10:08.000 --> 00:10:10.000
And that's definitely a big challenge.


00:10:10.000 --> 00:10:20.000
And that's one of the reasons why data engineering has become so multifaceted is because what you're doing with the data informs the ways that you prepare the data.


00:10:20.000 --> 00:10:32.000
You know, you need to make sure that you have a lot of the contextual information as well to make sure that the data scientists and data analysts are able to answer the questions accurately because data in isolation.


00:10:32.000 --> 00:10:36.000
You know, if you just give somebody the number five, it's completely meaningless.


00:10:36.000 --> 00:10:42.000
But if you tell them that a customer ordered five of this unit, well, then now you can actually do something with it.


00:10:42.000 --> 00:10:52.000
So the context helps to provide the information about, you know, just the isolated number and understanding where it came from and why it's important.


00:10:52.000 --> 00:10:53.000
Yeah, absolutely.


00:10:53.000 --> 00:10:58.000
You know, two things come to mind when I hear data engineering for me is like one is like pipelines of data.


00:10:58.000 --> 00:11:01.000
Maybe you've got to bring in data and do transformations to it to get it ready.


00:11:01.000 --> 00:11:10.000
This is part of that data cleanup maybe and taking disparate sources and unifying them under one canonical model or something in representation.


00:11:10.000 --> 00:11:21.000
And then ETL, I kind of like, you know, we get something terrible like FTP uploads of CSV files and we've got to turn those into databases like overnight jobs, right?


00:11:21.000 --> 00:11:24.000
Things like that, which probably still exist.


00:11:24.000 --> 00:11:26.000
They existed not too long ago.


00:11:26.000 --> 00:11:37.000
Yeah, every sort of legacy technology that you think has gone away because you're not working with it anymore is still in existence somewhere, which is why we still have COBOL.


00:11:37.000 --> 00:11:39.000
Exactly. Oh, my gosh.


00:11:39.000 --> 00:11:46.000
I've got some crazy, crazy COBOL stories for you that probably shouldn't go out public.


00:11:46.000 --> 00:11:49.000
But, you know, ask me, ask me over the next conference.


00:11:49.000 --> 00:11:51.000
The next time we get to travel somewhere, you know?


00:11:51.000 --> 00:11:52.000
All right. Sounds good.


00:11:52.000 --> 00:11:54.000
Yeah, for sure.


00:11:54.000 --> 00:11:59.000
So let's talk about trends, you know, because I made that joke, right?


00:11:59.000 --> 00:12:07.000
Like, well, maybe used to be CSV files or text files and FTP and then a job that would put that into a SQL database or some kind of relational database.


00:12:07.000 --> 00:12:09.000
What is it now?


00:12:09.000 --> 00:12:11.000
It's got to be better than that, right?


00:12:11.000 --> 00:12:13.000
I mean, again, depends where you are.


00:12:13.000 --> 00:12:15.000
I mean, CSV files are still a thing.


00:12:15.000 --> 00:12:17.000
You know, it might not be FTP anymore.


00:12:17.000 --> 00:12:21.000
It's probably going to be living in object storage like S3 or Google Cloud Storage.


00:12:21.000 --> 00:12:26.000
But, you know, you're still working with individual files in some places.


00:12:26.000 --> 00:12:35.000
A lot of it is coming from APIs or databases where you might need to pull all of the information from Salesforce to get your CRM data.


00:12:35.000 --> 00:12:39.000
Or you might be pulling data out of Google Analytics by their API.


00:12:39.000 --> 00:12:43.000
You know, there are a lot of evolutionary trends that have happened.


00:12:43.000 --> 00:12:52.000
I mean, the sort of first big movement in data engineering beyond just the sort of, well, there have been a few generations.


00:12:52.000 --> 00:13:01.000
So the first generation was the data warehouse where you took a database appliance, whether that was Oracle or Microsoft SQL Server or Postgres.


00:13:01.000 --> 00:13:09.000
You put all of your data into it, and then you had to do a lot of work to model it so that you could answer questions about that data.


00:13:09.000 --> 00:13:15.000
So in an application database, you're liable to just overwrite a record when something changes.


00:13:15.000 --> 00:13:22.000
Where in a data warehouse, you want that historical information about what changed and the evolution of that data.


00:13:22.000 --> 00:13:24.000
What about like normalization?


00:13:24.000 --> 00:13:29.000
You know, in operational databases, it's all about one source of truth.


00:13:29.000 --> 00:13:31.000
We better not have any duplication.


00:13:31.000 --> 00:13:33.000
It's fine if there's four joins to get there.


00:13:33.000 --> 00:13:40.000
Whereas in warehousing, it's maybe better to have that duplication so you can run different types of reports real quickly and easily.


00:13:40.000 --> 00:13:48.000
Exactly. Yeah. I mean, you still need to have one source of truth, but you will model the tables differently than in an application database.


00:13:48.000 --> 00:13:54.000
So there are things like the star schema or the snowflake schema that became popular in the initial phase of data warehousing.


00:13:54.000 --> 00:14:01.000
So Ralph Kimball is famous for building out the sort of star schema approach with facts and dimensions.


00:14:01.000 --> 00:14:06.000
Yeah, maybe describe that a little bit for people because maybe they don't know these terms.


00:14:06.000 --> 00:14:16.000
Sure. So facts are things like, you know, a fact is Tobias Macy works at MIT.


00:14:16.000 --> 00:14:22.000
And then a dimension might be he was hired in 2016 or whatever year it was.


00:14:22.000 --> 00:14:31.000
And, you know, now another dimension of it is he, you know, his work anniversary is X date.


00:14:31.000 --> 00:14:36.000
And so the way that you model it makes it so a fact is something that's immutable.


00:14:36.000 --> 00:14:41.000
And then a dimension or things that might evolve over time.


00:14:41.000 --> 00:14:54.000
And then in sort of the next iteration of data engineering and data management was the sort of quote unquote big data craze where Google released their paper about MapReduce.


00:14:54.000 --> 00:15:00.000
And so Hadoop came out as a open source option for that. And so everybody said, oh, it's going to take over the world. Right.


00:15:00.000 --> 00:15:02.000
Like that was the only way you could do anything.


00:15:02.000 --> 00:15:13.000
If you had big data, then you had to MapReduce it. And then maybe it had to do with one of these large scaled out databases like Spark or Cassandra or who knows something like that.


00:15:13.000 --> 00:15:19.000
Yeah, I mean, Spark and Cassandra came after Hadoop. So I mean, Hadoop was your option in the early 2000s.


00:15:19.000 --> 00:15:25.000
And so everybody said, oh, big data is the answer. If I just throw big data at everything, it'll solve all my problems.


00:15:25.000 --> 00:15:33.000
And so people built these massive data lakes using Hadoop and built these MapReduce jobs and then realized that what are we actually doing with all this data?


00:15:33.000 --> 00:15:41.000
It's costing us more money than it's worth. MapReduce jobs are difficult to scale. They're difficult to understand the order of dependencies.


00:15:41.000 --> 00:15:51.000
And so that's when things like Spark came out to use the data that you were already collecting, but be able to parallelize the operations and run it a little faster.


00:15:51.000 --> 00:16:04.000
And so that was sort of the era of batch oriented workflows. And then with the advent of things like Spark Streaming and Kafka, and there are a whole number of other tools out there now like Flink and Pulsar.


00:16:04.000 --> 00:16:13.000
The sort of real time revolution is where we're at now, where it's not enough to be able to understand what happened the next day.


00:16:13.000 --> 00:16:28.000
You have to understand what's happening within five minutes. And so there are principles like change data capture, where every time I write a new record into a database, it goes into a Kafka queue, which then gets replicated out to an Elasticsearch cluster, into my data warehouse.


00:16:28.000 --> 00:16:38.000
And so within five minutes, my business intelligence dashboard is updated with the fact that customer A bought product B, rather than having to wait 24 hours to get that.


00:16:38.000 --> 00:16:50.000
I think that makes tons of sense. So instead of going like, we're just going to pile the data into this, you know, some sort of data lake type thing, then we'll grab it. And we'll do our reports nightly or hourly or whatever.


00:16:50.000 --> 00:16:56.000
You just keep pushing it down the road as it comes in or as it's generated, right?


00:16:56.000 --> 00:17:04.000
Right. Yeah. So I mean, there are still use cases for batch. I mean, you know, and there are different ways of looking at it.


00:17:04.000 --> 00:17:14.000
So I mean, a lot of people view batch as just a special case of streaming where, you know, streaming is the sort of micro batches where as a record comes in, you operate on it.


00:17:14.000 --> 00:17:19.000
And then for large batch jobs, you're just doing the same thing, but multiple times for a number of records.


00:17:19.000 --> 00:17:26.000
Yeah. So I mean, there are a lot of paradigms that are building up. People are getting used to the idea. I mean, batch is still the easier thing to implement.


00:17:26.000 --> 00:17:37.000
It requires fewer moving pieces, but streaming the sort of availability of different technologies is making it more feasible for more people to be able to actually take advantage of that.


00:17:37.000 --> 00:17:42.000
And so there are, you know, managed platforms that help you with that problem.


00:17:42.000 --> 00:17:49.000
Yeah, there's whole platforms now that are basically, yeah, there's whole platforms that are just around to just do data streaming for you, right?


00:17:49.000 --> 00:18:00.000
Just like sort of manage that and keep that alive. And with the popularization of webhooks, right, it's easy to say if something changes here, you know, notify this other thing and that thing can call other things.


00:18:00.000 --> 00:18:03.000
And it seems like it's coming along. Yeah.


00:18:03.000 --> 00:18:16.000
Yeah. I mean, one of the interesting aspects, too, of a lot of the work that's been going into the data engineering space is that you're starting to see some of the architectural patterns and technologies move back into the application development domain


00:18:16.000 --> 00:18:31.000
where a lot of applications, particularly if you're working with microservices, will use something like a Kafka or a Pulsar queue as the communication layer for being able to propagate information across all the different decoupled applications.


00:18:31.000 --> 00:18:37.000
And that's the same technology and same architectural approaches that are being used for these real time data pipelines.


00:18:37.000 --> 00:18:48.000
Yeah. Man, aren't queues amazing for adding scale to systems, right? And if this is going to take too long, throw it in a queue and let the thing crank on over 30 seconds, it'll be good.


00:18:48.000 --> 00:19:03.000
Absolutely. I mean, Celery is, you know, the same idea, it's just a smaller scale. And so, you know, RabbitMQ, it's more ephemeral, whereas when you're putting it into these durable queues, you can do more with the information where you can rewind time to be able to say,


00:19:03.000 --> 00:19:13.000
"Okay, I changed my logic, I now want to reprocess all of these records from the past three months." Whereas if you had that on RabbitMQ, all of those records are gone unless you wrote them out somewhere else.


00:19:13.000 --> 00:19:21.000
Yeah, for sure. So a couple comments from the live stream, Defria says, "Airflow, Apache Airflow is really cool." For sure, we're going to talk about that.


00:19:21.000 --> 00:19:37.000
But I did want to ask you about the cloud. Stefan says, "I'm a little bit skeptical about the privacy and security on the cloud, so kind of want to use the zone server more often." So maybe that's a trend that you could speak to that you've seen with folks you've interviewed.


00:19:37.000 --> 00:19:55.000
This kind of data is really sensitive, sometimes, and people are very protective of it or whatever, right? So what is the cloud story versus, "Oh, we got to do this all on-prem," or maybe even some hybrid thereof?


00:19:55.000 --> 00:20:20.000
Right. So I mean, it's definitely an important question and something that is... it's a complicated problem. There are ways to solve it. I mean, data governance is kind of the umbrella term that's used for saying, "I want to keep control of my data and make sure that I am using the appropriate regulatory aspects and making sure that I am, you know, filtering out private information or encrypting data at rest, encrypting data in transit."


00:20:20.000 --> 00:20:31.000
So there are definitely ways that you can keep tight control over your data, even when you're in the cloud. And a lot of the cloud platforms have been building out capabilities to make it easier for you.


00:20:31.000 --> 00:20:48.000
So, for instance, if you're on Amazon, they have their key management service that you can use to encrypt all of your storage at rest. You can provide your own keys if you don't trust them to hold the keys to the kingdom there so that you are the person who's in control of being able to encrypt and decrypt your data.


00:20:48.000 --> 00:21:00.000
You know, there are a class of technologies used in data warehouses called privacy enhancing technologies, where you can actually have all of the rows in your database fully encrypted.


00:21:00.000 --> 00:21:17.000
And then you can encrypt the predicate of a SQL query to be able to see if the data matches the values in the database without ever actually having to decrypt anything so that you could do some rudimentary analytics like aggregates on that information so that it all stays safe.


00:21:17.000 --> 00:21:31.000
There's also a class of technologies that are still a little bit in the experimental phase called homomorphic encryption, where it actually, all of the data is never actually decrypted.


00:21:31.000 --> 00:21:46.000
So it lives in this encrypted enclave, your data processing job operates within that encrypted space. And so there's never any actual clear text information stored anywhere, not even in your computer's RAM.


00:21:46.000 --> 00:21:56.000
Wow. So if one of those like weird CPU bugs that lets you jump through the memory of different VMs, or something like that comes up, you're probably okay, right?


00:21:56.000 --> 00:22:06.000
Absolutely. Yeah, I mean, the homomorphic encryption, there are some companies out there that are offering that as a managed service, and it's becoming more viable.


00:22:06.000 --> 00:22:21.000
It's been something that's been discussed and theorized about for a long time. But because of the computational cost, it was something that was never really commercialized. But there are a lot of algorithms that have been discovered to help make it more feasible to actually use in production contexts.


00:22:21.000 --> 00:22:29.000
Yeah. I don't know about the other databases. I know MongoDB, they added some feature where you can encrypt just certain fields.


00:22:29.000 --> 00:22:46.000
Right. So maybe here's a field that is sensitive, but you don't really need to query by for your reports. But it needs to be in with, say, with a user or an order or something like that. So even even going to that part might be a pretty good step. But yeah, clouds, clouds are both amazing and scary, I suppose.


00:22:46.000 --> 00:23:00.000
Yeah, yeah, I mean, there's definitely a lot of options. It's something that requires a bit of understanding and legwork, but it's definitely possible to make sure that all your data stays secured and that you are in full control over where it's being used.


00:23:00.000 --> 00:23:22.000
Yeah. One of the next things I wanted to ask you about is languages. So you're probably familiar with this, this chart here, right? Which if people are not watching the stream, this is the Stack Overflow trend showing Python just trouncing the other languages, including Java.


00:23:22.000 --> 00:23:34.000
But I know Java had been maybe one of the main ways that probably has to do with Spark and whatnot, to some degree. What do you see Python's role relative to other technologies here?


00:23:34.000 --> 00:23:50.000
So Python has definitely been growing a lot in the data engineering space, largely because of the fact that it's so popular in data science. And so there are data scientists who have been moving further down the stack into data engineering as a requirement of their job.


00:23:50.000 --> 00:24:02.000
And so they are bringing Python into those layers of the stack. It's also being used as just a unifying language so that data engineers and data scientists can work on the same code bases.


00:24:02.000 --> 00:24:09.000
As you mentioned, Java has been popular for a long time in the data ecosystem because of things like Hadoop and Spark.


00:24:09.000 --> 00:24:27.000
And looking at the trend graph, I'd be interested to see what it looks like if you actually combine the popularities of Java and Scala, because Scala has become a strong contender in that space as well because of things like Spark and Flink that have native support for Scala.


00:24:27.000 --> 00:24:49.000
It's a bit more of an esoteric language, but it's used a lot in data processing. But Python has definitely gained a lot of ground. And also because of tools like Airflow, which was kind of the first generation tool built for data engineers by data engineers to be able to manage these dependency graphs of operations so that you can have these pipelines.


00:24:49.000 --> 00:24:58.000
To say, I need to pull data out of Salesforce and then land it into S3 and then I need to have another job that takes that data out of S3 and puts it into the database.


00:24:58.000 --> 00:25:04.000
And then also that same S3 data needs to go into an analytics job.


00:25:04.000 --> 00:25:20.000
And then once those two jobs are complete, I need to kick off another job that then runs a SQL query against the data warehouse to be able to provide some aggregate information to my sales and marketing team to say this is what your customer engagement is looking like or whatever it might be.


00:25:20.000 --> 00:25:32.000
And that was all written in Python. And also just because of the massive ecosystem of libraries that Python has for being able to interconnect across all these different systems.


00:25:32.000 --> 00:25:45.000
Data engineering at a certain level is really just a systems integration task where you need to be able to have information flowing across all of these different layers and all these different systems and get good control over it.


00:25:45.000 --> 00:25:53.000
Some of the interesting tools that have come out as a sort of generational improvement over Airflow are Dagster and Prefect.


00:25:53.000 --> 00:26:00.000
I've actually been using Dagster for my own work at MIT and been enjoying that tool. I'm always happy to dig into that.


00:26:00.000 --> 00:26:10.000
Yeah, absolutely. So let's sort of focus on those things. And one of the themes I wanted to cover is maybe the five most important packages or libraries for data engineering.


00:26:10.000 --> 00:26:22.000
You kind of hit the first one that will group together as a trifecta, right? So Airflow, Dagster and Prefect. You want to maybe tell us about those three a bit?


00:26:22.000 --> 00:26:36.000
So I personally use Dagster. I like a lot of the abstractions and the interface design that they provide. But they're all three grouped into a category of tools called sort of workflow management or data orchestration.


00:26:36.000 --> 00:26:56.000
And so the responsibility there is that you need to have a way to build these pipelines, build these DAGs or directed acyclic graphs of operations where the vertices of the graph are the data and the nodes are the jobs or the operations being performed on them.


00:26:56.000 --> 00:27:10.000
And so you need to be able to build up this dependency chain because you need to get information out of a source system, you need to get it into a target system, you might need to perform some transformations either en route or after it's been landed.


00:27:10.000 --> 00:27:23.000
One of the common trends that's happening is it used to be extract, transform and then load because you needed to have all of the information in that specialized schema for the data warehouse that we were mentioning earlier.


00:27:23.000 --> 00:27:32.000
Right. But now all the relational database databases, it's got to have these columns in this. It can't be a long character. It's got to be a VARCHAR10 or whatever. Right.


00:27:32.000 --> 00:27:48.000
Right. And then with the advent of the cloud data warehouses that have been happening in the past few years that was kicked off by Redshift from Amazon and then carried on by things like Google BigQuery, Snowflake that a lot of people will probably be aware of.


00:27:48.000 --> 00:27:57.000
There are a number of other systems and platforms out there. Presto out of Facebook that is now an open source project actually renamed to Trino.


00:27:57.000 --> 00:28:16.000
Those systems are allowing people to be very SQL oriented, but because of the fact that they're scalable and they provide more flexible data models, the trend has gone to extract, load and then transform because you can just replicate the schema as is into these destination systems and then you can perform all of your transformations in SQL.


00:28:16.000 --> 00:28:24.000
And so that brings us into another tool that is in the Python ecosystem that's been gaining a lot of ground called DBT or data build tool.


00:28:24.000 --> 00:28:49.000
And so this is a tool that actually brings data analysts and improves their skill set, makes them more self-sufficient within the organization and provides a lot of provides a great framework for them to operate in an engineering mindset where it helps to build up a specialized DAG within the context of the data warehouse.


00:28:49.000 --> 00:28:57.000
To take those source data sets that are landed into the data warehouse from the extract and load jobs and build these transformations.


00:28:57.000 --> 00:29:16.000
So you might have the user table from your application database and the orders table and then you also have the Salesforce information that's landed in a separate table and you want to be able to combine all of those to be able to understand your customer order, customer buying patterns.


00:29:16.000 --> 00:29:27.000
And so you use SQL to build either a view or build a new table out of that source information in the data warehouse and DBT will handle that workflow.


00:29:27.000 --> 00:29:34.000
It also has support for being able to build unit tests in SQL into your workflow. That's another great thing.


00:29:34.000 --> 00:29:48.000
Oh, how interesting. Yeah, that's something that you hadn't really heard very much of 10 years ago was testing in databases. It was usually how do I get the database out of the picture so I can test without depending upon it or something like that. That was the story.


00:29:48.000 --> 00:30:14.000
Yeah, that's another real growing trend is the overall aspect of data quality and confidence in your data flows. So things like in Dagster and Prefect and Airflow, they have support for being able to unit test your pipelines, which is another great aspect of the Python ecosystem as you can just write pytest code to ensure that all the operations on your data match your expectations and you don't have regressions and bugs.


00:30:14.000 --> 00:30:31.000
Right, right. Absolutely. The the complicating aspect of data engineering is that it's not just the code that you need to make sure is right, but you also need to make sure that the data is right. And so another tool that is helping in that aspect again from the Python ecosystem is great expectations.


00:30:31.000 --> 00:30:52.000
Right, so that's right in the realm of this testing your data. Yeah, exactly. So you can say, you know, I'm pulling data out of my application database, I expect the schema to have these columns in it. I expect the data distribution within this column to, you know, say, you know, the values are only going to range from zero to five.


00:30:52.000 --> 00:31:15.000
And then if I get a value outside of that range, then I can, it will fail the test and it will notify me that something's off. So you can build these very expressive and flexible expectations of what your data looks like, what your data pipeline is going to do, so that you can gain visibility and confidence into what's actually happening as you are propagating information across all these different systems.


00:31:15.000 --> 00:31:18.000
So do you make this part of your continuous integration tests?


00:31:18.000 --> 00:31:33.000
Absolutely. Yeah. So it would be part of your continuous integration as you're delivering new versions of your pipeline, but it's also something that executes in the context of the nightly batch job or of your streaming pipeline. So it's both a build time and a run time expectation.


00:31:33.000 --> 00:31:46.000
Yeah, so it's like a pre test. It's like an if test for your function, but for your data, right? Like, let's make sure everything's good before we run through this and actually drop the answer on to the dashboard for the morning or something like that. Okay.


00:31:46.000 --> 00:32:07.000
Right. And so, you know, it helps to build up that confidence because anybody who's been working in data has had the experience of, I delivered this report, I feel great about it, I'm happy that I was able to get this thing to run through and then you hand it off to your CEO or your CTO and they look at it and they say, well, this doesn't quite look right.


00:32:07.000 --> 00:32:18.000
And then you go back and realize, oh, crud, that's because I, you know, forgot to pull in this other column or whatever it is. And so this way, you can not have to have that sinking feeling in your gut when you hand off a report.


00:32:18.000 --> 00:32:25.000
That would be bad. What would be worse is we decided to invest by buying a significant position in this other company.


00:32:25.000 --> 00:32:26.000
Exactly.


00:32:26.000 --> 00:32:31.000
Oh, but it turned out, whoops, it was actually we had a negative sign. It wasn't really good for you to invest in this.


00:32:31.000 --> 00:32:32.000
Absolutely.


00:32:32.000 --> 00:32:53.000
Right. Like if it's actions have been taken. Hey, let me jump you back really quick to that language trends question real quick. So Anthony Lister asks if is R still widely used and sort of a strong competitor, let's say to Python. And what's your thoughts these days? I kind of honestly hear a little bit less of it in my world.


00:32:53.000 --> 00:33:08.000
So there are definitely a lot of languages. R is definitely one of them that's still popular in the data space. I don't really see R in the data engineering context. It's definitely still used for a lot of statistical modeling, machine learning, data science workloads.


00:33:08.000 --> 00:33:30.000
There's a lot of great interoperability between R and Python now, especially with the arrow project, which is a an in memory columnar representation that provides an interoperable in memory space where you can actually exchange data between R and Python and Java without having to do any IO copying between them.


00:33:30.000 --> 00:33:50.000
So it helps to reduce a lot of the impedance mismatch between those languages. Another language that's been gaining a lot of ground in the data ecosystem is Julia. And they're actually under the NumFocus organization that supports a lot of the Python data ecosystem.


00:33:50.000 --> 00:34:06.000
So Julia has been gaining a lot of ground, but Python, just because of its broad use, is still very popular. And there's an anecdote that I've heard a number of times, I don't remember where I first came across it, that Python isn't the best language for anything, but it's the second best language for everything.


00:34:06.000 --> 00:34:30.000
Yeah, that's a good quote. I think it does put a lot of perspective on it. I feel like it's just so approachable. And there's a lot of these languages that might make slightly more sense for a certain use case like R and statistics. But you better not want to have to build some other thing that reaches outside of what's easily possible.


00:34:30.000 --> 00:34:42.000
Right. Do you want to make that an API now? Well, all of a sudden, it's not so easy, or whatever, right? Something along those lines. All right. Next in our list here is Dask.


00:34:42.000 --> 00:35:00.000
Yeah, so Dask is a great tool. I kind of think about it as the Python version of Spark. There are a number of reasons that's not exactly accurate. But it's a tool that lets you parallelize your Python operations, scale it out into clusters.


00:35:00.000 --> 00:35:16.000
It also has a library called Dask.distributed that's used a lot for just scaling out Python independent of actually building the directed acyclic graphs in Dask. So one of the main ways that Spark is used is as an ETL engine.


00:35:16.000 --> 00:35:28.000
So you can build these graphs of tasks in Spark, you can do the same thing with Dask, it was actually built originally more for the hard sciences and for scientific workloads and not just for data science.


00:35:28.000 --> 00:35:52.000
But Dask is actually also used as a foundational layer for a number of the data orchestration tools out there. So Dask is the foundational layer for Prefect, you can use it as an execution substrate for the Dagster library, the Dagster framework, it also supports, it's also supported in Airflow as a execution layer.


00:35:52.000 --> 00:36:02.000
Then there are also a number of people who are using it as a replacement for things like Celery as just a means of running asynchronous tasks outside of the bounds of a request response cycle.


00:36:02.000 --> 00:36:21.000
So it's just growing a lot in the data ecosystem, both for data engineering and data science. And so it just provides that unified layer of being able to build your data engineering workflows and then hand that directly off into machine learning so that you don't have to jump between different systems, you can do it all in one layer.


00:36:21.000 --> 00:36:37.000
- Yeah, that's super neat. And Dask, I never really appreciated it, sort of its different levels at which you can use it, I guess I should say. When I thought about it, okay, well, this is like parallel computing for pandas or for NumPy or something like that, right?


00:36:37.000 --> 00:36:59.000
But it's also, it works well on just your single laptop, right? It'll let you run multi-core stuff locally, because Python doesn't always do that super well. And it'll even, I think it'll even do caching and stuff so it can actually work with more data than you have RAM, which is hard with just straight NumPy. But then of course, you can point it at a cluster and go crazy.


00:36:59.000 --> 00:37:20.000
- Exactly, yeah. And because of the fact that it has those transparent API layers for being able to swap out the upstream pandas with the Dask pandas library and NumPy, it's easy to go from working on your laptop to just changing an import statement and now you're scaling out across a cluster of hundreds of machines.


00:37:20.000 --> 00:37:40.000
- Yeah, that's pretty awesome, actually. And maybe that has something as well to do with the batch to real time, right? If you've got to run it on one core on one machine, it's a batch job. If you can run it on the entire cluster that's sitting around idle, well then all of a sudden it's real time.


00:37:40.000 --> 00:37:57.000
- Right. Yeah, there's a lot of interesting real time stuff. There's an interesting project, sort of a side note here called Wallaroo that's built for building stateful stream processing jobs using Python. And interestingly, it's actually implemented in a language called Pony.


00:37:57.000 --> 00:37:59.000
- Pony? Okay.


00:37:59.000 --> 00:38:18.000
- It's an interesting project, levels up your ability to scale out the speed of execution and the sort of just being able to build these complex pipelines, real time jobs without having to build all of the foundational layers of it.


00:38:18.000 --> 00:38:22.000
- Oh, okay. Interesting. I have not heard of this one. That sounds fun.


00:38:22.000 --> 00:38:35.000
- Yeah, it's not as widely known. I interviewed the creator of it on the Data Engineering Podcast a while back, but it's a tool that comes up every now and then. It's an interesting approach to it.


00:38:35.000 --> 00:38:46.000
- Yeah, right in that stream processing real time world. All right, the next one that you put on our list here is Meltano. Meltano, I got to say it right, yeah.


00:38:46.000 --> 00:39:02.000
- Yeah, so that one is an interesting project. It came from the GitLab folks. It's still supported by them. And in its earliest stage, they actually wanted it to be the full end to end solution for data analytics for startups.


00:39:02.000 --> 00:39:13.000
So Meltano is actually an acronym for, if I can remember correctly, Model, Extract, Load, Transform, Analyze, Notebook, and Orchestrate.


00:39:13.000 --> 00:39:19.000
- Okay, yeah, that's quite a wild one to put into something that can say well.


00:39:19.000 --> 00:39:34.000
- Exactly. And about a year, year and a half ago now, they actually decided that they were being a little too ambitious and trying to boil the ocean and scoped it down to doing the extract and load portions of the workflow really well.


00:39:34.000 --> 00:39:45.000
Because it's a very underserved market where you would think that given the amount of data we're all working with, you know, data integration, point to point data integration and extract and load would be a solved problem, easy to do.


00:39:45.000 --> 00:39:57.000
But there's a lot of nuance to it. And there isn't really one easy thing to say, yes, that's the tool you want to use all the time. And so, you know, there are some paid options out there that are good.


00:39:57.000 --> 00:40:14.000
- Meltano is aiming to be the default open source answer for data integration. And so it's building on top of the Singer specification, which is sort of an ecosystem of libraries that was built by a company called Stitch Data.


00:40:14.000 --> 00:40:26.000
But the idea is that you have the what they call taps and targets where a tap will tap into a source system, pull data out of it, and then the targets will load that data into a target system.


00:40:26.000 --> 00:40:37.000
And they have this interoperable specification that's JSON based, so that you can just wire together any two taps and targets to be able to pull data from a source into a destination system.


00:40:37.000 --> 00:40:48.000
- Nice. - Which, yeah, it's definitely a well designed specification. A lot of people like it. There's some issues with the way that the ecosystem was sort of created and fostered.


00:40:48.000 --> 00:40:56.000
So there's a lot of uncertainty or like variability in terms of the quality of the implementations of these taps and targets.


00:40:56.000 --> 00:41:06.000
And there was never really one cohesive answer to this is how you run these in a production context, partially because Stitch Data was the answer to that.


00:41:06.000 --> 00:41:12.000
So they wanted you to buy into this open source ecosystem so that you would then use them as the actual execution layer.


00:41:12.000 --> 00:41:24.000
And so Meltano is working to build an open source option for you to be able to wire together these taps and targets and be able to just have an easy out of the box data integration solution.


00:41:24.000 --> 00:41:31.000
So, you know, it's a small team from GitLab, but there's a large and growing community helping to support it.


00:41:31.000 --> 00:41:36.000
And they've actually been doing a lot to help push forward the state of the art for the Singer ecosystem,


00:41:36.000 --> 00:41:47.000
building things like a starter template for people building taps and targets so that there's a common baseline of quality built into these different implementations


00:41:47.000 --> 00:41:53.000
without having to wonder about, you know, is this tap going to support all of the features of the specification that I need?


00:41:53.000 --> 00:41:56.000
Nice. Is this actually from GitLab?


00:41:56.000 --> 00:42:07.000
Yeah, so it's sponsored by GitLab. It's the source code is within the GitLab organization on GitLab.com, but it's definitely a very community driven project.


00:42:07.000 --> 00:42:16.000
Yeah. Yeah, Stefan is quite excited about the open source default choice. Yeah.


00:42:16.000 --> 00:42:23.000
Well, I think there's two things. One, open source is amazing. But two, you get this paradox of choice. Right? Right.


00:42:23.000 --> 00:42:28.000
It's like, well, it's great. You can have anything. But there's so many things and I'm new to this. What do I do?


00:42:28.000 --> 00:42:34.000
Right. And so, yeah, Meltano is trying to be the answer to, you know, you just Meltano in it.


00:42:34.000 --> 00:42:38.000
You have a project. You say, I want these sources and destinations.


00:42:38.000 --> 00:42:47.000
And then it will help you handle things like making sure that the jobs run on a schedule, handling, tracking the state of the operations,


00:42:47.000 --> 00:42:56.000
because you can do either full extracts and loads every time or you can do incremental because you don't necessarily want to dump a 4 million line source table every single time it runs.


00:42:56.000 --> 00:43:02.000
You just want to pull the 15 lines that changed since the last operation. So it will help track that state for you.


00:43:02.000 --> 00:43:06.000
Oh, that's why we'll try to be real efficient and just get what it needs.


00:43:06.000 --> 00:43:14.000
Yeah. And it builds in some of the monitoring information that you want to be able to see as far as like execution time, performance of these jobs.


00:43:14.000 --> 00:43:20.000
And it actually out of the box will use Airflow as the orchestration engine for being able to manage these schedules.


00:43:20.000 --> 00:43:28.000
But everything is pluggable. So if you wanted to write your own implementation that will use Dagster as the orchestrator instead, then they'll do that.


00:43:28.000 --> 00:43:41.000
There's actually a ticket in their tracker for doing that work. So it's very pluggable, very flexible, but gives you a lot of out of the box answers to being able to just get something up and running quickly.


00:43:41.000 --> 00:43:52.000
And it looks like you can build custom loaders and custom extractors. So if you've got some internal API, that's who knows, maybe it's a SOAP XML endpoint or some random thing.


00:43:52.000 --> 00:44:00.000
You could exactly. Yeah. And they actually lean on DBT and other tool that we were just talking about as the transformation layer.


00:44:00.000 --> 00:44:09.000
So they hook directly into that so that you can very easily do the extract and load and then jump into DBT for doing the transformations.


00:44:09.000 --> 00:44:13.000
Yeah. Now, you didn't put this one on the list, but I do want to ask you about it.


00:44:13.000 --> 00:44:19.000
What's the story of something like Zapier in this whole, you know, get notified about these changes, push stuff here.


00:44:19.000 --> 00:44:26.000
I mean, it feels like if you are trying to wire things together, I've seen more than one Python developer reach for Zapier.


00:44:26.000 --> 00:44:33.000
Yeah. So Zapier is definitely a great platform, particularly for doing these event based workflows.


00:44:33.000 --> 00:44:39.000
You can use it as a data engineering tool if you want, but it's not really what it's designed for.


00:44:39.000 --> 00:44:51.000
It's more just for business automation aspects or maybe, you know, automation of my application did this thing and now I want to have it replicate some of that state out to a third party system.


00:44:51.000 --> 00:44:59.000
Zapier isn't really meant for the sort of full scale data engineering workflows, maintaining visibility.


00:44:59.000 --> 00:45:03.000
It's more just for this evented IO kind of thing.


00:45:03.000 --> 00:45:12.000
Yeah. So here on the Multano, it says pipelines are code ready to be version controlled and containerized and deployed continuously.


00:45:12.000 --> 00:45:21.000
The CI/CD side sounds pretty interesting, right? Especially with these workflows that might be like in flight while you're making changes.


00:45:21.000 --> 00:45:23.000
How does that work? Do you know?


00:45:23.000 --> 00:45:30.000
Yeah, so it's basically the point with Multano is that everything is versioned in Git.


00:45:30.000 --> 00:45:43.000
So that's another movement that's been happening in the data engineering ecosystem where early on, a lot of the people coming to it were systems administrators, database administrators, maybe data scientists who had a lot of the domain knowledge,


00:45:43.000 --> 00:45:50.000
but not as much of the engineering expertise to be able to build these workflows in a highly engineered, highly repeatable way.


00:45:50.000 --> 00:46:05.000
And the past few years has been seeing a lot of movement of moving to data ops and ML ops to make sure that all of these workflows are well engineered, well managed, version controlled, tested.


00:46:05.000 --> 00:46:17.000
And so having this DevOps oriented approach to data integration is what Multano is focusing on saying, all of your configuration, all of your workflows, it lives in Git.


00:46:17.000 --> 00:46:20.000
You can run it through your CI/CD pipeline to make sure that it's tested.


00:46:20.000 --> 00:46:31.000
And then when you deliver it, you know that you can trust that it's going to do what you want it to do rather than, you know, I just pushed this config from my laptop and hopefully it doesn't blow up.


00:46:31.000 --> 00:46:48.000
Right. It also sounds like there's a lot of interplay between these things like Multano might be leveraging Airflow and dbt. Maybe you want to test this through CI with great expectations before it goes through its CD side, like continuous deployment.


00:46:48.000 --> 00:46:49.000
Exactly.


00:46:49.000 --> 00:46:52.000
It seems like there's just a lot of interflow here.


00:46:52.000 --> 00:47:06.000
Yeah, definitely. And there have been a few times where I've been talking to people and they've asked me to kind of categorize different tools or like draw nice lines about what are the dividing layers of the different of the data stack.


00:47:06.000 --> 00:47:11.000
And it's not an easy answer because so many of these tools fit into a lot of different boxes.


00:47:11.000 --> 00:47:28.000
So, you know, Spark is a streaming engine, but it's also an ELT tool. And, you know, Dagster is a data orchestration tool, but it can also be used for, you know, managing delivery.


00:47:28.000 --> 00:47:35.000
You can write it to do arbitrary tasks so you can build up these chains of tasks. So if you wanted to use it for you CI/CD, you could.


00:47:35.000 --> 00:47:49.000
It's not quite what it's built for, but, you know, and then different databases have been growing a lot of different capabilities where, you know, it used to be you had your SQL database or you had your document database or you had your graph database.


00:47:49.000 --> 00:47:58.000
And then you have things like ArangoDB, which can be a graph database and a document database and a SQL database all on the same engine.


00:47:58.000 --> 00:48:00.000
So there's a lot of multi-model databases.


00:48:00.000 --> 00:48:02.000
All of the SQL and all the NoSQL all in one.


00:48:02.000 --> 00:48:07.000
Right. And, you know, JSON is being pushed into relational databases and data warehouses.


00:48:07.000 --> 00:48:12.000
So there's a lot of crossover between the different aspects of the data stack.


00:48:12.000 --> 00:48:19.000
Yeah, there's probably more of that, I would say, in this like data warehousing stuff.


00:48:19.000 --> 00:48:25.000
You know, in an operational database, it doesn't necessarily make a ton of sense to jam JSON blobs all over the place.


00:48:25.000 --> 00:48:27.000
You might as well just make tables and columns.


00:48:27.000 --> 00:48:34.000
That makes sense, but not that much. But in this space, you might get a bunch of things you don't really know what their shape is or you're not ready to process it.


00:48:34.000 --> 00:48:37.000
You just want to save it and then try to deal with it later.


00:48:37.000 --> 00:48:40.000
So do you see more of that, those kind of JSON columns or more?


00:48:40.000 --> 00:48:48.000
Absolutely. I mean, basically any data warehouse worth its salt these days has to have some sort of support for nested data.


00:48:48.000 --> 00:48:55.000
So a lot of that, too, comes out of the outgrowth of, you know, we had the first generation data warehouses.


00:48:55.000 --> 00:48:59.000
They did their thing, but they were difficult to scale and they were very expensive.


00:48:59.000 --> 00:49:05.000
And you had to buy these beefy machines so that you were planning for the maximum capacity that you're going to have.


00:49:05.000 --> 00:49:09.000
And then came things like Hadoop, where you said, oh, you can scale out as much as you want.


00:49:09.000 --> 00:49:11.000
Just add more machines. They're all commodity.


00:49:11.000 --> 00:49:17.000
And so that brought in the the area, the ecosystem for doing MapReduce jobs on that.


00:49:17.000 --> 00:49:20.000
And then that became the next generation data lake.


00:49:20.000 --> 00:49:29.000
And then things like Presto came along to be able to build a data warehouse interface on top of this distributed data and these various data sources.


00:49:29.000 --> 00:49:38.000
And then you had the dedicated data warehouses built for the cloud where they were designed to be able to ingest data from S3,


00:49:38.000 --> 00:49:44.000
where you might have a lot of unstructured information and then you can clean it up using things like DBT to build these transformations,


00:49:44.000 --> 00:49:52.000
to have these nicely structured tables built off of this, you know, nested or messy data that you're pulling in from various data sources.


00:49:52.000 --> 00:50:00.000
Yeah, interesting. When you see the story of versioning of this, the data itself, I'm thinking,


00:50:00.000 --> 00:50:06.000
so I've got this huge pile of data I've built up and we're using to drive these pipelines.


00:50:06.000 --> 00:50:10.000
But, you know, it seems like the kind of data that could change.


00:50:10.000 --> 00:50:17.000
And I brought in a new source now that we've switched credit card providers or we're now screen scraping extra data.


00:50:17.000 --> 00:50:19.000
Do you see anything interesting happen there?


00:50:19.000 --> 00:50:23.000
Yeah, so there's definitely a lot of interesting stuff happening in the data versioning space.


00:50:23.000 --> 00:50:29.000
So, I mean, one tool that was kind of early to the party is a platform called Packaderm.


00:50:29.000 --> 00:50:38.000
They're designed as a end to end solution built on top of Kubernetes for being able to do data science and data engineering and data versioning.


00:50:38.000 --> 00:50:42.000
So your code and your data all gets versioned together.


00:50:42.000 --> 00:50:52.000
There's a system called LakeFS that was released recently that provides a Git-like workflow on top of your data that lives in S3.


00:50:52.000 --> 00:51:00.000
And so they act as a proxy to S3, but it lets you branch your data to say, I want to bring in this new data source.


00:51:00.000 --> 00:51:10.000
And as long as everything is using LakeFS as the interface, then, you know, your main branch won't see any of this new data source until you are happy with it.


00:51:10.000 --> 00:51:14.000
And then you can commit it and merge it back into the main branch and then it becomes live.


00:51:14.000 --> 00:51:21.000
And so this is a way to be able to experiment with different processing workflows to say, I want to try out this new transformation job or this new batch job.


00:51:21.000 --> 00:51:25.000
Or I want to bring in this new data source, but I'm not quite confident about it yet.


00:51:25.000 --> 00:51:28.000
And so it brings in this versioning workflow.


00:51:28.000 --> 00:51:41.000
There's another system, a combination of two tools called Iceberg, which is a table format for use in these large scale data lakes, data warehouses that hooks into things like Spark and Presto.


00:51:41.000 --> 00:51:53.000
And there's another accompanying project called Nessie that is inspired by Git for being able to do the same type of branching and merging workflow for bringing in new data sources or changing table schemas and things like that.


00:51:53.000 --> 00:51:58.000
Wow, these all sound like such fun tools to learn and they're all solving painful problems.


00:51:58.000 --> 00:52:15.000
Right. And then another one actually from the Python ecosystem is DBC or Data Version Control that's built for machine learning and data science workflows that actually integrates with your source code management so that you, you know, git commit and git push.


00:52:15.000 --> 00:52:36.000
You know, there's some additional commands, but they're modeled after git where you commit your code and then you also push your data and it lives in S3 and it will version the data assets so that as you make different versions of your experiment with different versions of your data, it all lives together so that it's repeatable and easier for multiple data scientists or data engineers to be able to collaborate on it.


00:52:36.000 --> 00:53:03.000
Cool. Yeah, the versioning, the version control story around data has always been interesting, right? It's super tricky. On one hand, your schemas might have to evolve over time. If you've got a SQLAlchemy model trying to talk to a database, it really hates it if there's a mismatch at all. Right? And so you want those things to go, the database schema, maybe to change along with your code with like migrations or something.


00:53:03.000 --> 00:53:07.000
But then the data itself. Yeah, that's tricky.


00:53:07.000 --> 00:53:33.000
Yeah. And so there's actually a tool called Avro and another one called Parquet. Well, they're tools, they're data serialization formats and everyone particular has a concept of schema evolution for, you know, what are compatible evolutions of a given schema. So each record in an Avro file has the schema co-located with it. So it's kind of like a binary version of JSON, but the schema is embedded with it.


00:53:33.000 --> 00:53:56.000
Yeah. So if you say I want to change the type of this column from an int to a float, then, you know, maybe that's a supported conversion. And so it will let you change the schemas or add columns. But if you try to change the schema in a method that is not backwards compatible, it will actually throw an error.


00:53:56.000 --> 00:54:00.000
I see. Like a float data might drop data, but a data float probably wouldn't.


00:54:01.000 --> 00:54:27.000
Exactly. So it will let you evolve your schemas. And Parquet is actually built to be interoperable with Avro for being able to handle those schema evolutions as well, where Avro is a row or record oriented format and Parquet is column oriented, which is more powerful for being able to do aggregate analytics. And it's more efficient so that you're not pulling all of the data for every row, you're just pulling all of the data for a given column. So it's also more compressible.


00:54:27.000 --> 00:54:32.000
Yeah, I think I need to do more thinking to really fully grok the column oriented data stores.


00:54:32.000 --> 00:54:33.000
Yeah.


00:54:33.000 --> 00:54:34.000
It's a different way of thinking.


00:54:34.000 --> 00:54:58.000
Yeah, the column oriented aspect is also a major revolution in how data warehousing has come about where, you know, the first generation was all built on the same databases that we were using for our application. So it was all row oriented. And that was one of the inherent limits to how well they could scale their compute. Whereas all of the modern cloud data warehouses are all the modern even non cloud data warehouses are column oriented.


00:54:58.000 --> 00:55:21.000
And so if you have, you know, one column that is street addresses and another column that's integers and another column that is, you know, Vercare 15, all of those are the same data type. And so they can compress them down a lot more than if you have one row that is a street address and, you know, a text field and an integer and a float and a JSON array.


00:55:21.000 --> 00:55:40.000
If you try to compress all of those together, they're not compatible data types. And so you have a lot more inefficiency in terms of how well you can compress it. And then also as you're scanning, you know, a lot of analytics jobs are operating more on aggregates of information than on individual records.


00:55:40.000 --> 00:56:06.000
And so if you want to say, I want to find out, you know, what is the most common street name across all the street addresses that I have in my database, all I have to do is pull all the information out of that street address column. It's all co located on disk. So it's a faster seek time. And it's all compressed the same. And that way, you don't have to read all of the values for all of the rows to get all of the street addresses, which is what you would do in a relational database.


00:56:06.000 --> 00:56:20.000
Right, because probably those are co located on disk by row. Whereas if you're going to ask so all about the streets across everyone, then it's better to put all the streets, and then all the you know, cities or whatever, right?


00:56:20.000 --> 00:56:22.000
Exactly.


00:56:22.000 --> 00:56:34.000
Cool. I think I actually understand a little bit better now. Thanks. The final one that you put on the list that just maybe to put a pen in it. It's a very, very popular pandas. I'm never I never cease to be amazed with what you can do with pandas.


00:56:34.000 --> 00:57:00.000
Yeah, so I mean, pandas, you know, it's one of the most flexible tools in the Python toolbox. I've used it in web development contexts. I've used it for data engineering, I've used it for data analysis. And, you know, just it's definitely the Swiss Army knife of data. So it's absolutely one of the more critical tools in the toolbox of anybody who's working with data, regardless of the context. And so it's absolutely no surprise that data engineers reach for it a lot as well.


00:57:00.000 --> 00:57:24.000
So pandas is supported natively and things like Dagster, where it will, you know, give you a lot of rich metadata information about the column layouts and the data distributions. But yeah, it's just absolutely indispensable. I'm, you know, it's been covered enough times in both your show and mine, we don't need to go too deep into it. But if you're working with data, absolutely get at least a little bit familiar with pandas.


00:57:24.000 --> 00:57:52.000
Well, just to give people a sense, like one of the things I learned yesterday, I think it was, Chris Moffitt was showing off some things with pandas. And he's like, oh, over on this Wikipedia page, three fourths of the way down, there's a table, the table as a header, that has a name, and you can just say load HTML, give me the table called this as a data frame from screen scraping as part of the page. It's amazing.


00:57:52.000 --> 00:58:21.000
Yeah, another interesting aspect of the pandas ecosystem is the pandas extension arrays library that lets you create plugins for pandas to support custom data types. So I know that they have support for things like geo JSON, and IP addresses, so that you can do more interesting things out of the box in terms of aggregates and group buys and things like that. So you know, if you have the IP address pandas extension, then you can say, give me all of the rows that are grouped by this.


00:58:21.000 --> 00:58:34.000
Network prefix and things like that. Whereas just pandas out of the box, we'll just treat it as an object. And so you have to do a lot more additional coding around it. And it's not as efficient. So those are interesting.


00:58:34.000 --> 00:58:36.000
Yeah, that's, that's a cool aspect.


00:58:36.000 --> 00:58:37.000
Yeah.


00:58:37.000 --> 00:58:38.000
As well.


00:58:38.000 --> 00:58:53.000
Nice. One quick question. And then I think we should probably wrap this up. Stefan threw out some stuff about graph databases, particularly GraphQL. Or that's actually the API, right? It's efficient, but what about its maturity? Like, what do you think about some of these?


00:58:53.000 --> 00:59:05.000
Yeah, so GraphQL is definitely gaining a lot of popularity. I mean, so as you mentioned, there's sometimes a little bit of confusion about, you know, they both have the word graph in the name. So GraphQL and GraphDB.


00:59:05.000 --> 00:59:08.000
Oh yeah, like Neo4j. Wait, no, it has nothing to do with that.


00:59:08.000 --> 00:59:33.000
Right. So, you know, GraphQL is definitely a popular API design. Interesting side note is that the guy who created Dagster is also one of the co-creators of GraphQL. And Dagster has a really nice web UI that comes out of the box that has a GraphQL API to it so that you can do things like trigger jobs or introspect information about the running system.


00:59:33.000 --> 00:59:59.000
Another interesting use of GraphQL is there's a database engine called Dgraph that uses GraphQL as its query language. So it's a native graph storage engine. It's scalable, horizontally distributable. And so you can actually model your data as a graph and then query it using GraphQL. So definitely seeing a lot of interesting use cases within the data ecosystem as well.


00:59:59.000 --> 01:00:06.000
Yeah, for the right type of data, a graph database seems like it would really light up the speed of accessing certain things.


01:00:06.000 --> 01:00:18.000
Absolutely. Yeah. So the funny thing is, you know, there's, you have this concept of a relational database, but it's actually not very good at storing information about relationships.


01:00:18.000 --> 01:00:21.000
It is. The joins make them so slow and so on.


01:00:21.000 --> 01:00:22.000
Exactly.


01:00:22.000 --> 01:00:23.000
The lazy loading or whatever. Yeah.


01:00:23.000 --> 01:00:47.000
Right. So graph databases are entirely optimized for storing information about relationships so that you can do things like network traversals or understanding within this structure of relations, you know, things like social networks are kind of the natural example of a graph problem where I want to understand what are the degrees of separation between these people. So, you know, the six degrees of Kevin Bacon kind of thing.


01:00:47.000 --> 01:01:01.000
Yeah. Yeah. Seems like you could also model a lot of interesting things like the, I don't know how real it is, but you know, the bananas are at the back or the milk is at the back of the store. So you have to walk all the way through the store and you can find those kind of traversing those like behaviors.


01:01:01.000 --> 01:01:04.000
Exactly. Yeah. The traveling salesman problem, stuff like that.


01:01:04.000 --> 01:01:19.000
Yeah. Yeah, exactly. All right. Well, so many tools, way more than five that we actually made our way through, but very, very interesting because I think there's just so much out there and it sounds like a really fun place to work, like a technical space to work.


01:01:19.000 --> 01:01:44.000
You know, a lot of these ideas also seem like they're probably really ripe for people who have programming skills and software engineering mindsets like CI/CD testing and so on to come in and say, I could make a huge impact. We have this organization that has tons of data. We have people that work with the data, but not in this formalized way. So if people are interested in getting started with this kind of work, what would you recommend?


01:01:44.000 --> 01:02:02.000
So there's actually one resource I'll recommend. I'll see if I can dig up the link after the show. There's a gentleman called named Jesse Davidson, who wrote a really great resource that's a short ebook of kind of, you know, you think you might want to be a data engineer. Here's a good way to understand if that's actually what you want to do.


01:02:02.000 --> 01:02:20.000
So I'll share that. But more broadly, if you're interested in data engineering, you know, the first step is, you know, just kind of start to take a look at it. You know, you probably have data problems in your applications that you're working with that maybe you're just using a sequence of salary jobs and hoping that they complete in the right order.


01:02:20.000 --> 01:02:39.000
You know, maybe take a look at something like Dagster or Prefect to build a more structured graph of execution. If you don't want to go for a full fledged, you know, framework like that, there are also tools like Bonobo that are just command line oriented that help you build up that same structured graph of execution.


01:02:39.000 --> 01:02:57.000
So, you know, definitely just start to take a look and try and try and understand, like, what are the data flows in your system, because if you if you think about it more than just flows of logic and think about it in flows of data, then it starts to become a more natural space to solve it with some of these different tools and practices.


01:02:57.000 --> 01:03:20.000
So getting familiar with thinking about it in that way. Another really great book, if you're definitely interested in data engineering and want to kind of get deep behind the scenes is Designing Data Intensive Applications. I read that book recently and learned a whole lot more than I thought I would about just the entire space of building applications oriented around data. So great resource there.


01:03:20.000 --> 01:03:34.000
I mean, just also, yeah, and also just kind of raise your hand, say to your management or your team to say, hey, it looks like we have some data problems. I'm interested in digging into it. And chances are, they'll welcome the help.


01:03:34.000 --> 01:03:43.000
You know, lots of great resources out there if you want to get if you want to learn more about it, you know, shameless plug, the data engineering podcast is one of them.


01:03:43.000 --> 01:03:56.000
I'm always happy to help answer questions. I mean, basically, just start to dig into the space, take a look at some of the tools and frameworks and just try to implement them in your day to day work.


01:03:56.000 --> 01:04:10.000
A lot of data engineers come from software engineering backgrounds. A lot of data engineers might come from database administrator positions because they're familiar with the problem domain of the data. And then it's a matter of learning the actual engineering aspects of it.


01:04:10.000 --> 01:04:25.000
A lot of people come from data analyst or data scientist backgrounds where they actually decide that they enjoy working more with getting the data clean and well managed than doing the actual analysis on it. So there's not really any one concrete background to come from.


01:04:25.000 --> 01:04:51.000
It's more just a matter of being interested in making the data reproducible, helping make it valuable. Interesting note is that if you look at some of the statistics around it, there are actually more data engineering positions open, at least in the US, than there are data scientist positions because of the fact that is such a necessary step in the overall lifecycle of data.


01:04:51.000 --> 01:05:00.000
Yeah, how interesting. And probably traditionally those might have been just merged together into one group under the category of data science, but now it's getting a little more fine grained.


01:05:00.000 --> 01:05:15.000
Exactly. And you know, with the advent of data ops and ML ops, a lot of a lot of organizations are understanding that this is actually a first class consideration that they need dedicated people to be able to help build. And it's not just something that they can throw on the plate of the person who's doing the data science.


01:05:15.000 --> 01:05:29.000
Yeah, certainly if you can help organizations go from batch to real time or maybe shaky results because of shaky input to solid results because of solid input like those are extremely marketable skills. That's awesome.


01:05:29.000 --> 01:05:30.000
Exactly.


01:05:30.000 --> 01:05:40.000
All right. Well, Tobias, thanks so much for covering that. Before we get out of here, though, final two questions. So if you're going to write some Python code, what editor do you use these days?


01:05:40.000 --> 01:05:51.000
So I've been using Emacs for a number of years now. I've tried out things like PyCharm and VS Code here and there, but it just never feels quite right just because my fingers have gotten so used to Emacs.


01:05:51.000 --> 01:05:55.000
You just want to have an entire operating system as your editor, not just a piece of software.


01:05:55.000 --> 01:05:56.000
Exactly.


01:05:56.000 --> 01:06:00.000
And it has that ML background with Lisp as its language.


01:06:00.000 --> 01:06:01.000
Right.


01:06:01.000 --> 01:06:07.000
And then notable PyPI package or packages. People should check out. You kind of touched on some, right?


01:06:07.000 --> 01:06:14.000
Yeah, exactly. I mean, a lot of them in the list here. I'll just mention again, Dagster, DBT and Great Expectations.


01:06:14.000 --> 01:06:21.000
Yeah, very nice. All right. Final call to action. People are excited about this. What should they do?


01:06:21.000 --> 01:06:41.000
Listen to the Data Engineering Podcast. Listen to podcast.init if you want to understand a little bit more about the whole ecosystem, because since I do spend so much time in the data engineering space, I sometimes have crossover where if there's a data engineering tool that's implemented in Python, I'll have them on podcast.init just to make sure that I can get everybody out there.


01:06:41.000 --> 01:06:53.000
And yeah, feel free to send questions my way. I'll add the information about the podcast in the show notes. And yeah, just be curious.


01:06:53.000 --> 01:07:02.000
Yeah, absolutely. Well, like I said, it looks like a really interesting and growing space that has got a lot of low hanging fruit. So it sounds like a lot of fun.


01:07:02.000 --> 01:07:05.000
Yeah. All right. Well, thanks for being here. And thanks, everyone, for listening.


01:07:05.000 --> 01:07:06.000
Thanks for having me.


01:07:10.000 --> 01:07:35.000
Thanks for watching the Talk Python to Me podcast live stream. Be sure to subscribe to the podcast in your podcast player apps, or just search for Talk Python or visit talkpython.fm. And if you're looking for Python education and training, be sure to visit talkpython.fm and click on courses. We have around 200 hours of professionally produced content on the latest Python technologies. This is Michael Kennedy. Thanks for watching. Be well, friends.

