WEBVTT

00:00:00.000 --> 00:00:01.000
>> Hello, YouTube.


00:00:01.000 --> 00:00:02.000
Hello, Justin.


00:00:02.000 --> 00:00:03.000
>> Hello.


00:00:03.000 --> 00:00:13.160
>> Awesome to have you here on the channel and just in a minute on the podcast.


00:00:13.160 --> 00:00:17.240
So for those of you watching live, please put out questions, comments, thoughts.


00:00:17.240 --> 00:00:20.720
We'll try to make them part of the show for those of you watching the recording.


00:00:20.720 --> 00:00:24.080
No live chat, but thanks for being here and checking out anyway.


00:00:24.080 --> 00:00:25.080
All right.


00:00:25.080 --> 00:00:26.760
With that, let's go ahead and kick it off.


00:00:26.760 --> 00:00:29.400
Justin, welcome to Talk Python to Me.


00:00:29.400 --> 00:00:35.960
Thanks for having me. It's great to have you here. I'm a little suspicious. I got to know,


00:00:35.960 --> 00:00:42.440
I don't really know how to test whether you're actually Justin or an AI speaking as Justin.


00:00:42.440 --> 00:00:45.640
What's the deal here? Yeah, there's no way to know now.


00:00:45.640 --> 00:00:51.480
Apparently, I've recently learned from you that I can give you a bunch of Xs and other


00:00:51.480 --> 00:00:57.880
arbitrary characters. This is like the test. It's like asking Germans to say squirrel in


00:00:58.520 --> 00:01:02.520
World War II sort of thing, like it's the test, it's the tell.


00:01:02.520 --> 00:01:06.520
There's always going to be something, some sort of adversarial attack.


00:01:06.520 --> 00:01:12.520
Exactly. It's only going to get more interesting with this kind of stuff, for sure.


00:01:12.520 --> 00:01:19.520
So we're going to talk about using generative AI and large language models,


00:01:19.520 --> 00:01:22.520
paired with things like Pandas, or consumed with straight Python,


00:01:22.520 --> 00:01:26.520
with a couple of your projects, which are super exciting.


00:01:26.520 --> 00:01:31.960
I think it's going to empower a lot of people in ways that it hasn't really been done yet.


00:01:31.960 --> 00:01:33.400
So awesome on that.


00:01:33.400 --> 00:01:35.320
But before we get to it, let's start with your story.


00:01:35.320 --> 00:01:37.720
How did you get into programming in Python?


00:01:37.720 --> 00:01:38.720
Sure.


00:01:38.720 --> 00:01:39.720
Yeah.


00:01:39.720 --> 00:01:46.000
I got into programming just like when I was a kid, TI-83, learning to code on that.


00:01:46.000 --> 00:01:50.360
And then sort of just kept it up as a side hobby my whole life.


00:01:50.360 --> 00:01:53.960
Didn't ever sort of choose it as my career path or anything for a while.


00:01:53.960 --> 00:01:54.960
It chose you.


00:01:54.960 --> 00:01:55.960
Yeah.


00:01:55.960 --> 00:01:57.560
It just, I dragged it along with me everywhere.


00:01:57.560 --> 00:01:59.520
It's just like the toolkit.


00:01:59.520 --> 00:02:03.560
I got a, went to undergrad


00:02:03.560 --> 00:02:05.240
for physics electrical engineering,


00:02:05.240 --> 00:02:08.720
then did a physics PhD, experimental physics.


00:02:08.720 --> 00:02:12.400
During that, I did a lot of non-traditional languages,


00:02:12.400 --> 00:02:14.760
things like LabVIEW, Igor Pro,


00:02:14.760 --> 00:02:17.700
just weird Windows hotkey


00:02:17.700 --> 00:02:20.140
for like just trying to like automate things.


00:02:20.140 --> 00:02:24.120
So just was sort of dragging that along.


00:02:24.120 --> 00:02:26.920
But along that path, I sort of came across GPUs


00:02:26.920 --> 00:02:29.900
and used it for accelerating processing,


00:02:29.900 --> 00:02:32.200
specifically like particle detection.


00:02:32.200 --> 00:02:34.320
So it was doing some like electron counting


00:02:34.320 --> 00:02:38.240
in some detector experiments.


00:02:38.240 --> 00:02:41.160
- And just like two to cores on NVIDIA type things.


00:02:41.160 --> 00:02:42.000
- Precisely.


00:02:42.000 --> 00:02:42.820
- Stuff like that.


00:02:42.820 --> 00:02:43.660
- That was the--


00:02:43.660 --> 00:02:45.320
- Was that with Python or was that with C++ or what?


00:02:45.320 --> 00:02:48.240
- At the time it was C++ and I made like a DLL


00:02:48.240 --> 00:02:51.080
and then called it from LabVIEW, but--


00:02:51.080 --> 00:02:52.680
- Wow, that's some crazy integration.


00:02:52.680 --> 00:02:57.200
It's like drag and drop programming too on the memory GPU.


00:02:57.200 --> 00:02:59.480
- Exactly, it was all over the place.


00:02:59.480 --> 00:03:01.800
Also had, it was a distributed LabVIEW project.


00:03:01.800 --> 00:03:04.220
We had multiple machines that were coordinating


00:03:04.220 --> 00:03:08.000
and doing this, all just to move some motors


00:03:08.000 --> 00:03:09.280
and measure electrons.


00:03:09.280 --> 00:03:13.080
But it got me into CUDA stuff, which then at the time


00:03:13.080 --> 00:03:16.600
was around the time that the, like AlexNet,


00:03:16.600 --> 00:03:18.160
some of these like very first neural net stuff


00:03:18.160 --> 00:03:19.000
was happening.


00:03:19.000 --> 00:03:21.000
And so those same convolutional kernels


00:03:21.000 --> 00:03:22.720
where the same exact code I was trying to write


00:03:22.720 --> 00:03:24.640
to run convolutions on these images.


00:03:24.640 --> 00:03:26.440
And so it's like, oh, look at this paper.


00:03:26.440 --> 00:03:27.280
Oh, let me go read it.


00:03:27.280 --> 00:03:28.540
It seems like it's got so many citations.


00:03:28.540 --> 00:03:29.380
This is interesting.


00:03:29.380 --> 00:03:31.240
And then that sent me down the rabbit hole of like,


00:03:31.240 --> 00:03:32.080
oh, this AI stuff.


00:03:32.080 --> 00:03:34.400
Oh, okay, let me go deep dive into this.


00:03:34.400 --> 00:03:38.060
And then that just, I'd say that became the obsession


00:03:38.060 --> 00:03:38.900
from then.


00:03:38.900 --> 00:03:41.520
So it's been like eight years of doing that.


00:03:41.520 --> 00:03:43.800
Then sort of just after I left academia,


00:03:43.800 --> 00:03:46.680
tried my own startup, then joined multiple others


00:03:46.680 --> 00:03:48.280
and just sort of have been bouncing around


00:03:48.280 --> 00:03:53.240
as the founding engineer, early engineer at startups


00:03:53.240 --> 00:03:54.320
for a while now.


00:03:54.320 --> 00:03:58.280
And Python has been the choice ever since late grad school


00:03:58.280 --> 00:04:00.320
and on.


00:04:00.320 --> 00:04:04.080
I would say it sort of came through the pandas and NumPy


00:04:04.080 --> 00:04:08.000
part, but then stuck for the scripting, just power.


00:04:08.000 --> 00:04:10.040
Just can throw anything together at any time.


00:04:10.040 --> 00:04:11.760
Yeah.


00:04:11.760 --> 00:04:15.560
So it seems like there were two groups that were just


00:04:15.560 --> 00:04:18.160
hammering GPUs, hammering them.


00:04:18.160 --> 00:04:21.640
Crypto miners and AI people,


00:04:21.640 --> 00:04:26.360
but the physicists and some of those people doing large scale


00:04:26.360 --> 00:04:31.080
research like that they were the OG graphics card users right way


00:04:31.080 --> 00:04:35.440
before crypto mining existed and really before AI was using


00:04:35.440 --> 00:04:36.720
graphics cards all that much.


00:04:36.720 --> 00:04:39.720
I think yeah, I think when I was like looking at some of the


00:04:39.720 --> 00:04:42.200
code like pre CUDA, there were some like quant traders that


00:04:42.200 --> 00:04:46.760
doing some like crazy stuff on like off of shaders like it wasn't even CUDA yet but it


00:04:46.760 --> 00:04:51.240
was shaders and they were trying to like extract the compute power out of them from that so


00:04:51.240 --> 00:04:57.120
if we could shave one millisecond off this we can short them all day let's do it but


00:04:57.120 --> 00:05:01.880
yeah uh yeah the physicist has always been like yeah it's always the get as much


00:05:01.880 --> 00:05:07.160
compute as you can out of the you know devices you have because you get simulations are slow


00:05:07.160 --> 00:05:09.960
Yeah, I remember when I was in grad school studying math,


00:05:09.960 --> 00:05:13.280
actually senior year of regular college, my bachelor's,


00:05:13.280 --> 00:05:17.280
the research team that I was on had gotten


00:05:17.280 --> 00:05:20.920
a used Silicon Graphics computer for a quarter million


00:05:20.920 --> 00:05:24.280
dollars and some Onyx workstations that we all were given


00:05:24.280 --> 00:05:26.360
to, I'm like, this thing is so awesome.


00:05:26.360 --> 00:05:29.480
A couple of years later, like an NVIDIA graphics card


00:05:29.480 --> 00:05:32.320
and like a simple PC would crush it,


00:05:32.320 --> 00:05:35.240
like that's $2,000, it's just, yeah,


00:05:35.240 --> 00:05:36.920
There's so much power in those things


00:05:36.920 --> 00:05:40.160
to be able to harness them for whatever, I guess.


00:05:40.160 --> 00:05:42.240
- Yeah, as long as you don't have too much branching,


00:05:42.240 --> 00:05:43.240
it works really well.


00:05:43.240 --> 00:05:45.600
- Awesome.


00:05:45.600 --> 00:05:49.820
So let's jump in and start talking about,


00:05:49.820 --> 00:05:53.200
let's start to talk about ChatGP


00:05:53.200 --> 00:05:57.840
and some of this AI stuff before we totally get into


00:05:57.840 --> 00:05:59.200
the projects that you're working on,


00:05:59.200 --> 00:06:04.080
which brings that type of conversational generative AI


00:06:04.080 --> 00:06:07.200
to things like pandas, as you said.


00:06:07.200 --> 00:06:11.600
But to me, I don't know how--


00:06:11.600 --> 00:06:13.640
maybe you've been more on the inside than I have.


00:06:13.640 --> 00:06:17.520
But to me, it looks like AI has been


00:06:17.520 --> 00:06:20.320
one of those things that's 30 years in the future forever.


00:06:20.320 --> 00:06:23.120
It was like the Turing test, and oh, here's


00:06:23.120 --> 00:06:24.760
a chat I'm going to talk to this thing


00:06:24.760 --> 00:06:26.600
and see if it feels human or not.


00:06:26.600 --> 00:06:30.320
And then there was OCR.


00:06:30.320 --> 00:06:33.360
And then all of a sudden, we got self-driving cars.


00:06:33.360 --> 00:06:38.360
"Wait a minute, that's actually solving real problems."


00:06:38.360 --> 00:06:41.360
And then we got things like ChatGPT


00:06:41.360 --> 00:06:43.360
where people are like, "Wait, this can do my job."


00:06:43.360 --> 00:06:46.600
It seems like just in the last couple years


00:06:46.600 --> 00:06:49.560
there's been some inflection point in this world.


00:06:49.560 --> 00:06:53.400
What do you think?


00:06:53.400 --> 00:06:54.320
Paul: Yeah, I think there's two key things


00:06:54.320 --> 00:06:57.320
that have happened in the past four or five years.


00:06:57.320 --> 00:07:00.560
Four years, roughly.


00:07:00.560 --> 00:07:02.200
One is the "Attention is all you need" paper from Google.


00:07:01.760 --> 00:07:03.680
sort of this transformer architecture came out


00:07:03.680 --> 00:07:06.840
and it's sort of a good, very hungry model


00:07:06.840 --> 00:07:08.480
that can just sort of absorb a lot of facts


00:07:08.480 --> 00:07:11.720
and just like a nice learnable key value store almost


00:07:11.720 --> 00:07:12.920
that's stacked.


00:07:12.920 --> 00:07:15.440
So, and then the other thing is, is that GPUs,


00:07:15.440 --> 00:07:17.160
we were sort of just talking about GPU compute,


00:07:17.160 --> 00:07:20.800
but this has just been really,


00:07:20.800 --> 00:07:24.120
GPU compute has really been growing so fast.


00:07:24.120 --> 00:07:26.320
If you like look at the like Moore's law equivalent


00:07:26.320 --> 00:07:28.600
type things, like it's just, it's faster


00:07:28.600 --> 00:07:30.180
how much we're getting flops out of these things


00:07:30.180 --> 00:07:31.020
like faster and faster.


00:07:31.020 --> 00:07:32.700
So it's been really nice.


00:07:32.700 --> 00:07:34.700
I mean, obviously, there'll be a wall eventually.


00:07:34.700 --> 00:07:38.940
But it's been good riding this exponential curve for a bit.


00:07:38.940 --> 00:07:43.300
Yeah, is the benefit that we're getting from the faster GPUs,


00:07:43.300 --> 00:07:45.820
is that because people are able to program it better


00:07:45.820 --> 00:07:47.400
and the frameworks are getting better,


00:07:47.400 --> 00:07:50.460
or because just the raw processing power


00:07:50.460 --> 00:07:51.820
is getting better?


00:07:51.820 --> 00:07:53.940
All of the above.


00:07:53.940 --> 00:07:57.460
I think that there was a paper that tried to dissect this.


00:07:57.460 --> 00:07:58.780
I wish I knew the reference.


00:07:58.780 --> 00:08:00.380
But I believe that their argument


00:08:00.380 --> 00:08:03.340
was that it was actually more the processing power was getting


00:08:03.340 --> 00:08:03.580
better.


00:08:03.580 --> 00:08:05.240
The actual physical silicon, we're


00:08:05.240 --> 00:08:07.220
getting better at making that for specifically


00:08:07.220 --> 00:08:08.980
this type of stuff.


00:08:08.980 --> 00:08:09.500
But like--


00:08:09.500 --> 00:08:09.860
>>Luis: The power--


00:08:09.860 --> 00:08:10.740
>>Tom: --on exponentials.


00:08:10.740 --> 00:08:11.300
But yeah.


00:08:11.300 --> 00:08:12.300
>>Luis: Yeah, yeah, yeah.


00:08:12.300 --> 00:08:13.660
The power that those things take.


00:08:13.660 --> 00:08:16.100
I have a gaming system over there.


00:08:16.100 --> 00:08:20.740
And it has a GeForce 2070 Super.


00:08:20.740 --> 00:08:22.580
I don't know what the Super really gets me.


00:08:22.580 --> 00:08:25.020
But it's better than the not Super, I guess.


00:08:25.020 --> 00:08:30.740
Anyway, that one still plugs into the wall normal.


00:08:30.740 --> 00:08:34.300
But the newer ones, like the 4090s, those things,


00:08:34.300 --> 00:08:36.060
the amount of power they consume,


00:08:36.060 --> 00:08:39.780
it's like space heater level of power.


00:08:39.780 --> 00:08:43.820
I don't know, 800 watts or something just for the GPU?


00:08:43.820 --> 00:08:45.940
You're going to brown out the house if you plug


00:08:45.940 --> 00:08:47.220
in too many of those.


00:08:47.220 --> 00:08:48.540
Yeah.


00:08:48.540 --> 00:08:52.180
Go look at those DGX A100 clusters.


00:08:52.180 --> 00:08:55.000
And they've got eight of those A100s just stacked right


00:08:55.000 --> 00:09:00.680
there. They take really beefy power supplies. It's built right directly attached to the power


00:09:00.680 --> 00:09:08.360
the power plant, electrical power plant. Nuts. Okay, so yeah, so those things are getting really


00:09:08.360 --> 00:09:14.360
really massive. Here's the paper, "Attention is all you need" from Google Research. What was the


00:09:14.360 --> 00:09:19.480
story of that? Where's that play into things? Yeah, so this came up during like machine


00:09:19.480 --> 00:09:23.000
translation sort of research at Google.


00:09:23.000 --> 00:09:30.140
And the core thing is they present this idea of instead of just stacking these layers of


00:09:30.140 --> 00:09:35.580
neural nets like we're sort of used to, they replace the neural net layer with this concept


00:09:35.580 --> 00:09:38.500
of a transformer block.


00:09:38.500 --> 00:09:42.120
Transformer block has this concept inside that's an attention mechanism.


00:09:42.120 --> 00:09:47.880
The attention mechanism is effectively three matrices


00:09:47.880 --> 00:09:50.520
that you combine in a specific order.


00:09:50.520 --> 00:09:53.800
And the sort of logic is that one of the vectors


00:09:53.800 --> 00:09:56.960
takes you from some space to keys.


00:09:56.960 --> 00:09:59.160
So it's almost like it's identifying labels out


00:09:59.160 --> 00:09:59.960
of your data.


00:09:59.960 --> 00:10:03.880
Another one is taking you from your data to queries.


00:10:03.880 --> 00:10:06.360
And then it dot products those to find a weight.


00:10:06.360 --> 00:10:10.440
And then another one finds values for your things.


00:10:10.440 --> 00:10:13.920
So it takes this query and key, you get the weights for them,


00:10:13.920 --> 00:10:16.440
and then you take the ones that were sort of the closest


00:10:16.440 --> 00:10:18.760
to get those values from the third matrix.


00:10:18.760 --> 00:10:21.460
And just doing it sort of like looks a little bit


00:10:21.460 --> 00:10:24.440
like accessing an element in a dictionary,


00:10:24.440 --> 00:10:26.360
like key value lookup.


00:10:26.360 --> 00:10:29.440
And it's a differentiable version of that.


00:10:29.440 --> 00:10:31.880
And it did really well on their machine learning,


00:10:31.880 --> 00:10:33.840
oh, sorry, on their machine translation stuff.


00:10:33.840 --> 00:10:36.800
This was, I think it's like one of the first big ones,


00:10:36.800 --> 00:10:38.280
this BERT model.


00:10:38.280 --> 00:10:42.080
And that paper sort of--


00:10:42.080 --> 00:10:45.080
the architecture of the actual neural net code


00:10:45.080 --> 00:10:50.040
is effectively unchanged from this to ChatGPT.


00:10:50.040 --> 00:10:52.800
There's a lot of stuff for milking performance


00:10:52.800 --> 00:10:54.600
and increasing stability.


00:10:54.600 --> 00:10:57.120
But the actual core essence of the actual mechanism


00:10:57.120 --> 00:11:00.640
that drives it, it's the same thing since this paper.


00:11:00.640 --> 00:11:02.720
Interesting.


00:11:02.720 --> 00:11:05.880
It's funny that Google didn't release something sooner.


00:11:05.880 --> 00:11:07.720
It's wild that they've had--


00:11:07.720 --> 00:11:09.580
They keep showing off that they've


00:11:09.580 --> 00:11:12.740
got equivalent or better things at different times,


00:11:12.740 --> 00:11:14.440
but then not releasing it.


00:11:14.440 --> 00:11:18.180
When Dolly happened, they had Imagine, I guess.


00:11:18.180 --> 00:11:19.380
I don't know how you say it.


00:11:19.380 --> 00:11:21.060
And what was the--


00:11:21.060 --> 00:11:22.140
Parti, as the two?


00:11:22.140 --> 00:11:25.340
They had two different really good, way better than Dolly,


00:11:25.340 --> 00:11:28.820
way better than stable diffusion models that were out.


00:11:28.820 --> 00:11:31.460
And they showed it, demoed it, but never


00:11:31.460 --> 00:11:32.620
released it to be used.


00:11:32.620 --> 00:11:34.460
So yeah, it's one of these--


00:11:34.460 --> 00:11:37.120
who knows what's going to happen with Google if they keep holding


00:11:37.120 --> 00:11:39.520
- Yeah, well, I think there was some hesitation.


00:11:39.520 --> 00:11:42.480
I don't know, holds up on accuracy


00:11:42.480 --> 00:11:44.280
or weird stuff like that.


00:11:44.280 --> 00:11:45.760
(laughing)


00:11:45.760 --> 00:11:47.040
- Yeah, now it cuts out of the bag now,


00:11:47.040 --> 00:11:47.880
now it's happening.


00:11:47.880 --> 00:11:48.880
- Yeah, the cat's out of the bag


00:11:48.880 --> 00:11:52.920
and people are racing to do the best they can.


00:11:52.920 --> 00:11:56.920
And it's gonna have interesting consequences for us,


00:11:56.920 --> 00:11:58.480
both positive and negative, I think,


00:11:58.480 --> 00:12:01.240
but let's leverage the positive


00:12:01.240 --> 00:12:03.440
once the cat's out of the bag anyway, right?


00:12:03.440 --> 00:12:05.120
- Yeah, hopefully.


00:12:05.120 --> 00:12:06.680
- Might as well, like ask it.


00:12:06.680 --> 00:12:08.240
questions for pandas.


00:12:08.240 --> 00:12:11.040
So let's play a little bit with chat GDP


00:12:11.040 --> 00:12:14.360
and maybe another one of these image type things.


00:12:14.360 --> 00:12:18.800
So I came in here and I stole this example from a blog post


00:12:18.800 --> 00:12:23.280
that's pretty nice about not using deeply nested codes.


00:12:23.280 --> 00:12:27.800
You can use a design pattern called a guarding clause that


00:12:27.800 --> 00:12:30.240
will look and say if the conditions are not right,


00:12:30.240 --> 00:12:34.160
we're going to return early instead of having if something,


00:12:34.160 --> 00:12:36.360
if that also, if something else.


00:12:36.360 --> 00:12:39.320
So there's this example that is written in a poor way


00:12:39.320 --> 00:12:41.960
and it says like, it's checking for a platypus.


00:12:41.960 --> 00:12:44.640
So it says, if self-thought is mammal,


00:12:44.640 --> 00:12:46.600
if self-thought has fur,


00:12:46.600 --> 00:12:49.000
if self-thought has beak, et cetera, et cetera.


00:12:49.000 --> 00:12:50.720
It's all deeply nested.


00:12:50.720 --> 00:12:53.080
And just for people who haven't played with chat GDP,


00:12:53.080 --> 00:12:54.560
like I put that in and I said, sure.


00:12:54.560 --> 00:12:56.120
I told her I wanted to call this arrow


00:12:56.120 --> 00:12:58.040
'cause it looks like an arrow.


00:12:58.040 --> 00:13:00.600
And it says, it tells me a little bit about this.


00:13:00.600 --> 00:13:02.040
So I'm gonna ask it,


00:13:02.040 --> 00:13:10.160
please rewrite arrow to be less nested with girding clauses.


00:13:10.160 --> 00:13:11.320
This is like a machine.


00:13:11.320 --> 00:13:14.160
If I tell it this, what is it going to say?


00:13:14.160 --> 00:13:15.000
Let's see.


00:13:15.000 --> 00:13:18.640
It may fail, but I think it's going to get it.


00:13:18.640 --> 00:13:19.600
It's thinking.


00:13:19.600 --> 00:13:22.720
I mistakenly put it into chat GDP4, which takes longer.


00:13:22.720 --> 00:13:24.400
I might switch it over to 3.


00:13:24.400 --> 00:13:26.320
I don't know.


00:13:26.320 --> 00:13:30.080
But the understanding of these things,


00:13:30.080 --> 00:13:31.560
there's a lot of hype about it.


00:13:31.560 --> 00:13:37.760
Like, but you, I think you kind of agree with me that maybe this hype is worthwhile.


00:13:37.760 --> 00:13:38.160
Here we go.


00:13:38.160 --> 00:13:41.720
So, look, look at this.


00:13:41.720 --> 00:13:43.080
It rewrote.


00:13:43.080 --> 00:13:44.600
It's a deaf is platypus.


00:13:44.600 --> 00:13:46.600
If not self as man, I'll return false.


00:13:46.600 --> 00:13:48.920
If not has for, and there's no more nesting.


00:13:48.920 --> 00:13:49.440
That's pretty cool.


00:13:49.440 --> 00:13:49.760
Right?


00:13:49.760 --> 00:13:50.520
Yep.


00:13:50.520 --> 00:13:54.160
I mean, I'm sure you've, you've, you've played with stuff like this, right?


00:13:54.160 --> 00:13:58.720
Well, you've, so this is what kind, I mean, this is kind of interesting, right?


00:13:58.720 --> 00:14:00.520
Like it understood there was a structure


00:14:00.520 --> 00:14:01.720
and it understood what these were


00:14:01.720 --> 00:14:02.920
and it understood what I said.


00:14:02.920 --> 00:14:05.080
But what's more impressive is like,


00:14:05.080 --> 00:14:10.080
please rewrite the program to check for crocodiles.


00:14:10.080 --> 00:14:18.560
And what is it gonna do here?


00:14:18.560 --> 00:14:19.920
Let's see.


00:14:19.920 --> 00:14:21.680
It says, sure, no problem.


00:14:21.680 --> 00:14:23.360
Then writes the function isCrocodile.


00:14:23.360 --> 00:14:25.360
If not, self.isReptile.


00:14:25.360 --> 00:14:27.240
If not, self.hasScales.


00:14:27.240 --> 00:14:29.680
if not self-taught has a long snout.


00:14:29.680 --> 00:14:33.600
Oh my gosh, like it not only remembered,


00:14:33.600 --> 00:14:35.760
oh yeah, there's this new version I wrote in


00:14:35.760 --> 00:14:40.280
the garden clause format but then it rewrote the tests.


00:14:40.280 --> 00:14:45.440
I mean, and then it's explaining to me why it wrote it that way.


00:14:45.440 --> 00:14:52.300
It's just mind-blowing how much you can have conversations with this,


00:14:52.300 --> 00:14:54.580
and how much it understands things like code,


00:14:54.580 --> 00:14:56.960
or physics, or history.


00:14:56.960 --> 00:14:57.960
What do you think?


00:14:57.960 --> 00:14:58.960
- Yeah, it's really satisfying.


00:14:58.960 --> 00:15:01.960
I love that it's such a powerful generalist


00:15:01.960 --> 00:15:04.960
at these things that are found on the internet.


00:15:04.960 --> 00:15:07.960
So if it exists and it's in the training data,


00:15:07.960 --> 00:15:10.960
it can do so good at synthesizing, composing,


00:15:10.960 --> 00:15:11.960
bridging between them.


00:15:11.960 --> 00:15:12.960
It's really satisfying.


00:15:12.960 --> 00:15:15.960
So it's really fun asking it to, as you're doing,


00:15:15.960 --> 00:15:17.960
rewriting, changing language.


00:15:17.960 --> 00:15:20.960
I've been getting into a lot more JavaScript


00:15:20.960 --> 00:15:21.960
because I'm doing a bunch more front end stuff.


00:15:21.960 --> 00:15:23.960
And just I sometimes will write a quick one liner


00:15:23.960 --> 00:15:28.520
write a quick one-liner in Python that I know how to do with a list comprehension. And then


00:15:28.520 --> 00:15:32.480
I'll be like, make this for me in JavaScript because I can't figure out this, like, how


00:15:32.480 --> 00:15:39.400
to initialize an array with integers in it. It's great for just really quick spot checks.


00:15:39.400 --> 00:15:43.680
And it also seems to know a lot about really popular frameworks. So you can ask it things


00:15:43.680 --> 00:15:51.400
that are surprisingly detailed about, like, how would you do cores with requests in FastAPI?


00:15:51.400 --> 00:15:54.280
It can help you find that exact middleware.


00:15:54.280 --> 00:15:56.560
It's like boilerplatey, but it's great


00:15:56.560 --> 00:15:58.800
that it can just be a source for that.


00:15:58.800 --> 00:16:00.020
- It's insane.


00:16:00.020 --> 00:16:03.160
I don't know if I've got it in my history here.


00:16:03.160 --> 00:16:07.320
We're rewriting our mobile apps for Talk by The Unchained


00:16:07.320 --> 00:16:11.240
for our courses in Flutter,


00:16:11.240 --> 00:16:14.440
and we're having a problem downloading stuff concurrently


00:16:14.440 --> 00:16:17.360
using a particular library in Flutter.


00:16:17.360 --> 00:16:19.280
And so I asked it,


00:16:20.240 --> 00:16:25.280
I said, hey, I want some help with a Flutter and Dart


00:16:25.280 --> 00:16:25.780
program.


00:16:25.780 --> 00:16:26.820
He says, what do you want?


00:16:26.820 --> 00:16:29.680
I says, I'm using the DIO package.


00:16:29.680 --> 00:16:30.340
Do you know it?


00:16:30.340 --> 00:16:31.340
Oh, yes, I'm familiar.


00:16:31.340 --> 00:16:33.180
It does HTTP client stuff for Dart.


00:16:33.180 --> 00:16:33.900
OK.


00:16:33.900 --> 00:16:37.060
I want to download binary video files and a bunch of them,


00:16:37.060 --> 00:16:38.220
given a URL.


00:16:38.220 --> 00:16:40.900
I want to do them concurrently with three of them at a time.


00:16:40.900 --> 00:16:41.900
Write the code for that.


00:16:41.900 --> 00:16:43.660
And boom, it just writes it.


00:16:43.660 --> 00:16:47.300
Like, using that library I told you about, not just Dart.


00:16:47.300 --> 00:16:50.400
So I think that's just, that's incredible


00:16:50.400 --> 00:16:52.440
that we can get this kind of assistance


00:16:52.440 --> 00:16:54.760
for knowledge and programming.


00:16:54.760 --> 00:16:56.640
Like you'll never find, I mean, I take that back.


00:16:56.640 --> 00:16:59.280
You might find that if there's a very specific


00:16:59.280 --> 00:17:01.560
stack overflow question or something,


00:17:01.560 --> 00:17:03.460
but if there's not a write-on question for it,


00:17:03.460 --> 00:17:04.880
you're not gonna find it.


00:17:04.880 --> 00:17:05.720
- Yep, yep.


00:17:05.720 --> 00:17:08.640
It's the, yeah, I love the, when you have a,


00:17:08.640 --> 00:17:10.080
like you know the stack overflow would exist


00:17:10.080 --> 00:17:11.840
for a variant of your question,


00:17:11.840 --> 00:17:14.240
but it's like the exact one doesn't exist


00:17:14.240 --> 00:17:16.520
and you have to go grab like the three of them to synthesize


00:17:16.520 --> 00:17:18.080
And it's just great at that.


00:17:18.080 --> 00:17:20.000
It's also, yeah.


00:17:20.000 --> 00:17:22.080
It also is pretty good at fixing errors.


00:17:22.080 --> 00:17:23.560
I mean, sometimes it can walk itself


00:17:23.560 --> 00:17:26.320
into just like lying to you repeatedly, but yeah.


00:17:26.320 --> 00:17:27.960
- That's that accuracy part.


00:17:27.960 --> 00:17:28.800
- Code is code.


00:17:28.800 --> 00:17:29.960
- It's so problematic, yeah.


00:17:29.960 --> 00:17:32.520
But you can also ask it like, here's my program.


00:17:32.520 --> 00:17:34.040
Are there security vulnerabilities?


00:17:34.040 --> 00:17:35.080
Or do you see any bugs?


00:17:35.080 --> 00:17:36.680
And it'll find them.


00:17:36.680 --> 00:17:37.720
- Yep.


00:17:37.720 --> 00:17:39.760
- Yeah, it's nuts.


00:17:39.760 --> 00:17:41.200
So people may be wondering,


00:17:41.200 --> 00:17:43.320
we haven't talked yet about your project sketch.


00:17:43.320 --> 00:17:45.240
Why I'm talking so much about ChatsCP.


00:17:45.240 --> 00:17:49.220
So that is kind of the style of AI


00:17:49.220 --> 00:17:52.620
that your project brings to Pandas, which we're gonna get to.


00:17:52.620 --> 00:17:55.060
But I wanna touch on two more really quick AI things


00:17:55.060 --> 00:17:56.700
that we'll dive into it.


00:17:56.700 --> 00:17:59.220
The other is this just around images,


00:17:59.220 --> 00:18:01.260
just the ability to ask questions.


00:18:01.260 --> 00:18:04.760
You've already mentioned three, Dolly, Imagine,


00:18:04.760 --> 00:18:06.700
and then the other one, I don't remember from Google


00:18:06.700 --> 00:18:08.540
that they haven't put out yet.


00:18:08.540 --> 00:18:10.020
And Mid-Journey is another,


00:18:10.020 --> 00:18:11.260
like just the ability to say,


00:18:11.260 --> 00:18:12.960
hey, I want a picture of this.


00:18:12.960 --> 00:18:14.940
No, actually change it slightly like that.


00:18:14.940 --> 00:18:16.300
It's mind-blowing.


00:18:16.300 --> 00:18:18.940
- It's very, they're a lot of fun.


00:18:18.940 --> 00:18:20.860
They're great for sparking creativity


00:18:20.860 --> 00:18:23.380
or having an idea and just getting to see it in front of you.


00:18:23.380 --> 00:18:25.180
- I think it's more impressive to me


00:18:25.180 --> 00:18:28.460
than even this chat, GTP telling me how to write Dart


00:18:28.460 --> 00:18:31.560
is 'cause it's like, I gave you a blank canvas.


00:18:31.560 --> 00:18:33.460
And so for example, for this video,


00:18:33.460 --> 00:18:37.100
for this conversation, I'll probably use this


00:18:37.100 --> 00:18:41.420
as the YouTube thumbnail and image for this episode.


00:18:41.420 --> 00:18:44.340
So I want an artificial intelligence panda.


00:18:44.340 --> 00:18:46.580
And it came up, and I want it photorealistic


00:18:46.580 --> 00:18:48.860
in the style of National Geographic.


00:18:48.860 --> 00:18:51.140
And so it gave me this panda.


00:18:51.140 --> 00:18:54.060
You can see beautiful whiskers, but just behind the ear,


00:18:54.060 --> 00:18:55.780
you can see the fur is gone, and it's


00:18:55.780 --> 00:19:01.140
like an Android type of creature.


00:19:01.140 --> 00:19:01.860
That's incredible.


00:19:01.860 --> 00:19:03.940
That is a beautiful picture.


00:19:03.940 --> 00:19:05.740
It's pretty accurate.


00:19:05.740 --> 00:19:08.420
It's nuts that I can just go talk to these systems


00:19:08.420 --> 00:19:10.100
and ask them these questions.


00:19:10.100 --> 00:19:10.620
Yeah.


00:19:10.620 --> 00:19:13.340
I find it interesting you're comparing the ChatGPT


00:19:13.340 --> 00:19:18.220
in the Mid-Journey style and find the Mid-Journey ones


00:19:18.220 --> 00:19:19.060
impressive.


00:19:19.060 --> 00:19:20.340
They are, like, I completely get it.


00:19:20.340 --> 00:19:21.660
Like, it's very visceral.


00:19:21.660 --> 00:19:23.720
It's also, from like another perspective,


00:19:23.720 --> 00:19:26.140
I think of like the weights and the scale of the model.


00:19:26.140 --> 00:19:28.380
And the Mid-Journey, you know, these like image ones


00:19:28.380 --> 00:19:31.820
that like solve all images are so much smaller in scale


00:19:31.820 --> 00:19:32.980
than these like language ones


00:19:32.980 --> 00:19:34.580
that have all this other data and stuff.


00:19:34.580 --> 00:19:35.420
So it's fascinating how complex language is.


00:19:35.420 --> 00:19:37.060
- Yeah, I know the smarts is so much less.


00:19:37.060 --> 00:19:39.380
But just something about it actually came up


00:19:39.380 --> 00:19:43.100
with a creative picture that never existed.


00:19:43.100 --> 00:19:48.080
Yeah, right. You could show this to somebody like, Oh, that's an artificial panda. That's


00:19:48.080 --> 00:19:54.040
insane, right? But it's, but I just gave it like a sentence or two. Yeah, yeah, I don't


00:19:54.040 --> 00:19:59.080
know. Yeah, this is a sort of a technical interpretation. But I, I love it because it's


00:19:59.080 --> 00:20:04.240
like this. It's just phenomenal interpolation. It's like through semantically labeled space.


00:20:04.240 --> 00:20:09.080
So like the words have meaning, and it understands the meeting and can move sliders of like,


00:20:09.080 --> 00:20:12.080
well, I've seen lots of these machine things. I understand the concept of gears and this


00:20:12.080 --> 00:20:17.120
metal and this like the shiny texture and then the fur texture and like a specific they're very good


00:20:17.120 --> 00:20:23.920
at texture uh and so uh yeah it's uh yeah really great how uh it interprets all of that just to fit


00:20:23.920 --> 00:20:29.440
the you know the small prompt yeah there are other angles which is frustrating like i want it i want


00:20:29.440 --> 00:20:34.000
it in the back of the picture not the no it's always in the center uh one more thing really


00:20:34.000 --> 00:20:39.840
quick uh and this leads me into my final thing is uh github copilot github copilot is like


00:20:39.840 --> 00:20:42.720
this in your editor, which is kind of insane, right?


00:20:42.720 --> 00:20:46.720
You can just give it a comment or a series of comments,


00:20:46.720 --> 00:20:48.760
and it will write it.


00:20:48.760 --> 00:20:52.560
I think chat GDPs may be more open-ended and more creative,


00:20:52.560 --> 00:20:56.920
but this is also a pretty interesting way--


00:20:56.920 --> 00:21:00.080
I'm a heavy user of Copilot.


00:21:00.080 --> 00:21:03.160
If there's a weird crux, and I'm slowly


00:21:03.160 --> 00:21:05.960
developing a need to have this in my browser--


00:21:05.960 --> 00:21:10.160
I was on a flight recently and was with the internet,


00:21:10.160 --> 00:21:12.520
and Copilot wasn't working.


00:21:12.520 --> 00:21:15.200
And I felt the difference.


00:21:15.200 --> 00:21:18.160
I felt like I was walking through mud instead of just


00:21:18.160 --> 00:21:19.480
actually running a little bit.


00:21:19.480 --> 00:21:20.240
And I was like, oh.


00:21:20.240 --> 00:21:23.600
I've been disconnected from my distributed mind.


00:21:23.600 --> 00:21:26.960
I am broken partially.


00:21:26.960 --> 00:21:28.200
Incredible.


00:21:28.200 --> 00:21:34.280
So the last part, I guess, is what are the ethics of this?


00:21:34.280 --> 00:21:37.040
I went on very positively about MidJourney,


00:21:37.040 --> 00:21:40.480
but how much of that is trained on copyright material?


00:21:40.480 --> 00:21:42.080
Or there's GitHub Copilot.


00:21:42.080 --> 00:21:46.880
How much of that is trained on GPL-based stuff


00:21:46.880 --> 00:21:49.160
that was in GitHub?


00:21:49.160 --> 00:21:52.440
But when I use it, I don't have the GPL any longer on my code.


00:21:52.440 --> 00:21:54.240
I might use it on a commercial code.


00:21:54.240 --> 00:21:56.640
But just running it through the AI,


00:21:56.640 --> 00:21:59.160
does that strip licenses, or does it not?


00:21:59.160 --> 00:22:01.840
There's GitHubCopilotLitigation.com,


00:22:01.840 --> 00:22:04.040
which is interesting.


00:22:04.040 --> 00:22:08.720
We might be finding out. There's also, I think, Getty.


00:22:08.720 --> 00:22:10.720
I think it's Getty Images. I'm not 100% sure.


00:22:10.720 --> 00:22:16.320
But I think Getty Images is suing one of these image generation companies.


00:22:16.320 --> 00:22:18.320
I can't remember which one.


00:22:18.320 --> 00:22:20.320
Maybe Midjourney. I don't think it's Midjourney.


00:22:20.320 --> 00:22:23.040
I think it's Stable Diffusion. But anyway, it doesn't really matter.


00:22:23.040 --> 00:22:26.040
There's a bunch of things that are pushing back against us.


00:22:26.040 --> 00:22:29.040
Like, "Wait a minute. Where did you get this data?


00:22:29.040 --> 00:22:31.040
Did you have rights to use this data in this way?"


00:22:31.040 --> 00:22:34.720
And I mean, what are your thoughts on this angle of AI


00:22:34.720 --> 00:22:35.800
these days?


00:22:35.800 --> 00:22:37.520
Yeah.


00:22:37.520 --> 00:22:39.200
I know it sounds like--


00:22:39.200 --> 00:22:42.120
I don't have-- I don't worry too much about it


00:22:42.120 --> 00:22:43.280
in either direction.


00:22:43.280 --> 00:22:46.800
I think I believe in--


00:22:46.800 --> 00:22:48.800
like, I get personal ethics.


00:22:48.800 --> 00:22:51.960
I believe in open source things, availability of things,


00:22:51.960 --> 00:22:54.200
because it just sort of accelerates collective


00:22:54.200 --> 00:22:55.600
progress.


00:22:55.600 --> 00:22:58.240
But that said, I also believe in slightly different


00:22:58.240 --> 00:23:00.960
social structures to help support people.


00:23:00.960 --> 00:23:04.960
Like I'm a, I guess a personal believer in things like UBI


00:23:04.960 --> 00:23:06.880
or something like that on that direction.


00:23:06.880 --> 00:23:09.800
So I, when you combine those, I feel like it,


00:23:09.800 --> 00:23:11.360
things sort of work out kind of well,


00:23:11.360 --> 00:23:14.040
but when we like, but it is still a thing that like


00:23:14.040 --> 00:23:17.280
copyright exists and that there is this sense of ownership


00:23:17.280 --> 00:23:19.600
and this is my thing and I wanted to put licenses on it.


00:23:19.600 --> 00:23:22.600
And yeah, it's, I think it's,


00:23:22.600 --> 00:23:25.360
I think that this sort of story started presumably


00:23:25.360 --> 00:23:27.760
that I wasn't really having this conversation,


00:23:27.760 --> 00:23:29.360
But like when the internet came around


00:23:29.360 --> 00:23:31.920
and search engines happened and like Google could just go


00:23:31.920 --> 00:23:33.880
and pull up your thing from your page


00:23:33.880 --> 00:23:36.720
and summarize it in a little blob on the page.


00:23:36.720 --> 00:23:37.560
Was that fair?


00:23:37.560 --> 00:23:39.400
What if it starts, you know, your shop


00:23:39.400 --> 00:23:41.120
and it allows you to go buy that same product


00:23:41.120 --> 00:23:41.940
from other shops?


00:23:41.940 --> 00:23:44.720
Like it, I think that the same things are showing up.


00:23:44.720 --> 00:23:47.360
And in the same way that the web, like in the internet,


00:23:47.360 --> 00:23:50.240
sort of, it's sort of, it was a large thing,


00:23:50.240 --> 00:23:52.200
but then it sort of, I don't know if it got quieter,


00:23:52.200 --> 00:23:54.060
but it sort of became in the background,


00:23:54.060 --> 00:23:55.360
we sort of found new systems.


00:23:55.360 --> 00:23:59.680
stop being piracy and CDs and the music industry is going to struggle and hey, things like


00:23:59.680 --> 00:24:04.000
Spotify exist and streaming services exist and like, I don't know what the next wave


00:24:04.000 --> 00:24:05.000
is.


00:24:05.000 --> 00:24:06.000
Doing better than ever, basically.


00:24:06.000 --> 00:24:07.000
Yeah.


00:24:07.000 --> 00:24:08.000
Yeah.


00:24:08.000 --> 00:24:11.240
So I think it's just evolution and like some things will change and adopt, some things


00:24:11.240 --> 00:24:14.200
will like fall apart and new things will be born.


00:24:14.200 --> 00:24:17.760
It's just a great, it's a good time for lots of opportunity, I guess is the part that I'm


00:24:17.760 --> 00:24:18.760
excited about.


00:24:18.760 --> 00:24:19.760
Yeah, yeah, yeah, for sure.


00:24:19.760 --> 00:24:21.800
I think that's definitely true.


00:24:21.800 --> 00:24:22.800
Probably you're probably right.


00:24:22.800 --> 00:24:29.680
will turn out to be old man yells at cloud, cloud doesn't care sort of story in the end.


00:24:29.680 --> 00:24:37.360
Whereas on the other hand, if somebody came back and said, a court came back and said,


00:24:37.360 --> 00:24:41.560
you know what, actually anything trained on GPL and then you use Copilot on it, that's


00:24:41.560 --> 00:24:46.360
GPL, that would have instantly mega effects, right?


00:24:46.360 --> 00:24:47.360
Yeah.


00:24:47.360 --> 00:24:51.480
And I guess there's also stuff like the, I didn't actually read the article.


00:24:51.480 --> 00:24:52.480
I only saw the headline.


00:24:52.480 --> 00:24:53.560
And yeah, that's the worst thing to do


00:24:53.560 --> 00:24:55.040
is to repeat a thing, which has a headline.


00:24:55.040 --> 00:24:58.120
But there was that Italy thing that I saw about,


00:24:58.120 --> 00:25:00.600
like, I don't know the extent.


00:25:00.600 --> 00:25:01.960
Yeah, that was really clickbaity,


00:25:01.960 --> 00:25:03.480
but I didn't get a time to look at it yet.


00:25:03.480 --> 00:25:04.520
So.


00:25:04.520 --> 00:25:07.800
- You probably asked chat to be to summarize it for you.


00:25:07.800 --> 00:25:09.520
- If as long as it can be a Bing, I guess,


00:25:09.520 --> 00:25:10.480
get that updated.


00:25:10.480 --> 00:25:12.160
- Yeah, yeah, yeah.


00:25:12.160 --> 00:25:13.000
There's a lot of,


00:25:13.000 --> 00:25:16.840
there's a lot of things playing in that space, right?


00:25:16.840 --> 00:25:18.320
Some different places.


00:25:18.320 --> 00:25:21.320
Okay, so yeah, very cool.


00:25:21.320 --> 00:25:23.800
But as a regular user, I would say,


00:25:23.800 --> 00:25:25.560
regardless of how you feel about this,


00:25:25.560 --> 00:25:28.840
at least this is my viewpoint right now,


00:25:28.840 --> 00:25:30.640
it's like regardless of how I feel


00:25:30.640 --> 00:25:34.400
about which side is right in these kinds of disputes,


00:25:34.400 --> 00:25:35.840
this stuff is out of the bag.


00:25:35.840 --> 00:25:39.160
It's out there and available and it's a tool.


00:25:39.160 --> 00:25:41.100
And it's like saying, I don't wanna use spell check


00:25:41.100 --> 00:25:45.760
or I don't wanna use some kind of like code checking.


00:25:45.760 --> 00:25:47.880
I just wanna write like in straight notepad


00:25:47.880 --> 00:25:48.920
because it's pure, right?


00:25:48.920 --> 00:25:51.440
Like, sure, you could do that, but there's these tools


00:25:51.440 --> 00:25:53.240
that will help us be more productive,


00:25:53.240 --> 00:25:56.320
and it's better to embrace them and know them


00:25:56.320 --> 00:25:58.800
than to just like yell at them, I suppose.


00:25:58.800 --> 00:26:00.320
(laughing)


00:26:00.320 --> 00:26:02.320
- Yeah, a lot of accelerant you can get,


00:26:02.320 --> 00:26:04.680
really speed up whatever you wanna get done.


00:26:04.680 --> 00:26:05.660
- Yeah, absolutely.


00:26:05.660 --> 00:26:09.440
All right, so speaking of speeding up things,


00:26:09.440 --> 00:26:11.040
let's talk pandas.


00:26:11.040 --> 00:26:13.280
And not even my artificial pandas,


00:26:13.280 --> 00:26:15.720
but actual programming pandas.


00:26:15.720 --> 00:26:17.920
With this project that you all have


00:26:17.920 --> 00:26:22.920
from Approximate Labs called Sketch.


00:26:22.920 --> 00:26:26.040
So Sketch is pretty awesome.


00:26:26.040 --> 00:26:28.440
Sketch is actually why we're talking today


00:26:28.440 --> 00:26:31.200
because I first talked about this on Python Bytes


00:26:31.200 --> 00:26:34.720
and I saw this was sent over there by Jake Furman


00:26:34.720 --> 00:26:37.120
and to me and said, "You should check this thing out.


00:26:37.120 --> 00:26:38.200
"It's awesome."


00:26:38.200 --> 00:26:41.280
And yeah, it's pretty nuts.


00:26:41.280 --> 00:26:43.720
So tell us about Sketch.


00:26:43.720 --> 00:26:47.400
- Yeah, so even though I use a Copilot,


00:26:47.400 --> 00:26:50.280
as sort of described already, and it's become a crux,


00:26:50.280 --> 00:26:53.760
I found in Jupyter Notebooks, when I wanted to work with data,


00:26:53.760 --> 00:26:55.160
it just didn't--


00:26:55.160 --> 00:26:57.360
it doesn't actually apply.


00:26:57.360 --> 00:27:01.280
So on one side, it was sort of like missing the mark at times.


00:27:01.280 --> 00:27:02.920
And so it was sort of like, how can I


00:27:02.920 --> 00:27:05.600
get this integrated into my flow,


00:27:05.600 --> 00:27:07.560
the way I actually work in a Jupyter Notebook?


00:27:07.560 --> 00:27:09.880
Maybe I'm working a Jupyter Notebook on a remote server,


00:27:09.880 --> 00:27:11.440
and I don't want to set up VS Code to do it,


00:27:11.440 --> 00:27:12.920
so I don't have Copilot at all.


00:27:12.920 --> 00:27:14.160
Like, there's a bunch of different reasons


00:27:14.160 --> 00:27:15.560
that I was just like, in Jupyter.


00:27:15.560 --> 00:27:17.160
It's a very different IDE experience.


00:27:17.160 --> 00:27:18.440
- Yeah, it's super different.


00:27:18.440 --> 00:27:21.600
But also, you might want to ask questions about the data,


00:27:21.600 --> 00:27:24.320
not the structure of the code that analyzes the data, right?


00:27:24.320 --> 00:27:25.160
- Exactly, yeah.


00:27:25.160 --> 00:27:27.600
And so just a bunch of that type of stuff.


00:27:27.600 --> 00:27:29.640
And then also at the other side,


00:27:29.640 --> 00:27:31.840
I was, this sounds,


00:27:31.840 --> 00:27:34.840
I was trying to find something that I could throw together


00:27:34.840 --> 00:27:37.880
that I thought was a strong demonstration


00:27:37.880 --> 00:27:41.760
of the value Approximate Labs is trying to chase,


00:27:41.760 --> 00:27:43.720
but wouldn't take me too much time to make.


00:27:43.720 --> 00:27:44.920
So it was a,


00:27:44.920 --> 00:27:46.320
oh, I could probably just go throw this together


00:27:46.320 --> 00:27:47.140
pretty quickly.


00:27:47.140 --> 00:27:49.700
I bet this is gonna be actually useful and helpful.


00:27:49.700 --> 00:27:51.440
And so let's just do that.


00:27:51.440 --> 00:27:55.700
And so through on top of the actual library I was using


00:27:55.700 --> 00:27:58.440
that was Sketch, I put this on it and then shipped it.


00:27:58.440 --> 00:28:01.440
So sort of shifted what the project was.


00:28:01.440 --> 00:28:02.280
- Yeah, yeah.


00:28:02.280 --> 00:28:05.740
So you also have this other project called Lambda Prompt.


00:28:05.740 --> 00:28:08.280
So were you trying to play around Lambda Prompt


00:28:08.280 --> 00:28:10.640
and then like see what you could kind of apply here


00:28:10.640 --> 00:28:12.180
to leverage it or is that?


00:28:12.180 --> 00:28:15.400
- Yeah, yeah, the full journey I can get into


00:28:15.400 --> 00:28:20.400
It started with data sketches, left my last job


00:28:20.400 --> 00:28:24.200
to chase bringing the algorithm,


00:28:24.200 --> 00:28:26.680
like combining data sketches with AI,


00:28:26.680 --> 00:28:29.200
but just like the vague, like at that level.


00:28:29.200 --> 00:28:30.920
- Go through what a data sketch is real quick.


00:28:30.920 --> 00:28:31.760
- Sure, yeah.


00:28:31.760 --> 00:28:35.600
So a data sketch is a probabilistic aggregation of data.


00:28:35.600 --> 00:28:38.400
So if you have a, I think the most common one


00:28:38.400 --> 00:28:40.760
that people have heard of is HyperLogLog,


00:28:40.760 --> 00:28:42.940
and it's used to estimate cardinality.


00:28:42.940 --> 00:28:45.700
So estimate the number of unique values in a column.


00:28:45.700 --> 00:28:49.700
A data sketch is a class of algorithms


00:28:49.700 --> 00:28:54.980
that all use roughly fixed width in binary, usually,


00:28:54.980 --> 00:28:56.020
representations.


00:28:56.020 --> 00:28:59.780
And then in a single pass, so they're ON,


00:28:59.780 --> 00:29:02.860
will look at each row and hash the row


00:29:02.860 --> 00:29:04.020
and then update the sketch.


00:29:04.020 --> 00:29:07.260
Or not necessarily hash, but they update this sketch object,


00:29:07.260 --> 00:29:08.220
essentially.


00:29:08.220 --> 00:29:11.820
And those sketch objects also have another property,


00:29:11.820 --> 00:29:13.020
that they are mergeable.


00:29:13.020 --> 00:29:17.460
So you have this really fast ON to aggregate up,


00:29:17.460 --> 00:29:19.380
and you get this mergeability.


00:29:19.380 --> 00:29:23.060
So you can map reduce it in trivial speeds.


00:29:23.060 --> 00:29:27.500
The net result is that this tight binary packed object


00:29:27.500 --> 00:29:31.260
can be used to approximate measures you were looking


00:29:31.260 --> 00:29:33.060
for on the original data.


00:29:33.060 --> 00:29:36.580
So you could look at-- if you do a few of these--


00:29:36.580 --> 00:29:38.820
they're called theta sketches-- you can go and estimate


00:29:38.820 --> 00:29:40.660
not just the unique count, but you can also


00:29:40.660 --> 00:29:43.940
estimate if this one column would join well with this other column.


00:29:43.940 --> 00:29:47.540
Or you can estimate, oh, if I were to join this column to this column,


00:29:47.540 --> 00:29:51.300
then this third column that was on that other table would actually be correlated


00:29:51.300 --> 00:29:53.380
to this first column over here. So you get


00:29:53.380 --> 00:29:57.540
these like a bunch of different distributions,


00:29:57.540 --> 00:29:59.780
you get a whole bunch of these types of properties,


00:29:59.780 --> 00:30:04.740
and each sketch is sort of just, I would say, algorithmically engineered,


00:30:04.740 --> 00:30:09.460
like very, very engineered to be like information theory optimal at solving


00:30:09.460 --> 00:30:12.200
one of those measures on the data.


00:30:12.200 --> 00:30:15.080
And so they're tight packed binary representations.


00:30:15.080 --> 00:30:16.080
>> All right.


00:30:16.080 --> 00:30:20.800
So you thought about, well, that's cool, but chat CDP is cool too.


00:30:20.800 --> 00:30:21.800
>> Yeah.


00:30:21.800 --> 00:30:22.800
>> What else?


00:30:22.800 --> 00:30:23.800
>> Yeah.


00:30:23.800 --> 00:30:34.300
The core thing was, so those representations aren't usable by AI right now.


00:30:34.300 --> 00:30:39.900
And when you actually go and use GPT-3 or something like this, you have to figure out


00:30:39.900 --> 00:30:42.880
a way to build the prompt to get it to do what you want.


00:30:42.880 --> 00:30:47.000
This was especially true in a pre-instruction tuning world.


00:30:47.000 --> 00:30:52.300
You had to really play the prompt engineer role even more than you have to now.


00:30:52.300 --> 00:30:55.900
Now you can sort of get away with describing it to ChatGPT.


00:30:55.900 --> 00:31:00.000
And one of the things that you really have to play the game of is how do you get all


00:31:00.000 --> 00:31:03.400
all the information it's going to need into this prompt


00:31:03.400 --> 00:31:07.380
in a succinct but good enough way that it helps it do this.


00:31:07.380 --> 00:31:11.960
And so what Sketch was about was rather than just looking


00:31:11.960 --> 00:31:14.960
at the context of the data, like the metadata,


00:31:14.960 --> 00:31:17.360
the column names and the code you have,


00:31:17.360 --> 00:31:19.640
also go get some representation


00:31:19.640 --> 00:31:22.760
of the content of the data,


00:31:22.760 --> 00:31:25.360
turn that into a string and then bring that string in


00:31:25.360 --> 00:31:26.600
as part of the prompt.


00:31:26.600 --> 00:31:27.600
And then when it has that,


00:31:27.600 --> 00:31:32.040
it should understand much better at actually generating code,


00:31:32.040 --> 00:31:34.200
generating answers to questions.


00:31:34.200 --> 00:31:36.640
And that's what that sketch was a proof of concept of that,


00:31:36.640 --> 00:31:37.720
that worked very well.


00:31:37.720 --> 00:31:41.720
It really quickly showed how valuable actual data content


00:31:41.720 --> 00:31:43.920
context is.


00:31:43.920 --> 00:31:46.240
- Yeah, I would say people are, it's resonating with people.


00:31:46.240 --> 00:31:48.760
It's got 1.5 thousand stars on GitHub.


00:31:48.760 --> 00:31:51.160
It looks about six months old.


00:31:51.160 --> 00:31:53.920
So that's pretty good growth there.


00:31:53.920 --> 00:31:57.160
- Yeah, January 16th was the day I posted it on Hacker News


00:31:57.160 --> 00:32:00.220
and it had three, it was an empty repo at that point.


00:32:00.220 --> 00:32:04.080
- Three stars, it's like me and my friends.


00:32:04.080 --> 00:32:05.560
Okay, cool.


00:32:05.560 --> 00:32:10.560
So this is a tool that basically patches pandas


00:32:10.560 --> 00:32:15.280
to add functionality or functions,


00:32:15.280 --> 00:32:18.760
literally to pandas data frames


00:32:18.760 --> 00:32:22.520
that allows you to ask questions about it, right?


00:32:22.520 --> 00:32:23.360
- Yep.


00:32:23.360 --> 00:32:25.040
- So what kind of questions can you ask it


00:32:25.040 --> 00:32:27.360
and what can it help you with?


00:32:27.360 --> 00:32:30.480
- Yeah, so there's two classes of questions you can ask.


00:32:30.480 --> 00:32:33.120
You can ask it the ask type questions.


00:32:33.120 --> 00:32:36.200
These are sort of from that summary statistics data.


00:32:36.200 --> 00:32:40.320
So from the general representation of your data,


00:32:40.320 --> 00:32:43.240
ask it to like give you answers about it.


00:32:43.240 --> 00:32:44.080
Like what are the columns here?


00:32:44.080 --> 00:32:45.400
You sort of have a conversation


00:32:45.400 --> 00:32:50.280
where it sort of understands the general shape of the data,


00:32:50.280 --> 00:32:53.640
general distributions, things like that, number of uniques,


00:32:53.640 --> 00:32:58.200
and give that context to it, ask questions of that system.


00:32:58.200 --> 00:33:01.360
And then the other one is ask it how to do something.


00:33:01.360 --> 00:33:03.400
So you specifically can get it to write code


00:33:03.400 --> 00:33:04.520
to solve a problem you have.


00:33:04.520 --> 00:33:05.560
You describe the problem you want


00:33:05.560 --> 00:33:07.640
and you can ask it to do that.


00:33:07.640 --> 00:33:08.920
- I've got this data frame.


00:33:08.920 --> 00:33:12.840
I wanna plot a graph of this versus that,


00:33:12.840 --> 00:33:14.880
but color by the other thing.


00:33:14.880 --> 00:33:18.360
- Yep, and in the data space world,


00:33:18.360 --> 00:33:20.640
what I sort of decided to do is like in the demo here


00:33:20.640 --> 00:33:23.260
is just sort of walk through


00:33:23.260 --> 00:33:25.520
what are some standard things people wanna ask of data?


00:33:25.520 --> 00:33:27.980
Like what are those common questions that you hear


00:33:27.980 --> 00:33:32.860
like in Slack between a business team and an analyst team?


00:33:32.860 --> 00:33:35.220
And it's just sort of like, oh, can you do this?


00:33:35.220 --> 00:33:36.820
Can you get me this?


00:33:36.820 --> 00:33:38.180
Can you tell me if there's any PII?


00:33:38.180 --> 00:33:39.140
Is this safe to send?


00:33:39.140 --> 00:33:40.720
Can I send the CSV around?


00:33:40.720 --> 00:33:43.060
Can you clean up this CSV?


00:33:43.060 --> 00:33:44.980
Oh, I need to load this into our catalog.


00:33:44.980 --> 00:33:46.380
Can you describe each of these columns


00:33:46.380 --> 00:33:48.160
and check the data types?


00:33:48.160 --> 00:33:50.940
All the way to, can you actually go get me analytics


00:33:50.940 --> 00:33:51.780
or plot this?


00:33:52.760 --> 00:33:54.100
- Yeah, awesome.


00:33:54.100 --> 00:33:57.920
So, and it plugs right into Jupyter Notebooks.


00:33:57.920 --> 00:34:02.920
You can just import it and basically installing Sketch,


00:34:02.920 --> 00:34:05.720
which is a pip or conda type thing,


00:34:05.720 --> 00:34:08.480
and then you just import it and it's good to go, right?


00:34:08.480 --> 00:34:13.000
- Yep, using the Pandas extensions API,


00:34:13.000 --> 00:34:16.140
which allows you to essentially hook into their data frame,


00:34:16.140 --> 00:34:20.940
call back and register a, you know, a function.


00:34:20.940 --> 00:34:25.460
- Interesting, so it's not as jammed on from the outside.


00:34:25.460 --> 00:34:27.620
It's a little more, plays a little nicer with pandas


00:34:27.620 --> 00:34:30.220
rather than just like, we're gonna go to the class


00:34:30.220 --> 00:34:31.820
and just hack on it.


00:34:31.820 --> 00:34:35.060
- Yeah, yeah, yeah, not full monkey patching here.


00:34:35.060 --> 00:34:37.460
It's like half supported, I think.


00:34:37.460 --> 00:34:41.240
I don't see it used often, but it is somewhere in the docs.


00:34:41.240 --> 00:34:42.940
- Excellent, but here it is.


00:34:42.940 --> 00:34:46.340
So what I wanted to do for this is there's an example


00:34:46.340 --> 00:34:48.820
that you can do, like if you go to the repo,


00:34:48.820 --> 00:34:50.300
which obviously I'll link to.


00:34:50.300 --> 00:34:52.600
There's a video, which I mean, mad props to you


00:34:52.600 --> 00:34:54.900
because I review so many things,


00:34:54.900 --> 00:34:56.620
especially for the Python Bytes podcast,


00:34:56.620 --> 00:34:57.940
where there's a bunch of news items,


00:34:57.940 --> 00:34:59.300
new things we're just gonna check out.


00:34:59.300 --> 00:35:04.300
And we'll find people recommending GUI frameworks


00:35:04.300 --> 00:35:06.420
that have not a single screenshot.


00:35:06.420 --> 00:35:07.700
(laughs)


00:35:07.700 --> 00:35:09.100
Other types of things like,


00:35:09.100 --> 00:35:11.820
I have no way to judge whether this thing even might look,


00:35:11.820 --> 00:35:13.420
like, what does it even make?


00:35:13.420 --> 00:35:15.320
I don't even know, but somebody put a lot of effort,


00:35:15.320 --> 00:35:17.080
but they didn't bother to post an image.


00:35:17.080 --> 00:35:20.200
And you posted a minute and a half animation


00:35:20.200 --> 00:35:22.240
of it going through this process, which


00:35:22.240 --> 00:35:23.400
is really, really excellent.


00:35:23.400 --> 00:35:27.320
So people can go and watch that one minute, one minute 30


00:35:27.320 --> 00:35:29.080
video.


00:35:29.080 --> 00:35:31.960
But there's also a Colab, open in Google Colab,


00:35:31.960 --> 00:35:37.160
which gives you a running interactive variant here.


00:35:37.160 --> 00:35:42.440
So you can just follow along and play these pieces.


00:35:42.440 --> 00:35:45.080
It requires me to sign up on and run, but that's OK.


00:35:45.080 --> 00:35:47.880
So let me talk people through some of the things it does.


00:35:47.880 --> 00:35:51.440
And you can tell me what it's doing, how it's doing that,


00:35:51.440 --> 00:35:53.880
how people might find that advantageous.


00:35:53.880 --> 00:35:57.920
So import sketch, import pandas as PD standard.


00:35:57.920 --> 00:35:59.800
And then you can say pandas read CSV.


00:35:59.800 --> 00:36:03.980
And you give it one from some example CSV


00:36:03.980 --> 00:36:07.040
that you got on one of your GitHub repos, right?


00:36:07.040 --> 00:36:08.240
Or in your account.


00:36:08.240 --> 00:36:10.280
- Yeah, I found one online and then added


00:36:10.280 --> 00:36:12.160
just random synthetic data to it.


00:36:12.160 --> 00:36:13.680
- Yeah, like, oh, here's a data dump.


00:36:13.680 --> 00:36:14.520
No, just kidding.


00:36:14.520 --> 00:36:18.600
So then you need to go to that data frame called sales data.


00:36:18.600 --> 00:36:27.400
You say .sketch.ask as a string, what columns might have PII, personal identifying information,


00:36:27.400 --> 00:36:28.400
in them?


00:36:28.400 --> 00:36:29.400
Awesome.


00:36:29.400 --> 00:36:34.640
And so, it comes, tell me how that works and what it's doing here.


00:36:34.640 --> 00:36:41.400
So it does, I guess, it has to build up the prompt, which is sent to GPT, so to OpenAI's


00:36:41.400 --> 00:36:43.020
specific completion endpoint.


00:36:43.020 --> 00:36:46.260
The building up the prompt, it looks at the data frame.


00:36:46.260 --> 00:36:48.740
It does a bunch of summarization stats on it.


00:36:48.740 --> 00:36:52.540
So it calculates uniques and sums and things like that.


00:36:52.540 --> 00:36:54.020
There's two modes in the backend


00:36:54.020 --> 00:36:55.780
that either does sketches to do those


00:36:55.780 --> 00:36:58.980
or it just uses like df.describe type stuff.


00:36:58.980 --> 00:37:01.940
And then it pulls those summary stats together


00:37:01.940 --> 00:37:03.660
for all the columns,


00:37:03.660 --> 00:37:07.100
throws it together with the rest of the prompt I have.


00:37:07.100 --> 00:37:08.020
You can go find it.


00:37:08.020 --> 00:37:11.540
But then it sends that prompt.


00:37:11.540 --> 00:37:14.620
It also grabs some information off of inspect.


00:37:14.620 --> 00:37:17.340
So it sort of like walks the stack up


00:37:17.340 --> 00:37:18.800
to go and check the variable name


00:37:18.800 --> 00:37:20.740
'cause the data frame is named sales data.


00:37:20.740 --> 00:37:23.300
So it actually tries to go find that variable name


00:37:23.300 --> 00:37:25.060
in your call stack so that it can,


00:37:25.060 --> 00:37:27.260
when it writes code, it writes valid code.


00:37:27.260 --> 00:37:29.500
Puts all that together, send it off to OpenAI,


00:37:29.500 --> 00:37:32.740
gets code back, uses Python AST to parse it,


00:37:32.740 --> 00:37:33.740
check that it's valid.


00:37:33.740 --> 00:37:35.540
If it's not valid Python code,


00:37:35.540 --> 00:37:37.620
or you tried to import something that you don't have,


00:37:37.620 --> 00:37:40.140
it will ask it to rewrite once.


00:37:40.140 --> 00:37:42.820
So this is sort of like an iterative process.


00:37:42.820 --> 00:37:44.740
So it takes the error or it takes the thing


00:37:44.740 --> 00:37:46.180
and it sends it back to OpenAI.


00:37:46.180 --> 00:37:47.700
It's like, hey, fix this code.


00:37:47.700 --> 00:37:50.400
And then, or in this case, sorry, ask.


00:37:50.400 --> 00:37:53.020
It actually just takes this, sends that exact same prompt


00:37:53.020 --> 00:37:55.340
but it just changes the last question to,


00:37:55.340 --> 00:37:57.780
can you answer this question off of the information we have.


00:37:57.780 --> 00:37:58.620
- Yeah, yeah, yeah.


00:37:58.620 --> 00:38:02.400
And so that sounds very, very similar to my arrow program.


00:38:02.400 --> 00:38:04.180
Rewrite it with guarding clauses, redo it.


00:38:04.180 --> 00:38:07.660
Like kind of, I gave you this data and this code


00:38:07.660 --> 00:38:08.780
and I asked you this question


00:38:08.780 --> 00:38:10.780
and you can have a little conversation,


00:38:10.780 --> 00:38:12.780
but at some point you're like, "All right, well,


00:38:12.780 --> 00:38:14.780
we're going to take what it gives me


00:38:14.780 --> 00:38:16.780
after a couple of rounds at it."


00:38:16.780 --> 00:38:18.780
- Yeah, I take the first one that doesn't,


00:38:18.780 --> 00:38:21.780
that passes an import check and passes AST linting.


00:38:21.780 --> 00:38:24.780
When you use small models,


00:38:24.780 --> 00:38:26.780
you run into not valid Python a lot more,


00:38:26.780 --> 00:38:29.780
but with these ones, it's almost always good.


00:38:29.780 --> 00:38:32.780
- It's ridiculous, yeah. It's crazy.


00:38:32.780 --> 00:38:35.780
Okay, so it says, "The columns that might have PII in them


00:38:35.780 --> 00:38:39.820
in them are credit card, SSN, and purchase address.


00:38:39.820 --> 00:38:41.860
Okay, that's pretty excellent.


00:38:41.860 --> 00:38:46.860
And then you say, all right, salesdata.sketch.ask,


00:38:46.860 --> 00:38:50.000
can you give me friendly names for each column


00:38:50.000 --> 00:38:52.780
and output this as an HTML list,


00:38:52.780 --> 00:38:56.100
which is parsed as HTML and rendered


00:38:56.100 --> 00:38:58.000
in Jupyter Notebook accurately, right?


00:38:58.000 --> 00:39:00.000
So it says index, well, that's an index.


00:39:00.000 --> 00:39:01.980
- This one ends up being the same.


00:39:01.980 --> 00:39:04.780
- It's not a great, this one is not a great example


00:39:04.780 --> 00:39:07.100
because it doesn't have to infer


00:39:07.100 --> 00:39:11.700
because the names are like order space date, right?


00:39:11.700 --> 00:39:14.220
Instead of order, like maybe lowercase O


00:39:14.220 --> 00:39:16.660
and then like attach the big D or whatever,


00:39:16.660 --> 00:39:18.980
but it'll give you some more information.


00:39:18.980 --> 00:39:20.760
You can like kind of ask it questions


00:39:20.760 --> 00:39:22.920
about the type of data, right?


00:39:22.920 --> 00:39:24.580
- Yep, yeah, exactly.


00:39:24.580 --> 00:39:26.740
I found this is really good at,


00:39:26.740 --> 00:39:29.020
if you play the game and you just name all your columns,


00:39:29.020 --> 00:39:31.220
like call one, call two, call three, call four,


00:39:31.220 --> 00:39:32.940
and you ask it, give me new column names for all of these.


00:39:32.940 --> 00:39:34.500
It gives you something that's pretty reasonable


00:39:34.500 --> 00:39:35.340
based off of the data.


00:39:35.340 --> 00:39:36.160
So pretty useful.


00:39:36.160 --> 00:39:37.900
- Okay, so it's like, oh, these look like addresses.


00:39:37.900 --> 00:39:38.740
So we'll call that address.


00:39:38.740 --> 00:39:41.180
And this looks like social security numbers


00:39:41.180 --> 00:39:43.260
and credit scores and whatnot.


00:39:43.260 --> 00:39:44.100
- Yep, so it can really help


00:39:44.100 --> 00:39:47.140
with that quick first onboarding step.


00:39:47.140 --> 00:39:48.140
- Yeah, so, all right.


00:39:48.140 --> 00:39:49.500
Everyone heard it here first.


00:39:49.500 --> 00:39:52.660
Just name all your columns, one, two, three, four,


00:39:52.660 --> 00:39:53.860
and then just get help.


00:39:53.860 --> 00:39:56.740
AI, what do we call these?


00:39:56.740 --> 00:40:02.620
All right, so the next thing you did in this demo notebook


00:40:02.620 --> 00:40:04.980
was you said salesdata.sketch.


00:40:04.980 --> 00:40:07.540
And this is different before, I believe.


00:40:07.540 --> 00:40:09.860
Because before you were saying ask.


00:40:09.860 --> 00:40:12.220
And now you can say how to.


00:40:12.220 --> 00:40:16.420
Create some derived features from the address.


00:40:16.420 --> 00:40:17.460
Tell us about that.


00:40:17.460 --> 00:40:20.780
- Yeah, so this is the one that actually is the code writing.


00:40:20.780 --> 00:40:22.500
It's essentially the exact same prompt,


00:40:22.500 --> 00:40:24.780
but the change is the very end.


00:40:24.780 --> 00:40:27.160
It says, like, return this as Python code


00:40:27.160 --> 00:40:28.540
that you can execute to do this.


00:40:28.540 --> 00:40:30.340
So instead of answering the question directly,


00:40:30.340 --> 00:40:32.660
answer the question with code that will answer the question.


00:40:32.660 --> 00:40:33.500
- Right.


00:40:33.500 --> 00:40:36.580
Write a Python line of code that will answer this question


00:40:36.580 --> 00:40:38.180
given this data, something like that.


00:40:38.180 --> 00:40:39.660
- Yep, yep, something like that.


00:40:39.660 --> 00:40:41.580
I don't remember exactly anymore, it's been a while.


00:40:41.580 --> 00:40:44.020
But yeah, some, I've iterated a little bit


00:40:44.020 --> 00:40:46.660
until it started working and I was like, "Okay, cool."


00:40:46.660 --> 00:40:50.380
And so, ask it for that and then it spits back code.


00:40:50.380 --> 00:40:54.780
And that was, it sounds overly simple, but that was it.


00:40:54.780 --> 00:40:56.260
That was the moment and I was just like,


00:40:56.260 --> 00:40:59.020
"Oh, I could just ask it to do my analytics for me."


00:40:59.020 --> 00:41:00.660
And it's just all the, every other feature


00:41:00.660 --> 00:41:04.260
just sort of became like apparently solvable with this.


00:41:04.260 --> 00:41:06.260
And the more I played with it, the more it was just,


00:41:06.260 --> 00:41:08.380
oh, I don't have to think about,


00:41:08.380 --> 00:41:10.620
I don't even have to go to Google or Stack Overflow


00:41:10.620 --> 00:41:12.740
to ask the question to get the API stuff for me.


00:41:12.740 --> 00:41:15.640
I could, from zero to I have code that's working


00:41:15.640 --> 00:41:17.380
is one step in Jupyter.


00:41:17.380 --> 00:41:19.380
- So you wrote that how-to and you gave it the question


00:41:19.380 --> 00:41:21.580
and then it wrote the lines of code


00:41:21.580 --> 00:41:24.300
and you just drop that into the next cell


00:41:24.300 --> 00:41:25.860
and just run it, right?


00:41:25.860 --> 00:41:27.580
And so for example, in this example, it said,


00:41:27.580 --> 00:41:29.620
well, we can come up with city, state, and zip code


00:41:29.620 --> 00:41:34.620
and by writing a vector transform


00:41:34.620 --> 00:41:36.900
by passing a lambda that'll pull out, you know,


00:41:36.900 --> 00:41:39.380
the city from the string that was the full address


00:41:39.380 --> 00:41:41.060
and so on, right?


00:41:41.060 --> 00:41:41.900
- Yeah.


00:41:41.900 --> 00:41:43.220
- That's pretty neat.


00:41:43.220 --> 00:41:45.820
- Yeah, it's fun to see what it does.


00:41:45.820 --> 00:41:47.820
Again, not these things are always probabilistic,


00:41:47.820 --> 00:41:51.180
but it also usually serves as a great starting point


00:41:51.180 --> 00:41:52.700
if even if it doesn't get it right.


00:41:52.700 --> 00:41:53.540
- Yeah, sure.


00:41:53.540 --> 00:41:54.380
Like, okay, I see.


00:41:54.380 --> 00:41:55.900
Maybe that's not exactly right


00:41:55.900 --> 00:41:58.440
'cause we have Europeans in their city maybe,


00:41:58.440 --> 00:42:02.460
and their zip code are in different orders sometimes,


00:42:02.460 --> 00:42:07.860
but it gives you something to work with pretty quickly,


00:42:07.860 --> 00:42:10.780
right, by asking just, what can I do?


00:42:10.780 --> 00:42:13.340
And then another one, this one's a little more interesting.


00:42:13.340 --> 00:42:15.380
Instead of just saying like, well,


00:42:15.380 --> 00:42:16.780
what other things can we pull out?


00:42:16.780 --> 00:42:19.900
It's like, this gets towards the analytic side, right?


00:42:19.900 --> 00:42:22.980
It says, get the top five grossing states


00:42:22.980 --> 00:42:25.020
for the sales data, right?


00:42:25.020 --> 00:42:30.900
And it writes a group by some sorts and then it does a head given five.


00:42:30.900 --> 00:42:34.380
And yeah, that's pretty neat.


00:42:34.380 --> 00:42:35.380
Tell us about this.


00:42:35.380 --> 00:42:36.380
I mean, I guess it's about the same, right?


00:42:36.380 --> 00:42:37.380
Just ask more questions.


00:42:37.380 --> 00:42:38.380
Yeah.


00:42:38.380 --> 00:42:41.100
They all feel pretty similar to me.


00:42:41.100 --> 00:42:47.820
I think I guess I could jump towards like things that I wanted to put next, but we're


00:42:47.820 --> 00:42:49.980
not reliable enough to like really make the cut.


00:42:49.980 --> 00:42:55.660
I wanted to have it go like, in my question was like, go build a model that predicts sales


00:42:55.660 --> 00:43:00.980
for the next six months and then plot it on like, on a, you know, 2d plot with a dotted


00:43:00.980 --> 00:43:03.220
line for the predicted plot.


00:43:03.220 --> 00:43:07.740
And like, it would try, but it would, it would always do something off.


00:43:07.740 --> 00:43:10.740
And I found I always had to break up the prompt into like smaller, smaller.


00:43:10.740 --> 00:43:12.660
Get intern level code back.


00:43:12.660 --> 00:43:15.580
Yeah, it sort of works.


00:43:15.580 --> 00:43:16.980
Except, the real data.


00:43:16.980 --> 00:43:21.700
It was fun getting it to train models, but it was also its own separate thing I didn't


00:43:21.700 --> 00:43:24.220
play with too much.


00:43:24.220 --> 00:43:25.660
So that's that.


00:43:25.660 --> 00:43:30.460
There's another part of Sketch that I guess is not in this notebook, I didn't realize.


00:43:30.460 --> 00:43:35.380
Because you have to use the OpenAI API key, but it's the Sketch Apply.


00:43:35.380 --> 00:43:36.380
And that's the...


00:43:36.380 --> 00:43:42.180
I'll say this one is another just power tool.


00:43:42.180 --> 00:43:43.180
This one has...


00:43:43.180 --> 00:43:44.340
I don't really talk about...


00:43:44.340 --> 00:43:48.380
I don't even include it in the video because it's not just like as plug and play, you do


00:43:48.380 --> 00:43:50.600
have to go set an environment variable.


00:43:50.600 --> 00:43:53.620
And so it's like, it's one step further than I want to.


00:43:53.620 --> 00:43:55.780
It's not terrible, but it's a step.


00:43:55.780 --> 00:44:05.340
And so, but what it does is it lets you apply a completion endpoint of whatever your design


00:44:05.340 --> 00:44:06.340
row wise.


00:44:06.340 --> 00:44:09.400
So every single row, you can go and apply and run something.


00:44:09.400 --> 00:44:16.400
So if every row of your pandas data frame is a, I don't know, some serialized text from


00:44:16.400 --> 00:44:21.240
a PDF or a file in your directory structure, and you just load it as a data frame, you


00:44:21.240 --> 00:44:27.880
can do .df.sketch.apply, and it's almost the exact same as df.apply.


00:44:27.880 --> 00:44:31.860
But the thing you've put in as your function is now just a Jinja template that will fill


00:44:31.860 --> 00:44:38.100
in your column variables for that row, and then ask GPT to continue completing.


00:44:38.100 --> 00:44:41.780
So I think I did silly ones like here's a few states,


00:44:41.780 --> 00:44:45.860
and then the prompt is extract the state for it.


00:44:45.860 --> 00:44:47.980
Right, extract the capital of the state?


00:44:47.980 --> 00:44:48.780
Yeah.


00:44:48.780 --> 00:44:52.980
Yeah, so pure information extraction from it,


00:44:52.980 --> 00:44:54.260
but you can--


00:44:54.260 --> 00:44:56.740
this grows into a lot more.


00:44:56.740 --> 00:44:58.260
So does that come out of the data,


00:44:58.260 --> 00:45:02.140
or is that coming out of open AI where it sees


00:45:02.140 --> 00:45:04.580
where is the capital of state, and it sees New York.


00:45:04.580 --> 00:45:07.660
It's like, OK, well, all right, Albany.


00:45:07.660 --> 00:45:09.500
- Yeah, this is purely extracting


00:45:09.500 --> 00:45:11.100
out of the model weights, essentially.


00:45:11.100 --> 00:45:13.340
This is not like a factual extraction.


00:45:13.340 --> 00:45:15.780
So this is probably a bad example, 'cause it's like, eh.


00:45:15.780 --> 00:45:17.500
But the thing that I actually,


00:45:17.500 --> 00:45:19.380
actually the better example I did once was,


00:45:19.380 --> 00:45:21.120
what is like some interesting colors


00:45:21.120 --> 00:45:23.100
that are good for each state?


00:45:23.100 --> 00:45:25.340
And it like just came up with a sort of like flaggish colors


00:45:25.340 --> 00:45:26.620
or sports team colors.


00:45:26.620 --> 00:45:29.220
That was sort of fun when it wrote that as hex.


00:45:29.220 --> 00:45:30.860
You can also do things like,


00:45:30.860 --> 00:45:32.920
if you have a large text document,


00:45:32.920 --> 00:45:35.020
or you can, actually, I'll even do the more common one


00:45:35.020 --> 00:45:36.180
that I think everybody actually wants,


00:45:36.180 --> 00:45:38.740
is you have messy data, you have addresses


00:45:38.740 --> 00:45:40.900
that are like syntactically messy,


00:45:40.900 --> 00:45:42.700
and you can say, normalize these addresses


00:45:42.700 --> 00:45:43.660
to be in this form.


00:45:43.660 --> 00:45:45.620
And you sort of just write one example,


00:45:45.620 --> 00:45:48.160
say run.apply, and you get a new column


00:45:48.160 --> 00:45:50.460
that is that cleaned up data.


00:45:50.460 --> 00:45:52.220
- Yeah, incredible.


00:45:52.220 --> 00:45:57.620
Okay, a couple things here.


00:45:57.620 --> 00:46:02.620
It says I can use, can directly call OpenAI


00:46:02.620 --> 00:46:04.380
and not use your endpoints.


00:46:04.380 --> 00:46:07.940
So at the moment, it kind of proxies through a web service


00:46:07.940 --> 00:46:10.340
that you all have that somehow checks stuff,


00:46:10.340 --> 00:46:11.900
or what does that do?


00:46:11.900 --> 00:46:13.820
- Yeah, it was just a pure ease of use.


00:46:13.820 --> 00:46:15.780
I wanted people to be able to do pip install


00:46:15.780 --> 00:46:17.900
and import sketch and actually get it,


00:46:17.900 --> 00:46:22.300
'cause I know how much I use things in Colab


00:46:22.300 --> 00:46:24.420
or in Jupyter Notebooks on weird machines,


00:46:24.420 --> 00:46:27.340
and remembering an environment variable, managing secrets,


00:46:27.340 --> 00:46:29.660
it's like this whole overhead that I don't want to deal with.


00:46:29.660 --> 00:46:32.940
And so I wanted to just offer a lightweight way


00:46:32.940 --> 00:46:34.640
if you just want to be able to use it.


00:46:34.640 --> 00:46:37.360
But I know that that's not sufficient for security.


00:46:37.360 --> 00:46:39.020
People are going to be conscious of those things


00:46:39.020 --> 00:46:41.860
and want to be able to not go through my proxy thing


00:46:41.860 --> 00:46:42.700
that's there for help.


00:46:42.700 --> 00:46:44.140
So I offer this up.


00:46:44.140 --> 00:46:48.100
- What's next?


00:46:48.100 --> 00:46:49.360
Do you have a roadmap for this?


00:46:49.360 --> 00:46:52.160
Are you happy where it is and you're just letting it be?


00:46:52.160 --> 00:46:56.420
- I don't have much of a roadmap for this right now.


00:46:56.420 --> 00:46:58.940
I'm actually, I guess there's like grand roadmap,


00:46:58.940 --> 00:47:01.380
which is like at the company scale, what we're working on.


00:47:01.380 --> 00:47:07.500
I would say that if this, we're really trying to solve data and with AI just in general.


00:47:07.500 --> 00:47:11.740
And so these are the types of things we hope to open source and just give out there like


00:47:11.740 --> 00:47:13.820
actually everything we're hoping to open source.


00:47:13.820 --> 00:47:19.580
But the starting place is going to be a bunch of these like smaller toolkits or just utility


00:47:19.580 --> 00:47:22.340
things that hopefully save people time are very useful.


00:47:22.340 --> 00:47:28.540
The grand thing we're working towards, I guess, is this more like the full automated data


00:47:28.540 --> 00:47:29.540
stack.


00:47:29.540 --> 00:47:31.920
I think that people have wanted, where you just ask it


00:47:31.920 --> 00:47:34.940
questions, and it goes and pulls the data that you need.


00:47:34.940 --> 00:47:35.980
It cleans it.


00:47:35.980 --> 00:47:37.300
It builds up the full pipeline.


00:47:37.300 --> 00:47:38.300
It executes the pipeline.


00:47:38.300 --> 00:47:40.600
It gets you to the result, and it shows you the result.


00:47:40.600 --> 00:47:43.560
And you can inspect all of that, that whole DAG,


00:47:43.560 --> 00:47:45.820
and say, yes, I trust this.


00:47:45.820 --> 00:47:49.580
So we're working on getting full end to end.


00:47:49.580 --> 00:47:52.300
So when I went and asked about that Arrow program,


00:47:52.300 --> 00:47:54.100
I said, I think this will still do it.


00:47:54.100 --> 00:47:57.300
I think this will probably work again.


00:47:57.300 --> 00:48:00.840
And it did, which is awesome, just the way I expected.


00:48:00.840 --> 00:48:08.020
But AI is not as deterministic as read the number seven.


00:48:08.020 --> 00:48:11.180
If seven is less than eight, do this.


00:48:11.180 --> 00:48:12.760
What is the repeatability?


00:48:12.760 --> 00:48:16.300
What is the experience of doing this?


00:48:16.300 --> 00:48:16.800
I ran it.


00:48:16.800 --> 00:48:18.100
I ran it again.


00:48:18.100 --> 00:48:19.760
Is it going to be pretty much the same?


00:48:19.760 --> 00:48:20.900
Or is it going to have--


00:48:20.900 --> 00:48:24.940
what's the mood of the AI when it gets to you?


00:48:24.940 --> 00:48:27.260
I guess, yeah, this is sort of a parameter you can--


00:48:27.260 --> 00:48:28.260
there's a little bit of a parameter


00:48:28.260 --> 00:48:30.760
you can set if you want to play that game with the temperature


00:48:30.760 --> 00:48:32.580
parameter on these models.


00:48:32.580 --> 00:48:34.100
At higher and higher temperatures,


00:48:34.100 --> 00:48:35.340
you get more and more random.


00:48:35.340 --> 00:48:38.700
But it can also truly be out of left field random


00:48:38.700 --> 00:48:40.660
if you go too high temperature.


00:48:40.660 --> 00:48:42.700
You get maybe more creative solutions.


00:48:42.700 --> 00:48:43.860
Yeah, you could sometimes get that.


00:48:43.860 --> 00:48:45.400
And as you move toward zero, it gets


00:48:45.400 --> 00:48:47.100
more and more deterministic.


00:48:47.100 --> 00:48:50.540
Unfortunately, for really trying to do


00:48:50.540 --> 00:48:54.100
some good, provable, build chain type things


00:48:54.100 --> 00:48:56.820
with like hashing and caching and stuff.


00:48:56.820 --> 00:48:59.060
It's not fully deterministic even at zero temperature.


00:48:59.060 --> 00:49:00.900
But that's just--


00:49:00.900 --> 00:49:02.300
- I think it's worth thinking about,


00:49:02.300 --> 00:49:04.700
but at the same time, run it once,


00:49:04.700 --> 00:49:06.780
see the answers that it gives you,


00:49:06.780 --> 00:49:08.660
comment that business out and just like,


00:49:08.660 --> 00:49:09.900
put that as markdown.


00:49:09.900 --> 00:49:14.580
Freeze it, memorialize it in markdown.


00:49:14.580 --> 00:49:17.400
'Cause you don't need to ask it over and over


00:49:17.400 --> 00:49:19.620
what columns have PII.


00:49:19.620 --> 00:49:21.940
Well, probably the same ones as last time.


00:49:21.940 --> 00:49:24.300
We're just going to write these columns,


00:49:24.300 --> 00:49:26.140
credit card, social security, and purchase address.


00:49:26.140 --> 00:49:27.580
They have that.


00:49:27.580 --> 00:49:29.340
And so now you know, right?


00:49:29.340 --> 00:49:30.260
- Yeah.


00:49:30.260 --> 00:49:31.100
Yeah, yeah, there's always--


00:49:31.100 --> 00:49:33.660
- Is that a reasonable way to think about it?


00:49:33.660 --> 00:49:36.260
- I think, yeah, if you wanna get determinism


00:49:36.260 --> 00:49:37.980
or the performance is a thing that you're worried about,


00:49:37.980 --> 00:49:39.540
yeah, you can always cache.


00:49:39.540 --> 00:49:40.820
I think however you do it,


00:49:40.820 --> 00:49:43.260
comments or actually with systems.


00:49:43.260 --> 00:49:44.780
- Sure, sure, sure.


00:49:44.780 --> 00:49:49.780
Or that like, how do I do that group by sorting business?


00:49:50.700 --> 00:49:52.500
You don't have to ask that over and over.


00:49:52.500 --> 00:49:54.020
Once it gives you the answer--


00:49:54.020 --> 00:49:54.540
Yeah.


00:49:54.540 --> 00:49:58.460
Actually, yeah, my workflow when I use Sketch, definitely,


00:49:58.460 --> 00:50:00.060
I ask the question, I copy the code,


00:50:00.060 --> 00:50:01.440
and then I go delete the question


00:50:01.440 --> 00:50:04.620
or ask a different question for my next problem that I have.


00:50:04.620 --> 00:50:08.180
I don't really-- it's not code that--


00:50:08.180 --> 00:50:11.420
it is a little bit vestigial when you save


00:50:11.420 --> 00:50:13.140
your notebook at the end, and you sort of


00:50:13.140 --> 00:50:15.460
want to go back and delete all the questions you asked.


00:50:15.460 --> 00:50:18.140
Because you don't need to rerun it when you actually just


00:50:18.140 --> 00:50:20.340
go to execute the notebook later.


00:50:20.340 --> 00:50:21.460
>> Yeah, that makes a lot of sense.


00:50:21.460 --> 00:50:22.760
And plus, you look smarter if you


00:50:22.760 --> 00:50:24.760
don't have to show how you got the answers.


00:50:24.760 --> 00:50:26.760
>> Yeah, just look at this beautiful code that's


00:50:26.760 --> 00:50:28.000
even commented.


00:50:28.000 --> 00:50:29.760
>> Yeah, exactly.


00:50:29.760 --> 00:50:32.420
I guess you could probably ask it to comment your code, right?


00:50:32.420 --> 00:50:34.560
>> Yeah, you can ask it to describe.


00:50:34.560 --> 00:50:37.240
There's been some really cool things where people will throw


00:50:37.240 --> 00:50:39.060
like assembly at it and ask it to translate


00:50:39.060 --> 00:50:41.080
to different languages so they can interpret it.


00:50:41.080 --> 00:50:45.000
Or you could do really fun things like cross language,


00:50:45.000 --> 00:50:47.360
cross--


00:50:47.360 --> 00:50:49.480
I guess I'll say like levels of abstraction.


00:50:49.480 --> 00:50:52.060
You could ask it to describe it at a very top level,


00:50:52.060 --> 00:50:54.180
or you can get really precise, like for this line,


00:50:54.180 --> 00:50:56.100
what are all the implications if I change a variable


00:50:56.100 --> 00:50:57.780
or something like that.


00:50:57.780 --> 00:50:59.100
- Yeah, that's really cool.


00:50:59.100 --> 00:51:01.340
I suppose you could do that here.


00:51:01.340 --> 00:51:04.260
Can you converse with it?


00:51:04.260 --> 00:51:06.060
You can say, okay, you gave me this.


00:51:06.060 --> 00:51:08.200
I guess, what's the word?


00:51:08.200 --> 00:51:10.940
Does it have tokens and context like chat.ggp does?


00:51:10.940 --> 00:51:12.460
Can you say, okay, that's cool,


00:51:12.460 --> 00:51:17.460
but I want it as integers, not as strings.


00:51:19.100 --> 00:51:20.660
I did not include that in this.


00:51:20.660 --> 00:51:23.740
There was a version that had something like that


00:51:23.740 --> 00:51:25.940
where I was sort of just keeping the last few calls around,


00:51:25.940 --> 00:51:27.900
but it quickly became,


00:51:27.900 --> 00:51:30.300
it didn't align with the Jupyter IDE experience


00:51:30.300 --> 00:51:31.940
'cause you end up like scrolling up and down


00:51:31.940 --> 00:51:35.680
and you have too much power over how you execute


00:51:35.680 --> 00:51:36.560
in a Jupyter notebook.


00:51:36.560 --> 00:51:38.420
So your context can change dramatically


00:51:38.420 --> 00:51:42.460
by just scrolling up and trying to via inspect


00:51:42.460 --> 00:51:45.460
look across different, like across a Jupyter notebook


00:51:45.460 --> 00:51:46.860
as I was just a whole other nightmare.


00:51:46.860 --> 00:51:49.620
So I didn't try and extract the code out of the notebook


00:51:49.620 --> 00:51:51.440
so that it could understand the local context.


00:51:51.440 --> 00:51:53.960
- You could go straight to ChatsCP or something like that,


00:51:53.960 --> 00:51:56.460
take what it gave you and start asking it questions.


00:51:56.460 --> 00:51:57.300
- Yeah, yeah.


00:51:57.300 --> 00:52:01.760
- Okay, so another question that I had here about this,


00:52:01.760 --> 00:52:04.320
so in order for it to do its magic,


00:52:04.320 --> 00:52:10.040
like you said the really important thought


00:52:10.040 --> 00:52:11.620
or breakthrough idea you had was like,


00:52:11.620 --> 00:52:13.840
not just the structure of the pandas code


00:52:13.840 --> 00:52:14.840
or anything like that,


00:52:14.840 --> 00:52:16.740
but also a little bit about the data.


00:52:16.740 --> 00:52:20.680
What is the privacy implications


00:52:20.680 --> 00:52:23.020
of me asking this question about my data?


00:52:23.020 --> 00:52:27.320
Suppose I have super duper secret CSV


00:52:27.320 --> 00:52:31.760
and should I not ask or how to on it


00:52:31.760 --> 00:52:34.320
or what is the story there?


00:52:34.320 --> 00:52:38.920
If I work with data, how much sharing do I do


00:52:38.920 --> 00:52:40.480
of something I might not wanna share


00:52:40.480 --> 00:52:43.600
if I ask a question about it?


00:52:43.600 --> 00:52:47.860
I'd say the same discretion you'd use if you would copy like a row or a few rows of that


00:52:47.860 --> 00:52:53.500
data into ChatGPT to ask it a question about it.


00:52:53.500 --> 00:53:01.420
Is the level of concern I guess you should have like on the specifically I am not storing


00:53:01.420 --> 00:53:08.740
these things but I know like and I know OpenAI is at least it was it seems like they're going


00:53:08.740 --> 00:53:10.820
to start getting towards like a 30 day thing.


00:53:10.820 --> 00:53:12.740
But so there's a little bit of, yeah, I mean,


00:53:12.740 --> 00:53:14.260
you're sending your stuff over the wire,


00:53:14.260 --> 00:53:16.220
like over network, if you do this.


00:53:16.220 --> 00:53:18.780
And to use these language models until they come local,


00:53:18.780 --> 00:53:21.940
until these things like Llama and Alpaca get good enough,


00:53:21.940 --> 00:53:24.620
that they're, yeah, they're gonna be remote.


00:53:24.620 --> 00:53:26.060
Actually, that could be a fun, sorry.


00:53:26.060 --> 00:53:27.580
I just now thought, that could be a fun thing.


00:53:27.580 --> 00:53:30.540
Like just go get Alpaca working with Sketch


00:53:30.540 --> 00:53:32.580
so that it can be fully local in your--


00:53:32.580 --> 00:53:35.740
- Interesting, like a privacy preserving type of deal.


00:53:35.740 --> 00:53:37.980
- Yeah, hadn't actually, yeah, that's the power


00:53:37.980 --> 00:53:40.640
of these smaller models that are almost good enough.


00:53:40.640 --> 00:53:42.760
I could probably just quickly throw that in here


00:53:42.760 --> 00:53:45.840
and see if it has a wider audience of people.


00:53:45.840 --> 00:53:49.160
>>You have an option to not get through your API


00:53:49.160 --> 00:53:51.440
but directly go to OpenAI.


00:53:51.440 --> 00:53:55.280
You could have another one to pick other options, right?


00:53:55.280 --> 00:53:56.320
Potentially.


00:53:56.320 --> 00:53:57.160
>>Yep, yep.


00:53:57.160 --> 00:54:00.440
The interface to these,


00:54:00.440 --> 00:54:03.160
one thing that I think is not,


00:54:03.160 --> 00:54:05.840
maybe it's talked about it more in other places,


00:54:05.840 --> 00:54:07.960
but I haven't heard as much excitement about it,


00:54:07.960 --> 00:54:11.400
is that the APIs have gotten pretty nice for this whole space.


00:54:11.400 --> 00:54:16.960
The idea of a completion endpoint is pretty straightforward.


00:54:16.960 --> 00:54:19.920
You send it some amount of text, and it will continue that text.


00:54:19.920 --> 00:54:23.560
And it's so simple, but it's so generalizable.


00:54:23.560 --> 00:54:26.880
You could build so many tools off of just that one API endpoint, essentially.


00:54:26.880 --> 00:54:29.440
And so combine that with an embedding endpoint,


00:54:29.440 --> 00:54:33.640
and you sort of have all you need to make complex AI apps.


00:54:36.520 --> 00:54:37.360
- That's crazy.


00:54:37.360 --> 00:54:39.560
Speaking of making AI apps,


00:54:39.560 --> 00:54:43.560
maybe touch a bit on your other projects, Lambda Prompt.


00:54:43.560 --> 00:54:44.840
- So yeah, Lambda Prompt.


00:54:44.840 --> 00:54:45.680
- Yeah, Lambda Prompt.


00:54:45.680 --> 00:54:46.960
- But before you get into it,


00:54:46.960 --> 00:54:49.600
mad props for like Greek letter.


00:54:49.600 --> 00:54:52.600
Like that's a true physicist or mathematician.


00:54:52.600 --> 00:54:54.120
I can appreciate that there.


00:54:54.120 --> 00:54:57.240
- Yeah, I was excited to put it everywhere,


00:54:57.240 --> 00:55:00.000
but then of course, you know, these things don't,


00:55:00.000 --> 00:55:03.320
playing games with character sets and websites.


00:55:03.320 --> 00:55:04.720
I'm the one that causes,


00:55:04.720 --> 00:55:10.440
I both feel the pain, have to clean the data that I also put into these systems.


00:55:10.440 --> 00:55:11.760
People are like, "A prompt?


00:55:11.760 --> 00:55:13.240
Why is the A so italicized?"


00:55:13.240 --> 00:55:15.960
I don't get it.


00:55:15.960 --> 00:55:23.360
So this one came, I was working with, this is pre-GPT.


00:55:23.360 --> 00:55:24.560
This is October.


00:55:24.560 --> 00:55:28.040
I guess it was right around ChatGPT coming out, like around that time.


00:55:28.040 --> 00:55:31.720
But I was really just messing around a lot with completion endpoints as we were talking.


00:55:31.720 --> 00:55:35.440
And I kept rewriting the same request boilerplate over and over.


00:55:35.440 --> 00:55:40.160
And then I also kept rewriting f-strings that I was trying to like send in.


00:55:40.160 --> 00:55:42.920
And I was just like, "Ginja template solved this already."


00:55:42.920 --> 00:55:45.560
Like there already is formatting for strings in Python.


00:55:45.560 --> 00:55:49.000
Let me just use that, compose that into a function.


00:55:49.000 --> 00:55:50.680
And just let me call these completion endpoints.


00:55:50.680 --> 00:55:55.840
I don't want to think of them as like API endpoint or RPC is a nice mental model, but


00:55:55.840 --> 00:55:57.600
I want to use them as functions.


00:55:57.600 --> 00:55:59.240
I want to be able to put decorators on them.


00:55:59.240 --> 00:56:03.260
I want to be able to use them both async or not async


00:56:03.260 --> 00:56:04.520
in Python.


00:56:04.520 --> 00:56:07.400
I just want to have this as a thing


00:56:07.400 --> 00:56:09.560
that I can just call really quickly with one line


00:56:09.560 --> 00:56:11.520
and just do whatever I need to with it.


00:56:11.520 --> 00:56:15.120
And so through this together, it's very simple.


00:56:15.120 --> 00:56:16.760
Like, honestly, I mean, the hardest part


00:56:16.760 --> 00:56:20.000
was just getting all the layers of--


00:56:20.000 --> 00:56:22.000
there's actually two things.


00:56:22.000 --> 00:56:24.880
You can make a prompt that then--


00:56:24.880 --> 00:56:26.840
because I wrap any function as a prompt,


00:56:26.840 --> 00:56:29.880
So not just these calls to GPT.


00:56:29.880 --> 00:56:31.440
And then I do tracing on it.


00:56:31.440 --> 00:56:33.260
So as you like get into the call stack,


00:56:33.260 --> 00:56:35.860
every input and output is you can sort of like


00:56:35.860 --> 00:56:40.200
get hooked into and trace with some like call traces.


00:56:40.200 --> 00:56:43.560
And so there's a bunch of just like weird stuff


00:56:43.560 --> 00:56:44.840
to make the utility nice,


00:56:44.840 --> 00:56:46.840
but functionally, as you can see here,


00:56:46.840 --> 00:56:48.600
it's you just import it,


00:56:48.600 --> 00:56:50.720
you write a Jinja template with the class,


00:56:50.720 --> 00:56:53.920
and then you use that object that comes back as a function


00:56:53.920 --> 00:56:56.720
and your Jinja template variables get filled in


00:56:56.720 --> 00:56:58.480
And your result is the text string


00:56:58.480 --> 00:57:01.680
that comes back out of GPT.


00:57:01.680 --> 00:57:02.760
It's interesting.


00:57:02.760 --> 00:57:04.960
People probably-- some people might be thinking, like,


00:57:04.960 --> 00:57:07.240
Jinja, OK, well, I've got to create an HTML file,


00:57:07.240 --> 00:57:10.640
and not just a string that has double curlies for turning


00:57:10.640 --> 00:57:12.920
stuff into strings within the string.


00:57:12.920 --> 00:57:15.720
Kind of a different way to do f strings,


00:57:15.720 --> 00:57:17.160
as you were hinting at.


00:57:17.160 --> 00:57:18.360
Yeah, yeah.


00:57:18.360 --> 00:57:20.360
There was two pieces here.


00:57:20.360 --> 00:57:22.240
I realized as I was doing this also--


00:57:22.240 --> 00:57:24.600
I think I sort of mentioned with the sketch,


00:57:24.600 --> 00:57:29.800
I really often was taking the output of a language model prompt, doing something in


00:57:29.800 --> 00:57:35.000
Python or actually I can do a full example of the SQL writing like exploration we did.


00:57:35.000 --> 00:57:43.280
But we would do these things that were sort of run GPT-3 to ask it to write the SQL.


00:57:43.280 --> 00:57:48.560
You take the SQL, you go try and execute it, but it fails for whatever reason.


00:57:48.560 --> 00:57:51.600
Or you and you take that error, you say, hey, rewrite it.


00:57:51.600 --> 00:57:54.360
So we talked about that sort of pattern, which is sort of like rewriting.


00:57:54.360 --> 00:57:58.460
Another one of the patterns was increase the temperature, ask it to write the SQL, you


00:57:58.460 --> 00:58:01.420
get like 10 different SQL answers in parallel.


00:58:01.420 --> 00:58:04.540
And this is where the async was like really important for this, because I just wanted


00:58:04.540 --> 00:58:10.840
to use asyncio gather and run all 10 of these truly in parallel against the open AI endpoint.


00:58:10.840 --> 00:58:16.860
Get 10 different answers to the SQL, run all 10 queries against your database, then pull


00:58:16.860 --> 00:58:21.000
on what the most common like of the ones that successfully ran, which ones gave the same


00:58:21.000 --> 00:58:22.220
answer the most often.


00:58:22.220 --> 00:58:28.460
I see. And that's probably the correct answer. And just chaining that stuff. It's like very


00:58:28.460 --> 00:58:31.460
Pythonic functions like you can really just imagine like, oh, I just need to write a for


00:58:31.460 --> 00:58:35.420
loop. I just need to, you know, run this function, take the output feed into another function.


00:58:35.420 --> 00:58:43.940
Very procedural. But when you all the abstractions in the open at open AI API, the things like


00:58:43.940 --> 00:58:47.340
just everything else, there was nothing else really at the time. But even the new ones


00:58:47.340 --> 00:58:51.900
that have come out like Lang chain that have sort of like taken the space by storm now


00:58:51.900 --> 00:58:56.500
are not really just trying to offer the minimal ingredient, which is the function. And to


00:58:56.500 --> 00:58:59.660
me, it was just like, if I can just offer the function, I can write a for loop, I can


00:58:59.660 --> 00:59:06.380
write, I can store a variable and then keep passing it into it. You could do so many different


00:59:06.380 --> 00:59:10.580
emergent behaviors with just starting with the function and then simple Python scripting


00:59:10.580 --> 00:59:11.580
on top of it.


00:59:11.580 --> 00:59:14.740
- Got some interesting stuff here, Lando Prompt.


00:59:14.740 --> 00:59:19.740
So you can start, you can kind of start it,


00:59:19.740 --> 00:59:24.260
set it, I don't know, with ChatGPT,


00:59:24.260 --> 00:59:25.380
you can tell it a few things.


00:59:25.380 --> 00:59:28.460
I'm gonna ask you a question about a book, okay?


00:59:28.460 --> 00:59:30.980
The book is a choose your own adventure book, okay?


00:59:30.980 --> 00:59:33.660
Now here, I'm gonna, like you can prepare it, right?


00:59:33.660 --> 00:59:35.840
There's probably a more formal term for that,


00:59:35.840 --> 00:59:36.740
but you can do this here.


00:59:36.740 --> 00:59:40.940
Like you can say, "Hey, system, you are a type of bot."


00:59:40.940 --> 00:59:42.820
And then that creates you an object


00:59:42.820 --> 00:59:44.120
that you can have a conversation with


00:59:44.120 --> 00:59:45.940
and you say, what should we get for lunch?


00:59:45.940 --> 00:59:47.240
And your type of bot is pirate.


00:59:47.240 --> 00:59:48.640
And then so to say, as a pirate,


00:59:48.640 --> 00:59:50.820
I would suggest we have some hearty seafood or whatever.


00:59:50.820 --> 00:59:51.660
Right?


00:59:51.660 --> 00:59:54.780
Like that's beyond what you're doing in a sketch.


00:59:54.780 --> 00:59:56.420
I mean, obviously this is not so much for code.


00:59:56.420 --> 00:59:58.460
This is like conversing with Python


00:59:58.460 --> 01:00:03.100
rather than in Python, I don't know, in your editor.


01:00:03.100 --> 01:00:08.100
- Yeah, this one was the OpenAI chat API endpoint came out


01:00:08.100 --> 01:00:10.620
and I was just like, oh, I should support it.


01:00:10.620 --> 01:00:13.980
So that's what this, I wanted to be able to Jinja template


01:00:13.980 --> 01:00:15.260
inside of the conversation.


01:00:15.260 --> 01:00:17.820
So you can imagine a conversation that is prepared


01:00:17.820 --> 01:00:19.700
with like seven steps back and forth,


01:00:19.700 --> 01:00:21.820
but you want a hard code with the conversation,


01:00:21.820 --> 01:00:23.740
like how the flow of the conversation was going,


01:00:23.740 --> 01:00:26.640
and you want to template it so that like on message three,


01:00:26.640 --> 01:00:28.460
it put your new context problem.


01:00:28.460 --> 01:00:31.100
On message four, it put the output from another prompt


01:00:31.100 --> 01:00:31.940
that you ran.


01:00:31.940 --> 01:00:34.700
On message five, it is this other data thing.


01:00:34.700 --> 01:00:39.260
And then you ask it to complete the intent of like,


01:00:39.260 --> 01:00:41.520
that's arbitrarily complex, but still something like that


01:00:41.520 --> 01:00:44.340
would be just three lines or so in Lambda prompt.


01:00:44.340 --> 01:00:45.900
The idea was that it would offer up


01:00:45.900 --> 01:00:49.000
a really simple API for this.


01:00:49.000 --> 01:00:50.460
- Cool, other thing that's interesting


01:00:50.460 --> 01:00:52.460
is you have an async and async version.


01:00:52.460 --> 01:00:55.540
So that's cool, people can check that out.


01:00:55.540 --> 01:00:59.460
Also a way to make it a hosted as a web service


01:00:59.460 --> 01:01:02.520
with say like FastAPI or something like that.


01:01:02.520 --> 01:01:03.360
- Yeah.


01:01:03.360 --> 01:01:06.540
- And you can make it a decorator if you like,


01:01:06.540 --> 01:01:08.420
an app prompt decorator.


01:01:08.420 --> 01:01:10.540
- Yeah, on any function, you can just throw @prompt


01:01:10.540 --> 01:01:12.680
and it wraps it with the same class


01:01:12.680 --> 01:01:16.200
so that all the magic you get from that works.


01:01:16.200 --> 01:01:18.760
The server bit is I took,


01:01:18.760 --> 01:01:21.760
so FastAPI has that sort of like inspection


01:01:21.760 --> 01:01:24.300
on the function part.


01:01:24.300 --> 01:01:26.760
I did a little bit of middleware


01:01:26.760 --> 01:01:28.420
to get the two happy together.


01:01:28.420 --> 01:01:31.520
And then all you have to do is import FastAPI


01:01:31.520 --> 01:01:34.920
and then run, you know, gunicorn that app.


01:01:34.920 --> 01:01:36.880
And it's two lines


01:01:36.880 --> 01:01:41.880
And any prompts you have made become their own


01:01:41.880 --> 01:01:44.540
independent REST endpoint,


01:01:44.540 --> 01:01:47.220
where you can just do a GET or a POST to it.


01:01:47.220 --> 01:01:50.820
And it returns the response from calling the prompt.


01:01:50.820 --> 01:01:52.960
But these prompts can also be these chains of prompts.


01:01:52.960 --> 01:01:54.680
Like one prompt can call another prompt,


01:01:54.680 --> 01:01:55.960
which can call another prompt.


01:01:55.960 --> 01:01:58.280
And those prompts can call async to not async,


01:01:58.280 --> 01:01:59.640
back to async and things like that.


01:01:59.640 --> 01:02:01.280
And it should work.


01:02:01.280 --> 01:02:03.000
Pretty sure.


01:02:03.000 --> 01:02:04.680
This one actually, I did test everything as far as I know.


01:02:04.680 --> 01:02:06.040
I'm pretty sure I've got pretty good coverage.


01:02:06.040 --> 01:02:08.700
- Yeah, super cool.


01:02:08.700 --> 01:02:10.580
All right, well, getting a little short on time,


01:02:10.580 --> 01:02:13.820
but I think people are gonna really, really dig this,


01:02:13.820 --> 01:02:14.660
especially Sketch.


01:02:14.660 --> 01:02:17.280
I think there's a lot of folks out there doing pandas


01:02:17.280 --> 01:02:22.280
that would love an AI buddy to help them do things,


01:02:22.280 --> 01:02:27.760
like not just analyze the code, but the data as well.


01:02:27.760 --> 01:02:30.740
- Yeah, just, I think anybody's, I know it's for me,


01:02:30.740 --> 01:02:34.060
but it's just like Copilot in VS Code ID,


01:02:34.060 --> 01:02:37.020
a sketch in your Jupyter ID, it takes almost nothing to add.


01:02:37.020 --> 01:02:39.360
And whenever you're just sort of sitting there,


01:02:39.360 --> 01:02:41.440
you think you're about to alt-tab to go to Google,


01:02:41.440 --> 01:02:42.820
you can just try the sketch.ask.


01:02:42.820 --> 01:02:46.340
And it's surprising how often that sketch.ask or sketch.howto


01:02:46.340 --> 01:02:48.160
gets you way closer to a solution


01:02:48.160 --> 01:02:49.580
without even having to leave--


01:02:49.580 --> 01:02:51.580
you don't even have to leave your environment.


01:02:51.580 --> 01:02:54.700
>>Victor: It's like a whole other level of autocomplete.


01:02:54.700 --> 01:02:55.420
For sure.


01:02:55.420 --> 01:02:56.780
That's super cool.


01:02:56.780 --> 01:02:59.100
All right, now before I let you out of here,


01:02:59.100 --> 01:03:00.980
you've got to answer the final two questions.


01:03:00.980 --> 01:03:03.420
If you're going to write some Python code,


01:03:03.420 --> 01:03:06.160
and it's not a Jupyter notebook, what editor are you using?


01:03:06.160 --> 01:03:08.560
It sounds to me like you may have just given a strong hint


01:03:08.560 --> 01:03:10.300
at what that might be.


01:03:10.300 --> 01:03:13.200
- Yeah, I've switched almost entirely to VS Code


01:03:13.200 --> 01:03:16.860
and I've been really liking it with the remote development


01:03:16.860 --> 01:03:21.860
and I work across many machines,


01:03:21.860 --> 01:03:24.940
both cloud and local and some five, six different machines


01:03:24.940 --> 01:03:26.620
are my primary working machines


01:03:26.620 --> 01:03:29.360
and I use the remote VS Code thing


01:03:29.360 --> 01:03:32.220
and I have a unified environment that gives me terminal,


01:03:32.220 --> 01:03:34.460
the files and the code all in one,


01:03:34.460 --> 01:03:36.380
and Copilot on all of them.


01:03:36.380 --> 01:03:37.820
- Yeah, it's wild.


01:03:37.820 --> 01:03:41.340
All right, and then notable PyPI package,


01:03:41.340 --> 01:03:42.740
I mean, pip install sketch,


01:03:42.740 --> 01:03:44.060
you can throw that out there if you like,


01:03:44.060 --> 01:03:45.100
that's pretty awesome.


01:03:45.100 --> 01:03:46.260
But anything you've run across,


01:03:46.260 --> 01:03:47.980
you're like, oh, this is,


01:03:47.980 --> 01:03:49.540
people should know about this.


01:03:49.540 --> 01:03:50.700
- Yeah. - Doesn't have to be popular,


01:03:50.700 --> 01:03:52.260
just like, wow, this is cool.


01:03:52.260 --> 01:03:54.920
- In the, I guess these two are very popular,


01:03:54.920 --> 01:03:59.100
but in the data space, I really,


01:03:59.100 --> 01:04:03.060
I'm a huge fan of Ray and also Arrow.


01:04:03.060 --> 01:04:06.860
I use those two tools as my back end bread and butter


01:04:06.860 --> 01:04:07.740
for everything I do.


01:04:07.740 --> 01:04:11.420
And so those have just been really great workhorses.


01:04:11.420 --> 01:04:12.740
- Apache Arrow, right?


01:04:12.740 --> 01:04:15.460
And then Ray, I'm not sure.


01:04:15.460 --> 01:04:19.980
- Ray is a distributed scheduling compute framework.


01:04:19.980 --> 01:04:22.500
It's sort of like a, I don't know what they--


01:04:22.500 --> 01:04:24.500
- Yeah, I remember seeing about this, yeah.


01:04:24.500 --> 01:04:28.020
- This is, it is, I'm parsing,


01:04:28.020 --> 01:04:28.940
we didn't talk about other things,


01:04:28.940 --> 01:04:30.480
But I'm parsing Common Crawl, which


01:04:30.480 --> 01:04:32.160
is like 25 petabytes of data.


01:04:32.160 --> 01:04:33.880
And Ray is great.


01:04:33.880 --> 01:04:34.940
It's just a workhorse.


01:04:34.940 --> 01:04:38.280
It's really useful.


01:04:38.280 --> 01:04:41.560
I find it's so snappy and good, but it


01:04:41.560 --> 01:04:43.720
offers everything I need in a distributed environment.


01:04:43.720 --> 01:04:46.400
So I can write code that runs on 100 machines


01:04:46.400 --> 01:04:47.680
and not have to think about it.


01:04:47.680 --> 01:04:48.840
It works really well.


01:04:48.840 --> 01:04:50.840
That's pretty nuts.


01:04:50.840 --> 01:04:53.000
Not as nuts as chat GDP and Midjourney,


01:04:53.000 --> 01:04:54.240
but still pretty nuts.


01:04:54.240 --> 01:04:56.440
So before we call it a day, do you


01:04:56.440 --> 01:04:58.680
want to just tell people about Approximate Labs?


01:04:58.680 --> 01:05:01.520
It sounds like you guys are making some good progress.


01:05:01.520 --> 01:05:04.360
Might have some jobs for people to work in this kind of area


01:05:04.360 --> 01:05:05.360
as well.


01:05:05.360 --> 01:05:07.740
- Yeah, so we're working at the intersection


01:05:07.740 --> 01:05:09.440
of AI and tabular data.


01:05:09.440 --> 01:05:11.240
So anything related to these training,


01:05:11.240 --> 01:05:14.020
these large language models and also tabular data.


01:05:14.020 --> 01:05:15.780
So things with columns and rows.


01:05:15.780 --> 01:05:17.820
We are trying to like solve that problem,


01:05:17.820 --> 01:05:19.440
try and bridge the gap here


01:05:19.440 --> 01:05:21.200
'cause there's a pretty big gap.


01:05:21.200 --> 01:05:24.120
We have three main initiatives that we're working on,


01:05:24.120 --> 01:05:26.280
which is we're trying to build up the data set of data sets.


01:05:26.280 --> 01:05:31.280
So just like the pile or the stack or Leon5b,


01:05:31.280 --> 01:05:33.120
these like big data sets that were used


01:05:33.120 --> 01:05:34.280
to train all these big models,


01:05:34.280 --> 01:05:36.720
we're making our own on tabular data.


01:05:36.720 --> 01:05:37.880
We are training models.


01:05:37.880 --> 01:05:41.200
So this is actually training large language models,


01:05:41.200 --> 01:05:43.760
doing these training, these full transformer models.


01:05:43.760 --> 01:05:46.220
And then we're also building apps like Sketch,


01:05:46.220 --> 01:05:48.000
like UIs, things that are actually there


01:05:48.000 --> 01:05:50.540
to help make data more accessible to people.


01:05:50.540 --> 01:05:54.080
So anything that helps people get value from data


01:05:54.080 --> 01:05:56.240
and make it open source, that's what we're working on.


01:05:56.240 --> 01:05:59.400
We just raised our seed round,


01:05:59.400 --> 01:06:01.040
so we are now officially hiring.


01:06:01.040 --> 01:06:04.040
So looking for people who are interested in the space


01:06:04.040 --> 01:06:06.600
and who are enthusiastic about these problems.


01:06:06.600 --> 01:06:07.800
- Awesome.


01:06:07.800 --> 01:06:12.800
Well, very exciting demo libraries, I guess,


01:06:12.800 --> 01:06:15.000
however you call them.


01:06:15.000 --> 01:06:16.920
But I think these are neat.


01:06:16.920 --> 01:06:18.920
People are gonna find a lot of cool uses for them.


01:06:18.920 --> 01:06:23.400
So excellent work and congrats on all the success so far.


01:06:23.400 --> 01:06:24.960
It sounds like you're starting to take off.


01:06:24.960 --> 01:06:26.000
Yeah.


01:06:26.000 --> 01:06:27.280
Thank you.


01:06:27.280 --> 01:06:28.760
- All right, Justin, final call to action.


01:06:28.760 --> 01:06:29.800
People wanna get started.


01:06:29.800 --> 01:06:31.320
Let's pick Sketch.


01:06:31.320 --> 01:06:32.440
People wanna get started with Sketch.


01:06:32.440 --> 01:06:33.520
What do you tell them?


01:06:33.520 --> 01:06:36.220
- Just pip install it.


01:06:36.220 --> 01:06:38.520
Give Sketch a try.


01:06:38.520 --> 01:06:39.900
Pip install it, import it,


01:06:39.900 --> 01:06:42.020
and then throw it on your data frame.


01:06:42.020 --> 01:06:42.860
- Awesome.


01:06:42.860 --> 01:06:45.800
And then ask it questions or how-tos, yeah?


01:06:45.800 --> 01:06:47.640
- Yeah, yep, whatever you like.


01:06:47.640 --> 01:06:50.080
If you really want to and you trust the model,


01:06:50.080 --> 01:06:53.840
like throw some applies and have it clean your data for you.


01:06:53.840 --> 01:06:55.400
- Cool, awesome.


01:06:55.400 --> 01:06:57.840
All right, well, thanks for being on the show.


01:06:57.840 --> 01:06:59.600
- Come here and tell us about all your work.


01:06:59.600 --> 01:07:00.720
It's great.


01:07:00.720 --> 01:07:01.760
- Yeah, thank you.


01:07:01.760 --> 01:07:02.920
- Yeah, see you later.


01:07:02.920 --> 01:07:03.760
Bye.

