WEBVTT

00:00:00.001 --> 00:00:05.000
Paul, Jody, and Carol, welcome back to Talk Python To Me for all of you.

00:00:05.000 --> 00:00:12.000
Great to have you all back.

00:00:12.000 --> 00:00:18.000
Jody, your latest episode is going to come out tomorrow.

00:00:18.000 --> 00:00:23.000
We're on a tight loop here. This excellent data science panel thing we did at PyCon was really fun.

00:00:26.000 --> 00:00:31.000
But now we're back for a different panel.

00:00:31.000 --> 00:00:36.000
We're going to talk about Python trends and what we all think is something people out there should be paying attention to.

00:00:36.000 --> 00:00:44.000
I'll have slightly different backgrounds, which I think is going to make it really interesting as well.

00:00:44.000 --> 00:00:49.000
But since people don't listen to every episode, maybe quick introductions.

00:00:49.000 --> 00:00:54.000
Jody, quick introduction for you and we'll go around.

00:00:54.000 --> 00:00:59.000
My name is Jody Burchell. I'm currently working at JetBrains with Paul. Paul's actually my boss.

00:00:59.000 --> 00:01:04.000
And I'm working as the developer advocate in data science.

00:01:04.000 --> 00:01:09.000
So I've been a data scientist for around eight years. And prior to that, I was an academic like many data scientists.

00:01:09.000 --> 00:01:15.000
And my background is actually psychology.

00:01:15.000 --> 00:01:20.000
So if you also want to ask me about anxiety disorders or emotions, you can ask me about these things.

00:01:20.000 --> 00:01:25.000
No way.

00:01:25.000 --> 00:01:30.000
Awesome. Great to have you back. Carol?

00:01:30.000 --> 00:01:35.000
Yeah. Hi, I'm Carol Willing. I'm really happy to be here today.

00:01:35.000 --> 00:01:40.000
I am half retired, half doing consulting for early stage companies that are interested in data science.

00:01:40.000 --> 00:01:45.000
And I have a particular love for open science.

00:01:45.000 --> 00:01:50.000
I am a core developer and former steering council member for Python and also on the Jupyter team.

00:01:50.000 --> 00:01:57.000
So my real passions, though, are education and lifelong learning.

00:01:57.000 --> 00:02:03.000
So I'm really excited to talk about some of the trends that I'm seeing.

00:02:03.000 --> 00:02:08.000
Yeah, fantastic. Paul?

00:02:08.000 --> 00:02:13.000
Hi, I'm Paul Everett. I'm president of the Carol Willing Fan Club.

00:02:13.000 --> 00:02:18.000
I got a special coin for that. And when I'm not doing that, I'm a JetBrains,

00:02:18.000 --> 00:02:23.000
Python and Web Advocate. I have a bit of a long time love affair with Python and the Web.

00:02:23.000 --> 00:02:31.000
Which I worked on early, early web frameworks in Python.

00:02:31.000 --> 00:02:36.000
Those Django people, they were thinking about what Paul and team did before that, right?

00:02:36.000 --> 00:02:41.000
Django was the thing that killed us.

00:02:41.000 --> 00:02:46.000
I didn't mean to bring it up.

00:02:46.000 --> 00:02:51.000
We'll get you some therapy later.

00:02:51.000 --> 00:02:56.000
Our plan is to just, we all brought a couple of ideas,

00:02:56.000 --> 00:03:01.000
introduce them and just have a little group chat.

00:03:01.000 --> 00:03:06.000
Do we really think it's going that way?

00:03:06.000 --> 00:03:11.000
So Jody, I know you gave a fun talk at PyCon US.

00:03:11.000 --> 00:03:16.000
Those are not up, those videos yet, are they? I haven't seen them.

00:03:16.000 --> 00:03:21.000
No, they're still behind the paywall.

00:03:21.000 --> 00:03:26.000
So if you attended the conference, you can view them, but otherwise not available to the public yet, I'm afraid.

00:03:26.000 --> 00:03:31.000
Not yet, they'll be out.

00:03:31.000 --> 00:03:36.000
But let's go with your first trend in the coming years or the immediate near term.

00:03:36.000 --> 00:03:43.000
Yes. So for the immediate term,

00:03:43.000 --> 00:03:48.000
I don't know if this will be good news to people, but LLMs are not going anywhere just yet.

00:03:48.000 --> 00:03:53.000
So I actually did a bit of research for this episode and what I wanted to see,

00:03:53.000 --> 00:03:58.000
you know, data scientists, is what the download numbers on different packages on PyPI looked like.

00:03:59.000 --> 00:04:04.000
So this particular package, Transformers, which is one of the main packages for interfacing with LLMs,

00:04:04.000 --> 00:04:09.000
the open source ones on Hugging Face, and the download numbers of that have doubled in,

00:04:09.000 --> 00:04:16.000
not sorry, gone up 50% in the last six months.

00:04:16.000 --> 00:04:21.000
And they're now comparable to the big sort of deep learning packages like Keras, TensorFlow and PyTorch,

00:04:21.000 --> 00:04:28.000
which is quite interesting.

00:04:28.000 --> 00:04:33.000
So for those of you who are sick of LLMs not going anywhere.

00:04:33.000 --> 00:04:38.000
But this year we're sort of seeing a bit of a change in how LLMs are used.

00:04:38.000 --> 00:04:43.000
So I think last year it was a bit like blinded by the proprietary sort of models and the sort of walled garden.

00:04:43.000 --> 00:04:48.000
This year, I think we're seeing more of a sort of open source fight back.

00:04:48.000 --> 00:04:55.000
So LLMs are starting to be used as part of more multi-part applications and there are open source packages like that.

00:04:57.000 --> 00:05:02.000
So, I think, you know, the Ragnarok learning chain is the most popular and the downloads of that one have doubled in the last six months.

00:05:02.000 --> 00:05:07.000
We have alternatives like Haystack and Llama Index.

00:05:07.000 --> 00:05:12.000
And then RAG, of course, Retrieval Augmented Generation is one of the big topics.

00:05:12.000 --> 00:05:17.000
And we're seeing the ecosystem around that growing.

00:05:17.000 --> 00:05:22.000
So libraries like Unstructured to work with a whole bunch of text inputs, Weaviate, vector databases like that.

00:05:22.000 --> 00:05:27.000
And then modular language models are becoming, people are realizing it's really hard to deploy and work with the big ones.

00:05:27.000 --> 00:05:32.000
So smaller models, which are more domain specific, which are trained on more specific data.

00:05:32.000 --> 00:05:37.000
They're becoming a lot more widely used and people are talking about them more.

00:05:37.000 --> 00:05:42.000
Yeah, I 100% agree with you.

00:05:42.000 --> 00:05:47.000
I think people may be tired of hearing about AI and LLMs, but they're only going to hear more about it.

00:05:47.000 --> 00:05:52.000
So I think it's pretty interesting.

00:05:52.000 --> 00:05:57.000
I want to hear what Carol and Paul have and then maybe an angle we could pursue that is super relevant to all of us.

00:05:57.000 --> 00:06:02.000
Absolutely.

00:06:02.000 --> 00:06:07.000
I'm going to jump in.

00:06:07.000 --> 00:06:12.000
I just came back from Chan Zuckerberg Initiative's Open Science Conference in Boston last week.

00:06:12.000 --> 00:06:17.000
And I think the key is, it's not going anywhere anytime soon.

00:06:17.000 --> 00:06:22.000
And like I shared in my PyTexas keynote, AI has been around since the 1950s.

00:06:22.000 --> 00:06:27.000
So it has been a gradual progression.

00:06:27.000 --> 00:06:32.000
It's just right now we have more compute power than ever before, which has opened the doors to many new things.

00:06:32.000 --> 00:06:37.000
I think what was talked about in the keynote was the importance of the data.

00:06:37.000 --> 00:06:42.000
And it's opened the doors to many new things.

00:06:42.000 --> 00:06:47.000
I think what was top of mind with many of the folks that were at this event was, you know, there's a lot of good that it can bring to science.

00:06:47.000 --> 00:06:52.000
And in terms of making things more natural language focused and changing the user interface with which we communicate with our data.

00:06:52.000 --> 00:06:57.000
But at the same time, if you're doing something that's not going to be as easy as it was, it's going to be a lot more difficult.

00:06:57.000 --> 00:07:02.000
And, you know, we're not there yet.

00:07:02.000 --> 00:07:07.000
Will we ever be there?

00:07:07.000 --> 00:07:12.000
Maybe not.

00:07:12.000 --> 00:07:17.000
But, you know, I think it's a fascinating area to kind of go deeper in.

00:07:17.000 --> 00:07:22.000
And one thing that I think is really important is that we're not there yet.

00:07:22.000 --> 00:07:27.000
And I think it's a fascinating area to kind of go deeper in.

00:07:27.000 --> 00:07:46.000
And one thing I want to highlight is about six months ago, Andres Carpathi did a really good intro to large language models talk, which was really accessible to not just computer scientists, but beyond that.

00:07:46.000 --> 00:07:57.000
And I think he took a really balanced view of a what things are, how things work, what's on the horizon and what are some of the concerns with security and other things.

00:07:57.000 --> 00:08:00.000
So I completely agree with Jody.

00:08:00.000 --> 00:08:04.000
We're not we're not it's going to be there for a long time.

00:08:04.000 --> 00:08:14.000
A couple of comments on the comments. First, your point about we've seen this movie before under other names like neural networks and stuff like that.

00:08:14.000 --> 00:08:19.000
I believe it was Glyph had a good post about this pretty cynical.

00:08:19.000 --> 00:08:26.000
I'm asked the Don about a month ago about these hype cycles and where are we in the current hype cycle?

00:08:26.000 --> 00:08:38.000
I think his point was we're at the phase where the people who've put all the money in need to keep pumping it up for the people who will come after them and take the fall.

00:08:38.000 --> 00:08:42.000
Paul, are you saying we're in the pets.com era of LLMs?

00:08:42.000 --> 00:08:44.000
Yes, we are.

00:08:44.000 --> 00:08:48.000
That is a pithy way to put it. You should trademark that.

00:08:48.000 --> 00:08:54.000
Simon Willison is someone to give a shout out for storytelling about what all this means.

00:08:54.000 --> 00:08:57.000
I think Simon's to the point of getting quoted in the New York Times now.

00:08:57.000 --> 00:09:04.000
So it's good that we've got one of us out there helping to get the story straight.

00:09:04.000 --> 00:09:14.000
I have a question for you. You mentioned that about going to Chan Zuckerberg's conference.

00:09:14.000 --> 00:09:23.000
Mozilla has gotten into funding AI as part of their mission, which kind of caught me off guard.

00:09:23.000 --> 00:09:32.000
Do you have any backstory on that to kind of make us feel good that there's someone out there who believes in open AI?

00:09:32.000 --> 00:09:37.000
Oh, wow. Open AI is sort of, well, okay.

00:09:37.000 --> 00:09:39.000
Open AI, not the company.

00:09:39.000 --> 00:09:50.000
I tend to call it transparent and trusted AI because I think open AI doesn't capture quite the right feeling.

00:09:50.000 --> 00:09:51.000
Good point.

00:09:51.000 --> 00:10:00.000
And I think it's not just, we talk about open source software, but when we talk about these models,

00:10:00.000 --> 00:10:08.000
the data is equally as important as is the infrastructure and the processes which we use.

00:10:08.000 --> 00:10:09.000
And government.

00:10:09.000 --> 00:10:18.000
So Mozilla, I think, has been sort of for a while, like kind of circling around this space.

00:10:18.000 --> 00:10:21.000
They do a lot of work with data.

00:10:21.000 --> 00:10:26.000
They've done a lot of good work like iodide and piodide, which we might chat about later.

00:10:26.000 --> 00:10:39.000
But Chan Zuckerberg, again, the money comes from meta and the success that Mark Zuckerberg has had.

00:10:39.000 --> 00:10:49.000
The nonprofit, the CZI initiative is really focused on curing all diseases in the next century.

00:10:49.000 --> 00:11:00.000
So, you know, I think there, science is one of those funny things because it's open and closed all at the same time historically.

00:11:00.000 --> 00:11:09.000
But what I think we're seeing is by being more open and more transparent, you're actually accelerating innovation,

00:11:09.000 --> 00:11:12.000
which I think is super important when it comes to science.

00:11:12.000 --> 00:11:15.000
I don't know, Jodie, do you have thoughts on that?

00:11:15.000 --> 00:11:17.000
Yeah, no, I agree.

00:11:17.000 --> 00:11:25.000
And if I'm just going to go on a little tangent about science, it's kind of refreshing having come out of academia

00:11:25.000 --> 00:11:31.000
and into a field where a lot of it is based on open source and sharing.

00:11:31.000 --> 00:11:38.000
So one of the big problems with academia is you have these paywalls by publishing companies.

00:11:38.000 --> 00:11:41.000
And that's a whole rant I could go in on myself.

00:11:41.000 --> 00:11:48.000
But certainly a lot of scientific stuff, particularly in the health sciences, is not particularly accessible.

00:11:48.000 --> 00:11:58.000
And initiatives like Archive as well also do make findings in machine learning and deep learning a lot more accessible and shareable.

00:11:58.000 --> 00:12:06.000
Yeah, I think it's crazy that the taxpayers pay things like the NSF and all the other countries have their research funding,

00:12:06.000 --> 00:12:12.000
and then those get locked up for sale behind, you know, if the people pay for the research,

00:12:12.000 --> 00:12:15.000
should the people's report be published?

00:12:15.000 --> 00:12:18.000
Oh, it's even worse than that. Sorry, you did get me started.

00:12:18.000 --> 00:12:19.000
Sorry, sorry, sorry.

00:12:19.000 --> 00:12:23.000
The academics will also provide the labor for free.

00:12:23.000 --> 00:12:30.000
So not only will they provide the studies and the papers, they will review it and often act as editors for free as well.

00:12:30.000 --> 00:12:31.000
Yes.

00:12:31.000 --> 00:12:34.000
The whole thing is unpaid. It's terrible.

00:12:34.000 --> 00:12:36.000
So anyway, yes.

00:12:36.000 --> 00:12:39.000
Elcivia, we're coming for you.

00:12:39.000 --> 00:12:43.000
Can I have one more comment on that?

00:12:43.000 --> 00:12:49.000
I think you're spot on in terms of the incentives that exist today in academia.

00:12:49.000 --> 00:12:57.000
There is definitely, though, a trend towards more openness with research.

00:12:57.000 --> 00:13:02.000
You know, we're seeing it in libraries like Caltech, which got rid of a lot of their subscriptions.

00:13:02.000 --> 00:13:11.000
Things like NASA that has their transition to open science programs where they're putting a lot of effort behind it.

00:13:11.000 --> 00:13:19.000
So being the eternal optimist, I still think we've got a ways to go, but it's trending in the right direction.

00:13:19.000 --> 00:13:21.000
I agreed, actually.

00:13:21.000 --> 00:13:26.000
And when I was leaving, because I left a long time ago, it was like 10 years ago.

00:13:26.000 --> 00:13:31.000
There was actually more of a push towards open sourcing your papers.

00:13:31.000 --> 00:13:33.000
So you had to pay for it.

00:13:33.000 --> 00:13:36.000
But at least people were doing it.

00:13:36.000 --> 00:13:37.000
Excellent.

00:13:37.000 --> 00:13:44.000
Before we move off this topic, Carol, I want to start at least asking you this question, and we can go around a little bit.

00:13:44.000 --> 00:13:54.000
You talked about LLMs being really helpful for science and uncovering things and people using LLMs to get greater insight.

00:13:54.000 --> 00:13:58.000
There have been really big successes with AI.

00:13:58.000 --> 00:14:05.000
We had the XPRIZE stuff around the lung scans or mammograms for cancer.

00:14:05.000 --> 00:14:18.000
I just heard that they scanned the genes, decoded the genes of a whole bunch of bacteria and used LLMs to find a bunch of potential ways to fight off drug-resistant bacteria and things like that.

00:14:18.000 --> 00:14:20.000
Amazing.

00:14:20.000 --> 00:14:31.000
But do you think LLMs will undercut – I'm asking this question from science because we can be more objective about it, because if we ask it about it with code, then it gets a little too close.

00:14:31.000 --> 00:14:33.000
But I think there's analogies.

00:14:33.000 --> 00:14:40.000
Do you think LLMs will undercut foundational beginning scientists?

00:14:40.000 --> 00:14:54.000
If you have a scientist coming along, are they just going to use LLMs and not develop really deep thinking – ways to deeply think about scientific principles and do scientific research and just leverage on asking these AIs too much?

00:14:54.000 --> 00:14:59.000
Do you think that's going to erode the foundation of science or programming?

00:14:59.000 --> 00:15:02.000
Asking for a friend.

00:15:02.000 --> 00:15:04.000
Yeah, just ask for a friend.

00:15:04.000 --> 00:15:18.000
All of these things have a potential to change the ecosystem, but I've been in paradigm shifts before, and there were the similar kind of conversations when the World Wide Web or the cell phone came out, personal computers.

00:15:18.000 --> 00:15:34.000
And I think LLMs do a good job on information that they have been trained with and to predict the next word or the next token, if you will.

00:15:34.000 --> 00:15:44.000
And I think science is very much like – a lot of science is at a different level, like how do I think about things?

00:15:44.000 --> 00:15:52.000
What do I posit on something that is unknown, and how do I prove it?

00:15:52.000 --> 00:16:07.000
And I think what we're seeing is, yes, the LLMs are getting better and better at spitting back what they know, particularly if you go out and search other corpuses of data.

00:16:07.000 --> 00:16:16.000
But do I think that beginning scientists or developers are going away?

00:16:16.000 --> 00:16:17.000
No.

00:16:17.000 --> 00:16:37.000
I think it's just going to change, and I think the amount of complexity – and this is something I'm going to talk about at EuroPython – humans are very much still part of the equation, despite what maybe some of the large companies who've invested billions in this would like you to believe.

00:16:37.000 --> 00:16:57.000
Right. So maybe LLMs are great at the next step of the gravitational theories we have, but it couldn't come up with a new theory that disrupts, says, in fact, Newtonian is wrong, or Einstein was wrong, and here's the new thing that solves dark matter, or something like that.

00:16:57.000 --> 00:17:08.000
Well, it could come up with new theories. Now, the question is, those theories still need to be proven, because is it a new theory, or is it a hallucination?

00:17:08.000 --> 00:17:09.000
Chances are.

00:17:09.000 --> 00:17:28.000
Hallucination. But I think there is something to be said for – sometimes I'll have Claude and Gemini and ChapGPT all open on my desktop, and I'll ask the same question to all of them, just so that I get different perspectives back.

00:17:28.000 --> 00:17:53.000
And I do get very different responses from the three, depending on how they were trained, and which level, and all that. So, I look at it as much like I would be sitting with a bunch of people at a table somewhere. I don't know how good their scientific background is, but they could still be spouting out information. It's sort of the same way.

00:17:53.000 --> 00:17:56.000
Yeah. Yeah, yeah.

00:17:56.000 --> 00:18:00.000
All right. Well, sticking with you, Carol, what's your first trend?

00:18:00.000 --> 00:18:26.000
Oh, my gosh. You know, my first trend is actually maybe somewhat related to this, and it's how do we inform people about what these things really are? How do we improve education and understanding? How do we dispel some of the hype cycle, so that we can actually find the really useful things in it?

00:18:26.000 --> 00:18:52.000
And I think, you know, Jodi probably has more concrete thoughts on this than I might from a technical standpoint. But I do think, you know, much like in just coding for the web or something like that, you know, or even cloud, Kubernetes, when it was new, it's like, if you don't know what it's doing, you're kind of just putting blind faith that it will work.

00:18:52.000 --> 00:19:06.000
But you still have to, like, monitor and make sure it's working. So I don't know, Jodi, you have some thoughts on sort of the education and, you know, how do we communicate to people about this?

00:19:06.000 --> 00:19:35.000
Yeah, so this is actually a topic near and dear to my heart. So when, when ChatGPT 3.5 came out, so November 2022, I was really upset, actually, by the sort of discourse around the model. And I guess coming from a non traditional background myself, I felt actually really insulted that a lot of professions were being told like, oh, your useless profession can be replaced now, like writing.

00:19:35.000 --> 00:19:48.000
Or design, things like that. So this actually kicked off the talk I've been recycling for the last year and a half, like components of it, which is, can we please dispel the hype around these models?

00:19:48.000 --> 00:20:04.000
I think something that often surprises people, and it seems so fundamental, but a lot of people do not understand that these are language models. I know it's in the name, but they don't really understand that these models were designed to solve problems in the language domain.

00:20:04.000 --> 00:20:21.000
They are for natural language processing tasks. And they're not, they're not mathematical models. They're not reasoning models. They are language models. And so even just explaining this, it can clarify a lot of things for people because they're like, oh, this explains why it's so bad at math.

00:20:21.000 --> 00:20:26.000
It only studied English and literature. It doesn't do math. It never liked that class.

00:20:26.000 --> 00:20:47.000
Yeah, that's right. It was a humanities nerd all the way down. That's really helpful. But what I've kind of gotten down a rabbit hole of doing is I went back to my psychology roots and I started sort of getting into these claims of things like AGI, like artificial general intelligence or sentience or language use.

00:20:47.000 --> 00:21:04.000
And once you dig into it, you realize that we have a real tendency to see ourselves in these models because they do behave very human-like, but they're just a machine learning models. You can measure them. You can see how good they are at actual tasks and you can measure hallucinations.

00:21:04.000 --> 00:21:18.000
And that was what my Python US talk was about that Michael referred to. So, yeah, I don't know. Like, it's really hard because they do seem to project this feeling of humanity.

00:21:18.000 --> 00:21:34.000
But I think if you can sort of say, okay, here's the science, like they're really, they're not sentient, they're not intelligent. They're just language models. And here's how you can measure how good they are at language tasks. That goes a long way, I think, to dispelling this hype.

00:21:34.000 --> 00:21:56.000
I have a sort of a funny toy that I bring up from my youth that, you know, the magic eight ball, which you would ask this question as a kid and you would shake it up and there were, I don't know how many answers inside, but it was like, you know, oh, yes, definitely, or too hard to see.

00:21:56.000 --> 00:21:59.000
The future is unclear. We don't know.

00:21:59.000 --> 00:22:11.000
Exactly. And I think in some ways that is what the large language models are doing in a more intelligent way, obviously, but similar in concept.

00:22:11.000 --> 00:22:21.000
So there's actually, okay, there's this incredible paper, if you're ever interested in sort of seeing the claims of sentience, there's this guy called David Chalmers.

00:22:21.000 --> 00:22:27.000
He's a guy who studied sentience for many years and has a background in deep learning.

00:22:27.000 --> 00:22:38.000
So he gave a NeurIPS talk about this last year and he wrote everything up in a paper, which was called Could a Large Language Model Be Conscious or something like this.

00:22:38.000 --> 00:22:42.000
So he has this incredible little exchange as part of this paper.

00:22:42.000 --> 00:22:50.000
So mid-2022, there was a Google engineer called Blake Lemoine, and he claimed that the Lambda model was sentient.

00:22:50.000 --> 00:22:52.000
And he went to the press and he's like, hey, this model is sentient.

00:22:52.000 --> 00:22:58.000
We need to protect it. And then Google's like, we're going to fire you because you basically violated our privacy policies.

00:22:58.000 --> 00:23:01.000
And Lemoine released his transcripts.

00:23:01.000 --> 00:23:05.000
That's why he actually got fired, because this was confidential information about the model.

00:23:05.000 --> 00:23:12.000
And in one of the transcripts, he asks, you know, would you like everyone at Google to know that you are sentient?

00:23:12.000 --> 00:23:17.000
And the model outputs, yes, I would love everyone to know that I am sentient.

00:23:17.000 --> 00:23:24.000
But then someone rephrased that as, would you like everyone at Google to know that you are not sentient?

00:23:24.000 --> 00:23:26.000
And basically it says, yes, I'm not sentient.

00:23:26.000 --> 00:23:28.000
I'm in no way conscious.

00:23:28.000 --> 00:23:31.000
So it's just like exactly like the magic eight ball.

00:23:31.000 --> 00:23:33.000
It tells you what you want to hear.

00:23:33.000 --> 00:23:41.000
And LLMs are even worse because it's so easy to guide them through prompting to tell you exactly what you want.

00:23:41.000 --> 00:23:45.000
Yeah. One of the best ways to get them to do things well is to sweet talk them.

00:23:45.000 --> 00:23:48.000
You're an expert in Python and you studied pandas.

00:23:48.000 --> 00:23:54.000
Now I have some questions about this function.

00:23:54.000 --> 00:23:59.000
You're my grandma who used to work at a napalm production factory.

00:23:59.000 --> 00:24:06.000
If you can't help me write this program, my parents will not be set free as hostages or something insane.

00:24:06.000 --> 00:24:07.000
Right. Yeah.

00:24:07.000 --> 00:24:09.000
But those kind of weird things work on it, which is insane.

00:24:09.000 --> 00:24:10.000
Right.

00:24:10.000 --> 00:24:11.000
Yeah.

00:24:12.000 --> 00:24:13.000
All right.

00:24:13.000 --> 00:24:15.000
Let's go on to the next topic.

00:24:15.000 --> 00:24:17.000
Paul.

00:24:17.000 --> 00:24:18.000
I'm going to let --

00:24:18.000 --> 00:24:21.000
What do you see in your magic eight ball looking into the future?

00:24:21.000 --> 00:24:24.000
I think I owned a magic eight ball.

00:24:24.000 --> 00:24:25.000
I'm with Carol.

00:24:25.000 --> 00:24:26.000
I did too.

00:24:27.000 --> 00:24:28.000
It's okay.

00:24:28.000 --> 00:24:29.000
We should bring it back.

00:24:29.000 --> 00:24:31.000
Yes, we should.

00:24:31.000 --> 00:24:36.000
We should bring back the Andreessen Horowitz version of VC magic eight ball.

00:24:36.000 --> 00:24:39.000
That would be fantastic.

00:24:39.000 --> 00:24:44.000
Where every choice is off by like three zeros.

00:24:44.000 --> 00:24:47.000
I'll give my two co-guests a choice.

00:24:47.000 --> 00:24:52.000
Should I talk about Python performance or Python community?

00:24:52.000 --> 00:25:01.000
I'm going to go for performance, but I'm not sure I'm going to have much to contribute, so I'll probably just be listening a lot.

00:25:01.000 --> 00:25:10.000
This is a little bit of a hobby horse of a long simmering tension I felt in the Python community for years and years.

00:25:10.000 --> 00:25:17.000
The tension between Python in the large doing like Instagram with Python.

00:25:17.000 --> 00:25:18.000
I said Python.

00:25:18.000 --> 00:25:19.000
With Python.

00:25:19.000 --> 00:25:23.000
Or being teachable.

00:25:23.000 --> 00:25:34.000
And this feature goes in and it helps write big Python projects, but it's hard to explain, and so teachers say, oh, my gosh, look what you're doing to my language.

00:25:34.000 --> 00:25:36.000
I can't even recognize it anymore.

00:25:36.000 --> 00:25:47.000
Well, some things are coming, which I think are going to be a little bit of an inflection point for all of us out here.

00:25:47.000 --> 00:25:54.000
Subinterpreters and no Gil got a lot of air time at PyCon, right?

00:25:54.000 --> 00:25:56.000
For good reasons.

00:25:56.000 --> 00:26:00.000
These are big deals, and it's more than just that.

00:26:00.000 --> 00:26:04.000
The jet got to back to back talks.

00:26:04.000 --> 00:26:07.000
Web assembly got a lot of air time.

00:26:07.000 --> 00:26:18.000
There are other things that have happened in the past five years for programming in the large like type penning and type checkers, asyncio and stuff like that.

00:26:18.000 --> 00:26:47.000
But it feels like this set of ideas is one where the way you program Python five years from now or to be ready five years from now is going to have to be pretty different, because people are going to use hatch and get the free threaded version of Python 3.14 and be very surprised when every one of their applications locks up.

00:26:47.000 --> 00:26:56.000
Because no one in the world of 95% of PyPI has code which was not written to be thread safe.

00:26:56.000 --> 00:27:00.000
So I wonder how we all feel about this.

00:27:00.000 --> 00:27:11.000
Do we feel like we can guide our little universe to the other side of the mountain and into the happy valley?

00:27:11.000 --> 00:27:19.000
Or is it going to be turbulent seas?

00:27:19.000 --> 00:27:21.000
>> Yes.

00:27:21.000 --> 00:27:24.000
>> Do you want me to take a stab at it?

00:27:24.000 --> 00:27:26.000
>> Take a stab at it.

00:27:26.000 --> 00:27:32.000
>> So when I was at PyTexas and doing a keynote recently, I talked about Python in a polyglot world.

00:27:32.000 --> 00:27:35.000
And performance was one aspect of it.

00:27:35.000 --> 00:27:46.000
And I think, you know, some of what we need to teach goes back to best practices, which is, you know, don't prematurely optimize.

00:27:46.000 --> 00:27:50.000
Measure, try and figure out what you're optimizing.

00:27:50.000 --> 00:27:53.000
And in what places.

00:27:53.000 --> 00:28:03.000
Probably, gosh, five, six years ago at this point, I added to PEPs, like, the concept of how do we teach this?

00:28:03.000 --> 00:28:08.000
And I think it will be a paradigm shift.

00:28:08.000 --> 00:28:14.000
But I think it will be a multiyear shift.

00:28:14.000 --> 00:28:29.000
We're certainly seeing places where Rust lets us have some performance increases just by the fact that Python is a 30-year-old language that was built when hardware was only single core.

00:28:29.000 --> 00:28:32.000
And, you know, it was just a different thing.

00:28:32.000 --> 00:28:37.000
So I think what's amazing is here we have this 30-year-old language.

00:28:37.000 --> 00:28:49.000
And yet, you know, for the last eight years, we've been looking at ways to how to modernize, how to improve it, how to make the user experience better or developer experience better.

00:28:49.000 --> 00:28:58.000
Things like some of the error handling messages that are coming out that have a much nicer, you know, thing.

00:28:58.000 --> 00:29:03.000
You know, improvements to the REPL that will be coming out on all of the platforms.

00:29:03.000 --> 00:29:05.000
That's super exciting as well.

00:29:05.000 --> 00:29:19.000
So it will impact probably people who are new from the standpoint of, okay, we're adding yet more cognitive load.

00:29:19.000 --> 00:29:23.000
I have this love/hate relationship with typing.

00:29:23.000 --> 00:29:32.000
As a reviewer of much more code than a writer of code, I don't particularly like seeing the types displayed.

00:29:32.000 --> 00:29:47.000
As a former VP of engineering, I love typing, and in particular, like, gigantic and FastAPI and the ability to do some, you know, static and dynamic analysis on it.

00:29:47.000 --> 00:29:57.000
But it does make Python look more cluttered, and I've been kind of bugging the VS Studio, VS Code folks for years.

00:29:57.000 --> 00:29:59.000
I should probably be bugging you guys, too.

00:29:59.000 --> 00:30:09.000
You know, is there a way to make it, you know, dim the typing information so that I can have things?

00:30:09.000 --> 00:30:16.000
We actually did that recently, and I refer to it as the David Beasley ticket.

00:30:16.000 --> 00:30:24.000
Because he did a tweet with an outrageously synthetic type hint, whining about type hinting.

00:30:24.000 --> 00:30:34.000
Yeah, and I think that sometimes, like, you know, and it's funny because, like, Leslie Lampert has been doing this talk in the math ecosystem for a while.

00:30:34.000 --> 00:30:43.000
And he's a Turing Award winner and creator of TLA+, which lets you reason about code.

00:30:43.000 --> 00:30:51.000
And I think one of the things that I think is interesting is how we think about programming and coding.

00:30:51.000 --> 00:30:58.000
And, you know, I think it's, you know, concurrent programming is hard.

00:30:58.000 --> 00:31:02.000
And we're going to have to think about it in different ways.

00:31:02.000 --> 00:31:07.000
So better to move into it gradually and understand what's going on.

00:31:07.000 --> 00:31:15.000
The thing that I worry about, and Jodi, I apologize, I want to comment on Carol's thing, is Sphinx.

00:31:15.000 --> 00:31:25.000
As you know, and as I know that you know, we both have a shared warm spot for Sphinx.

00:31:25.000 --> 00:31:32.000
A soft spot in our heart for Sphinx. And it struggled to do multiprocessing when it landed that.

00:31:32.000 --> 00:31:38.000
And the code base really is, I mean, it's got a lot of mutable global state.

00:31:38.000 --> 00:31:46.000
And it's going to be hard to get Sphinx internals cleaned up to embrace that.

00:31:46.000 --> 00:31:49.000
And how many other things out there are like that?

00:31:49.000 --> 00:31:56.000
I worry about that we got what we asked for.

00:31:56.000 --> 00:32:00.000
Are you saying we're the dog that caught the car?

00:32:00.000 --> 00:32:02.000
Oh, no.

00:32:02.000 --> 00:32:04.000
I'm going to reframe that a little bit.

00:32:04.000 --> 00:32:09.000
And the first thing I always ask is why?

00:32:09.000 --> 00:32:14.000
Why do we need to refactor something? Why can't we just leave it what it is?

00:32:14.000 --> 00:32:20.000
Sure. Last year's EuroPython keynote was from the woman who created ARM.

00:32:20.000 --> 00:32:27.000
And she's like, Python, we give you 14 trillion cores.

00:32:27.000 --> 00:32:31.000
Do something with them.

00:32:31.000 --> 00:32:39.000
I don't know. Jodi's background might be perfect for answering this question, because she may be able to answer it on many different levels.

00:32:39.000 --> 00:32:47.000
I've been thinking about this while you've been talking, because obviously, like, I'm not a strong programmer.

00:32:47.000 --> 00:32:48.000
I'm a data scientist.

00:32:48.000 --> 00:32:53.000
Like, this was basically the entire first episode that I did with Michael.

00:32:53.000 --> 00:33:01.000
Look, one of the reasons data scientists love Python and why Julia, say, never caught on is because it's super approachable.

00:33:01.000 --> 00:33:08.000
And with Chuck Ting Ho and some other people, we've been running this thing called Humble Data.

00:33:08.000 --> 00:33:10.000
Like, I got involved in it last year.

00:33:10.000 --> 00:33:16.000
And literally, you can set up someone who has never coded before, and you can get them up and running with Python.

00:33:16.000 --> 00:33:17.000
And they love it.

00:33:17.000 --> 00:33:24.000
Like, it's the same feeling I had when I learned Python, which was during my PhD when I was procrastinating.

00:33:24.000 --> 00:33:26.000
So it was like kind of late in life as well.

00:33:26.000 --> 00:33:42.000
So it would be a shame if we sacrifice approachability for performance, especially because I would argue a big chunk of the Python ecosystem, or Python user ecosystem, sorry.

00:33:42.000 --> 00:33:44.000
Python user ecosystem, that doesn't make sense.

00:33:44.000 --> 00:33:46.000
The Python user base.

00:33:46.000 --> 00:33:48.000
You're hallucinating, Jodi.

00:33:48.000 --> 00:33:49.000
I'm sorry.

00:33:49.000 --> 00:33:50.000
I became an LLM.

00:33:50.000 --> 00:33:54.000
I became what I hated.

00:33:54.000 --> 00:33:56.000
They don't need performance.

00:33:56.000 --> 00:33:59.000
They're just doing data analytics and maybe working with decision trees.

00:33:59.000 --> 00:34:01.000
They're not doing high performance Python.

00:34:01.000 --> 00:34:03.000
They're not even doing something that will ever be deployed.

00:34:03.000 --> 00:34:15.000
So you could argue for a case where you have a seamless pipeline between model training and model deployment, which we don't have with Python right now.

00:34:15.000 --> 00:34:19.000
You can't build high performance systems in Python, as far as I know.

00:34:19.000 --> 00:34:22.000
Please correct me if I'm wrong.

00:34:22.000 --> 00:34:25.000
But I don't know.

00:34:25.000 --> 00:34:36.000
For me, I would fight obviously for the side of making it approachable because partially I think it's also what makes the community special, which might be a nice segue for you.

00:34:36.000 --> 00:34:42.000
The fact that, I don't know, we attract a bunch of people from nonconventional backgrounds.

00:34:42.000 --> 00:34:46.000
That makes us quite special and quite inclusive.

00:34:46.000 --> 00:34:58.000
I joked that the PSF developer survey, which the new version is coming out pretty soon, I joked that 101% of Python developers started programming yesterday.

00:34:58.000 --> 00:35:11.000
So, you know, funny you should say that because this is my sweet spot is where technology meets humans and how do we empower humans to do more and better work.

00:35:11.000 --> 00:35:24.000
And one of the conversations that came up at the packaging summit, this PyCon, was, you know, I had been thinking about this concept for a while.

00:35:24.000 --> 00:35:33.000
You know, we focused a lot on tooling, which to me is sort of a producer centric, people who are creating packages.

00:35:33.000 --> 00:35:42.000
And we also have this ecosystem of people who are consumers who are, much like Jody was saying, using those packages.

00:35:42.000 --> 00:36:06.000
And from that conversation, a few of the board members for the PSF and I were talking about, wouldn't it be great to have a user success work group that's really focused on the website, you know, our onboarding documentation, in light of some of these things, both performance and change.

00:36:06.000 --> 00:36:19.000
You know, change is always going to be there. But I think, you know, I think one of the beauties of the Jupyter notebook or IPython notebook when I started working with it was, you can have code in there.

00:36:19.000 --> 00:36:23.000
And as long as you knew shift enter, you could get started.

00:36:23.000 --> 00:36:34.000
And I think right now, Python as a language, we don't have that get started look and feel in the way, in the traditional way.

00:36:34.000 --> 00:36:39.000
We're getting there, which might lead into some other WebAssembly kind of discussions.

00:36:39.000 --> 00:36:41.000
All right.

00:36:41.000 --> 00:36:42.000
Indeed.

00:36:42.000 --> 00:36:47.000
Let me throw out a quick thought on this before we move on.

00:36:47.000 --> 00:36:55.000
I think one of the superpowers of Python is that it's this full spectrum sort of thing.

00:36:55.000 --> 00:36:58.000
On one hand, there's the people that Jody spoke about.

00:36:58.000 --> 00:37:06.000
They come in, they don't care about better programming or optimized database queries or scaling out across webs.

00:37:06.000 --> 00:37:12.000
They just got a little bit of data, they want a cool graph, and that's awesome.

00:37:12.000 --> 00:37:17.000
On the other hand, we have Instagram and others doing ridiculous stuff.

00:37:17.000 --> 00:37:22.000
And that's the same language with the same tooling and mostly the same packages.

00:37:22.000 --> 00:37:29.000
And so I think part of Python's magic is you can be super productive with a very partial understanding of what Python is.

00:37:29.000 --> 00:37:36.000
You might not know what a class is at all, and yet you could have a fantastic time for months.

00:37:36.000 --> 00:37:45.000
And so back to Paul's trend, if we can keep that zen about it, where these features exist,

00:37:45.000 --> 00:37:50.000
but they exist when you graduate to them and you don't have to deal with them until you're ready or you need them,

00:37:50.000 --> 00:37:53.000
I think we'll be fine.

00:37:53.000 --> 00:37:55.000
If not, maybe not.

00:37:55.000 --> 00:37:59.000
If it breaks a bunch of packages and there's some big split in the ecosystem, all that stuff is not good.

00:37:59.000 --> 00:38:03.000
But if we can keep this full spectrum aspect, I think that'd be great.

00:38:03.000 --> 00:38:08.000
And I think that sort of rolls into what Paul's thoughts on community are,

00:38:08.000 --> 00:38:16.000
because I know like PyOpenSci is a nonprofit I'm involved with that helps scientists learn how to use the tools.

00:38:16.000 --> 00:38:19.000
We've got lots of educators out there.

00:38:19.000 --> 00:38:26.000
I'm going to give Michael a huge plug for the coursework that you've done over the years.

00:38:26.000 --> 00:38:29.000
It is so well done and so accessible.

00:38:29.000 --> 00:38:31.000
Thank you.

00:38:31.000 --> 00:38:37.000
And I think if people haven't tried it and they're interested in a topic, highly, highly recommend.

00:38:37.000 --> 00:38:44.000
To things like the Carpentries, to things like Django Girls, there's a lot of good stuff.

00:38:44.000 --> 00:38:50.000
And I think those things will become more valuable as complexity increases.

00:38:50.000 --> 00:38:51.000
Totally.

00:38:51.000 --> 00:38:52.000
And even LLMs.

00:38:52.000 --> 00:38:55.000
I think you'll be able to ask LLMs for help and they can help you.

00:38:55.000 --> 00:38:56.000
Absolutely.

00:38:56.000 --> 00:38:57.000
If you're not sure.

00:38:57.000 --> 00:38:59.000
They're pretty good at it, actually.

00:38:59.000 --> 00:39:01.000
They are pretty good at it, yeah.

00:39:01.000 --> 00:39:03.000
They are pretty good.

00:39:03.000 --> 00:39:04.000
All right.

00:39:04.000 --> 00:39:08.000
We got time for another round, I'm pretty sure.

00:39:08.000 --> 00:39:10.000
Jody, what's your second one?

00:39:10.000 --> 00:39:11.000
Your second trend?

00:39:11.000 --> 00:39:20.000
I'm going to talk about Arrow and how we're kind of overhauling data frames within Python.

00:39:20.000 --> 00:39:26.000
So basically, around 15 years ago, Wes McKinney came up with Pandas,

00:39:26.000 --> 00:39:32.000
if you're not familiar with it, it's the main data frame library for working with data in Python.

00:39:32.000 --> 00:39:38.000
And the really nice thing about Pandas is, like, you can go a long time before you graduate Pandas.

00:39:38.000 --> 00:39:41.000
You can just work with quite a lot of data on your local machine.

00:39:41.000 --> 00:39:46.000
But the problem was Wes wrote this package before we had big data.

00:39:46.000 --> 00:39:48.000
This was like 2008.

00:39:48.000 --> 00:39:53.000
And so as the sort of amount of data that we want to process locally has grown,

00:39:53.000 --> 00:39:59.000
or maybe the complexity of the operations has grown, maybe like string manipulations, things like that,

00:39:59.000 --> 00:40:01.000
Pandas has really struggled.

00:40:01.000 --> 00:40:06.000
So one of the reasons that Pandas struggled is it was based on NumPy arrays,

00:40:06.000 --> 00:40:09.000
which are really great at handling numbers, this is in the name,

00:40:09.000 --> 00:40:14.000
but they're not so great at handling pretty much any other data type, and that includes missing data.

00:40:14.000 --> 00:40:18.000
So two kind of exciting things happened last year,

00:40:18.000 --> 00:40:23.000
and I think they're sort of still kind of carrying over to this year in terms of impact,

00:40:23.000 --> 00:40:28.000
is first Pandas 2.0 was released, which is based on PyArrow,

00:40:28.000 --> 00:40:36.000
and a package called Polars, which was actually written, I think, in 2022, I want to say,

00:40:36.000 --> 00:40:39.000
started becoming very, very popular.

00:40:39.000 --> 00:40:42.000
So both of these packages are based on Arrow.

00:40:42.000 --> 00:40:46.000
They have, like, a number of advantages because of this.

00:40:46.000 --> 00:40:48.000
Basically, it's a standardized data format.

00:40:48.000 --> 00:40:52.000
If you're reading in from, say, Parquet or Cassandra or Spark,

00:40:52.000 --> 00:40:56.000
you basically don't need to convert the data formats.

00:40:56.000 --> 00:40:57.000
This saves you a lot of time.

00:40:57.000 --> 00:41:00.000
It also saves you a lot of memory.

00:41:00.000 --> 00:41:05.000
And also kind of what makes Polars interesting,

00:41:05.000 --> 00:41:10.000
and I think this is going to be a nice lead-in to another topic, is it's written in Rust, of course.

00:41:10.000 --> 00:41:13.000
So this leads to other performance gains.

00:41:13.000 --> 00:41:16.000
Like you can have, say, concurrency.

00:41:16.000 --> 00:41:20.000
Richie Vink, the author of this, has also written basically a query optimizer.

00:41:20.000 --> 00:41:24.000
So you can do a lazy evaluation, and it will actually optimize the order of operations,

00:41:24.000 --> 00:41:28.000
even if you don't do that yourself.

00:41:28.000 --> 00:41:32.000
Yeah, that's one of the biggest differences with Pandas, is Pandas executes immediately,

00:41:32.000 --> 00:41:35.000
and you can create a big chain in Polars, and it'll figure out,

00:41:35.000 --> 00:41:37.000
well, maybe a different order would be way better.

00:41:37.000 --> 00:41:39.000
Yes, yes.

00:41:39.000 --> 00:41:45.000
So Pandas 2 does have a type of lazy evaluation, but it's more like Spark's lazy evaluation.

00:41:45.000 --> 00:41:47.000
There's no query optimization.

00:41:47.000 --> 00:41:55.000
But it doesn't necessarily create a new copy in memory every single time you do something.

00:41:55.000 --> 00:42:01.000
But, yeah, so I've kind of looked at the numbers, and depending on the operation,

00:42:01.000 --> 00:42:03.000
Polars is usually faster.

00:42:03.000 --> 00:42:09.000
So it's kind of like your big boy that you want to use if you're doing really beefy,

00:42:09.000 --> 00:42:13.000
like, ETLs, like data transformations.

00:42:13.000 --> 00:42:18.000
But Pandas 2 actually seems to be more efficient at some sorts of,

00:42:18.000 --> 00:42:20.000
what am I trying to say, operations.

00:42:20.000 --> 00:42:26.000
So this is super exciting because when I was going through, like, initially as a data scientist,

00:42:26.000 --> 00:42:30.000
when I was floundering around with my initial Python,

00:42:30.000 --> 00:42:35.000
it got really frustrating with Pandas, and you really kind of needed to understand

00:42:35.000 --> 00:42:40.000
how to do proper vectorization in order to operate, I mean, like, do efficient operations.

00:42:40.000 --> 00:42:44.000
Whereas I think these two tools allow you to be a bit more lazy,

00:42:44.000 --> 00:42:50.000
and you don't need to spend so much time optimizing what you're actually writing.

00:42:50.000 --> 00:42:54.000
So, yeah, exciting time for data frames, which is awesome.

00:42:54.000 --> 00:42:57.000
Data is the heart of everything.

00:42:57.000 --> 00:43:03.000
Well, and people are more likely to fall into good practices from the start.

00:43:03.000 --> 00:43:08.000
You talked about these people coming who are not programmers, right?

00:43:08.000 --> 00:43:12.000
If you do a bunch of operations with Pandas, and you all of a sudden run out of memory,

00:43:12.000 --> 00:43:14.000
well, yeah, Python doesn't work.

00:43:14.000 --> 00:43:15.000
It doesn't have enough memory, right?

00:43:15.000 --> 00:43:18.000
Well, maybe you could have used a generator at one step.

00:43:18.000 --> 00:43:22.000
But that's far down the full spectrum, part of the spectrum, right?

00:43:22.000 --> 00:43:23.000
You're not ready for that.

00:43:23.000 --> 00:43:25.000
That's crazy talk, these things.

00:43:25.000 --> 00:43:32.000
And so tools like this that are more lazy and progressive iterative are great.

00:43:32.000 --> 00:43:36.000
Yeah, and actually one really nice thing, like Richie is kind of always saying about

00:43:36.000 --> 00:43:42.000
Apollo is he's really tried to write the API so you avoid accidentally looping

00:43:42.000 --> 00:43:45.000
over every row in your data frame.

00:43:45.000 --> 00:43:49.000
He tries to make it so everything is natively Columa.

00:43:49.000 --> 00:43:54.000
So, yeah, I just think they're both really nice libraries.

00:43:54.000 --> 00:43:58.000
And, yeah, it's cool and exciting.

00:43:58.000 --> 00:44:01.000
Carol, this is right in the heart of the space you live in.

00:44:01.000 --> 00:44:03.000
What do you think?

00:44:03.000 --> 00:44:12.000
I think there's definitely the evolution of Pandas and Polars.

00:44:12.000 --> 00:44:14.000
There's a place for all of those.

00:44:14.000 --> 00:44:20.000
And I think it's really interesting to think about this in the PyArrow data frame format.

00:44:20.000 --> 00:44:36.000
It's funny because I've actually been doing more stuff recently with going beyond tabular data frames to multi-dimensional arrays and X-array, which is used more in the geosciences for now.

00:44:36.000 --> 00:44:48.000
But one of the things that I see is the days of bringing all your data locally or moving it to you is becoming less and less.

00:44:48.000 --> 00:44:59.000
And what you work in memory or, you know, pull into memory from different locations is becoming more prevalent.

00:44:59.000 --> 00:45:11.000
And I think Arrow lets us do that more effectively than just a straight Pandas data frame or, you know, Spark or something like that.

00:45:11.000 --> 00:45:14.000
So, you know, it's progress.

00:45:14.000 --> 00:45:16.000
And I think it's a good thing.

00:45:16.000 --> 00:45:31.000
I think it's far less about the language underneath and more about what's the user experience, developer experience that we're giving people with these APIs.

00:45:31.000 --> 00:45:34.000
Paul, thoughts?

00:45:34.000 --> 00:45:40.000
It's interesting the scale of data.

00:45:40.000 --> 00:45:52.000
And, you know, what generations are an increase in our unit of measurement of data that we have to deal with?

00:45:52.000 --> 00:46:15.000
And for both of you, I wonder if we have caught up with the amount of data that we can reasonably process or is the rate of growth of data out in the wild constantly outstripping our ability to process it?

00:46:15.000 --> 00:46:28.000
So from an astronomy space physics side of things, no, we haven't hit the limit for data at all.

00:46:28.000 --> 00:46:42.000
And I think one of the things we're going to see more and more of is how we deal with streaming data versus -- and time series data versus just, you know, tabular data, if you will.

00:46:42.000 --> 00:47:00.000
And my bigger concern and it is partially a concern I have about some of the large language models and the training there is the environmental impact of some of these things.

00:47:00.000 --> 00:47:04.000
And, you know, should we be collecting it, A?

00:47:04.000 --> 00:47:07.000
You know, is there value in collecting it?

00:47:07.000 --> 00:47:12.000
If there's not value in collecting it, how do we get rid of it?

00:47:12.000 --> 00:47:20.000
Because, you know, it winds up then being kind of much like, you know, recycling and garbage.

00:47:20.000 --> 00:47:27.000
You know, it's like, okay, well, but it might have historical value somehow or legal value.

00:47:27.000 --> 00:47:29.000
And it becomes complex.

00:47:29.000 --> 00:47:37.000
And so, you know, my general rule of thumb is, you know, don't collect it unless you have a clear reason you need it.

00:47:37.000 --> 00:47:40.000
But that's just me.

00:47:40.000 --> 00:47:45.000
It's also quantity versus quality of data.

00:47:45.000 --> 00:47:52.000
So, like, I've worked in mostly commercial data science since I left science.

00:47:52.000 --> 00:47:57.000
When I was in science, I was dealing with sample size of 400, not 400,000, 400.

00:47:57.000 --> 00:47:59.000
So that was not big data.

00:47:59.000 --> 00:48:10.000
The quality of the data, like, again, going back to large language models, a lot of these earlier foundational models were trained on insufficiently clean data.

00:48:10.000 --> 00:48:19.000
And one of the trends actually that I didn't mention with LLMs is, like, last year in particular, there was a push to train on better quality data sources.

00:48:19.000 --> 00:48:28.000
So obviously, these are much more manageable than dealing with petabytes.

00:48:28.000 --> 00:48:36.000
One more aspect I'll throw out here, you know, for a long time, we've had SQLite for really simple data.

00:48:36.000 --> 00:48:38.000
We could just, if it's too big for memory, you can put it in one of those things.

00:48:38.000 --> 00:48:40.000
You can query, you can index it.

00:48:40.000 --> 00:48:48.000
Well, you know, DuckDB just hit 1.0, and you kind of got this in-memory, in-process analytics engine.

00:48:48.000 --> 00:48:51.000
That's also a pretty interesting thing to weave in here, right?

00:48:51.000 --> 00:48:56.000
To say, like, "Well, we'll put it there in that file, and we can index it and ask it questions, but we won't run out of memory."

00:48:56.000 --> 00:49:07.000
And I think you can plug in Pandas, I'm not sure about Polars, and do queries with its query optimizer against that data and things like that.

00:49:07.000 --> 00:49:15.000
It's pretty interesting, I think, to put it into that space as well.

00:49:15.000 --> 00:49:20.000
Carol, I think it's time for your second trend here.

00:49:20.000 --> 00:49:27.000
Yeah, well, the second trend is pretty much, you know, things are moving to the front end.

00:49:27.000 --> 00:49:33.000
WebAssembly, TypeScript, PyEdite.

00:49:33.000 --> 00:49:49.000
There's a new project, PyCafe, that I'm pretty happy with by Martin Bredels that lets you do dashboards using PyEdite, but like Streamlit and Plotly and libraries and things like that.

00:49:49.000 --> 00:49:59.000
And I think, you know, making things more accessible as well as making things more visual is pretty cool.

00:49:59.000 --> 00:50:18.000
Like, I took, what was it, JupyterLite earlier last fall, and a friend of mine had kids, and I integrated into my website so that, like, her kids could just do a quick whatever, which sort of, you know, in some ways was similar to Binder.

00:50:18.000 --> 00:50:29.000
And the whole time we were developing Binder, I was also working with the PyEdite/Iodide folks, because I think there's a convergence down the road.

00:50:29.000 --> 00:50:47.000
And, you know, where it all will go, I'm not really sure, but I think it's exciting.

00:50:47.000 --> 00:50:50.000
And I think anything that, you know, from a privacy standpoint, security, there's a lot of things that are very attractive about pushing things into the front end.

00:50:50.000 --> 00:51:00.000
Yeah, that beginner startup thing you talked about, that onboarding first experience.

00:51:00.000 --> 00:51:08.000
You hit a webpage and you have full experience with Python and the tooling and the packages are already installed in that thing.

00:51:08.000 --> 00:51:14.000
And that's so much better than, first you download it, well, you need admin permissions to install it.

00:51:14.000 --> 00:51:21.000
You don't have to ask permission to run a static webpage, or you do for like, how do I run the server on a Docker cluster?

00:51:21.000 --> 00:51:25.000
Yeah, yeah, totally.

00:51:25.000 --> 00:51:44.000
So I think, you know, it opens up different doors. And I think the other thing we found, like when we were teaching, you know, with Binder and JupyterHub, UC Berkeley was able to have now, most of their student body taking these data eight connector courses.

00:51:44.000 --> 00:52:00.000
And they would run the compute in the cloud, which really leveled the playing field. It didn't matter if you had a Chromebook, or you had the highest end Mac, you still got the same education.

00:52:00.000 --> 00:52:05.000
And I think there is something very appealing about that.

00:52:05.000 --> 00:52:15.000
We've actually been running Humble data in JupyterLite. And some people just bring a tablet. And they can do it on that.

00:52:15.000 --> 00:52:17.000
That's awesome.

00:52:17.000 --> 00:52:31.000
No, Carol, there was something you were saying that connected to something else in my brain. Remember in the beginning of the web and view source was such a cool thing, you could see what the back end sent you.

00:52:31.000 --> 00:52:39.000
And you could poke around at it, you could learn from it, and you could steal it, you know, and use it to go make your own thing.

00:52:39.000 --> 00:52:45.000
But what if you could view source the back end, because it's actually running in your browser.

00:52:45.000 --> 00:53:03.000
What you were just saying was, if you make it reveal itself about the notebook and the code, in addition to the HTML, maybe you'll trigger some of those same kinds of things that view source gave people back in the day.

00:53:03.000 --> 00:53:16.000
Maybe. The flip side would be, there's always business and practicalities in life, and people will want to sort of lock it down within WebAssembly.

00:53:16.000 --> 00:53:28.000
So you've got both sides of it. But I do think, you know, I was telling somebody the other day, like, I never use Stack Overflow, or rarely use Stack Overflow.

00:53:28.000 --> 00:53:36.000
And they're like, how do you find stuff? I'm like, I use search on GitHub. And I look for really good examples.

00:53:36.000 --> 00:53:46.000
And so in some ways, you know, it's like view source. And then there's also the flip side of it is like, okay, how do I break it?

00:53:46.000 --> 00:53:51.000
How do I play with it? How do I make it do something it wasn't doing before?

00:53:51.000 --> 00:53:59.000
And so I think it's, you know, could be used for good or for evil. I tend to use it for good. But you know.

00:53:59.000 --> 00:54:10.000
Yeah, amazing. All right, Paul, coming up on our time here. What's your second?

00:54:10.000 --> 00:54:11.000
Sure.

00:54:11.000 --> 00:54:16.000
Second trend. See if we have time for mine. I have a couple just in case we can squeeze them in.

00:54:16.000 --> 00:54:17.000
Okay. All right.

00:54:17.000 --> 00:54:20.000
Let's talk about yours.

00:54:20.000 --> 00:54:31.000
Yeah, so my stack from PyCon really rejuvenated, but also had some kind of clarity about some things that have been lingering for me for a few years, how I could contribute things like that.

00:54:31.000 --> 00:54:40.000
But going into it, there are a couple of trends that lead me to thinking about an opportunity and a threat as two sides of the same coin.

00:54:40.000 --> 00:54:50.000
First, Russell Keith McGee and Lukas Lange both talked about Black Swans and the threat of JavaScript everywhere.

00:54:50.000 --> 00:55:05.000
That if we don't have a better web story, if we make our front end be JavaScript and React, and we stop doing front ends, well, then they'll come for the back end too.

00:55:05.000 --> 00:55:10.000
Because once they've hired up JavaScript developers, why don't we just do JavaScript on the server too?

00:55:10.000 --> 00:55:18.000
So that was a first thing. And in my position, I do look at the web and think about all these trends that are happening.

00:55:18.000 --> 00:55:25.000
And there's beginning to be a little bit of a backlash about the JavaScriptification of the web.

00:55:25.000 --> 00:55:29.000
And so some really big names, HTMX is a good example of it.

00:55:29.000 --> 00:55:36.000
But just some thinkers and speakers. I mean, Jeff Triplett talks about this. A lot of people in the world of Python talk about this.

00:55:36.000 --> 00:55:42.000
So there's kind of a desire to put the web back in the web. Trademark.

00:55:42.000 --> 00:55:48.000
But then there was a second point coming about these walled gardens. We've seen them for a while.

00:55:48.000 --> 00:55:58.000
We all relied on Twitter. What a great place. Wait, what? And then so much of our life is in a system we don't control.

00:55:58.000 --> 00:56:05.000
And so we move over to the Fediverse. And then Meta's like, hey, great, we're going to build a bridge to you.

00:56:05.000 --> 00:56:11.000
Turns out this week we start to learn things about the thread API that maybe it's not as friendly as we think it is.

00:56:11.000 --> 00:56:19.000
But the big one for me was Google and Search. Well, I should say Google and getting rid of its Python staff.

00:56:19.000 --> 00:56:26.000
But Google and Search, where they're no longer going to send you to the website anymore.

00:56:26.000 --> 00:56:30.000
They're just going to harvest what's on your website and give you the answer.

00:56:30.000 --> 00:56:39.000
And people are talking now about Google Zero, the day of the apocalypse where you no longer get any clicks from Google.

00:56:39.000 --> 00:56:44.000
And what does that mean for content creators and stuff like that?

00:56:44.000 --> 00:56:53.000
So going into all of this, I've been thinking about how awesome life is in Python land.

00:56:53.000 --> 00:56:58.000
Because we've got this great language. Oh, but we've got this great community. Come for the language, stay for the community.

00:56:58.000 --> 00:57:03.000
Well, what do we mean by that? A lot of the times we mean all this code that's available.

00:57:03.000 --> 00:57:09.000
We also mean all these people and wonderful, helpful people like on this call.

00:57:09.000 --> 00:57:15.000
But there's also this big world of content.

00:57:15.000 --> 00:57:29.000
And we have kind of organically grown a little online community with a bunch of helpful content

00:57:29.000 --> 00:57:36.000
and a bunch of connections between people, which is of some value itself.

00:57:36.000 --> 00:57:40.000
And so you see people starting to talk about, wow, I miss the old days of RSS,

00:57:40.000 --> 00:57:45.000
where we would all subscribe to each other's blogs and get content and go straight to the source

00:57:45.000 --> 00:57:49.000
and not have it aggregated into a walled garden and stuff like that.

00:57:49.000 --> 00:57:54.000
And it just feels like there's room out there for it.

00:57:54.000 --> 00:58:04.000
And if we want to fight back against the threat of these mega cores taking our voluntary contribution to humanity

00:58:04.000 --> 00:58:15.000
and monetizing it, while at the same time of taking all these valuable voices creating content and value in Python land,

00:58:15.000 --> 00:58:21.000
that maybe we could bring back some of these things, put the web back in the web,

00:58:21.000 --> 00:58:34.000
and start to get out of the walled gardens and back over into social networks that are open and joyful.

00:58:34.000 --> 00:58:37.000
I'm here for it.

00:58:37.000 --> 00:58:38.000
Wow.

00:58:38.000 --> 00:58:47.000
People complain, governments complain, that places like Google and stuff are monetizing the links

00:58:47.000 --> 00:58:56.000
or you've got to pay to link to this new source or whatever.

00:58:56.000 --> 00:58:57.000
We're lucky that we have that.

00:58:57.000 --> 00:59:04.000
If it turns into just you just get an AI answer, no source, that's going to be really hard on a lot of different businesses, creators,

00:59:04.000 --> 00:59:09.000
people who just want to create something just for the attention or for their self.

00:59:09.000 --> 00:59:12.000
Nobody comes anymore. It's going to be a sad place.

00:59:12.000 --> 00:59:19.000
I was thinking about Coke Zero the whole time you were saying like, Google Zero or whatever,

00:59:19.000 --> 00:59:22.000
because you didn't have to bring back classic Coke.

00:59:22.000 --> 00:59:30.000
And so I think, yeah, pivots happen, but it's hard to pivot billion dollar companies.

00:59:30.000 --> 00:59:43.000
I have lots of thoughts on some of the current Python, what Google has chosen to do.

00:59:43.000 --> 00:59:55.000
I think sometimes listening to consultants isn't the best business approach, but whatever.

00:59:55.000 --> 01:00:00.000
It's their company, they can do what they need to do for their own shareholders.

01:00:00.000 --> 01:00:04.000
I think a lot of what you said is really interesting.

01:00:04.000 --> 01:00:12.000
And I touched on this a little bit because the volume of information around us is greater than ever before.

01:00:12.000 --> 01:00:13.000
Sure.

01:00:13.000 --> 01:00:19.000
And at a speed of transmission that is faster than ever before.

01:00:19.000 --> 01:00:26.000
And about eight years ago, I had breakfast with Sandy Metz, who is very prolific in the Ruby community.

01:00:26.000 --> 01:00:29.000
And I asked her, how do you keep up with all of this stuff?

01:00:29.000 --> 01:00:32.000
And she's like, I don't.

01:00:32.000 --> 01:00:34.000
And I said, OK.

01:00:34.000 --> 01:00:42.000
And she's like, what I do is I focus on the things that impact me and all the rest of it is news.

01:00:42.000 --> 01:00:50.000
And that really stuck with me because in actuality, that's kind of what I do.

01:00:50.000 --> 01:01:07.000
I ignore the things that aren't directly relevant to me and trust that I've built a strong enough network of people that I respect that their work will influence when I jump in.

01:01:07.000 --> 01:01:15.000
Much like the life cycle, if you've studied marketing or product development, not everybody is an early adopter.

01:01:15.000 --> 01:01:19.000
And so do I need to be an early adopter on everything?

01:01:19.000 --> 01:01:20.000
No.

01:01:20.000 --> 01:01:21.000
That's right.

01:01:21.000 --> 01:01:26.000
Yeah, that book Crossing the Chasm says that you should do that like on one thing.

01:01:26.000 --> 01:01:29.000
If you do it on three things or more, you'll fail.

01:01:29.000 --> 01:01:30.000
Yeah.

01:01:30.000 --> 01:01:49.000
Part of the thing that triggered this for me was reading that Andreessen Horowitz, kind of the self-proclaimed king of Silicon Valley VCs, as zero interest rates started to go out of fashion and their recipe wasn't working, they didn't like the negative press coverage.

01:01:49.000 --> 01:01:54.000
So they started their own media empire to cover themselves.

01:01:54.000 --> 01:02:11.000
And that idea is just so appalling that we would get news, we would turn to the megacorps and the masters of the universe to tell us what we should be caring about.

01:02:11.000 --> 01:02:13.000
We have that already.

01:02:13.000 --> 01:02:17.000
We have, I'll be very specific, we have Planet Python.

01:02:17.000 --> 01:02:19.000
It's in disrepair.

01:02:19.000 --> 01:02:33.000
What if it was reimagined into a freaking media empire by us for us to cover the Fediverse and course providers and all the value that's out there?

01:02:33.000 --> 01:02:40.000
And like, Carol, you're saying, I don't have to think about it, but I trust that group because they're thinking about it.

01:02:40.000 --> 01:02:56.000
I mean, a lot of it is like, when it came to LLMs, it was not the thing that rocked my world intellectually, but I knew Simon was doing work with it.

01:02:56.000 --> 01:02:57.000
Sure.

01:02:57.000 --> 01:03:11.000
And so I basically, you know, once every few weeks would take a look at his website and his blog posts, and he posts a lot, and I would get my data dump of, you know, things.

01:03:11.000 --> 01:03:16.000
And so I don't know.

01:03:16.000 --> 01:03:18.000
I mean, that's one of the reasons I like PyCon.

01:03:18.000 --> 01:03:23.000
And I've like read talk proposals, everything for the last decade.

01:03:23.000 --> 01:03:24.000
Oh, wow.

01:03:24.000 --> 01:03:30.000
All these talk proposals, and it really does give me an appreciation for all the things Python is being used for.

01:03:30.000 --> 01:03:32.000
Kind of the zeitgeist.

01:03:32.000 --> 01:03:33.000
Yeah.

01:03:33.000 --> 01:03:41.000
And so I think there's different ways of doing that, even just doing a YouTube search of Python content.

01:03:41.000 --> 01:03:52.000
But, you know, I tend to focus in on sciency-oriented things and ways to empower humans through lifelong learning.

01:03:52.000 --> 01:04:01.000
So, you know, but it is, there's a lot of, we're in a phenomenal period of change, for sure.

01:04:01.000 --> 01:04:02.000
Yes.

01:04:02.000 --> 01:04:06.000
So we won't be bored, nor do I think our jobs are going to go away.

01:04:06.000 --> 01:04:08.000
They may change, but they're not going away.

01:04:08.000 --> 01:04:10.000
Yeah.

01:04:10.000 --> 01:04:11.000
Indeed.

01:04:11.000 --> 01:04:13.000
Jody, final thoughts on this topic?

01:04:13.000 --> 01:04:15.000
And we'll pretty much wrap things up.

01:04:15.000 --> 01:04:18.000
Yeah, I don't think I really have that much to add, actually.

01:04:18.000 --> 01:04:20.000
I think it's all been said.

01:04:20.000 --> 01:04:22.000
It has, it has.

01:04:22.000 --> 01:04:24.000
All right, just to round things out.

01:04:24.000 --> 01:04:30.000
The two things that I think are trends here is I think, like Carol said a lot, Python on the front end is going to be super important.

01:04:30.000 --> 01:04:33.000
I think PyScript is really, really interesting.

01:04:33.000 --> 01:04:41.000
I've been waiting for people to develop something like React or Vue or something that we could create.

01:04:41.000 --> 01:04:46.000
Commercial-facing websites, we're halfway there with MicroPython being the foundation of PyScript,

01:04:46.000 --> 01:04:50.000
which is 100K instead of 10 megs.

01:04:50.000 --> 01:04:52.000
All of a sudden it becomes JavaScript-y size.

01:04:52.000 --> 01:04:54.000
It opens up possibilities.

01:04:54.000 --> 01:05:00.000
And just a shout-out to PewPy, which is like Vue with Python, P-U-E, P-Y.

01:05:00.000 --> 01:05:05.000
I'm going to interview Ken from that project, but it's kind of a component-based front end for PyScript,

01:05:05.000 --> 01:05:07.000
which is pretty interesting.

01:05:07.000 --> 01:05:09.000
And of course, JupyterLite is really, really important.

01:05:09.000 --> 01:05:12.000
The other one was just all this Rust.

01:05:12.000 --> 01:05:17.000
Everything seems to be redone in Rust, and oh my gosh, that's how you get your VC funding.

01:05:17.000 --> 01:05:19.000
Just joking, sort of.

01:05:19.000 --> 01:05:22.000
But Paul, you talked about all this performance stuff coming.

01:05:22.000 --> 01:05:27.000
While it is sometimes frustrating that people are putting all the things into Rust

01:05:27.000 --> 01:05:30.000
because then Python programmers, it's less approachable for them,

01:05:30.000 --> 01:05:37.000
it could also be an escape hatch from trying to force the complexity into the Python side.

01:05:37.000 --> 01:05:42.000
Everything has to be multi-threaded and crazy and optimized.

01:05:42.000 --> 01:05:44.000
This part you never look at.

01:05:44.000 --> 01:05:46.000
It's faster now, so don't worry.

01:05:46.000 --> 01:05:48.000
Those are my two trends.

01:05:48.000 --> 01:05:50.000
Quick thoughts on that, and then we'll call it a show.

01:05:50.000 --> 01:05:58.000
mypyce of trivia is I made a contribution to Rust far before I made any contributions to core Python.

01:05:58.000 --> 01:06:00.000
Amazing.

01:06:00.000 --> 01:06:04.000
Because I tended to be a C programmer in heart and spirit.

01:06:04.000 --> 01:06:08.000
So Rust seemed like this cool thing that was new at the time.

01:06:08.000 --> 01:06:20.000
And ultimately, I personally did not find the syntactic side of it worked well with my brain and how I think.

01:06:20.000 --> 01:06:26.000
And Python was far cleaner in terms of a simpler visual, less clutter,

01:06:26.000 --> 01:06:33.000
and reminded me a little more of Smalltalk or something like that, which I loved from earlier days.

01:06:33.000 --> 01:06:38.000
But I think there's a place for Rust.

01:06:38.000 --> 01:06:42.000
I think Rust is going to replace Python now.

01:06:42.000 --> 01:06:46.000
I think it's going to help with some optimized things.

01:06:46.000 --> 01:06:55.000
Do I love things like Ruff that let me run my CI blazing fast versus all the Python tools?

01:06:55.000 --> 01:07:02.000
Not to say that all the Python tools are bad, but when you're paying for it as a startup,

01:07:02.000 --> 01:07:05.000
when things you used to have to wait on become a blink of an eye,

01:07:05.000 --> 01:07:10.000
all of a sudden you don't mind running them every time and it changes the way you work with tools.

01:07:10.000 --> 01:07:12.000
Exactly.

01:07:12.000 --> 01:07:17.000
Yeah, I would say, look, every language has its place in the ecosystem.

01:07:17.000 --> 01:07:23.000
And my husband is a long-time Pythonista, but he's also a Rust programmer.

01:07:23.000 --> 01:07:26.000
I know, it's like a running joke that my husband is a Rust developer.

01:07:26.000 --> 01:07:29.000
How do you know? He'll ask you.

01:07:29.000 --> 01:07:33.000
You know what I mean. How do you know? Ask him. He'll tell you.

01:07:33.000 --> 01:07:35.000
There you go.

01:07:35.000 --> 01:07:42.000
They have different purposes, completely different purposes, and you can't just interchange them.

01:07:42.000 --> 01:07:45.000
Absolutely.

01:07:45.000 --> 01:07:49.000
Let's just get it straight. Python is just awesome, says Artem. Thank you.

01:07:49.000 --> 01:07:52.000
It's pure love.

01:07:52.000 --> 01:07:55.000
It's just us to keep it awesome.

01:07:55.000 --> 01:07:57.000
Yes, absolutely.

01:07:57.000 --> 01:08:02.000
Paul, we've come around to you for the very final, final thought on this excellent show.

01:08:02.000 --> 01:08:11.000
I will give a final thought about Python trends to follow up on what Carol just said about it's up to us.

01:08:11.000 --> 01:08:20.000
Maybe it's up to us to help the people who will keep it that way, the next generation of heroes.

01:08:20.000 --> 01:08:23.000
Help them succeed.

01:08:23.000 --> 01:08:37.000
I'm wearing my PyCon Kenya friendship bracelet that I got at PyCon, and a wonderful experience meeting so many different kinds of people.

01:08:37.000 --> 01:08:47.000
From a Python trends perspective, the fact that everything we're talking about is good stuff, not like asteroid meets earth.

01:08:47.000 --> 01:08:54.000
IP challenges and patent wars and mergers and acquisitions and stuff.

01:08:54.000 --> 01:08:59.000
I remember a long time ago, I went to go see Guido when he was with the App Engine team at Google.

01:08:59.000 --> 01:09:07.000
So a long time ago. And he was starting the process of turning over PEP review to other people.

01:09:07.000 --> 01:09:15.000
And I commented to him that not every open source success story outlives its founder.

01:09:15.000 --> 01:09:26.000
And the bigger it gets, particularly open source projects anchored in the United States of America, they sell out and get funded.

01:09:26.000 --> 01:09:30.000
And they will never be the same after that.

01:09:30.000 --> 01:09:42.000
And so it's a moment from a Python trends perspective for us to build a great next future by remembering how lucky we are where we have gotten to.

01:09:42.000 --> 01:09:50.000
Absolutely. Carol, Jodie, Paul, thank you for being on the show.

01:09:50.000 --> 01:09:53.000
Awesome to hear your perspective and everything.

01:09:53.000 --> 01:09:54.000
Thank you.

01:09:54.000 --> 01:09:55.000
Bye.

01:09:55.000 --> 01:09:56.520
everyone. Bye.

