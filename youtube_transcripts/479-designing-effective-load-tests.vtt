WEBVTT

00:00:00.001 --> 00:00:05.000
Anthony, welcome back to Talk Python To Me again.

00:00:05.000 --> 00:00:08.080
Again and again.

00:00:08.080 --> 00:00:08.900
- Hey, Mike.

00:00:08.900 --> 00:00:09.840
- So awesome to have you back.

00:00:09.840 --> 00:00:10.680
Always good.

00:00:10.680 --> 00:00:12.940
- Yeah, yeah, great to be here, hey.

00:00:12.940 --> 00:00:15.240
- Good to catch up, good to have you on the show.

00:00:15.240 --> 00:00:18.800
I know we've got a really interesting topic

00:00:18.800 --> 00:00:22.560
to talk about, performance, load testing,

00:00:22.560 --> 00:00:26.600
how do you know if your website or your API is gonna work

00:00:26.600 --> 00:00:28.280
when you ship it to the world.

00:00:29.220 --> 00:00:34.080
In a real world way, not just how many requests per second

00:00:34.080 --> 00:00:37.440
can it take, but real use case, as I know.

00:00:37.440 --> 00:00:38.560
You're gonna tell us all about,

00:00:38.560 --> 00:00:40.740
so that's gonna be awesome.

00:00:40.740 --> 00:00:44.120
Before we do, just quick introduction to who you are

00:00:44.120 --> 00:00:47.580
and maybe for most 98% of the world who knows you,

00:00:47.580 --> 00:00:49.320
what have you been up to?

00:00:49.320 --> 00:00:54.220
- Yeah, so I am the Python Advocacy Lead at Microsoft.

00:00:54.220 --> 00:00:57.120
I do a bunch of open source work,

00:00:57.120 --> 00:00:59.760
maintain some projects and stuff like that.

00:00:59.760 --> 00:01:02.680
Wrote a book on Python,

00:01:02.680 --> 00:01:05.020
the Python compiler called CPython Internals.

00:01:05.020 --> 00:01:09.280
But these days, I'm mostly known as the person

00:01:09.280 --> 00:01:11.320
that created VS Code Pets.

00:01:11.320 --> 00:01:16.780
And that's, which was a bit of fun,

00:01:16.780 --> 00:01:18.800
but it's become the most popular piece of software

00:01:18.800 --> 00:01:21.580
I've ever written, so that's been interesting.

00:01:21.580 --> 00:01:23.740
It's now got over a million active users.

00:01:23.740 --> 00:01:26.200
- Careful what you create,

00:01:26.200 --> 00:01:28.160
you might get known for it.

00:01:28.160 --> 00:01:29.380
- Yeah, I know.

00:01:29.380 --> 00:01:32.680
Yeah, it's interesting when you go to conferences

00:01:32.680 --> 00:01:33.760
and stuff now and I'm like,

00:01:33.760 --> 00:01:36.220
oh, I work on this project and this project,

00:01:36.220 --> 00:01:37.760
and then you'd mention the pets thing

00:01:37.760 --> 00:01:39.640
and they're like, oh, you're the pets person.

00:01:39.640 --> 00:01:41.440
Oh, you're the developer of the--

00:01:41.440 --> 00:01:44.560
- You spent a year writing a deep book

00:01:44.560 --> 00:01:47.880
on the internals of CPython and its runtime.

00:01:47.880 --> 00:01:48.720
I don't know who you are,

00:01:48.720 --> 00:01:50.880
but this pets thing is killer, man.

00:01:50.880 --> 00:01:52.880
- Yeah, there's a cat that runs around in VS Code,

00:01:52.880 --> 00:01:54.520
so that's cool.

00:01:54.520 --> 00:01:55.760
- Can it be a dog as well?

00:01:55.760 --> 00:01:57.040
What kind of pets can we have?

00:01:57.040 --> 00:01:58.080
- Oh, there's everything.

00:01:58.080 --> 00:02:00.600
Yeah, it can be a dog.

00:02:00.600 --> 00:02:04.600
There's chickens and turtles and snakes

00:02:04.600 --> 00:02:06.840
and yeah, everything you can think of.

00:02:06.840 --> 00:02:08.720
It's a pretty active repository as well.

00:02:08.720 --> 00:02:13.000
We get a lot of feature requests for new shades of fur

00:02:13.000 --> 00:02:16.800
and new pets and new behaviors and stuff like that.

00:02:16.800 --> 00:02:17.680
- Yeah, that's incredible.

00:02:17.680 --> 00:02:19.240
- Yeah, if you haven't checked it out,

00:02:19.240 --> 00:02:22.220
then check out the VS Code Pets extension for VS Code.

00:02:22.220 --> 00:02:24.000
- Yeah.

00:02:24.120 --> 00:02:26.160
(laughing)

00:02:26.160 --> 00:02:27.000
I installed it for a while.

00:02:27.000 --> 00:02:28.240
I had to uninstall it.

00:02:28.240 --> 00:02:30.760
It was too much.

00:02:30.760 --> 00:02:32.960
- Yeah, I think if you're one of those people

00:02:32.960 --> 00:02:36.760
that likes having distractions, then it's helpful.

00:02:36.760 --> 00:02:40.840
If you find it hard to have a little thing running around

00:02:40.840 --> 00:02:44.680
whilst you're trying to code, then it might be a bit much.

00:02:44.680 --> 00:02:46.720
- It's a little bit like power mode.

00:02:46.720 --> 00:02:47.720
- Oh yeah.

00:02:47.720 --> 00:02:50.120
- Are you familiar with the power mode?

00:02:50.120 --> 00:02:51.800
- I use the one in JetBrains.

00:02:52.760 --> 00:02:54.560
Yeah, I used the power mode for JetBrains

00:02:54.560 --> 00:02:56.640
when that was around.

00:02:56.640 --> 00:02:57.480
It was pretty cool.

00:02:57.480 --> 00:03:01.760
- Yeah, if you want maximum distraction for your work.

00:03:01.760 --> 00:03:03.800
- Yeah, it reminds me of Unreal Tournament,

00:03:03.800 --> 00:03:05.960
the sort of power modes for that.

00:03:05.960 --> 00:03:09.880
- That's right, that's awesome.

00:03:09.880 --> 00:03:15.760
All right, well, let's start off with some stories.

00:03:15.760 --> 00:03:21.640
So I think everyone has a sense of like,

00:03:21.680 --> 00:03:24.200
why do you want your website or your API

00:03:24.200 --> 00:03:28.360
or your microservice thing of API and website,

00:03:28.360 --> 00:03:30.360
however you combine these things,

00:03:30.360 --> 00:03:32.440
to work well and understand.

00:03:32.440 --> 00:03:35.040
But it's always fun to share some stories.

00:03:35.040 --> 00:03:39.680
- Yeah, so we wanna talk about load testing.

00:03:39.680 --> 00:03:43.720
And I think a fun thing to do with load testing

00:03:43.720 --> 00:03:47.120
is to reflect on times where people haven't done it

00:03:47.120 --> 00:03:50.640
the right way and it's caused a really public,

00:03:50.640 --> 00:03:52.240
like a disaster.

00:03:52.240 --> 00:03:58.400
But the analogy that I use is the halftime problem,

00:03:58.400 --> 00:04:02.880
which I'm English, and so this is mostly a soccer thing.

00:04:02.880 --> 00:04:07.440
But we've got really big soccer games

00:04:07.440 --> 00:04:10.080
and it would be the same, I guess, with the Super Bowl.

00:04:10.080 --> 00:04:12.400
- Yeah, I was just thinking American Super Bowl,

00:04:12.400 --> 00:04:13.240
football Super Bowl.

00:04:13.240 --> 00:04:15.040
- Yeah, but the difference is that in soccer,

00:04:15.040 --> 00:04:18.800
at 45 minutes, you've got a 15 minute break.

00:04:18.800 --> 00:04:19.640
- Yes.

00:04:19.640 --> 00:04:20.760
- And in the Super Bowl, every 15 minutes,

00:04:20.760 --> 00:04:22.600
you've got a 15 minute break.

00:04:22.600 --> 00:04:25.880
- Well, and it's loaded with commercials.

00:04:25.880 --> 00:04:27.440
So you've got like every five minutes,

00:04:27.440 --> 00:04:29.680
there's like, oh, here's two more commercial breaks.

00:04:29.680 --> 00:04:31.120
Where soccer is a flowing game,

00:04:31.120 --> 00:04:33.720
there's like no break until halftime, right?

00:04:33.720 --> 00:04:34.560
- Yeah, exactly.

00:04:34.560 --> 00:04:35.600
- That's the difference, I think.

00:04:35.600 --> 00:04:37.800
- So what happens with a big game,

00:04:37.800 --> 00:04:39.760
so if it's like a FA Cup final,

00:04:39.760 --> 00:04:42.880
or like a Champions League final or something,

00:04:42.880 --> 00:04:49.200
then you basically have 100 million people

00:04:49.400 --> 00:04:54.400
all stop watching the TV at the same time for 15 minutes,

00:04:54.400 --> 00:04:56.960
go to the kitchen, turn on the kettle,

00:04:56.960 --> 00:05:00.080
make a cup of tea, go to the bathroom.

00:05:00.080 --> 00:05:03.600
And so the electricity providers

00:05:03.600 --> 00:05:05.520
actually have to plan for this

00:05:05.520 --> 00:05:09.680
because a kettle uses like a couple of kilowatts of energy.

00:05:09.680 --> 00:05:14.000
- Especially in the UK where it's 240.

00:05:14.000 --> 00:05:14.840
- Yeah, exactly.

00:05:14.840 --> 00:05:15.920
- Yeah.

00:05:15.920 --> 00:05:18.280
- And all of a sudden, you've got like tens of millions

00:05:18.280 --> 00:05:21.600
of people all switching on their two kilowatt kettles

00:05:21.600 --> 00:05:23.080
at the same time.

00:05:23.080 --> 00:05:26.120
So the electricity providers basically have to plan

00:05:26.120 --> 00:05:27.880
around the soccer games

00:05:27.880 --> 00:05:33.520
so that they get this massive spike in load in their grid.

00:05:33.520 --> 00:05:35.920
So they actually do look at the sports schedules

00:05:35.920 --> 00:05:38.800
to basically plan around this,

00:05:38.800 --> 00:05:40.400
especially if it's something like a World Cup

00:05:40.400 --> 00:05:41.560
or something like that.

00:05:41.560 --> 00:05:44.600
And this is kind of like a load testing thing

00:05:44.600 --> 00:05:45.640
where you kind of...

00:05:45.640 --> 00:05:48.480
(audio cuts out)

00:05:48.480 --> 00:05:51.680
On the system is gonna be...

00:05:51.680 --> 00:05:58.560
And is it a spike in traffic or is it like distributed?

00:05:58.560 --> 00:06:04.960
And I've seen where it's gone a bit wrong.

00:06:04.960 --> 00:06:11.320
So here in Australia, we had a census

00:06:11.320 --> 00:06:12.520
maybe eight years ago.

00:06:14.000 --> 00:06:15.840
And normally it's a paper census.

00:06:15.840 --> 00:06:18.920
You fill in the form, say who you are, what you do.

00:06:18.920 --> 00:06:21.960
But this time they wanted to do it online as well.

00:06:21.960 --> 00:06:25.680
And they really encouraged people to use the online version.

00:06:25.680 --> 00:06:29.360
And they set up a system, set up the website

00:06:29.360 --> 00:06:31.280
and said, "Okay, everyone can fill in the census."

00:06:31.280 --> 00:06:34.400
There's only 20 something million people in Australia.

00:06:34.400 --> 00:06:36.000
It's not a very big population.

00:06:36.000 --> 00:06:41.520
But on the last night before the census was due, it crashed

00:06:41.520 --> 00:06:43.760
because everybody logged on at the last minute

00:06:43.760 --> 00:06:45.720
and tried to fill it in.

00:06:45.720 --> 00:06:47.160
And they hadn't tested it properly

00:06:47.160 --> 00:06:49.120
and it just wasn't ready for the load.

00:06:49.120 --> 00:06:51.720
And in the postmortem, they said,

00:06:51.720 --> 00:06:54.440
"Oh, we did the load testing and we tested it

00:06:54.440 --> 00:06:57.120
and it seemed fine, but we expected everybody

00:06:57.120 --> 00:06:59.800
to fill in the census over the six months

00:06:59.800 --> 00:07:01.640
they had to do it."

00:07:01.640 --> 00:07:03.000
We didn't think that people would leave it

00:07:03.000 --> 00:07:05.360
to the last minute, which is more like,

00:07:05.360 --> 00:07:06.760
well, what did you expect to happen?

00:07:06.760 --> 00:07:10.360
This is just human nature that you're gonna put it off

00:07:10.360 --> 00:07:12.520
until the last possible moment.

00:07:12.520 --> 00:07:14.240
- Yeah, I could only see two spikes,

00:07:14.240 --> 00:07:16.680
a small spike at the beginning and a huge spike at the end

00:07:16.680 --> 00:07:18.240
and like nothing in the middle.

00:07:18.240 --> 00:07:21.560
- Yeah, exactly.

00:07:21.560 --> 00:07:24.200
So they had to kind of delay the deadline

00:07:24.200 --> 00:07:27.400
and then provision new infrastructure and stuff like that.

00:07:27.400 --> 00:07:31.640
So yeah, it was interesting, but it kind of highlighted

00:07:31.640 --> 00:07:33.680
how you need to think about load testing.

00:07:33.680 --> 00:07:37.520
- Yeah, that's nuts.

00:07:37.520 --> 00:07:41.880
And I guess the mode before, the style before

00:07:41.880 --> 00:07:45.240
was kind of a distributed queuing system

00:07:45.240 --> 00:07:47.800
using paper envelopes where the queuing

00:07:47.800 --> 00:07:50.680
literally was physical and then it would be processed

00:07:50.680 --> 00:07:53.200
at the rate at which the system can handle it, right?

00:07:53.200 --> 00:07:57.960
Which, however fast you can get them in based on paper,

00:07:57.960 --> 00:08:01.120
but then you try to turn it into an interactive system

00:08:01.120 --> 00:08:01.960
and oh no.

00:08:01.960 --> 00:08:04.280
- Yeah, yeah, exactly.

00:08:04.280 --> 00:08:09.480
- I have another example to share.

00:08:09.480 --> 00:08:14.480
Like we've seen websites that fall down like this,

00:08:14.480 --> 00:08:16.680
it fell down.

00:08:16.680 --> 00:08:18.440
And when something has to be done,

00:08:18.440 --> 00:08:21.960
I think it's probably really tricky to communicate.

00:08:21.960 --> 00:08:26.120
Let's say 10 million people tried to fill it out that day

00:08:26.120 --> 00:08:27.240
and it crashed on them.

00:08:27.240 --> 00:08:30.760
Well, how do they know that the deadline's extended

00:08:30.760 --> 00:08:33.120
and how do they come back, right?

00:08:33.120 --> 00:08:34.640
It creates this cascading chain

00:08:34.640 --> 00:08:37.520
of really challenging problems all of a sudden

00:08:37.520 --> 00:08:38.440
to deal with it.

00:08:39.280 --> 00:08:43.040
We had the healthcare, Obamacare stuff in the US

00:08:43.040 --> 00:08:45.720
where as soon as that thing opened, it just completely died

00:08:45.720 --> 00:08:48.360
and for weeks people couldn't get insurance, which was bad.

00:08:48.360 --> 00:08:49.800
But that's not the story I wanna share.

00:08:49.800 --> 00:08:51.680
I wanna share a different one.

00:08:51.680 --> 00:08:56.680
There was this app, this person who was frustrated about,

00:08:56.680 --> 00:08:59.640
they're an artist, like a photographer, I believe.

00:08:59.640 --> 00:09:02.560
And they're frustrated about LLMs and diffusion models

00:09:02.560 --> 00:09:06.600
taking everyone's art and generating new art from it

00:09:06.600 --> 00:09:11.480
and kind of, I guess it's up to people's opinion

00:09:11.480 --> 00:09:14.120
whether or not it counts as stealing or copyright theft

00:09:14.120 --> 00:09:15.400
or it's fair use or whatever.

00:09:15.400 --> 00:09:18.560
But this person wasn't a fan of that.

00:09:18.560 --> 00:09:22.560
So they came up with a web app based on serverless

00:09:22.560 --> 00:09:25.400
that would look at stuff and say, this is real art,

00:09:25.400 --> 00:09:27.720
this is AI generated art.

00:09:27.720 --> 00:09:30.480
And it's not exactly the same,

00:09:30.480 --> 00:09:35.480
but they ended up with a $96,000 Vercel bill

00:09:36.480 --> 00:09:40.720
because they didn't really take into account

00:09:40.720 --> 00:09:44.360
how much it's going to cost per request

00:09:44.360 --> 00:09:45.920
to handle this on their serverless.

00:09:45.920 --> 00:09:48.320
It was like 20 cents a user and everyone's like,

00:09:48.320 --> 00:09:51.480
you know, this is right at the height of the LLM boom

00:09:51.480 --> 00:09:55.160
and all the AI art boom and stuff

00:09:55.160 --> 00:09:56.640
and people just swarmed to it.

00:09:56.640 --> 00:10:01.040
So that's a different kind of problem, right?

00:10:01.040 --> 00:10:02.640
- Yeah, that's a cost issue.

00:10:04.520 --> 00:10:08.080
Yeah, there are definitely other ones to keep in mind.

00:10:08.080 --> 00:10:11.440
And we mentioned like the census and the Obamacare thing

00:10:11.440 --> 00:10:12.920
and you might be listening and thinking,

00:10:12.920 --> 00:10:15.800
well, you know, my website is not gonna get,

00:10:15.800 --> 00:10:19.240
you know, 15 million people logging in

00:10:19.240 --> 00:10:20.240
all at the same time.

00:10:20.240 --> 00:10:23.320
So like, is this really an issue that I need to handle?

00:10:23.320 --> 00:10:27.880
I like a few months ago, I was doing a load test

00:10:27.880 --> 00:10:32.560
of an AI app, so like a chat app

00:10:34.040 --> 00:10:37.560
and designed a load test and ran it with 10 users

00:10:37.560 --> 00:10:39.040
and it had issues.

00:10:39.040 --> 00:10:42.360
So like, this doesn't need to be

00:10:42.360 --> 00:10:44.360
a tens of millions of users problem.

00:10:44.360 --> 00:10:47.520
And the issue that it uncovered was like,

00:10:47.520 --> 00:10:50.760
oh, there's a rate limit on the number of tokens

00:10:50.760 --> 00:10:52.840
you can send to the LLM and by default,

00:10:52.840 --> 00:10:54.680
it's a really small number.

00:10:54.680 --> 00:10:57.920
And if you have 10 people asking questions simultaneously,

00:10:57.920 --> 00:11:01.080
then that gets exhausted really quickly

00:11:01.080 --> 00:11:03.320
and it just throttles you.

00:11:03.320 --> 00:11:06.960
And then like the app just says, oh, rate limit error.

00:11:06.960 --> 00:11:09.160
And you just wouldn't have noticed

00:11:09.160 --> 00:11:12.040
'cause if you're a single user clicking around,

00:11:12.040 --> 00:11:13.760
typing in, you know, questions

00:11:13.760 --> 00:11:15.720
or even if you've written some fancy,

00:11:15.720 --> 00:11:18.680
like UI testing or something, you're like,

00:11:18.680 --> 00:11:20.720
oh, we've tested it and it works great.

00:11:20.720 --> 00:11:25.000
But you know, you run 10 people using it at the same time

00:11:25.000 --> 00:11:27.240
and that's where you start to notice problems.

00:11:27.240 --> 00:11:28.600
- Yeah.

00:11:28.600 --> 00:11:30.840
I've more than once considered putting

00:11:30.840 --> 00:11:34.200
some kind of LLM AI augment, you know,

00:11:34.200 --> 00:11:36.880
rag sort of thing in front of Talk Python

00:11:36.880 --> 00:11:39.440
because God, at this point,

00:11:39.440 --> 00:11:43.240
eight, nine years of full transcripts,

00:11:43.240 --> 00:11:45.080
human corrected with all sorts of,

00:11:45.080 --> 00:11:47.720
you know, pretty highly accurate.

00:11:47.720 --> 00:11:48.840
So that would be awesome to have people

00:11:48.840 --> 00:11:51.040
have conversations about it, but I don't know,

00:11:51.040 --> 00:11:53.120
just the workload.

00:11:53.120 --> 00:11:54.920
I can see so many people coming to you and go,

00:11:54.920 --> 00:11:57.320
that's neat, I'm gonna ask it about my homework.

00:11:57.320 --> 00:11:58.480
It has nothing to do with it.

00:11:58.760 --> 00:12:03.120
Just using it as like a free AI to just,

00:12:03.120 --> 00:12:04.640
instead of bothering to download something

00:12:04.640 --> 00:12:07.120
or use ChatGPT, I'll just use yours.

00:12:07.120 --> 00:12:10.520
And I'm like, eh, probably not worthwhile, all the trouble.

00:12:10.520 --> 00:12:12.280
- Yeah, you can set them up so that

00:12:12.280 --> 00:12:14.720
they only answer questions about your own stuff.

00:12:14.720 --> 00:12:19.720
They don't act as a fancy front end for ChatGPT.

00:12:19.720 --> 00:12:20.880
- Yeah.

00:12:20.880 --> 00:12:22.480
- We've got plenty of demos that are like that,

00:12:22.480 --> 00:12:25.000
but you give it your own internal documents

00:12:25.000 --> 00:12:26.920
and it only answers questions about those

00:12:26.920 --> 00:12:29.840
and you can set it up so that if it doesn't,

00:12:29.840 --> 00:12:32.920
if it can't figure out a reliable answer, it tells you.

00:12:32.920 --> 00:12:36.280
It says, I can't, rather than just making something up.

00:12:36.280 --> 00:12:40.120
- Yeah, that's really nice.

00:12:40.120 --> 00:12:44.080
Yeah, I forgot how closely you guys work with open AI

00:12:44.080 --> 00:12:45.160
and all that, right?

00:12:45.160 --> 00:12:46.360
- Yeah. - Yeah, yeah.

00:12:46.360 --> 00:12:49.680
- I mean, the compute in Azure to run

00:12:49.680 --> 00:12:52.800
and train all that stuff is, it's out of control.

00:12:52.800 --> 00:12:55.800
- Yeah, the GPUs alone are insane.

00:12:55.800 --> 00:12:59.520
- Yeah, it's, I talked to Mark Russinovich,

00:12:59.520 --> 00:13:03.040
bit of a diversion, but about some of the hardware

00:13:03.040 --> 00:13:04.200
and some of the data center stuff.

00:13:04.200 --> 00:13:08.120
And it's like, we're gonna create distributed GPUs

00:13:08.120 --> 00:13:11.000
that connect back over fiber to the actual hardware

00:13:11.000 --> 00:13:14.200
so you can scale your GPUs and CPUs independently.

00:13:14.200 --> 00:13:17.720
There's just all kinds of crazy, interesting stuff there.

00:13:17.720 --> 00:13:21.720
But that's a side, bit of a side note, bit of a side note.

00:13:23.280 --> 00:13:24.880
All right, so let's talk about,

00:13:24.880 --> 00:13:29.600
basically, what do you wanna do to design a load test?

00:13:29.600 --> 00:13:34.600
Like, it's more than just, let me write a loop

00:13:34.600 --> 00:13:37.320
and see how many requests I can make,

00:13:37.320 --> 00:13:41.080
while true, use requests and call this

00:13:41.080 --> 00:13:43.560
or call this endpoint or something, right?

00:13:43.560 --> 00:13:46.880
- Yeah, yeah, so there's, I guess,

00:13:46.880 --> 00:13:48.520
what I normally highlight with load testing

00:13:48.520 --> 00:13:51.960
is that the wrong way to start

00:13:51.960 --> 00:13:56.200
is to try and work at how many requests per second

00:13:56.200 --> 00:13:58.240
your server can handle.

00:13:58.240 --> 00:14:02.240
Like, that's interesting for throughput,

00:14:02.240 --> 00:14:06.480
but it's not a good measure of real user traffic.

00:14:06.480 --> 00:14:12.560
And the main reason is that every user is unique.

00:14:12.560 --> 00:14:15.080
They do different things, they follow different paths,

00:14:15.080 --> 00:14:17.440
they put in different data.

00:14:17.960 --> 00:14:21.720
And also, when you're running a benchmark,

00:14:21.720 --> 00:14:25.880
then it's not waiting between requests,

00:14:25.880 --> 00:14:28.720
it's just trying to hammer as much as it can.

00:14:28.720 --> 00:14:34.720
So real users pause and read and click and wait,

00:14:34.720 --> 00:14:36.320
and there's normally latency.

00:14:36.320 --> 00:14:44.680
I can see your mouse moving, Mike, but I can't hear you.

00:14:46.320 --> 00:14:51.320
- Sorry, so they don't request the same page

00:14:51.320 --> 00:14:55.520
over and over and over, right?

00:14:55.520 --> 00:14:58.040
They request a blend of pages.

00:14:58.040 --> 00:14:59.880
Either they go to the homepage, then they do a search,

00:14:59.880 --> 00:15:01.560
and they explore what they find in the search,

00:15:01.560 --> 00:15:02.880
and they go back and do another search,

00:15:02.880 --> 00:15:05.800
and then they go to checkout or whatever, right?

00:15:05.800 --> 00:15:06.960
And as you're pointing out,

00:15:06.960 --> 00:15:09.640
they don't just hold down Control or Command + R

00:15:09.640 --> 00:15:12.680
and just flicker the screen as hard as they can.

00:15:12.680 --> 00:15:15.160
They're reading and they're interacting.

00:15:15.160 --> 00:15:17.560
And so what you're saying is,

00:15:17.560 --> 00:15:21.120
if you really wanna say how many actual users,

00:15:21.120 --> 00:15:23.600
not just kind of a throughput number,

00:15:23.600 --> 00:15:28.440
but how many people using the app do you possibly expect

00:15:28.440 --> 00:15:30.200
it could keep working,

00:15:30.200 --> 00:15:32.960
you gotta factor all these things in, right?

00:15:32.960 --> 00:15:36.640
- Yeah, so you've got to factor in the randomness

00:15:36.640 --> 00:15:39.320
of where they go, what they type in,

00:15:39.320 --> 00:15:43.960
the waits between each click or each API request.

00:15:44.400 --> 00:15:47.240
And then something else that's really important

00:15:47.240 --> 00:15:50.760
is that most modern applications,

00:15:50.760 --> 00:15:53.320
it's not just the initial HTTP request,

00:15:53.320 --> 00:15:58.320
it's the 95 additional requests to all the scripts

00:15:58.320 --> 00:16:01.840
and the resources and the AJAX calls

00:16:01.840 --> 00:16:02.680
and everything like that.

00:16:02.680 --> 00:16:05.760
So if it's a browser-based application,

00:16:05.760 --> 00:16:09.400
then typically, it's not just the initial request,

00:16:09.400 --> 00:16:11.320
it's everything that happens after it.

00:16:11.320 --> 00:16:12.800
And I've definitely seen times

00:16:12.800 --> 00:16:15.080
where people have done a load test and said,

00:16:15.080 --> 00:16:17.120
"Oh yeah, the website runs great."

00:16:17.120 --> 00:16:21.640
And then in their app, they had like a JavaScript poller

00:16:21.640 --> 00:16:24.640
that would like refresh the data or something every minute.

00:16:24.640 --> 00:16:28.120
And then they didn't test that.

00:16:28.120 --> 00:16:30.800
And then you get thousands of users

00:16:30.800 --> 00:16:33.120
who leave the tab in their browser,

00:16:33.120 --> 00:16:35.160
and it's just polling in the background.

00:16:35.160 --> 00:16:38.160
So you've basically got this continuous stream of traffic

00:16:38.160 --> 00:16:42.440
to a special API that does like polling.

00:16:42.440 --> 00:16:43.560
And they hadn't load tested that,

00:16:43.560 --> 00:16:45.800
and that produced a huge spike in load

00:16:45.800 --> 00:16:47.360
and that caused issues.

00:16:47.360 --> 00:16:50.920
- It's like 75% of the workload is the polling.

00:16:50.920 --> 00:16:53.480
- Yeah, yeah, exactly.

00:16:53.480 --> 00:16:56.120
- What's your browser tab story?

00:16:56.120 --> 00:16:58.960
Are you a person that just has tons and tons of tabs open?

00:16:58.960 --> 00:17:00.920
- I don't know.

00:17:00.920 --> 00:17:03.080
I try and clean them up as much as possible

00:17:03.080 --> 00:17:07.000
and have like a couple open maybe.

00:17:07.000 --> 00:17:09.520
Unless I'm researching something,

00:17:09.520 --> 00:17:12.320
and then when I'm finished, I just do close all tabs

00:17:12.320 --> 00:17:15.000
and then start again.

00:17:15.000 --> 00:17:15.840
- Yeah, that's me as well.

00:17:15.840 --> 00:17:17.880
I think there's a good number of people

00:17:17.880 --> 00:17:21.600
that just have like 50 tabs open, they just leave them.

00:17:21.600 --> 00:17:23.400
And think about what that does for your website

00:17:23.400 --> 00:17:27.560
if you've got some kind of timer-based deal, right?

00:17:27.560 --> 00:17:30.840
You gotta consider those people that have just left it.

00:17:30.840 --> 00:17:31.680
- Yeah.

00:17:31.680 --> 00:17:35.520
- Yeah, yeah, so you gotta take those things into account,

00:17:35.520 --> 00:17:37.120
I suppose, yeah.

00:17:37.120 --> 00:17:37.960
- Yeah.

00:17:38.640 --> 00:17:43.640
- You did mention the CSS and the,

00:17:43.640 --> 00:17:45.760
talked about the AJAX,

00:17:45.760 --> 00:17:49.520
but I think also things like CSS, images,

00:17:49.520 --> 00:17:51.560
JavaScript files, not JavaScript execution,

00:17:51.560 --> 00:17:53.120
but just getting the files, right?

00:17:53.120 --> 00:17:55.640
Some of these frameworks are a couple hundred megs.

00:17:55.640 --> 00:17:59.400
Like if you're not somehow distributing that through a CDN,

00:17:59.400 --> 00:18:02.640
that could be worth considering as well, I think.

00:18:02.640 --> 00:18:03.840
- Yeah, it is, definitely.

00:18:03.840 --> 00:18:08.560
And so in Python, for example, you've got,

00:18:08.560 --> 00:18:13.080
in Django, there's an extension called White Noise.

00:18:13.080 --> 00:18:17.760
So in Django, you've got static content.

00:18:17.760 --> 00:18:20.240
There is a really easy extension you can install,

00:18:20.240 --> 00:18:22.240
'cause normally you have to kind of configure static

00:18:22.240 --> 00:18:25.480
and say, okay, my static files are here,

00:18:25.480 --> 00:18:27.400
and then you have to set up Nginx

00:18:27.400 --> 00:18:28.880
or whichever web server you're doing

00:18:28.880 --> 00:18:32.200
to serve that static content directly.

00:18:32.200 --> 00:18:33.840
So as a bit of a workaround,

00:18:33.840 --> 00:18:37.560
people often install this White Noise extension

00:18:37.560 --> 00:18:41.960
that basically uses Python as their static server.

00:18:41.960 --> 00:18:45.440
So like every time you request a CSS or whatever,

00:18:45.440 --> 00:18:47.880
Python actually is the thing that reads it from disks

00:18:47.880 --> 00:18:51.920
and serves it back, which is great for development,

00:18:51.920 --> 00:18:53.720
but you should never use that in production

00:18:53.720 --> 00:18:56.640
because Python is not a very good CDN.

00:18:56.640 --> 00:19:01.800
So yeah, that's kind of, with load testing,

00:19:01.800 --> 00:19:03.680
you would just test the endpoint and say,

00:19:03.680 --> 00:19:04.640
oh yeah, it works great,

00:19:04.640 --> 00:19:06.640
but then you actually run it in a browser.

00:19:06.640 --> 00:19:08.320
And if you're using something like White Noise,

00:19:08.320 --> 00:19:12.360
it's actually creating 10 times, 20 times more load

00:19:12.360 --> 00:19:15.880
than you expected because it's pulling in all this CSS

00:19:15.880 --> 00:19:19.000
and JavaScript and images and stuff like that.

00:19:19.000 --> 00:19:20.560
- Yeah, it can be out of control.

00:19:20.560 --> 00:19:24.200
I ran into an issue with my website where,

00:19:24.200 --> 00:19:29.120
I don't know, I got tired of being too careful

00:19:29.120 --> 00:19:32.800
with font awesome fonts and I had missed one.

00:19:32.800 --> 00:19:34.160
Like, I'll just put the whole thing,

00:19:34.160 --> 00:19:37.040
I'll just put the whole CSS file in.

00:19:37.040 --> 00:19:38.040
I'm sure it'll be fine.

00:19:38.040 --> 00:19:38.880
It wasn't fine.

00:19:38.880 --> 00:19:43.120
Like the web app is running at like 10% CBO usage

00:19:43.120 --> 00:19:46.120
and Nginx trying to serve up all the JavaScript is at like 80.

00:19:46.120 --> 00:19:47.440
I'm like, what is it doing?

00:19:47.440 --> 00:19:52.440
Like, oh, it was serving up a megabyte sized font stuff,

00:19:52.440 --> 00:19:54.800
every request, this is not good.

00:19:54.800 --> 00:19:57.080
- Yeah.

00:19:57.080 --> 00:20:00.760
So yeah, in terms of projects that I've worked on

00:20:00.760 --> 00:20:05.680
in the past, I've worked on load testing some like big

00:20:05.680 --> 00:20:07.360
campaigns and stuff like that,

00:20:07.360 --> 00:20:12.840
particularly around sports events, awesome television.

00:20:12.840 --> 00:20:16.480
Back when people used to watch live television

00:20:16.480 --> 00:20:17.880
instead of streaming at all,

00:20:17.880 --> 00:20:21.000
they'd have like a murder mystery or something

00:20:21.000 --> 00:20:22.240
on a big soap opera.

00:20:22.240 --> 00:20:26.880
And so I'd like load test the application

00:20:26.880 --> 00:20:30.080
so that when everyone wants to find out who murdered

00:20:30.080 --> 00:20:31.960
the Vickers wife or whatever,

00:20:31.960 --> 00:20:34.960
then they'd all kind of click on the website

00:20:34.960 --> 00:20:37.280
at the same time and just trying to load test it.

00:20:37.280 --> 00:20:40.000
So some of the things I'd seen were trying to make sure

00:20:40.000 --> 00:20:43.200
you simulate browser load correctly,

00:20:43.200 --> 00:20:47.440
trying to distribute traffic in a realistic way,

00:20:47.440 --> 00:20:49.680
and then spikes in traffic

00:20:49.680 --> 00:20:51.520
are basically a different problem.

00:20:51.520 --> 00:20:54.920
So the thing we talked about, like the halftime problem

00:20:54.920 --> 00:20:56.680
where you've got everybody turning on the kettle

00:20:56.680 --> 00:21:01.280
at the same time, that's a special type of load test.

00:21:01.280 --> 00:21:04.040
- It's a predictable spike as opposed to just

00:21:04.040 --> 00:21:05.920
an out of the blue spike, right?

00:21:05.920 --> 00:21:08.840
- Yeah, that one is predictable, which is really nice.

00:21:08.840 --> 00:21:11.080
And then you get things like seasonal traffic.

00:21:11.080 --> 00:21:15.400
So for a lot of e-commerce applications and stuff like that,

00:21:15.400 --> 00:21:17.640
you would expect in the lead up to Christmas

00:21:17.640 --> 00:21:21.320
or in the Black Friday sale in the US, for example,

00:21:21.320 --> 00:21:24.720
like Cyber Monday, you'd expect like a spike in traffic

00:21:24.720 --> 00:21:28.200
for those, but it would be distributed over a day or two.

00:21:28.200 --> 00:21:33.280
So you want to be able to like properly assess those.

00:21:33.280 --> 00:21:35.960
- Yeah, another spike that I think you and I

00:21:35.960 --> 00:21:40.280
can both relate to is the driver of the day, Formula One.

00:21:40.280 --> 00:21:41.320
- Oh, right, yeah.

00:21:41.320 --> 00:21:43.560
- You've got 10 laps left within the race.

00:21:43.560 --> 00:21:48.280
Everyone now go to this thing and press this button.

00:21:48.280 --> 00:21:50.280
You have five minutes to do it or something, right?

00:21:50.280 --> 00:21:54.120
Like that's gonna be a mega spike.

00:21:54.120 --> 00:21:58.080
- Yeah, yeah, there can be some big, big lows in traffic.

00:21:58.080 --> 00:22:01.800
So yeah, sudden spikes are really hard to handle

00:22:01.800 --> 00:22:04.720
because you get bottlenecks in the network.

00:22:04.720 --> 00:22:06.360
You get bottlenecks in the database.

00:22:06.360 --> 00:22:08.440
You get bottlenecks in the web server.

00:22:08.440 --> 00:22:12.240
So those are kind of a unique type of problem to test for.

00:22:12.240 --> 00:22:15.920
If you're looking more at like waves of traffic,

00:22:15.920 --> 00:22:18.600
so like, oh, you know, traffic would build up

00:22:18.600 --> 00:22:21.680
during the weekday or the type of application I have

00:22:21.680 --> 00:22:24.440
actually gets busier at the weekend or in the evenings.

00:22:24.440 --> 00:22:29.360
Then those are kind of the ones where you test ramped traffic.

00:22:29.360 --> 00:22:32.400
So all these load testing tools have a ramp up time

00:22:32.400 --> 00:22:36.720
or ramp configuration where, so in Locust, for example,

00:22:36.720 --> 00:22:40.160
you say, how many concurrent users do you want to get to?

00:22:40.160 --> 00:22:43.600
So let's say that's like a thousand or something.

00:22:43.600 --> 00:22:46.680
And then how much do you want to ramp up?

00:22:46.680 --> 00:22:49.120
And you generally should always use a ramp

00:22:49.120 --> 00:22:53.680
because if you've got a thousand users,

00:22:53.680 --> 00:22:56.840
unless you've put them all in a room and said,

00:22:56.840 --> 00:23:00.000
okay, everybody click on the button at the same time,

00:23:00.000 --> 00:23:01.520
like that's not gonna happen.

00:23:01.520 --> 00:23:04.680
People log in gradually over a period of time.

00:23:04.680 --> 00:23:08.560
And if you don't use ramping and load testing tools,

00:23:08.560 --> 00:23:13.480
you actually create, you kind of end up simulating stuff

00:23:13.480 --> 00:23:14.560
that's not realistic.

00:23:14.560 --> 00:23:18.320
So you'd see like a massive spike in response times

00:23:18.320 --> 00:23:20.520
as the application gets backed up.

00:23:20.520 --> 00:23:23.040
But if it's not really realistic

00:23:23.040 --> 00:23:24.480
that you'd get those types of spikes,

00:23:24.480 --> 00:23:26.240
then don't simulate it that way.

00:23:26.240 --> 00:23:30.440
- Yeah, there's, we're gonna talk about it a little bit,

00:23:30.440 --> 00:23:33.760
but there's a lot of different layers of caching as well.

00:23:33.760 --> 00:23:37.000
And if the app has just come to life,

00:23:37.000 --> 00:23:39.840
many of those layers of caching are not set up

00:23:39.840 --> 00:23:41.560
and like warmed up.

00:23:41.560 --> 00:23:43.720
So that's a big deal.

00:23:43.720 --> 00:23:46.920
And it could be as simple as the web app parsing,

00:23:46.920 --> 00:23:51.520
the Jinja or Chameleon or Django template, right?

00:23:51.520 --> 00:23:53.240
That's the very first time that can be slow,

00:23:53.240 --> 00:23:56.640
but then if you set it up right in production,

00:23:56.640 --> 00:24:01.280
it will not parse at the second, third and fourth request.

00:24:01.280 --> 00:24:02.120
- Yeah, exactly.

00:24:02.120 --> 00:24:04.320
And so if we talk about Locust as an example,

00:24:04.320 --> 00:24:06.440
you've got on the screen,

00:24:06.440 --> 00:24:07.880
I think it's my favorite load testing.

00:24:07.880 --> 00:24:09.760
- Locust is so good.

00:24:09.760 --> 00:24:11.600
- Yeah, it's really flexible

00:24:11.600 --> 00:24:16.600
and you define the user flows in Python code.

00:24:16.720 --> 00:24:19.680
So you write a class that represents a user

00:24:19.680 --> 00:24:22.400
and then you program in the steps that they would take.

00:24:22.400 --> 00:24:24.880
So like, oh, they start on the homepage,

00:24:24.880 --> 00:24:26.640
then they click on this page,

00:24:26.640 --> 00:24:28.280
then they click on this page.

00:24:28.280 --> 00:24:31.080
And then you can program in like the pauses

00:24:31.080 --> 00:24:33.520
and the randomness and stuff like that.

00:24:33.520 --> 00:24:36.360
And actually set up different types of personas

00:24:36.360 --> 00:24:37.720
or different types of users.

00:24:37.720 --> 00:24:41.320
I think this is a great way of designing a load test

00:24:41.320 --> 00:24:44.240
because you're thinking about what would the user do

00:24:44.240 --> 00:24:46.240
rather than like, what's the throughput.

00:24:47.000 --> 00:24:50.200
So if you set up a Locust test where,

00:24:50.200 --> 00:24:52.440
they start off at a point in your site,

00:24:52.440 --> 00:24:55.800
if your site has a login,

00:24:55.800 --> 00:25:00.680
how many of your users would log in?

00:25:00.680 --> 00:25:03.080
Because I think it's important to start off,

00:25:03.080 --> 00:25:06.080
if this is a new application, it's really hard to know.

00:25:06.080 --> 00:25:09.320
So you're gonna have to come up with an educated guess

00:25:09.320 --> 00:25:12.520
or test like a range of parameters.

00:25:12.520 --> 00:25:15.440
If this isn't a website that you've had running for,

00:25:15.440 --> 00:25:18.480
a year or two, and you've got like a history of traffic,

00:25:18.480 --> 00:25:19.880
then you can look at it and say, okay,

00:25:19.880 --> 00:25:23.080
how many people on the homepage are logged in?

00:25:23.080 --> 00:25:24.560
Because there's a big difference.

00:25:24.560 --> 00:25:26.440
Because when you talk about caching

00:25:26.440 --> 00:25:28.480
and like CDNs and stuff like that,

00:25:28.480 --> 00:25:31.840
you can cache the rendered template,

00:25:31.840 --> 00:25:36.720
but you generally wouldn't cache the whole rendered template

00:25:36.720 --> 00:25:37.840
if they're logged in.

00:25:37.840 --> 00:25:42.680
Because at the top, it might say, hello, Anthony,

00:25:42.680 --> 00:25:43.880
and you wouldn't wanna cache that

00:25:43.880 --> 00:25:45.560
so that when Mike clicks on the website,

00:25:45.560 --> 00:25:46.480
it says, hello, Anthony.

00:25:46.480 --> 00:25:47.320
He's like, who the hell's that?

00:25:47.320 --> 00:25:49.080
- That would be very disturbing, wouldn't it?

00:25:49.080 --> 00:25:50.800
(laughing)

00:25:50.800 --> 00:25:54.160
- So there's a balance between caching

00:25:54.160 --> 00:25:55.840
and serving up dynamic content.

00:25:55.840 --> 00:25:58.400
And then also if you're presenting lists of products

00:25:58.400 --> 00:26:00.720
or someone's got a shopping cart or something like that,

00:26:00.720 --> 00:26:02.440
obviously that's unique to them.

00:26:02.440 --> 00:26:06.240
So you're kind of trying to simulate this

00:26:06.240 --> 00:26:07.360
as best as possible.

00:26:07.360 --> 00:26:10.160
Otherwise, you just create these really optimistic load tests

00:26:10.160 --> 00:26:12.800
where you're like, oh, we tested the homepage

00:26:12.800 --> 00:26:15.480
and it can handle 50,000 users.

00:26:15.480 --> 00:26:18.560
But in practice, if 1,000 of those log in,

00:26:18.560 --> 00:26:19.880
then the whole thing falls apart

00:26:19.880 --> 00:26:21.360
because all of a sudden you're not using

00:26:21.360 --> 00:26:23.400
the cached version of the homepage.

00:26:23.400 --> 00:26:24.760
- Right.

00:26:24.760 --> 00:26:27.320
You know, another example that you brought up is,

00:26:27.320 --> 00:26:28.720
let's just take Talk Python, right?

00:26:28.720 --> 00:26:31.760
It's got, coming up on 500 episodes.

00:26:31.760 --> 00:26:35.480
If you test, and suppose I put really aggressive caching,

00:26:35.480 --> 00:26:37.600
like on the database results that come back

00:26:37.600 --> 00:26:40.760
from a single episode for the page,

00:26:40.760 --> 00:26:43.360
and then just render that out of memory, basically,

00:26:43.360 --> 00:26:46.600
at least just out of the objects that are in memory.

00:26:46.600 --> 00:26:48.760
If you do your test to just hit the one,

00:26:48.760 --> 00:26:50.760
like let's just randomly pick episode 300,

00:26:50.760 --> 00:26:52.200
hit that one a bunch of times.

00:26:52.200 --> 00:26:56.360
But in reality, people are hitting all four or 500

00:26:56.360 --> 00:26:59.120
and it's not all cached the same,

00:26:59.120 --> 00:27:01.680
or maybe there's so much memory that each one has to hold

00:27:01.680 --> 00:27:03.720
that it like, putting all 500 in memory

00:27:03.720 --> 00:27:06.480
and the cache runs it out of RAM on the server,

00:27:06.480 --> 00:27:08.760
all sorts of stuff that happens

00:27:08.760 --> 00:27:10.960
as you get this kind of dynamic mix, right?

00:27:10.960 --> 00:27:15.720
- Yeah, so you want to introduce randomness.

00:27:15.720 --> 00:27:20.000
So for example, on the login, if you've got a login flow,

00:27:20.000 --> 00:27:22.300
or if you're clicking on a particular product,

00:27:22.300 --> 00:27:27.800
then try not to hard code, you know,

00:27:27.800 --> 00:27:30.360
which one it is, if you've got caching.

00:27:30.360 --> 00:27:32.760
You might not even see the caching,

00:27:32.760 --> 00:27:35.600
or they're like databases do a lot of caching.

00:27:35.600 --> 00:27:38.560
So like, if you're always pulling the same thing

00:27:38.560 --> 00:27:40.320
from the database, the database server

00:27:40.320 --> 00:27:41.760
is probably gonna cache that.

00:27:41.760 --> 00:27:45.240
So you know, it's gonna be a lot faster.

00:27:45.240 --> 00:27:49.760
So if you can randomize the, like the inputs

00:27:49.760 --> 00:27:51.520
or the pages that you click on,

00:27:51.520 --> 00:27:54.440
or the flows that people take, then that's great.

00:27:54.440 --> 00:27:56.720
Locust actually has like an extra,

00:27:56.720 --> 00:28:00.520
when you define a task, which is a decorator on a function,

00:28:00.520 --> 00:28:03.760
you can have like, how often this happens.

00:28:03.760 --> 00:28:05.840
So you can have some tasks which happen

00:28:05.840 --> 00:28:07.880
more sort of frequently than others.

00:28:08.280 --> 00:28:11.720
So you can say, okay, you know, five times more people

00:28:11.720 --> 00:28:14.400
go to the homepage, and then, you know,

00:28:14.400 --> 00:28:16.560
every so often somebody does a search.

00:28:16.560 --> 00:28:19.120
But then when you do the search,

00:28:19.120 --> 00:28:21.160
when you wanna load test the search,

00:28:21.160 --> 00:28:23.440
you know, you wanna randomize that a bit more

00:28:23.440 --> 00:28:25.960
than just always searching for the same thing.

00:28:25.960 --> 00:28:27.720
- Right, right, right, right.

00:28:27.720 --> 00:28:30.720
Yeah, and just, so for people who haven't seen Locust,

00:28:30.720 --> 00:28:34.200
you create a class, you give it a couple of functions,

00:28:34.200 --> 00:28:36.400
and then, for example, to test the homepage,

00:28:36.400 --> 00:28:39.280
you just literally put the task decorator on the function.

00:28:39.280 --> 00:28:43.360
You say self.client.get/self.client.get,

00:28:43.360 --> 00:28:46.040
maybe some additional assets or not.

00:28:46.040 --> 00:28:48.400
And then you can even hard code into the class

00:28:48.400 --> 00:28:51.920
what the domain is, or local host, or support, or whatever.

00:28:51.920 --> 00:28:53.760
And that's it.

00:28:53.760 --> 00:28:56.120
And you just, you can assign a weight to these tasks,

00:28:56.120 --> 00:28:58.360
like homepage five times more likely than about.

00:28:58.360 --> 00:29:01.880
So just put task of five instead of task by itself, right?

00:29:01.880 --> 00:29:02.880
It's incredible.

00:29:04.160 --> 00:29:06.320
- Yeah, yeah, it's really, really helpful.

00:29:06.320 --> 00:29:09.040
And then there's a library.

00:29:09.040 --> 00:29:12.320
There's a couple of things you need to keep in mind as well.

00:29:12.320 --> 00:29:15.320
Like if the user logs in,

00:29:15.320 --> 00:29:18.160
and then it probably creates some sort of session,

00:29:18.160 --> 00:29:20.880
whether that's like a cookie, or a token,

00:29:20.880 --> 00:29:22.680
or something like that.

00:29:22.680 --> 00:29:25.360
So you need to store that somewhere in the class

00:29:25.360 --> 00:29:28.940
so that subsequent requests use the same login session.

00:29:30.480 --> 00:29:35.480
And then also, you know, frameworks like Django and Flask

00:29:35.480 --> 00:29:40.800
have got cross-site request forgery, like protection.

00:29:40.800 --> 00:29:44.400
So they generate these CSRF tokens in the forms as well.

00:29:44.400 --> 00:29:47.600
So there's normally like a bit involved

00:29:47.600 --> 00:29:50.480
in getting the cookie or the session ID,

00:29:50.480 --> 00:29:52.120
getting the CSRF value.

00:29:52.120 --> 00:29:54.400
And then like, say if you're submitting forms,

00:29:54.400 --> 00:29:56.960
or you're like doing a search or something,

00:29:57.600 --> 00:30:01.600
you need to code a bit in Locust

00:30:01.600 --> 00:30:03.920
to work around the security controls.

00:30:03.920 --> 00:30:07.640
- Right, for example, you might do a request to the page

00:30:07.640 --> 00:30:11.760
that has the form, and then pull back the CRF token

00:30:11.760 --> 00:30:14.440
from a cookie, and then use that

00:30:14.440 --> 00:30:16.960
as part of the form submission data.

00:30:16.960 --> 00:30:19.660
Otherwise it might just say invalid.

00:30:19.660 --> 00:30:23.880
You know, another one that would be really tricky

00:30:23.880 --> 00:30:27.120
would be like turnstile or recaptcha.

00:30:27.120 --> 00:30:30.240
You're probably basically not getting that.

00:30:30.240 --> 00:30:31.080
Not worth it.

00:30:31.080 --> 00:30:33.440
- Yeah, you're stuck there.

00:30:33.440 --> 00:30:34.680
You've got to fill in those ones.

00:30:34.680 --> 00:30:37.160
- Just maybe turn it off real quick for your test

00:30:37.160 --> 00:30:38.120
and then turn it back on.

00:30:38.120 --> 00:30:39.160
I don't know, I mean, you could do it,

00:30:39.160 --> 00:30:41.780
turn it off in development or something like that, right?

00:30:41.780 --> 00:30:42.620
- Yeah.

00:30:42.620 --> 00:30:46.840
- So when I look at this, this Locust stuff,

00:30:46.840 --> 00:30:53.560
I see behind the scenes something like requests or HTTPX

00:30:53.840 --> 00:30:57.600
where all it does is pull back the string of the HTML,

00:30:57.600 --> 00:31:00.000
or maybe even it just gets the head

00:31:00.000 --> 00:31:01.640
and actually throws away the content,

00:31:01.640 --> 00:31:02.640
I bet it streams it back.

00:31:02.640 --> 00:31:06.300
But what I don't imagine it does

00:31:06.300 --> 00:31:08.840
is it doesn't parse the HTML,

00:31:08.840 --> 00:31:11.160
realize that it's a view front end,

00:31:11.160 --> 00:31:15.200
execute the JavaScript that then has three API calls

00:31:15.200 --> 00:31:16.960
to more stuff on the back on the server, right?

00:31:16.960 --> 00:31:19.440
It doesn't, if it's a rich front end app,

00:31:19.440 --> 00:31:22.600
it probably doesn't treat it the same

00:31:22.600 --> 00:31:25.080
if we write it just like this, right?

00:31:25.080 --> 00:31:27.240
- Yeah, so the other one I wanted to highlight

00:31:27.240 --> 00:31:31.160
is it's an extension to Locust,

00:31:31.160 --> 00:31:33.520
so you can connect it with Playwright.

00:31:33.520 --> 00:31:36.440
It does HTML parsing.

00:31:36.440 --> 00:31:40.680
That's more like looking at the HTML

00:31:40.680 --> 00:31:42.760
to see if like a particular,

00:31:42.760 --> 00:31:47.320
you know, like a beautiful soup or responses

00:31:47.320 --> 00:31:49.080
was the framework for this.

00:31:49.080 --> 00:31:51.640
- Right, make sure timeout does not appear in the text

00:31:51.640 --> 00:31:52.480
or something like that.

00:31:52.480 --> 00:31:54.240
- Yeah, make sure the page doesn't have error

00:31:54.240 --> 00:31:55.200
in big letters.

00:31:55.200 --> 00:31:58.000
So like that's one thing you can do

00:31:58.000 --> 00:31:59.600
is checking that the content of the page

00:31:59.600 --> 00:32:01.300
actually contains the right thing.

00:32:01.300 --> 00:32:07.120
And then Playwright is a UI,

00:32:07.120 --> 00:32:08.980
like a web testing tool.

00:32:08.980 --> 00:32:12.920
Playwright works really well with pytest.

00:32:12.920 --> 00:32:14.720
So I recommend using Playwright anyway

00:32:14.720 --> 00:32:17.200
if you're running a web application

00:32:18.480 --> 00:32:23.480
because you can write pytest tests in Playwright.

00:32:23.480 --> 00:32:27.200
Even better is if you wanna get started with Playwright,

00:32:27.200 --> 00:32:29.420
it has a code generator.

00:32:29.420 --> 00:32:30.600
So when you pip install it,

00:32:30.600 --> 00:32:33.160
you can run it in the code gen mode.

00:32:33.160 --> 00:32:34.920
It pops up a browser,

00:32:34.920 --> 00:32:36.280
and then you just go on your website

00:32:36.280 --> 00:32:37.680
and just click around

00:32:37.680 --> 00:32:39.320
and then do what you would do normally,

00:32:39.320 --> 00:32:41.240
type in, you know, fill in the forms,

00:32:41.240 --> 00:32:42.760
click on the buttons.

00:32:42.760 --> 00:32:44.320
And then whilst you're doing that,

00:32:44.320 --> 00:32:46.120
in another tab, it actually generates

00:32:46.120 --> 00:32:48.480
all the Python code for the pytests.

00:32:48.480 --> 00:32:54.080
So it basically generates the pytest test code

00:32:54.080 --> 00:32:56.120
in a separate window automatically

00:32:56.120 --> 00:32:57.640
as you're clicking around in the browser.

00:32:57.640 --> 00:33:00.840
So like in terms of writing UI tests,

00:33:00.840 --> 00:33:01.720
it's quite difficult

00:33:01.720 --> 00:33:02.760
'cause often you have to be like,

00:33:02.760 --> 00:33:04.840
okay, how do I find the button?

00:33:04.840 --> 00:33:06.680
How do I click on the right button?

00:33:06.680 --> 00:33:08.400
How do I find the form?

00:33:08.400 --> 00:33:11.000
Especially with JavaScript 'cause it's not,

00:33:11.000 --> 00:33:13.480
you know, often things don't have a specific idea

00:33:13.480 --> 00:33:15.880
or you've got to figure out what selectors to use

00:33:15.880 --> 00:33:16.840
and stuff like that.

00:33:16.840 --> 00:33:18.080
So this makes it a lot easier

00:33:18.080 --> 00:33:19.720
because you can use the code gen.

00:33:19.720 --> 00:33:23.120
So that's a browser test.

00:33:23.120 --> 00:33:25.120
So with a load test normally in Locust,

00:33:25.120 --> 00:33:27.040
you're just making HTTP requests,

00:33:27.040 --> 00:33:28.600
but you're not actually rendering the page

00:33:28.600 --> 00:33:32.200
or running the AJAX or the JavaScript code.

00:33:32.200 --> 00:33:34.080
Whereas with Playwright,

00:33:34.080 --> 00:33:35.000
when you run Playwright,

00:33:35.000 --> 00:33:36.520
it's actually spinning up a browser

00:33:36.520 --> 00:33:38.760
and then driving the browser from Python.

00:33:38.760 --> 00:33:42.720
So you can plug Locust and Playwright together.

00:33:42.720 --> 00:33:45.760
And there's a Playwright extension for Locust.

00:33:45.760 --> 00:33:49.520
So you can say, okay, each load test user,

00:33:49.520 --> 00:33:55.160
I actually want that to be a browser.

00:33:55.160 --> 00:33:56.200
And so when you say,

00:33:56.200 --> 00:33:58.720
I want to test 100 concurrent users,

00:33:58.720 --> 00:34:02.400
it actually spins up 100 browsers.

00:34:02.400 --> 00:34:04.640
(laughing)

00:34:04.640 --> 00:34:07.640
In a very, well, Playwright's actually really interesting

00:34:07.640 --> 00:34:10.960
how it works, but like there's a headless mode

00:34:10.960 --> 00:34:13.360
for browsers these days.

00:34:13.360 --> 00:34:15.400
So it doesn't actually run 100 windows.

00:34:15.400 --> 00:34:16.680
Like you can't see them.

00:34:16.680 --> 00:34:19.520
- I can't use my computer while this is running.

00:34:19.520 --> 00:34:21.240
It's just overwhelmed by.

00:34:21.240 --> 00:34:25.000
- But yeah, I don't recommend running 10,000 concurrent

00:34:25.000 --> 00:34:28.560
on a single laptop because it's gonna run pretty slowly.

00:34:28.560 --> 00:34:30.400
- It's gonna have a bad time, yeah.

00:34:30.400 --> 00:34:33.760
But that is really important to actually,

00:34:33.760 --> 00:34:34.800
so that will test it,

00:34:34.800 --> 00:34:36.280
all the stuff that we've been talking about.

00:34:36.280 --> 00:34:39.440
It'll test if there's polling, it'll test that.

00:34:39.440 --> 00:34:43.800
It'll pull, if the CSS is not delivered over a CDN,

00:34:43.800 --> 00:34:46.200
it's gonna go get that potentially white noise

00:34:46.200 --> 00:34:47.800
or wherever it's coming from.

00:34:47.800 --> 00:34:50.200
Right, it's gonna do all the things.

00:34:50.200 --> 00:34:51.040
- Yeah.

00:34:51.040 --> 00:34:54.600
Yeah, the challenge with it is that,

00:34:54.600 --> 00:34:56.520
like you mentioned, running a browser,

00:34:56.520 --> 00:35:00.240
even if it's just a tab, uses a lot of resources.

00:35:00.240 --> 00:35:03.600
Whereas making a HTTP request using a request or something,

00:35:03.600 --> 00:35:05.760
it doesn't really need anything.

00:35:05.760 --> 00:35:07.760
So you can quite happily in Locust,

00:35:07.760 --> 00:35:11.280
make a hundred thousand requests

00:35:11.280 --> 00:35:15.960
and your local machine that you're testing on will be fine.

00:35:15.960 --> 00:35:22.920
So yeah, you can actually get low testing as a service.

00:35:22.920 --> 00:35:26.560
And the reason you'd probably wanna use that

00:35:26.560 --> 00:35:28.720
is if you're testing a scenario

00:35:28.720 --> 00:35:32.480
where your local dev environment

00:35:32.480 --> 00:35:35.080
or your test environment just isn't big enough,

00:35:35.080 --> 00:35:36.720
where you need to distribute it,

00:35:36.720 --> 00:35:39.680
then you need more horsepower basically

00:35:40.160 --> 00:35:42.040
to go and run all the requests,

00:35:42.040 --> 00:35:44.480
especially for something like Playwright

00:35:44.480 --> 00:35:48.200
or where you need a lot of resources for the browser.

00:35:48.200 --> 00:35:51.400
- Yeah, yeah, the little screenshot they have for Locust

00:35:51.400 --> 00:35:55.400
says 21,400 users are currently accessing the website.

00:35:55.400 --> 00:35:57.960
It's like, that's a lot of browser instances.

00:35:57.960 --> 00:36:00.280
- That's a lot of browser instances.

00:36:00.280 --> 00:36:02.320
Yeah, that looks like an API test,

00:36:02.320 --> 00:36:04.440
which would be a lot easier to do.

00:36:04.440 --> 00:36:06.960
- Yeah, yeah, yeah, APIs are easy.

00:36:06.960 --> 00:36:11.960
- Yeah, so there's a service called Azure Load Testing.

00:36:11.960 --> 00:36:14.280
There is other options as well.

00:36:14.280 --> 00:36:18.640
I'm sure AWS has one and Google probably has one as well.

00:36:18.640 --> 00:36:19.760
Azure Load Testing I know,

00:36:19.760 --> 00:36:24.200
and we are launching Locust support for that.

00:36:24.200 --> 00:36:26.400
At the moment it supports Jmeter tests,

00:36:26.400 --> 00:36:30.240
but yeah, we're gonna be launching Locust support for that.

00:36:30.240 --> 00:36:32.240
I think by the time this episode comes out,

00:36:32.240 --> 00:36:34.320
it will probably be in public preview.

00:36:35.600 --> 00:36:38.600
So I've been using that and testing that.

00:36:38.600 --> 00:36:41.000
And the reason you would use it is, like I said,

00:36:41.000 --> 00:36:45.680
if you can run the Locust test locally on your machine

00:36:45.680 --> 00:36:48.160
and it runs great and brilliant,

00:36:48.160 --> 00:36:52.920
but you can ask us to spin up 50, 100

00:36:52.920 --> 00:36:55.640
or even more instances running in parallel.

00:36:55.640 --> 00:36:59.560
And so if you want to do a really large scale test,

00:36:59.560 --> 00:37:03.240
then you can just basically get that

00:37:03.240 --> 00:37:06.680
from a cloud service provider like Azure.

00:37:06.680 --> 00:37:07.840
- Yeah, yeah, awesome.

00:37:07.840 --> 00:37:09.440
I didn't realize you guys were bringing that online.

00:37:09.440 --> 00:37:10.360
That's cool.

00:37:10.360 --> 00:37:12.480
- Yeah, nobody knows about this.

00:37:12.480 --> 00:37:13.720
- Can you do distributed,

00:37:13.720 --> 00:37:15.520
I think you can do some distributed stuff

00:37:15.520 --> 00:37:17.200
with Locust even, can't you?

00:37:17.200 --> 00:37:18.160
Just, yeah.

00:37:18.160 --> 00:37:19.600
- Yeah, you can.

00:37:19.600 --> 00:37:21.480
- But you gotta have your own infrastructure, right?

00:37:21.480 --> 00:37:23.840
- Yeah, and a lot of it uses SSH

00:37:23.840 --> 00:37:27.800
and you've gotta, basically it kind of works over the shell.

00:37:27.800 --> 00:37:31.960
So yeah, we've got our own distribution

00:37:31.960 --> 00:37:34.520
and coordination system and stuff like that.

00:37:34.520 --> 00:37:37.800
And then also when you're running the test,

00:37:37.800 --> 00:37:45.000
your output, I guess, is what is the response time

00:37:45.000 --> 00:37:50.240
of the pages and what does that look like?

00:37:50.240 --> 00:37:52.200
And I think what confuses a lot of people

00:37:52.200 --> 00:37:55.200
is the first time they use Locust or JMeter

00:37:55.200 --> 00:37:56.720
or one of the other tools,

00:37:56.720 --> 00:37:58.480
they get all these percentiles back

00:37:58.480 --> 00:38:01.320
and they're like, oh, the 90th percentile response time

00:38:01.320 --> 00:38:04.120
is this and the 95th is this and the 99th is that

00:38:04.120 --> 00:38:07.120
and they're like, okay, well,

00:38:07.120 --> 00:38:08.720
if I go back to my high school math,

00:38:08.720 --> 00:38:10.640
I think I can remember what percentiles are,

00:38:10.640 --> 00:38:13.240
but which one matters?

00:38:13.240 --> 00:38:15.720
Because normally there's a massive difference

00:38:15.720 --> 00:38:19.940
between the 90th and the 99th percentiles with load testing.

00:38:19.940 --> 00:38:25.480
And you might say, oh, well, the 99th is like 10 seconds,

00:38:25.480 --> 00:38:29.480
but the 90th is like 300 milliseconds.

00:38:29.480 --> 00:38:32.440
So was that a good output or a bad output?

00:38:32.440 --> 00:38:34.920
I'm not really sure how to interpret the results.

00:38:34.920 --> 00:38:38.960
So yeah, you'll see in the UI for Locust,

00:38:38.960 --> 00:38:41.160
it gives you percentiles.

00:38:41.160 --> 00:38:43.440
All the other load testing tools are very similar.

00:38:43.440 --> 00:38:48.200
It's basically like a distribution of the response times

00:38:48.200 --> 00:38:50.020
for those particular pages.

00:38:50.020 --> 00:38:52.840
And what you're trying to understand

00:38:52.840 --> 00:38:57.320
is what is the expected response time?

00:38:57.320 --> 00:38:58.700
So if it's a bell curve,

00:38:58.700 --> 00:39:01.240
then what's the center point of that?

00:39:01.240 --> 00:39:04.240
Because the 99th is interesting,

00:39:04.240 --> 00:39:07.600
but often it's because the cache was warming up

00:39:07.600 --> 00:39:12.040
or there was one user that took 10 seconds.

00:39:12.040 --> 00:39:15.600
- The cache expired and then it got recreated right then

00:39:15.600 --> 00:39:17.000
or something, yeah.

00:39:17.000 --> 00:39:17.840
- Yeah, exactly.

00:39:17.840 --> 00:39:20.960
So for a 99th percentile, if it's 10 seconds,

00:39:20.960 --> 00:39:23.800
you might have one user that took 10 seconds

00:39:23.800 --> 00:39:27.760
and 99 users that took a couple of hundred milliseconds.

00:39:27.760 --> 00:39:32.760
So it's, do you want to factor for that one user

00:39:32.760 --> 00:39:36.260
or do you wanna focus on the other,

00:39:36.260 --> 00:39:38.100
on the bulk of the group?

00:39:38.100 --> 00:39:39.220
- Yeah, and if you're like my daughter,

00:39:39.220 --> 00:39:41.420
you'll just say that the internet's broken.

00:39:41.420 --> 00:39:42.260
(both laughing)

00:39:42.260 --> 00:39:43.420
- Yeah.

00:39:43.420 --> 00:39:46.060
- It could be, or it could be that YouTube is slow

00:39:46.060 --> 00:39:47.620
for some odd reason for you for a minute.

00:39:47.620 --> 00:39:49.540
Like it's not necessarily the entire internet

00:39:49.540 --> 00:39:50.880
that is the fault here.

00:39:50.880 --> 00:39:51.900
- Yeah, well, most users,

00:39:51.900 --> 00:39:54.500
I think if it took 10 seconds to respond,

00:39:54.500 --> 00:39:56.940
would just keep clicking the refresh button.

00:39:57.040 --> 00:39:58.920
(both laughing)

00:39:58.920 --> 00:40:00.240
- Exactly.

00:40:00.240 --> 00:40:02.720
- So just generate more load.

00:40:02.720 --> 00:40:05.000
Yeah, you might have to program that into your load test.

00:40:05.000 --> 00:40:06.600
If it takes longer than five seconds,

00:40:06.600 --> 00:40:09.320
then issue another three requests.

00:40:09.320 --> 00:40:11.760
- Yeah, that's a really good point actually.

00:40:11.760 --> 00:40:15.240
Let's see, are there any pictures of the reports here?

00:40:15.240 --> 00:40:16.080
Let's see.

00:40:16.080 --> 00:40:24.160
There we go, maybe.

00:40:24.160 --> 00:40:25.000
Yeah.

00:40:25.000 --> 00:40:26.360
- Yeah, you got these nice graphs

00:40:26.360 --> 00:40:28.680
that shows you response times.

00:40:28.680 --> 00:40:30.240
- Yeah.

00:40:30.240 --> 00:40:31.760
They're not just graphs, they're live graphs.

00:40:31.760 --> 00:40:35.520
You can kind of see it like flowing as it's testing, right?

00:40:35.520 --> 00:40:38.240
As it's ramping up or whatever.

00:40:38.240 --> 00:40:39.760
- Yeah, definitely.

00:40:39.760 --> 00:40:42.200
So what we do with the load testing services

00:40:42.200 --> 00:40:43.900
is we've got that graph,

00:40:43.900 --> 00:40:46.400
but then you can also say,

00:40:46.400 --> 00:40:50.160
I also want to see how many queries a second

00:40:50.160 --> 00:40:51.480
were happening on the database.

00:40:51.480 --> 00:40:54.520
What was the memory usage of the web applications?

00:40:54.520 --> 00:40:55.620
Like how many,

00:40:55.620 --> 00:41:00.000
like what was the pod size if you're using Kubernetes,

00:41:00.000 --> 00:41:01.400
like stuff like that.

00:41:01.400 --> 00:41:03.800
I've got a couple of demos where I've got

00:41:03.800 --> 00:41:07.340
like parameter-based load testing,

00:41:07.340 --> 00:41:09.520
where when you're setting up a Kubernetes environment,

00:41:09.520 --> 00:41:12.480
you're like, oh, I'm not sure how much memory

00:41:12.480 --> 00:41:14.000
I should allocate the containers

00:41:14.000 --> 00:41:17.200
or like how many of them I should have in a pod.

00:41:17.200 --> 00:41:20.880
So you can basically just use something like GitHub actions

00:41:20.880 --> 00:41:22.400
to give a matrix and say, okay,

00:41:22.400 --> 00:41:24.580
I want to test these configurations.

00:41:24.580 --> 00:41:27.880
Let's see what happens with half a gig of RAM per container

00:41:27.880 --> 00:41:29.840
or two gigs of RAM per container.

00:41:29.840 --> 00:41:33.040
Let's do two, four, eight in a cluster.

00:41:33.040 --> 00:41:36.720
And it will run the same load test for every configuration.

00:41:36.720 --> 00:41:39.180
And then you can just compare all the graphs and say,

00:41:39.180 --> 00:41:42.140
okay, we don't want to over-allocate infrastructure

00:41:42.140 --> 00:41:44.700
because people often go a bit nuts.

00:41:44.700 --> 00:41:46.880
They're like, oh, how many instances in a cluster

00:41:46.880 --> 00:41:47.960
do we need on the front end?

00:41:47.960 --> 00:41:50.280
And they allocate like 16.

00:41:51.520 --> 00:41:53.560
But then when it's actually running,

00:41:53.560 --> 00:41:55.000
15 of them are idle.

00:41:55.000 --> 00:41:57.920
So it's kind of just over-provisioning.

00:41:57.920 --> 00:42:01.440
So load testing can be a great way

00:42:01.440 --> 00:42:04.520
of not just like planning for spikes in traffic

00:42:04.520 --> 00:42:06.520
or being able to like cater for it,

00:42:06.520 --> 00:42:07.760
but actually the other way round,

00:42:07.760 --> 00:42:10.360
which is you run a load test and you realize

00:42:10.360 --> 00:42:13.920
you could probably actually do with less infrastructure.

00:42:13.920 --> 00:42:16.280
So like things like memory as well,

00:42:16.280 --> 00:42:19.340
like memory is expensive when you're buying it

00:42:19.340 --> 00:42:20.800
from a service provider.

00:42:21.760 --> 00:42:24.960
And, you know, CPUs and stuff like that,

00:42:24.960 --> 00:42:26.080
or the number of instances.

00:42:26.080 --> 00:42:28.000
And actually, let's see what happens

00:42:28.000 --> 00:42:30.000
if we turn some of those things down.

00:42:30.000 --> 00:42:33.040
Will that impact the response times?

00:42:33.040 --> 00:42:34.440
Can we get away with it basically?

00:42:34.440 --> 00:42:35.600
'Cause in a lot of cases,

00:42:35.600 --> 00:42:37.820
you can actually spend a lot less money

00:42:37.820 --> 00:42:39.160
and get the same performance

00:42:39.160 --> 00:42:41.160
or even just a negligible difference.

00:42:41.160 --> 00:42:44.920
- Or maybe you identify somewhere

00:42:44.920 --> 00:42:47.160
where you could add some level of caching.

00:42:47.160 --> 00:42:48.880
- Yeah.

00:42:48.880 --> 00:42:50.000
- Then all of a sudden you get,

00:42:50.000 --> 00:42:53.880
you can prove you get 10X the load per worker process.

00:42:53.880 --> 00:42:56.120
You know, we could have a smaller machine

00:42:56.120 --> 00:42:58.680
or smaller cluster or whatever.

00:42:58.680 --> 00:43:00.320
- Yeah, exactly.

00:43:00.320 --> 00:43:04.560
- So one thing I would like maybe to talk to the folks about

00:43:04.560 --> 00:43:08.240
is just how do you interpret these graphs?

00:43:08.240 --> 00:43:11.000
So you have like a request per second,

00:43:11.000 --> 00:43:14.000
and then you talked about say the 95th percentile

00:43:14.000 --> 00:43:16.560
response time and this ramping up.

00:43:16.560 --> 00:43:18.240
And usually when you look at these graphs,

00:43:18.240 --> 00:43:21.560
it's really obvious like, yeah, we can add more users,

00:43:21.560 --> 00:43:25.840
but here is where we start to suffer consequences

00:43:25.840 --> 00:43:28.000
if we add any more users than this.

00:43:28.000 --> 00:43:30.000
Doesn't always just completely fall over.

00:43:30.000 --> 00:43:33.440
It just behaves worse.

00:43:33.440 --> 00:43:38.240
- Yeah, so it's kind of a bit like a feedback system

00:43:38.240 --> 00:43:41.080
for anyone who studies that.

00:43:41.080 --> 00:43:45.480
When you configure a ramp in Locust,

00:43:45.480 --> 00:43:47.720
you're saying, okay, how many users per second

00:43:47.720 --> 00:43:48.560
do we wanna add?

00:43:48.560 --> 00:43:55.040
And you start off with a slow ramp is my suggestion.

00:43:55.040 --> 00:44:00.120
So like, you know, every 10 seconds add one user

00:44:00.120 --> 00:44:03.120
would like be a really slow way of doing it.

00:44:03.120 --> 00:44:07.200
And then what you're looking at is the response times

00:44:07.200 --> 00:44:12.200
for each page, or you can get an average of everything.

00:44:12.200 --> 00:44:14.920
So if you're starting off with a load test

00:44:14.920 --> 00:44:19.000
that just tests one page, you slowly ramp up the users.

00:44:19.000 --> 00:44:21.360
Then you're looking at the response time graph.

00:44:21.360 --> 00:44:25.640
Often when you run a load test where it starts,

00:44:25.640 --> 00:44:28.400
the response times spike up at the beginning

00:44:28.400 --> 00:44:32.200
because the service was probably asleep.

00:44:32.200 --> 00:44:34.560
The database server was probably asleep.

00:44:34.560 --> 00:44:36.840
You know, it needs to like kick a few things

00:44:36.840 --> 00:44:39.480
into action to get it responding.

00:44:39.480 --> 00:44:41.560
So you often see like a spike at the beginning.

00:44:41.560 --> 00:44:43.360
That's fine, don't worry about that.

00:44:44.240 --> 00:44:46.080
If as long as it's a short spike and not,

00:44:46.080 --> 00:44:47.440
it doesn't go on for hours.

00:44:47.440 --> 00:44:49.960
But we know once everything's warmed up,

00:44:49.960 --> 00:44:53.520
then you should see a stable graph.

00:44:53.520 --> 00:44:57.360
So the response times should stick around the same level,

00:44:57.360 --> 00:45:00.680
even as you add more and more users to a point.

00:45:00.680 --> 00:45:03.240
So it's basically looking at the two graphs,

00:45:03.240 --> 00:45:06.560
which is the response time and the number of users

00:45:06.560 --> 00:45:09.360
and trying to understand how many users

00:45:09.360 --> 00:45:12.400
does it need to get to before that response time line

00:45:12.400 --> 00:45:13.560
starts going up.

00:45:13.560 --> 00:45:16.760
And you basically know that's where you've introduced

00:45:16.760 --> 00:45:18.440
some sort of bottleneck.

00:45:18.440 --> 00:45:22.000
- Something that's reaching its limit, yeah.

00:45:22.000 --> 00:45:23.880
- That's where someone, you know,

00:45:23.880 --> 00:45:28.280
people are all of a sudden sitting in a queue somewhere.

00:45:28.280 --> 00:45:31.800
The hard part is actually figuring out where that queue is.

00:45:31.800 --> 00:45:34.160
If you don't have the time, but you've got the money,

00:45:34.160 --> 00:45:36.400
you can just throw more infrastructure at it.

00:45:36.400 --> 00:45:41.280
But often you can fix some of those queues

00:45:41.280 --> 00:45:44.240
by looking at what are we caching and where.

00:45:44.240 --> 00:45:48.560
- Right, look at the CPU usage of the various aspects.

00:45:48.560 --> 00:45:50.960
What's the database CPU load?

00:45:50.960 --> 00:45:53.200
What's the web process?

00:45:53.200 --> 00:45:56.280
If the web bit is kind of chill,

00:45:56.280 --> 00:45:57.880
but the database is at 100%,

00:45:57.880 --> 00:46:00.400
maybe you need better indexes or something.

00:46:00.400 --> 00:46:03.840
- Yeah, 'cause often you're looking at the infrastructure

00:46:03.840 --> 00:46:06.280
on the back end, and even though the response times

00:46:06.280 --> 00:46:07.640
are going up, so there's a bottleneck,

00:46:07.640 --> 00:46:10.160
the CPU's maybe still at 60%.

00:46:11.160 --> 00:46:13.840
And the memory might only still be at 50%.

00:46:13.840 --> 00:46:16.160
So you're like, okay, you know,

00:46:16.160 --> 00:46:19.720
more CPUs isn't necessarily the issue,

00:46:19.720 --> 00:46:21.680
but things are getting stuck somewhere.

00:46:21.680 --> 00:46:25.840
And that might be that, you know, each page,

00:46:25.840 --> 00:46:28.080
actually I remember one load test I did

00:46:28.080 --> 00:46:34.600
where it was a PHP application using like some framework.

00:46:34.600 --> 00:46:37.480
And the performance like was horrible

00:46:37.480 --> 00:46:39.360
and we couldn't figure out why.

00:46:39.360 --> 00:46:41.280
And we put on some debugging tools

00:46:41.280 --> 00:46:45.160
and realized that every single page ran 200 SQL queries.

00:46:45.160 --> 00:46:49.240
I guess it's the way they'd written it.

00:46:49.240 --> 00:46:51.800
It was like, oh, that'd be why.

00:46:51.800 --> 00:46:54.440
'Cause you're looking at the CPU and memory

00:46:54.440 --> 00:46:56.920
of the actual web servers, and they're fine.

00:46:56.920 --> 00:46:59.040
Like the web servers are just like,

00:46:59.040 --> 00:47:00.440
basically just sitting there,

00:47:00.440 --> 00:47:03.440
continually waiting for the database server to respond.

00:47:03.440 --> 00:47:07.360
So, you know, you look at the resource usage

00:47:07.360 --> 00:47:10.480
and you might be thinking, well, why is it getting slower?

00:47:10.480 --> 00:47:14.080
But often it's 'cause you're making calls to the database

00:47:14.080 --> 00:47:16.240
or a backend API or something.

00:47:16.240 --> 00:47:19.040
And it's just sitting there idle, waiting for the responses.

00:47:19.040 --> 00:47:22.480
So, you know, I know you've mentioned on the show

00:47:22.480 --> 00:47:25.520
a few times, but like the N plus one issues

00:47:25.520 --> 00:47:28.400
and stuff like that for ORMs in particular

00:47:28.400 --> 00:47:30.840
is where you get those types of scale

00:47:30.840 --> 00:47:34.560
where you should not be seeing that many SQL queries.

00:47:34.560 --> 00:47:38.200
It's so easy to do because you write the same code,

00:47:38.200 --> 00:47:40.320
get the things and then loop over some

00:47:40.320 --> 00:47:42.840
and interact with some aspect of them.

00:47:42.840 --> 00:47:47.200
And if you don't eagerly do the query at the first part,

00:47:47.200 --> 00:47:48.760
each one of those is a separate query.

00:47:48.760 --> 00:47:50.680
The more you get back, the worse it is.

00:47:50.680 --> 00:47:51.800
- Yeah, exactly.

00:47:51.800 --> 00:47:53.600
And I mentioned like an AI app,

00:47:53.600 --> 00:47:57.080
like with this LLM rag app that we did a load test on

00:47:57.080 --> 00:47:58.280
with like 10 users.

00:47:58.280 --> 00:48:02.000
And like the CPU and memory were fine on the front end

00:48:02.000 --> 00:48:03.120
'cause it's not really doing anything.

00:48:03.120 --> 00:48:04.600
It's just calling the LLM,

00:48:04.600 --> 00:48:10.320
but it hit like a token limit like really, really quickly.

00:48:10.320 --> 00:48:12.600
And 'cause we were capturing that and tracing it,

00:48:12.600 --> 00:48:15.480
then we could see like that's what the bottleneck was,

00:48:15.480 --> 00:48:19.280
that it was getting rate limited on the backend.

00:48:19.280 --> 00:48:21.800
- Yeah.

00:48:21.800 --> 00:48:26.360
So I think one of the challenges that people can run into,

00:48:26.360 --> 00:48:27.840
this is really important,

00:48:27.840 --> 00:48:32.120
is testing with way less data

00:48:32.120 --> 00:48:34.000
than you're gonna have in production.

00:48:34.000 --> 00:48:37.400
I've got 10 entries in the database

00:48:37.400 --> 00:48:40.000
'cause that's what I bothered to test type in

00:48:40.000 --> 00:48:41.560
while I was playing with the app,

00:48:41.560 --> 00:48:44.200
but I've got a million in production.

00:48:44.200 --> 00:48:45.240
Stuff like that, right?

00:48:45.240 --> 00:48:46.080
I've got three products

00:48:46.080 --> 00:48:48.640
and there's a million in production.

00:48:48.640 --> 00:48:58.280
- Yeah, so, I think I might.

00:48:58.280 --> 00:48:59.120
- Now go ahead.

00:48:59.120 --> 00:49:01.200
So you actually had some really interesting tools

00:49:01.200 --> 00:49:02.960
that you recommended to deal with that.

00:49:02.960 --> 00:49:05.120
- Yeah, I'd seen this a few times

00:49:05.120 --> 00:49:07.800
where people had done load testing

00:49:07.800 --> 00:49:10.800
and they're like, "Oh, on the all users page

00:49:10.800 --> 00:49:14.320
"or on the list products page, it runs super fast."

00:49:14.320 --> 00:49:17.080
'Cause they've deployed the dev environment

00:49:17.080 --> 00:49:19.520
and it's got like no products in the database.

00:49:19.520 --> 00:49:22.800
- It doesn't matter if you have indexes

00:49:22.800 --> 00:49:25.040
'cause there's only three things, just return them all.

00:49:25.040 --> 00:49:27.840
- Yeah, and you don't see things at the end plus one

00:49:27.840 --> 00:49:30.840
'cause there's only like a couple of products.

00:49:30.840 --> 00:49:32.040
If any at all.

00:49:32.040 --> 00:49:35.840
So you often wanna seed the application

00:49:35.840 --> 00:49:38.520
with as much fake data as possible.

00:49:38.520 --> 00:49:41.760
And the library that I absolutely love for this

00:49:41.760 --> 00:49:46.760
is called Mimesis, which is a fake data generator

00:49:46.760 --> 00:49:47.840
kind of similar to Faker,

00:49:47.840 --> 00:49:51.120
but it's got a lot more support

00:49:51.120 --> 00:49:55.640
for like different languages and environments

00:49:55.640 --> 00:49:57.400
and stuff like that.

00:49:57.400 --> 00:49:58.240
So if you wanted to say,

00:49:58.320 --> 00:50:01.320
"Okay, let's create 100,000 users."

00:50:01.320 --> 00:50:04.800
And you need to generate names and addresses

00:50:04.800 --> 00:50:06.680
and locations and stuff like that,

00:50:06.680 --> 00:50:09.480
you can do that using Mimesis really easily.

00:50:09.480 --> 00:50:13.800
And also if you want to do like test

00:50:13.800 --> 00:50:19.600
like different cultures or locales.

00:50:19.600 --> 00:50:23.760
So not just testing like English names,

00:50:23.760 --> 00:50:25.800
but testing all sorts of different countries

00:50:25.800 --> 00:50:28.160
and stuff like that or different phone numbers.

00:50:28.160 --> 00:50:31.440
Then yeah, you can use Mimesis to do that.

00:50:31.440 --> 00:50:35.520
- Yeah, it's got all kinds of different things

00:50:35.520 --> 00:50:39.000
that get credit cards and phone numbers.

00:50:39.000 --> 00:50:42.840
- Yeah, you can say, "I need 100,000 Brazilian phone numbers

00:50:42.840 --> 00:50:44.440
and it will just give you them."

00:50:44.440 --> 00:50:46.680
(laughing)

00:50:46.680 --> 00:50:48.120
And exactly the right format.

00:50:48.120 --> 00:50:53.080
- Yeah, so then you save those to your database once

00:50:53.080 --> 00:50:54.440
and then you can run your tests

00:50:54.440 --> 00:50:56.760
and see if you have your N plus one problem

00:50:56.760 --> 00:51:00.920
or your indexes don't fit into memory

00:51:00.920 --> 00:51:03.160
or whatever the problem might be, right?

00:51:03.160 --> 00:51:07.480
- Yeah, so for Django or for SQLAlchemy,

00:51:07.480 --> 00:51:12.480
you can do the like these load and dump commands.

00:51:12.480 --> 00:51:15.400
So what I kind of recommend is to keep it fast

00:51:15.400 --> 00:51:19.640
is to use Mimesis to generate a seed data file,

00:51:19.640 --> 00:51:22.880
even if that's like JSON or something,

00:51:22.880 --> 00:51:26.600
and then just do like a bulk load in the test environment

00:51:26.600 --> 00:51:28.440
and then you can reset it every time.

00:51:28.440 --> 00:51:31.000
So you can basically just do a rollback

00:51:31.000 --> 00:51:33.120
and then just reset and do a bulk load.

00:51:33.120 --> 00:51:36.800
If using like a document database,

00:51:36.800 --> 00:51:39.680
then they've got similar tools for that.

00:51:39.680 --> 00:51:43.000
So if using Mongo, then you could just do a bulk load

00:51:43.000 --> 00:51:44.960
from like a test command basically.

00:51:44.960 --> 00:51:47.160
- Right, just to load it up with some fake data,

00:51:47.160 --> 00:51:49.520
do a Mongo dump and then whenever you're ready to reset it,

00:51:49.520 --> 00:51:51.760
just do a Mongo restore, dash, dash, drop

00:51:51.760 --> 00:51:54.720
and it'll be-- - Yeah, exactly.

00:51:54.720 --> 00:51:58.200
And you can reuse those for your integration tests as well.

00:51:58.200 --> 00:51:59.600
You know, if you're writing,

00:51:59.600 --> 00:52:02.440
if you've got integration tests with Django or Flask,

00:52:02.440 --> 00:52:05.320
then often you need a database fixture

00:52:05.320 --> 00:52:07.440
and you wanna see that with some information,

00:52:07.440 --> 00:52:09.340
then you can just reuse the same data.

00:52:09.340 --> 00:52:13.440
- Yeah, you could also do transactions, right?

00:52:13.440 --> 00:52:16.360
Like put a transactions that's always rolled back.

00:52:16.360 --> 00:52:20.320
So once you load it up, you're not breaking it,

00:52:20.320 --> 00:52:23.160
but that's tricky if the code itself calls commit,

00:52:23.160 --> 00:52:24.920
I suppose. (laughs)

00:52:24.920 --> 00:52:26.240
- Yeah.

00:52:26.240 --> 00:52:28.880
- So no, it changed it, darn it.

00:52:28.880 --> 00:52:30.640
- Yeah, there was a question in the chat,

00:52:30.640 --> 00:52:32.160
how does load testing change

00:52:32.160 --> 00:52:34.640
when you have non-deterministic processes,

00:52:34.640 --> 00:52:37.960
e.g. LLM queries, which is a really good question.

00:52:37.960 --> 00:52:40.600
And I think it's kind of related to this

00:52:40.600 --> 00:52:42.560
where you've got a,

00:52:42.560 --> 00:52:45.280
you want to introduce like an element of randomness,

00:52:45.280 --> 00:52:48.560
but we talked about like a user search page

00:52:48.560 --> 00:52:51.260
or if you've got like a chat feature or something,

00:52:51.260 --> 00:52:55.600
then you want to kind of vary the question,

00:52:55.600 --> 00:52:59.120
especially if you've got any kind of caching.

00:52:59.120 --> 00:53:01.280
So that is tricky though,

00:53:01.280 --> 00:53:04.680
because at the moment, like the LLM calls,

00:53:04.680 --> 00:53:05.960
depending on which model you're using

00:53:05.960 --> 00:53:07.840
and how it's set up and stuff like that,

00:53:07.840 --> 00:53:10.960
but they can take a second,

00:53:10.960 --> 00:53:13.640
up to 10 seconds to get a response.

00:53:13.640 --> 00:53:15.320
- Easier, yeah.

00:53:15.320 --> 00:53:17.520
- So you need to kind of factor that in,

00:53:17.520 --> 00:53:19.320
is like how is your app,

00:53:19.320 --> 00:53:23.360
how does your app handle sitting and waiting for 10 seconds

00:53:23.360 --> 00:53:26.640
before it gets a response back?

00:53:26.640 --> 00:53:29.520
And often you'll find that you'll max out

00:53:29.520 --> 00:53:32.360
the number of like threads you've got,

00:53:32.360 --> 00:53:35.560
or the number of workers in like gUnicorn

00:53:35.560 --> 00:53:36.800
or uvicorn or something like that,

00:53:36.800 --> 00:53:39.100
because they're all just sitting there waiting.

00:53:39.100 --> 00:53:42.960
- That's a place where async and await

00:53:42.960 --> 00:53:44.200
would be pretty awesome.

00:53:44.200 --> 00:53:45.040
- Yeah.

00:53:45.040 --> 00:53:48.160
- Because you can sort of let the thread go

00:53:48.160 --> 00:53:50.280
and just you'll get back to it.

00:53:50.280 --> 00:53:52.880
Also, maybe you just need a different architecture.

00:53:52.880 --> 00:53:56.480
Maybe it's not just more caching or more hardware.

00:53:56.480 --> 00:53:58.960
It's like, so we're gonna put the,

00:53:58.960 --> 00:54:01.320
there's a pending request in the database

00:54:01.320 --> 00:54:02.560
and we're gonna push that off to somewhere

00:54:02.560 --> 00:54:04.760
and then we'll just set up like some kind of JavaScript thing

00:54:04.760 --> 00:54:06.760
to check if it's done and then pull down the answer

00:54:06.760 --> 00:54:11.480
or something that's not necessarily blocking potentially.

00:54:11.480 --> 00:54:14.040
- Yeah, I think in a way it's great for,

00:54:14.040 --> 00:54:15.280
like you mentioned, great for that.

00:54:15.280 --> 00:54:17.460
Actually, there was an announcement pretty recently

00:54:17.460 --> 00:54:22.460
that you can run uvicorn now, like as a production.

00:54:22.460 --> 00:54:23.920
- Yeah, directly, yeah.

00:54:23.920 --> 00:54:28.920
Without being wrapped to gUnicorn, yeah.

00:54:28.920 --> 00:54:30.920
- Yeah. - That's awesome.

00:54:30.920 --> 00:54:33.240
- So yeah, for this kind of scenario we just talked about

00:54:33.240 --> 00:54:35.760
where you're waiting on backend calls and stuff like that,

00:54:35.760 --> 00:54:38.840
use the async version of those libraries.

00:54:38.840 --> 00:54:40.840
Like if it's OpenAI, there's an async version

00:54:40.840 --> 00:54:44.320
of the OpenAI SDK, use that.

00:54:44.320 --> 00:54:48.360
And then at least like uvicorn is gonna just have,

00:54:48.360 --> 00:54:50.960
it will quite happily run like hundreds and hundreds

00:54:50.960 --> 00:54:54.120
of those requests that are just waiting on backend calls.

00:54:54.120 --> 00:54:56.080
- Yeah, absolutely.

00:54:56.080 --> 00:54:57.680
So one thing I wanna throw in here,

00:54:57.680 --> 00:55:00.520
this is coming from me, not from you,

00:55:00.520 --> 00:55:02.120
but I wanna get your thoughts on it.

00:55:02.120 --> 00:55:06.720
So I recently set up this thing called Uptime Kuma.

00:55:06.720 --> 00:55:08.560
Are you familiar with Uptime Kuma?

00:55:08.560 --> 00:55:10.140
- No.

00:55:10.140 --> 00:55:12.040
- So this is an open source,

00:55:12.040 --> 00:55:15.040
self-hosted uptime monitoring tool.

00:55:15.040 --> 00:55:19.400
And just run it in Docker, off it goes.

00:55:19.400 --> 00:55:22.360
And one of the things that's nice about this is,

00:55:22.360 --> 00:55:26.880
well, over on Talk Python now, if you go to the bottom,

00:55:26.880 --> 00:55:28.880
it's got a server status and you can come in here,

00:55:28.880 --> 00:55:31.720
you can see the website's behaving well,

00:55:31.720 --> 00:55:35.000
the RSS feed is behaving well, the courses size,

00:55:35.000 --> 00:55:36.960
but also the mobile API, all these things.

00:55:36.960 --> 00:55:40.100
But the reason I think this is, I mean, status,

00:55:40.100 --> 00:55:41.940
like is it working or not, it's not really that relevant.

00:55:41.940 --> 00:55:43.380
That's like operational thing.

00:55:43.380 --> 00:55:47.300
But what's cool about it is if you go and dig into the thing

00:55:47.300 --> 00:55:49.580
and you go to the dashboard, like let's say,

00:55:49.580 --> 00:55:52.260
it'll even, you could even group them, right?

00:55:52.260 --> 00:55:55.540
So like, you know, let's do this, I'll do the API.

00:55:55.540 --> 00:55:58.300
So for like the mobile apps API,

00:55:58.300 --> 00:56:03.300
or the mobile app for Talk Python courses,

00:56:03.900 --> 00:56:07.280
you can actually see over time what it's seen

00:56:07.280 --> 00:56:10.400
in response time for like days.

00:56:10.400 --> 00:56:14.920
So with your load tests, you're saying,

00:56:14.920 --> 00:56:19.920
let me just hammer it and see how it responds to that, right?

00:56:19.920 --> 00:56:21.400
And if you point something like this,

00:56:21.400 --> 00:56:22.740
or there's many other tools like this,

00:56:22.740 --> 00:56:24.740
but this is a nice one, you can just say,

00:56:24.740 --> 00:56:28.480
just keep a record and let me go back and look for,

00:56:28.480 --> 00:56:33.040
you know, for 24 hours, how has the load looked?

00:56:33.040 --> 00:56:34.540
You can see there's some weird spike there,

00:56:34.540 --> 00:56:35.860
probably did like a deployment,

00:56:35.860 --> 00:56:38.240
like right around then or something.

00:56:38.240 --> 00:56:43.240
But in general, you know, 40, 50 milliseconds, 35, 81,

00:56:43.240 --> 00:56:45.900
right, it's handling it.

00:56:45.900 --> 00:56:48.540
'Cause if the thing was overwhelmed,

00:56:48.540 --> 00:56:52.480
then these tests would also suffer the latency

00:56:52.480 --> 00:56:54.040
that everyone else is suffering, you know?

00:56:54.040 --> 00:56:56.380
And this is like every 30 seconds.

00:56:56.380 --> 00:56:57.740
- Yeah, that's really cool.

00:56:57.740 --> 00:57:01.840
I do wanna give a shout out to OpenTelemetry.

00:57:02.040 --> 00:57:06.320
I hope more people, every year,

00:57:06.320 --> 00:57:08.200
more and more people find out about this

00:57:08.200 --> 00:57:11.000
and they're like, oh, this solves a lot of my problems.

00:57:11.000 --> 00:57:15.560
It's a CNCF project.

00:57:15.560 --> 00:57:18.840
So it's a big open source standard.

00:57:18.840 --> 00:57:23.760
It basically is a standard for instrumentation,

00:57:23.760 --> 00:57:27.040
like an observability.

00:57:27.040 --> 00:57:32.040
So it's, you kind of plug these instrumentation libraries

00:57:32.040 --> 00:57:34.400
into your app.

00:57:34.400 --> 00:57:38.500
And there's ones for FastAPI, Django, Flask,

00:57:38.500 --> 00:57:44.120
like the backends, like Mongo, like SQLAlchemy,

00:57:44.120 --> 00:57:44.960
stuff like that.

00:57:44.960 --> 00:57:48.560
And you basically just install these pip packages

00:57:48.560 --> 00:57:51.160
and it will just start to capture all this information

00:57:51.160 --> 00:57:54.240
about, oh, there's a request and it had these parameters

00:57:54.240 --> 00:57:55.380
and it took this long.

00:57:56.960 --> 00:58:00.320
And so in Python, like with a very small amount of code

00:58:00.320 --> 00:58:01.560
and a few pip packages,

00:58:01.560 --> 00:58:04.760
you can instrument all this stuff in your application.

00:58:04.760 --> 00:58:08.000
And then the other thing that OpenTelemetry does

00:58:08.000 --> 00:58:11.600
is it will have, it's kind of like a pluggable exporter.

00:58:11.600 --> 00:58:15.900
So wherever you want to send all that data to,

00:58:15.900 --> 00:58:18.400
you can kind of pick and choose

00:58:18.400 --> 00:58:21.160
like which platform you wanna send it to.

00:58:21.160 --> 00:58:24.560
And there are some like local ones you can use as well.

00:58:24.560 --> 00:58:27.200
We showed this tool for uptime.

00:58:27.200 --> 00:58:31.540
There's some Docker containers for that.

00:58:31.540 --> 00:58:33.400
So you just spin up a Docker container

00:58:33.400 --> 00:58:35.280
and it'll just take all that telemetry data

00:58:35.280 --> 00:58:37.080
and give it to you in a GUI.

00:58:37.080 --> 00:58:41.120
So you can see like all the traces, how long they took,

00:58:41.120 --> 00:58:43.000
what calls it made on the backend,

00:58:43.000 --> 00:58:44.760
what queries it was running.

00:58:44.760 --> 00:58:48.440
Like OpenTelemetry is a brilliant way of doing that.

00:58:48.440 --> 00:58:50.600
And it's not just Python.

00:58:50.600 --> 00:58:54.180
Like if you've got some components of your application

00:58:54.180 --> 00:58:55.680
that are written in other languages,

00:58:55.680 --> 00:58:58.980
like odds are that it is also supported.

00:58:58.980 --> 00:59:01.960
So it's kind of like a framework, I guess,

00:59:01.960 --> 00:59:03.360
for capturing data.

00:59:03.360 --> 00:59:05.160
And when you're doing load tests,

00:59:05.160 --> 00:59:09.560
this is how we do like postmortems to figure out,

00:59:09.560 --> 00:59:12.480
or even just looking at the stats and seeing like,

00:59:12.480 --> 00:59:14.000
where did things go slow?

00:59:14.000 --> 00:59:18.440
And I've got some videos and stuff of demos

00:59:18.440 --> 00:59:21.360
I've done with this, where I've got like applications,

00:59:21.360 --> 00:59:22.480
I've done a load test on them,

00:59:22.480 --> 00:59:24.900
and then I can see, oh, there was a spike in load.

00:59:24.900 --> 00:59:28.900
Let's go back and look at the data to see what caused that

00:59:28.900 --> 00:59:30.740
and which calls was it,

00:59:30.740 --> 00:59:33.140
what was the resources, the memory usage,

00:59:33.140 --> 00:59:37.020
and often like, was it the database or was it an API call

00:59:37.020 --> 00:59:38.880
and why did it take so long?

00:59:38.880 --> 00:59:40.940
What were the special parameters?

00:59:40.940 --> 00:59:42.100
- What were we waiting on?

00:59:42.100 --> 00:59:44.100
Was it the actual web app or was it database?

00:59:44.100 --> 00:59:46.540
But like you can retroactively know that, right?

00:59:46.540 --> 00:59:49.900
- Yeah, and OpenTelemetry is a great way of doing that.

00:59:49.900 --> 00:59:51.340
- Yeah, that's super cool.

00:59:51.340 --> 00:59:53.560
That's way more holistic than my just,

00:59:53.560 --> 00:59:55.680
give me a graph or response time.

00:59:55.680 --> 00:59:57.840
- Yeah, that's still cool.

00:59:57.840 --> 00:59:58.680
- It's still cool.

00:59:58.680 --> 01:00:01.800
It's 20 minutes to set it up in the Docker cluster.

01:00:01.800 --> 01:00:06.760
Awesome, well, let's talk about,

01:00:06.760 --> 01:00:08.600
I know we're basically out of time, Anthony.

01:00:08.600 --> 01:00:10.360
Let's just talk really quickly about one thing

01:00:10.360 --> 01:00:13.200
that I think is a little bit tricky

01:00:13.200 --> 01:00:18.200
and maybe just get your thoughts on my Vercel example

01:00:18.200 --> 01:00:20.640
of cost is sort of in this space.

01:00:20.640 --> 01:00:22.400
And that's serverless.

01:00:22.400 --> 01:00:24.900
What are your thoughts on serverless?

01:00:24.900 --> 01:00:28.360
Like you have way more control when you have a VM

01:00:28.360 --> 01:00:30.440
or you've got a Kubernetes cluster

01:00:30.440 --> 01:00:33.160
or whatever it is you're working with, right?

01:00:33.160 --> 01:00:37.040
- Yeah, it's kind of a bit, I don't know.

01:00:37.040 --> 01:00:39.440
I always kind of see serverless.

01:00:39.440 --> 01:00:42.420
It's a really cool idea, but it's more of a commercial

01:00:42.420 --> 01:00:44.060
concept than the technical one.

01:00:44.060 --> 01:00:49.160
- Yeah, I'm thinking more about just the,

01:00:49.160 --> 01:00:50.880
you don't control the warmup.

01:00:50.880 --> 01:00:53.560
You don't control the machine.

01:00:53.560 --> 01:00:57.280
There's just a lot of stuff that's black box to you.

01:00:57.280 --> 01:01:00.400
- I think most of the platforms and most of the providers

01:01:00.400 --> 01:01:04.940
have got a range of options.

01:01:04.940 --> 01:01:07.520
I speak to Azure Functions,

01:01:07.520 --> 01:01:11.420
which is like our serverless Python engine.

01:01:11.420 --> 01:01:18.440
You can use that as just pay per request.

01:01:19.280 --> 01:01:21.560
But it then has certain optimization

01:01:21.560 --> 01:01:23.560
so that it has like a warm startup time.

01:01:23.560 --> 01:01:27.280
So if you haven't made any requests

01:01:27.280 --> 01:01:29.040
for a certain number of hours,

01:01:29.040 --> 01:01:30.560
then the application will fall asleep.

01:01:30.560 --> 01:01:31.840
So it's not using resources

01:01:31.840 --> 01:01:34.300
and we're not charging you money.

01:01:34.300 --> 01:01:35.560
But if you don't want to have that,

01:01:35.560 --> 01:01:37.520
if you want the startup time to be super fast,

01:01:37.520 --> 01:01:42.320
then you can use a different service level basically

01:01:42.320 --> 01:01:44.720
and have like faster startups.

01:01:44.720 --> 01:01:48.280
You can have like pre-allocated resources,

01:01:48.280 --> 01:01:51.200
which then becomes not serverless,

01:01:51.200 --> 01:01:53.560
but it's the same framework, it's the same architecture.

01:01:53.560 --> 01:01:55.960
And Lambda's the same, AWS Lambda's the same.

01:01:55.960 --> 01:01:59.280
You can kind of set it up in those different options.

01:01:59.280 --> 01:02:02.920
Things kind of get tricky when you get into Kubernetes

01:02:02.920 --> 01:02:07.720
because you've got real infrastructure on the background.

01:02:07.720 --> 01:02:12.260
And when you look at Kubernetes clusters

01:02:12.260 --> 01:02:15.320
or Kubernetes as a service or however you're setting it up,

01:02:15.320 --> 01:02:16.360
the first question is like,

01:02:16.360 --> 01:02:18.200
how many VMs do I need to provision

01:02:18.200 --> 01:02:19.760
and how big do they need to be?

01:02:19.760 --> 01:02:23.560
So yes, you're kind of building

01:02:23.560 --> 01:02:27.800
in like a serverless abstraction on top of it,

01:02:27.800 --> 01:02:30.960
but you've got real infrastructure in the background.

01:02:30.960 --> 01:02:32.520
And that's actually really hard

01:02:32.520 --> 01:02:34.800
because in a lot of cases,

01:02:34.800 --> 01:02:37.780
you've just got tons of idle infrastructure.

01:02:37.780 --> 01:02:41.420
So I think load testing is a good way of looking at,

01:02:41.420 --> 01:02:44.280
like trying to right size what you've got provisioned.

01:02:45.880 --> 01:02:48.360
And in many cases, scaling down some pits

01:02:48.360 --> 01:02:49.800
and scaling up others.

01:02:49.800 --> 01:02:54.240
- Yeah, absolutely.

01:02:54.240 --> 01:02:57.040
I don't mess with Kubernetes, it's too much for me.

01:02:57.040 --> 01:02:58.640
I don't need all that.

01:02:58.640 --> 01:03:00.440
I don't need all that stuff.

01:03:00.440 --> 01:03:02.720
I do use Docker though, which is really, really nice.

01:03:02.720 --> 01:03:04.640
I know a lot of these tools we talked about

01:03:04.640 --> 01:03:07.960
support running in Docker or working with Docker and so on.

01:03:07.960 --> 01:03:14.080
All right, well, hopefully people have

01:03:14.080 --> 01:03:16.680
some really concrete ideas and tools that they can use

01:03:16.680 --> 01:03:20.320
like locust.io, which we're both huge fans of.

01:03:20.320 --> 01:03:22.440
Playwright, same deal,

01:03:22.440 --> 01:03:24.820
but also some of the philosophy and ideas behind it,

01:03:24.820 --> 01:03:26.600
which is super important too.

01:03:26.600 --> 01:03:29.520
So much appreciate you coming on the show to share that,

01:03:29.520 --> 01:03:30.880
just the chance to catch up.

01:03:30.880 --> 01:03:33.620
How about you give us a quick wrap up,

01:03:33.620 --> 01:03:35.440
important takeaways for people

01:03:35.440 --> 01:03:38.200
who want to go out and test their stuff now?

01:03:38.200 --> 01:03:41.960
- Yeah, so I think step one is to look at your application

01:03:41.960 --> 01:03:44.620
and understand what your users are likely to do.

01:03:44.620 --> 01:03:49.600
So if you want to design a load test, start simple,

01:03:49.600 --> 01:03:52.080
maybe even start with Playwright,

01:03:52.080 --> 01:03:54.800
because you can just spin up the browser recorder,

01:03:54.800 --> 01:03:57.320
click around in the website

01:03:57.320 --> 01:04:00.640
and simulate what a user would be doing,

01:04:00.640 --> 01:04:03.320
stitch that together with locust and test.

01:04:03.320 --> 01:04:04.880
10 users, don't go nuts.

01:04:04.880 --> 01:04:08.160
Start off with a small number, a simple test,

01:04:09.560 --> 01:04:12.800
and you will uncover things that need optimizing.

01:04:12.800 --> 01:04:15.040
I don't think I've ever encountered an application

01:04:15.040 --> 01:04:17.540
that just ran really efficiently the first time.

01:04:17.540 --> 01:04:21.640
And then just keep working on that.

01:04:21.640 --> 01:04:23.920
So yeah, instead,

01:04:23.920 --> 01:04:26.600
'cause often you would end up trying to optimize things

01:04:26.600 --> 01:04:27.640
that are not gonna get touched

01:04:27.640 --> 01:04:30.640
or don't really make a difference when you actually test it.

01:04:30.640 --> 01:04:32.940
So yeah, start simple.

01:04:32.940 --> 01:04:36.980
I recommend using locust and Playwright if you want,

01:04:36.980 --> 01:04:39.080
or you can just write a simple locust test.

01:04:39.520 --> 01:04:43.000
And then put the instrumentation in the backend

01:04:43.000 --> 01:04:45.920
so that you can see not just the response times,

01:04:45.920 --> 01:04:49.200
but you can see as much data as possible

01:04:49.200 --> 01:04:52.200
on what's happening and how long it's taking.

01:04:52.200 --> 01:04:55.480
And I'll share like a couple of links

01:04:55.480 --> 01:04:59.520
of some simple dashboards you can use with OpenTelemetry,

01:04:59.520 --> 01:05:02.800
where they will capture that data locally or in the cloud

01:05:02.800 --> 01:05:06.740
and show you a trace of every request.

01:05:07.760 --> 01:05:09.320
- Yeah, awesome.

01:05:09.320 --> 01:05:11.640
And do that with real data.

01:05:11.640 --> 01:05:13.400
Don't do it with three entries.

01:05:13.400 --> 01:05:14.800
- Yeah.

01:05:14.800 --> 01:05:17.280
- It's not gonna mean what you think it means

01:05:17.280 --> 01:05:19.080
if you do it with only three entries.

01:05:19.080 --> 01:05:19.920
- Yeah.

01:05:19.920 --> 01:05:20.740
- Awesome.

01:05:20.740 --> 01:05:23.160
All right, well, always great you having the show.

01:05:23.160 --> 01:05:24.560
Thanks for being here, Anthony.

01:05:24.560 --> 01:05:25.480
Catch you later. - Yes.

01:05:25.480 --> 01:05:26.320
Great to be back.

01:05:26.320 --> 01:05:27.840
Thanks for watching. - Yeah, bye.

