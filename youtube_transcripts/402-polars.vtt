WEBVTT

00:00:00.000 --> 00:00:05.000
- Hey, Richie, welcome to Talk Python to Me.


00:00:05.000 --> 00:00:08.440
- Hi, Michael, thanks for having me.


00:00:08.440 --> 00:00:09.400
Great to be here.


00:00:09.400 --> 00:00:11.800
- Yeah, it's good to have you here.


00:00:11.800 --> 00:00:15.120
I feel like maybe I should rename my podcast,


00:00:15.120 --> 00:00:16.440
Talk Rust to Me or something.


00:00:16.440 --> 00:00:18.200
I don't know, Rust is taking over


00:00:18.200 --> 00:00:22.360
as the low level part of,


00:00:22.360 --> 00:00:24.000
how do we make Python go fast?


00:00:24.000 --> 00:00:27.360
I don't know, there's some kind of synergy with Rust.


00:00:27.360 --> 00:00:28.200
What's going on there?


00:00:28.200 --> 00:00:37.240
Yeah, there is. So I'd say Python already was low-level languages that succeeded, that


00:00:37.240 --> 00:00:44.800
made Python a success. I mean, like NumPy, Pandas, everything that was reasonable fast


00:00:44.800 --> 00:00:54.080
was so because of C, or Cyton, which is also C. But Rust, different from C, Rust has made


00:00:54.080 --> 00:00:59.360
level programming a lot more fun to use and a lot more safe.


00:00:59.360 --> 00:01:03.680
And especially if you regard multi-threaded programming,


00:01:03.680 --> 00:01:07.760
parallel programming, concurrent programming,


00:01:07.760 --> 00:01:11.480
it is a lot easier in Rust.


00:01:11.480 --> 00:01:15.600
And it's way-- yeah.


00:01:15.600 --> 00:01:19.640
So it opens a lot of possibilities.


00:01:19.640 --> 00:01:21.520
Yeah, my understanding, I've only


00:01:21.520 --> 00:01:25.800
given a cursory look to Rust, just sort of scan some examples.


00:01:25.800 --> 00:01:28.200
And we're going to see some examples of code


00:01:28.200 --> 00:01:31.760
in a little bit, actually, related to Polars.


00:01:31.760 --> 00:01:35.480
But it's kind of a low-level language.


00:01:35.480 --> 00:01:40.600
It's not as simple as Python or maybe a JavaScript.


00:01:40.600 --> 00:01:44.720
But it is easier than C, C++, not just in the syntax.


00:01:44.720 --> 00:01:48.640
But it does better memory tracking for you


00:01:48.640 --> 00:01:51.440
and the concurrency especially, right?


00:01:51.440 --> 00:01:58.960
Yeah, well, so Rust has got a, brings a whole new thing to the table, which is called ownership


00:01:58.960 --> 00:02:05.040
and a borrower checker. And Rust is really strict. There are things in Rust you cannot do in C or C++


00:02:05.040 --> 00:02:16.080
because at a time there can only be one owner of a piece of memory and other people can,


00:02:16.080 --> 00:02:20.640
you can lend out this piece of memory to other users, but then they cannot mutate it. So,


00:02:20.640 --> 00:02:23.920
So there can be only one owner which is able to mutate something.


00:02:23.920 --> 00:02:29.160
And this restriction makes Rust a really hard language to learn.


00:02:29.160 --> 00:02:35.440
But once you, once it's clicked, once you went over that steep learning curve,


00:02:35.440 --> 00:02:44.640
it becomes a lot easier because it doesn't allow you things that you could do in C and C++.


00:02:44.640 --> 00:02:50.320
But those things were also things you shouldn't do in C and C++ because they probably led to


00:02:50.320 --> 00:02:52.120
to set faults and to memory issues.


00:02:52.120 --> 00:02:54.520
And this borrow checker


00:02:54.520 --> 00:02:59.400
also makes writing concurrent programming safe.


00:02:59.400 --> 00:03:03.640
And sorry, you're muted.


00:03:03.640 --> 00:03:06.080
- I am, thank you.


00:03:06.080 --> 00:03:11.080
You can have many threads reading a variable all they want.


00:03:11.080 --> 00:03:13.040
They can read concurrently.


00:03:13.040 --> 00:03:14.880
It's when you have writers and readers


00:03:14.880 --> 00:03:19.000
that this whole thread safety critical section,


00:03:19.000 --> 00:03:23.800
take your locks or the locks re-enter and all of that really difficult stuff comes in.


00:03:23.800 --> 00:03:30.040
And so, yeah, it sounds like an important key to making that.


00:03:30.040 --> 00:03:36.840
Yeah, and the same borrower checker also knows when memory has to be freed and not.


00:03:36.840 --> 00:03:42.800
But it doesn't have to, unlike in Go or Java where you have a garbage collector, it doesn't


00:03:42.800 --> 00:03:46.680
have to do garbage collection and it doesn't have to do reference counting like Python


00:03:46.680 --> 00:03:47.680
does.


00:03:47.680 --> 00:03:53.840
by just statically. So at compile time, it knows when something is out of scope and not used anymore.


00:03:53.840 --> 00:03:56.240
And this is real powerful.


00:03:56.240 --> 00:04:02.720
Yeah. So I guess the takeaway for listeners who are wondering, you know, why is Rust seemingly


00:04:02.720 --> 00:04:08.080
taking over so much of the job that C and variations of C, right? Like you said,


00:04:08.080 --> 00:04:16.960
Cython have traditionally played in Python. It's easier to write modern, faster, safer code.


00:04:16.960 --> 00:04:23.280
Yeah. Yeah. Okay. Probably more fun too, right? Yeah, definitely. And it's a language which has


00:04:23.280 --> 00:04:28.800
got its tools right. So it's got a package manager, which is really great to use. It's got a real


00:04:28.800 --> 00:04:36.720
creates.io, which is similar to the PyPy index. It feels like a modern language to build


00:04:36.720 --> 00:04:41.520
low level, more low level code. You can also write high level stuff like


00:04:43.840 --> 00:04:51.840
REST APIs, which is, I must say, also for high-level stuff, I like to write it in ROS


00:04:51.840 --> 00:04:56.880
because of the safety guarantees and also the correctness guarantees. If my program compiles


00:04:56.880 --> 00:05:03.520
in ROS, I'm much more certain it is correct than when I write my Python program, which is dynamic


00:05:03.520 --> 00:05:10.720
and types are not enforced, so it's always a bit graying on that side. Python is great to use, but


00:05:11.440 --> 00:05:15.080
it's harder to write correct code in Python.


00:05:15.080 --> 00:05:18.520
Yeah, and you can optionally write very loose code,


00:05:18.520 --> 00:05:23.080
or you could opt in to things like type hints and even


00:05:23.080 --> 00:05:27.560
mypy, and then you get closer to the static languages, right?


00:05:27.560 --> 00:05:30.560
Are you a fan of Python typing?


00:05:30.560 --> 00:05:33.360
Definitely, but because they're optional,


00:05:33.360 --> 00:05:35.880
they are as strong as the weakest link.


00:05:35.880 --> 00:05:39.760
So one library which you use, if it


00:05:39.760 --> 00:05:45.360
doesn't do this type correct or it doesn't do it, it breaks. It's


00:05:45.360 --> 00:05:49.400
quite brittle because it's optional. I hope we get something that really


00:05:49.400 --> 00:05:53.240
enforces it and really can check it. I don't know if it's possible because of


00:05:53.240 --> 00:05:59.400
the dynamic nature of Python. Python can do so many things just dynamically and


00:05:59.400 --> 00:06:05.240
statically. We just cannot know probably. I don't know how far it can go.


00:06:05.240 --> 00:06:13.600
But I, yeah, in Polaris as well, we use mypy type ints, which prevent us from having a


00:06:13.600 --> 00:06:18.720
lot of bugs and also make the IDE experience much nicer.


00:06:18.720 --> 00:06:20.960
Yeah, type ints are great.


00:06:20.960 --> 00:06:25.480
They really help you also think about your library.


00:06:25.480 --> 00:06:32.600
I think you really see a shift in modern Python and Python 10 years ago, where it was more


00:06:32.600 --> 00:06:39.320
dynamic and dynamic, the dynamic dynamic, I'm probably, Python were more seen as a strength


00:06:39.320 --> 00:06:46.440
than than currently, I believe. Yeah, I totally agree. And I feel like when type hints first


00:06:46.440 --> 00:06:51.880
came out, you know, this was, I guess, wow, at this point, kind of early Python three,


00:06:51.880 --> 00:06:56.280
but it didn't feel like it at the time, you know, Python three had been out for quite a while.


00:06:56.920 --> 00:07:01.640
when type hints were introduced, I feel like that was Python 3.4. But anyway, that was,


00:07:01.640 --> 00:07:07.400
put it maybe six years into the life cycle of Python 3. But still, I feel like a lot of people


00:07:07.400 --> 00:07:12.600
were suspicious of that at the moment. You know, they're like, Oh, what is this weird thing? We're


00:07:12.600 --> 00:07:18.120
not really sure we want to put these types into our Python. And now a lot less. There's a lot


00:07:18.120 --> 00:07:26.200
less of those reactions. Yeah, yeah, I see Python having to, probably more, but I often see Python


00:07:26.200 --> 00:07:29.520
and that's the really fun, nice to use,


00:07:29.520 --> 00:07:31.120
Duck Tape language where I can,


00:07:31.120 --> 00:07:33.000
in my, for instance, in Jupyter Notebook,


00:07:33.000 --> 00:07:37.040
I can just hack away and try interactively what happens.


00:07:37.040 --> 00:07:40.120
And for such code, type ints don't matter.


00:07:40.120 --> 00:07:45.120
But once I write more of a library or product or tool,


00:07:45.120 --> 00:07:49.320
then type ints are really great.


00:07:49.320 --> 00:07:53.120
I believe they came about, the Dropbox really needed them.


00:07:53.120 --> 00:07:55.200
They had a huge Python code base


00:07:55.200 --> 00:07:57.320
and I had really trouble maintaining it


00:07:57.320 --> 00:07:59.800
without the Pythons, but I'm not really sure.


00:07:59.800 --> 00:08:01.240
- Yeah, and I heard some guy who has something


00:08:01.240 --> 00:08:03.440
to do with Python used to work there.


00:08:03.440 --> 00:08:07.240
Guido used to work there, I think even at that time.


00:08:07.240 --> 00:08:08.520
All right, so a bit of a diversion


00:08:08.520 --> 00:08:10.660
from how I often start the show.


00:08:10.660 --> 00:08:13.040
So let's just circle back real quick and get your story.


00:08:13.040 --> 00:08:15.120
How'd you get into programming and Python


00:08:15.120 --> 00:08:16.760
and Rust as well, I suppose?


00:08:16.760 --> 00:08:22.520
- I got into programming by,


00:08:23.440 --> 00:08:24.720
I just wanted to learn programming.


00:08:24.720 --> 00:08:28.720
A friend of mine who programmed a lot of PHP


00:08:28.720 --> 00:08:30.960
said, "Learn Python, you'll like that more."


00:08:30.960 --> 00:08:34.000
And he gave me an interactive website


00:08:34.000 --> 00:08:37.060
where I could do some puzzles,


00:08:37.060 --> 00:08:39.600
and I really got hooked to it.


00:08:39.600 --> 00:08:40.760
It was a fun summer.


00:08:40.760 --> 00:08:44.040
I was programming a lot.


00:08:44.040 --> 00:08:45.220
I started automating.


00:08:45.220 --> 00:08:48.080
My job was a civil engineer at the moment,


00:08:48.080 --> 00:08:50.480
and I started, there was a lot of mundane tasks


00:08:50.480 --> 00:08:52.240
which were repetitive, and I just...


00:08:52.240 --> 00:09:01.280
I found ways to automate my job and eventually I was doing that for a year or three, four,


00:09:01.280 --> 00:09:04.840
and then I got into data science and I switched jobs.


00:09:04.840 --> 00:09:09.880
I became a data scientist and later a data engineer.


00:09:09.880 --> 00:09:13.640
Yeah, so that was Python mostly.


00:09:13.640 --> 00:09:17.720
And I've always been looking for more languages.


00:09:17.720 --> 00:09:23.080
languages, the playing with Haskell, the playing with Go, the playing with JavaScript,


00:09:23.080 --> 00:09:31.840
I'll just look at it. The playing with Scala and then I found Rust and Rust really


00:09:31.840 --> 00:09:39.400
really made me happy, like you learn a lot about how computers work.


00:09:39.400 --> 00:09:44.800
I had a new renaissance of the first experience with Python, another summer


00:09:44.800 --> 00:09:48.600
and I've been doing a lot of toy projects,


00:09:48.600 --> 00:09:52.320
like writing an interpreter.


00:09:52.320 --> 00:09:53.760
I don't know, a lot of projects.


00:09:53.760 --> 00:09:57.280
And Polar became one of those hobby projects


00:09:57.280 --> 00:10:01.200
just to use Raspbob.


00:10:01.200 --> 00:10:04.160
>>Now it's got quite the following,


00:10:04.160 --> 00:10:06.160
and we're going to definitely dive into that.


00:10:06.160 --> 00:10:08.920
But let me pull it up.


00:10:08.920 --> 00:10:11.440
It says right here, 13,000 GitHub stars.


00:10:11.440 --> 00:10:16.440
That's a good number of people using that project.


00:10:16.440 --> 00:10:19.080
That's pretty crazy, isn't it?


00:10:19.080 --> 00:10:19.920
- Yeah, it is.


00:10:19.920 --> 00:10:20.760
It is.


00:10:20.760 --> 00:10:22.100
It's on GitHub stars,


00:10:22.100 --> 00:10:24.700
it's the fastest growing data tool, I believe.


00:10:24.700 --> 00:10:26.020
- Wow, incredible.


00:10:26.020 --> 00:10:27.180
- Yeah.


00:10:27.180 --> 00:10:29.500
- You must be really proud of that.


00:10:29.500 --> 00:10:30.820
- Yeah, yeah.


00:10:30.820 --> 00:10:33.220
If you would have told me this two years ago,


00:10:33.220 --> 00:10:34.420
I wouldn't have believed it,


00:10:34.420 --> 00:10:39.420
but it happens slow enough so you can get accustomed to that.


00:10:40.820 --> 00:10:43.700
Yeah, that's cool.


00:10:43.700 --> 00:10:45.580
Kind of like being a parent.


00:10:45.580 --> 00:10:47.900
The challenges of the kids are small.


00:10:47.900 --> 00:10:50.260
They're intense, but there are only a few things


00:10:50.260 --> 00:10:54.500
they need when they're small, and you kind of grow with it.


00:10:54.500 --> 00:10:56.100
So a couple of thoughts.


00:10:56.100 --> 00:11:01.500
One, you had the inverse style of learning to program


00:11:01.500 --> 00:11:03.700
that I think a lot of computer science people do,


00:11:03.700 --> 00:11:05.860
and certainly that I did.


00:11:05.860 --> 00:11:08.820
It could also just be that I learned it a long time ago.


00:11:08.820 --> 00:11:12.280
But when I learned programming, it was,


00:11:12.280 --> 00:11:15.140
I'm gonna learn C and C++.


00:11:15.140 --> 00:11:17.520
And then you're kind of allowed to learn


00:11:17.520 --> 00:11:20.800
the easier languages, but you will learn your pointers.


00:11:20.800 --> 00:11:23.840
You'll have your void star star, and you're gonna like it.


00:11:23.840 --> 00:11:25.920
You're gonna understand what a pointer to a pointer means.


00:11:25.920 --> 00:11:28.200
And we're gonna get, I mean, you know,


00:11:28.200 --> 00:11:32.580
you start inside and you, of the most complex,


00:11:32.580 --> 00:11:34.400
closest to the machine, you work your way out.


00:11:34.400 --> 00:11:35.880
You kind of took this opposite, like,


00:11:35.880 --> 00:11:38.800
let me learn Python where it's much more high level


00:11:38.800 --> 00:11:44.320
it's much, you know, if you choose to be often stay very much more away from the hardware and


00:11:44.320 --> 00:11:49.920
the ideas of memories and threads and all that. And then you went to Rust. So was it kind of an


00:11:49.920 --> 00:11:55.360
intense experience? You're like, oh my gosh, this is intense. Or had you studied enough languages by


00:11:55.360 --> 00:12:05.040
then to become comfortable? Well, yeah, no. So the going from high level to low level, I think


00:12:05.680 --> 00:12:12.480
makes natural sense if you learn it yourself. There's no professor telling me you learn your


00:12:12.480 --> 00:12:19.120
pointers. So, I think this also helps a lot because at that point you're really accustomed


00:12:19.120 --> 00:12:26.400
to programming, to algorithms. So, I believe you should learn one thing, one new thing at a time,


00:12:26.400 --> 00:12:30.400
and then you can really own that knowledge later on.


00:12:30.400 --> 00:12:32.400
But Rust...


00:12:32.400 --> 00:12:37.400
I wouldn't say you should learn Rust as a first language.


00:12:37.400 --> 00:12:40.400
It would be really terrible, because you need...


00:12:40.400 --> 00:12:42.400
That would be terrible.


00:12:42.400 --> 00:12:46.400
But other languages also don't help you much.


00:12:46.400 --> 00:12:49.400
Because the borrow checker is quite unique.


00:12:49.400 --> 00:12:52.400
It doesn't let you do things you can do in other languages.


00:12:52.400 --> 00:12:58.160
what you learned there, the languages that allow you to do that, they just hurt you.


00:12:58.160 --> 00:13:00.800
Because you were...


00:13:00.800 --> 00:13:04.160
They encourage the wrong behavior, right?


00:13:04.160 --> 00:13:12.400
Yeah. So, nine out of ten times, it turns out by compiling and not letting you do that one thing,


00:13:12.400 --> 00:13:15.360
that one thing you wanted was probably really bad to begin with.


00:13:15.360 --> 00:13:20.360
Laptop rarely... So in Rust your code is always a lot flatter.


00:13:20.360 --> 00:13:25.360
It's always really clear who owns the memory, how deep your nesting is.


00:13:25.360 --> 00:13:31.360
It's always one D deeper. Most of the times it's not that complicated.


00:13:31.360 --> 00:13:36.360
You make things really flat and really easy to reason about.


00:13:36.360 --> 00:13:41.360
And in the beginning of a project it seems okay, a bit over-constraining.


00:13:41.360 --> 00:13:45.920
But when I mean software will become complex and complicated,


00:13:45.920 --> 00:13:49.320
and then you're happy that the compiler not doing this.


00:13:49.320 --> 00:13:51.080
>> Yeah, absolutely.


00:13:51.080 --> 00:13:52.320
>> In this direction.


00:13:52.320 --> 00:13:54.320
>> It seems like a better way, honestly.


00:13:54.320 --> 00:13:59.400
You get a sense of programming in a more simple language that


00:13:59.400 --> 00:14:03.620
doesn't ask so many low-level concepts of you,


00:14:03.620 --> 00:14:06.480
and then you can add on these new ones.


00:14:06.480 --> 00:14:09.760
I feel like a lot of how we teach programming,


00:14:09.760 --> 00:14:11.720
and how people learn programming is a little bit backwards,


00:14:11.720 --> 00:14:12.560
to be honest.


00:14:12.560 --> 00:14:15.020
Anyway, enough on that.


00:14:15.020 --> 00:14:17.400
So you were a civil engineer for a while,


00:14:17.400 --> 00:14:19.120
and then you became a data scientist


00:14:19.120 --> 00:14:20.920
and now you've created this library.


00:14:20.920 --> 00:14:24.620
Still working as a data scientist now?


00:14:24.620 --> 00:14:26.320
- No, no.


00:14:26.320 --> 00:14:29.960
I got sponsored two years ago for two days a week.


00:14:29.960 --> 00:14:36.680
And yeah, just use the time to develop Polar.


00:14:37.560 --> 00:14:40.880
And currently I stopped all my day jobs


00:14:40.880 --> 00:14:44.320
and I'm going full-time on other side


00:14:44.320 --> 00:14:45.920
trying to live from sponsorships,


00:14:45.920 --> 00:14:48.960
which is not really working.


00:14:48.960 --> 00:14:50.480
It's not enough at this time,


00:14:50.480 --> 00:14:51.880
but I hope to start a foundation


00:14:51.880 --> 00:14:54.760
and get some proper sponsors in.


00:14:54.760 --> 00:14:55.920
- Yeah, that'd be great.


00:14:55.920 --> 00:14:59.320
That's awesome.


00:14:59.320 --> 00:15:01.520
It's still awesome that you're able to do that


00:15:01.520 --> 00:15:05.440
even if you still needed to grow a little bit.


00:15:05.440 --> 00:15:06.720
Yeah, we'll have you on a podcast


00:15:06.720 --> 00:15:08.180
and let other people know out there


00:15:08.180 --> 00:15:10.820
who maybe are using your library.


00:15:10.820 --> 00:15:13.160
Maybe they could put a little sponsorship


00:15:13.160 --> 00:15:14.560
in GitHub sponsors.


00:15:14.560 --> 00:15:16.860
I feel like GitHub sponsors


00:15:16.860 --> 00:15:21.620
really made it a lot easier for people to support.


00:15:21.620 --> 00:15:24.800
'Cause there used to be like PayPal donate buttons


00:15:24.800 --> 00:15:27.640
and other things like that.


00:15:27.640 --> 00:15:30.200
And one, those are not really recurring.


00:15:30.200 --> 00:15:32.360
And two, you've got to go find some place


00:15:32.360 --> 00:15:33.840
and put your credit card.


00:15:33.840 --> 00:15:35.640
Many of us already have a credit card


00:15:35.640 --> 00:15:36.840
registered at GitHub.


00:15:36.840 --> 00:15:38.320
It's just a matter of checking the box


00:15:38.320 --> 00:15:40.360
and monthly it'll just go.


00:15:40.360 --> 00:15:41.720
You know, it's kind of like the app store


00:15:41.720 --> 00:15:43.120
versus buying independent apps.


00:15:43.120 --> 00:15:45.280
It just cuts down a lot of friction.


00:15:45.280 --> 00:15:47.000
I feel like it's been really positive


00:15:47.000 --> 00:15:49.720
mostly for open source.


00:15:49.720 --> 00:15:54.720
- Yeah, I think it's good as a way to say thank you.


00:15:54.720 --> 00:15:57.240
It isn't enough to pay the bills.


00:15:57.240 --> 00:16:00.520
I think for most developers it isn't,


00:16:00.520 --> 00:16:03.120
but I hope we get there.


00:16:03.120 --> 00:16:09.200
companies who use it should give a bit more back. I mean, they have a lot of money.


00:16:09.200 --> 00:16:16.480
I agree. It's really, really ridiculous that there are banks and VC funded companies and


00:16:16.480 --> 00:16:22.640
things like that, that have, not necessarily in terms of the VC ones, but definitely in terms of


00:16:22.640 --> 00:16:27.840
financial and other large companies that make billions and billions of dollars in profit on


00:16:27.840 --> 00:16:30.120
on top of open source technology.


00:16:30.120 --> 00:16:32.320
And many of them don't give anything back,


00:16:32.320 --> 00:16:36.280
which is, I don't know, it's not criminal


00:16:36.280 --> 00:16:38.020
because the licenses allow it,


00:16:38.020 --> 00:16:41.360
but it's certainly borders on immoral to say,


00:16:41.360 --> 00:16:42.540
we're gonna make all this money


00:16:42.540 --> 00:16:45.520
and not at all support the people


00:16:45.520 --> 00:16:48.600
who are really building the foundations that we build upon.


00:16:48.600 --> 00:16:52.160
- Most of my sponsors are developers.


00:16:52.160 --> 00:16:53.440
- Yeah.


00:16:53.440 --> 00:16:54.260
- Yeah.


00:16:54.260 --> 00:16:57.800
So yeah, let's hope it changes.


00:16:57.800 --> 00:17:01.800
- Yeah, well, I'll continue to beat that drum,


00:17:01.800 --> 00:17:03.880
but let's talk about your project.


00:17:03.880 --> 00:17:08.880
So Polars and the RS is for Rust,


00:17:08.880 --> 00:17:11.240
I imagine at the end,


00:17:11.240 --> 00:17:12.840
but tell us about the name Polars,


00:17:12.840 --> 00:17:14.720
like Polar Bear, but Polars.


00:17:14.720 --> 00:17:20.320
- Yeah, so I started writing a data from library


00:17:20.320 --> 00:17:24.520
and initially it was only for Rust,


00:17:24.520 --> 00:17:26.200
was my idea until.


00:17:26.200 --> 00:17:29.320
- Until you saw all the people doing data science


00:17:29.320 --> 00:17:31.240
and Python, you're like, well,


00:17:31.240 --> 00:17:33.160
what can I do for these people, right?


00:17:33.160 --> 00:17:34.000
- Yeah, yeah.


00:17:34.000 --> 00:17:39.680
And I wanted to give a wink to the Pandas project,


00:17:39.680 --> 00:17:42.080
but I wanted a beer that was better, faster,


00:17:42.080 --> 00:17:43.400
I don't know, stronger.


00:17:43.400 --> 00:17:48.400
So luckily a Panda beer isn't the most frightful beer.


00:17:48.400 --> 00:17:52.160
So I had a few to choose,


00:17:52.160 --> 00:17:55.080
But the grizzly, yeah, the polar has the RS.


00:17:55.080 --> 00:17:57.260
So that's a lucky coincidence.


00:17:57.260 --> 00:17:58.100
Yeah.


00:17:58.100 --> 00:17:59.680
- Yeah.


00:17:59.680 --> 00:18:01.080
Yeah, so the subtitle here is


00:18:01.080 --> 00:18:05.440
lightning fast data frame library for Rust and Python.


00:18:05.440 --> 00:18:08.140
And you have two APIs that people can use.


00:18:08.140 --> 00:18:10.400
We'll get to dive into those.


00:18:10.400 --> 00:18:11.240
- Yeah.


00:18:11.240 --> 00:18:13.040
- I guess it's worth maybe.


00:18:13.040 --> 00:18:14.820
- Correct anymore, I'd say.


00:18:14.820 --> 00:18:19.600
I mean, we're, it's just we're written in Rust


00:18:19.600 --> 00:18:23.100
And it's a complete different library in Rust


00:18:23.100 --> 00:18:25.060
and you can expose it to many frontends.


00:18:25.060 --> 00:18:27.700
So you've got DLT already, frontend in Rust,


00:18:27.700 --> 00:18:31.500
Python, Node.js, R is coming up


00:18:31.500 --> 00:18:35.660
and normal JavaScript is coming up and Ruby.


00:18:35.660 --> 00:18:38.260
There is also a Polart and Ruby.


00:18:38.260 --> 00:18:39.480
- So interesting.


00:18:39.480 --> 00:18:43.400
So for the JavaScript one, are you gonna use WebAssembly?


00:18:43.400 --> 00:18:44.480
- Yeah.


00:18:44.480 --> 00:18:45.900
- Right, which is pretty straightforward


00:18:45.900 --> 00:18:48.260
'cause Rust comes from Mozilla.


00:18:48.260 --> 00:18:50.580
WebAssembly, I believe also originated,


00:18:50.580 --> 00:18:54.340
they kind of originated as a somewhat tied together store.


00:18:54.340 --> 00:18:58.540
- Yeah, so Rust C++ C can compile to WebAssembly.


00:18:58.540 --> 00:19:00.140
It's not really straightforward


00:19:00.140 --> 00:19:02.100
because the WebAssembly virtual machine


00:19:02.100 --> 00:19:04.580
isn't like your normal OS.


00:19:04.580 --> 00:19:07.220
So there are a lot of things harder,


00:19:07.220 --> 00:19:10.100
but we are working on the challenges there.


00:19:10.100 --> 00:19:11.780
- Okay, well, that's pretty interesting,


00:19:11.780 --> 00:19:14.900
but for now you got Python and you've got Rust


00:19:14.900 --> 00:19:16.220
and that's great.


00:19:16.220 --> 00:19:19.260
Let's, I think a lot of people listening,


00:19:19.260 --> 00:19:22.140
myself included when I started looking into this,


00:19:22.140 --> 00:19:26.180
immediately go to, it's like pandas, but rust.


00:19:26.180 --> 00:19:30.280
It's like pandas, but instead of C at the bottom,


00:19:30.280 --> 00:19:32.100
it's rust at the bottom.


00:19:32.100 --> 00:19:35.540
And that's somewhat true, but mostly not true.


00:19:35.540 --> 00:19:38.140
So let's start with you telling us,


00:19:38.140 --> 00:19:39.820
how is this like pandas


00:19:39.820 --> 00:19:41.620
and how is it different from pandas?


00:19:42.560 --> 00:19:45.920
- Yeah, so it's not like Pandas.


00:19:45.920 --> 00:19:50.340
I think it's different on two ways.


00:19:50.340 --> 00:19:53.540
So we have the API and we have the implementation.


00:19:53.540 --> 00:19:55.340
And which one should I start with?


00:19:55.340 --> 00:19:57.580
Bottom up?


00:19:57.580 --> 00:19:59.120
- Yeah, bottom up, sure.


00:19:59.120 --> 00:20:00.300
All right, let's do it.


00:20:00.300 --> 00:20:04.340
- So that was my critique from Pandas


00:20:04.340 --> 00:20:07.500
and that they didn't start bottom up.


00:20:07.500 --> 00:20:10.280
They took whatever was there already,


00:20:10.280 --> 00:20:12.860
which work good for that purpose.


00:20:12.860 --> 00:20:15.900
And Panda's built on NumPy.


00:20:15.900 --> 00:20:17.700
And NumPy is a great library,


00:20:17.700 --> 00:20:20.820
but it's built for numerical processing


00:20:20.820 --> 00:20:23.500
and not for relational processing.


00:20:23.500 --> 00:20:25.780
Relational data is completely different.


00:20:25.780 --> 00:20:28.140
You have string data, you have message data,


00:20:28.140 --> 00:20:31.980
and this data is currently just put as Python object


00:20:31.980 --> 00:20:34.380
in those NumPy arrays.


00:20:34.380 --> 00:20:38.340
And if you know anything about memory,


00:20:38.340 --> 00:20:40.880
then in this array you have a pointer


00:20:40.880 --> 00:20:43.580
with where each Python object is somewhere else.


00:20:43.580 --> 00:20:45.240
So if you traverse this memory,


00:20:45.240 --> 00:20:47.880
every pointer you hit, you must look it up somewhere else.


00:20:47.880 --> 00:20:49.040
That memory is not in cache,


00:20:49.040 --> 00:20:50.580
so we have a cache miss,


00:20:50.580 --> 00:20:55.220
which is a 200x slowdown per element you traverse.


00:20:55.220 --> 00:20:56.500
So this is--


00:20:56.500 --> 00:20:57.900
- So for people listening,


00:20:57.900 --> 00:21:00.380
what you're saying the 200x slowdown is,


00:21:00.380 --> 00:21:02.840
the L1, L2, L3 caches,


00:21:02.840 --> 00:21:04.200
which all have different speeds and stuff,


00:21:04.200 --> 00:21:06.900
but the caches that are near the CPU


00:21:06.900 --> 00:21:10.980
versus main memory, it's like 200 to 400 times slower.


00:21:10.980 --> 00:21:13.380
Not paging off a disk or something.


00:21:13.380 --> 00:21:14.460
It's really different, right?


00:21:14.460 --> 00:21:16.180
It's really a big deal.


00:21:16.180 --> 00:21:17.020
- It's a big deal.


00:21:17.020 --> 00:21:18.400
It's terribly slow.


00:21:18.400 --> 00:21:22.060
It also, Python has a GIL.


00:21:22.060 --> 00:21:24.980
It also blocks multi-threading.


00:21:24.980 --> 00:21:26.460
If you want to read the string,


00:21:26.460 --> 00:21:29.260
you cannot do this from different threads.


00:21:29.260 --> 00:21:30.340
If you want to modify the string,


00:21:30.340 --> 00:21:33.060
there's only one thread that connects this Python GIL.


00:21:35.580 --> 00:21:40.580
And they also didn't take into account


00:21:40.580 --> 00:21:42.740
anything from databases.


00:21:42.740 --> 00:21:47.740
So databases are basing from the 1950s.


00:21:47.740 --> 00:21:50.820
There's been a lot of research in databases


00:21:50.820 --> 00:21:54.660
and how we do things fast, how we write a query


00:21:54.660 --> 00:21:56.780
and then optimize this query because the user


00:21:56.780 --> 00:21:59.620
that uses your library is not the expert.


00:21:59.620 --> 00:22:01.620
It doesn't write optimized queries.


00:22:01.620 --> 00:22:03.500
No, but we have a lot of information


00:22:03.500 --> 00:22:06.660
so we can optimize this query and execute this


00:22:06.660 --> 00:22:10.080
in the most, in a very efficient way.


00:22:10.080 --> 00:22:11.740
- That's an interesting idea.


00:22:11.740 --> 00:22:16.380
- Yeah, but Apondos just executes it


00:22:16.380 --> 00:22:17.860
and gives you what you ask.


00:22:17.860 --> 00:22:20.820
And what you ask is probably not the best thing.


00:22:20.820 --> 00:22:23.420
- Yeah, that's interesting because as programmers,


00:22:23.420 --> 00:22:26.700
as when I have my Python hat on,


00:22:26.700 --> 00:22:29.500
I want my code to run exactly as I wrote it.


00:22:29.500 --> 00:22:33.820
I don't want it to get clever and change it.


00:22:33.820 --> 00:22:35.800
If I said do a loop, do a loop.


00:22:35.800 --> 00:22:39.340
If I said put it in a dictionary, put it in a dictionary.


00:22:39.340 --> 00:22:41.560
But when I write a database query,


00:22:41.560 --> 00:22:46.140
be that against Postgres with relational or MongoDB,


00:22:46.140 --> 00:22:48.200
there's a query planner.


00:22:48.200 --> 00:22:51.200
And the query planner looks at all the different steps.


00:22:51.200 --> 00:22:52.660
Should we do the filter first?


00:22:52.660 --> 00:22:53.740
Can we use an index?


00:22:53.740 --> 00:22:55.020
Can we use a compo?


00:22:55.020 --> 00:22:57.460
Which index should we choose?


00:22:57.460 --> 00:22:59.020
All of those things, right?


00:22:59.020 --> 00:23:01.840
And so what you tell it and what happens,


00:23:01.840 --> 00:23:04.860
you don't tell it how to do finding the data,


00:23:04.860 --> 00:23:06.880
the database, you just give it,


00:23:06.880 --> 00:23:09.460
here's kind of the expressions that I need,


00:23:09.460 --> 00:23:12.220
the predicates that I need you to work with.


00:23:12.220 --> 00:23:14.060
And then you figure it out, you're smart,


00:23:14.060 --> 00:23:15.500
you're the database.


00:23:15.500 --> 00:23:18.180
So one of the differences I got from reading


00:23:18.180 --> 00:23:21.620
what you've got here so far is it looks like,


00:23:21.620 --> 00:23:25.540
I don't know if it goes as far as this database stuff


00:23:25.540 --> 00:23:26.380
that we're talking about,


00:23:26.380 --> 00:23:31.820
there's a way for it to build up the code it's supposed to run and it can decide things like,


00:23:31.820 --> 00:23:36.620
you know, these two things could go in parallel or things along those lines, right?


00:23:36.620 --> 00:23:42.300
>> Yeah, yeah. Well, it is actually very similar. It is a vectorized query engine. And you can,


00:23:42.300 --> 00:23:48.620
the only thing that doesn't make us a database is that we don't have any, we don't bother with


00:23:48.620 --> 00:23:53.820
file structures. >> Right, like the persistence


00:23:53.820 --> 00:23:55.500
and transactions and all that?


00:23:55.500 --> 00:23:57.900
Yeah, so we have different kinds of databases.


00:23:57.900 --> 00:24:01.340
You have OLAP and OLTP, transactional modeling,


00:24:01.340 --> 00:24:03.740
which works often on one.


00:24:03.740 --> 00:24:07.580
So if you do a REST API query and you modify one user ID,


00:24:07.580 --> 00:24:09.260
then you're transactional.


00:24:09.260 --> 00:24:11.580
And if you do OLAP, that's more analytical.


00:24:11.580 --> 00:24:16.300
And then you do large aggregations of large whole tables.


00:24:16.300 --> 00:24:17.820
And then you need to process all the data.


00:24:17.820 --> 00:24:20.460
And those different database designs


00:24:20.460 --> 00:24:23.500
lead to different query optimizers.


00:24:23.500 --> 00:24:30.620
and Polis is focused on Ola. But yeah, we... So, as you described, you've got two ways of


00:24:30.620 --> 00:24:36.700
programming things. One is procedural, which Python mostly is. You tell exactly if you want


00:24:36.700 --> 00:24:42.380
to get a cup of coffee, how many steps it should take forward, then rotate 90 degrees, take three


00:24:42.380 --> 00:24:49.340
steps, then rotate 90 degrees. You can put right down the whole algorithm how to get a coffee,


00:24:49.340 --> 00:24:57.100
You could just say, "Get me a coffee and I'd like some sugar," and then let the query engine


00:24:57.100 --> 00:25:01.740
decide how to best get it done.


00:25:01.740 --> 00:25:03.320
And that's more declarative.


00:25:03.320 --> 00:25:07.380
You describe the end result.


00:25:07.380 --> 00:25:11.700
And as it turns out, this is also very readable because you declare what you want and the


00:25:11.700 --> 00:25:16.540
intent is readable in the query.


00:25:16.540 --> 00:25:19.200
And if you're doing more procedural programming,


00:25:19.200 --> 00:25:20.900
you describe what you're doing.


00:25:20.900 --> 00:25:24.020
And the intent often needs to come from comments,


00:25:24.020 --> 00:25:27.820
like what are we trying to do when we follow this out?


00:25:27.820 --> 00:25:29.860
- Right, yeah, that makes a lot of sense.


00:25:29.860 --> 00:25:31.960
Very interesting.


00:25:31.960 --> 00:25:33.960
- That's why, yeah, sorry.


00:25:33.960 --> 00:25:37.060
And that's why the, so the first thing is,


00:25:37.060 --> 00:25:41.260
we write a database engine, a query engine from scratch,


00:25:41.260 --> 00:25:45.660
and really think about multiprocessing,


00:25:45.660 --> 00:25:49.440
about caches, also out of core,


00:25:49.440 --> 00:25:51.900
we can process data that doesn't fit into memory.


00:25:51.900 --> 00:25:56.700
So we really built this from scratch


00:25:56.700 --> 00:25:57.840
with all those things in mind.


00:25:57.840 --> 00:26:02.840
And then at first we wanted to expose the Pondus API


00:26:02.840 --> 00:26:08.960
and then we noticed how bad it was for writing fast data.


00:26:08.960 --> 00:26:11.800
The Pondus API just isn't really good


00:26:11.800 --> 00:26:15.620
for this declarative analyzing of what the user wants to do.


00:26:15.620 --> 00:26:19.380
So we just cut it off and took the freedom


00:26:19.380 --> 00:26:21.580
to design an API that makes most sense.


00:26:21.580 --> 00:26:22.580
>>That's interesting.


00:26:22.580 --> 00:26:24.300
I didn't realize that you had started


00:26:24.300 --> 00:26:28.380
trying to be closer to pandas than you ended up.


00:26:28.380 --> 00:26:33.140
>>Yeah, well, it was very short-lived, I must say.


00:26:33.140 --> 00:26:35.900
>>Yeah, and that's not necessarily saying pandas


00:26:35.900 --> 00:26:37.140
are bad, I don't think.


00:26:37.140 --> 00:26:39.540
It's approaching the problem differently,


00:26:39.540 --> 00:26:42.420
and it has different goals, right?


00:26:42.420 --> 00:26:43.140
>>Yeah.


00:26:43.140 --> 00:26:45.460
Yeah.


00:26:45.460 --> 00:26:46.300
- Yeah.


00:26:46.300 --> 00:26:49.940
- So maybe we could look at...


00:26:49.940 --> 00:26:50.780
- Yeah.


00:26:50.780 --> 00:26:51.620
- Yeah.


00:26:51.620 --> 00:26:52.900
Yeah, maybe we could look at an example


00:26:52.900 --> 00:26:56.060
of some of the code that we're talking about.


00:26:56.060 --> 00:27:00.100
I guess also one of the other differences there is


00:27:00.100 --> 00:27:04.700
much of this has to do with what you would call,


00:27:04.700 --> 00:27:08.700
I guess you refer to them as lazy APIs or streaming APIs,


00:27:08.700 --> 00:27:10.080
kind of like a generator.


00:27:10.080 --> 00:27:13.140
- Yeah.


00:27:13.140 --> 00:27:16.580
If you think about a join, for instance, in Pandas,


00:27:16.580 --> 00:27:22.020
if you would write a join and then take only one to the first 100 rows


00:27:22.020 --> 00:27:25.340
of that result, then it would first do the join,


00:27:25.340 --> 00:27:29.340
and then that might produce 1 million or 10 million rows.


00:27:29.340 --> 00:27:34.540
And then you take only 100 of them, and then you have materialized a million,


00:27:34.540 --> 00:27:35.980
but you take only a fraction of that.


00:27:35.980 --> 00:27:38.540
And by having that lazy, you can...


00:27:41.140 --> 00:27:43.660
you can optimize for the whole query at a time


00:27:43.660 --> 00:27:45.540
and just see how we do this join,


00:27:45.540 --> 00:27:46.820
but we only need 100 rows.


00:27:46.820 --> 00:27:49.340
So that's how we materialize 100 rows.


00:27:49.340 --> 00:27:51.300
So it gets more of a holistic approach.


00:27:51.300 --> 00:27:54.140
- That's really cool.


00:27:54.140 --> 00:27:57.980
I didn't realize it had so many similarities to databases,


00:27:57.980 --> 00:28:00.260
but yeah, it makes a lot of sense.


00:28:00.260 --> 00:28:05.260
All right, let's look at maybe a super simple example.


00:28:05.260 --> 00:28:08.540
You've got on polar.rs.


00:28:08.540 --> 00:28:09.860
What country is RS?


00:28:09.860 --> 00:28:14.620
I always love how different countries that often have nothing to do with domain names


00:28:14.620 --> 00:28:19.660
get grabbed because they have a cool ending like Libya.ly for a while.


00:28:19.660 --> 00:28:23.300
It still is, but it was used frequently, like Bitly and stuff.


00:28:23.300 --> 00:28:24.740
Do you know what RS is?


00:28:24.740 --> 00:28:26.220
I believe it's Serbia.


00:28:26.220 --> 00:28:27.220
Serbia.


00:28:27.220 --> 00:28:28.220
Okay, cool.


00:28:28.220 --> 00:28:29.220
I'm not sure.


00:28:29.220 --> 00:28:30.900
Yeah, yeah, very cool.


00:28:30.900 --> 00:28:36.180
All right, so polar.rs, like polar.rs.


00:28:36.180 --> 00:28:41.460
Over here, you've got on the homepage here, the landing page, and then through the documentation


00:28:41.460 --> 00:28:44.580
as well, you've got a lot of places where you're like, "Show me the Rust API or show


00:28:44.580 --> 00:28:47.900
me the Python API."


00:28:47.900 --> 00:28:50.020
People can come and check out the Rust code.


00:28:50.020 --> 00:28:56.020
It's a little bit longer because it's that kind of language, but it's not terribly more


00:28:56.020 --> 00:28:57.700
complex.


00:28:57.700 --> 00:29:02.620
Maybe talk us through this little example here on the homepage in Python, just to give


00:29:02.620 --> 00:29:06.380
give people a sense of what the API looks like.


00:29:06.380 --> 00:29:12.460
Yeah, so we start with a scanned CSV, which is a lazy read,


00:29:12.460 --> 00:29:14.500
which is--


00:29:14.500 --> 00:29:17.700
so a read CSV tells what you do, and then it reads the CSV,


00:29:17.700 --> 00:29:19.540
and you get the data frame.


00:29:19.540 --> 00:29:23.300
In a scanned CSV, we start a computation graph.


00:29:23.300 --> 00:29:24.500
We call this a lazy frame.


00:29:24.500 --> 00:29:27.740
A lazy frame is actually just--


00:29:27.740 --> 00:29:31.720
it holds-- it remembers the steps of the operations


00:29:31.720 --> 00:29:34.640
you want to do, then it sends a boilerplate


00:29:34.640 --> 00:29:37.360
that looks at this very plan and optimize it


00:29:37.360 --> 00:29:40.520
and it will think of how to execute it.


00:29:40.520 --> 00:29:43.080
And we have different engines, so you can have an engine


00:29:43.080 --> 00:29:45.760
that's more specialized for data that doesn't fit


00:29:45.760 --> 00:29:47.920
into memory and an engine that's more specialized


00:29:47.920 --> 00:29:49.680
for data that does fit into memory.


00:29:49.680 --> 00:29:55.560
So we start with a scan and then we do a dot filter


00:29:55.560 --> 00:30:00.160
and we want to use verbs.


00:30:00.160 --> 00:30:02.660
that's the declarative part.


00:30:02.660 --> 00:30:04.940
In pandas we often do indexes.


00:30:04.940 --> 00:30:09.780
And those indexes are ambiguous in my opinion


00:30:09.780 --> 00:30:14.180
because you can pause in a NumPy array with booleans,


00:30:14.180 --> 00:30:17.620
but you can also pause in a NumPy array with integers.


00:30:17.620 --> 00:30:21.100
So you can do slicing, you can also pause in a NumPy,


00:30:21.100 --> 00:30:23.620
a list of strings, and then you do column selection.


00:30:23.620 --> 00:30:25.980
So it has three functions.


00:30:25.980 --> 00:30:28.920
- One thing that I find really interesting about pandas


00:30:28.920 --> 00:30:33.240
is it's so incredible and people who are very good with pandas,


00:30:33.240 --> 00:30:35.720
they can just make it fly.


00:30:35.720 --> 00:30:37.640
They can make it really right expressions


00:30:37.640 --> 00:30:39.280
that are super powerful.


00:30:39.280 --> 00:30:41.240
But it's not obvious that you should


00:30:41.240 --> 00:30:43.840
have been able to do that before you see it.


00:30:43.840 --> 00:30:46.680
There's a lot of not quite magic,


00:30:46.680 --> 00:30:50.060
but stuff that doesn't seem to come really straight out


00:30:50.060 --> 00:30:52.120
of the API directly.


00:30:52.120 --> 00:30:56.680
You pass in some sort of a Boolean expression


00:30:56.680 --> 00:31:01.680
that involves a vector and some other test into the brackets.


00:31:01.680 --> 00:31:04.000
Like, wait, how did I know I could do that?


00:31:04.000 --> 00:31:07.760
Whereas this, your API is a lot more of a fluent API


00:31:07.760 --> 00:31:11.240
where you say, you know, instead of PD,


00:31:11.240 --> 00:31:18.520
you'd say PL, PL.scan, CSV.filter.groupby.aggregate.collect.


00:31:18.520 --> 00:31:21.880
And it kind of just flows together.


00:31:21.880 --> 00:31:24.840
Does that mean that the editors and IDEs


00:31:24.840 --> 00:31:28.200
can be more helpful suggesting what happens at each step?


00:31:28.200 --> 00:31:34.940
- Yes, we are really strict on type.


00:31:34.940 --> 00:31:39.520
So we also only return a single type from a method.


00:31:39.520 --> 00:31:44.520
And we only, a dot filter just expects a Boolean expression


00:31:44.520 --> 00:31:47.860
that produces a Boolean, not an integer, not a string.


00:31:47.860 --> 00:31:52.320
So we want our methods to, from reading our code,


00:31:52.320 --> 00:31:56.600
you should be able to understand what you're doing. That's really


00:31:56.600 --> 00:31:59.800
important to me. It should be unambiguous, it should be


00:31:59.800 --> 00:32:02.480
consistent, and you your knowledge of the API should


00:32:02.480 --> 00:32:09.320
expand to different parts of the API. Right. And that's where I


00:32:09.320 --> 00:32:11.080
think we're going to talk about this later. But that's where


00:32:11.080 --> 00:32:12.640
expressions really come in.


00:32:12.640 --> 00:32:21.440
Yep. Okay, so I just derailed you a little bit here, as you're


00:32:21.440 --> 00:32:22.280
describing this.


00:32:22.280 --> 00:32:25.760
So you start out with scanning a CSV,


00:32:25.760 --> 00:32:29.600
which is sort of creating and kicking off a data frame


00:32:29.600 --> 00:32:31.000
equivalent here.


00:32:31.000 --> 00:32:32.560
And then you- - A lazy frame.


00:32:32.560 --> 00:32:33.520
- A lazy frame, okay.


00:32:33.520 --> 00:32:36.200
And then you say a dot filter and you give it an expression


00:32:36.200 --> 00:32:40.840
like this column is greater than five, right?


00:32:40.840 --> 00:32:44.560
Or some expression that we would understand in Python.


00:32:44.560 --> 00:32:46.960
And that's the filter statement, right?


00:32:46.960 --> 00:32:48.160
- Yeah.


00:32:48.160 --> 00:32:51.400
Yeah, and then we follow it a group by argument


00:32:51.400 --> 00:32:53.400
and then an aggregation where we say,


00:32:53.400 --> 00:32:56.240
okay, take all columns and sum them.


00:32:56.240 --> 00:32:57.600
And this again is an expression.


00:32:57.600 --> 00:33:00.460
These are really easy expressions.


00:33:00.460 --> 00:33:04.040
And then we take this lazy frame


00:33:04.040 --> 00:33:06.120
and we materialize it into a data frame


00:33:06.120 --> 00:33:07.560
that can collect on it.


00:33:07.560 --> 00:33:11.200
And collect means, okay, all those steps you recorded,


00:33:11.200 --> 00:33:14.760
now you can do your magic, query optimizer,


00:33:14.760 --> 00:33:18.000
get all the stuff.


00:33:18.000 --> 00:33:21.120
And what this will do here, it will recognize that,


00:33:21.120 --> 00:33:25.580
Okay, we've taken the iris.csv, which got different columns.


00:33:25.580 --> 00:33:29.460
I know in this case it won't.


00:33:29.460 --> 00:33:31.700
So if you would have finished with a select


00:33:31.700 --> 00:33:33.260
where we only select a few columns,


00:33:33.260 --> 00:33:35.020
it would have recognized,


00:33:35.020 --> 00:33:38.460
oh, we don't need all those columns in the csv.


00:33:38.460 --> 00:33:40.740
- I see. - So we only take the ones we need.


00:33:40.740 --> 00:33:43.220
What it will do, it will push the filter,


00:33:43.220 --> 00:33:45.620
the predicate, down to the scan.


00:33:45.620 --> 00:33:48.340
So during the reading of the csv,


00:33:48.340 --> 00:33:50.180
we will take this predicate.


00:33:50.180 --> 00:33:53.380
we say, okay, the sample length is larger than five,


00:33:53.380 --> 00:33:55.180
the rows that don't match this predicate


00:33:55.180 --> 00:33:56.960
will not be materialized.


00:33:56.960 --> 00:33:59.740
So if you have a really large CSV file,


00:33:59.740 --> 00:34:00.580
this will really,


00:34:00.580 --> 00:34:05.380
let's say you have a CSV file of tens of gigabytes,


00:34:05.380 --> 00:34:08.460
but your predicate only selects 5% of that,


00:34:08.460 --> 00:34:12.700
then you only materialize 5% of the 10 gigabytes.


00:34:12.700 --> 00:34:16.940
- Yeah, so 500 megs instead of 10 gigabytes


00:34:16.940 --> 00:34:19.460
or something like that, or 200 megs,


00:34:19.460 --> 00:34:21.460
whatever it is, quite a bit less.


00:34:21.460 --> 00:34:22.820
That's really interesting.


00:34:22.820 --> 00:34:25.480
And this is all part of the benefits


00:34:25.480 --> 00:34:27.920
of what we were talking about with the lazy,


00:34:27.920 --> 00:34:29.820
lazy frames, lazy APIs,


00:34:29.820 --> 00:34:33.600
and building up all of the steps before you say go.


00:34:33.600 --> 00:34:36.380
Because in pandas, you would say read CSV.


00:34:36.380 --> 00:34:38.300
So, okay, it's gonna read the CSV.


00:34:38.300 --> 00:34:39.220
Now what?


00:34:39.220 --> 00:34:40.460
Right? - Yes.


00:34:40.460 --> 00:34:42.300
- And then you apply your filter


00:34:42.300 --> 00:34:43.780
if that's the order you wanna do it in,


00:34:43.780 --> 00:34:46.900
and then you group and then, and so on and so on, right?


00:34:46.900 --> 00:34:51.220
So this, yeah, it's interesting in that it does allow


00:34:51.220 --> 00:34:54.680
more database-like behavior behind the scenes.


00:34:54.680 --> 00:34:57.540
- Yeah, yeah.


00:34:57.540 --> 00:35:01.260
In the end, in my opinion, the data frame


00:35:01.260 --> 00:35:05.660
should be seen as a table in a database.


00:35:05.660 --> 00:35:10.220
It's the final view of the computation.


00:35:10.220 --> 00:35:12.420
Like you could see it as a materialized view.


00:35:12.420 --> 00:35:14.760
We have some data on this,


00:35:14.760 --> 00:35:18.240
and we want to get it into another table


00:35:18.240 --> 00:35:23.240
which we would feed into our machine learning models


00:35:23.240 --> 00:35:30.280
or whatever, and we do a lot of operations on them


00:35:30.280 --> 00:35:33.640
before we get there.


00:35:33.640 --> 00:35:37.400
So I wouldn't see a data frame as a data,


00:35:37.400 --> 00:35:39.760
it's not only a data structure,


00:35:39.760 --> 00:35:42.400
it's not only a list or a dictionary.


00:35:42.400 --> 00:35:46.220
There are lots of steps before we get into those tables


00:35:46.220 --> 00:35:48.780
we eventually need.


00:35:48.780 --> 00:35:49.680
- Right.


00:35:49.680 --> 00:35:52.660
So here's an interesting challenge.


00:35:52.660 --> 00:35:56.460
There's a lot of visualization libraries.


00:35:56.460 --> 00:36:01.460
There are a lot of other data science libraries


00:36:01.460 --> 00:36:05.380
that know and expect Pandas DataFrames.


00:36:05.380 --> 00:36:06.980
So like, okay, what you do is you send me


00:36:06.980 --> 00:36:08.820
the Pandas DataFrame here,


00:36:08.820 --> 00:36:10.380
or we're going to patch Pandas


00:36:10.380 --> 00:36:12.660
that if you call this function on the data frame,


00:36:12.660 --> 00:36:15.340
it's going to do this thing.


00:36:15.340 --> 00:36:17.580
And they may say, Richie, fantastic job


00:36:17.580 --> 00:36:18.940
you've done here in Polars.


00:36:18.940 --> 00:36:22.020
But my stuff is already all built around Pandas,


00:36:22.020 --> 00:36:23.820
so I'm not going to use this.


00:36:23.820 --> 00:36:26.220
But it's worth pointing out there's some cool Pandas


00:36:26.220 --> 00:36:28.700
integration, right?


00:36:28.700 --> 00:36:29.540
Yeah.


00:36:29.540 --> 00:36:30.040
Yeah.


00:36:30.040 --> 00:36:34.860
So Polars doesn't want to do plotting.


00:36:34.860 --> 00:36:38.380
I don't think it should be in the data frame library.


00:36:38.380 --> 00:36:41.300
Maybe another library can do it on top of Polar


00:36:41.300 --> 00:36:42.980
if they feel like it.


00:36:42.980 --> 00:36:46.260
It shouldn't be in Polar, in my opinion.


00:36:46.260 --> 00:36:50.180
But often when you do plotting, you're plotting--


00:36:50.180 --> 00:36:52.900
the number of rows will not be billions.


00:36:52.900 --> 00:36:55.980
I mean, there's no plotting engine that can deal with that.


00:36:55.980 --> 00:37:00.740
So you will be reducing your big data set to something small.


00:37:00.740 --> 00:37:01.340
And then you--


00:37:01.340 --> 00:37:03.660
>>Victor: There's hardly a monitor


00:37:03.660 --> 00:37:06.700
that has enough pixels to show you that in here, right?


00:37:06.700 --> 00:37:07.420
So yeah.


00:37:07.420 --> 00:37:08.220
>>Jeroen: Right.


00:37:08.220 --> 00:37:13.020
So we can call to pandas and then we transform our polars data frame to pandas


00:37:13.020 --> 00:37:18.700
and then you can integrate with scikit-learn with and we often find that


00:37:18.700 --> 00:37:25.740
progressively rewriting some pandas code into polars already is cheaper than keeping it in


00:37:25.740 --> 00:37:31.580
pandas. If you do a if you come from pandas to polars do a join in polars and then back to pandas


00:37:31.580 --> 00:37:36.540
we probably made up for those double copies. Pandas does a lot of internal copies if you do a


00:37:36.540 --> 00:37:40.580
a reset index, copies all data, if you do, there are a lot of


00:37:40.580 --> 00:37:43.620
internal copies and bombers which are implicit. So I


00:37:43.620 --> 00:37:49.420
wouldn't worry about an explicit copy in the end of your ETL to


00:37:49.420 --> 00:37:52.820
go to plotting when the day right, right, right. So let's


00:37:52.820 --> 00:37:55.620
look at the benchmarks, because it sounds like to a large


00:37:55.620 --> 00:38:01.380
degree. Even if you do have to do this conversion in the end,


00:38:01.420 --> 00:38:06.420
many times, it still might even be quicker.


00:38:06.420 --> 00:38:08.540
So you've got some benchmarks over here


00:38:08.540 --> 00:38:12.480
and you compared, I'm gonna need some good vision


00:38:12.480 --> 00:38:16.800
for this one, you compared Polars, Pandas, Dask,


00:38:16.800 --> 00:38:20.500
and then two things which are too small for me to read.


00:38:20.500 --> 00:38:21.580
Tell us what you compared.


00:38:21.580 --> 00:38:22.940
- Modin and Vax.


00:38:22.940 --> 00:38:24.820
- Modin and Vax, okay.


00:38:24.820 --> 00:38:27.540
And for people listening, if you go out here


00:38:27.540 --> 00:38:29.020
and look at these benchmarks,


00:38:29.020 --> 00:38:32.180
if you're linked right off the homepage.


00:38:32.180 --> 00:38:35.740
There's like a little tiny purple thing


00:38:35.740 --> 00:38:38.660
and a whole bunch of really tall bar graphs


00:38:38.660 --> 00:38:39.520
about the rest.


00:38:39.520 --> 00:38:41.540
- Yeah.


00:38:41.540 --> 00:38:43.660
- And the little tiny thing that you can kind of miss


00:38:43.660 --> 00:38:44.700
if you don't look carefully,


00:38:44.700 --> 00:38:47.460
that's the time it takes for polars.


00:38:47.460 --> 00:38:51.340
And then all the others are up there in like 60 seconds,


00:38:51.340 --> 00:38:52.540
a hundred seconds.


00:38:52.540 --> 00:38:55.700
And then polars is like quarter of a second.


00:38:55.700 --> 00:38:58.020
So, you know, it's easy to miss it in the graph,


00:38:58.020 --> 00:39:00.420
But the quick takeaway here, I think,


00:39:00.420 --> 00:39:04.660
is this is some fast stuff.


00:39:04.660 --> 00:39:05.860
Yeah.


00:39:05.860 --> 00:39:10.060
So we're often orders of magnitudes faster than pandas.


00:39:10.060 --> 00:39:14.660
So it's not uncommon to hear it's 10 to 20x times faster.


00:39:14.660 --> 00:39:19.460
Especially if you write proper pandas and proper polos,


00:39:19.460 --> 00:39:23.540
it's probably 20x if we deal with I/O as well.


00:39:23.540 --> 00:39:26.140
So what we see here are the TPCH benchmarks.


00:39:26.140 --> 00:39:31.140
GPC-H is a database query benchmark standard,


00:39:31.140 --> 00:39:36.260
which this is used by every query engine


00:39:36.260 --> 00:39:38.260
to show how fast it is.


00:39:38.260 --> 00:39:41.340
And those are really hard questions


00:39:41.340 --> 00:39:46.340
that really flex the muscles of a query engine.


00:39:46.340 --> 00:39:51.300
So you have joins on several tables


00:39:51.300 --> 00:39:52.520
with different group bys,


00:39:52.520 --> 00:39:56.280
different nested group bys, et cetera.


00:39:56.280 --> 00:40:01.280
And yeah, I really tried to make those other tools faster.


00:40:01.280 --> 00:40:08.400
So in memory, DOS can mode in,


00:40:08.400 --> 00:40:12.240
no, it was really hard to make stuff faster than Pandas,


00:40:12.240 --> 00:40:15.760
except for Polar, on a few occasions.


00:40:15.760 --> 00:40:20.760
But once we include IO, all those tools first needed


00:40:20.840 --> 00:40:23.080
go via pandas and yeah.


00:40:23.080 --> 00:40:29.720
What this sort of shows is that we have pandas,


00:40:29.720 --> 00:40:34.160
which is a single threaded data frame engine,


00:40:34.160 --> 00:40:36.800
and then we have tools that parallelize pandas,


00:40:36.800 --> 00:40:39.080
and it's not always,


00:40:39.080 --> 00:40:44.360
they don't, just parallelizing pandas


00:40:44.360 --> 00:40:45.720
doesn't make it faster.


00:40:45.720 --> 00:40:49.280
So if we have a filter


00:40:49.280 --> 00:40:51.800
or a element-wise multiplication.


00:40:51.800 --> 00:40:52.960
Parallelization is easy.


00:40:52.960 --> 00:40:55.960
You just split it up in chunks and do your parallelization.


00:40:55.960 --> 00:40:58.080
And then there's tools.


00:40:58.080 --> 00:41:00.000
- You got 10 cores, you can start 10 threads


00:41:00.000 --> 00:41:01.520
and I can take one 10th of the data


00:41:01.520 --> 00:41:04.760
and start to answer yes or no for the filter question,


00:41:04.760 --> 00:41:05.600
for example, right?


00:41:05.600 --> 00:41:06.440
- Yes.


00:41:06.440 --> 00:41:07.260
- Yeah, okay.


00:41:07.260 --> 00:41:08.140
- A lot of people don't realize


00:41:08.140 --> 00:41:10.440
that a lot of data frame operations


00:41:10.440 --> 00:41:12.640
are not embarrassingly parallel.


00:41:12.640 --> 00:41:16.240
A group by is definitely not embarrassingly parallel.


00:41:16.240 --> 00:41:19.660
a filter or sorry, a join needs a shuffle.


00:41:19.660 --> 00:41:22.140
It doesn't, it's not there in parallel.


00:41:22.140 --> 00:41:26.600
And that's where, that's why you see those tools


00:41:26.600 --> 00:41:30.280
being slower than Pandas because they're string data


00:41:30.280 --> 00:41:32.240
and then you have a problem because,


00:41:32.240 --> 00:41:34.800
or we need to do multi-processing


00:41:34.800 --> 00:41:36.760
and we need to send those Python objects


00:41:36.760 --> 00:41:40.920
to another project and we copy data, which is slow,


00:41:40.920 --> 00:41:43.880
or we need to do multi-threading and we're bound by the gil


00:41:43.880 --> 00:41:45.200
and we're single threaded.


00:41:45.200 --> 00:41:50.200
And then there is the expensive structure.


00:41:50.200 --> 00:41:57.200
Yeah, I think there's some interesting parallels for Dask and Polars.


00:41:57.200 --> 00:42:02.200
On these benchmarks, at least, you're showing much better performance than Dask.


00:42:02.200 --> 00:42:08.200
I've had Matthew Rocklin on a couple of times to talk about Dask and some of the work they're doing there.


00:42:08.200 --> 00:42:09.200
And it's very cool.


00:42:09.200 --> 00:42:13.080
And one of the things that I think Dask is interesting for


00:42:13.080 --> 00:42:15.920
is allowing you to scale your code out


00:42:15.920 --> 00:42:19.360
to multi-cores on your machine, or even distributed grid


00:42:19.360 --> 00:42:22.800
computing, or process data that doesn't fit in memory.


00:42:22.800 --> 00:42:27.760
And they can, behind the scenes, juggle all that for you.


00:42:27.760 --> 00:42:31.120
So I feel like Polar's kind of has a different way,


00:42:31.120 --> 00:42:35.480
but attempts to solve some of those problems as well.


00:42:35.480 --> 00:42:41.280
yeah the Polar has full control over it over everything so it's built from the


00:42:41.280 --> 00:42:45.800
ground up and it controls the I/O, it controls their own memory, it controls which


00:42:45.800 --> 00:42:51.880
thread gets which data and in DOS it goes through it takes this other tool


00:42:51.880 --> 00:42:58.680
and then parallelizes that but it is limited by what this other tool also is


00:42:58.680 --> 00:43:03.680
But I think, so on a single machine,


00:43:03.680 --> 00:43:05.840
it has those challenges.


00:43:05.840 --> 00:43:08.760
I think Dask distributed doesn't have these challenges.


00:43:08.760 --> 00:43:12.580
And I think for distributed, it can work really well.


00:43:12.580 --> 00:43:16.140
- Yeah, the interesting part with Dask, I think,


00:43:16.140 --> 00:43:18.080
is that it's kind of like pandas,


00:43:18.080 --> 00:43:21.200
but it scales in all these interesting ways.


00:43:21.200 --> 00:43:24.800
Across cores, bigger memory, but also across machines,


00:43:24.800 --> 00:43:26.540
and then across cores, across machines,


00:43:26.540 --> 00:43:28.560
like all that sort of stuff.


00:43:28.560 --> 00:43:29.960
I feel like Dask is a little bit,


00:43:29.960 --> 00:43:32.560
maybe it's trying to solve a little bit bigger


00:43:32.560 --> 00:43:35.720
compute problem, like how can we use a cluster of computers


00:43:35.720 --> 00:43:38.760
to answer these questions, whereas I don't get that sense.


00:43:38.760 --> 00:43:41.000
- The documentation also says it themselves.


00:43:41.000 --> 00:43:44.680
They say that they're probably not faster than Pandas


00:43:44.680 --> 00:43:45.680
on a single machine.


00:43:45.680 --> 00:43:52.360
So they're more for the large, the big data.


00:43:52.360 --> 00:43:53.200
- Yeah.


00:43:53.200 --> 00:43:56.160
- Yeah.


00:43:56.160 --> 00:43:58.920
But Polaris wants to be and a lot faster on a single machine,


00:43:58.920 --> 00:44:02.280
but also wants to be able to do out-of-core processing


00:44:02.280 --> 00:44:04.280
on a single machine.


00:44:04.280 --> 00:44:07.960
We don't support all queries yet,


00:44:07.960 --> 00:44:12.480
but we already do basic joins, group by sorts,


00:44:12.480 --> 00:44:16.400
predicates, element-wise operations.


00:44:16.400 --> 00:44:22.160
And then we can process, I process 500 gigabytes on my laptop.


00:44:22.160 --> 00:44:25.040
That's pretty good.


00:44:25.800 --> 00:44:28.840
Your laptop probably doesn't have 500 gigs of memory either, does it?


00:44:28.840 --> 00:44:30.280
No, no, no, no. It's 16 gigs.


00:44:30.280 --> 00:44:39.160
Nice. It's probably actually a value to, as you develop this product, to not have


00:44:39.160 --> 00:44:46.520
too massive of a computer to work on. You know, if you had a $5,000 workstation,


00:44:46.520 --> 00:44:52.680
you know, you might be a little out of touch with many people using your code and so on.


00:44:52.680 --> 00:44:56.120
- Yeah, well, though, I think there,


00:44:56.120 --> 00:45:02.240
I think Polars, like scaling on a single machine


00:45:02.240 --> 00:45:04.760
makes sense for different reasons as well.


00:45:04.760 --> 00:45:08.760
I think a lot of people talk about distributed,


00:45:08.760 --> 00:45:11.420
but if you think about the complexity of distributed,


00:45:11.420 --> 00:45:12.640
you need to send data,


00:45:12.640 --> 00:45:15.040
shuffle data over the network to other machines.


00:45:15.040 --> 00:45:19.800
So there are a lot of people using Polars in our Discord


00:45:19.800 --> 00:45:23.080
who have one terabyte of RAM and say,


00:45:23.080 --> 00:45:25.120
it's cheaper and a lot faster than Spark


00:45:25.120 --> 00:45:28.120
because one, Polis is faster on a single machine


00:45:28.120 --> 00:45:33.120
and one, two, they have a beefy machine with like 120 cores


00:45:33.120 --> 00:45:43.040
and they don't have to go over the network to parallelize.


00:45:43.040 --> 00:45:48.200
So I think times are changing.


00:45:48.200 --> 00:45:53.200
I think also scaling out data


00:45:53.200 --> 00:45:56.720
on a single machine is getting more and more interesting.


00:45:56.720 --> 00:45:57.640
- It is.


00:45:57.640 --> 00:46:00.600
One of the areas in which it's interesting is GPUs.


00:46:00.600 --> 00:46:03.320
Do you have any integration with GPUs


00:46:03.320 --> 00:46:04.320
or any of those sorts of things?


00:46:04.320 --> 00:46:05.140
- No.


00:46:05.140 --> 00:46:07.200
- Not suggesting necessarily is even a good idea.


00:46:07.200 --> 00:46:08.680
I'm just wondering if it does.


00:46:08.680 --> 00:46:10.640
- No, I get this question,


00:46:10.640 --> 00:46:15.640
but I'm not really convinced I can get the data


00:46:15.640 --> 00:46:18.040
fast enough into the memory.


00:46:18.040 --> 00:46:20.680
we want to process gigabytes of data.


00:46:20.680 --> 00:46:24.760
And the challenge already on the CPU


00:46:24.760 --> 00:46:31.880
is getting the data from cache, from memory, fast enough.


00:46:31.880 --> 00:46:35.240
And I think on a CPU, this is--


00:46:35.240 --> 00:46:35.880
I don't know.


00:46:35.880 --> 00:46:38.400
I don't know.


00:46:38.400 --> 00:46:39.040
Yeah.


00:46:39.040 --> 00:46:42.200
So maybe we could talk really quickly about platforms


00:46:42.200 --> 00:46:44.680
that it runs on.


00:46:44.680 --> 00:46:49.480
This is the very first show that I'm doing on my M2 Pro processor,


00:46:49.480 --> 00:46:53.640
which is fun. I literally been using it for like an hour and a half,


00:46:53.640 --> 00:46:57.240
so I don't really have much to say, but it looks neat. Anyway, you know,


00:46:57.240 --> 00:46:59.120
that's very different than an Intel machine,


00:46:59.120 --> 00:47:02.800
which is different than a Raspberry Pi, which is different than, you know,


00:47:02.800 --> 00:47:07.800
some version of Linux running on ARM or on AMD.


00:47:07.800 --> 00:47:11.200
So where, where do these, you know, what's the,


00:47:12.440 --> 00:47:15.120
the reach, like who can use it?


00:47:15.120 --> 00:47:18.240
Well, we support it.


00:47:18.240 --> 00:47:20.120
We support it.


00:47:20.120 --> 00:47:24.720
So Parallels also has a lot of like SIMD optimizations.


00:47:24.720 --> 00:47:27.560
SIMD stands for Single Instruction Multiple Data,


00:47:27.560 --> 00:47:30.920
where, for instance, if you do a floating point operation


00:47:30.920 --> 00:47:34.760
instead of doing a single floating point at a time,


00:47:34.760 --> 00:47:36.840
you can fill in those vector lanes


00:47:36.840 --> 00:47:40.200
into your CPU, which can fit eight floating points


00:47:40.200 --> 00:47:44.640
and in a single operation can compute eight at a time.


00:47:44.640 --> 00:47:48.560
And then you have eight times the parallelism on a single core.


00:47:48.560 --> 00:47:52.440
Those instructions are only activated for Intel.


00:47:52.440 --> 00:47:58.720
So we don't have these instructions activated for ARM,


00:47:58.720 --> 00:48:00.320
but we do compile to ARM.


00:48:00.320 --> 00:48:09.000
And how it performs, I think it performs fast.


00:48:09.000 --> 00:48:11.960
- Yeah, but so if the standard machines, right?


00:48:11.960 --> 00:48:16.140
macOS, Windows, Linux, or we're all good to go.


00:48:16.140 --> 00:48:18.480
- Yeah, yeah.


00:48:18.480 --> 00:48:21.280
- And it ships as a wheel, so you don't have to have any,


00:48:21.280 --> 00:48:22.200
you don't have to have Rusty


00:48:22.200 --> 00:48:23.400
or anything like that hanging around?


00:48:23.400 --> 00:48:25.760
- No, no, no, it ships as a wheel, yeah.


00:48:25.760 --> 00:48:26.600
- Okay.


00:48:26.600 --> 00:48:29.380
- We also have Conda,


00:48:29.380 --> 00:48:32.160
but the Conda is always a bit lagging behind,


00:48:32.160 --> 00:48:35.360
so I'd advise to install Compit


00:48:35.360 --> 00:48:38.760
because we can pre-control this deployment.


00:48:38.760 --> 00:48:39.600
- Yeah, exactly.


00:48:39.600 --> 00:48:42.240
You push it out to pipe your eye in.


00:48:42.240 --> 00:48:43.080
That's it.


00:48:43.080 --> 00:48:44.520
That's what pip sees and it's gonna go, right?


00:48:44.520 --> 00:48:45.620
Pretty much instantly.


00:48:45.620 --> 00:48:49.660
I guess it's worth pointing out while we're sitting here


00:48:49.660 --> 00:48:52.400
is not that thing I highlighted this.


00:48:52.400 --> 00:48:55.160
You do have a whole section in your user guide,


00:48:55.160 --> 00:48:58.160
the Polars book called "Coming from Pandas"


00:48:58.160 --> 00:49:01.040
that actually talks about the differences,


00:49:01.040 --> 00:49:04.000
not just how do I do this versus, you know,


00:49:04.000 --> 00:49:06.400
this operation in Pandas versus Polars,


00:49:06.400 --> 00:49:09.360
but it also talks about some of the philosophy,


00:49:09.360 --> 00:49:12.320
like this lazy concepts that we've spoken about


00:49:12.320 --> 00:49:14.060
and a query optimization.


00:49:14.060 --> 00:49:17.420
I feel like we covered it pretty well,


00:49:17.420 --> 00:49:20.900
unless there's maybe some other stuff


00:49:20.900 --> 00:49:22.460
that you wanna throw in here really quick,


00:49:22.460 --> 00:49:25.200
but I mostly just wanna throw this out as a resource


00:49:25.200 --> 00:49:27.620
'cause I know many people are coming from pandas


00:49:27.620 --> 00:49:29.000
and they may be interested in this


00:49:29.000 --> 00:49:30.840
and this is probably a good place to start.


00:49:30.840 --> 00:49:32.680
I'll link to it in the show notes.


00:49:32.680 --> 00:49:35.520
- Yeah, I think the most controversial one


00:49:35.520 --> 00:49:38.080
that we don't have the multi-index.


00:49:38.080 --> 00:49:43.360
- Right, you don't have anything other than zero base,


00:49:43.360 --> 00:49:45.640
zero, one, two, three, where is it in the range?


00:49:45.640 --> 00:49:47.720
- Yeah, in index, yeah.


00:49:47.720 --> 00:49:51.280
Yeah, well, we will support data structures


00:49:51.280 --> 00:49:54.740
that make lookups faster, like index in a database sense,


00:49:54.740 --> 00:50:01.660
but it will not change the semantics of the query.


00:50:01.660 --> 00:50:04.000
That's an important thing.


00:50:04.000 --> 00:50:06.620
Yeah, okay.


00:50:06.620 --> 00:50:09.860
Yeah, so I encourage people who are mostly pandas people


00:50:09.860 --> 00:50:11.780
to come down here and look through this.


00:50:11.780 --> 00:50:13.900
It's pretty straightforward.


00:50:13.900 --> 00:50:17.620
Another thing that I think is interesting


00:50:17.620 --> 00:50:20.220
and we're talking about maybe is we could touch a little bit


00:50:20.220 --> 00:50:23.460
on some of the, how can I, in your user guide,


00:50:23.460 --> 00:50:26.220
you've got, how can I work with IO?


00:50:26.220 --> 00:50:27.900
How can I work with time series?


00:50:27.900 --> 00:50:31.060
How can I work with multiprocessing and so on?


00:50:31.060 --> 00:50:33.620
What do you think is good to highlight out of here?


00:50:33.620 --> 00:50:49.260
Yeah, the user guide is a bit outdated. So, I think it's a year old. So, for instance,


00:50:49.260 --> 00:50:58.300
I/O is changing. Polaris writes as its own I/O readers. So, we've written our


00:50:58.300 --> 00:51:03.300
our own CSV reader, JSON reader, Parquet, IPC, Arrow,


00:51:03.300 --> 00:51:09.600
and that's all in our control.


00:51:09.600 --> 00:51:13.560
But for interaction with databases,


00:51:13.560 --> 00:51:15.960
it's often a bit more complicated.


00:51:15.960 --> 00:51:19.280
You deal with different drivers, different ways,


00:51:19.280 --> 00:51:22.120
and currently we do this with Connector X,


00:51:22.120 --> 00:51:24.480
which is really great and allows us to read


00:51:24.480 --> 00:51:26.480
from a lot of different databases,


00:51:26.480 --> 00:51:29.440
but it doesn't allow us to write from databases yet.


00:51:29.440 --> 00:51:34.440
And this is happening, this is not changing.


00:51:34.440 --> 00:51:39.040
I want to explain a bit why.


00:51:39.040 --> 00:51:44.040
So Polaris builds upon the arrow memory specification.


00:51:44.040 --> 00:51:49.680
And the arrow memory specification is sort of standard


00:51:49.680 --> 00:51:53.960
of how memory for data,


00:51:53.960 --> 00:51:56.360
our memory for columnar data should look into


00:51:56.360 --> 00:52:00.920
how columnar data should be represented in memory.


00:52:00.920 --> 00:52:03.760
And this is becoming a new standard.


00:52:03.760 --> 00:52:08.440
And Spark is using it, Dremel,


00:52:08.440 --> 00:52:15.000
Pandas itself, for instance, if you read a parquet in Pandas,


00:52:15.000 --> 00:52:16.600
it reads in first into error memory


00:52:16.600 --> 00:52:18.880
and then copies that into Pandas memory.


00:52:20.760 --> 00:52:23.360
So the arrow memory specification


00:52:23.360 --> 00:52:28.360
becoming a standard, and this is a way to share data


00:52:28.360 --> 00:52:33.440
to processes, also to other libraries within the process


00:52:33.440 --> 00:52:36.500
without copying data.


00:52:36.500 --> 00:52:38.720
We can just swap our pointers


00:52:38.720 --> 00:52:41.680
if we know that we both support arrow.


00:52:41.680 --> 00:52:45.620
- Oh, so arrow defines basically in memory,


00:52:45.620 --> 00:52:47.360
it looks like this.


00:52:47.360 --> 00:52:48.320
And so...


00:52:49.520 --> 00:52:52.720
And if you both agree on that, we can just swap our pointers.


00:52:52.720 --> 00:52:56.080
- Right, because a .NET object, a C++ object,


00:52:56.080 --> 00:52:59.760
and a Python object, those don't look like anything similar


00:52:59.760 --> 00:53:02.500
to any of them, right, in memory.


00:53:02.500 --> 00:53:07.500
And yeah, so this is from the Apache Arrow project, yeah?


00:53:07.500 --> 00:53:12.400
- Yeah, and this is really, really used


00:53:12.400 --> 00:53:15.120
by a lot of different tools already.


00:53:15.120 --> 00:53:18.640
And currently there is coming the ADBC,


00:53:18.640 --> 00:53:21.460
which is the Apache Arrow database connector,


00:53:21.460 --> 00:53:23.060
which will solve all those problems


00:53:23.060 --> 00:53:25.280
because then we can read and write


00:53:25.280 --> 00:53:27.020
from a lot of databases in Arrow


00:53:27.020 --> 00:53:28.360
and then it will be really fast


00:53:28.360 --> 00:53:30.500
and really easy for us to do.


00:53:30.500 --> 00:53:35.500
So luckily, that's one of those foundations of Polars


00:53:35.500 --> 00:53:44.360
I'm really happy about because supporting Arrow


00:53:44.360 --> 00:53:49.360
and using Arrow memory gives us a lot of interaction


00:53:49.360 --> 00:53:51.700
interval with other libraries.


00:53:51.700 --> 00:53:54.480
- Right.


00:53:54.480 --> 00:53:55.920
Yeah, that's interesting.


00:53:55.920 --> 00:53:57.200
When you think of pandas,


00:53:57.200 --> 00:54:01.040
it's kind of built on top of NumPy as its core foundation


00:54:01.040 --> 00:54:03.420
and it can exchange NumPy arrays


00:54:03.420 --> 00:54:06.360
with other things that do that.


00:54:06.360 --> 00:54:11.360
And so Apache Arrow is kind of your base there, right?


00:54:11.360 --> 00:54:14.240
- Yeah, well, it's kind of full circle


00:54:14.240 --> 00:54:17.200
because Apache Arrow is started by Wes McKinney.


00:54:17.200 --> 00:54:21.640
Wes McKinney being the creator of Pandas.


00:54:21.640 --> 00:54:24.360
And when he got out of Pandas,


00:54:24.360 --> 00:54:28.440
he thought, okay, the memory representation of NumPy


00:54:28.440 --> 00:54:32.120
is just not, we should not use it.


00:54:32.120 --> 00:54:35.560
And then he was inspired to build Apache Arrow,


00:54:35.560 --> 00:54:40.500
which learned from Pandas and yeah.


00:54:41.600 --> 00:54:44.960
Also, that's how you learn about these projects, right?


00:54:44.960 --> 00:54:49.440
This is how you realize, oh, we had put this thing in place.


00:54:49.440 --> 00:54:50.640
Maybe it would work better, right?


00:54:50.640 --> 00:54:52.320
You work on a project for five years


00:54:52.320 --> 00:54:54.800
and you're like, if I got a chance to start over,


00:54:54.800 --> 00:54:57.360
but it's too late now, but every now and then


00:54:57.360 --> 00:55:00.320
you do actually get a chance to start over.


00:55:00.320 --> 00:55:02.540
- Yeah, yeah.


00:55:02.540 --> 00:55:05.560
- Interesting.


00:55:05.560 --> 00:55:08.020
I didn't realize that Wes was involved with both.


00:55:08.020 --> 00:55:09.200
I mean, I knew from Pandas,


00:55:09.200 --> 00:55:11.360
but I didn't realize he's working on Arrow.


00:55:11.360 --> 00:55:19.960
Yeah, he is CTO of Fultron, which, no, he started Apache Arrow.


00:55:19.960 --> 00:55:26.960
And that's, Apache Arrow is sort of super big, like used everywhere, but sort of middleware.


00:55:26.960 --> 00:55:32.040
Like it's end users are developers and not, end users are developers who build tools and


00:55:32.040 --> 00:55:36.000
not developers who use libraries.


00:55:36.000 --> 00:55:38.440
You might not even know that you're using it.


00:55:38.440 --> 00:55:41.440
I just use pullers and oh, by the way,


00:55:41.440 --> 00:55:44.680
it happens to internally be better because of this.


00:55:44.680 --> 00:55:45.720
Yeah.


00:55:45.720 --> 00:55:46.560
- Yeah.


00:55:46.560 --> 00:55:47.380
- Yeah, very cool.


00:55:47.380 --> 00:55:48.220
Okay, let's see.


00:55:48.220 --> 00:55:50.720
We've got a little bit of time left to talk about it.


00:55:50.720 --> 00:55:54.000
So for example, some of these, how can I,


00:55:54.000 --> 00:55:56.040
let me just touch on a couple that are nice here.


00:55:56.040 --> 00:55:58.080
So you talked about ConnectorX,


00:55:58.080 --> 00:55:59.160
you talked about the database,


00:55:59.160 --> 00:56:01.440
but it's like three lines of code


00:56:01.440 --> 00:56:04.000
to define a connection string,


00:56:04.000 --> 00:56:06.320
define a SQL query,


00:56:06.320 --> 00:56:08.960
and then just, you can just say PL.readSQL.


00:56:08.960 --> 00:56:11.680
And there you go.


00:56:11.680 --> 00:56:13.720
You've, you call it data frame


00:56:13.720 --> 00:56:16.520
or what do you call the thing you get back here?


00:56:16.520 --> 00:56:17.360
- A data frame.


00:56:17.360 --> 00:56:18.180
- Data frame, okay.


00:56:18.180 --> 00:56:19.760
- So reading is always a data frame,


00:56:19.760 --> 00:56:21.680
scanning will be a lazy frame.


00:56:21.680 --> 00:56:24.480
- Got it, okay.


00:56:24.480 --> 00:56:27.180
Is there a scan SQL as well?


00:56:27.180 --> 00:56:32.440
- No, this might happen in the future.


00:56:33.280 --> 00:56:38.800
The challenge is, are we going to push back our optimizations?


00:56:38.800 --> 00:56:43.400
So we write a policy query, and then we


00:56:43.400 --> 00:56:49.280
must translate that into the SQL we send to the database.


00:56:49.280 --> 00:56:54.440
But that needs to be consistent over different databases.


00:56:54.440 --> 00:56:58.320
That's a whole other rabbit hole we might get into.


00:56:58.320 --> 00:56:59.560
I'm not sure.


00:56:59.560 --> 00:57:04.480
Because you can already do many of these operations


00:57:04.480 --> 00:57:06.800
in the SQL query that you're sending over, right?


00:57:06.800 --> 00:57:10.520
You have sort of two layers of query engines and optimizers


00:57:10.520 --> 00:57:11.440
and query plans.


00:57:11.440 --> 00:57:17.880
And it's not like you can't add on additional filters, joins,


00:57:17.880 --> 00:57:22.280
sorts, and so on before it ever gets back to you, right?


00:57:22.280 --> 00:57:24.400
But it would be terrible if someone writes


00:57:24.400 --> 00:57:28.840
SELECT * FROM TABLE and then writes the filters in POLARS


00:57:28.840 --> 00:57:32.600
and then the database has sent all those data over the network.


00:57:32.600 --> 00:57:41.400
So, yeah, ideally, we'd be able to push those predicates down into the SQL.


00:57:41.400 --> 00:57:45.560
Yeah, but you know somebody's going to do it because they're more comfortable writing


00:57:45.560 --> 00:57:49.720
Polar API in Python than they are writing T-SQL.


00:57:49.720 --> 00:57:53.320
Yeah, yeah. You will not, yeah.


00:57:53.320 --> 00:57:54.920
It is not ideal.


00:57:54.920 --> 00:57:57.720
It's not optimal. That's right. That is right.


00:57:57.720 --> 00:58:00.480
Let's see what else can you do here.


00:58:00.480 --> 00:58:04.960
So we've already talked about the CSV files


00:58:04.960 --> 00:58:07.040
and this is the part of that I was talking about


00:58:07.040 --> 00:58:10.460
where you've got the toggle to see the Rust code


00:58:10.460 --> 00:58:11.520
and the Python code.


00:58:11.520 --> 00:58:13.520
So I think people might appreciate that.


00:58:13.520 --> 00:58:15.680
Parquet files.


00:58:15.680 --> 00:58:19.980
So Parquet files is a more efficient format.


00:58:19.980 --> 00:58:22.900
Maybe talk about using Parquet files versus CSV


00:58:22.900 --> 00:58:25.960
and why you might wanna get rid of CSV


00:58:25.960 --> 00:58:29.160
and like store these intermediate files


00:58:29.160 --> 00:58:31.500
and then load them when you, yeah.


00:58:31.500 --> 00:58:35.320
- So Parthos is a really fast CSV reader.


00:58:35.320 --> 00:58:37.240
I really did my best on that one.


00:58:37.240 --> 00:58:42.240
But if you can use Parquet or Arrow IPC


00:58:42.240 --> 00:58:47.480
because your data is typed,


00:58:47.480 --> 00:58:49.760
there's no ambiguity on reading.


00:58:49.760 --> 00:58:51.120
We know which type it is.


00:58:51.120 --> 00:58:53.540
- Right, because CSV files,


00:58:53.540 --> 00:58:55.800
even though it might be representing a date,


00:58:55.800 --> 00:58:59.600
it's still a string and a substring.


00:58:59.600 --> 00:59:01.880
Yeah, it's slow to parse.


00:59:01.880 --> 00:59:02.920
Yeah, exactly.


00:59:02.920 --> 00:59:06.480
- There's also, we can just,


00:59:06.480 --> 00:59:10.480
so it, Parquet interacts really nicely


00:59:10.480 --> 00:59:11.880
with query optimization.


00:59:11.880 --> 00:59:14.620
So we can select just a single column from the file


00:59:14.620 --> 00:59:16.720
without touching any of the other columns.


00:59:16.720 --> 00:59:19.440
We can read statistics and,


00:59:19.440 --> 00:59:23.340
so Parquet file can write statistics,


00:59:23.340 --> 00:59:26.620
which knows, okay, this page has got this maximum value,


00:59:26.620 --> 00:59:27.860
this minimum value.


00:59:27.860 --> 00:59:29.460
And if you have written a POTUS query,


00:59:29.460 --> 00:59:32.980
which says also only gives me the result


00:59:32.980 --> 00:59:35.940
where the value is larger than this.


00:59:35.940 --> 00:59:39.820
And we see that the statistics say


00:59:39.820 --> 00:59:41.100
it cannot be in this file.


00:59:41.100 --> 00:59:43.300
We can just skip the whole column.


00:59:43.300 --> 00:59:44.980
We don't have to read. - Oh, interesting.


00:59:44.980 --> 00:59:45.860
Oh, okay. - Yeah.


00:59:45.860 --> 00:59:51.560
So there are a lot of optimizations.


00:59:52.780 --> 00:59:55.820
So the best work is work you don't have to do.


00:59:55.820 --> 00:59:56.720
- Exactly.


00:59:56.720 --> 00:59:59.460
Or you've done it when you created the file


00:59:59.460 --> 01:00:01.900
and you never do it again or something like that.


01:00:01.900 --> 01:00:03.780
- Yeah, yeah.


01:00:03.780 --> 01:00:09.300
- Yeah, so you've got a read parquet, a scan parquet,


01:00:09.300 --> 01:00:12.820
I suppose that's the data frame versus lazy frame.


01:00:12.820 --> 01:00:13.640
- Yeah.


01:00:13.640 --> 01:00:15.500
- Yeah, and then you also have the ability to write them.


01:00:15.500 --> 01:00:16.660
That's pretty interesting.


01:00:16.660 --> 01:00:18.740
JSON, multiple files.


01:00:20.940 --> 01:00:24.240
It's just a whole bunch of how do I, how can I rather,


01:00:24.240 --> 01:00:25.240
but a bunch of neat things.


01:00:25.240 --> 01:00:26.760
What else would you like to highlight here


01:00:26.760 --> 01:00:28.120
in the next couple of minutes?


01:00:28.120 --> 01:00:32.880
- I think the most important thing I want to touch on


01:00:32.880 --> 01:00:34.000
is the expression API.


01:00:34.000 --> 01:00:36.340
So that's a bit, if you go a bit higher.


01:00:36.340 --> 01:00:40.500
So just follow up.


01:00:40.500 --> 01:00:42.160
- I see, there you go.


01:00:42.160 --> 01:00:43.600
- They got their own chapter.


01:00:43.600 --> 01:00:50.440
So one of the goals of the Polar's API


01:00:50.640 --> 01:00:54.360
is to keep the API service small,


01:00:54.360 --> 01:00:56.940
but give you a lot of things you can do.


01:00:56.940 --> 01:00:59.460
And this is where the Polis expressions come in.


01:00:59.460 --> 01:01:04.460
So Polis expressions are expressions of what you want to do,


01:01:04.460 --> 01:01:10.220
which are run and parallelized on a query engine.


01:01:10.220 --> 01:01:12.460
And you can combine them indefinitely.


01:01:12.460 --> 01:01:15.740
So an expression takes a series and produces a series.


01:01:15.740 --> 01:01:19.300
And because the input is the same as the output,


01:01:19.300 --> 01:01:20.800
you can combine them.


01:01:20.800 --> 01:01:24.980
And as you can see, we can do pretty complicated stuff


01:01:24.980 --> 01:01:28.680
and you can keep chaining them.


01:01:28.680 --> 01:01:30.880
And this is the same like,


01:01:30.880 --> 01:01:33.760
how I'd like to see it.


01:01:33.760 --> 01:01:38.760
For instance, the Python vocabulary is quite small.


01:01:38.760 --> 01:01:41.520
So we have a while, we have a loop,


01:01:41.520 --> 01:01:43.480
we have a variable assignment.


01:01:43.480 --> 01:01:48.480
But if you, I think it fits into maybe two pieces of paper.


01:01:49.240 --> 01:01:52.600
But with this, you can write any program you want


01:01:52.600 --> 01:01:55.840
with the combination of all those,


01:01:55.840 --> 01:01:59.920
all those, yeah, this vocabulary.


01:01:59.920 --> 01:02:02.800
And that's what we want to do


01:02:02.800 --> 01:02:04.360
with the Polar Expressions as well.


01:02:04.360 --> 01:02:09.360
So you've got a lot of small building blocks


01:02:09.360 --> 01:02:14.760
which can be combined into, yeah, anything.


01:02:14.760 --> 01:02:17.160
- Yeah, so somebody could say,


01:02:17.160 --> 01:02:20.580
I wanna select a column back,


01:02:20.580 --> 01:02:22.740
but then I don't want the actual values.


01:02:22.740 --> 01:02:26.420
I want the unique ones, a uniqueness.


01:02:26.420 --> 01:02:28.540
So if there's duplicate, remove those,


01:02:28.540 --> 01:02:30.180
and then you can do a .account.


01:02:30.180 --> 01:02:33.380
Then you can add an alias, which gives it a new,


01:02:33.380 --> 01:02:35.840
which basically defines the column name.


01:02:35.840 --> 01:02:39.780
And it's not names, it's some other word.


01:02:39.780 --> 01:02:41.380
- You could read it as an ads.


01:02:41.380 --> 01:02:45.900
So take column names as unique names to in SQL.


01:02:45.900 --> 01:02:46.740
- Yeah.


01:02:46.740 --> 01:02:48.400
a keyword and five, so in other words--


01:02:48.400 --> 01:02:49.460
>> Right.


01:02:49.460 --> 01:02:51.140
It means something else, yeah.


01:02:51.140 --> 01:02:52.660
>> Yeah.


01:02:52.660 --> 01:02:53.420
>> Interesting.


01:02:53.420 --> 01:02:54.020
OK.


01:02:54.020 --> 01:02:54.900
Yeah, so people--


01:02:54.900 --> 01:02:57.940
>> That's pretty good.


01:02:57.940 --> 01:03:02.540
>> They use these expressions to do lots of transformations


01:03:02.540 --> 01:03:04.620
and filtering and things like that.


01:03:04.620 --> 01:03:05.180
>> Yeah.


01:03:05.180 --> 01:03:06.660
>> It's very powerful.


01:03:06.660 --> 01:03:07.160
>> Yeah.


01:03:07.160 --> 01:03:10.100
So these expressions can be used in a select


01:03:10.100 --> 01:03:15.140
on different places, but the knowledge of expressions


01:03:15.140 --> 01:03:17.140
extrapolates to different locations.


01:03:17.140 --> 01:03:19.140
You can do it in a select statement,


01:03:19.140 --> 01:03:22.860
and then you select this expression,


01:03:22.860 --> 01:03:24.860
and you get a result.


01:03:24.860 --> 01:03:26.860
You can also do this in a group by aggregation.


01:03:26.860 --> 01:03:30.740
And then the same logic applies.


01:03:30.740 --> 01:03:32.740
It runs on the same engine,


01:03:32.740 --> 01:03:34.740
and we make sure everything is consistent.


01:03:34.740 --> 01:03:38.740
This is really powerful


01:03:38.740 --> 01:03:41.860
because it's so expressive,


01:03:41.860 --> 01:03:43.860
people don't have to use


01:03:43.860 --> 01:03:48.180
custom apply with a lambda because when you use a lambda it's a black box to us


01:03:48.180 --> 01:03:51.220
it will be slow because it's python and we don't know what happens


01:03:51.220 --> 01:03:58.260
so a lambda is it will be slow it will kill parallelization because gills block


01:03:58.260 --> 01:04:07.860
um yeah a lambda is three times bad for us right yeah it it gets in the way of a lot of your


01:04:07.860 --> 01:04:13.780
optimizations and a lot of your speed ups there. Yeah and that's why we want to


01:04:13.780 --> 01:04:23.140
make this expression API very complete so you don't need them as much. Yeah so people are


01:04:23.140 --> 01:04:27.860
wanting to get this get seriously into this they should check out chapter three expressions right


01:04:27.860 --> 01:04:32.900
and just go through there. Yeah. Probably especially you know sort of browse through the


01:04:33.460 --> 01:04:39.380
Python examples that they can see where go back and see what they need to learn more about but


01:04:39.380 --> 01:04:42.660
Yeah, it's


01:04:42.660 --> 01:04:44.740
It's a very interesting api


01:04:44.740 --> 01:04:47.460
The speed is very


01:04:47.460 --> 01:04:48.980
compelling


01:04:48.980 --> 01:04:50.980
I think I think it's a cool project


01:04:50.980 --> 01:04:53.300
Like I said, how many people we got here?


01:04:53.300 --> 01:04:58.420
13 000 people using already. So that's a pretty big community


01:04:58.420 --> 01:05:00.740
Yeah


01:05:00.740 --> 01:05:01.700
Yeah


01:05:01.700 --> 01:05:02.740
Thanks


01:05:02.740 --> 01:05:03.560
Join us.


01:05:03.560 --> 01:05:08.420
Yeah, so if you're interested in the project,


01:05:08.420 --> 01:05:13.260
we have a Discord where you can chat with us


01:05:13.260 --> 01:05:16.780
and ask questions and see how you can best do things.


01:05:16.780 --> 01:05:19.820
It's pretty easy.


01:05:19.820 --> 01:05:22.060
- Cool, the Discord's linked right off the homepage,


01:05:22.060 --> 01:05:23.100
so that's awesome.


01:05:23.100 --> 01:05:24.800
People can find it there.


01:05:24.800 --> 01:05:27.160
Contributions, people wanna make contributions.


01:05:27.160 --> 01:05:31.940
I'm sure you're willing to accept PRs and other feedback.


01:05:31.940 --> 01:05:37.380
Welcome. Before you put in a really large PR, please first open an issue with a...


01:05:37.380 --> 01:05:51.700
To start the discussion, this contribution is welcome. And we also have a few getting


01:05:51.700 --> 01:05:56.300
started, good for beginners, good for new contributors.


01:05:56.300 --> 01:06:01.060
- Okay, yes, you've tagged or labeled some of the issues


01:06:01.060 --> 01:06:05.860
as look here if you wanna get into this, yeah?


01:06:05.860 --> 01:06:10.020
- Yeah, I must say, I think we're an interesting project


01:06:10.020 --> 01:06:13.160
to contribute to because we're,


01:06:13.160 --> 01:06:21.940
you can, it's not everything is set in stone.


01:06:21.940 --> 01:06:25.340
So there are still places where you can


01:06:25.340 --> 01:06:28.300
Clay, I'm not sure.


01:06:28.300 --> 01:06:32.000
- There's still interesting work to be done.


01:06:32.000 --> 01:06:36.880
It's not completely 100% polished and finalized.


01:06:36.880 --> 01:06:39.060
- Yeah, yeah.


01:06:39.060 --> 01:06:41.800
On the periphery, yeah, yeah.


01:06:41.800 --> 01:06:45.060
- Yeah, very cool.


01:06:45.060 --> 01:06:46.360
All right, well, let's wrap it up


01:06:46.360 --> 01:06:48.260
with a comment from the audience here.


01:06:48.260 --> 01:06:51.060
Ajit says, "Excellent content, guys.


01:06:51.060 --> 01:06:53.140
"It certainly helps me kickstart my journey


01:06:53.140 --> 01:06:55.620
from pandas to pollers.


01:06:55.620 --> 01:06:56.900
Awesome, awesome.


01:06:56.900 --> 01:06:58.100
Glad to help.


01:06:58.100 --> 01:07:00.060
I'm sure it will help many people do that.


01:07:00.060 --> 01:07:03.980
So Richie, let's close it out with final call to action.


01:07:03.980 --> 01:07:05.820
People are interested in this project.


01:07:05.820 --> 01:07:08.460
They want to start playing and learning pollers.


01:07:08.460 --> 01:07:10.100
Maybe try it out on some of their code


01:07:10.100 --> 01:07:11.940
that is pandas at the moment.


01:07:11.940 --> 01:07:14.980
What do they do?


01:07:14.980 --> 01:07:17.180
I'd recommend, if you have a new project,


01:07:17.180 --> 01:07:19.860
just start in Pollard Arsenal.


01:07:19.860 --> 01:07:23.500
because you can also rewrite some pomdes,


01:07:23.500 --> 01:07:26.980
but the most fun experience will just start a new project


01:07:26.980 --> 01:07:30.500
in Polars because then you can really enjoy


01:07:30.500 --> 01:07:32.140
what Polars offers.


01:07:32.140 --> 01:07:33.540
Learn the expression API,


01:07:33.540 --> 01:07:36.660
learn how you use it declaratively,


01:07:36.660 --> 01:07:38.480
and yeah,


01:07:38.480 --> 01:07:44.100
then it will be most fun.


01:07:44.100 --> 01:07:46.740
- Absolutely, sounds great.


01:07:46.740 --> 01:07:48.820
And like we did point out,


01:07:48.820 --> 01:07:52.260
It has the to and from pandas data frame.


01:07:52.260 --> 01:07:54.540
So you could work on a section of your code


01:07:54.540 --> 01:07:56.700
and still have it consistent, right?


01:07:56.700 --> 01:07:58.940
With other parts that have to be in this.


01:07:58.940 --> 01:08:04.780
- Yeah, you can progressively rewrite


01:08:04.780 --> 01:08:06.420
some performance heavy parts.


01:08:06.420 --> 01:08:11.020
I also think it's,


01:08:11.020 --> 01:08:14.740
so Polymer is really strict on the schema, on the types.


01:08:14.740 --> 01:08:16.700
It's also, if you write any ETL,


01:08:16.700 --> 01:08:19.540
you will be really happy to do that in Polaris


01:08:19.540 --> 01:08:22.380
because you can check the schema of a laser frame


01:08:22.380 --> 01:08:23.860
before executing it.


01:08:23.860 --> 01:08:28.860
And then you know the D types before running the query.


01:08:28.860 --> 01:08:32.020
And if the data comes in and it doesn't apply


01:08:32.020 --> 01:08:34.980
to this schema, you can fill fast


01:08:34.980 --> 01:08:37.300
and instead of having strange outputs.


01:08:37.300 --> 01:08:41.140
- Oh, that's interesting because you definitely don't want


01:08:41.140 --> 01:08:45.280
zero when you expected something else


01:08:45.280 --> 01:08:47.480
because it couldn't parse or other weird,


01:08:47.480 --> 01:08:48.320
you know, whatever, right?


01:08:48.320 --> 01:08:51.000
- Yeah, yeah, yeah, yeah.


01:08:51.000 --> 01:08:55.960
So this was my, so missing data in Polars


01:08:55.960 --> 01:08:57.600
doesn't change the schema.


01:08:57.600 --> 01:09:01.840
So Polars is really, the schema is defined


01:09:01.840 --> 01:09:06.120
by the operations and the data


01:09:06.120 --> 01:09:09.860
and not by the values in the data.


01:09:09.860 --> 01:09:12.940
So you can statically check your code.


01:09:14.240 --> 01:09:15.080
- Excellent.


01:09:15.080 --> 01:09:18.200
All right, well, congratulations on a cool project.


01:09:18.200 --> 01:09:19.480
I'm glad we got to share with everybody.


01:09:19.480 --> 01:09:21.420
Thanks for coming on the show.


01:09:21.420 --> 01:09:23.140
- Yeah, thanks for having me.


01:09:23.140 --> 01:09:24.780
- You bet, bye.


01:09:24.780 --> 01:09:25.620
- Bye.

