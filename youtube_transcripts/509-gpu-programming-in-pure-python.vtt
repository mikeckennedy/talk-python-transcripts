WEBVTT

00:00:00.020 --> 00:00:03.080
Bryce, welcome to Talk Python to Me.

00:00:03.910 --> 00:00:04.400
Awesome to have you here.

00:00:04.580 --> 00:00:05.660
I'm thrilled to be here.

00:00:05.820 --> 00:00:08.540
It's my first appearance on a podcast, actually.

00:00:09.280 --> 00:00:10.620
Okay, very cool, very cool.

00:00:10.740 --> 00:00:20.540
Well, we're going to talk a lot about GPUs and not that much about graphics, which is ironic, but that's the world we live in these days, right?

00:00:20.840 --> 00:00:21.300
It's amazing.

00:00:21.820 --> 00:00:22.480
It's funny.

00:00:22.580 --> 00:00:25.200
I've worked at NVIDIA for eight years now.

00:00:25.350 --> 00:00:27.660
I know next to nothing about graphics.

00:00:29.440 --> 00:00:30.260
That's pretty funny.

00:00:30.980 --> 00:00:39.500
I do on my gaming PC have a 2070 RTX, but I don't do any programming on it.

00:00:39.500 --> 00:00:40.040
I probably should.

00:00:40.440 --> 00:00:42.300
It's just so loud when that thing is turned on.

00:00:43.520 --> 00:00:44.320
It's like

00:00:44.320 --> 00:00:44.940
it's going to take off.

00:00:48.020 --> 00:00:49.060
But they sure are powerful.

00:00:49.360 --> 00:00:50.720
They sure are powerful, these GPUs.

00:00:50.780 --> 00:00:52.920
So it's going to be exciting to talk about it, what you can do with them.

00:00:53.920 --> 00:00:54.420
That's true.

00:00:54.500 --> 00:00:56.420
I remember when we launched the 2070.

00:00:58.380 --> 00:01:00.900
Yeah. I'm trying to think of what my first GPU was.

00:01:02.660 --> 00:01:05.840
Because when I used to play video games when I was a kid, I didn't play video games.

00:01:05.950 --> 00:01:08.440
I played text-based games, MUDs.

00:01:09.240 --> 00:01:11.940
So I never really needed a graphics card because it was all

00:01:11.940 --> 00:01:12.780
just command

00:01:12.780 --> 00:01:13.360
-mine interface.

00:01:13.900 --> 00:01:16.080
But the modem is the limiting factor

00:01:16.080 --> 00:01:17.620
or whatever

00:01:17.620 --> 00:01:18.440
for your MUD, right?

00:01:18.640 --> 00:01:19.920
I'm presuming it's on the internet.

00:01:20.780 --> 00:01:21.140
Yeah.

00:01:22.180 --> 00:01:24.580
I used to play a MUD called Shadow's Edge.

00:01:24.670 --> 00:01:26.500
I don't know if people heard of this one out there in the world.

00:01:27.700 --> 00:01:28.940
I don't think I've heard of that one.

00:01:28.940 --> 00:01:29.100
Long time ago.

00:01:29.840 --> 00:01:29.900
Yeah.

00:01:30.180 --> 00:01:30.480
This

00:01:30.480 --> 00:01:32.040
is early 90s.

00:01:32.200 --> 00:01:32.960
Early 90s.

00:01:33.140 --> 00:01:33.360
Okay.

00:01:33.520 --> 00:01:35.220
That was a little before my time.

00:01:35.300 --> 00:01:40.060
I used to play like a lot of Star Wars MUDs, and I played a pretty popular one that was called Avatar.

00:01:41.000 --> 00:01:41.900
That was one of the big ones.

00:01:42.860 --> 00:01:43.720
And that's actually how I

00:01:43.720 --> 00:01:44.840
got started programming.

00:01:46.140 --> 00:01:46.300
Yeah?

00:01:46.980 --> 00:01:47.500
Actually, I know a

00:01:47.500 --> 00:01:49.040
friend who got started that way as well.

00:01:49.440 --> 00:01:49.620
All right.

00:01:49.720 --> 00:01:50.920
Just acronym police here.

00:01:51.000 --> 00:01:52.000
What is a MUD?

00:01:52.670 --> 00:01:55.560
So it stands for Multi-User Dungeon.

00:01:56.340 --> 00:02:03.180
And it's like a very weird little corner of the gaming world because MUDs are largely not run for profit.

00:02:04.080 --> 00:02:10.160
It's like somebody creates it, and for the most part, they host it, and then they build a little community around it.

00:02:10.979 --> 00:02:12.980
And so they're little multiplayer games.

00:02:13.420 --> 00:02:16.200
Some of them are like a role-playing theme to them.

00:02:17.340 --> 00:02:20.980
And it's usually coded by volunteers, run by volunteers.

00:02:22.800 --> 00:02:25.260
And I always found it a very pure form of gaming.

00:02:26.200 --> 00:02:27.920
Yeah, I really like them as well.

00:02:28.280 --> 00:02:34.820
I just, you know, take your time, you build a little community and friends and a little world and you just go

00:02:34.820 --> 00:02:35.260
live in it.

00:02:35.340 --> 00:02:43.940
It's been a long time since I played it and I recently played something like a MUD but it wasn't multiplayer with my daughter and she thought it was the coolest thing.

00:02:44.160 --> 00:02:48.500
So there's hope for the new generation to carry on the torch, you know?

00:02:49.300 --> 00:02:57.620
Yeah, I've played some like text-based games powered by large language models where you're basically just interacting with a large language model recently.

00:02:58.800 --> 00:03:05.560
And it kind of reminded me of the MUDs from back in the day because it's all just textual interaction.

00:03:06.300 --> 00:03:06.500
Yeah.

00:03:07.030 --> 00:03:14.900
I think as the LLMs get better and the tools for building these things get better, there'll probably be some really interesting stories that

00:03:14.900 --> 00:03:15.560
are powered

00:03:15.560 --> 00:03:16.240
by them, yeah.

00:03:16.880 --> 00:03:18.720
Really interesting stories or interesting games, yeah.

00:03:18.900 --> 00:03:19.960
It's an exciting future.

00:03:20.340 --> 00:03:20.420
Yeah,

00:03:20.470 --> 00:03:21.460
I'm kind of looking forward to it.

00:03:21.760 --> 00:03:24.000
And that brings us kind of to our topic a little bit.

00:03:24.320 --> 00:03:26.360
Like, how do those LLMs get trained?

00:03:26.700 --> 00:03:27.060
Well, well,

00:03:27.100 --> 00:03:27.820
probably on

00:03:27.820 --> 00:03:28.420
NVIDIA things.

00:03:29.040 --> 00:03:31.700
Before we jump into that, though, you know, give us a quick introduction.

00:03:32.260 --> 00:03:32.580
Who are you?

00:03:33.160 --> 00:03:39.420
Well, so as I said, I got my start at programming, teaching myself how to program a MUD.

00:03:39.720 --> 00:03:41.860
And that was when I was about 19.

00:03:42.300 --> 00:03:51.500
And from there, I got involved in open source working on the Boost C++ libraries.

00:03:52.000 --> 00:03:53.760
And I was a college dropout at that point.

00:03:54.800 --> 00:03:56.200
and was looking for a job.

00:03:56.660 --> 00:04:02.820
And so I just asked somebody who I knew through open source communities, through the Boost community, like, hey, I need a job.

00:04:04.000 --> 00:04:11.960
And this guy said, you should come down to Louisiana, come work for me at my supercomputing research center.

00:04:12.720 --> 00:04:13.320
And I was like, sure.

00:04:13.750 --> 00:04:15.160
Like, I'm a college shop out, why not?

00:04:15.859 --> 00:04:20.340
My parents didn't think this was such a good idea, but I managed to convince them.

00:04:20.840 --> 00:04:51.120
So I went down there and I worked there about four years working on HPX, which was a distributed C++ runtime for HPC or high performance computing. And I sort of learned under this professor, Hartman Kaiser, who was my first mentor. And he kind of tricked me into going back to college. And so I completed my degree when I was there. And we

00:04:51.120 --> 00:04:51.740
started.

00:04:52.580 --> 00:05:14.200
It was applied mathematics. I figured if I tried to get a degree in computer science, I knew that I was an arrogant teenager and I figured I'd clash with my professors. So I was like, I got to get my major in a field where I don't know anything so that I'll respect the professors and I won't get in trouble in school. So that's the math degree.

00:05:14.900 --> 00:05:20.040
Yeah, there you go. It's easy to feel like you don't know too much doing math.

00:05:20.580 --> 00:05:21.020
Exactly.

00:05:22.510 --> 00:06:34.500
And so when I was there, we started this research group. We developed this HPX runtime together. And then after that, I went to work at Lawrence Berkeley National Lab in California, which is part of the U.S. Department of Energy research apparatus. And I was there for about two years also doing high performance computing stuff and C++ stuff. And around that time, I got involved in the C++ Standards Committee. And then I went to NVIDIA. And I've been at NVIDIA for, I think, eight years now, 2017. And at NVIDIA, what I primarily do is I work on programming language evolution. So that means thinking about how should languages like C++, Python, and Rust evolve. And in particular, because it's NVIDIA, my focus is thinking about concurrency, parallelism, and GPU acceleration. So how do we make it easier and more accessible to write parallel and GPU accelerated code in these programming languages.

00:06:35.460 --> 00:06:36.500
And it's not just programming languages.

00:06:36.550 --> 00:06:41.600
I also work a lot on library design, interface design.

00:06:42.330 --> 00:06:47.960
I started the CUDA C++ Core Libraries team here at NVIDIA.

00:06:49.860 --> 00:06:59.480
But I spent maybe the first six or seven years of my career at NVIDIA almost exclusively doing C++ stuff.

00:07:00.480 --> 00:07:08.920
And then the last one, two years, I've been getting ramped up on Python things, learning more about Python.

00:07:09.280 --> 00:07:17.080
And now I'm involved in a lot of NVIDIA's Python efforts, although I am by no means a Python expert, I would say.

00:07:18.820 --> 00:07:20.180
Interesting. That's quite the background.

00:07:21.020 --> 00:07:22.520
A couple of things I want to ask you about.

00:07:22.520 --> 00:07:24.860
I guess start from the beginning. I'm less likely to forget them.

00:07:25.440 --> 00:07:30.680
So you were doing high-performance computing and grid computing type stuff in the early days.

00:07:31.040 --> 00:07:35.840
I mean, these are like SETI at home and protein folding days and all that kind of stuff.

00:07:35.850 --> 00:07:40.840
I know that's not exactly the same computers, but what were some of the cool things you were working on?

00:07:40.890 --> 00:07:45.140
Did you come across some neat projects or have some neat stories to share from there?

00:07:45.760 --> 00:07:48.020
Yeah, so I came in.

00:07:48.740 --> 00:07:55.940
you can sort of break up the HPC era into the, like, what scale of compute we were at.

00:07:56.070 --> 00:08:03.460
And so I came in after TeraScale, you know, TeraFlops, but right at the advent of the Petaflop era.

00:08:06.180 --> 00:08:12.000
And there was sort of this, like, big question of how do we go from Petaflop to Exaflop?

00:08:12.300 --> 00:08:19.940
And at the time, we thought it was going to be really, really hard because this was before accelerators were a thing.

00:08:21.440 --> 00:08:28.680
And so the plan for going from petaflops to exaflops was just to scale it up with CPUs.

00:08:29.400 --> 00:08:39.919
And to scale from petaflops to exaflops with CPUs, to be able to do an exaflop with CPUs, you would need millions of processors and cores.

00:08:40.659 --> 00:08:45.600
And the mean time to failure for hardware components was very low.

00:08:46.020 --> 00:08:55.420
Like if you're running something on a million CPUs, a million nodes, a hard drive is going to fail every two to three minutes on average.

00:08:55.860 --> 00:08:57.480
Something's breaking all the time, right?

00:08:58.279 --> 00:08:58.720
And

00:08:58.720 --> 00:09:06.280
the computing modalities that we had at the time, we didn't think that they were going to be resilient at that scale.

00:09:07.920 --> 00:09:12.180
And so there was this big challenge of how do we come up with new computing modalities?

00:09:12.260 --> 00:09:25.400
The main modality at the time was what's called MPI, message passing interface, plus X, where X is your on-node solution for parallelism, how you're going to use the threads on your system.

00:09:27.480 --> 00:09:43.420
And so what I worked on was HPX, which was this sort of one of a variety of different competing research runtimes that were exploring new parallel computing models.

00:09:43.780 --> 00:09:50.840
And HPX was this fine-grained tasking model, so you could launch tasks that were very, very lightweight.

00:09:51.740 --> 00:09:55.160
And it also had what we'd call an active address space.

00:09:55.440 --> 00:10:05.540
So it could dynamically load balance work, and this would give you both load balancing but also resiliency, because if a node went down, you could move the work to another node.

00:10:05.980 --> 00:10:16.760
And I mostly worked on the thread scheduling system and the migration and global address space system.

00:10:18.020 --> 00:10:25.300
But what ended up happening in this push to the Exascale era is that GPUs came onto the scene.

00:10:25.360 --> 00:10:39.640
accelerators came onto the scene. And it turned out that we could get to an exaflop of compute with 10,000 nodes that had GPUs in them or 20,000 nodes that had GPUs in them.

00:10:40.700 --> 00:10:52.500
And so we were able to scale up a different way. And so it ended up that the existing modalities, the existing ways that we did parallel computing, they were basically good enough.

00:10:53.060 --> 00:10:56.480
Like, you know, the way that we did the distributed computing was more or less good enough.

00:10:56.580 --> 00:10:58.700
We just had to add the GPU programming aspect.

00:10:59.500 --> 00:11:02.100
Right, get a whole lot more parallels than per node, right?

00:11:02.640 --> 00:11:03.260
Yep, exactly.

00:11:04.440 --> 00:11:04.840
Interesting.

00:11:06.880 --> 00:11:15.460
Today, obviously, there's huge clusters of very powerful GPUs like H100s, H200 sort of processors that you all are making, right?

00:11:16.160 --> 00:11:20.920
Yep, and our new, the Blackwell processors, which somebody can get their hands on.

00:11:20.980 --> 00:11:21.620
I don't know who.

00:11:21.860 --> 00:11:24.400
It's hard for me to get my hands on them, but they're out there somewhere.

00:11:26.200 --> 00:11:26.380
Cool.

00:11:26.500 --> 00:11:28.560
What are your thoughts on ARM in the data center?

00:11:30.140 --> 00:11:31.020
I mean, I think

00:11:31.020 --> 00:11:47.000
that ARM is ultimately going to take over everything for the very simple reason that the software world loves simplicity and consistency.

00:11:47.260 --> 00:11:55.480
And if we can support one type of CPU instead of two types of CPUs, that's so great.

00:11:55.600 --> 00:11:56.420
It's like a 10x win.

00:11:57.800 --> 00:12:07.120
I often like to say with programming languages, for a new programming language to be successful, to gain inertia, it needs to be 10x better in some way.

00:12:08.279 --> 00:12:13.320
And for hardware, it's probably more like 20x or maybe 100x better.

00:12:15.000 --> 00:12:18.780
And so what is the 10x or 100x advantage that ARM has?

00:12:19.720 --> 00:12:22.740
there's lots of like actual advantages.

00:12:22.930 --> 00:12:24.980
We could talk about the merits of the hardware itself.

00:12:25.860 --> 00:12:34.500
But at the end of the day, the only thing that matters is that ARM is the architecture that's in your phones and in your tablets.

00:12:35.440 --> 00:12:41.500
And so naturally, but like x86 doesn't really scale down to the phone and the tablets.

00:12:41.940 --> 00:12:48.640
People have tried, not really a good option, not a good architecture for going at the low end of computing for a variety of reasons.

00:12:49.500 --> 00:13:03.640
And so because x86 can't survive at the low scale of computing, then even if it's a better processor at the high end of computing, naturally ARM is going to push it out of the high end of computing.

00:13:03.730 --> 00:13:09.380
And so I think that ARM is the inevitable future for CPU architectures.

00:13:09.820 --> 00:13:12.160
Eventually something will come around to disrupt.

00:13:15.079 --> 00:13:17.740
But we tend to like uniformity.

00:13:17.960 --> 00:13:21.860
And so I think that ARM will be dominant for a while.

00:13:22.380 --> 00:13:23.340
Yeah, pretty interesting.

00:13:23.780 --> 00:13:27.200
I also think ARM has a massive energy benefit or

00:13:27.200 --> 00:13:27.540
advantage.

00:13:28.240 --> 00:13:28.420
Right?

00:13:28.740 --> 00:13:33.600
And data centers are almost limited by energy these days more than

00:13:33.600 --> 00:13:35.780
energy and cooling.

00:13:36.560 --> 00:13:36.720
Yeah.

00:13:37.160 --> 00:13:39.560
And I mean, our NVIDIA, we also make CPUs.

00:13:39.560 --> 00:13:42.320
We make ARM processors for both data center and mobile.

00:13:42.540 --> 00:13:46.540
And our ARM processors are, yes, very energy efficient.

00:13:46.920 --> 00:13:50.420
And that's one of the big advantages for us in picking that architecture.

00:13:50.700 --> 00:13:57.920
Yeah, when you hear headlines like, Microsoft is starting up Three Mile Island again for a data center.

00:13:58.460 --> 00:13:58.740
Yeah.

00:13:58.960 --> 00:14:00.760
Oh, that's a lot of energy they need.

00:14:01.420 --> 00:14:02.080
Yep, yeah.

00:14:02.610 --> 00:14:06.640
Yeah, it is becoming the limiting factor on compute, yeah.

00:14:07.400 --> 00:14:07.520
Yeah.

00:14:08.620 --> 00:14:10.820
Okay, that was one thing.

00:14:11.030 --> 00:14:30.940
The other thing I want to talk to you about is, before we get into the topic exactly, is with all of your background in C++, working with the standards, you know, give people a sense of, like, how has C++ evolved? Like, when you're talking to me, you're talking to a guy who did some professional C++, but stopped at the year 2000. You know what I mean?

00:14:31.080 --> 00:14:35.580
Like, there's one view of C++, and there's probably something different now.

00:14:36.560 --> 00:14:44.640
So, C++ is, the evolution of C++ is managed by an international standards organization, ISO.

00:14:46.160 --> 00:14:55.760
And ISO has a very interesting stakeholder model where the stakeholders in ISO are national delegations or national bodies, as we call them.

00:14:56.020 --> 00:15:00.860
So each different country that participates in ISO can choose to send a national delegation.

00:15:01.760 --> 00:15:05.040
And each different country has different roles for how their national delegations work.

00:15:05.720 --> 00:15:10.960
In the U.S., membership in the national delegation is by company.

00:15:11.820 --> 00:15:15.000
And there's no real requirement other than that you have to pay a fee.

00:15:15.680 --> 00:15:21.640
It's like 2.5K a year, and you can join the C++ committee if you're a US-based organization.

00:15:23.220 --> 00:15:28.320
In other countries, like in the UK, it's a panel of experts.

00:15:29.200 --> 00:15:32.920
And you're invited by the panel of experts to join the panel of experts.

00:15:34.500 --> 00:15:37.420
And there's different, like some other countries have different models.

00:15:37.420 --> 00:15:40.780
In some countries, the standards body is actually run by the government.

00:15:42.520 --> 00:15:48.500
And so you have all these experts, these national delegates that come together, and then they all work on C++.

00:15:51.000 --> 00:16:01.620
And there's a lot of bureaucracy and procedure, and it's sort of like a model UN, but for a programming language.

00:16:02.100 --> 00:16:05.140
Yeah, for angle brackets and semicolons.

00:16:05.160 --> 00:16:05.440
Got it.

00:16:06.040 --> 00:16:10.680
A lot of people on the committee love to talk about all the details of how the committee works.

00:16:11.660 --> 00:16:14.220
I don't really think that it's particularly important.

00:16:14.420 --> 00:16:18.880
I think that the key thing to understand is that it's sort of got an odd stakeholder model.

00:16:19.540 --> 00:16:25.260
It's not a stakeholder model where it's like, oh, let's get the major implementations together or let's get the major users together.

00:16:26.120 --> 00:16:31.680
For the most part, it's like anybody who can figure out how to join a national body can participate.

00:16:32.380 --> 00:16:47.960
And if you happen to live from some small country where you're the only delegate, then you get the same vote as like the entire United States national body, because at the end of the day, votes on the C++ standard are by national body.

00:16:49.360 --> 00:16:53.580
And so there's some people that have an outsized influence in the C++ committee.

00:16:54.760 --> 00:16:58.160
The C++ committee itself is organized into a number of subgroups.

00:16:59.140 --> 00:17:00.680
There's one for language evolution.

00:17:01.620 --> 00:17:05.560
There's one for library evolution, which I chaired for about three years.

00:17:06.480 --> 00:17:12.079
And then there are core groups for both language and library.

00:17:12.720 --> 00:17:19.540
And so the evolution groups, they work on, you know, the design of new features and proposals.

00:17:19.780 --> 00:17:24.140
And then the core groups sort of vet those designs and make sure that they're really solid.

00:17:24.920 --> 00:17:29.000
And then there's a bunch of study groups that there's one for concurrency.

00:17:29.360 --> 00:17:33.840
There's one for particular features like ranges or reflection.

00:17:34.660 --> 00:17:43.140
And those groups develop those particular features or represent a particular interest area, like game developers, for example.

00:17:43.770 --> 00:17:49.360
And proposals flow from those study groups to those evolution groups and then through the core groups.

00:17:49.960 --> 00:17:53.340
And then eventually they go into the standard and then the national bodies vote on the standard.

00:17:54.840 --> 00:17:56.300
Sounds pretty involved.

00:17:56.380 --> 00:17:57.200
I guess the

00:17:57.200 --> 00:17:57.540
main thing

00:17:57.540 --> 00:18:04.260
I was wondering about is like, how different is C++ today versus 20, 30 years ago?

00:18:06.100 --> 00:18:08.680
I mean, I think it's a radically different language.

00:18:12.359 --> 00:18:23.300
C++11 completely changed the language in many ways and really revitalized it after a very long period after the first standard, which was C++98.

00:18:24.040 --> 00:18:34.000
And then after C++11, C++ began shipping new versions every three years, and whatever features were ready would ship.

00:18:34.060 --> 00:18:36.700
So we adopted a consistent ship cycle.

00:18:38.100 --> 00:18:42.620
And the next big revision after C++11 was C++20.

00:18:44.900 --> 00:18:47.840
Not as transformative, I think, as C++11, but pretty close.

00:18:48.820 --> 00:18:57.140
And then we're just about to finish C++26, which will also be a pretty substantial release.

00:18:57.580 --> 00:19:07.800
And probably by the time that this goes out to your podcast audience, we'll be right around when we finalize the feature set for C++26.

00:19:09.120 --> 00:19:09.440
Interesting.

00:19:09.630 --> 00:19:12.760
So if I want to do more C++, I probably need to start over and learn it again.

00:19:14.900 --> 00:19:20.280
If you learned it before C++11, yeah you'd have to relearn some patterns

00:19:21.180 --> 00:19:21.780
we

00:19:21.780 --> 00:19:31.200
like to talk about modern C++ versus you know old C++ C++11 is sort of like a Python 2 to Python 3 sort of jump

00:19:31.650 --> 00:19:33.380
except without

00:19:34.540 --> 00:19:46.299
as much breaking behavior there was very little breaking behavior but the best practices changed drastically from the pre C++11 era to the modern era

00:19:46.960 --> 00:19:55.060
Okay. Yeah, super interesting. I honestly could talk to you for a long time about this, but this is not a C++ show, so let's move on to Python.

00:19:55.180 --> 00:20:06.360
But I've been focused a little bit on C++ because traditionally that's been one of the really important ways to program GPUs and work with CUDA and things like that, right?

00:20:06.620 --> 00:20:16.720
And now one of the things that you all have announced, released, are all working on is CUDA-Python, right?

00:20:16.920 --> 00:20:18.220
How long has this been out here for?

00:20:19.120 --> 00:20:20.420
Five months, something like that?

00:20:20.940 --> 00:20:21.460
Not terribly long.

00:20:21.480 --> 00:20:21.840
I don't know

00:20:21.840 --> 00:20:25.100
how long the repo's been out, but the CUDA-Python effort's been around for about a year or two.

00:20:25.620 --> 00:20:37.820
And you're absolutely right that for a long time, C++ was the primary interface to not just our compute platform but to most compute platforms.

00:20:39.860 --> 00:20:41.100
And so what changed?

00:20:41.460 --> 00:20:48.480
Well, the two big things is that data science and machine learning happened.

00:20:49.940 --> 00:21:11.980
Both fields that tend to have a lot of domain experts, computational scientists, who are not necessarily interested in writing low-level code in C++ and learning the best practices of software engineering in a system's language like C++.

00:21:12.540 --> 00:21:17.780
They just want to be able to do their domain expertise, to do their data science, or to build their machine learning models.

00:21:18.420 --> 00:21:24.540
So naturally they gravitated towards a more accessible and user-friendly language, Python.

00:21:26.340 --> 00:21:36.880
And it became apparent a couple of years ago within NVIDIA that we needed to make our platform language agnostic.

00:21:37.500 --> 00:21:40.860
And that's really what we've been focusing on the last year or two.

00:21:42.639 --> 00:21:48.580
And CUDA Python is not just us saying, let's add another language.

00:21:48.940 --> 00:21:50.720
Okay, now we're going to do Python and C++.

00:21:50.980 --> 00:21:54.600
It really reflects our overall goal of making the platform more language agnostic.

00:21:56.080 --> 00:22:14.600
And you'll see that in our focus more and more on exposing things at the compiler level, exposing ways for other languages, other compilers, other DSLs to target our platform via things like MLIR dialects, which we've announced a bunch of recently.

00:22:16.220 --> 00:22:20.380
But, you know, CUDA Python obviously was the place where we needed to start.

00:22:20.460 --> 00:22:31.360
So the goal of CUDA Python is to provide the same experience with more or less the same performance that you would get in C++.

00:22:33.980 --> 00:22:43.560
And when I say more or less, I mean that there are some higher level parts of CUDA Python that don't necessarily map directly to C++ things.

00:22:44.160 --> 00:22:53.880
but the parts of Cuda Python that have direct counterparts in Cuda C++, we expect that you will get the same performance.

00:22:54.740 --> 00:23:02.960
And we think that's really important because we don't want users to have to sacrifice performance to be able to do things natively within Python.

00:23:04.940 --> 00:23:06.200
That's pretty impressive, honestly.

00:23:08.060 --> 00:23:22.560
I think a lot of times when people think about doing stuff with Python, And you'll, I don't know, they're shown or they discover some kind of benchmark that is 100% Python or 100% the other language.

00:23:23.320 --> 00:23:27.000
Like, oh, here's the three-body problem implemented in Python.

00:23:27.160 --> 00:23:28.720
And here it is implemented in Rust.

00:23:28.760 --> 00:23:31.260
And look, it's 100 times slower or something.

00:23:31.880 --> 00:23:43.300
But much of this work, much of the data science work especially, but even in the web, a lot of times what you're doing is you take a few pieces together in Python and you hand it off to something else.

00:23:44.060 --> 00:23:53.760
right in this case you're handing it off to the gpu through the cuda bindings the c bindings and once it's in there it's off to the races internally

00:23:53.760 --> 00:23:54.380
right and

00:23:54.380 --> 00:24:13.280
yeah and when i think of like the web you very likely are taking a little bit of data a little bit of string stuff doing some dictionary things handing it to a database or handing it to a third-party api and again it it doesn't matter what you're doing like it's off into this whatever that's written in c or whatever for the database and

00:24:14.500 --> 00:24:15.840
but it's interesting

00:24:16.020 --> 00:24:22.680
tell me a bit about the work you had to do to sort of juggle that and your dog is also super interesting so I would like to hear from him or her?

00:24:23.260 --> 00:24:25.080
Her, her Okay I'd like to hear from

00:24:25.960 --> 00:24:26.600
what's her name?

00:24:26.800 --> 00:24:31.840
Her name is Looney she's a six year old Australian cattle dog and

00:24:31.840 --> 00:24:32.540
she's excited

00:24:32.540 --> 00:24:33.740
because her mom just got home

00:24:34.200 --> 00:24:34.600
Beautiful,

00:24:35.000 --> 00:24:35.500
well we

00:24:35.500 --> 00:24:37.920
can hear from her her thoughts on Kuda Python as well

00:24:39.080 --> 00:24:46.400
I'll first getting back to what you're getting at about sometimes people think of Python as being a slow language.

00:24:46.840 --> 00:24:56.520
I actually will make the claim that Python is a great language for doing high-performance work.

00:24:56.860 --> 00:25:01.740
And the reason for that is because Python, it's very easy.

00:25:02.520 --> 00:25:05.720
Python's a very flexible language where it's very easy to do two things.

00:25:06.000 --> 00:25:07.320
One, to

00:25:07.320 --> 00:25:10.320
optimize

00:25:10.320 --> 00:25:15.120
the fast path to either through JIT or through Cython extensions.

00:25:16.640 --> 00:25:17.960
It's very amenable to that.

00:25:18.980 --> 00:25:32.680
And two, the language semantics are flexible enough that it's in the AST and the parsing is accessible enough that it's super, super easy to build a DSL in Python.

00:25:33.230 --> 00:25:48.000
And because the language semantics are flexible, it's very easy to build a DSL where it's like, okay, well, our DSL, you write the syntax of Python, And there's some caveats here where some of the things that you know about Python are maybe a little bit different in this DSL.

00:25:48.600 --> 00:25:51.520
But with those relaxations, we can give you super fast code.

00:25:52.100 --> 00:26:02.260
And that's how things like Numba and things like Numbacuda, things like Coupyx and things like Triton, Leng, that's how those things all work.

00:26:03.640 --> 00:26:06.340
They build the DSL where they take Python-like syntax.

00:26:07.080 --> 00:26:11.260
They follow most of the rules of Python with a couple, you know, relaxations, restrictions, et cetera.

00:26:11.960 --> 00:26:14.880
and then they give you super fast code that has native performance.

00:26:15.520 --> 00:26:30.700
And if you look at other languages that have tried to deliver on this, that have tried to have a managed runtime, tried to give you portability and high-level ease of use and also performance, a lot of the other ones have failed.

00:26:31.120 --> 00:26:42.260
I remember seeing a talk a couple of years ago from one of the lead engineers on, I forget which JVM, but of a particular JVM implementation.

00:26:42.940 --> 00:26:51.300
And he was talking about everything that goes into making a native call from Java, like the protocol for Java to call a C function.

00:26:51.860 --> 00:26:58.660
And he was showing us in assembly, like the call convention, and you have to do all this stuff and save and restore all this stuff.

00:26:59.260 --> 00:27:03.040
And he was like, and we have to do all this work to be able to make this fast.

00:27:04.559 --> 00:27:07.560
And because Java doesn't have as flexible as

00:27:07.560 --> 00:27:07.960
semantics,

00:27:08.220 --> 00:27:09.140
you have to do all that.

00:27:09.500 --> 00:27:10.860
But in Python, it's so much easier.

00:27:11.520 --> 00:27:21.600
And this is, I think, one of the reasons why Python has succeeded as a language because it's so easy when you need to put something in production.

00:27:21.900 --> 00:27:24.340
If it is slow, it's so easy to make that slow thing fast.

00:27:26.100 --> 00:27:27.020
That's a really interesting take.

00:27:27.040 --> 00:27:27.920
I hadn't really thought of that.

00:27:28.900 --> 00:27:31.900
Because it's a more malleable language that can be

00:27:31.900 --> 00:27:32.700
shaped

00:27:32.700 --> 00:27:34.700
to adapt to something underlying that's faster.

00:27:35.100 --> 00:27:36.400
Yeah, exactly.

00:27:36.700 --> 00:27:41.940
How much does Numba or Cython or things like that factor into what's happening here?

00:27:41.940 --> 00:27:46.460
I see it's 16% Cython according to GitHub, but

00:27:46.460 --> 00:27:47.540
those

00:27:47.540 --> 00:27:49.120
stats are sometimes crazy.

00:27:49.780 --> 00:27:54.080
I mean, we use Cython in a lot of places on fast paths.

00:27:55.500 --> 00:27:59.620
Now, with CUDA, there's a couple different types of things that you do with CUDA.

00:27:59.800 --> 00:28:10.020
The first with CUDA is when you're writing code that runs on your CPU that is managing and orchestrating GPU actions.

00:28:10.650 --> 00:28:12.580
That could be allocating GPU memory.

00:28:13.340 --> 00:28:17.420
That could be launching and waiting on CUDA kernels.

00:28:18.560 --> 00:28:20.780
It could be that sort of thing.

00:28:21.340 --> 00:28:28.280
Making work, making memory, transferring work in memory, waiting on things, setting up dependencies, et cetera.

00:28:29.440 --> 00:29:00.660
for the most part, a lot of those tasks are not going to be performance critical. And the reason for that is because the limiting factor for performance is typically around like synchronization costs. And like if your major cost is like, you know, acquiring a lock or allocating memory, that whether you call Cuda Malik from C++ or Python, it doesn't really matter what language you're calling it from.

00:29:01.040 --> 00:29:05.320
Cuda Malik is going to take a little while because it's got to go and get storage.

00:29:06.260 --> 00:29:08.960
Now, one of the exceptions to this is when you're launching a kernel.

00:29:09.240 --> 00:29:11.540
That's a thing that we want to be very, very fast.

00:29:12.160 --> 00:29:17.880
So some of our frameworks have their kernel launch path siphonized.

00:29:19.280 --> 00:29:21.740
Numba, we use pretty extensively.

00:29:22.280 --> 00:29:26.100
So we use Numba for the other piece.

00:29:26.320 --> 00:29:31.020
So I just talked about the orchestration and management of GPU work and memory.

00:29:31.740 --> 00:29:36.660
But how do you actually write your own algorithms that run in the GPU?

00:29:37.580 --> 00:29:43.080
So you don't have to do this frequently because we provide a whole huge library of existing algorithms.

00:29:43.900 --> 00:29:48.280
And generally we advise you should try to use the existing algorithms.

00:29:48.520 --> 00:29:52.460
We've got a lot of top people who spend a lot of time making sure that those algorithms are fast.

00:29:52.640 --> 00:29:56.200
So you should try really hard to use the algorithms that we provide.

00:29:56.600 --> 00:29:57.920
But sometimes you've got to write your own algorithm.

00:29:58.460 --> 00:30:14.560
And if you want to do that, you need some sort of, if you want to do that natively in Python, you need some sort of JIT compiler that's going to know how to compile some DSL that's going to JIT compile it down to native code that can run on the device.

00:30:14.700 --> 00:30:15.940
And we use Numba for that.

00:30:16.380 --> 00:30:17.840
There's a Numba backend called CUDA.

00:30:17.880 --> 00:30:27.400
that allows you to write CUDA kernels in Python with the same speed and performance that you'd get out of writing those kernels in CUDA C++.

00:30:28.120 --> 00:30:35.480
And then we have a number of libraries for the device-side programming that you can use in Numba CUDA.

00:30:36.140 --> 00:30:36.480
Yeah.

00:30:37.340 --> 00:30:44.700
Well, that sounds pretty useful, like a pretty good library to start with instead of trying to just straight talk to it that you use directly.

00:30:46.880 --> 00:30:56.440
So how much do people need to understand GPUs, GPU architecture and compute, especially Python people, to work with it?

00:30:56.700 --> 00:31:00.140
I see a bunch of things like you talked about kernels.

00:31:01.720 --> 00:31:04.580
There's warps, memory hierarchies, threads.

00:31:05.880 --> 00:31:09.920
Give us a sense of some of these definitions and then which ones we need to pay attention to.

00:31:10.440 --> 00:31:14.180
So most people need to know very little of this.

00:31:15.300 --> 00:31:40.120
When we teach CUDA programming, when we teach it in C++ even these days, we start off by teaching you how to use the CUDA algorithms, how to use CUDA accelerated libraries, how to use kernels or GPU code that people have already written, where you just plug in operators in the data that you want, and how to use that.

00:31:40.920 --> 00:31:49.160
And then we teach you about the notion of different kinds of memory, memory that the CPU can access and that the GPU can access.

00:31:51.120 --> 00:31:56.680
And then we teach you about how to optimize using those algorithms and that memory.

00:31:57.200 --> 00:32:03.360
And it's only after hour three or four of the education do we introduce to you the idea of writing a kernel.

00:32:03.880 --> 00:32:07.020
Because we don't want you to have to write your own kernels.

00:32:07.240 --> 00:32:13.440
because it's not the highest productivity thing, and oftentimes it's not going to be the highest performance thing.

00:32:13.920 --> 00:32:33.080
Writing GPU kernels has gotten more and more complex as GPUs have matured because the way that we get more and more perf out of our hardware today because we can no longer just get more saw scaling is that we have to expose more and more of the complexity of the hardware to the programmer.

00:32:33.920 --> 00:32:43.640
And so writing a good GPU kernel today is in some ways easier, but in some ways more challenging than it was five to 10 years ago.

00:32:44.140 --> 00:32:53.060
Because five to 10 years ago, there were a lot less tools to help you out in writing a GPU algorithm, but the hardware was a bit simpler.

00:32:55.320 --> 00:33:02.120
Most people, I would say 90 to 95% of people do not need to write their own kernels.

00:33:02.460 --> 00:33:07.700
And if you don't need to write your own kernels, you don't have to understand the CUDA thread hierarchy.

00:33:08.480 --> 00:33:12.280
And the CUDA thread hierarchy is what warps and blocks are.

00:33:12.520 --> 00:33:23.260
So on the GPU, you've got a bunch of different threads, and those threads are grouped into subsets that are called blocks.

00:33:24.080 --> 00:33:27.020
And the blocks all run at the same time.

00:33:28.780 --> 00:33:32.600
So all the threads in the block run at the same time, rather.

00:33:33.320 --> 00:33:37.920
And all the threads within the block have fast ways of communicating to each other.

00:33:38.680 --> 00:33:42.120
And they have fast memory that they can use to communicate to each other.

00:33:42.700 --> 00:33:44.820
This scratchpad memory that we call shared memory.

00:33:45.820 --> 00:33:56.100
And blocks are further divided into warps, which are smaller subsets of threads that are executed as one.

00:33:56.720 --> 00:34:03.360
like they do the same operations on the same particular physical piece of hardware at a time.

00:34:03.840 --> 00:34:09.740
They do each have an individual state so they can be at different positions.

00:34:11.020 --> 00:34:14.700
They essentially each have their own thread but they're executed in lockstep.

00:34:16.800 --> 00:34:22.820
But you don't need to know most of that if you're looking to use CUDA.

00:34:22.990 --> 00:34:30.020
What you need to understand is some of the basics of parallel programming and how to use algorithms.

00:34:31.090 --> 00:34:38.260
And there are, I would say, three main generic algorithms that matter.

00:34:39.960 --> 00:34:44.240
And the first is just like four each, just a for loop, you know.

00:34:45.540 --> 00:34:56.980
And the for loop that we most often think about is the one that we call transform, where you've got an input of an array of, some shape and an output of an array of some shape.

00:34:57.910 --> 00:35:04.140
And you just apply a function to every element of the input, and then that produces the output for you.

00:35:05.200 --> 00:35:05.420
Interesting.

00:35:05.550 --> 00:35:05.680
The

00:35:05.680 --> 00:35:06.560
second algorithm is...

00:35:06.560 --> 00:35:08.740
Sounds a little like pandas or something like that, right?

00:35:09.050 --> 00:35:09.160
Yeah.

00:35:09.220 --> 00:35:09.980
Vectorized math

00:35:09.980 --> 00:35:10.420
and so on.

00:35:10.940 --> 00:35:11.540
Yes, exactly.

00:35:13.480 --> 00:35:14.560
Yeah, that's exactly right.

00:35:14.650 --> 00:35:26.560
Or in Numba, it's like the notion of a generalized universal function or like a ufunk from NumPy or something like that, just an element-wise function.

00:35:28.640 --> 00:35:32.240
The next algorithm is reduction.

00:35:33.780 --> 00:35:35.500
So this is like doing a sum.

00:35:35.550 --> 00:35:38.260
It's just a generalized version of doing a sum over something.

00:35:40.959 --> 00:35:52.700
And reduction is a basis operation that you can use to implement all sorts of things like counting, any form of counting, any form of searching.

00:35:52.880 --> 00:35:55.740
If you're looking for the maximum of something, you can do that with a reduction.

00:35:56.900 --> 00:36:04.540
And then the last algorithm is a scan, which is also known in some circles as a partial sum.

00:36:04.640 --> 00:36:12.700
So a scan works somewhat similar to a reduction, but it gives you the intermediate sums.

00:36:13.380 --> 00:36:30.940
So if you're scanning an array of length n, the output of that scan is the sum of the first element and the second element, the first element, the second element, and the third element, the first element, the second element, the third element, and the fourth element, et cetera, through to the end.

00:36:31.680 --> 00:36:39.800
And scans are very useful for anything that has position-aware logic.

00:36:40.240 --> 00:36:55.660
Like if you want to do something where you want to reason about adjacent values, or if you want to do something like a copy if or like a filter or something like that, that's what you'd use a scan for.

00:36:57.420 --> 00:37:04.940
And those three, I think, are the basis of programming with parallel algorithms.

00:37:05.180 --> 00:37:17.160
And you would be surprised at how often a parallel programming problem will break down into calling some combination of those three algorithms.

00:37:17.960 --> 00:37:19.680
Yeah, that's very interesting.

00:37:21.780 --> 00:37:29.340
So with this version of Python 3.13, we kind of have no more gil.

00:37:30.200 --> 00:37:30.420
Right.

00:37:30.860 --> 00:37:31.780
How much does that matter?

00:37:31.960 --> 00:37:36.020
It sounds like it might not actually really make much difference for this project.

00:37:38.600 --> 00:37:40.380
So this is a question I get asked a lot internally.

00:37:42.100 --> 00:37:43.880
No, I do think it matters.

00:37:45.540 --> 00:37:56.080
One of the reasons it matters is that these days, you don't normally have just one GPU in a high-end system.

00:37:56.280 --> 00:37:58.320
You tend to have multiple GPUs.

00:37:59.380 --> 00:38:02.020
Multiple mean two or four or 20?

00:38:03.160 --> 00:38:05.100
Two, four, or eight. Eight is

00:38:05.100 --> 00:38:05.720
what you typically

00:38:05.720 --> 00:38:05.960
see.

00:38:06.000 --> 00:38:14.640
You typically see like two CPUs and eight GPUs in like your default like high-end compute

00:38:14.640 --> 00:38:16.380
node.

00:38:17.280 --> 00:38:24.100
Yeah, but even if you just serialize one of those, that's only 16% of the GPU processing, right?

00:38:24.240 --> 00:38:24.400
Like

00:38:24.400 --> 00:38:25.000
one of the...

00:38:25.480 --> 00:38:25.660
Yeah.

00:38:25.840 --> 00:38:25.940
Right.

00:38:27.020 --> 00:38:38.860
And so oftentimes to feed all the GPUs, need to have parallelism on the CPU.

00:38:39.180 --> 00:38:52.080
It's oftentimes not sufficient to just have one thread in the CPU, launch everything. And there's also like, GPUs are not the answer for everything.

00:38:53.839 --> 00:39:07.320
People often have ideas of what they think a GPU is, but I'll tell you my definition of what a GPU is. A GPU is just a bandwidth-optimized processor. It's a general-purpose processor, just like a CPU.

00:39:08.360 --> 00:39:32.560
But a CPU is optimized for latency. It's optimized for the single-threaded case. It's optimized for getting you an answer for every operation as quickly as possible. A GPU is optimized for bandwidth. If you ask a GPU to load from memory, it's going to take a while. If you ask a GPU to add something, it's going to take a while. But it will have a higher bandwidth of doing those things.

00:39:33.340 --> 00:39:48.980
And so for some tasks, a lot of like data loading, storing, and ingestion tasks, the CPU might be the better fit. And also the CPU is generally the better thing to talk to disk and to talk to network directly.

00:39:49.700 --> 00:39:57.280
And so for a lot of applications, the CPU's got to do work to get data ready and then to communicate that data to the GPU.

00:39:58.280 --> 00:40:11.200
And oftentimes the highest performance architectures are going to be ones where that data prep and loading and command and control work is being done in parallel.

00:40:12.260 --> 00:40:22.260
And a gill-less Python will enable us to be able to express those efficient command and control architectures directly in Python.

00:40:23.200 --> 00:40:24.020
And to be able to have

00:40:24.020 --> 00:40:26.060
Python efficiently communicate with the GPU.

00:40:26.320 --> 00:40:30.160
Maybe make a thread per GPU or something like that.

00:40:30.290 --> 00:40:33.960
Right. Or even multiple threads per GPU you may need in some cases.

00:40:35.619 --> 00:40:39.000
Yeah. Probably depends on how many threads your CPU has also.

00:40:40.160 --> 00:40:40.620
Yes, definitely.

00:40:41.520 --> 00:40:56.900
Yeah. Okay. Well, that all sounds super interesting. I want to dive into the different building blocks. There's all these, like CUDA Python is at least in its current and ongoing, maybe future form, what's called a meta package

00:40:56.900 --> 00:40:58.480
in the sense that

00:40:58.480 --> 00:41:35.500
it is not itself a thing that has a bunch of functionality, but it sort of claims dependency on a bunch of things and does an import of them and centralizes that to like sort of bundle a bunch of pieces, right? So I want about all of those but before we do give people out there who maybe have Python I guess mostly data science problems maybe AI/ML problems or it'd be interesting if there was something that didn't fit either of those that is like good to solve with CUDA and GBU programming like maybe give us some examples of what people build with this

00:41:35.500 --> 00:41:44.940
so I think the more important question is usually what order of magnitude of data do you have to work with?

00:41:45.020 --> 00:41:50.300
If you don't have a large enough problem size, you're not going to get a benefit out of using a GPU.

00:41:51.020 --> 00:41:52.200
Okay, define large.

00:41:53.340 --> 00:41:53.760
That

00:41:53.760 --> 00:41:55.300
means different things from different people.

00:41:55.540 --> 00:41:55.740
Measured in

00:41:55.740 --> 00:41:56.080
gigabytes.

00:41:56.760 --> 00:41:58.940
Measured in gigabytes of memory footprint.

00:41:59.900 --> 00:42:00.060
Okay.

00:42:00.640 --> 00:42:04.780
You typically will need to have, well, it's a little more nuanced than that.

00:42:05.920 --> 00:42:12.240
If the compute that you're doing is linear in your problem size.

00:42:13.480 --> 00:42:25.520
That is, if you're doing like O-N, you know, operations, like you're doing, you know, some linear number of operations like per element in your data size, then you need gigabytes.

00:42:27.819 --> 00:42:53.940
If you're doing more than that per element, if you've got something that's like, you know, N squared or exponential or N log in, something like sorting, where it's not going to scale linearly with the number of elements, where it's going to be worse than that, then you might have a smaller problem size that will make sense to run on the GPU.

00:42:55.080 --> 00:43:00.300
But the majority of people who have a compute task have things that fall into the ON regime.

00:43:00.860 --> 00:43:08.140
It's like, oh, I've got a set of N things and I want to apply this function to each one of them.

00:43:09.660 --> 00:43:11.740
And in that case, you normally need gigabytes.

00:43:12.110 --> 00:43:20.840
If you're sorting things, if you're sorting 100 megabytes of ints, you'll probably see a speedup on a GPU.

00:43:21.910 --> 00:43:32.860
If you're just adding a million ints to a million ints, that's probably about the cutoff point for seeing a benefit from using the GPU.

00:43:33.200 --> 00:43:48.040
Um, the types of workloads, um, I think generally, uh, it's, it's sort of hard to say because it's so broad in what you can do.

00:43:48.620 --> 00:43:51.560
You need to have something that has some amount of parallelism.

00:43:52.059 --> 00:43:58.780
Um, so there needs to be some, you know, uh, data parallel aspect to, to your problem.

00:43:59.360 --> 00:44:01.720
If you can't paralyze it, you're not going to benefit from the GPS.

00:44:02.040 --> 00:44:12.400
All right. Maybe I have questions about all the, I don't know, sales or some sort of prediction per state for all 50 states in the U.S. or all countries

00:44:12.400 --> 00:44:12.700
in the

00:44:12.700 --> 00:44:15.840
world. You could just break it up by country or state and let it rip.

00:44:15.860 --> 00:44:16.020
Right.

00:44:16.620 --> 00:44:25.900
Yeah. And those are definitely the easiest. When it's completely embarrassingly parallel, that's usually a good sign that it's something that will fit well in GPO.

00:44:26.360 --> 00:44:43.500
But also, if you have something like, you know, I want to add up, I want to take the sum of every integer, you know, in this huge data set or something like that, that's also a task that GPUs can be good for, even though it's not embarrassingly parallel, even though there are data dependencies.

00:44:44.420 --> 00:44:44.820
Sure.

00:44:45.980 --> 00:44:46.140
Okay.

00:44:49.580 --> 00:44:51.020
Where is a good place to run it?

00:44:51.080 --> 00:45:00.800
Something I've been fascinated with is this thing you guys announced but have not yet shipped, the home AI computer, this little golden

00:45:00.800 --> 00:45:01.240
box.

00:45:03.700 --> 00:45:06.500
The DGX, I think we originally called it Digits.

00:45:06.580 --> 00:45:07.800
Now we call it DGX Spark.

00:45:09.480 --> 00:45:24.380
So this is a box that has, I think, a 20-core NVIDIA ARM CPU and the small latest generation NVIDIA GPU, an NVIDIA Blackwell.

00:45:25.820 --> 00:45:26.940
And it's a little box.

00:45:27.280 --> 00:45:28.720
It'll sit on your desktop.

00:45:30.080 --> 00:45:31.340
It goes right next to my Mac Mini.

00:45:31.530 --> 00:45:32.060
It would be perfect.

00:45:32.240 --> 00:45:32.360
Yeah.

00:45:32.960 --> 00:45:34.320
Yeah, it's about the size of a Mac Mini.

00:45:34.740 --> 00:45:37.200
A little bit more oomph to it.

00:45:38.640 --> 00:45:53.680
Yeah, I think that for a lot of people who are doing compute development work, if you're not doing anything graphics-related, this is probably the best entry-level thing for you.

00:45:55.520 --> 00:46:00.340
If you have a graphics card, you can use those for compute, too.

00:46:10.540 --> 00:46:11.560
Oh, I just lost you.

00:46:13.320 --> 00:46:23.200
hold on let me Bryce I lost you for a second are you still there?

00:46:24.060 --> 00:46:28.540
did your microphone maybe did the battery go out or something?

00:46:29.180 --> 00:46:29.600
is that possible?

00:46:48.720 --> 00:46:49.400
okay how about now

00:46:49.400 --> 00:46:51.660
bingo yeah

00:46:51.660 --> 00:46:56.880
sorry audio quality a little bit worse i guess by a better idea um yeah we're

00:46:56.880 --> 00:47:02.300
just gonna we're gonna bring it home we're gonna bring it home a little a little scratchy but that's all right it still sounds fine let's let's keep going

00:47:03.240 --> 00:47:41.580
the so the dgx sparks great it like great entry-level platform um but there is no reason to have to buy something if you want to play around with cuda um one of the best places to play around with cuda python i think is google colab google colab it's a jupiter notebook environment. And they have three GPU instances with T4 GPUs. It's not there by default, but if you go in, if you go to runtime, you can change to use a GPU environment.

00:47:42.700 --> 00:47:53.900
And then you can play around with Python in the CoLab environment. There is also Compiler Explorer, which is an online platform.

00:47:54.640 --> 00:47:57.320
It's godbolt.org is the link.

00:47:57.320 --> 00:48:09.760
It's an online compiler sandbox, and it has GPU environments.

00:48:10.240 --> 00:48:21.120
And it does also have Python support, although I think they're still working on getting packaging to be available here.

00:48:21.860 --> 00:48:36.220
But for something like CUDA C++, you can use this to write code and then see what the assembly is that you'd get, make sure that your code compiles, and then also run that code, and you can even run the code on a GPU.

00:48:36.920 --> 00:48:44.400
So I think if you want to get started, there's a lot of different places where you can do GPU development without having to have your own GPU.

00:48:46.040 --> 00:49:12.300
Okay. I guess another couple possibilities, you know, it's not too bad. It's not that cheap, but if you don't leave it on all the time, it's not too bad to get a cloud Linux machine that's got some GPU component. But if you leave it on all the time, they do get expensive. So set yourself a reminder to turn it off. Another one that's, you know, I was just thinking about with my Mac Mini here.

00:49:13.840 --> 00:49:20.240
I don't know that you've said it yet, but I'm pretty sure it's still true that CUDA requires NVIDIA GPUs, not just a GPU, right?

00:49:20.540 --> 00:49:21.640
That is true, yes.

00:49:22.140 --> 00:49:22.240
Yeah.

00:49:22.820 --> 00:49:25.280
So it's not going to work super well on my Apple Silicon

00:49:25.280 --> 00:49:26.600
for

00:49:26.600 --> 00:49:27.060
that reason,

00:49:27.400 --> 00:49:29.360
amongst possibly others, but certainly that's one of them.

00:49:30.040 --> 00:49:35.060
But I have like a gaming PC, or you might have a workstation with a really good GPU.

00:49:35.800 --> 00:49:46.000
You could set up things like change your Docker host on your computer and anything you run on Docker will run over there on that machine or build on that machine and so on.

00:49:46.720 --> 00:49:54.560
So that's a pretty interesting way to say, well, everybody in the lab, we're changing your Docker host to like the one big machine that's got the NVIDIA GPU.

00:49:55.760 --> 00:49:56.680
I think there's a lot of flexibility.

00:49:57.760 --> 00:50:06.100
You don't even need to have the highest-end GPU to get started with CUDA programming.

00:50:07.440 --> 00:50:19.160
And there are a lot of people who have, use cases that would benefit from GPU acceleration where an everyday commodity gaming GPU would be fine.

00:50:20.580 --> 00:50:21.880
Not going to be true for every application.

00:50:22.140 --> 00:50:32.540
And of course, if you scale up, what usually happens when people start with GPU acceleration is first they take their existing thing and then they add the GPU acceleration.

00:50:32.780 --> 00:50:33.840
Now it runs a lot faster.

00:50:33.880 --> 00:50:39.260
And then they start thinking, aha, now that it runs faster, I can increase my problem size.

00:50:39.480 --> 00:50:40.580
I no longer have these constraints.

00:50:41.290 --> 00:50:42.700
And then they end up needing a bigger GPU.

00:50:43.510 --> 00:50:51.400
But for that initial speed-up, you're usually fine to start prototyping and developing on your everyday modern-day GPU.

00:50:51.820 --> 00:50:52.700
Not usually going to

00:50:52.700 --> 00:50:54.840
be the thing that makes sense to take to production.

00:50:56.600 --> 00:51:12.440
And one of the biggest downsides to the GPU that's in your gaming box is that you're going to have greater latency between the CPU and the GPU, and you're not going to have as much memory compared to what you get in a server GPU.

00:51:13.880 --> 00:51:14.320
Okay.

00:51:15.090 --> 00:51:21.400
But still, it's a step on the staircase, maybe, of getting

00:51:21.400 --> 00:51:21.760
started.

00:51:22.580 --> 00:51:22.840
Yes.

00:51:23.440 --> 00:51:24.680
Prototyping and so on.

00:51:25.920 --> 00:51:26.100
Okay.

00:51:26.620 --> 00:51:28.260
We don't have too much time left, but maybe let's

00:51:28.260 --> 00:51:28.640
dive into

00:51:28.640 --> 00:51:33.000
each of these pieces here that make up CUDA Python.

00:51:33.540 --> 00:51:42.120
So I would first say, there's some that are listed here, Let me give people the overview of what parts the CUDA Python you can get started with.

00:51:42.240 --> 00:51:44.840
The first one isn't even listed here, and

00:51:44.840 --> 00:51:45.840
that is Kupy.

00:51:47.260 --> 00:51:50.940
So it's not listed here because it's not part of that CUDA Python meta package.

00:51:51.100 --> 00:51:59.580
So Kupy is a NumPy and SciPy-like library that is GPU-accelerated.

00:52:00.200 --> 00:52:04.980
So it's the interface that you know from NumPy and SciPy.

00:52:05.580 --> 00:52:09.300
When you invoke the operations, they run on your GPU.

00:52:10.760 --> 00:52:14.540
So this is by far where everybody should start.

00:52:15.760 --> 00:52:16.000
Okay.

00:52:16.450 --> 00:52:20.080
Could I get away with even saying import QPy as NP?

00:52:21.020 --> 00:52:21.900
Is it that compatible?

00:52:22.060 --> 00:52:22.700
You

00:52:22.700 --> 00:52:31.280
could get away with that, but there are certainly going to be cases where the semantics may not 100% line up.

00:52:31.420 --> 00:52:34.980
I think for the most part, you would be fine doing that.

00:52:36.220 --> 00:52:42.860
but there is no way to make a 100% fully compatible drop and replacement.

00:52:43.360 --> 00:52:52.780
And so it's important to read the docs and make sure you understand where there may be little subtle differences.

00:52:53.000 --> 00:52:54.780
But yeah, definitely think of it as drop and replacement.

00:52:55.040 --> 00:53:00.280
Yeah, the reason I was asking is maybe that's a super simple way to experiment, right?

00:53:00.440 --> 00:53:00.960
I've got something

00:53:00.960 --> 00:53:01.800
written in

00:53:01.800 --> 00:53:04.480
Pandas and NumPy and so on.

00:53:06.040 --> 00:53:08.320
could I just change the import statement and see what happens?

00:53:08.610 --> 00:53:10.620
You know, without completely rewriting it.

00:53:10.740 --> 00:53:11.520
That's kind of what's getting at.

00:53:12.460 --> 00:53:14.020
Yeah, you could definitely do that.

00:53:14.160 --> 00:53:18.020
I mean, I think the docs even say right there that, yeah, it's meant to be a drop and replacement.

00:53:18.400 --> 00:53:20.380
So yeah, that's definitely how you could get started.

00:53:21.720 --> 00:53:31.080
The one thing to keep in mind is that if you're running in a single thread on the CPU, the problem size that you're running with may not be large enough to see an impact.

00:53:31.300 --> 00:53:36.780
So you may have to think about running with a larger problem size than it makes sense with NumPy.

00:53:37.460 --> 00:53:40.260
It might even be slower, right, because of the overhead of GPU?

00:53:40.360 --> 00:53:40.880
That's right.

00:53:41.210 --> 00:53:44.420
There may be cases if you do a sum of

00:53:44.420 --> 00:53:45.260
three elements.

00:53:46.290 --> 00:53:53.840
Although I would hope and assume that that path we maybe don't dispatch to GPU acceleration, but I suspect we still do.

00:53:56.100 --> 00:53:56.540
So

00:53:56.540 --> 00:53:59.840
Kupi is sort of the foundational thing that I'd say everybody should get started with.

00:54:00.820 --> 00:54:02.120
Okay, let me ask you a follow-up question.

00:54:02.340 --> 00:54:03.880
So this is like NumPy.

00:54:04.200 --> 00:54:08.140
NumPy is the foundation mostly, starting to

00:54:08.140 --> 00:54:08.700
change

00:54:08.700 --> 00:54:09.040
a little bit.

00:54:09.060 --> 00:54:15.760
But for pandas, is there a way to kind of pandify my GPU programming?

00:54:16.760 --> 00:54:17.560
Yes, there is.

00:54:17.740 --> 00:54:23.100
We have LibQDF, which is a part of Rapids.

00:54:23.720 --> 00:54:29.440
And LibQDF, it's a data frame library, and it has a panda mode.

00:54:29.920 --> 00:54:36.220
I think the module is just like KUDIF.pandas that aims to be a drop and replacement.

00:54:36.980 --> 00:54:44.220
And then it also has its own data frame interface that's, I think, a little bit different than pandas in some ways.

00:54:44.590 --> 00:54:50.420
It allows it to be more efficient for GPU and parallel programming.

00:54:51.280 --> 00:54:58.220
And there's a whole bunch of other libraries that are a part of the Rapids frameworks for doing accelerated data science.

00:54:59.260 --> 00:54:59.480
Okay.

00:55:00.640 --> 00:55:05.440
Yeah, I'm going to probably be doing an episode on Rapids later as well.

00:55:05.460 --> 00:55:06.420
So diving more into that.

00:55:06.560 --> 00:55:09.620
But okay, it's good to know that that kind of is the parallel there.

00:55:10.000 --> 00:55:10.300
Yeah.

00:55:11.040 --> 00:55:16.180
And that was actually the next piece I was going to mention was going to be Rapids and Kudia.

00:55:16.820 --> 00:55:27.860
Now, the next two most frequent things that you might need is like a Pythonic interface to the CUDA runtime.

00:55:28.480 --> 00:55:35.140
So the CUDA runtime is the thing that you use to do that command and control, that orchestration of NVIDIA GPUs.

00:55:35.200 --> 00:55:58.960
Things like managing configurations and settings in the GPUs, loading programs, compiling and linking programs, doing things like launching work on the GPU, allocating memory, creating queues of work, creating dependencies, etc.

00:55:59.740 --> 00:56:09.000
CUDA.Core is a pithonic API to all of those things, and that's what almost everybody should be using for

00:56:09.000 --> 00:56:09.940
doing those

00:56:09.940 --> 00:56:10.920
sorts of management tasks.

00:56:12.140 --> 00:56:21.260
It pretty much one-to-one maps to things, to ideas, concepts from the CUDA C++ runtime.

00:56:23.380 --> 00:56:34.180
And then the final piece would be Numba CUDA, which is the thing that you would use to write your own CUDA kernels.

00:56:37.119 --> 00:56:42.340
And when you're writing those CUDA kernels, there are some libraries that can help you with that.

00:56:42.470 --> 00:56:49.020
And one of them is CUDA Cooperative, which is going to be the thing I'll probably talk about the most in my Python talk.

00:56:50.180 --> 00:56:57.320
And so CUDA Cooperative provides you with these algorithmic building blocks for writing CUDA kernels.

00:56:58.340 --> 00:57:25.060
and we also have a package called nvmath python which provides you with uh more cuda cooperative is generic algorithmic building blocks for things like loading and storing or doing a sum or a scan nvmath python provides you with building blocks for things like a matrix multiply or random numbers or a Fourier transform, et cetera.

00:57:26.820 --> 00:57:32.100
And NVMath Python also has host side APIs, so you can use it.

00:57:32.420 --> 00:57:45.760
For the most part, you can access those with Kupi's scipy packages, but you can also go directly through NVMath Python if you want to get to the slightly lower level APIs that give you a little bit more control.

00:57:47.060 --> 00:57:47.320
Interesting.

00:57:47.540 --> 00:57:53.120
I've heard the docs talk about host side operations versus not. What does that mean?

00:57:54.360 --> 00:58:25.200
Hostline operations means things that you call from the CPU so that like your Python program calls just as a regular Python program would and then it runs some work on the GPU and then like when it's done it reports back to the CPU and typically for a lot of cases like Kupi these operations are synchronous so like you call like KupiSum, it launches the work on the GPU.

00:58:26.260 --> 00:58:27.540
The work on the GPU finishes.

00:58:28.200 --> 00:58:32.360
And this whole time, the KupiSum has been waiting for that work by default.

00:58:33.460 --> 00:58:33.840
I see.

00:58:34.120 --> 00:58:36.500
So a real simple distributed programming model, right?

00:58:36.740 --> 00:58:37.060
It looks

00:58:37.060 --> 00:58:37.500
like you're

00:58:37.500 --> 00:58:40.940
just calling local functions, but it's kind of distributed computing.

00:58:41.820 --> 00:58:42.040
Yes.

00:58:43.760 --> 00:58:46.500
Distributed in the sense of it's distributed from the host to the device.

00:58:47.620 --> 00:58:53.080
Device-side operations are things that you're calling from within a CUDA kernel.

00:58:53.980 --> 00:59:07.320
And a CUDA kernel is a function that gets run by every thread on the GPU or every thread within the collection that you tell it to run on.

00:59:08.260 --> 00:59:10.720
For simplicity, let's just assume every thread on the GPU.

00:59:12.680 --> 00:59:17.100
And so those operations are what we call cooperative operations.

00:59:17.400 --> 00:59:34.560
in that a cooperative sum is a sum where every thread is expected to call the sum function at the same time, and they all cooperate together and communicate amongst themselves to compute the sum across all the threads.

00:59:35.120 --> 00:59:35.620
Okay.

00:59:36.760 --> 00:59:37.180
Yeah, very cool.

00:59:39.660 --> 00:59:41.700
Well, we're just about out of time.

00:59:42.000 --> 00:59:48.240
You mentioned it in passing, but you're going to have a talk coming up at PyCon.

00:59:49.160 --> 00:59:49.280
Yes.

00:59:49.480 --> 00:59:52.500
Which, if people are watching the YouTube live stream, starts in just a couple days.

00:59:53.060 --> 00:59:56.260
If they're listening to the podcast, maybe you can catch the video now.

00:59:56.440 --> 00:59:57.220
It's about time enough.

00:59:58.240 --> 01:00:00.360
But it's called GPU Programming in Pure Python.

01:00:01.060 --> 01:00:02.500
You want to give a quick shout-out to your talk?

01:00:03.500 --> 01:00:03.640
Yeah.

01:00:03.840 --> 01:00:13.840
So in this talk, we're going to look at how you write CUDA kernels or how you can write CUDA kernels in Python.

01:00:14.900 --> 01:00:27.800
without having to go to CUDA C++ and how you have access to all the tools that you have in CUDA C++ and you can get the same performance that you would have in CUDA C++.

01:00:29.099 --> 01:00:29.460
Okay.

01:00:29.850 --> 01:00:39.200
Yeah, it sounds like a good hands-on, not exactly hands-on, but at least concrete code version of a lot of the stuff we talked about here.

01:00:39.500 --> 01:00:39.680
Yeah.

01:00:40.600 --> 01:00:41.000
Yeah, great.

01:00:41.640 --> 01:00:44.180
And now people are excited.

01:00:44.460 --> 01:00:45.340
They're interested in this.

01:00:46.220 --> 01:00:46.680
What do you tell them?

01:00:46.740 --> 01:00:47.280
How do they get started?

01:00:47.440 --> 01:00:47.780
What do they do?

01:00:48.320 --> 01:00:53.720
I would say they should go to the Accelerated Computing Hub, which is another GitHub repo that we have.

01:00:54.320 --> 01:01:01.480
And on the Accelerated Computing Hub, we have open source learning materials and courses, self-guided courses.

01:01:01.740 --> 01:01:04.320
And one of them is a GPU Python tutorial.

01:01:04.740 --> 01:01:06.420
So you just go to Accelerated Computing Hub.

01:01:06.540 --> 01:01:06.920
It's on GitHub.

01:01:07.800 --> 01:01:09.400
You click on GPU Python tutorial.

01:01:10.280 --> 01:01:14.840
And it takes you to a page with a whole bunch of Jupyter Notebooks.

01:01:16.080 --> 01:01:18.820
And you start with the first one.

01:01:19.000 --> 01:01:20.200
It opens up in CoLab.

01:01:20.400 --> 01:01:24.020
It uses those CoLab GPU instances.

01:01:26.000 --> 01:01:28.440
And you can start learning there.

01:01:28.740 --> 01:01:32.720
And there's other resources available on Accelerated Computing Hub.

01:01:33.120 --> 01:01:35.780
And that is, we're always working on new stuff.

01:01:36.820 --> 01:01:40.400
So that is a good place to look.

01:01:40.540 --> 01:01:52.560
There's also, I think, a PyTorch tutorial there, and we have the accelerated Python user guide, which has some other useful learning material.

01:01:53.760 --> 01:01:53.980
Okay.

01:01:55.040 --> 01:01:55.300
Excellent.

01:01:56.180 --> 01:01:58.500
And, you know, thanks for being here, Bryce.

01:01:59.820 --> 01:02:00.340
Good luck with

01:02:00.340 --> 01:02:05.000
your talk, and thanks for giving us this look at GPU programming in Python.

01:02:05.880 --> 01:02:06.240
Thank you.

01:02:06.460 --> 01:02:07.180
It was a great being here.

01:02:08.380 --> 01:02:15.160
I should plug my podcast, ADSP, the podcast, Algorithms Plus Data Structures Equals Programming.

01:02:15.320 --> 01:02:16.640
We talk about parallel programming.

01:02:17.760 --> 01:02:28.300
We talk about those three algorithms that I mentioned, transform, reduce, and scan, and how you can use them to write the world's fastest GPU-accelerated code.

01:02:28.370 --> 01:02:32.460
And we talk a lot about array programming languages and all sorts of fun stuff.

01:02:32.530 --> 01:02:33.100
So check it out.

01:02:34.060 --> 01:02:34.260
Excellent.

01:02:34.420 --> 01:02:35.860
Yeah, I'll link to it in the show notes for people.

01:02:36.020 --> 01:02:36.160
Thanks.

01:02:37.060 --> 01:02:38.240
thanks for being here see you later

