WEBVTT

00:00:00.920 --> 00:00:03.860
Christopher, welcome to Talk Python. Welcome back to Talk Python.

00:00:04.140 --> 00:00:04.740
Thank you.

00:00:05.160 --> 00:00:06.720
Third time's the charm. We'll see how it goes.

00:00:08.320 --> 00:00:19.760
It's going to go well, I'm sure. We've got a really fun topic lined up. I think this is one of those kinds of shows where there's going to be something for everybody.

00:00:20.340 --> 00:00:36.080
There might be a lot for some people and there's going to be like, oh, that's the thing that made it worth listening because we're going to talk about a bunch of different extensions, tools, techniques, et cetera, for working with Polars, one of the exciting data frame libraries that we have these days.

00:00:37.290 --> 00:00:40.400
So super excited to have you here to tell us about that.

00:00:40.580 --> 00:00:45.480
And I'm really interested to hear how it works with Django, because that's what you talk about when you come here, right?

00:00:45.840 --> 00:00:46.140
I

00:00:46.140 --> 00:00:47.500
honestly--

00:00:47.580 --> 00:00:48.080
I'm just kidding.

00:00:48.190 --> 00:00:49.240
--I haven't done it.

00:00:49.620 --> 00:00:49.880
There's

00:00:49.880 --> 00:00:51.300
no reason it shouldn't.

00:00:51.900 --> 00:00:52.000
But

00:00:52.000 --> 00:00:52.840
I have yet

00:00:52.840 --> 00:00:54.840
to actually combine the two pieces together.

00:00:57.600 --> 00:00:59.280
So-- and pandas together.

00:00:59.580 --> 00:01:02.400
So surely Django and Polaris will go together.

00:01:02.600 --> 00:01:02.720
I

00:01:02.720 --> 00:01:03.300
know actually

00:01:03.300 --> 00:01:03.860
that they would.

00:01:04.239 --> 00:01:04.920
Shouldn't be a problem.

00:01:05.440 --> 00:01:15.600
I think it's one of the beauties of these kinds of libraries is at the heart, although there are Rust underneath or other things underneath to get you the performance, they're still just Python.

00:01:16.980 --> 00:01:19.500
So you've got access to the code and you can do what you want.

00:01:20.860 --> 00:01:26.280
Yeah, I think there's probably some combinations that people don't typically think of.

00:01:26.660 --> 00:01:37.780
Like, for example, one of the things that I like to do is I've got some dashboard-y thing, either for Talk Python in the course, I can't remember, but I want a bar graph.

00:01:38.880 --> 00:01:51.660
And so I do some matplotlib thing and have it generate a picture and just return that as a picture dynamically at a URL, you know, and just set the source to the image to be wherever that thing gets generated from.

00:01:52.220 --> 00:01:59.780
So there are really cool combinations, though I suspect we will not be doing much of our typical web stuff this show, will we?

00:02:01.300 --> 00:02:04.440
I'll drop mentions to my book because I have to.

00:02:04.620 --> 00:02:05.720
That's the way it's supposed to work.

00:02:05.880 --> 00:02:08.420
But otherwise, yeah, we'll stick to the data science world this time around.

00:02:09.140 --> 00:02:10.280
Yeah, well, let's start there.

00:02:11.080 --> 00:02:17.700
I know people probably have listened to you on the show or on the Real Python podcast or in other places that you're doing all the things.

00:02:18.740 --> 00:02:21.140
But, you know, tell us about yourself.

00:02:21.680 --> 00:02:23.880
Feel free to mention your book so people know about them as well.

00:02:24.140 --> 00:02:24.620
There you go.

00:02:25.380 --> 00:02:26.420
Yeah, there's an old joke.

00:02:26.480 --> 00:02:28.000
How do you know somebody runs a marathon?

00:02:28.360 --> 00:02:29.420
Well, they'll effing tell you.

00:02:29.560 --> 00:02:31.740
Well, authors are more or less in the same package.

00:02:33.080 --> 00:02:34.560
It's a contractual obligation.

00:02:35.180 --> 00:02:35.580
It is.

00:02:35.660 --> 00:02:36.680
I wrote it about in my book, Chris.

00:02:37.140 --> 00:02:37.580
There you go.

00:02:37.900 --> 00:02:38.000
Exactly.

00:02:38.520 --> 00:02:38.880
That's right.

00:02:39.220 --> 00:02:43.200
And I guess while I've got you on air, thank you for writing the foreword.

00:02:43.400 --> 00:02:49.960
So, yeah, I guess now I should sell myself as an author.

00:02:51.140 --> 00:02:54.080
It's nowhere near as impressive as people make it think it's sound to be.

00:02:54.680 --> 00:03:03.560
I guess I spend my spare time doing sort of Python education-y stuff with you and other organizations, and writing is kind of part of that.

00:03:04.040 --> 00:03:07.860
My day job is still helping organizations do technical stuff.

00:03:08.480 --> 00:03:14.800
Sometimes that's architectural advice, and sometimes that's things like agile and lean and process-driven things.

00:03:15.960 --> 00:03:19.120
My marketing people tell me to sell myself as a fractional CTO.

00:03:19.740 --> 00:03:24.000
I have a hard time saying that with a straight face, but I've got to put it on the brochure.

00:03:24.070 --> 00:03:24.640
So there you go.

00:03:25.800 --> 00:03:29.620
Well, if you don't want to take it so seriously, you could be an improper fraction.

00:03:30.120 --> 00:03:30.580
There you go.

00:03:31.060 --> 00:03:31.880
You could be like

00:03:31.880 --> 00:03:33.180
a two-eighths CTO.

00:03:34.280 --> 00:03:36.040
I'm an irrational CTO.

00:03:36.650 --> 00:03:37.280
That actually makes

00:03:37.280 --> 00:03:37.580
sense.

00:03:39.660 --> 00:03:41.040
I'm radical two over seven.

00:03:41.280 --> 00:03:41.400
Let's

00:03:41.400 --> 00:03:41.500
go.

00:03:41.640 --> 00:03:42.020
That's how much

00:03:42.020 --> 00:03:42.700
a CTO I am.

00:03:44.680 --> 00:03:48.440
More seriously, though, I'm sure there are people listening who find that idea pretty interesting.

00:03:48.700 --> 00:03:58.180
fractional CTO or sort of fractional something beyond just like low-level coder. Tell people what that, what is that like? How'd you get into that?

00:03:58.780 --> 00:04:55.020
I got into it because I kind of fell into software management early on in my career. There was a company I was at where I was, it was a Windows-based shop and I was hired as the Unix guy. We all had this naive idea that we could port the product to Unix easily. So I was their sole Unix architect person. After about six months worth of work, we figured out this just wasn't going to happen. And my VP got pregnant and went, want to be a manager? And basically handed me the keys. So I kind of learned by being thrown in the deep end as to sort of how to do the technical management type stuff. So my career has always kind of been a bit of a balance between that. I like smaller organizations and I like grow. So what that tends to mean is when it's really early on, when you're number two or three, you're coding like a line developer. And as the organization grows, you spend less time doing that.

00:04:55.320 --> 00:04:57.740
And because I've spent a lot of time in startups, I've done that journey

00:04:57.740 --> 00:04:58.760
many times.

00:04:59.240 --> 00:05:04.400
So the fractional CTO idea essentially is to try to help smaller organizations that

00:05:04.400 --> 00:05:05.120
wouldn't necessarily

00:05:05.120 --> 00:05:18.040
be able to afford a full-time CTO with that kind of thing and sort of provide the kind of advice from things I've learned throughout that process over the years and then sort of help out as you go along.

00:05:18.960 --> 00:05:25.060
And sometimes that's come in with a very specific target of help us architect something.

00:05:25.190 --> 00:05:39.860
And sometimes that's more of a, we need some, it's sometimes referred to as adult supervision, where somebody comes in and just sort of offers sort of the process advice, right?

00:05:40.340 --> 00:05:40.480
Right.

00:05:40.920 --> 00:05:45.380
Who's going to be this adult in the room that says, no, we're not rewriting that in a new JavaScript framework.

00:05:45.670 --> 00:05:46.800
It was a third time this year.

00:05:46.950 --> 00:05:47.060
Yeah.

00:05:47.590 --> 00:05:55.820
And often it's very hard in a startup to concentrate on things like CICD and testing and all the rest of it because you're trying to get to market.

00:05:55.970 --> 00:05:57.140
So it's all about those features.

00:05:58.060 --> 00:06:00.960
Well, if you don't balance that, it's going to bite you later on.

00:06:01.110 --> 00:06:01.240
Right.

00:06:01.480 --> 00:06:10.040
So I try to help with sort of that overall bigger picture as part of the organizations and get them going.

00:06:10.600 --> 00:06:31.140
Cool. Yeah, I find even outside of that kind of stuff, sometimes it's hard to focus on the boring but important features that matter rather than like, hey, wouldn't it be cool if we refactored this or made that faster? Like, it doesn't need to be faster yet until you have more customers. What you need is this other thing, this PDF export that nobody wants to write.

00:06:31.360 --> 00:06:46.920
Yeah. And as developers, you know, like the entertaining part of our job is learning the new thing. And oftentimes that's learning the new library. And sometimes that's great. And sometimes that's wasting a whole bunch of time because the old library would have been just as good and keep sort of plowing along. Right.

00:06:48.060 --> 00:06:51.500
There's a phrase that I stole in my book.

00:06:51.570 --> 00:06:52.900
See, I keep bringing it back to the book.

00:06:54.420 --> 00:06:58.200
People talk about Django being old, and it's like the difference between, you know, it's a dinosaur.

00:06:58.460 --> 00:06:59.640
No, no, it's a shark.

00:07:00.000 --> 00:07:00.820
They're still around.

00:07:01.260 --> 00:07:02.060
They're still valuable.

00:07:02.560 --> 00:07:07.940
They're just as old as the dinosaurs, but, you know, they're still part of the ecosystem.

00:07:09.460 --> 00:07:09.900
And

00:07:09.900 --> 00:07:19.800
hopefully, you know, the gray hair at the temples here helps me provide a little more insight into that than, say, somebody fresh out of school.

00:07:21.560 --> 00:07:22.680
See those gray

00:07:22.680 --> 00:07:23.000
hairs?

00:07:23.200 --> 00:07:24.780
That's C++. Exactly.

00:07:28.700 --> 00:07:48.920
Yeah, indeed. Okay. Well, let's talk data science and let's talk pollers. So I guess there's a lot of people who have heard of pollers and are experts. I see out in the live audience already. There are some folks whose things they've created for pollers we're going to talk about in this show, actually. So

00:07:48.920 --> 00:07:50.100
some

00:07:50.100 --> 00:08:04.480
people need no introduction to pollers, but there are many people listening to the show who are just getting into Python, using this as like one of the footholds to kind of get a feel for the space. And they might go like, what is pollers? Sure. You know,

00:08:05.039 --> 00:08:05.400
what's

00:08:05.400 --> 00:08:07.320
a data frame? Like, tell us about this. Yeah,

00:08:07.330 --> 00:08:10.860
I was about to say, before we get to the So maybe we should just start with a data frame.

00:08:10.970 --> 00:08:14.420
So a data frame is kind of an in-memory spreadsheet sort of thing.

00:08:14.600 --> 00:08:16.800
It's a data structure that consists of rows and columns.

00:08:17.680 --> 00:08:22.440
And you do the same kind of things that you might do in Excel, but in your code.

00:08:23.240 --> 00:08:26.560
There are a whole bunch of advantages to doing it in your code over doing it in Excel.

00:08:26.770 --> 00:08:29.620
And in fact, the course that we're going to talk about gets into some of that.

00:08:30.200 --> 00:08:33.099
But there are a bunch of different libraries out there that help you do this.

00:08:33.800 --> 00:08:39.520
The big one that has been around for probably the longest and is the most popular is Pandas.

00:08:41.519 --> 00:08:44.480
Pandas has made some interesting design decisions.

00:08:45.360 --> 00:08:53.520
And as a result, there are some other more recent additions to the space that are trying to address some of those things that Pandas has done.

00:08:54.200 --> 00:08:56.380
And Polars has become one of the more popular ones.

00:08:57.180 --> 00:09:03.820
I saw just as we kicked in here, there was a comment already about, I like the coding style of Polar better than Pandas.

00:09:04.070 --> 00:09:07.240
And this to me, this is also why I actually prefer Polars.

00:09:07.780 --> 00:09:11.860
The interface to it is a lot more like object-oriented code.

00:09:13.300 --> 00:09:16.240
And it feels more Pythonic to me.

00:09:17.040 --> 00:09:25.000
And as a result, it tends to be one of the libraries that I now head towards if I need to muck with a bunch of data and do those kind of data-framing things.

00:09:26.160 --> 00:09:32.980
It is almost every benchmark out there has marked it as being significantly faster than pandas.

00:09:33.020 --> 00:09:34.420
So that's a nice little advantage.

00:09:35.820 --> 00:09:43.140
And it also includes methods to convert your data into pandas and other data frames as well.

00:09:43.540 --> 00:09:55.500
So if you do get stuck and there's something that isn't there and because it's a newer library, you can always just go, oh, give me the pandas version of this, run my pandas commands, and then bring it back into the polar space where you make it a little more comfortable.

00:09:56.040 --> 00:09:56.100
So

00:09:56.100 --> 00:09:56.880
it's a nice...

00:09:56.880 --> 00:09:58.600
Two pandas and a from pandas sort of

00:09:58.600 --> 00:09:58.860
thing.

00:09:59.240 --> 00:10:03.580
Yeah, and so it's a nice flexible sort of approach, and it's a quick bear.

00:10:03.940 --> 00:10:09.600
And one of the things I find, I'm not trying to dump on pandas.

00:10:09.720 --> 00:10:12.260
It's going to sound like that occasionally, but I really do.

00:10:12.260 --> 00:10:12.940
I like the library.

00:10:12.940 --> 00:10:14.580
It's very, very powerful, very, very useful.

00:10:15.560 --> 00:10:16.920
It uses a lot of black magic.

00:10:17.800 --> 00:11:26.580
And I find as a developer, as somebody with a software engineering background, sometimes code can be distracting at it and go, "How does that even compile? What is that? What are they doing?" And that pulls me away from the, "I'm trying to do something." And I find some of the stuff in Pandas kind of falls into that bucket. Whereas you just do nice little chained function calls in pollers. And so you're reading filter, select kinds of things. And it feels a lot cleaner to me. So it tends to be where I want to go. And then the other big advantage of it, which some of the other data frame libraries have, but Pandas, as far as I know, doesn't, is lazy evaluation. And this is the idea of rather than executing each operation as you call it, you can chain a bunch of things together and then call them all at the same time. This tends to give you large amounts of speed up. So for example, if you're reading in a CSV file and you want to do something only on certain rows in that CSV file, the filter can throw those rows that aren't important, and then you would only run the operation on those rows that you were interested in.

00:11:26.700 --> 00:11:31.660
Of course, that tends to be an awful lot faster than going through every single row at a time.

00:11:31.920 --> 00:11:37.940
It also tends to use less memory for the same reason because you don't have to keep as much in memory at any given time.

00:11:39.260 --> 00:11:45.580
So lazy evaluation can be a really, really powerful tool, particularly if you're using with really, really large data frames.

00:11:46.420 --> 00:11:56.800
So that to me is, you know, the coding style was a nice selling point, but being able to see that performance difference with the lazy evaluation was really what hit it home for me.

00:11:57.399 --> 00:12:10.620
Yeah. They're both super cool. You know, I talked to Richie Bink, who is a creator of Polars, about three years ago or so on the podcast. And it was very interesting to talk about some of the philosophies.

00:12:11.460 --> 00:12:11.580
So

00:12:11.580 --> 00:12:21.620
for example, the lazy evaluation is great because you could theoretically change the order like if i'm going to do some complicated math computation on a

00:12:21.620 --> 00:12:22.300
thing we'll

00:12:22.440 --> 00:12:44.280
talk about some of those and then i'm going to filter it down based on some unrelated criteria like state or whatever gender who knows and then get the answer if i did that in pandas it would do the math on every single thing and then filter it down and give you the answer and maybe take the US as an example, that might be 1/50 of the data that you wouldn't have to multiply.

00:12:46.280 --> 00:12:52.160
So with pullers, it can even optimize-- well, if there's a filter, do the filter first, and then the calculation.

00:12:53.540 --> 00:12:56.200
So it's got kind of an optimizer built in there as well.

00:12:56.200 --> 00:12:56.240
Yeah.

00:12:58.300 --> 00:13:01.140
This technology is very similar to what happens in databases.

00:13:01.660 --> 00:13:09.820
So if you're doing more complex SQL, behind the scenes, the database for you goes and tries to optimize that to try to figure out what the best way of doing this for you.

00:13:10.600 --> 00:13:12.240
And Polars has that built in as well.

00:13:12.800 --> 00:13:16.680
It's got it built in on your calls specifically.

00:13:16.990 --> 00:13:19.940
So you can take advantage of that even if it isn't the lazy evaluation.

00:13:20.920 --> 00:13:23.460
But it can really, really shine in the case of lazy evaluation.

00:13:23.750 --> 00:13:23.900
So

00:13:23.900 --> 00:13:24.380
if you're doing a

00:13:24.380 --> 00:13:33.440
complex query all in one chunk, like a group by with a bunch of different aggregates, it does still do the same thing, which again gives you one of the ways that it'll speed up.

00:13:34.520 --> 00:13:43.400
But particularly with lazy evaluation, where you might chain together 15 or 20 calls, it'll figure out for you what the best order on that is, and that can make a big difference.

00:13:44.220 --> 00:13:50.580
You can also see what it wants to do and what its execution plan is.

00:13:51.180 --> 00:13:53.340
I don't understand the output.

00:13:55.580 --> 00:13:56.300
It's deep

00:13:56.300 --> 00:13:57.240
relational algebra.

00:13:58.680 --> 00:14:06.200
But for those who are into this stuff, you can actually see exactly what it's doing, and it gives you a little peek behind the curtain.

00:14:07.720 --> 00:14:13.000
Yeah, with databases, there's usually some way to ask for what's called an explain.

00:14:13.490 --> 00:14:20.320
Like you've got some query, and instead of saying run the query, parse it and do the optimizer, and then tell me what you would do.

00:14:20.330 --> 00:14:23.520
And that can be really helpful for saying like finding an index or something.

00:14:24.280 --> 00:14:24.880
Yeah, and in fact— Is it

00:14:24.880 --> 00:14:25.400
actually using

00:14:25.400 --> 00:14:26.540
this index or is it not?

00:14:26.610 --> 00:14:28.560
But then there's something like that in pollers as well, yeah?

00:14:28.740 --> 00:14:30.880
Yeah, and it calls it explain as well.

00:14:31.120 --> 00:14:35.100
So when you've got the lazy object, you can call explain on it, and it'll show it.

00:14:35.480 --> 00:14:43.800
It also has a graphical equivalent, so it'll spit out a little image, the sort of flowcharty kind of thing that shows you what it is as well.

00:14:44.230 --> 00:14:51.160
So if you prefer the pictures over the obscure Greek letter references, then that can show you what's going on.

00:14:52.060 --> 00:14:53.280
Yeah, very nice, very nice.

00:14:54.420 --> 00:14:54.620
All right.

00:14:55.140 --> 00:15:06.160
Well, we are going to not talk about the exact subject of your course that you wrote, but I think we should maybe give it a quick shout out.

00:15:06.500 --> 00:15:12.820
So your course that you wrote at Talk Python is Polars for the Power Users, Transform Your Data Analysis Game.

00:15:14.460 --> 00:15:19.840
What we're going to talk about is a bunch of different cool tools that extend and power up pollers.

00:15:20.340 --> 00:15:24.080
But if people are interested in learning pollers more from scratch, they could take your course.

00:15:24.220 --> 00:15:25.780
So I'll be sure to link to that in the show notes.

00:15:25.920 --> 00:15:32.220
Maybe give people the super quick rundown on what's on the course, and then we'll talk about all the items.

00:15:32.660 --> 00:15:32.920
Sure.

00:15:33.420 --> 00:15:36.600
As you might guess from the title, it kind of teaches your pollers.

00:15:37.380 --> 00:15:40.760
And you can see on the screen there, there's a bunch of different icons as well.

00:15:42.200 --> 00:15:47.100
What we're trying to do is essentially take you to that next step.

00:15:47.260 --> 00:15:58.700
So if you're doing data science-y stuff in Excel and you're trying to figure out how to move to the Python world with this, then this is kind of a course to try and help you do that.

00:15:58.770 --> 00:16:00.420
And it does it through the pollers library.

00:16:01.180 --> 00:16:03.560
So there's a whole bunch of different examples as you go along.

00:16:04.140 --> 00:16:07.120
A lot of the examples will start with, hey, here's a spreadsheet.

00:16:07.420 --> 00:16:08.980
Here's a thing that you're doing in the spreadsheet.

00:16:09.340 --> 00:16:14.720
Here is how you would do that with the Polars library and Python code instead.

00:16:15.460 --> 00:16:18.400
And why you might do that instead of doing a spreadsheet.

00:16:19.380 --> 00:16:23.020
Polars has this concept of expressions and filters.

00:16:23.620 --> 00:16:29.020
And they're the ways of performing operations on your data and then looking at different columns or different rows.

00:16:29.680 --> 00:16:35.940
So the course starts out there and then starts getting even deeper into real world examples.

00:16:37.040 --> 00:16:40.360
The final lesson is actually a case study.

00:16:41.030 --> 00:16:50.960
We took a bunch of the GDP data from the United Nations and showed you how to clean some of it, how to combine it.

00:16:51.720 --> 00:16:58.940
Whenever you've got data in the real world, even if it's from the same source, you're going to go, oh, I want this information from this column and that information from that column.

00:16:59.140 --> 00:17:00.560
but they aren't related.

00:17:00.900 --> 00:17:02.300
So now we have to cross-reference.

00:17:02.760 --> 00:17:04.900
So it covers how to sort of do all of that kind of stuff.

00:17:05.079 --> 00:17:22.360
So it's sort of this journey of how do you take some of the more basic CSV and Excel type stuff and change it into Python all the way along the line to how do we actually create a report on data from the real world using Polars as your library for doing it.

00:17:22.880 --> 00:17:23.280
Excellent.

00:17:24.100 --> 00:17:25.100
Yeah, it's also very funny.

00:17:25.360 --> 00:17:26.420
A very funny course.

00:17:26.500 --> 00:17:27.020
I enjoyed it.

00:17:27.180 --> 00:17:27.300
Okay.

00:17:29.040 --> 00:17:31.080
pollers is pretty awesome wouldn't you say yes

00:17:31.080 --> 00:17:36.280
that's a nice you know there you go how's that for a segue pretty

00:17:36.280 --> 00:18:11.940
awesome in fact let's reverse that we're gonna talk about awesome pollers so there are so many of these awesome lists out there and you know if you are interested in something and you haven't looked yet for an awesome list for it go do that there's got to be an awesome django i know there's there's got to be an awesome flask and there's an awesome async there's an awesome Python and Ddata has gone and created awesome pollers. So what we're going to do is we're going to go to this curated list and pull out 10-ish number of things that we think.

00:18:12.020 --> 00:18:12.200
On the

00:18:12.200 --> 00:18:14.080
order of 10. Yes, we didn't count.

00:18:15.060 --> 00:18:15.460
One

00:18:15.460 --> 00:18:16.760
of these days we'll start prepared.

00:18:17.480 --> 00:18:53.640
Well, my tabs scroll off the screen so I can't even count them. So there we go. But now it's something on that scale. We're going to go through a bunch of different little things that are great. And for me, I just, like I said at the opening of the show, I really like these because it's like, oh, it's not a big deal, but it just made all of my work way easier and it's really easy to adopt. A lot of these things are actually not huge commitments. It's not like redo your data stack in some other data frame library because you'll get some benefit. Like, no, you just plug this thing into your puller's work you're already doing and it's really good so with that let's start out on

00:18:53.640 --> 00:19:21.600
visualization what do you think sure yeah and this is this one's almost independent of polars i i guess there's there's sort of this whole category of things that are already in the data science space and what is specific to polars is the good news which is polars works with all those tools that you're used to working with. So if you're already playing with matplotlib or seaborn or any of those, Polars will integrate with it just like Pandas does and most of the others.

00:19:22.760 --> 00:20:11.060
Polars does have sort of specific integrations with Altair and HVplot. So there's built-in calls on the data frame that essentially say go plot this and they will use those tools. But even if you don't want to use those, getting at a column, slicing a column out and then handing that column down the matplotlib works more or less just like pandas does so this integration works quite nicely likewise with tools like jupiter or marimo again this isn't really polar specific but there's nothing in polars that stops you from using these tools and so you know there you're not you know if you're moving from pandas this isn't this isn't a stopper all the other things you don't have to learn everything else from scratch everything you bring with you will continue to work without any problem.

00:20:11.840 --> 00:20:20.600
Yeah. So on the Polars docs, we've got examples for HV Plot, Matplotlib, Plot9, Seaborn, Plotly, all these things. And yeah, they look great.

00:20:21.120 --> 00:20:22.140
Have you played with Altair much?

00:20:23.180 --> 00:20:28.600
You know, I've not played with Altair much. I think I've done mostly Plotly and Seaborn lately.

00:20:28.980 --> 00:20:48.560
Okay. Yeah. I still, I have a love-hate relationship with Matplotlib. I'm usually too lazy to learn anything else and I grumble every time I use it because I can't remember how to set X underscore or set underscore X and it drives me crazy, but it is, gives you results and that's what's important.

00:20:49.440 --> 00:21:00.020
For sure. So you mentioned it, Marimo. I recently covered, talked with Akshay from the Marimo team.

00:21:01.260 --> 00:21:05.800
I'm impressed. I am super impressed with the way that Marimo came out, the way it looks.

00:21:06.280 --> 00:21:12.780
It seems like a really nice, just a very nice modern Jupyter style.

00:21:13.380 --> 00:21:14.260
What do you think of it?

00:21:14.980 --> 00:21:16.500
So I've yet to play with it.

00:21:16.600 --> 00:21:17.720
I've done a bunch of reading on it.

00:21:18.500 --> 00:21:21.500
The theory behind it is something that is beautiful to me.

00:21:21.680 --> 00:21:23.460
I actually don't use Jupyter.

00:21:23.840 --> 00:21:29.440
And the big thing, this big stopping thing for Jupyter for me is it doesn't feel like it scales.

00:21:30.200 --> 00:21:32.140
And that's because the cells are stored as JSON.

00:21:32.200 --> 00:21:37.200
So it's a great tool like a REPL where if you're experimenting with something, it's fantastic.

00:21:37.780 --> 00:21:43.460
But as soon as you're trying to start doing that on a team, it can be problematic because now you're trying to merge those JSON pieces.

00:21:43.780 --> 00:21:44.760
It doesn't work very well.

00:21:45.840 --> 00:21:49.420
And Marimo has essentially tried to solve this from the ground up.

00:21:49.960 --> 00:22:01.560
Instead of creating JSON to store everything, it's actually creating Python output, which means the documents that you're creating in Marimo actually can be managed just like any other code.

00:22:02.780 --> 00:22:13.380
and although I haven't played with it myself, the theory behind that to me is very, very beautiful and the next time I dive into this space, this is definitely going to be the one I'm going to be playing with.

00:22:13.860 --> 00:22:16.160
Yeah, it looks great to me too.

00:22:16.940 --> 00:22:19.660
I think even just from a visual, like the way the

00:22:19.660 --> 00:22:21.100
UI works, I really,

00:22:21.260 --> 00:22:21.760
really like it.

00:22:22.280 --> 00:22:33.340
The other thing in addition to the Python backing instead of the JSON backing end, as in file storage, is the reactive bits.

00:22:33.810 --> 00:22:40.340
Like one of the things that always scares me about notebooks is like it can go one, five, two, seven.

00:22:40.680 --> 00:22:40.880
Yeah.

00:22:41.660 --> 00:22:45.040
You don't even know what happened into the like number six line, right?

00:22:45.440 --> 00:22:47.760
But it affects the way the notebook runs.

00:22:48.040 --> 00:22:51.120
Like it's kind of like go-to statements, but you get to pick randomly.

00:22:52.360 --> 00:22:54.840
And this has a way to say these cells depend in this order.

00:22:54.990 --> 00:22:59.560
And so there's a automatic refresh and that's that.

00:22:59.680 --> 00:23:03.120
Anyway, a bit of a diversion, but I think it's worth paying attention to.

00:23:06.620 --> 00:23:06.760
Okay.

00:23:08.270 --> 00:23:08.920
Where were we again?

00:23:10.580 --> 00:23:20.880
Well, next thing we want to talk about is data framely, declarative, Polars native data frame validation library.

00:23:21.860 --> 00:23:27.680
So the data science, the first 80% of your work is cleaning the data and getting it into the right shape.

00:23:28.500 --> 00:23:31.260
And then the second 80% of your work is coding.

00:23:32.000 --> 00:23:34.880
And then there's that last 80% of the work, which is output.

00:23:35.420 --> 00:23:37.300
I think my math might be off

00:23:37.300 --> 00:23:37.560
there.

00:23:38.460 --> 00:23:39.540
It feels like it's right, though.

00:23:39.780 --> 00:23:43.740
It feels that the reality is true, though.

00:23:44.700 --> 00:23:48.320
And what DataFramely helps you do is that first 80%.

00:23:48.980 --> 00:23:52.580
So this is from a group of folks at a company called Quantco.

00:23:53.420 --> 00:23:55.860
And they're a data science consulting firm.

00:23:56.560 --> 00:24:00.640
And it works a bit like pedantic or Django RRM.

00:24:00.860 --> 00:24:02.060
See there, I brought it back.

00:24:02.460 --> 00:24:03.100
There's a book on that.

00:24:04.080 --> 00:24:09.860
So you can create a data class like object here, which inherits from data framely's schema class.

00:24:10.330 --> 00:24:14.760
And inside of that, you declare what your expected columns in a data frame are.

00:24:15.360 --> 00:24:26.320
So for example, you could specify a weight, something that is a weight like a heavier light is a float value and an American zip code is a string.

00:24:27.200 --> 00:24:42.840
And in addition to that declaration syntax, you can also write rule methods on the class to give you more complex validation code, like this plus this have to be, you know, these two values have to work together or these two values have to be unique or those kinds of validation pieces.

00:24:43.700 --> 00:24:49.140
Now, once you've got one of these schema objects, you then call its validation method, passing in your data frame.

00:24:49.920 --> 00:24:52.200
And if the results are clean, you get back your data frame.

00:24:52.490 --> 00:24:59.440
And if they aren't, you get an exception that describes each validation, telling you what rule failed, what rows it failed upon.

00:25:00.300 --> 00:25:10.300
And there's also a soft validation mechanism called filter, which returns a data frame with the rows that passed and a failure info object with the rows that went wrong.

00:25:10.840 --> 00:25:19.360
So if you don't want to just sort of stop because it's too bad, you want to react to the parts that aren't, you can dig in and sort of go from there.

00:25:20.100 --> 00:25:24.160
So this is a nice little piece and it can definitely help you.

00:25:24.880 --> 00:25:31.280
It doesn't fix the dirty data, but helps you detect it, which of course is your sort of first step as you go along.

00:25:33.760 --> 00:25:43.780
And one of the real challenges you can run into is the data, it parses or something, right?

00:25:44.200 --> 00:25:44.360
Yes.

00:25:44.440 --> 00:25:45.040
And so then

00:25:45.040 --> 00:25:46.020
math works.

00:25:47.260 --> 00:25:50.700
But at the same time, that's not necessarily giving the right answers.

00:25:50.870 --> 00:25:54.740
And you've got to figure out, well, why is this not working?

00:25:55.280 --> 00:25:55.420
Yeah.

00:25:55.660 --> 00:25:56.420
In fact, in the course,

00:25:56.680 --> 00:26:09.260
one of the examples we use, which is what you've got up on the screen here as well, is oftentimes people will put an American zip code, which for our non-American friends is a five-digit number or could be a nine one, which that just makes it messy.

00:26:09.960 --> 00:26:11.680
But let's stick with the five digits for now.

00:26:12.300 --> 00:26:13.480
Well, it could have a leading zero.

00:26:13.720 --> 00:26:19.960
Well, if you've stored that in, say, Excel as a number, then that leading zero gets chopped.

00:26:20.220 --> 00:26:23.500
And now all of a sudden you've got this four digit thing, which isn't developed zip code.

00:26:24.000 --> 00:26:29.260
And so as part of your data cleaning, you have to detect that or as part of loading the file.

00:26:29.460 --> 00:26:32.500
And you might have to say, hey, wait a second, this shouldn't be an integer.

00:26:32.800 --> 00:26:33.880
Please load this as a string.

00:26:34.679 --> 00:26:39.120
And so what these validation mechanisms allow for is to help you sort of catch this.

00:26:40.000 --> 00:26:55.920
And so, for example, if you were processing that failed info object and the only things that failed were zip codes, well, then you could have some code in there that went, okay, I'll just make sure that I fix the padding on this and maybe deal with it automatically as part of your data pipeline.

00:26:57.740 --> 00:26:59.900
Yeah, you can do something like this to catch it potentially.

00:27:02.140 --> 00:27:09.040
It's weird how the, it's weird the shadow that Excel casts, especially onto the data science world.

00:27:09.700 --> 00:27:14.080
you know i'm thinking of how or even science in general there was some gene as in

00:27:14.080 --> 00:27:17.340
i was just mentioned the same example i think it

00:27:17.340 --> 00:27:28.060
was like mar223 or something like that but if you if you put that to excel it's like oh march the second you're like no no no no yeah yeah yeah

00:27:28.060 --> 00:28:29.540
and in fact you know even polars to a certain extent has some of those kinds of problems um the routine that you use to read in a CSV, because CSV files don't actually have data type information inside of them, it essentially makes a guess. And it uses, I think by default, it's like the first thousand rows of the file to make an educated guess as to what the data types are. And what can happen is, let's say your data is consistent in the first thousand rows, and then row 1001 has something else, well, then it's going to die. Because it'll say, well, this is an integer, and now you're giving me a float and because it didn't detect that this was supposed to be a float in the first place now the difference between uh polars and excel is excel will just keep chugging along and you may not discover that you've caused yourself this problem whereas polars actually expects those columns to be a data type so it'll scream and part of your data cleaning process would be to catch those kinds of things and fix that

00:28:29.540 --> 00:28:44.880
yeah i mean spreadsheets are awesome they're they're really cool but they get overused like i was doing something where i had a bunch of dates in a column and i'd selected all of them just to copy them but you know at the bottom it says here's the sum of this or whatever

00:28:44.880 --> 00:28:45.660
yes it

00:28:45.660 --> 00:28:53.880
told me like the sum of these 20 dates was like march 12th of some i mean nope don't think it is but you know okay yeah if

00:28:53.880 --> 00:29:01.380
you well the beauty of the spreadsheets is they're very very flexible and the pain of a spreadsheet is they're very very flexible

00:29:01.380 --> 00:29:03.060
and so you

00:29:03.060 --> 00:29:07.960
know it's you could end up missing some toes depending on where you're pointing the gun.

00:29:09.000 --> 00:29:15.120
So one of the challenges, for example, with your zip code example, is maybe the first 1,000 rows are West Coast people.

00:29:15.480 --> 00:29:18.620
So every number looks like a five-digit proper number.

00:29:19.420 --> 00:29:26.780
But after that, you get to East Coast people, which start with zeros because it goes old to new, I guess, is the way they came up with it.

00:29:28.620 --> 00:29:29.600
And that could be a challenge, right?

00:29:29.640 --> 00:29:31.120
Even just simple stuff like parsing that.

00:29:31.640 --> 00:29:31.820
Okay,

00:29:31.980 --> 00:29:33.220
what is up next?

00:29:33.660 --> 00:29:33.700
uh

00:29:33.700 --> 00:29:34.440
so we go out

00:29:34.440 --> 00:29:34.500
to

00:29:34.500 --> 00:29:38.200
the patio and have this conversation yeah so essentially just

00:29:38.200 --> 00:30:22.500
as a quick aside there are a couple different libraries that essentially do the same thing as what we just talked about uh patio potato i'm not sure how you're supposed to say this uh which is uh wasn't actually on the awesome upholders list but it's one i've come across before this is by jacob gerard martinson and it's built on top of pidantic like with data friendly you declare a class inheriting from a model, then validate against it. The twist on this one that I kind of liked is it also has a mechanism for generating data. So if you're writing unit tests, once you've got this validator, you can go through and say, oh, give me some data to unit test with, and it will give you data that complies with your model. So I thought that was kind of an interesting little twist for helping you build your pipelines.

00:30:23.220 --> 00:30:38.680
Yeah, it's super neat. If you're a fan of Pydantic, this looks really, really cool. You get basically the type of errors you would get parsing JSON with a Pydantic model. It's the exact same error messages you get, but for data frame validation.

00:30:39.020 --> 00:30:39.900
So I think that that's pretty

00:30:39.900 --> 00:30:40.480
cool.

00:30:40.480 --> 00:30:41.600
Yeah, so it's nice and familiar.

00:30:42.420 --> 00:30:46.640
Yeah, definitely. If you're a FastAPI person or Pydantic for whatever reason.

00:30:47.059 --> 00:31:03.720
Also, Patito, not Patio, I suspect it probably is. I want to start a trend. I want to try to put something out there in the world for people If you've got something that could have multiple pronunciations, put just a little MP3 or something

00:31:03.720 --> 00:31:07.260
in your repo that sounds like this.

00:31:07.460 --> 00:31:08.100
You know

00:31:08.100 --> 00:31:08.320
what?

00:31:08.460 --> 00:31:14.140
I'm not sure that that's helpful because you remember the whole how do you pronounce Linux debate back in the 90s?

00:31:15.340 --> 00:31:22.200
One of the, I think it was Slackware distribution I had, came with an MP3 of Torvalds pronouncing it.

00:31:22.660 --> 00:31:25.020
And of course, with his accent, it didn't solve the problem.

00:31:25.320 --> 00:31:29.040
So I'm not sure that MP3 is actually the answer.

00:31:29.700 --> 00:31:32.940
Maybe just a little hyphenated where you want to put the stress of it.

00:31:33.780 --> 00:31:38.040
Yeah, I think I saw a GIF, or was it a GIF about it?

00:31:38.340 --> 00:31:38.920
Yeah, exactly.

00:31:39.220 --> 00:31:39.520
That's right.

00:31:39.740 --> 00:31:42.300
Just something to help us figure it out.

00:31:42.980 --> 00:31:44.580
Gives us a little fight about it on the internet.

00:31:45.020 --> 00:31:45.720
Yeah, that's right.

00:31:46.860 --> 00:31:47.760
Let's make memes about it.

00:31:47.850 --> 00:31:47.940
Okay,

00:31:48.120 --> 00:31:48.960
so the next

00:31:48.960 --> 00:31:50.600
one is Polars IP tools.

00:31:50.790 --> 00:31:52.040
I'm pretty sure that's how we'd say this.

00:31:52.660 --> 00:31:54.100
Yeah, this one's less debating.

00:31:54.640 --> 00:31:55.220
So this is

00:31:55.220 --> 00:31:57.300
a library by Eric Hutchins.

00:31:57.660 --> 00:32:01.340
And if you're playing with IP addresses, this could be helpful to you.

00:32:01.880 --> 00:32:06.740
It essentially provides a bunch of data frame methods for mucking around with IP addresses in a column.

00:32:07.540 --> 00:32:11.320
One of the functions it provides is private.

00:32:11.660 --> 00:32:14.340
This returns true if the address is a private network.

00:32:14.490 --> 00:32:20.400
So if you know 192, 168 or one of those groupings that's private, then you get true or false based back from it.

00:32:21.380 --> 00:32:23.020
Another method is in.

00:32:23.620 --> 00:32:32.340
So you give it a list of addresses with masks, and then it'll give you true or false for every IP address as to whether or not it's in that specific group of ranges.

00:32:33.460 --> 00:32:45.520
The library also integrates MaxMind's GeoIP address database, if you've got access to that, as well as the Spur VPN database, which contains information about what's a proxy, what's a VPN, that kind of stuff.

00:32:45.920 --> 00:32:59.660
So if you're doing a whole bunch of log management and you're dealing with all those IP addresses that are coming in your logs and you want to use pullers to muck around with that and create reports, then IP tools might be helpful for some of that kind of stuff.

00:33:00.100 --> 00:33:00.380
Yeah.

00:33:00.760 --> 00:33:04.920
If you're doing online sales, you want to know where the sales coming from or where's the

00:33:04.920 --> 00:33:05.720
interest coming from,

00:33:05.960 --> 00:33:06.600
say, for marketing.

00:33:06.800 --> 00:33:07.960
This would be super valuable.

00:33:09.200 --> 00:33:15.220
Once a sale happens, often you can say, well, they put their address or zip code or credit card information.

00:33:15.310 --> 00:33:16.000
We can see that.

00:33:16.760 --> 00:33:21.820
But there's way more traffic upstream from that that doesn't necessarily buy things, right?

00:33:21.880 --> 00:33:23.900
You can see where the interest is and so on.

00:33:24.320 --> 00:33:29.520
I actually don't use this library, but I use the MaxMine stuff over at Talk Python.

00:33:30.040 --> 00:33:32.720
Because when you come to visit a course, I want to show it in your currency.

00:33:32.850 --> 00:33:33.680
Well, what is your currency?

00:33:33.750 --> 00:33:33.920
Well, it

00:33:33.920 --> 00:33:34.180
belongs

00:33:34.180 --> 00:33:34.820
to your country.

00:33:35.360 --> 00:33:35.960
What is your country?

00:33:36.600 --> 00:33:38.360
You probably haven't told me, but I know your IP address.

00:33:38.560 --> 00:33:41.620
So let me see if I can figure that out and at least get it right most of the time.

00:33:42.780 --> 00:33:43.660
So yeah, this is cool.

00:33:45.780 --> 00:33:46.040
Right.

00:33:48.460 --> 00:33:49.040
Fuzzy match.

00:33:50.240 --> 00:33:51.180
This is great.

00:33:51.960 --> 00:33:53.020
Goose, geese, which is it?

00:33:53.160 --> 00:33:53.700
Which am I finding?

00:33:54.140 --> 00:33:54.540
Exactly.

00:33:55.100 --> 00:34:05.360
So moving from IP addresses, which are very specific strings, to some more generic ones, the Polars fuzzy match library does kind of what you would expect from its name.

00:34:05.780 --> 00:34:09.100
The GitHub user's name was bnmock3.

00:34:09.899 --> 00:34:13.360
He didn't write out his full name, so that's what we're going to go with.

00:34:14.120 --> 00:34:16.419
His avatar, or their avatar, is delightful.

00:34:16.960 --> 00:34:19.840
I square symbol, non-ASCII characters, which is funny.

00:34:21.000 --> 00:34:26.379
So if you're not familiar with fuzzy matching, it's about approximate matches on a string.

00:34:27.300 --> 00:34:31.659
So this library comes with a function that returns a score based on the thing that you're matching.

00:34:31.919 --> 00:34:39.820
So let's say I had a series of strings that I wanted to match against the word car, and that list had care and card in it.

00:34:39.870 --> 00:34:42.500
Those are close, so they might have a score of like 80-something.

00:34:43.179 --> 00:34:48.360
Well, frog, which has no relationship to car, that scores a null.

00:34:48.889 --> 00:34:51.780
So this allows you to sort of see how close you are.

00:34:52.600 --> 00:35:03.620
And the fuzzy matching routine that they're using is based on an underlying Rust library called Nucleo, which comes out of the Helix editor.

00:35:04.110 --> 00:35:06.260
So it's supported in a whole bunch of different ways.

00:35:06.460 --> 00:35:08.020
This isn't just some random guy on the Internet.

00:35:09.060 --> 00:35:19.740
And it also has a bunch of ways of specifying patterns like submatches, starts with suffix matches, inverted matches, and more.

00:35:19.980 --> 00:35:28.300
I kind of like the suffix match feature, so it allows you to do fuzzy matching just on, say, the suffix part of the extension part of a file name and ignore the rest.

00:35:28.520 --> 00:35:31.900
So you can, you know, whether they mucked up MP3 or MP4 or whatever.

00:35:32.680 --> 00:35:34.160
So that's kind of a neat little piece.

00:35:34.900 --> 00:35:40.480
And then along the same lines, the next one's Polars strsim.

00:35:41.480 --> 00:35:45.420
It is, so fuzzy matching is based on something called a distance calculation.

00:35:46.320 --> 00:35:55.600
And this essentially takes two strings and then gives you a value for typically it's between one and zero as to how close they match.

00:35:56.020 --> 00:35:57.720
So if they're identical, it gives you a one.

00:35:57.980 --> 00:36:01.160
And if they have nothing in column, it gives you a zero.

00:36:01.700 --> 00:36:07.000
The most popular of these is Levenstein distances, but there are other ways out there.

00:36:09.380 --> 00:36:13.580
The Jacquard, for example, or the Sorensen dice or whatever.

00:36:14.320 --> 00:36:15.900
Yeah, I wasn't even going to attempt that.

00:36:16.220 --> 00:36:16.940
So thank you.

00:36:19.020 --> 00:36:22.000
These kinds of calculations get used in things like, say, spell checking software.

00:36:22.440 --> 00:36:31.980
So if you've got something that isn't in the spelling dictionary and it wants to make a suggestion, then what it does is it runs one of these kinds of distances on a bunch of words in the dictionary.

00:36:32.420 --> 00:36:35.700
And those values that are close are the suggestions that it'll give you.

00:36:36.040 --> 00:36:45.740
So the Polar STR SIM library, which is from a fellow Canadian, Jeremy Foxcroft, allows you to calculate these string distances on your data frames.

00:36:47.220 --> 00:37:12.779
as you mentioned it supports a whole bunch of mechanisms which I'm not even going to pretend to pronounce and each of the calculators basically takes two column names and then calculates the distances between them which makes it really easy to include in something like a with_columns call so that you can just sort of add this to your data frame and go along with your distance information so if you're trying to do partial matching either of these two libraries could be helpful for you

00:37:13.380 --> 00:37:39.740
yeah they seem really great just a little bit of a side note here i think this is an interesting pattern with this nucleo rust library that is actually kind of the foundation of um the polar fuzzy match right i think this is something people should kind of keep their eye on out there is like okay so this is not super popular but so you might think well maybe it's not going to work that well but if

00:37:39.740 --> 00:37:40.120
it's just

00:37:40.120 --> 00:37:41.460
kind of a wrapper around something

00:37:41.460 --> 00:37:41.980
that is

00:37:41.980 --> 00:37:43.600
super popular, right?

00:37:43.950 --> 00:37:47.200
An example would be Granian.

00:37:47.310 --> 00:37:54.740
I had Giovanni on the show to talk about the Granian Python production worker process thing.

00:37:56.120 --> 00:38:01.840
And it's getting pretty popular, 3.5K, but you look at, say, Uveacorn or something like that.

00:38:01.870 --> 00:38:04.740
But then really what it is kind of a wrapper around Hypercrate.

00:38:05.260 --> 00:38:09.100
Well, Hypercrate is way more popular than written in Rust, right?

00:38:09.220 --> 00:38:13.280
used by 360,000 projects and 400 contributors.

00:38:13.380 --> 00:38:18.860
Like, oh, okay, so Gradient is kind of like an interface to hyper in

00:38:18.860 --> 00:38:19.240
a sense,

00:38:19.400 --> 00:38:19.520
right?

00:38:20.060 --> 00:38:22.620
And that's also a little bit what's going on here.

00:38:22.800 --> 00:38:26.220
And so when you're evaluating these libraries, I think that's worth keeping in mind.

00:38:26.340 --> 00:38:26.680
What do you think?

00:38:26.880 --> 00:38:27.660
Yeah, no, for sure.

00:38:27.820 --> 00:38:35.300
Like it's, you know, not that you can't, not that you can't create bugs in a wrapper because you can create bugs in

00:38:35.300 --> 00:38:35.580
anything.

00:38:36.040 --> 00:38:39.420
But, you know, the less code, the less likely there's a problem.

00:38:39.900 --> 00:38:48.740
And, you know, if it's built, like you say, if it's just a thin veneer on something that is big and popular and well-maintained, then, you know, the fact that it's a thin veneer becomes less

00:38:48.740 --> 00:38:49.560
important.

00:38:50.600 --> 00:38:50.940
Right, right.

00:38:51.140 --> 00:38:52.620
Like, well, that veneer is not that popular.

00:38:52.740 --> 00:38:54.200
Like, that veneer doesn't matter that

00:38:54.200 --> 00:38:54.400
much.

00:38:54.480 --> 00:38:54.560
Okay.

00:38:56.540 --> 00:38:57.280
Maybe you should encrypt

00:38:57.280 --> 00:38:57.580
it, though.

00:38:57.900 --> 00:38:58.160
If it

00:38:58.160 --> 00:38:59.580
has some sort of data, you should encrypt it.

00:38:59.840 --> 00:39:00.320
There you go.

00:39:00.460 --> 00:39:04.180
And this is another one, actually, that, you know, we'll pretend we planned that.

00:39:04.480 --> 00:39:06.360
So this is another one, which is a thin veneer.

00:39:07.040 --> 00:39:16.140
So if you want to be sneaky with your text here, Niazi Garagashli, which I'm sure I'm butchering, has written Polars Encryption.

00:39:16.940 --> 00:39:23.140
This library incorporates AES ciphers into Polars, providing encrypt and decrypt functions as expressions.

00:39:23.700 --> 00:39:37.000
So if you're mucking around with, say, personally identifiable information, or you're just sending other data scientists secret messages about your pointy-haired boss, then this little veneer could actually be fairly useful to you.

00:39:39.200 --> 00:39:46.720
Well, I'm thinking of maybe there's like encryption at rest type of stuff you want to be having going on.

00:39:47.480 --> 00:39:51.300
You know, there's a lot of different file formats that you can save in, right?

00:39:51.300 --> 00:39:58.040
I mean, there's CSV or Excel, but you don't really want them necessarily if you're doing like native pullers, right?

00:39:58.100 --> 00:40:03.160
we have Parquet, PyArrow, other better types, right?

00:40:03.690 --> 00:40:11.000
So maybe you import some data and you then encrypt it so that file sitting on disk, somebody grabs it like, so what?

00:40:11.400 --> 00:40:18.640
I mean, your app still has to know the encryption key and maybe decrypts it in a data frame in memory, but it's a really nice sort of back-and-forth library for that, right?

00:40:18.940 --> 00:40:21.800
Yeah, and data frames are a perfect place to do that kind of thing.

00:40:21.900 --> 00:40:29.900
Encryption tends to be expensive, but data frames tend to be fairly fast about that kind of stuff.

00:40:30.220 --> 00:40:36.520
So you're going to outperform using a mechanism like this that's built underneath as a Polaris plug-in than, say,

00:40:36.660 --> 00:40:37.440
building a

00:40:37.440 --> 00:40:38.940
loop yourself and doing it.

00:40:39.140 --> 00:40:45.800
So it tends to, like you said, it gives you that safety, but it gives you it in a way that's a little speedier than doing it yourself.

00:40:46.420 --> 00:40:49.220
Yeah, I don't remember if I spoke to Richie about this.

00:40:49.340 --> 00:40:51.300
Maybe I did, but I don't remember what he said if I did.

00:40:52.220 --> 00:40:52.880
That's for sure.

00:40:54.780 --> 00:41:09.780
does does Polars do parallelism automatically or is there a way to ask it to do it easily right because this like this encryption per entry in a column is one of the embarrassingly parallelizable algorithms right I

00:41:09.780 --> 00:41:25.000
know there are async pieces underneath I haven't played with them myself so I'm not sure exactly how you trigger that I know so Polars itself although it's very much a Python library the part of it underneath is still rust.

00:41:25.290 --> 00:41:29.040
And I believe that has been built so that you can get some of that concurrency.

00:41:29.680 --> 00:41:32.220
I don't know how easy it is to just turn that on.

00:41:33.040 --> 00:41:34.120
It hasn't been

00:41:34.120 --> 00:41:40.780
something, you know, one of the things with concurrency, I guess it's actually what I did my master's degree in.

00:41:40.830 --> 00:41:45.960
And the only thing I really remember is really, really don't do it unless you absolutely have to.

00:41:46.960 --> 00:42:01.220
And, you know, I routinely ingest a million rows with pollers in a sub-second, so I've yet to need concurrency because the size of the data stuff that I deal with isn't in that petabyte range where it actually would start to matter.

00:42:01.950 --> 00:42:05.060
So there are mechanisms there, but I can't speak to them very well.

00:42:06.179 --> 00:42:06.900
Yeah, I

00:42:06.900 --> 00:42:07.460
haven't done them either.

00:42:10.900 --> 00:42:11.820
Time to move on?

00:42:12.490 --> 00:42:13.160
Yes, time to move on.

00:42:13.320 --> 00:42:18.060
Apparently there are ways to do it, but it's not worth going over in audio.

00:42:18.920 --> 00:42:24.220
okay let's move on to xdt so sorry yeah i was i

00:42:24.220 --> 00:42:26.520
wasn't being snarky there i was no no i know

00:42:32.140 --> 00:42:32.660
this

00:42:32.660 --> 00:42:42.580
is a date time functionality library uh and it actually comes from the polars folks so i think what they're kind of doing here is kind of like that contrib thing inside of django which is

00:42:42.580 --> 00:42:42.860
yeah

00:42:42.870 --> 00:42:43.340
we don't

00:42:43.340 --> 00:42:53.320
really need this we'll keep it separate but if you need it it's there it essentially is collection, I think it's like 10 or 12 functions that essentially are extra date stuff.

00:42:53.920 --> 00:42:54.940
For example...

00:42:54.940 --> 00:42:58.600
The last commit was by Marco Guarelli, who's been on the show before.

00:43:00.760 --> 00:43:30.000
So for example, the date range function returns a series of dates and can be used to populate a column in data frame. So it takes a start and end date as arguments and then will work with a variety of intervals and allows you to ignore weekends and holidays. So if you want to generate like a time series kind of thing. This will do that. The day name function returns the name of the day of the week, supporting localization. Similarly, there's format localized, and I think there's other stuff for months and things like that in here as well.

00:43:31.740 --> 00:43:48.920
The one I kind of liked, I've done a couple courses on astronomy because a colleague of mine is working on his degree in this space. And so it always fascinates me when you move sort of sideways in a science and you're like, you're doing what? So this library

00:43:48.920 --> 00:43:49.560
supports

00:43:49.560 --> 00:44:36.040
calculating a Julian date, which is kind of like a Unix epoch, except day one is in 4713 BCE. And although this is fun for history buffs, it gets used in astronomy so that they can do time-lapse calculations as simple addition and subtraction. And so it's like Julian one versus whatever we're in now, which is julian 6000 something uh and so if you're as strange as this may seem if you happen to be working in that space this plugin allows you to switch between julian and back and forth and then of course julian dates also mean something completely different so that gets confusing as well but you know welcome to software and welcome to date times and software for sure yes we double so in date time

00:44:36.040 --> 00:45:00.940
yeah so there's some interesting things here and i have some real-time follow up here in a second. So it says Blazing Flask, written in Rust, convert to and from multiple time zones, which is great, format date, time zones, and different locales, dueling dates. And then there's a couple of things that are crossed off and say they've been upstream to polars, which is really cool. It's like, actually, this turns out to be important enough. It should just be in polars. So there's this cool

00:45:00.940 --> 00:45:02.840
interplay here for sure.

00:45:03.180 --> 00:45:11.080
Yeah, I'm pretty sure this is somebody's playground. So I think they're maintaining it separately and i think if it's something that's popular enough it'll move up but uh

00:45:11.820 --> 00:45:12.480
and it isn't a

00:45:12.480 --> 00:45:20.400
github sorry it is in the github uh pollers repos not an organization not just yeah yeah and

00:45:20.400 --> 00:45:30.540
that was kind of what i was about to say right back to your question about sort of a trust thing well the people who write the library when they write another library to go with it you're probably okay absolutely um

00:45:30.540 --> 00:45:42.540
marco garelli this is my real time fellow up says hey some of these didn't fit into polars because they would have increased the size of the binary too much due to extra dependencies right so

00:45:42.540 --> 00:45:44.080
that's great like too

00:45:44.080 --> 00:45:46.200
heavy weight but let's make it an extra so

00:45:46.200 --> 00:45:49.240
yeah pretty cool that's a good reason to do it yeah yeah

00:45:49.240 --> 00:45:57.860
indeed all right let's go and talk statistics a bit yeah what's the chances of that huh so

00:45:57.860 --> 00:46:02.240
so from making from simple math to harder math. Yeah.

00:46:04.700 --> 00:46:14.640
So let's say you're doing some audio signal processing and you need to pull some of the frequencies out or you want to try and remove some noise. This process is called filtering.

00:46:15.540 --> 00:46:57.500
One way of doing this is to use something called a least mean square filter, which is a special calculation that tries to minimize the mean square of the error between the signal and the desired result. You too can sound like you know what you're talking about by reading a Wikipedia entry into the microphone. So if most of what I said was meaningful to you, then the Polars OLS library might be your thing. This is by Azmi Rajab and performs least mean squares calculations using a variety of different mechanisms. It allows you to do this as a Polars expression. And of course, the underlying code, like a lot of these, is Rust-based, so it's nice and snappy performance-wise.

00:46:58.030 --> 00:47:12.500
The expression even includes different ways of dealing with nulls, which is one of those things that gets in the way with real-world data. So this library strikes me as one of those things that will save someone a whole lot of time. So if you happen to be that someone, there you go.

00:47:13.820 --> 00:47:15.860
Yeah, that's excellent. You know, I

00:47:15.860 --> 00:47:26.540
think in data science, there's a lot of this tension of, do I actually need to know the algorithm and the details of this or do I just need to know that I can call this function

00:47:26.540 --> 00:47:27.400
and when

00:47:27.400 --> 00:47:40.580
I should and when I should call it and I think that's a valid tension right because if something you call it on the wrong data and you get the wrong answer but you're like look I've discovered life on another planet oh no I discovered dust yeah right

00:47:40.760 --> 00:48:01.620
you know it's um I was uh doing a bit of tutoring for a friend of mine's daughter a few years back and one of the things I ran into is you're allowed to use calculators now which when I was a kid calculators basically didn't exist and you weren't allowed to touch them but now this is part of the math program and one of the things we were talking about a lot is well if you

00:48:01.620 --> 00:48:01.900
don't

00:48:01.900 --> 00:48:29.180
know what answer you're expecting then when it says a thousand and it was supposed to be c10 you're not going to know you hit the wrong button so I think that's kind of that this is exactly what you're talking about you need to understand it enough to sort of go am I getting something back that makes sense but if you just call the function blindly, garbage in, garbage out but if you, there's a lot of depth to some of these things and you could spend an entire career trying to understand it all and that's not

00:48:29.180 --> 00:48:29.520
going to help

00:48:29.520 --> 00:48:38.280
you solve the problem you're trying to solve, right? So yeah, like you said, balance Indeed What about pairing?

00:48:38.850 --> 00:49:15.340
Okay, so this one's off the charts so so a pairing function is one that takes two natural numbers and maps that to a single natural number this process can also be undone so you can take a single natural number and find out what pairs could be used to create it this is related to set theory and gets into that funky domain where you can start to talk about some kinds of infinity are bigger than other kinds of infinity the Polars Pairing Library by Antonio Camargo provides three different pairing functions.

00:49:16.640 --> 00:49:17.480
I'm not even going to try.

00:49:18.020 --> 00:49:19.880
One of which is Cantor, that one I can pronounce,

00:49:20.320 --> 00:49:20.780
which allows

00:49:20.780 --> 00:49:25.060
you to pair columns in a data frame or unpair a result.

00:49:25.800 --> 00:49:25.900
I've got

00:49:25.900 --> 00:49:27.180
Sudzik and

00:49:27.180 --> 00:49:28.220
Hagen as well.

00:49:28.430 --> 00:49:28.880
Got them all.

00:49:29.160 --> 00:49:29.660
There you go.

00:49:30.460 --> 00:49:35.940
If you Google around, you can find all sorts of articles on what this is and how it gets used in set theory.

00:49:36.720 --> 00:49:41.740
How you'd use it in your code is a mystery to me, but maybe I just didn't dig enough.

00:49:43.020 --> 00:49:48.800
But if you're mucking around and trying to understand canter pairing, why not?

00:49:49.660 --> 00:49:58.280
If on the other hand, one of the things I saw come up a couple of times was that some of these libraries are often used as examples for how to write a Polars library.

00:49:59.120 --> 00:50:01.820
So there are certain things you need to do if you're writing a plugin.

00:50:02.680 --> 00:50:12.480
And so some of these small libraries, as much as this might not apply to your day-to-day, because they're a small library, they can actually be helpful for you to understand the structure of things.

00:50:13.060 --> 00:50:14.920
Because you don't need to understand the math underneath this.

00:50:15.000 --> 00:50:18.040
That's just a function call, but you can see the structure of things out there.

00:50:18.200 --> 00:50:23.520
Sometimes something like this that is, why would I do that, could actually be a good lesson.

00:50:25.000 --> 00:50:25.400
Yeah.

00:50:26.160 --> 00:50:29.540
OK, well, it seems like maybe encryption.

00:50:30.180 --> 00:50:30.640
I don't know.

00:50:31.220 --> 00:50:32.400
But yeah, if you're doing set theory.

00:50:33.220 --> 00:50:36.040
In theory, this has to do with hashing.

00:50:36.320 --> 00:50:38.660
In practice, I think there are better ways of hashing.

00:50:38.800 --> 00:50:57.480
so um yeah yeah they read a question how about that yeah all right so as a complete aside um there's a several list items on the polar's package list use the little bear emoji in their docs and i'm not sure if this is just my font but it looks like a panda and

00:50:57.480 --> 00:50:59.840
it looks like a panda to me too yeah so so

00:50:59.840 --> 00:51:18.820
to be clear if polar bears and panda bears lived in the same place panda bears would be appetizers and they even come with their own little bamboo garnish so uh funnily enough i'm not sure if this is a strong signal but not one of the ones that used the baron emoji was one i could get working on my sim system so i don't know for whatever that's worth

00:51:18.820 --> 00:51:19.800
funny

00:51:19.800 --> 00:51:29.940
anyways uh so we kind of got into the weeds there so now we'll like step back a little bit and talk about a few things that are a little more higher level.

00:51:31.120 --> 00:51:35.120
The Polars List Utils, a good name, kind of describes what it does.

00:51:35.600 --> 00:51:42.580
This is by Travis Hammond, and it gives you a series of functions that operate on columns that have lists within the column.

00:51:44.339 --> 00:51:59.160
So one of the stronger uses here is something called Aggregate List Column Element Wise, which is a long function name, but it does aggregation operations on the values inside the list data, supporting the same kind of aggregation that regular pullers does.

00:51:59.290 --> 00:52:01.960
So you can do a mean account or sum or whatever on

00:52:01.960 --> 00:52:02.620
those things that

00:52:02.620 --> 00:52:03.480
are in the column.

00:52:04.080 --> 00:52:20.700
So the library seems to be coming out of the signal processing space, I suspect, because it supports a whole bunch of fast Fourier transform mechanisms, including applying them, getting frequency bins, linear spacing, and all that kind of stuff.

00:52:21.620 --> 00:52:23.820
The library also has some decent examples in its repo.

00:52:24.840 --> 00:52:34.240
This was obviously written by somebody doing something with audio because there's a whole bunch of sample data and graph pictures and things like that on it.

00:52:34.940 --> 00:52:44.740
As a quick aside, this project explicitly required Python 3.10 through 3.12, but I was able to get it working using the ignore requires Python flag to pip.

00:52:44.840 --> 00:52:45.820
So I don't think there's anything.

00:52:46.100 --> 00:52:50.320
I was using 3.13 and there wasn't anything that was actually 3.12 specific.

00:52:51.120 --> 00:52:54.580
So if you bump into that, if you're playing with it, don't worry about that too much.

00:52:55.960 --> 00:53:00.140
interesting yeah probably they just over constrained their requirements when specifying

00:53:00.140 --> 00:53:09.440
the package right i ran into i ran into it with a couple of other libraries that i tried out and they actually didn't work uh this wasn't that case though so this worked fine

00:53:09.440 --> 00:53:10.280
okay yeah

00:53:10.280 --> 00:53:12.520
very interesting and

00:53:12.520 --> 00:53:14.680
then i hardly believe it i hardly believe it

00:53:14.680 --> 00:53:20.140
uh i love the image with this one that's a nice biker going thing here. So

00:53:20.140 --> 00:53:21.080
this is Harley.

00:53:21.230 --> 00:53:35.500
This is by Tom Burge. And although the docs explicitly say it isn't associated with the bikes or the comic, there's a nice picture of a motorcycle in acknowledgement and an acknowledgement that this is a port of a Pi Spark library called Quinn.

00:53:35.570 --> 00:53:41.720
So that's where it comes from. This is a catch all library. It's got a bunch of different things.

00:53:42.660 --> 00:53:52.540
So, for example, anti-trim, remove all white space, and single space are all functions that deal with leading, trailing, multi-white space inside of strings.

00:53:54.099 --> 00:53:58.940
Approximate equal compares two columns for equality within a given threshold.

00:53:59.320 --> 00:54:04.240
So that's kind of useful if you've got a couple of floats and, you know, if it's within 10% of each other, then that's good enough.

00:54:05.380 --> 00:54:09.320
Multi equals checks if multiple columns have a specified value.

00:54:09.620 --> 00:54:11.520
So, you know, again, different comparison things.

00:54:12.580 --> 00:54:25.640
div or else divides one column by another but allows for default value if the divisor is zero so that's useful when you're not supposed to be dividing by zero i'm told that's not allowed uh and then there's some validation tools and

00:54:25.640 --> 00:54:28.140
it's just a good recommendation yeah

00:54:28.140 --> 00:55:01.300
yeah and there's also some validation tools boolean tools and a few things in here that can help you get information about your schema so nice little all told collection uh and yeah i love these kinds of things. There's a couple ones in the Django world that, oh, there's one I wrote, and there's the extensions one as well. These collections are often, I find these are the ones that often save me the most amount of time because there's usually something in them that I'm going to use again because this is written by somebody who was trying to solve some problem and just didn't want to do it twice.

00:55:02.480 --> 00:55:03.020
Yeah, very cool.

00:55:05.500 --> 00:55:08.000
Yeah, I think that might be all over.

00:55:08.340 --> 00:55:08.460
That's

00:55:08.460 --> 00:55:09.940
the

00:55:09.940 --> 00:55:10.500
code list.

00:55:10.640 --> 00:55:18.560
And I was thinking after we went through them, I might know if it were 10 or not, but I'm even more confused because we went down so many tangents.

00:55:19.060 --> 00:55:20.280
I don't even know whether some of them count.

00:55:20.800 --> 00:55:26.060
But I can tell you that those are a lot of helpful tools and libraries for people.

00:55:26.779 --> 00:55:29.400
So thanks for putting that list together and doing all the research, Chris.

00:55:29.860 --> 00:55:30.360
Yeah, for sure.

00:55:30.640 --> 00:55:33.420
And we only really covered a few of the plug-ins.

00:55:33.660 --> 00:55:47.940
The awesome pollers list also has links to articles, links to blog posts, things on different interfaces for pollers on other languages that you can use to interact with it.

00:55:48.580 --> 00:55:54.460
So even over and above the libraries we talked about there, there's a fair amount of decent content there.

00:55:55.740 --> 00:55:56.200
Yeah, absolutely.

00:55:57.320 --> 00:55:57.680
All right.

00:55:58.580 --> 00:55:59.940
A couple of final calls to action.

00:56:00.680 --> 00:56:03.120
I'll let you have the final word, but I'll do a few for you.

00:56:04.140 --> 00:56:09.900
check out your django book very cool even has some htmx in it check out

00:56:09.900 --> 00:56:10.380
your polars

00:56:10.380 --> 00:56:11.400
course which

00:56:11.400 --> 00:56:12.020
i'll

00:56:12.020 --> 00:56:25.620
link to in the show notes and is very germane to this conversation and with that people want to do some polars maybe want to level up a little bit with some of these things we talked about what do you tell them um

00:56:25.620 --> 00:56:31.260
you know it's a very approachable library i think it's uh It's one of those things you can dig in fairly easily to.

00:56:32.840 --> 00:56:40.520
Not to say, hey, you know, the course is very helpful, but the guides that come with pullers are also very, very readable as well.

00:56:41.560 --> 00:56:46.080
So if you're saving up to take the course later, hit the guides up as well.

00:56:46.100 --> 00:56:46.700
They can be helpful.

00:56:47.560 --> 00:56:49.300
And the community seems to be pretty good.

00:56:49.380 --> 00:56:53.720
I've seen a fair amount of your typical Python help each other out kind of space as well.

00:56:53.960 --> 00:56:54.460
So, yeah,

00:56:54.520 --> 00:56:55.440
it's a

00:56:55.440 --> 00:57:00.720
fun little library and very, very useful for solving some of your data science problems.

00:57:01.980 --> 00:57:03.100
Awesome. Thank you.

00:57:03.520 --> 00:57:07.340
Let me do one more quick little shout-out now that I'm thinking about it at the end here.

00:57:07.860 --> 00:57:19.300
Speaking of Marco Garelli, I had him on recently to talk about Narwhals, which is a library that will help you bridge code between working with Pandas and working with Polars and other data frame libraries.

00:57:19.480 --> 00:57:30.620
So if you've got some code and you want to try out Polars on it, but it's mostly in Pandas or some other data frame library, like, I don't know, Dask or something, you can check out Narwhals as well.

00:57:31.000 --> 00:57:33.940
And the episode is 480, so not too long ago.

00:57:34.380 --> 00:57:37.240
Anyway, one more thing for people to help them get started.

00:57:40.160 --> 00:57:41.860
All right, Christopher, thanks for being on the show.

00:57:42.240 --> 00:57:43.660
Always fun to catch up with you.

00:57:44.050 --> 00:57:44.540
Yeah, great.

00:57:44.820 --> 00:57:45.280
Thanks for having me.

00:57:45.880 --> 00:57:46.360
Yeah, bye.

