WEBVTT

00:00:00.001 --> 00:00:02.400
Alex, welcome to Talk Python To Me.

00:00:02.400 --> 00:00:04.640
Howdy, thanks so much for having me.

00:00:04.640 --> 00:00:06.440
Yeah, it's fabulous to have you here.

00:00:06.440 --> 00:00:09.000
I'm really excited to talk about databases.

00:00:09.000 --> 00:00:13.440
Databases are so important for making your app work well,

00:00:13.440 --> 00:00:15.160
making them fast.

00:00:15.160 --> 00:00:18.440
It surprises me how many people have slow databases.

00:00:18.440 --> 00:00:19.680
It doesn't make sense.

00:00:19.680 --> 00:00:22.080
We're pretty big fans of databases around here, for sure.

00:00:22.080 --> 00:00:23.960
Yeah, and fast ones as well, right?

00:00:23.960 --> 00:00:25.880
As best we can.

00:00:25.880 --> 00:00:27.720
That's right.

00:00:27.720 --> 00:00:35.360
Well, DuckDB is certainly on the rise in terms of me hearing people talk about,

00:00:35.360 --> 00:00:36.680
like, "Oh, have you heard DuckDB?

00:00:36.680 --> 00:00:37.680
Have you seen DuckDB?

00:00:37.680 --> 00:00:39.880
Have you worked with it?"

00:00:39.880 --> 00:00:46.400
And I think it continues an interesting tradition of lots of low stress

00:00:46.400 --> 00:00:48.560
near your application type of databases.

00:00:48.560 --> 00:00:51.120
So it's going to be fun to dive into that, for sure.

00:00:51.120 --> 00:00:53.600
Before we do, tell us about yourself.

00:00:53.600 --> 00:00:54.600
Sure.

00:00:54.600 --> 00:00:55.960
Well, thanks so much for having me.

00:00:55.960 --> 00:00:56.960
Huge fan of the show.

00:00:56.960 --> 00:00:57.960
I'm really excited to be here.

00:00:57.960 --> 00:00:58.960
So I appreciate that.

00:00:58.960 --> 00:01:01.720
I work at MotherDuck.

00:01:01.720 --> 00:01:04.600
I'm a forward deployed software engineer, which is kind of a made up title.

00:01:04.600 --> 00:01:08.600
A couple of companies use it, but it's really a customer facing software engineer.

00:01:08.600 --> 00:01:14.640
So working closely with folks to see how MotherDuck fits in with all the rest of their stack.

00:01:14.640 --> 00:01:18.720
So MotherDuck is a cloud data warehouse with DuckDB at its core.

00:01:18.720 --> 00:01:20.920
So absolutely a huge part of what we do.

00:01:20.920 --> 00:01:25.680
I also work part time for DuckDB Labs, which is our partner organization.

00:01:25.680 --> 00:01:29.560
And I do blogging and some developer advocacy types of things for them.

00:01:29.560 --> 00:01:34.920
And so it's a separate company based in Amsterdam, the Netherlands.

00:01:34.920 --> 00:01:38.320
And they are database experts and researchers.

00:01:38.320 --> 00:01:40.080
And they're kind of the stewards of the open source project.

00:01:40.080 --> 00:01:41.080
Yeah.

00:01:41.080 --> 00:01:42.080
Awesome.

00:01:42.080 --> 00:01:45.080
What is a forward deployed engineer do?

00:01:45.080 --> 00:01:49.720
Do you get called into big companies who want to adopt the technology and they say,

00:01:49.720 --> 00:01:54.680
Hey, could you help us do this quicker or are they running in trouble?

00:01:54.680 --> 00:01:56.480
And they're like, you said it was fast.

00:01:56.480 --> 00:01:57.080
It's not fast.

00:01:57.080 --> 00:01:57.480
Help.

00:01:57.480 --> 00:02:00.680
Well, don't copy this every query so it'll be better.

00:02:00.680 --> 00:02:02.480
There's quite a lot of variety.

00:02:02.480 --> 00:02:08.240
I think there's definitely some aspects of, you know, hey, can you help me with this?

00:02:08.240 --> 00:02:13.040
There's also aspects of as folks are getting started, they're kind of picking out the rest of the tools around the database.

00:02:13.040 --> 00:02:15.120
So what BI tool should I use?

00:02:15.120 --> 00:02:17.200
What orchestrator should I use?

00:02:17.200 --> 00:02:20.720
And then, you know, we do get into some SQL optimization stuff like that.

00:02:20.720 --> 00:02:22.240
So that's kind of the customer facing side.

00:02:22.240 --> 00:02:23.400
That's the majority of the role.

00:02:23.400 --> 00:02:28.600
There's also troubleshooting that goes along with it, as well as kind of feeding things back into the company,

00:02:28.600 --> 00:02:31.960
being the voice of the customer saying, Hey, we've had three people hit this.

00:02:31.960 --> 00:02:34.440
Is there a way we can build something different?

00:02:34.440 --> 00:02:35.440
Yeah.

00:02:35.440 --> 00:02:37.920
Like everyone thinks they should do it this way.

00:02:37.920 --> 00:02:42.280
We need to change our docs or they want this particular feature and we don't have it.

00:02:42.280 --> 00:02:43.280
Yeah.

00:02:43.280 --> 00:02:44.280
Yep.

00:02:44.280 --> 00:02:46.280
It's a good cycle that we've got going.

00:02:46.280 --> 00:02:47.800
It sounds really fun.

00:02:47.800 --> 00:02:48.800
It's a lot of fun.

00:02:48.800 --> 00:02:50.800
The variety is my favorite part.

00:02:50.800 --> 00:02:50.800
Yeah, exactly.

00:02:50.800 --> 00:02:54.040
And you know, a lot of fun investigation, troubleshooting, you know,

00:02:54.040 --> 00:02:56.400
got to reframe it as a scavenger hunt.

00:02:56.400 --> 00:02:57.680
You know, it's fun.

00:02:57.680 --> 00:02:58.680
Yeah.

00:02:58.680 --> 00:03:00.800
Every day is like a box of chocolates, huh?

00:03:00.800 --> 00:03:01.800
Oh yeah.

00:03:01.800 --> 00:03:02.800
A little different all the time.

00:03:02.800 --> 00:03:05.800
You know, you don't know what you're going to get.

00:03:05.800 --> 00:03:10.320
I used to do in-person professional training.

00:03:10.320 --> 00:03:18.320
You know, I'd travel to Chicago or New York or Beijing and I would show up Monday morning,

00:03:18.320 --> 00:03:20.840
I folks, what are we, you know, what's the situation?

00:03:20.840 --> 00:03:25.920
Obviously, I'd know what we're going to talk about, but it was still, it was

00:03:25.920 --> 00:03:26.920
different every time.

00:03:26.920 --> 00:03:27.920
Every week was different.

00:03:27.920 --> 00:03:31.840
Sounds like it might be not, not so different from what you're doing.

00:03:31.840 --> 00:03:35.840
Are a lot of these in-person things or are they mostly Zoom style?

00:03:35.840 --> 00:03:36.840
It's Zoom style.

00:03:36.840 --> 00:03:41.440
It's Zoom style, you know, just we work with a variety of folks, not, you know,

00:03:41.440 --> 00:03:43.080
like a couple of really giant customers.

00:03:43.080 --> 00:03:46.360
So with that variety means that I couldn't fly fast enough.

00:03:46.360 --> 00:03:48.560
Yeah, for sure.

00:03:48.560 --> 00:03:52.240
Or, you know, issues I've had as well as like three people on the team are in this city,

00:03:52.240 --> 00:03:54.320
three people are in that other city and one is here.

00:03:54.320 --> 00:03:57.320
It's like, even if you flew somewhere, you would only be, you'd still be on Zoom.

00:03:57.320 --> 00:03:58.520
So you might as well just be on Zoom.

00:03:58.520 --> 00:04:00.520
Definitely, definitely.

00:04:00.520 --> 00:04:01.520
Absolutely.

00:04:01.520 --> 00:04:02.520
The world we live in.

00:04:02.520 --> 00:04:05.600
And well, I mean, that's a blessing, honestly,

00:04:05.600 --> 00:04:10.800
to not have to fly to all these places as well as just be able to pop in and

00:04:10.800 --> 00:04:11.800
out, right?

00:04:11.800 --> 00:04:14.320
I mean, this work from home story.

00:04:14.320 --> 00:04:15.320
It's amazing.

00:04:15.320 --> 00:04:18.120
It's only getting better all the time.

00:04:18.120 --> 00:04:23.000
So let's talk DuckDB and databases.

00:04:23.000 --> 00:04:26.080
I think maybe setting a bit of a mental model.

00:04:26.080 --> 00:04:31.600
A lot of folks are familiar with SQLite, right?

00:04:31.600 --> 00:04:33.360
Yeah, absolutely.

00:04:33.360 --> 00:04:35.360
And so SQLite is a lot of

00:04:35.360 --> 00:04:40.360
library, not a server that you can use that then plugs into your application,

00:04:40.360 --> 00:04:46.080
works on a local file rather than a certain thing you run in another VM or however you,

00:04:46.080 --> 00:04:48.080
you know, there's not a managed SQLite.

00:04:48.080 --> 00:04:49.080
Not that I know.

00:04:49.080 --> 00:04:50.080
I know maybe there's a managed SQLite.

00:04:50.080 --> 00:04:51.080
I don't know of one.

00:04:51.080 --> 00:04:52.080
There are.

00:04:52.080 --> 00:04:53.080
There are.

00:04:53.080 --> 00:04:54.080
Are there?

00:04:54.080 --> 00:04:56.800
Yeah, there's a bunch of cool stuff that you can do if you have it.

00:04:56.800 --> 00:05:00.080
A database that's just part of your application instead of somewhere else.

00:05:00.080 --> 00:05:06.800
You know, another one that I just ran across is Mocha dash pie or there's no

00:05:06.800 --> 00:05:11.800
dash pie, which is a Python wrapper around the I think it's rust Mocha caching thing.

00:05:11.800 --> 00:05:15.800
But it's instead of being Redis or something like that, it's like an in memory.

00:05:15.800 --> 00:05:19.520
Thing that you can run for sort of the same purpose.

00:05:19.520 --> 00:05:23.520
So DuckDB sort of falls into that realm, right?

00:05:23.520 --> 00:05:28.320
Yes, I think it's an in process database, just like you said.

00:05:28.320 --> 00:05:31.440
So it's definitely a library, you know, it's a import DuckDB.

00:05:31.440 --> 00:05:36.880
So it's not in the Python standard library, but it is on PyPI, just pip install DuckDB.

00:05:36.880 --> 00:05:39.680
It's pre-compiled zero dependencies.

00:05:39.680 --> 00:05:41.040
So it is super easy.

00:05:41.040 --> 00:05:43.760
Basically anywhere you can get to pip, you can get DuckDB running.

00:05:43.760 --> 00:05:45.360
So that could be your laptop.

00:05:45.360 --> 00:05:47.280
That could be a big beefy server.

00:05:47.280 --> 00:05:49.600
It could be a Lambda function.

00:05:49.600 --> 00:05:53.280
It could be inside of your orchestrator, you know, in a GitHub action.

00:05:53.280 --> 00:05:55.200
Kind of slots in everywhere.

00:05:55.200 --> 00:05:56.080
That's part of the magic.

00:05:56.080 --> 00:06:02.000
Presumably when you pip install DuckDB, it's just downloading a wheel for your

00:06:02.000 --> 00:06:03.200
architecture, for your platform?

00:06:03.200 --> 00:06:03.760
Absolutely.

00:06:03.760 --> 00:06:04.080
Yep.

00:06:04.080 --> 00:06:05.920
Pre-compiled all the accommodations.

00:06:05.920 --> 00:06:07.200
Yeah, that's really good.

00:06:07.200 --> 00:06:10.640
Yeah, if you just sit on the DuckDB.org homepage,

00:06:10.640 --> 00:06:14.960
you can learn about it as it animates different messages to you.

00:06:14.960 --> 00:06:20.000
DuckDB is a fast analytical database system, open source database system,

00:06:20.000 --> 00:06:22.320
in-memory database system.

00:06:22.320 --> 00:06:23.920
We'll see what else.

00:06:23.920 --> 00:06:24.880
Yeah, portable.

00:06:24.880 --> 00:06:30.320
So yeah, it works on most of the places, the main operating systems, right?

00:06:30.320 --> 00:06:31.440
Yes.

00:06:31.440 --> 00:06:32.320
Yep, it's all over.

00:06:32.320 --> 00:06:34.720
Can I Raspberry Pi it?

00:06:34.720 --> 00:06:36.160
I'd imagine the answer is yes.

00:06:36.160 --> 00:06:36.320
You can.

00:06:36.320 --> 00:06:38.160
Can I circuit Python it?

00:06:38.160 --> 00:06:39.280
Probably not right.

00:06:39.280 --> 00:06:44.480
You can do iPhone and Android. There's a Swift client so you can build iPhone apps and

00:06:44.480 --> 00:06:45.440
Oh, really?

00:06:45.440 --> 00:06:49.840
It works as well. So yeah, you could do that. It works in your browser with WebAssembly.

00:06:49.840 --> 00:06:53.920
It'll work with Pyodide in the browser as well. So it really goes all over.

00:06:53.920 --> 00:06:58.080
Oh, that's really cool. Yeah, one of the things that I said people are doing interesting stuff

00:06:58.080 --> 00:07:05.520
with and in mind with SQLite is I saw some folks suggesting using SQLite in the browser

00:07:05.520 --> 00:07:11.760
instead of local storage and like the local, like all the JavaScripty things. And

00:07:11.760 --> 00:07:13.440
could I do that with DuckDB?

00:07:13.440 --> 00:07:18.640
You could definitely do that with DuckDB. I think the use case is slightly different,

00:07:18.640 --> 00:07:21.760
but that's sort of the benefit is you can use both. So

00:07:21.760 --> 00:07:25.920
definitely could talk a bit more about the differences, but I'd say

00:07:25.920 --> 00:07:28.480
in general, a good mental model for DuckDB is kind of

00:07:28.480 --> 00:07:33.520
a from scratch separate project from SQLite, but taking a lot of the lessons

00:07:33.520 --> 00:07:37.680
and learnings about what makes people love SQLite and bringing that into the

00:07:37.680 --> 00:07:42.880
analytical realm. And that's really instead of inserting and updating tons of tiny

00:07:42.880 --> 00:07:46.720
transactions all the time really fast. It's big bulk operations, you know,

00:07:46.720 --> 00:07:52.080
aggregate a billion rows, join a billion rows to a billion rows, that type of thing.

00:07:52.080 --> 00:07:55.200
Yeah, doing that in memory in JavaScript is not that great.

00:07:55.200 --> 00:07:55.760
No.

00:07:55.760 --> 00:07:59.840
Doing that in memory in Python is not that great. But it sounds a little bit more

00:07:59.840 --> 00:08:04.480
like vector programming in the sense of kind of like

00:08:04.480 --> 00:08:10.640
you would do with pandas or pollers, right? You wouldn't loop over,

00:08:10.640 --> 00:08:16.000
you shouldn't loop over a pandas data frame processing each item. You should issue

00:08:16.000 --> 00:08:21.600
vector operations that apply to every row, like multiply this column by two,

00:08:21.600 --> 00:08:25.920
rather than for every C in call, you know, whatever, like looping over.

00:08:25.920 --> 00:08:31.520
Yes. So it is, it is exactly what you said. It's designed for that. It's vectorized

00:08:31.520 --> 00:08:36.000
execution. So it's a little different than pandas. It's a, you know, pollers is a bit more

00:08:36.000 --> 00:08:36.320
similar.

00:08:36.320 --> 00:08:40.480
Where instead pandas will process a whole column at a time.

00:08:40.480 --> 00:08:45.440
And that works a lot better than one row at a time until your columns get too big and then

00:08:45.440 --> 00:08:50.560
it crashes. Yeah. So what DuckDB does is it does it in chunks of about 2000 rows at a time.

00:08:50.560 --> 00:08:54.400
And so that way you get all the benefits of your modern CPU, you know, hierarchy,

00:08:54.400 --> 00:09:00.720
your caches, all that, where it's just pumping data through. But you also conserve memory and you

00:09:00.720 --> 00:09:07.120
can not get into issues with that. I will also say it's technically we are an in-process

00:09:07.120 --> 00:09:08.640
database, not an in-memory database.

00:09:08.640 --> 00:09:11.440
Oh, sorry. If I meant, if I said in-memory, I meant in-process.

00:09:11.440 --> 00:09:12.000
Yeah.

00:09:12.000 --> 00:09:15.600
It's, it is, I, you know, very much an interchangeable word, like in English.

00:09:15.600 --> 00:09:18.480
We just are very specific about it in database land.

00:09:18.480 --> 00:09:21.680
No, you do. And you do support in-memory databases.

00:09:21.680 --> 00:09:22.480
Yes.

00:09:22.480 --> 00:09:25.680
But that's not the same as in-process. Yeah, sorry. In-process is what I meant.

00:09:25.680 --> 00:09:27.760
Oh, no problem. All I mean to say there is, you know,

00:09:27.760 --> 00:09:32.480
you don't need to limit yourself to your size of RAM. You know, you can solve one terabyte

00:09:32.480 --> 00:09:34.960
problems on your laptop with DuckDB, no problem.

00:09:34.960 --> 00:09:36.240
Really?

00:09:36.240 --> 00:09:42.080
Yes. It handles larger than memory data, both streaming from disk. So it'll just read just

00:09:42.080 --> 00:09:47.440
the pieces it needs as it goes, and even in the intermediates. So let's say you join two tables

00:09:47.440 --> 00:09:53.360
together and you, you know, multiply the number of rows. It'll send things to disk as it gets memory

00:09:53.360 --> 00:09:58.480
pressure close to your memory limit. So it's a lot of effort has been put into that in the last year or

00:09:58.480 --> 00:10:04.240
so. So it's working pretty smooth now. Yeah, I can imagine that that's super important.

00:10:04.240 --> 00:10:10.640
So I'm really looking forward to talking about schemas, indexes, those kinds of things,

00:10:10.640 --> 00:10:15.840
which sound like they may play important roles here. But you talked about these different ways in which

00:10:15.840 --> 00:10:23.440
you can use DuckDB. Looking through the documentation, it looks like it works with not just Python, but quite

00:10:23.440 --> 00:10:25.520
a few platforms or languages.

00:10:25.520 --> 00:10:31.360
Yes, somewhere north of 15. So Python's the most popular way to use DuckDB. We've got some

00:10:31.360 --> 00:10:34.640
really tight integrations with data frame libraries, which we can definitely talk more about.

00:10:34.640 --> 00:10:40.720
There's a lot of Java use out there. So JDBC, you've got a JDBC driver, you can use it in

00:10:40.720 --> 00:10:48.880
JavaScript, both in the browser with WASM, or on the server with Node, popular Node package. You can also do

00:10:48.880 --> 00:10:56.880
Go, Rust, C++, C. I think there's a community driven Ruby client, really all over the place.

00:10:56.880 --> 00:10:57.840
Okay.

00:10:57.840 --> 00:11:01.040
.NET, C#, you can use it from .NET, all over.

00:11:01.040 --> 00:11:06.640
Yeah. Yeah, that's awesome. And I'm guessing it's interchangeable, the file?

00:11:06.640 --> 00:11:14.160
Yes. So just like SQLite files are designed to be read for a long time, they're very portable.

00:11:14.160 --> 00:11:19.760
DuckDB just kind of reached the 1.0 milestone in June. So super excited about that.

00:11:19.760 --> 00:11:25.360
And with that, promised stability with the file format. And that means that we expect for the

00:11:25.360 --> 00:11:31.600
next somewhere around five years to have your DuckDB files be totally compatible, where you can read

00:11:31.600 --> 00:11:38.720
a five-year-old file five years from now type thing. Which means it's like SQLite in that you can fit all your

00:11:38.720 --> 00:11:43.760
tables and all the relationships between them all in one file. So that's one nice thing about

00:11:43.760 --> 00:11:49.200
SQLite. You can email the whole database. You can do the same with DuckDB. But DuckDB is going to store it a

00:11:49.200 --> 00:11:55.440
lot more compactly if you've got a lot of data. Oh, really? Okay. You guys have worked a lot on the file format?

00:11:55.440 --> 00:12:02.000
It's, again, taking inspiration from SQLite, the format is very separate. And in databases, you kind of have

00:12:03.200 --> 00:12:06.720
a pretty hard fork in the road pretty early on in your architecture, which is you want

00:12:06.720 --> 00:12:13.360
to be row-based or do you want to be column-based? And row-based is SQLite and column-based would be

00:12:13.360 --> 00:12:19.120
your pandas, your pollers, and most of your cloud data warehouses, your snowflakes, that type of thing.

00:12:19.120 --> 00:12:24.960
And once you go column-based, suddenly your compression is amazing because you store data

00:12:24.960 --> 00:12:29.280
that's very similar right next to each other. So you can really compress it. Your dates are back-to-back

00:12:29.760 --> 00:12:34.240
Instead of a date column, string column, integer column in a row-based store,

00:12:34.240 --> 00:12:38.080
they don't compress well. So we can get somewhere around 5x the compression typically.

00:12:38.080 --> 00:12:43.440
Okay. That's really great. And you could take this file and you could put it

00:12:43.440 --> 00:12:52.960
into S3 or use it with some sort of Jupyter notebook, right? And just, they could just grab the file and run

00:12:52.960 --> 00:12:57.360
with it and it's all the data, yeah? Yes. Object stores are really the

00:12:57.920 --> 00:13:03.120
the name of the game there. You can even work with a file on object store and read it without

00:13:03.120 --> 00:13:08.000
having to download the whole file first. So DuckDB supports something like SQLite in that it's got

00:13:08.000 --> 00:13:13.280
this attach syntax. So you can say, "Hey, I want you to attach this file and then just read the pieces

00:13:13.280 --> 00:13:18.000
of it that I actually query," which is pretty slick. Yeah, that's super slick.

00:13:19.440 --> 00:13:27.920
All right. Let's-- let me scroll down here and let's talk about maybe using it from

00:13:27.920 --> 00:13:32.880
Python. Give us a sense of like what working with DuckDB is like here.

00:13:32.880 --> 00:13:36.000
You bet. So you've got your pip install DuckDB,

00:13:36.000 --> 00:13:42.160
which will download that pre-compiled binary. It's like 20 or 30 megabytes, so pretty close to instant.

00:13:42.160 --> 00:13:49.840
Yeah. So you can do DuckDB.sql. And then that-- within that function, you just pass in a string

00:13:49.840 --> 00:13:56.080
of your SQL statement and it'll execute. It's really that simple. And so it follows the DB API if you

00:13:56.080 --> 00:14:02.240
want, so you can get back your typical pupils. Or you can say .df at the end and get back a pandas data

00:14:02.240 --> 00:14:09.200
frame. Or .pl, get pullers, all kinds of stuff. Yeah, I saw that there's a lot of to and from the

00:14:09.200 --> 00:14:16.560
popular data science libraries, which seems really nice. You work in your data science library,

00:14:16.560 --> 00:14:24.160
or maybe run a query and get a subset of your data. And then maybe plot that with Plotly or Altair or

00:14:24.160 --> 00:14:27.120
pass it along to other things that expect the data frame, right?

00:14:27.120 --> 00:14:33.600
Yes, exactly. A lot of the DuckDB ethos is don't ask folks to ship their data necessarily

00:14:33.600 --> 00:14:38.160
somewhere else. Don't ask them to change their workflow dramatically to use a database. Meet folks

00:14:38.160 --> 00:14:42.320
right where they are. And that means you can have one line of code be in your data frame library of

00:14:42.320 --> 00:14:48.480
choice, you know, pandas, pullers, or arrow. And then a line of code in DuckDB, and then you're right

00:14:48.480 --> 00:14:53.840
back in your data frame in the next line. So it fits really wherever you want. It's a great fit if you

00:14:53.840 --> 00:14:58.480
want to throw some SQL at a problem. Some problems are easier to formulate in SQL, I think, than in

00:14:58.480 --> 00:15:05.120
data frame libraries and vice versa. But also, it's-- some libraries, it's harder to scale larger than memory.

00:15:05.120 --> 00:15:10.320
And DuckDB does have a lot of nice performance under the hood. So maybe the hardest

00:15:10.320 --> 00:15:16.400
part of your workflow, you can slide in DuckDB and make the minimum change to get it working.

00:15:16.400 --> 00:15:22.560
Yeah, well, it seems to me like maybe you put all of the data into DuckDB,

00:15:23.280 --> 00:15:27.440
Yeah, well, it's a good thing. And then you can ask questions, which result in Python

00:15:27.440 --> 00:15:32.960
data science objects that then you can work on where those questions result in, give me the stuff for

00:15:32.960 --> 00:15:38.000
this state or this time period or something like that where it's a reasonable amount of data. But

00:15:38.000 --> 00:15:41.120
if you tried to just load it all, then it would say no.

00:15:41.120 --> 00:15:44.240
I think that's a great way to go about it for sure.

00:15:44.240 --> 00:15:53.040
Another common workflow is to do some similar things, but with Parquet files or CSVs or JSON. So

00:15:53.040 --> 00:15:58.800
in the same way you can store a lot of your data in DuckDB, you can periodically write chunks out to Parquet,

00:15:58.800 --> 00:16:00.960
or read in from Parquet, all that type of stuff too.

00:16:00.960 --> 00:16:07.040
Yeah. Tell people what Parquet is. I know it comes from Pi Arrow or the Arrow project,

00:16:07.040 --> 00:16:09.120
right? Where does it come from?

00:16:09.120 --> 00:16:09.680
Definitely.

00:16:09.680 --> 00:16:12.000
I know they use it. I don't know if they came up with it, but yeah.

00:16:12.000 --> 00:16:17.600
They definitely collaborated and were a big part of it. So it's a community developed open

00:16:17.600 --> 00:16:25.280
format. It's Apache Parquet. So it's open source, community driven, and it's a columnar format.

00:16:25.840 --> 00:16:30.320
So instead of storing things by rows, it's columnar storage, and it's chunked columnar

00:16:30.320 --> 00:16:35.200
storage. So it's sort of like somewhere around every million rows or so, you kind of write out another

00:16:35.200 --> 00:16:42.800
chunk. And it ends up being a really good format to store large amounts of data in the cloud. And it's

00:16:42.800 --> 00:16:48.320
typically what's underneath a data lake or a data lake house. The vast majority of those have Parquet

00:16:48.320 --> 00:16:54.000
under the hood. We love the Parquet format. At DuckDB, we read it and write it. We have a custom from scratch

00:16:54.000 --> 00:16:59.920
reader, custom from scratch writer for it. We actually prefer our own format, given the choice

00:16:59.920 --> 00:17:05.520
as well. Parquet's been around for a decade or so at this point. And we've added a few things over time

00:17:05.520 --> 00:17:06.800
into DuckDB that are new.

00:17:06.800 --> 00:17:15.360
Yeah, Parquet is one of those things that Pandas 2.0 supports it, right? Polar supports it.

00:17:15.360 --> 00:17:22.000
It's pretty universal. So it might be a good way to get data from that you've worked on

00:17:22.000 --> 00:17:23.120
before into DuckDB.

00:17:23.120 --> 00:17:27.120
Yes, it's great. I think, you know, most universal,

00:17:27.120 --> 00:17:32.800
CSV, most difficult, also CSV. Parquet is a really great compromise there.

00:17:32.800 --> 00:17:33.280
Mm-hmm.

00:17:33.280 --> 00:17:36.560
Yeah.

00:17:38.640 --> 00:17:42.880
Hold on. My computer is lagging and distracting me. I apologize.

00:17:42.880 --> 00:17:54.560
One sec, scroll, scroll.

00:17:54.560 --> 00:17:56.560
Alex Bialik:

00:18:00.560 --> 00:18:12.560
Yeah. So DuckDB works well, it sounds like, for the data science story. Like, it's built for the data science story. If I was wanting to store relational data,

00:18:12.560 --> 00:18:18.960
you know, maybe I've already got DuckDB working for something like, you know, do I necessarily want to bring

00:18:18.960 --> 00:18:25.840
SQLite into the story as well and have these two things? Does it, is it possible to, say,

00:18:26.960 --> 00:18:29.600
do relational type things with DuckDB?

00:18:29.600 --> 00:18:37.920
Yes, that's a great question. So I think DuckDB is trying to bring the best of both. It is a full relational database,

00:18:37.920 --> 00:18:43.040
and as well as supporting some of the, you know, data science workflows that you typically in the past

00:18:43.040 --> 00:18:48.960
would have been doing in a data frame. So DuckDB is an analytical system, not transactional. So it's not going

00:18:48.960 --> 00:18:56.160
to be the fastest at doing individual row operations, but it supports them all. And it supports them with full

00:18:56.160 --> 00:19:00.960
asset transactions that you'd expect from a database where you won't get database corruption,

00:19:00.960 --> 00:19:05.440
even if your computer cuts out. You know, it's, it's got that kind of robustness that you expect from

00:19:05.440 --> 00:19:11.920
a database. So part of the story, though, is that with an, with an analytical database, it's typically

00:19:11.920 --> 00:19:16.080
not your only database, unless you're doing data science work. If you're building an application,

00:19:16.080 --> 00:19:21.440
you're going to want a transactional database for, you know, saving your, you know, when a customer places

00:19:21.440 --> 00:19:26.560
is in order, you know, you know, all that type of transactional stuff is still critical. So typically,

00:19:26.560 --> 00:19:31.600
DuckDB is going to be layered in addition. And the interoperability story there with DuckDB is pretty,

00:19:31.600 --> 00:19:38.080
pretty fantastic. You can actually read SQLite files directly with DuckDB. And it's going to be,

00:19:38.080 --> 00:19:41.520
if you're doing any sort of advanced processing on them, it's going to be a lot quicker

00:19:41.520 --> 00:19:46.960
in DuckDB. You can even read Postgres databases, you can read MySQL databases,

00:19:46.960 --> 00:19:47.520
Oh, wow.

00:19:47.520 --> 00:19:52.960
There's a community extension to read BigQuery. So it's, it's really a universal Swiss army knife

00:19:52.960 --> 00:19:58.240
in a lot of ways to read from those operational data stores, but to do those analytical tasks,

00:19:58.240 --> 00:20:03.840
you know, look at all my orders, what was the most popular five products? You know, that that's going

00:20:03.840 --> 00:20:07.360
to be a tough query for a transactional system, but bread and butter for DuckDB.

00:20:08.720 --> 00:20:16.480
So when you say read from, let's say Postgres, does that, does that import it into a table

00:20:16.480 --> 00:20:24.480
in DuckDB or does it translate the queries over to the underlying data system? Like what's happening

00:20:24.480 --> 00:20:24.720
there?

00:20:24.720 --> 00:20:31.680
That's a great question. We are communicating with Postgres. So we're not importing everything. We're

00:20:31.680 --> 00:20:38.080
sending the query to it and we're reading all the data. Typically that's going to be sparing the table

00:20:38.080 --> 00:20:43.760
or forming a view from Postgres. And then after that point, bring it into the DuckDB engine for,

00:20:43.760 --> 00:20:48.640
for subsequent processing, but you don't have to migrate all your data. It's, it's kind of a data

00:20:48.640 --> 00:20:54.960
virtualization is one of the buzzwords it's been around a little, but you know, it's getting to get some

00:20:54.960 --> 00:21:00.960
of the benefits of analytical performance without having to move your data first. It'll definitely

00:21:00.960 --> 00:21:06.000
be more performant if you put it in an analytical format, like Parquet, like DuckDB, like a analytical

00:21:06.000 --> 00:21:12.160
database, but you get a big fraction of that benefit with the engine part, which is the vectorized

00:21:12.160 --> 00:21:17.120
engine of doing things 2000 rows at a time, instead of one row at a time. That's a lot of it.

00:21:17.120 --> 00:21:18.960
Yeah. That's a huge benefit.

00:21:20.320 --> 00:21:26.640
It's almost like microservices for databases. You can collect the things together, but the benefit

00:21:26.640 --> 00:21:33.760
would be, you know, that if you have an operational database that is, it's getting read to and written

00:21:33.760 --> 00:21:40.400
to and read from in real time. When you ask questions about it, it's, it's using that real time data. It's

00:21:40.400 --> 00:21:46.880
not, well, we did an export last night and here's what we know, right? You can just get more live data,

00:21:46.880 --> 00:21:53.360
I guess. Yes. That's definitely a use case for that. You can kind of go either end of that spectrum.

00:21:53.360 --> 00:21:59.360
You can use that as an easy way to import, or you can pull it live, or you can kind of do midway and say

00:21:59.360 --> 00:22:05.040
for any data older than an hour, use what I extracted, but grab the last hour right out of

00:22:05.040 --> 00:22:10.480
the operational database and get the best of both. Oh, wow. Okay. Yeah. That'd be pretty wild, wouldn't it?

00:22:11.120 --> 00:22:19.520
So you say that you all have the simplest or friendliest SQL syntax. What's the story with that?

00:22:19.520 --> 00:22:26.720
Sure. I think a lot of that is around part of the origin story of DuckDB is reaching out to the data

00:22:26.720 --> 00:22:31.680
science community and, and hearing feedback that we've invented this entire other thing called data

00:22:31.680 --> 00:22:37.360
frames because we didn't love databases. Yeah, exactly. They didn't answer the questions we

00:22:37.360 --> 00:22:42.960
wanted or whatever. Yeah. Right. It was, there was a lot of friction and, you know, if you've been,

00:22:42.960 --> 00:22:47.760
you know, folks have installed Postgres and set it up, there's a lot more to it than, you know,

00:22:47.760 --> 00:22:55.520
import pandas as PD. It's, it's just a different level of, of hassle that you have to do. The SQL syntax is no

00:22:55.520 --> 00:23:01.680
different. You know, SQL is an old language, much older than Python. And it's got a lot of very

00:23:01.680 --> 00:23:07.680
interesting behavior that, you know, in, by modern standards is very different. So with DuckDB, we've

00:23:07.680 --> 00:23:13.840
really pushed the envelope in a lot of ways. Nice. Well, you know, many of the things that relational

00:23:13.840 --> 00:23:21.520
databases were built for and when they're designed, this was the seventies and they had different, you know,

00:23:21.520 --> 00:23:28.000
memory was expensive. Disc was expensive. So they made these trade-offs in that world, right?

00:23:28.000 --> 00:23:36.320
Yes. And it's absolutely. There's definitely some aspects of SQL where it's been around a long time.

00:23:36.320 --> 00:23:40.880
So it'll be around forever. But what DuckDB is trying to do is have it not be frozen in time

00:23:40.880 --> 00:23:46.320
forever and really push the whole language forward. So there's a couple of cool ways. The, the syntax

00:23:46.320 --> 00:23:51.280
itself is designed to be both easier to use and also push it into areas that are, are new and

00:23:51.280 --> 00:23:57.200
different for databases. So easier to use, for example, in SQL, if you want to aggregate, you kind

00:23:57.200 --> 00:24:02.800
of typically need two ingredients. You need an aggregation function, like a sum or a max or an average.

00:24:02.800 --> 00:24:08.400
And then you want to group by clause to say, what's the level of detail I want to group at? Do I want to

00:24:08.400 --> 00:24:13.200
group by customer or customer and order? You know, what, what level of detail am I looking at?

00:24:13.200 --> 00:24:20.000
DuckDB can actually infer that level of detail with group by all and just say, whichever columns you're looking at,

00:24:20.000 --> 00:24:24.800
if you're looking at the customer ID and you're getting an average, okay, I'll just aggregate it

00:24:24.800 --> 00:24:29.520
up at customer ID level for you. And so it cuts out a whole clause of SQL. You basically never have to

00:24:29.520 --> 00:24:34.240
worry about the group by clause ever again. Just put the keyword all, you're good to go.

00:24:34.240 --> 00:24:39.680
I love it because the group by clause is hard. It's anytime you have to edit something in two

00:24:39.680 --> 00:24:44.720
places, just you're already in trouble, right? I mean, just, you know, the don't repeat yourself is

00:24:44.720 --> 00:24:50.800
very valuable. Yeah. Yeah. The joins and the group buys and the whole aggregation pipeline stuff,

00:24:50.800 --> 00:24:57.040
which is incredibly powerful, especially in analytical databases like DuckDB, but also hard.

00:24:57.040 --> 00:25:04.000
Yes, absolutely. And I think obviously anything we can do to reduce the friction there is helpful.

00:25:04.000 --> 00:25:09.040
So we are talking about the friendly SQL dialect. I'll throw a teaser in there to say there are ways

00:25:09.040 --> 00:25:14.880
to use DuckDB without SQL as well in very Pythonic ways. So you really can have it both ways if you want.

00:25:14.880 --> 00:25:20.800
Okay. What about ORMs? Is there a concept of a ORM for DuckDB?

00:25:20.800 --> 00:25:27.680
I'd have to check exactly which ones support it. But I know at MotherDuck, we're working on,

00:25:27.680 --> 00:25:35.280
you know, a TypeScript ORM that'll support DuckDB SQLize. I'd have to check which other ones are out

00:25:35.280 --> 00:25:40.160
there that support it. But nothing in principle really stops it. We support the fundamental things

00:25:40.160 --> 00:25:45.120
you need like transactions, like, you know, all the other pieces you need.

00:25:45.120 --> 00:25:54.240
Yeah. Basically, if it runs on pure SQL, then, you know, you should ORMs are mostly about,

00:25:54.240 --> 00:25:58.240
I'm querying against classes, and then I'm going to turn that into SQL for you. And then

00:25:58.240 --> 00:26:02.560
the things that come back, I'm going to turn them into objects, right? Like, I feel like both sides of

00:26:02.560 --> 00:26:08.720
that would be fine. Yes. And it is, you know, it's a relational database. So it's not,

00:26:09.600 --> 00:26:14.720
there are many, it'll look a lot very similar to the ORMs themselves. So it should be,

00:26:14.720 --> 00:26:21.120
should be very compatible. Yeah, it seems, it seems possible. It seems totally possible. All right.

00:26:21.120 --> 00:26:31.280
So maybe let's talk through a little bit of the ways of working with DuckDB through maybe like connecting,

00:26:31.280 --> 00:26:39.040
running some queries, updating. But let me first ask you, what does, what does a schema look like?

00:26:39.040 --> 00:26:46.160
If, for example, if I'm going to work with Postgres or SQLite, I need to run create scripts to create

00:26:46.160 --> 00:26:52.080
the tables and the indices and that kind of stuff before I can even talk to it, generally speaking.

00:26:52.080 --> 00:26:58.480
Is that needed for DuckDB? It doesn't look like it from the examples that I've seen, but how does that work?

00:27:00.160 --> 00:27:04.000
Yeah. And I think in those other databases, there are some ways where you don't necessarily have to do

00:27:04.000 --> 00:27:10.400
that. And DuckDB follows that same approach where it can automatically create tables for you based on

00:27:10.400 --> 00:27:17.360
the query that you pipe into it. So you create a table as the result of another SQL statement, and it will

00:27:17.360 --> 00:27:23.440
just automatically create that table with the data types that you pass through. So one key note,

00:27:23.440 --> 00:27:28.160
if folks are really know a lot about SQLite, one of the defining characteristics of it is that it is very

00:27:28.160 --> 00:27:35.200
much non-typed as much as possible. DuckDB is very much more traditional in that it does have typing.

00:27:35.840 --> 00:27:42.240
It does take full advantage of having nice numeric data types, date times that are true date times,

00:27:42.240 --> 00:27:48.160
not strings. It'll do its best to auto convert for you. But it really does make it very easy. You

00:27:48.160 --> 00:27:55.120
can just create a table as, for example, create table as select star from this parquet file. It can

00:27:55.120 --> 00:27:58.320
really be that easy. It'll just auto create the table based on the parquet file.

00:27:58.320 --> 00:28:03.120
Okay. Yeah, that's really neat. And a lot of those have their columns have data types and stuff,

00:28:03.120 --> 00:28:11.200
right? Yes. And we have also invested quite a lot in our CSV reader as well. So the CSV reader,

00:28:11.200 --> 00:28:19.680
we would consider to be up there with the best on the planet. It auto infers the data types by sniffing

00:28:19.680 --> 00:28:26.080
the CSV to really deduce it. And it handles some seriously wacky CSV files.

00:28:26.080 --> 00:28:31.360
Okay. I can see files. I mean, whatever you're imagining, it's worse.

00:28:31.360 --> 00:28:40.320
It's worse than that. But DuckTV is okay and ready to handle it and making it easy. Because

00:28:40.320 --> 00:28:46.960
it really aligns with the DuckTV ethos, which is a database shouldn't be extra work. It should be

00:28:46.960 --> 00:28:48.320
saving you work. Yeah.

00:28:48.320 --> 00:28:48.320
Yeah.

00:28:48.320 --> 00:28:55.040
Yeah. And most of the time, step one of a database is import your data. And a lot of databases,

00:28:55.040 --> 00:28:58.000
that's not that easy to just import your data. No, it's not.

00:28:58.000 --> 00:29:05.520
Especially if it's not exactly the format of that database, right? It's like, well, okay, I have a .SQL file.

00:29:05.520 --> 00:29:11.280
It's a bunch of update statements. Sure, that's easy to import. But here's my CSV and actually you need to

00:29:11.280 --> 00:29:12.880
transform it or whatever. Yeah.

00:29:12.880 --> 00:29:20.720
Yes. There's a long tail of complexity there, but DuckTV is really trying to handle that and just make that first

00:29:20.720 --> 00:29:24.960
experience using a database just straight out of the box really easy.

00:29:24.960 --> 00:29:31.760
Yeah. So you have some interesting data ingestion. This is a good thing, not a bad thing.

00:29:32.880 --> 00:29:40.080
Data ingestion options. It feels a little bit pandas-like. You have a read CSV, read parquet,

00:29:40.080 --> 00:29:48.160
read JSON. And then it's just, as you sort of hinted there, it's like part of the database now and you

00:29:48.160 --> 00:29:57.680
can ask it questions. Yes. So you can absolutely do that just with the read CSV and it'll import there.

00:29:57.680 --> 00:30:05.360
You can also even just read it without importing. So for example, when we're running that read parquet,

00:30:05.360 --> 00:30:11.440
it's lazily evaluated, much like Polar's or some of the other more modern data frame libraries are,

00:30:11.440 --> 00:30:17.760
where it's building up this relation object that says, once you look at the results, start by reading

00:30:17.760 --> 00:30:22.720
the parquet file and then process the next few steps. You could do something like read parquet, filter to

00:30:22.720 --> 00:30:28.240
these things, aggregate up on these columns, and it will actually do all those operations together.

00:30:28.240 --> 00:30:33.120
It'll optimize them to do them in the right order to pull the least amount of data. And then only at

00:30:33.120 --> 00:30:38.400
that point will it, you know, materialize what it needs to. So the examples that you're looking at on

00:30:38.400 --> 00:30:46.080
screen are, you know, DuckDB.SQL select star from a parquet file. You don't ever even have to read that part, you

00:30:46.080 --> 00:30:50.800
know, write that parquet file into DuckDB format. It's just going to read it and return the results to you

00:30:51.680 --> 00:31:00.000
straight as a pass through as an engine. That's really cool. So if it were a million lines of that,

00:31:00.000 --> 00:31:06.000
I could just iteratively pull through it. Yes, it'll just chunk through it at nice low memory usage and

00:31:06.000 --> 00:31:11.680
just, you know, pass those results out to you. And then if you process them also in batches, you can

00:31:11.680 --> 00:31:17.600
keep your memory, you're super low and take advantage of DuckDB being a streaming engine and not bring it on

00:31:17.600 --> 00:31:23.040
the memory. Right. Well, let's say I want to read a CSV file, but then put it into a persistent table

00:31:23.040 --> 00:31:32.160
in DuckDB. What do I do for this? Yes, you could say, create table as my improved CSV table,

00:31:32.160 --> 00:31:41.920
as select star from path to my CSV file. Will it copy over the schema? It will infer the schema as best as

00:31:41.920 --> 00:31:48.960
it possibly can. And it will automatically define the schema for you. Nice. What's the nullability

00:31:48.960 --> 00:31:55.440
story in your schema? I think, like get do you support nullability or are there things that cannot be

00:31:55.440 --> 00:32:02.000
null? Yes, it's more traditional in the database and that everything can have a null value in the column.

00:32:02.800 --> 00:32:08.480
I think in pandas, that was a little bit complicated by the fact that they were using numpy in some cases,

00:32:08.480 --> 00:32:14.640
but extending beyond numpy and others. But in DuckDB, it's more of a traditional database story in that

00:32:14.640 --> 00:32:20.160
you can have a null value wherever you choose, unless you define a table to say, this column is doesn't

00:32:20.160 --> 00:32:24.960
allow nulls. Right. You can do that if you want. Yeah, I remember that was one of the things that caught

00:32:24.960 --> 00:32:31.280
me out with SQLite. I think it doesn't support, I can't remember which way it went. It's been a while.

00:32:31.280 --> 00:32:36.640
It doesn't support non-nullable columns, like everything is nullable or something like that.

00:32:36.640 --> 00:32:41.680
And it was like, wait, why is this not failing when I put in this wrong data or something along those lines?

00:32:41.680 --> 00:32:49.120
You know, SQLite is incredibly flexible and there's a lot of benefits to it. At times that can mean that

00:32:49.120 --> 00:32:53.680
things pop up later in the life cycle than at the beginning, like you said. So with SQLite, you can,

00:32:53.680 --> 00:32:57.200
you could put a number in a string column and a string column and a number column, you know,

00:32:57.200 --> 00:33:03.360
and it'll, it'll mostly work until it doesn't work. DeckDB will, will, you know, be much more explicit

00:33:03.360 --> 00:33:08.880
about that. But it offers a lot of tools where you, you can auto convert, where it'll auto convert for

00:33:08.880 --> 00:33:13.520
you. So you kind of get the benefits without the cost. And it actually has what's called a union type,

00:33:13.520 --> 00:33:18.400
where you can actually have one column that can have strings and numbers in it. If you know that you're

00:33:18.400 --> 00:33:23.440
going to have a little bit of both. So really, best of all worlds, I'd say.

00:33:23.440 --> 00:33:28.000
Yeah, I think that's more common in the data science data lake space. You got a bunch of

00:33:28.000 --> 00:33:32.560
data, you pull it in, and then you're going to clean it up and work on it. And whereas in

00:33:32.560 --> 00:33:38.960
maybe a relational database powering a web API, like, nope, users always have an email.

00:33:38.960 --> 00:33:40.240
That's just how it is.

00:33:40.240 --> 00:33:42.320
Right. Yes.

00:33:44.080 --> 00:33:48.080
Okay, so I'm looking at some code on your documentation here about getting started with

00:33:48.080 --> 00:33:54.000
Python for pandas. And I need some, I need some help. I need some help.

00:33:54.000 --> 00:33:58.480
It says import DuckDB, import pandas as PD, this is all normal. And then it says pandas

00:33:58.480 --> 00:34:03.280
DF, it's a variable equals PD dot data frames, so creates one. And here's where I need the help.

00:34:03.280 --> 00:34:12.400
The next line is DuckDB dot SQL, select star in the quotes, pandas DF. What, how do those two things

00:34:12.400 --> 00:34:18.960
connect? That's a great question. So you never like take the actual object of the data frame and

00:34:18.960 --> 00:34:22.960
show it to DuckDB in any way, like you don't pass through a function or set of property or nothing.

00:34:22.960 --> 00:34:29.280
Exactly. And this is the benefit of being an in-process database. We are in the same memory space

00:34:29.280 --> 00:34:36.320
as Python, which means we can actually read the Python locals for its local variables. And DuckDB has

00:34:36.320 --> 00:34:42.400
been built from the very beginning with this concept of interchangeability of storage format. That's why you can read

00:34:42.400 --> 00:35:12.340
Parquet files or read CSV files, because we treat them as first class citizens in our, in our database. And data frames are no different. What we'll do first when we see a select statement come in, and it's got a table in there. In this case, it's pandas DF is the name of the table. First thing we'll do is, hey, do we have a physical duckDB table like that? Is there a table of that name? And if we don't find one, we don't give up, we keep looking, we say, okay, is there another object we know how to read with that name? And so we look in Python's local variables, there's a variable named pandas DF, and we check the

00:35:12.340 --> 00:35:23.340
class name, what class name, what class name? What class is this? Is it pandas? Is it polars? Is it Apache arrow? And if it's one of those, we can treat it just like a table, and then run SQL right on top of it.

00:35:23.340 --> 00:35:26.280
Pretty wild.

00:35:26.280 --> 00:35:27.920
It's pretty wild.

00:35:27.920 --> 00:35:41.760
I think one of the real values there is that you don't have to do, presumably, I'm presuming, you don't have to do a data conversion from pandas into whatever in-memory,

00:35:42.280 --> 00:35:44.960
thing duckDB thinks is ideal, and then back.

00:35:44.960 --> 00:35:59.540
That's a great question. So we, you definitely don't have to do it all at once. So pandas, sometimes it is Python objects under the hood, whereas duckDB is, you know, written in C++, so it will change some things, but it'll do it in chunks, like we're talking about, so it won't be, you know, all up front.

00:36:00.620 --> 00:36:18.760
But for things like polars and arrow, in most data types, it's zero copy, where it's the same format. DuckDB has slightly different data formats, and Apache arrow is actually adjusted to align closer to duckDB in some of their format, like how they store strings.

00:36:19.300 --> 00:36:27.520
So duckDB has really been one of the leading databases in terms of how it handles strings, and Apache arrow is also making those changes.

00:36:28.140 --> 00:36:31.640
Yeah, that's super cool. You can't write back to these, can you?

00:36:31.640 --> 00:36:42.300
You can't say, wait, you can, you can say update, like, the data frame variable, PD data frame, set, whatever, yeah?

00:36:43.140 --> 00:36:52.420
So you, if you're updating the data frame, you will overwrite the whole data frame. But a duckDB table, you can run updates on that object, for sure.

00:36:52.420 --> 00:36:53.080
Yeah.

00:36:53.080 --> 00:36:57.320
But you can write, you know, data frames, for sure. So you can replace the whole thing.

00:36:57.320 --> 00:37:04.380
Right, but I couldn't change the 172nd row of that data frame with a SQL query, or could I?

00:37:06.240 --> 00:37:09.380
You would just be making a copy of the object.

00:37:09.380 --> 00:37:15.280
Got it, got it. Yeah, yeah, that's what I thought. You would make the changes, but then you would return as data frame and make a copy of it.

00:37:15.280 --> 00:37:23.320
Correct. We would return a different object in memory, but functionally, it would be the same, and duckDB is fast enough where it'd probably be quite performant still.

00:37:23.320 --> 00:37:29.000
Yeah, probably fine, yeah. Okay, I'm not saying that it should be able to do, I'm just trying to figure out how far down this.

00:37:29.000 --> 00:37:30.600
How far down, yeah, yeah.

00:37:32.020 --> 00:37:40.040
How far down does it go? Yeah, so it does the same kind of magic with the locals, with Pyro and pullers and so on.

00:37:40.040 --> 00:37:55.580
So there's different ways to get data back, and it doesn't really matter where it came from, if it came from pandas or duckDB files itself, or even just hard-coded SQL.

00:37:56.580 --> 00:38:04.900
But you can say things like fetch all, which will return you Python objects, or .df will return you a pandas data frame, or different things, right?

00:38:04.900 --> 00:38:11.500
Yes, yep, we've got .pl for polars, .arrow. You could do NumPy arrays if you'd like.

00:38:11.500 --> 00:38:20.360
We also support outputting to TensorFlow tensors and PyTorch tensors, because those are NumPy under the hood in a lot of ways.

00:38:20.860 --> 00:38:22.580
So we can output to that as well.

00:38:22.580 --> 00:38:24.800
Okay, that's super cool.

00:38:24.800 --> 00:38:30.700
Do I need to be worried about little Bobby tables, SQL injections?

00:38:32.420 --> 00:38:36.480
We could definitely pass in parameters that will get escaped properly and all that.

00:38:36.480 --> 00:38:44.320
With DuckDB, I think that, in general, we take the approach of being super flexible, but there's a lot of options to lock it down, should you choose.

00:38:44.320 --> 00:38:51.580
So the select star from any CSV file on my hard drive, you could turn that off if you want, for example.

00:38:51.580 --> 00:38:54.480
Likewise, with the data frames, you can disable that also.

00:38:55.060 --> 00:39:00.840
Yeah, I guess what I'm asking more, is there a concept of a parameterized query, or something like that?

00:39:00.840 --> 00:39:06.580
Yes, yes. So you can follow the same Python DV 2.0 kind of API spec.

00:39:06.580 --> 00:39:12.920
You could do .execute, and pass in a query with placeholder parameters, and then pass in those as arguments.

00:39:12.920 --> 00:39:14.300
So you can absolutely do that.

00:39:14.300 --> 00:39:18.360
It tends to be not good for bulk ingestion.

00:39:18.360 --> 00:39:21.780
The reason for that is Python objects just have a lot more...

00:39:21.780 --> 00:39:38.160
Uh-oh.

00:39:38.160 --> 00:39:48.420
Uh-oh, Alex, I lost your audio.

00:39:48.420 --> 00:39:51.500
Did you possibly hit something that hit a mute button?

00:39:51.500 --> 00:39:53.340
I did. Sorry. I'm back.

00:39:53.340 --> 00:39:55.500
Okay. Sorry. Give us the...

00:39:55.500 --> 00:39:59.760
You said it's not good for bulk ingestion. Let's go from there.

00:39:59.760 --> 00:40:00.860
Yep. Yep. Sorry.

00:40:00.860 --> 00:40:03.060
Oh, all good. That's on me.

00:40:03.060 --> 00:40:08.180
So you can insert straight from Python objects with parameters.

00:40:08.180 --> 00:40:12.360
It works really great. It's very convenient, but it's not great for bulk ingestion.

00:40:12.360 --> 00:40:17.120
Inserting a million rows that way versus a million rows in a panache data frame,

00:40:17.280 --> 00:40:20.860
it's going to be a couple orders of magnitude faster to do it through a data frame.

00:40:20.860 --> 00:40:22.660
Yeah. Yeah, absolutely.

00:40:22.660 --> 00:40:25.540
So if...

00:40:25.540 --> 00:40:30.100
By default, if I just do WDB.SQL,

00:40:30.100 --> 00:40:32.120
this is working in this...

00:40:33.120 --> 00:40:33.940
correctly this time,

00:40:33.940 --> 00:40:35.200
in-memory database.

00:40:35.200 --> 00:40:37.240
Oh, yeah.

00:40:37.240 --> 00:40:41.000
But if you create a connection to a file,

00:40:41.000 --> 00:40:42.140
then that...

00:40:42.140 --> 00:40:44.080
where you just call it whatever you want, right?

00:40:44.080 --> 00:40:45.660
Like in your examples, you got file.db.

00:40:46.340 --> 00:40:50.040
That will then start persisting to that file, right?

00:40:50.040 --> 00:40:52.280
But it's pretty simple to do either.

00:40:52.280 --> 00:40:53.820
Yes.

00:40:53.820 --> 00:40:55.740
You're just changing the connection path.

00:40:55.740 --> 00:40:59.840
And if you pass in no path, it'll be in memory.

00:40:59.840 --> 00:41:01.320
Otherwise, you can point to a file.

00:41:01.320 --> 00:41:04.400
And it's really that straightforward.

00:41:04.400 --> 00:41:06.680
It'll save all your tables into that same file.

00:41:06.680 --> 00:41:08.840
Nice and compressed in columnar format.

00:41:09.380 --> 00:41:11.560
And then now the format is all stabilized.

00:41:11.560 --> 00:41:13.580
You can throw that on an object store,

00:41:13.580 --> 00:41:14.440
share it with anybody,

00:41:14.440 --> 00:41:15.440
put it in your data lake,

00:41:15.440 --> 00:41:16.560
anywhere you want it to go.

00:41:16.560 --> 00:41:19.540
Can I read from objects or can I give it an S3 connection?

00:41:19.540 --> 00:41:21.020
Yes, indeed.

00:41:21.020 --> 00:41:23.240
You can read from any of the major clouds.

00:41:23.240 --> 00:41:26.160
So AWS, Google, Azure.

00:41:26.160 --> 00:41:28.620
You can read individual files

00:41:28.620 --> 00:41:31.140
or actually it'll understand hierarchies of files.

00:41:31.140 --> 00:41:33.820
So if you have like a top-level folder for a year

00:41:33.820 --> 00:41:35.660
and a folder for month and day,

00:41:35.660 --> 00:41:37.300
if you filter for,

00:41:37.300 --> 00:41:38.800
I only want three days worth of data,

00:41:39.340 --> 00:41:40.980
DuckDB will only read three files.

00:41:40.980 --> 00:41:41.780
So it's pretty slick.

00:41:41.780 --> 00:41:42.460
Oh, wow. That's wild.

00:41:42.460 --> 00:41:43.000
Okay.

00:41:43.000 --> 00:41:45.060
Yeah, that is really slick.

00:41:45.060 --> 00:41:48.060
And you can even write out to those as well.

00:41:48.060 --> 00:41:50.440
So you can actually write on the object stores too.

00:41:50.440 --> 00:41:52.220
That's amazing, actually.

00:41:52.220 --> 00:41:57.600
And a lot of less hyperscale clouds,

00:41:57.600 --> 00:41:58.040
I don't know.

00:41:58.040 --> 00:41:59.400
I was almost calling them lesser clouds,

00:41:59.400 --> 00:42:01.560
but I think of almost as like even better clouds.

00:42:01.560 --> 00:42:05.760
You know, DigitalOcean, Hetzner, Linode,

00:42:05.760 --> 00:42:07.500
some of these smaller places,

00:42:08.520 --> 00:42:10.560
a lot of them have their own object store

00:42:10.560 --> 00:42:14.240
and the objects store API

00:42:14.240 --> 00:42:16.320
is just a different connection string

00:42:16.320 --> 00:42:19.760
for the S3 API, right?

00:42:19.760 --> 00:42:23.000
So it sounds like you could probably get that to work as well.

00:42:23.000 --> 00:42:23.960
Let's pass that.

00:42:24.580 --> 00:42:26.020
I believe I've seen MinIO used,

00:42:26.020 --> 00:42:27.940
which is even like self-hosted if you want.

00:42:27.940 --> 00:42:29.360
So if it handles MinIO,

00:42:29.360 --> 00:42:30.880
you can kind of put it wherever you want, I think.

00:42:30.880 --> 00:42:31.940
Yeah, yeah, that's for sure.

00:42:31.940 --> 00:42:33.240
MinIO is awesome.

00:42:33.240 --> 00:42:35.960
I haven't got a chance to put MinIO

00:42:35.960 --> 00:42:39.960
into any useful work,

00:42:40.100 --> 00:42:44.480
but it's like a self-hosted S3, basically,

00:42:44.480 --> 00:42:46.060
which is really cool.

00:42:46.060 --> 00:42:48.180
Yeah, so I don't know.

00:42:48.180 --> 00:42:51.340
It seems like a really neat system.

00:42:51.340 --> 00:42:53.440
But it's also MinIO is complicated.

00:42:53.440 --> 00:42:58.260
You know, it's not easy to set up and run.

00:42:58.260 --> 00:43:00.220
It seems like there's a lot going on there.

00:43:00.220 --> 00:43:03.340
So that's why I haven't done anything with it yet.

00:43:03.340 --> 00:43:04.200
I'm like, this looks amazing.

00:43:04.200 --> 00:43:05.220
Oh, this looks complicated.

00:43:05.380 --> 00:43:06.360
What else am I going to do today?

00:43:06.360 --> 00:43:09.860
I think S3 is pretty great.

00:43:09.860 --> 00:43:11.900
Drop a file, read a file.

00:43:11.900 --> 00:43:14.960
So it's not a bad starting point for quite a while.

00:43:14.960 --> 00:43:16.360
Yeah, absolutely.

00:43:16.360 --> 00:43:18.500
So with this connection object,

00:43:18.500 --> 00:43:21.220
like if you want to have a persistent database, right,

00:43:21.220 --> 00:43:22.680
you create a connection object,

00:43:22.680 --> 00:43:27.560
and then it has basically the same API as the DuckDB itself.

00:43:27.560 --> 00:43:28.760
So you can run SQL queries,

00:43:28.760 --> 00:43:30.320
you can do table stuff on it, and so on.

00:43:33.080 --> 00:43:36.120
What's the concurrency story with this thing?

00:43:36.120 --> 00:43:42.460
So, for example, what if I try to use DuckDB in Flask,

00:43:42.460 --> 00:43:45.820
and I have Flask running in G Unicorn,

00:43:45.820 --> 00:43:49.400
and I say, G Unicorn, please use four worker processes,

00:43:49.400 --> 00:43:50.640
and I try to talk to it?

00:43:50.640 --> 00:43:52.800
It's a great question.

00:43:52.800 --> 00:43:54.400
Am I going to be having a bad time,

00:43:54.400 --> 00:43:55.380
or am I having a good time?

00:43:55.380 --> 00:43:57.500
It depends a little bit.

00:43:57.500 --> 00:44:00.500
But yes, there's some trade-offs.

00:44:00.500 --> 00:44:03.180
There's reasons that there are databases that are in process

00:44:03.180 --> 00:44:05.160
and that there are databases that are client-server.

00:44:05.160 --> 00:44:09.260
And so whenever you want to write data to DuckDB,

00:44:09.260 --> 00:44:12.560
you can only write data from within a single process.

00:44:12.560 --> 00:44:15.120
And you'll open up a read-write connection,

00:44:15.120 --> 00:44:18.860
and it will actually disallow any access from any other process,

00:44:18.860 --> 00:44:20.820
and it'll be single-process access

00:44:20.820 --> 00:44:22.280
while you're doing reads and writes.

00:44:22.280 --> 00:44:24.520
It does handle multi-threading just fine,

00:44:24.520 --> 00:44:26.340
so you can have multiple Python threads.

00:44:26.340 --> 00:44:29.680
So you get nice parallel performance that way, no problem.

00:44:29.680 --> 00:44:31.480
But it's all within the same memory space

00:44:31.480 --> 00:44:32.460
because you're using threads.

00:44:32.460 --> 00:44:35.500
So we're pretty excited about free threading coming soon in Python.

00:44:35.500 --> 00:44:35.960
Yes.

00:44:35.960 --> 00:44:42.480
Yeah, well, that's going to knock out the need for the web garden,

00:44:42.480 --> 00:44:44.080
not the web farm, the web garden,

00:44:44.080 --> 00:44:48.440
where you've got a bunch of these copies of the web server running in parallel

00:44:48.440 --> 00:44:52.360
because they're all trying to have separate gills

00:44:52.360 --> 00:44:53.540
so they can be more concurrent.

00:44:53.540 --> 00:44:56.700
That's not the only reason, but that's a big reason in Python.

00:44:56.700 --> 00:44:58.040
That's definitely a big reason.

00:44:58.040 --> 00:44:59.280
So that we're excited about.

00:44:59.280 --> 00:45:01.920
If you want to be read-only,

00:45:01.920 --> 00:45:04.500
so since DuckDB is an analytical database,

00:45:04.500 --> 00:45:05.920
and since we can read from files,

00:45:05.920 --> 00:45:08.580
there are a lot of use cases that are read-only,

00:45:08.580 --> 00:45:11.440
where if I'm reading from parquet files on an object store,

00:45:12.120 --> 00:45:14.440
I might not be doing any right operations at all,

00:45:14.440 --> 00:45:18.360
at which point multi-process access is totally fine

00:45:18.360 --> 00:45:21.420
as long as you're in read-only mode for all of those processes.

00:45:21.420 --> 00:45:25.140
I will add one big other thing to look into, though,

00:45:25.140 --> 00:45:27.180
is where I work, which is Mother Duck.

00:45:27.180 --> 00:45:30.800
So we're a cloud serverless data warehouse built around DuckDB,

00:45:30.800 --> 00:45:33.100
and we'll handle the concurrency side.

00:45:33.100 --> 00:45:37.120
So it's really taking this amazing single-player DuckDB experience

00:45:37.120 --> 00:45:38.780
and bringing it into the cloud

00:45:38.780 --> 00:45:41.680
as a full multiplayer cloud data warehouse experience

00:45:41.680 --> 00:45:43.300
so you get concurrency,

00:45:43.300 --> 00:45:46.200
you get automatic managed storage.

00:45:46.200 --> 00:45:47.020
It's very efficient,

00:45:47.020 --> 00:45:49.340
and you also get access control

00:45:49.340 --> 00:45:52.680
and some of the things you associate with a large-scale database.

00:45:52.680 --> 00:45:55.580
So that's the story.

00:45:55.580 --> 00:45:57.380
If you need concurrency but you're using DuckDB,

00:45:57.380 --> 00:45:59.720
I think Mother Duck is a great option.

00:45:59.720 --> 00:46:01.120
Right.

00:46:01.120 --> 00:46:04.500
So this is kind of DuckDB as a service.

00:46:04.500 --> 00:46:05.320
Is that right?

00:46:05.320 --> 00:46:08.920
That's definitely a key piece of it.

00:46:08.920 --> 00:46:13.400
I think DuckDB is a huge part of what we do,

00:46:13.400 --> 00:46:15.720
and it is the engine under the hood.

00:46:15.720 --> 00:46:21.320
I think we are using that to really be meeting the use case

00:46:21.320 --> 00:46:26.500
of being not just sort of a cloud hosting for DuckDB.

00:46:26.500 --> 00:46:30.180
It's to really be a full data warehouse with DuckDB as the engine.

00:46:30.420 --> 00:46:31.740
So a little bit of a distinction there,

00:46:31.740 --> 00:46:35.320
but just as an organization could kind of use it

00:46:35.320 --> 00:46:37.740
as opposed to just one person cloud hosting it,

00:46:37.740 --> 00:46:38.260
something like that.

00:46:38.260 --> 00:46:39.760
Yeah.

00:46:39.760 --> 00:46:42.220
Well, it seems like a really, really great service.

00:46:42.220 --> 00:46:46.320
And I guess it's worth calling out that DuckDB,

00:46:47.420 --> 00:46:51.520
the thing that runs in your process is available on,

00:46:51.520 --> 00:46:54.560
as an MIT licensed thing, right?

00:46:54.560 --> 00:46:56.980
Which is, that's pretty amazing.

00:46:56.980 --> 00:46:59.340
And you, I saw somewhere, I can't remember where I saw it,

00:46:59.340 --> 00:47:01.360
but it's something like you'll promise to be MIT,

00:47:01.360 --> 00:47:03.660
to stay MIT.

00:47:03.660 --> 00:47:05.260
No rug pulling.

00:47:05.260 --> 00:47:06.860
The rugs will stay.

00:47:07.860 --> 00:47:08.140
Yes.

00:47:08.140 --> 00:47:11.500
So it's a pretty cool kind of corporate structure.

00:47:11.500 --> 00:47:13.220
And if you can say a cool corporate structure,

00:47:13.220 --> 00:47:14.920
we'll see if we can get away with that.

00:47:14.920 --> 00:47:17.560
But as far as corporate structures go, it's pretty cool, right?

00:47:17.560 --> 00:47:20.160
So there's actually three organizations at play.

00:47:20.160 --> 00:47:23.060
And one of them is a nonprofit foundation

00:47:23.060 --> 00:47:24.440
called the DuckDB Foundation.

00:47:24.440 --> 00:47:27.520
And they actually own the intellectual property for DuckDB.

00:47:27.520 --> 00:47:30.700
And that is what ensures that it'll be MIT licensed forever

00:47:30.700 --> 00:47:33.940
because it is not a company that owns DuckDB,

00:47:33.940 --> 00:47:35.540
which is amazing.

00:47:35.740 --> 00:47:38.520
So there is, like, it's not even possible to do a rug pull,

00:47:38.520 --> 00:47:40.360
even if somebody wanted, which nobody does, right?

00:47:40.360 --> 00:47:41.540
It's not even possible.

00:47:41.540 --> 00:47:43.280
It's against the law, which is pretty cool.

00:47:43.280 --> 00:47:44.340
So, yeah.

00:47:44.340 --> 00:47:47.860
I guess maybe give people, I threw that out there.

00:47:47.860 --> 00:47:49.500
There might be people who don't know what that means.

00:47:49.500 --> 00:47:52.200
Oh, what a rug pull is?

00:47:52.200 --> 00:47:53.120
Yes, exactly.

00:47:53.120 --> 00:47:53.880
Yeah, what is that?

00:47:53.880 --> 00:47:56.680
You know, certain products, once they're open source,

00:47:56.680 --> 00:48:00.860
they begin to, they get funded in certain ways

00:48:00.860 --> 00:48:03.760
to where they need to be, you know,

00:48:03.980 --> 00:48:05.740
selling it as a product.

00:48:05.740 --> 00:48:08.500
And at times that means that either they don't always

00:48:08.500 --> 00:48:10.140
put everything back in open source

00:48:10.140 --> 00:48:13.860
or there's different licensing models that get changed

00:48:13.860 --> 00:48:16.180
so that the license is a little bit more restrictive

00:48:16.180 --> 00:48:17.000
than it was in the past.

00:48:17.000 --> 00:48:19.420
Kind of the typical maneuver there

00:48:19.420 --> 00:48:21.160
for a couple of different companies has been around.

00:48:21.160 --> 00:48:24.620
Amazon will just host your product

00:48:24.620 --> 00:48:27.200
and then it's very hard to have your own business

00:48:27.200 --> 00:48:30.020
if Amazon just hosts it.

00:48:30.020 --> 00:48:33.140
So we're not in that situation with DuckDB

00:48:33.140 --> 00:48:35.460
because it's open source forever.

00:48:35.460 --> 00:48:39.220
But also Mother Duck is not just wrapping DuckDB.

00:48:39.220 --> 00:48:42.980
We're innovating at that service layer

00:48:42.980 --> 00:48:45.500
where with Mother Duck,

00:48:45.500 --> 00:48:47.320
one of its secret sauce ingredients

00:48:47.320 --> 00:48:49.200
is it's not just a cloud service.

00:48:49.780 --> 00:48:51.100
When you connect to Mother Duck,

00:48:51.100 --> 00:48:53.680
you have DuckDB running locally and in the cloud.

00:48:53.680 --> 00:48:56.420
And one query, one Python statement

00:48:56.420 --> 00:48:58.140
can actually run partially in the cloud

00:48:58.140 --> 00:48:59.500
and partially on your laptop.

00:48:59.500 --> 00:49:01.400
And we actually can,

00:49:01.400 --> 00:49:04.820
we can choose the optimal location for that to run

00:49:04.820 --> 00:49:06.280
based on your query plan.

00:49:06.280 --> 00:49:07.880
We actually do that as a part of the optimization

00:49:07.880 --> 00:49:09.900
to say, where's the most efficient place

00:49:09.900 --> 00:49:10.500
for this to run?

00:49:11.360 --> 00:49:13.460
And that's some serious secret sauce

00:49:13.460 --> 00:49:14.280
that, you know,

00:49:14.280 --> 00:49:15.360
it's not just wrapping,

00:49:15.360 --> 00:49:17.700
you know, an open source solution and hosting it.

00:49:17.700 --> 00:49:19.820
That gives some really nice benefits

00:49:19.820 --> 00:49:22.140
for things like the local development experience.

00:49:22.140 --> 00:49:25.560
You can develop locally at light speed

00:49:25.560 --> 00:49:27.980
and then push to the cloud at the final step.

00:49:27.980 --> 00:49:30.860
Or alternatively, kind of reverse the polarity.

00:49:30.860 --> 00:49:31.920
Maybe your first step

00:49:31.920 --> 00:49:33.960
is to bring everything down locally in a cache

00:49:33.960 --> 00:49:36.860
and then do all kinds of data science processing on it.

00:49:36.860 --> 00:49:41.500
So it's very convenient to have both local and cloud.

00:49:41.500 --> 00:49:43.040
I see.

00:49:43.040 --> 00:49:46.220
Kind of like we talked about writing a query

00:49:46.220 --> 00:49:49.000
and then turn it into a data science object

00:49:49.000 --> 00:49:50.140
and then asking questions.

00:49:50.140 --> 00:49:54.520
This is a little bit like distributed versus local,

00:49:54.520 --> 00:49:56.760
but staying within DuckDB.

00:49:56.760 --> 00:49:59.220
It's definitely,

00:49:59.220 --> 00:50:00.900
you get to use server-side compute

00:50:00.900 --> 00:50:02.400
or local compute, however you'd like.

00:50:02.400 --> 00:50:03.800
Yeah.

00:50:06.080 --> 00:50:06.820
Super cool.

00:50:06.820 --> 00:50:08.000
I love the idea of it.

00:50:08.000 --> 00:50:10.360
It's a lot of fun.

00:50:10.360 --> 00:50:11.900
We're doing a lot of fun things

00:50:11.900 --> 00:50:14.600
and you got to have fun with it as well.

00:50:14.600 --> 00:50:15.060
You know,

00:50:15.060 --> 00:50:17.060
what do you call someone who works at Mother Duck?

00:50:17.060 --> 00:50:19.080
A duckling.

00:50:19.080 --> 00:50:21.820
That's the second most popular one.

00:50:21.820 --> 00:50:22.260
We actually,

00:50:22.260 --> 00:50:23.780
we're Mother Duckers, Michael.

00:50:23.780 --> 00:50:25.740
Oh, okay.

00:50:25.740 --> 00:50:26.500
Mother Duckers.

00:50:26.500 --> 00:50:26.860
All right.

00:50:26.860 --> 00:50:28.600
You got to have fun with it, right?

00:50:28.600 --> 00:50:28.860
Otherwise,

00:50:28.860 --> 00:50:30.760
it's all just ones and zeros at the end of the day, right?

00:50:30.760 --> 00:50:31.940
You have to have fun with it.

00:50:31.940 --> 00:50:32.220
Yeah.

00:50:32.220 --> 00:50:33.260
You have to have fun with it.

00:50:33.260 --> 00:50:33.940
You should.

00:50:35.520 --> 00:50:36.140
I don't know.

00:50:36.140 --> 00:50:37.440
Ducklings.

00:50:37.440 --> 00:50:38.840
But Mother Duckers is good still.

00:50:38.840 --> 00:50:41.100
I guess ducklings might send the wrong message.

00:50:41.100 --> 00:50:42.280
They're all good.

00:50:42.280 --> 00:50:44.840
Do you want to,

00:50:44.840 --> 00:50:46.780
do you want to like sort of tough,

00:50:46.780 --> 00:50:50.660
tough persona or facade,

00:50:50.660 --> 00:50:51.500
or do you want to like,

00:50:51.500 --> 00:50:52.320
you know,

00:50:52.320 --> 00:50:52.820
coddled,

00:50:52.820 --> 00:50:53.200
whatever?

00:50:53.200 --> 00:50:53.800
No,

00:50:53.800 --> 00:50:54.440
it's a great name.

00:50:56.040 --> 00:50:57.020
It's like,

00:50:57.020 --> 00:50:59.260
if you look at the branding on our website,

00:50:59.260 --> 00:50:59.520
you know,

00:50:59.520 --> 00:51:00.380
we're not going for like,

00:51:00.380 --> 00:51:02.820
we're not like a super muscly duck,

00:51:02.820 --> 00:51:03.040
you know,

00:51:03.040 --> 00:51:03.780
in our element ridges.

00:51:03.780 --> 00:51:05.220
We're definitely in the playful side.

00:51:05.220 --> 00:51:07.520
And part of this,

00:51:07.520 --> 00:51:11.120
because we really think that back in the big data era,

00:51:11.120 --> 00:51:14.360
we believe the big data era is over here at Mother Duck.

00:51:14.480 --> 00:51:19.380
And the reason for that is you no longer need 50 servers to answer your questions.

00:51:19.380 --> 00:51:22.260
It turns out laptops are pretty daggone fast now.

00:51:22.260 --> 00:51:22.640
Yeah.

00:51:22.640 --> 00:51:23.920
And you can do a lot.

00:51:24.400 --> 00:51:24.800
Even,

00:51:24.800 --> 00:51:26.640
even if you're not able to handle it on a laptop,

00:51:26.640 --> 00:51:28.600
how about one giant node on AWS?

00:51:28.600 --> 00:51:29.460
Yeah.

00:51:29.460 --> 00:51:33.080
You can rent a node with multiple terabytes of RAM.

00:51:33.080 --> 00:51:34.580
Of RAM.

00:51:34.580 --> 00:51:35.160
I mean,

00:51:35.160 --> 00:51:36.160
you could do a lot,

00:51:36.160 --> 00:51:36.580
right?

00:51:36.580 --> 00:51:37.640
So at that point,

00:51:37.640 --> 00:51:39.480
what is a single node?

00:51:39.480 --> 00:51:40.480
You can kind of do,

00:51:40.480 --> 00:51:42.000
do almost anything that you need.

00:51:42.000 --> 00:51:42.240
So,

00:51:42.240 --> 00:51:44.220
and as a result,

00:51:44.220 --> 00:51:48.040
and single server versus distributed is way easier.

00:51:48.040 --> 00:51:49.180
Yes.

00:51:49.180 --> 00:51:50.760
It allows you to innovate a lot faster.

00:51:50.760 --> 00:51:52.080
You can use better algorithms.

00:51:52.080 --> 00:51:53.100
So in many cases,

00:51:53.100 --> 00:51:55.820
it's actually just plain old faster to do it that way.

00:51:55.820 --> 00:51:57.760
A lot of benefits,

00:51:57.760 --> 00:52:00.140
but what we realized fundamentally at Mother Duck is,

00:52:00.140 --> 00:52:01.300
in a lot of ways,

00:52:01.300 --> 00:52:02.140
DuckDB as well.

00:52:02.140 --> 00:52:06.800
The scale of data shouldn't be the most important thing anymore.

00:52:06.800 --> 00:52:09.600
It should be how easy it is to use.

00:52:09.600 --> 00:52:12.800
You should be able to do select star from a CSV.

00:52:12.800 --> 00:52:13.800
You know,

00:52:13.800 --> 00:52:17.400
it doesn't take scale to parse a CSV file in an intelligent way.

00:52:17.400 --> 00:52:18.780
It takes elbow grease.

00:52:18.780 --> 00:52:21.180
It takes innovation in terms of the algorithm,

00:52:21.440 --> 00:52:22.920
but at the end of the day,

00:52:22.920 --> 00:52:24.840
it's not about how big is your CSV file.

00:52:24.840 --> 00:52:26.780
It's about how long did it take me to get my job done.

00:52:26.780 --> 00:52:27.660
Yeah.

00:52:27.660 --> 00:52:28.140
And Mother Duck,

00:52:28.140 --> 00:52:29.740
it's a similar kind of pragmatism.

00:52:29.740 --> 00:52:30.640
And our branding,

00:52:30.640 --> 00:52:32.320
I think does a really good job of showing that.

00:52:32.320 --> 00:52:34.060
I think so.

00:52:34.060 --> 00:52:34.660
Well,

00:52:34.660 --> 00:52:39.380
I asked you about half of what I was said,

00:52:39.380 --> 00:52:40.480
I was looking forward to asking about,

00:52:40.480 --> 00:52:41.760
which was the table schemas,

00:52:41.760 --> 00:52:42.700
how to do that.

00:52:43.320 --> 00:52:43.720
However,

00:52:43.720 --> 00:52:46.620
what about indexes?

00:52:46.620 --> 00:52:49.100
Sure.

00:52:49.100 --> 00:52:51.360
I think indexes are a great question.

00:52:51.360 --> 00:52:54.140
So DuckDB does support indexes,

00:52:54.140 --> 00:52:55.380
but in many cases,

00:52:55.380 --> 00:52:57.480
it's not necessarily something that you need.

00:52:57.860 --> 00:53:02.200
And the reason for that is because of the workloads that it's best for,

00:53:02.200 --> 00:53:04.020
but also because of columnar databases.

00:53:04.020 --> 00:53:06.580
So when you have a columnar database,

00:53:06.580 --> 00:53:08.360
by default,

00:53:08.360 --> 00:53:12.520
it's going to create some rough indexes for you automatically.

00:53:12.980 --> 00:53:15.460
So every about a hundred thousand rows in DuckDB,

00:53:15.460 --> 00:53:17.680
we create an index there that says,

00:53:17.680 --> 00:53:19.460
what's the minimum value in this chunk?

00:53:19.460 --> 00:53:21.640
And then the maximum value in this chunk.

00:53:21.640 --> 00:53:25.740
And what we'll do is then if you run a query that says select data from this

00:53:25.740 --> 00:53:26.020
week,

00:53:26.020 --> 00:53:27.520
we're going to check,

00:53:27.520 --> 00:53:27.820
Hey,

00:53:27.820 --> 00:53:31.120
which blocks have any data from this week?

00:53:31.120 --> 00:53:32.240
And we can skip,

00:53:32.240 --> 00:53:32.480
you know,

00:53:32.480 --> 00:53:34.740
90% of your data because it's,

00:53:34.740 --> 00:53:37.140
we know that it doesn't match that filter.

00:53:37.140 --> 00:53:38.060
Right.

00:53:38.060 --> 00:53:40.340
But that's not like an individual row index.

00:53:40.340 --> 00:53:42.960
It's an index for every hundred thousand rows.

00:53:43.620 --> 00:53:46.640
And it's for the analytical queries where you're looking at trends,

00:53:46.640 --> 00:53:48.620
where you're analyzing large amounts of data,

00:53:48.620 --> 00:53:51.540
it tends to be really the optimal way to go about it.

00:53:51.540 --> 00:53:53.380
And that's the way most systems do it.

00:53:53.380 --> 00:53:55.800
There is an external index you can add.

00:53:55.800 --> 00:53:59.900
It's an art index and adaptive radix try index,

00:53:59.900 --> 00:54:01.700
which is a pretty cool way of doing it.

00:54:01.700 --> 00:54:03.580
It's a little different than the B tree in SQLite,

00:54:03.580 --> 00:54:08.100
but it allows for those vast point lookups where if you want to grab one row,

00:54:08.100 --> 00:54:10.940
you can get a very quick mapping to that individual row.

00:54:12.280 --> 00:54:15.400
It tends to be just less necessary inductive B to do that.

00:54:15.400 --> 00:54:17.520
But we have it if you need it.

00:54:17.520 --> 00:54:23.360
I'm realizing that I need to learn more about databases,

00:54:23.360 --> 00:54:25.020
especially columnar ones,

00:54:25.020 --> 00:54:28.520
because they're not matching my mental model as,

00:54:28.520 --> 00:54:29.400
as much as for,

00:54:29.400 --> 00:54:29.600
say,

00:54:29.600 --> 00:54:31.040
for relational or document ones.

00:54:32.020 --> 00:54:35.540
it's definitely a different world.

00:54:35.540 --> 00:54:36.500
And that's,

00:54:36.500 --> 00:54:36.760
I think,

00:54:36.760 --> 00:54:43.400
the fun of it is I think of databases as an infinitely deep rabbit hole of really just fun optimizations,

00:54:43.400 --> 00:54:46.480
fun things to learn about.

00:54:46.480 --> 00:54:47.800
So a lot of exciting stuff.

00:54:48.180 --> 00:54:49.200
when you get that stuff right,

00:54:49.200 --> 00:54:49.880
it's like magic.

00:54:49.880 --> 00:54:50.620
It's like,

00:54:50.620 --> 00:54:55.060
how could it possibly answer that so quickly?

00:54:55.060 --> 00:54:56.840
At the same time,

00:54:56.840 --> 00:54:57.360
when it's wrong,

00:54:57.360 --> 00:54:58.240
it's really frustrating.

00:54:59.240 --> 00:55:01.040
you go to a website and you'll say,

00:55:01.040 --> 00:55:01.840
oh,

00:55:01.840 --> 00:55:02.280
you know,

00:55:02.280 --> 00:55:04.460
click this thing here.

00:55:04.460 --> 00:55:05.480
And then you try to,

00:55:05.480 --> 00:55:07.120
and it just sits there and spins like,

00:55:07.120 --> 00:55:08.640
I know this has happened without an index.

00:55:08.640 --> 00:55:11.340
And I know this could be so easily fixed.

00:55:11.340 --> 00:55:12.220
And it's just not.

00:55:12.220 --> 00:55:12.520
Why,

00:55:12.520 --> 00:55:13.640
why is it so bad?

00:55:13.640 --> 00:55:13.920
But,

00:55:13.920 --> 00:55:14.360
you know,

00:55:14.540 --> 00:55:16.640
you can make your stuff fast.

00:55:16.640 --> 00:55:18.000
You can't make other people's stuff fast.

00:55:18.000 --> 00:55:19.340
So I guess it's an advantage.

00:55:19.340 --> 00:55:20.060
It makes you look good.

00:55:20.060 --> 00:55:21.300
Yes.

00:55:21.300 --> 00:55:25.400
So I think that the workloads are just so different,

00:55:25.400 --> 00:55:26.540
but the mental model,

00:55:26.540 --> 00:55:27.440
it does open,

00:55:27.440 --> 00:55:28.920
open doors in a lot of ways.

00:55:28.920 --> 00:55:29.560
So for example,

00:55:29.560 --> 00:55:30.120
if you want to,

00:55:30.120 --> 00:55:33.400
if you want to get the average of like a billion rows at DuckDB,

00:55:33.400 --> 00:55:35.160
it's a couple seconds.

00:55:35.160 --> 00:55:36.380
Wow.

00:55:36.380 --> 00:55:37.560
It's that fast,

00:55:37.560 --> 00:55:38.080
you know?

00:55:38.080 --> 00:55:41.980
So just the things that you can do with that kind of tool,

00:55:41.980 --> 00:55:44.100
an analytical oriented tool,

00:55:44.500 --> 00:55:46.960
really unlocks things you can do in an application sense.

00:55:46.960 --> 00:55:49.300
So data apps,

00:55:49.300 --> 00:55:53.320
we think are a really interesting market for both DuckDB and MotherDuck,

00:55:53.320 --> 00:55:53.680
where,

00:55:53.680 --> 00:55:54.560
you know,

00:55:54.560 --> 00:55:57.000
it's not just show me a little bit of data.

00:55:57.000 --> 00:55:58.260
Maybe it's an event tracker,

00:55:58.260 --> 00:55:59.280
like a fitness tracker.

00:55:59.280 --> 00:55:59.820
You know,

00:55:59.820 --> 00:56:03.800
I want to see the history of my bike rides for the last month.

00:56:03.800 --> 00:56:04.960
And I want to see the trends,

00:56:04.960 --> 00:56:06.000
my moving averages,

00:56:06.000 --> 00:56:06.800
you know,

00:56:06.800 --> 00:56:08.280
highlight the outliers for me.

00:56:08.280 --> 00:56:08.880
You know,

00:56:08.880 --> 00:56:10.020
there's a lot of,

00:56:10.020 --> 00:56:14.280
of heavy analytical work to be done in data apps that an analytical database

00:56:14.280 --> 00:56:14.820
is really good for.

00:56:14.820 --> 00:56:17.840
Yeah.

00:56:17.840 --> 00:56:20.360
Give us some,

00:56:20.360 --> 00:56:22.480
I don't know how much you can share.

00:56:22.480 --> 00:56:24.420
I know you interact with these customers quite a bit,

00:56:24.420 --> 00:56:24.960
so you know,

00:56:24.960 --> 00:56:26.140
but I don't know how much you can share.

00:56:26.140 --> 00:56:28.360
Give us some senses of what people are doing.

00:56:30.420 --> 00:56:30.720
Sure.

00:56:30.720 --> 00:56:32.740
On the MotherDuck side or the DuckDB side?

00:56:32.740 --> 00:56:33.500
Well,

00:56:33.500 --> 00:56:34.060
you know,

00:56:34.060 --> 00:56:35.220
give us a little example from,

00:56:35.220 --> 00:56:35.660
from each,

00:56:35.660 --> 00:56:36.220
maybe that's,

00:56:36.220 --> 00:56:37.100
that you think is representative.

00:56:37.100 --> 00:56:38.420
You bet.

00:56:38.420 --> 00:56:38.800
I think,

00:56:38.800 --> 00:56:39.240
um,

00:56:39.240 --> 00:56:41.880
on the DuckDB side,

00:56:41.880 --> 00:56:42.660
I think some of the,

00:56:42.660 --> 00:56:44.400
the very interesting things are when you,

00:56:44.400 --> 00:56:44.920
um,

00:56:44.920 --> 00:56:47.660
use DuckDB as a data processing engine,

00:56:47.660 --> 00:56:50.680
where each individual processing task is,

00:56:50.680 --> 00:56:51.120
is,

00:56:51.120 --> 00:56:51.580
you know,

00:56:51.580 --> 00:56:52.100
one node,

00:56:52.100 --> 00:56:53.680
but you're doing a ton of them in parallel.

00:56:53.680 --> 00:56:58.760
So some companies will actually take DuckDB and use it to transform one parquet file into a,

00:56:58.760 --> 00:56:59.080
uh,

00:56:59.080 --> 00:57:00.320
an aggregated parquet file.

00:57:00.320 --> 00:57:01.860
And they'll just set it up on a trigger.

00:57:01.860 --> 00:57:03.340
So when a parquet file gets added,

00:57:03.340 --> 00:57:08.420
they'll automatically just create a little pipeline with DuckDB and convert it into a much,

00:57:08.420 --> 00:57:08.720
you know,

00:57:08.720 --> 00:57:09.460
cleaner,

00:57:09.460 --> 00:57:10.400
more aggregated form.

00:57:10.400 --> 00:57:12.380
And you could do that at huge scale.

00:57:12.380 --> 00:57:16.280
So some companies that are doing huge amounts of data processing,

00:57:16.280 --> 00:57:17.440
you know,

00:57:17.440 --> 00:57:18.020
with DuckDB,

00:57:18.020 --> 00:57:18.720
um,

00:57:18.720 --> 00:57:20.080
tons of tiny DuckDBs,

00:57:20.080 --> 00:57:21.600
a flock of tiny DuckDBs,

00:57:21.600 --> 00:57:22.100
uh,

00:57:22.100 --> 00:57:22.860
running behind the scenes.

00:57:22.860 --> 00:57:24.900
So that's an interesting use case that,

00:57:24.900 --> 00:57:25.220
you know,

00:57:25.220 --> 00:57:27.820
DuckDB originated as a data science tool,

00:57:27.820 --> 00:57:30.320
but I think the data engineering side of,

00:57:30.320 --> 00:57:31.780
of DuckDB is also very exciting.

00:57:31.780 --> 00:57:33.900
where it fits really well in those pipelines.

00:57:33.900 --> 00:57:34.580
Um,

00:57:34.580 --> 00:57:35.480
on the mother duck side,

00:57:35.480 --> 00:57:39.640
I think what we see is a lot of cases where the transactional database,

00:57:39.640 --> 00:57:40.540
you know,

00:57:40.540 --> 00:57:42.660
a lot of cases it's Postgres.

00:57:42.660 --> 00:57:47.840
It's got a really friendly SQL syntax and it aligns very closely with DuckDB SQL syntax,

00:57:47.840 --> 00:57:48.760
um,

00:57:48.760 --> 00:57:54.840
where Postgres just is too slow for those kind of trend oriented aggregates or join queries that you want to run.

00:57:54.840 --> 00:57:55.700
Uh,

00:57:55.700 --> 00:57:56.080
and,

00:57:56.080 --> 00:58:01.180
and mother duck ends up being a really great way to very easily get a lot more speed in those cases.

00:58:01.180 --> 00:58:01.780
Uh,

00:58:01.780 --> 00:58:03.880
and that could be speed for data engineering tasks.

00:58:03.880 --> 00:58:05.160
Like some folks are running,

00:58:05.160 --> 00:58:05.620
um,

00:58:05.620 --> 00:58:06.720
uh,

00:58:06.720 --> 00:58:10.920
data processing jobs with this tool called DBT stands for data build tool.

00:58:10.920 --> 00:58:11.880
Um,

00:58:11.880 --> 00:58:16.160
and it's a Python library and they were running these,

00:58:16.160 --> 00:58:16.520
you know,

00:58:16.520 --> 00:58:17.560
sets of,

00:58:17.560 --> 00:58:19.560
of computation and aggregations and they would take,

00:58:19.560 --> 00:58:19.840
you know,

00:58:19.840 --> 00:58:20.680
eight hours.

00:58:21.960 --> 00:58:23.180
And if it starts running,

00:58:23.180 --> 00:58:23.500
you know,

00:58:23.500 --> 00:58:25.520
2 AM by the time it,

00:58:25.520 --> 00:58:26.360
it errors out,

00:58:26.360 --> 00:58:27.740
you've,

00:58:27.740 --> 00:58:29.140
you've lost your whole day of,

00:58:29.140 --> 00:58:30.640
of your data analysis being up today.

00:58:30.640 --> 00:58:31.120
Yeah.

00:58:31.120 --> 00:58:32.120
You better get it right.

00:58:32.120 --> 00:58:32.460
Right.

00:58:32.460 --> 00:58:33.580
Better get it right.

00:58:33.580 --> 00:58:34.280
Unfortunately,

00:58:34.280 --> 00:58:35.620
DBT does also stand for,

00:58:35.620 --> 00:58:35.940
you know,

00:58:35.940 --> 00:58:36.580
uh,

00:58:36.580 --> 00:58:38.040
behavioral therapy of a certain kind.

00:58:38.040 --> 00:58:38.840
So it's,

00:58:38.840 --> 00:58:39.000
uh,

00:58:39.000 --> 00:58:40.180
you gotta search for DBT data.

00:58:40.180 --> 00:58:40.880
Uh,

00:58:40.880 --> 00:58:41.340
uh,

00:58:41.340 --> 00:58:42.040
it's a tough acronym.

00:58:42.040 --> 00:58:42.800
Uh,

00:58:42.800 --> 00:58:43.440
yeah,

00:58:43.440 --> 00:58:44.760
but it's a little short.

00:58:44.760 --> 00:58:46.300
Yeah.

00:58:46.300 --> 00:58:47.400
DBT Python.

00:58:47.400 --> 00:58:48.120
That'll get you there too.

00:58:48.120 --> 00:58:48.340
Yeah.

00:58:48.340 --> 00:58:49.000
Um,

00:58:49.000 --> 00:58:50.560
with,

00:58:50.560 --> 00:58:51.360
with mother duck and,

00:58:51.360 --> 00:58:51.980
and with duck DB,

00:58:51.980 --> 00:58:53.060
you can take that.

00:58:53.060 --> 00:58:54.280
And instead of being eight hours,

00:58:54.280 --> 00:58:55.020
you can run it in,

00:58:55.020 --> 00:58:55.260
you know,

00:58:55.260 --> 00:58:56.420
15 or 30 minutes.

00:58:57.420 --> 00:58:59.880
and just that scale of change,

00:58:59.880 --> 00:59:00.740
um,

00:59:00.740 --> 00:59:01.480
means that,

00:59:01.480 --> 00:59:02.360
you know,

00:59:02.360 --> 00:59:06.940
it's just a far more delightful experience to do those kinds of data pipelines on an analytical tool,

00:59:06.940 --> 00:59:07.520
uh,

00:59:07.520 --> 00:59:08.400
as opposed to a transaction.

00:59:08.400 --> 00:59:09.460
So that's one.

00:59:09.460 --> 00:59:09.980
The other one is,

00:59:09.980 --> 00:59:10.180
is,

00:59:10.180 --> 00:59:10.400
uh,

00:59:10.400 --> 00:59:11.420
business intelligence tools.

00:59:11.420 --> 00:59:12.080
In a lot of cases,

00:59:12.080 --> 00:59:14.180
those start to get really slow on a transactional database,

00:59:14.180 --> 00:59:16.300
because if you're looking at a graph,

00:59:16.300 --> 00:59:18.840
it's by most of the time,

00:59:18.840 --> 00:59:19.320
it's a trend.

00:59:19.320 --> 00:59:21.500
And most of the time it's looking at a lot of data to,

00:59:21.500 --> 00:59:21.780
to,

00:59:21.780 --> 00:59:22.420
to plot.

00:59:22.420 --> 00:59:24.100
Even if it's just plotting a few points,

00:59:24.100 --> 00:59:25.820
it's looking at a lot of data to plot it.

00:59:25.820 --> 00:59:27.280
And duck DB is really,

00:59:27.280 --> 00:59:28.080
excellent for that.

00:59:28.080 --> 00:59:28.420
And,

00:59:28.420 --> 00:59:29.440
and mother duck as well.

00:59:29.440 --> 00:59:33.220
There's actually a lot of business intelligence tools where they are powered by duck DB.

00:59:33.220 --> 00:59:34.400
So,

00:59:34.400 --> 00:59:34.760
uh,

00:59:34.760 --> 00:59:35.180
mode,

00:59:35.180 --> 00:59:36.380
I was acquired by hotspot.

00:59:36.380 --> 00:59:36.660
They're,

00:59:36.660 --> 00:59:37.640
they're powered by duck DB.

00:59:37.640 --> 00:59:38.320
Um,

00:59:38.320 --> 00:59:39.980
hex is a data science notebook.

00:59:39.980 --> 00:59:42.400
They have duck DB as a part of their architecture.

00:59:42.400 --> 00:59:43.460
Um,

00:59:43.460 --> 00:59:45.220
there's a couple others.

00:59:45.220 --> 00:59:49.980
We talked about some of the data types.

00:59:49.980 --> 00:59:51.320
What about.

00:59:51.320 --> 00:59:54.840
Jason or document type of things,

00:59:54.840 --> 00:59:55.180
right?

00:59:55.180 --> 00:59:56.860
Where you've got a little hierarchical data,

00:59:57.140 --> 00:59:57.680
you know,

00:59:57.680 --> 00:59:58.080
there's,

00:59:58.080 --> 00:59:58.580
you know,

00:59:58.580 --> 00:59:59.720
there's on one end of that spectrum,

00:59:59.720 --> 01:00:00.500
we have manga DB.

01:00:00.500 --> 01:00:01.380
That's all it does.

01:00:01.380 --> 01:00:02.500
And on the other,

01:00:02.500 --> 01:00:05.000
you've got relational databases like Postgres that say,

01:00:05.000 --> 01:00:05.220
well,

01:00:05.220 --> 01:00:06.440
this column can just be.

01:00:06.440 --> 01:00:10.120
Jason arbitrary stuff.

01:00:12.120 --> 01:00:12.540
Yes.

01:00:12.540 --> 01:00:12.540
Yes.

01:00:12.540 --> 01:00:12.620
Yes.

01:00:12.620 --> 01:00:16.000
And we're in that space where we absolutely believe that's how most,

01:00:16.000 --> 01:00:17.680
most things are a little bit of both.

01:00:17.680 --> 01:00:20.800
So there is a full JSON data type in duck DB.

01:00:20.800 --> 01:00:21.740
Uh,

01:00:21.740 --> 01:00:26.540
and so you can store any arbitrary document in there and then process it with some really fast,

01:00:26.540 --> 01:00:26.840
uh,

01:00:26.840 --> 01:00:27.600
JSON processing.

01:00:27.840 --> 01:00:31.960
we can also take JSON data and automatically split it out into columns.

01:00:31.960 --> 01:00:33.140
If you want to unnest it.

01:00:33.140 --> 01:00:34.920
So you can kind of go back and forth there.

01:00:34.920 --> 01:00:36.420
There are also,

01:00:36.420 --> 01:00:37.060
uh,

01:00:37.060 --> 01:00:38.780
specific duck to be nested types.

01:00:38.940 --> 01:00:42.500
If you want to enforce the typing at the column level,

01:00:42.500 --> 01:00:43.640
but you want to be nested.

01:00:43.640 --> 01:00:44.440
So you could say,

01:00:44.440 --> 01:00:46.320
I know these are going to be my keys.

01:00:46.320 --> 01:00:47.640
I know this will be my structure.

01:00:47.640 --> 01:00:48.340
It,

01:00:48.340 --> 01:00:49.800
you can store it all in one column,

01:00:49.800 --> 01:00:50.800
uh,

01:00:50.800 --> 01:00:51.380
and it'll be very,

01:00:51.380 --> 01:00:52.160
very quick to,

01:00:52.160 --> 01:00:52.780
to work with.

01:00:52.780 --> 01:00:53.480
Um,

01:00:53.480 --> 01:00:54.880
so you kind of have both,

01:00:54.880 --> 01:00:55.500
you have the full,

01:00:55.500 --> 01:00:56.560
full flexibility with JSON,

01:00:56.560 --> 01:00:57.380
and then you also can,

01:00:57.380 --> 01:00:58.640
can have nested types.

01:00:58.640 --> 01:01:01.580
Can you query into them?

01:01:01.580 --> 01:01:02.800
So if I have a JSON thing,

01:01:02.800 --> 01:01:07.400
that's got like purchases and then it's got a bunch of purchase objects and then like values,

01:01:07.400 --> 01:01:07.860
can I say,

01:01:07.960 --> 01:01:10.140
give me the ones that have over a hundred dollar purchases?

01:01:10.140 --> 01:01:13.460
You can do much like,

01:01:13.460 --> 01:01:13.760
uh,

01:01:13.760 --> 01:01:16.980
Postgres and SQLite have the ability to extract pieces of JSON.

01:01:16.980 --> 01:01:17.820
Uh,

01:01:17.820 --> 01:01:18.060
you know,

01:01:18.060 --> 01:01:21.120
with the path syntax of like go to this path and pull that value out.

01:01:21.120 --> 01:01:23.380
You absolutely can do that in DuckDB as well.

01:01:23.380 --> 01:01:23.880
And say,

01:01:23.880 --> 01:01:24.200
okay,

01:01:24.200 --> 01:01:24.780
you know,

01:01:24.780 --> 01:01:25.640
navigate to,

01:01:25.640 --> 01:01:25.940
you know,

01:01:25.940 --> 01:01:26.400
customer,

01:01:26.400 --> 01:01:27.360
um,

01:01:27.360 --> 01:01:29.740
the customer object and then inside the customer object,

01:01:29.740 --> 01:01:30.500
go to this object.

01:01:30.500 --> 01:01:30.700
And,

01:01:30.700 --> 01:01:31.340
and,

01:01:31.340 --> 01:01:31.680
um,

01:01:31.680 --> 01:01:32.440
absolutely.

01:01:32.440 --> 01:01:33.480
Yeah.

01:01:33.480 --> 01:01:34.240
Super cool.

01:01:34.240 --> 01:01:35.940
All right.

01:01:35.940 --> 01:01:36.180
Well,

01:01:36.740 --> 01:01:38.440
I think there's probably more to dive into,

01:01:38.440 --> 01:01:38.860
honestly,

01:01:38.860 --> 01:01:40.020
but,

01:01:40.020 --> 01:01:41.740
uh,

01:01:41.740 --> 01:01:42.660
but we've covered it.

01:01:42.660 --> 01:01:43.920
I think we've covered it pretty well.

01:01:43.920 --> 01:01:44.260
I,

01:01:44.260 --> 01:01:48.900
I think DuckDB is super exciting for the possibilities that it opens up.

01:01:48.900 --> 01:01:49.700
You know,

01:01:49.700 --> 01:01:54.880
there might be a ton of people out there who haven't heard of it or really just heard of it in passing and didn't really know.

01:01:56.300 --> 01:01:59.460
this in process stuff that you can do,

01:01:59.460 --> 01:02:01.880
it really makes it quite accessible,

01:02:01.880 --> 01:02:02.520
quite easy,

01:02:02.520 --> 01:02:04.540
it lowers the bar to getting started,

01:02:04.540 --> 01:02:04.760
right?

01:02:04.760 --> 01:02:06.120
You don't have to understand connections,

01:02:06.120 --> 01:02:06.740
security,

01:02:06.740 --> 01:02:07.920
servers,

01:02:07.920 --> 01:02:08.980
daemons,

01:02:08.980 --> 01:02:09.980
all of that.

01:02:09.980 --> 01:02:11.420
So I'm,

01:02:11.420 --> 01:02:12.140
I'm excited for,

01:02:12.140 --> 01:02:13.080
see where this goes.

01:02:13.080 --> 01:02:14.660
It's already got a ton of traction.

01:02:14.660 --> 01:02:17.180
So final call to action,

01:02:17.300 --> 01:02:20.080
maybe tell people who are in that group,

01:02:20.080 --> 01:02:21.260
what do they do?

01:02:21.260 --> 01:02:22.520
You bet.

01:02:22.520 --> 01:02:22.860
I think,

01:02:22.860 --> 01:02:23.360
you know,

01:02:23.360 --> 01:02:23.580
obviously,

01:02:23.580 --> 01:02:24.500
pip install DuckDB,

01:02:24.500 --> 01:02:25.200
give it a try.

01:02:25.200 --> 01:02:25.580
It's,

01:02:25.580 --> 01:02:26.020
it's,

01:02:26.020 --> 01:02:27.120
it's anywhere you can use it.

01:02:27.120 --> 01:02:28.200
MIT license,

01:02:28.200 --> 01:02:28.740
it's,

01:02:28.740 --> 01:02:30.960
it's really can fit anywhere that you're running Python.

01:02:31.340 --> 01:02:34.100
a whole lot of other fun things you could do on top of it.

01:02:34.100 --> 01:02:35.820
There's a new extension ecosystem.

01:02:35.820 --> 01:02:38.100
So if it doesn't exist in DuckDB,

01:02:38.100 --> 01:02:38.980
but you'd like it to,

01:02:38.980 --> 01:02:42.540
you can actually build an extension for DuckDB in a variety of languages.

01:02:42.540 --> 01:02:43.060
And,

01:02:43.060 --> 01:02:44.440
and so we can,

01:02:44.440 --> 01:02:46.640
we can all together make DuckDB into whatever we'd like.

01:02:46.640 --> 01:02:47.700
So it's awesome.

01:02:47.700 --> 01:02:48.300
Pretty neat.

01:02:48.300 --> 01:02:49.220
Yeah.

01:02:49.220 --> 01:02:49.960
Well,

01:02:49.960 --> 01:02:50.880
good work.

01:02:50.880 --> 01:02:52.240
And I love to see,

01:02:52.240 --> 01:02:53.920
I love to see the,

01:02:53.920 --> 01:02:59.780
the open source side and then maybe a strong company that's built around it in a way

01:02:59.780 --> 01:03:03.100
that doesn't really undermine the open source value.

01:03:03.100 --> 01:03:04.800
So MotherDuck,

01:03:04.800 --> 01:03:05.840
DuckDB,

01:03:05.840 --> 01:03:07.620
it looks like a really good relationship there.

01:03:07.620 --> 01:03:08.080
So that's,

01:03:08.080 --> 01:03:09.260
that's nice.

01:03:09.260 --> 01:03:09.960
Yeah.

01:03:09.960 --> 01:03:11.160
We even have two companies.

01:03:11.160 --> 01:03:12.660
We got DuckDB Labs and MotherDuck.

01:03:12.660 --> 01:03:14.060
Yeah,

01:03:14.060 --> 01:03:14.360
that's right.

01:03:14.360 --> 01:03:15.820
We got the foundational side as well.

01:03:15.820 --> 01:03:16.220
Yeah.

01:03:16.220 --> 01:03:17.180
Awesome.

01:03:17.180 --> 01:03:17.500
Well,

01:03:17.500 --> 01:03:17.780
Alex,

01:03:17.780 --> 01:03:19.780
thanks for being on the show and yeah.

01:03:19.780 --> 01:03:22.160
Thanks for sharing all this stuff with us.

01:03:22.160 --> 01:03:22.500
It's great.

01:03:22.500 --> 01:03:23.900
Thanks for having me.

01:03:23.900 --> 01:03:24.320
Cheers,

01:03:24.320 --> 01:03:24.640
folks.

01:03:24.640 --> 01:03:25.460
Happy analyzing.

01:03:25.460 --> 01:03:26.040
Yep.

01:03:26.040 --> 01:03:26.900
Bye.

01:03:26.900 --> 01:03:27.900
Bye.

01:03:27.900 --> 01:03:29.960
Thank you.

