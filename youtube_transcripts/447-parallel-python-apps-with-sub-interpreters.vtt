WEBVTT

00:00:00.000 --> 00:00:05.000
Anthony, Eric, hello, and welcome to Talk Python.

00:00:05.000 --> 00:00:10.680
Hey, guys, it's really good to have you both here.

00:00:10.680 --> 00:00:15.400
You both have been on the show before, which is awesome.

00:00:15.400 --> 00:00:20.680
And Eric, we've talked about Subber Interpreters before, but they were kind of a dream almost

00:00:20.680 --> 00:00:21.680
at the time.

00:00:21.680 --> 00:00:22.680
That's right.

00:00:22.680 --> 00:00:24.680
And now, now they feel pretty real.

00:00:24.680 --> 00:00:25.680
That's right.

00:00:25.680 --> 00:00:31.440
Yeah, it's been a long time coming, and I think the last time we talked, I've always

00:00:31.440 --> 00:00:35.000
been hopeful, but it seemed like it was getting closer.

00:00:35.000 --> 00:00:42.320
So with 3.12, we were able to land Per Interpreter Gil, which kind of was the last piece, the

00:00:42.320 --> 00:00:43.320
foundational part.

00:00:43.320 --> 00:00:48.220
I wanted to do a lot of cleanup, a lot of work that had to get done, but that last piece

00:00:48.220 --> 00:00:49.800
got in for 3.12.

00:00:49.800 --> 00:00:51.440
Excellent, excellent.

00:00:51.440 --> 00:00:53.240
So good.

00:00:53.240 --> 00:00:56.800
And maybe let's just do a quick check in with you all.

00:00:56.800 --> 00:01:01.480
It's been a while, you know, Anthony, start with you, I guess.

00:01:01.480 --> 00:01:04.560
Quick intro for people who don't know you, although I don't know how that's possible,

00:01:04.560 --> 00:01:07.320
and then just what you've been up to.

00:01:07.320 --> 00:01:09.320
Yeah, I'm Anthony Shaw.

00:01:09.320 --> 00:01:10.320
I work at Microsoft.

00:01:10.320 --> 00:01:19.400
I lead the Python advocacy team, and I do lots of Python stuff, open source, testing

00:01:19.400 --> 00:01:25.560
things, building tools, blogging, building projects, sharing things.

00:01:25.560 --> 00:01:28.200
You have a book, something about the inside of Python?

00:01:28.200 --> 00:01:29.200
Oh yeah, I have a book as well.

00:01:29.200 --> 00:01:30.200
I forgot about that.

00:01:30.200 --> 00:01:35.240
Yeah, there's a book called CPython Internals, which is a book all about the Python compiler

00:01:35.240 --> 00:01:36.240
and how it works.

00:01:36.240 --> 00:01:38.480
Did you suppress the memory of writing it?

00:01:38.480 --> 00:01:42.080
Like it was too traumatic, it's down there deep in the clouds?

00:01:42.080 --> 00:01:44.440
Yeah, I keep forgetting.

00:01:45.040 --> 00:01:47.340
Yeah, that book was for 3.9.

00:01:47.340 --> 00:01:53.240
And people keep asking me if I'm going to update it for 3.13, maybe, because things

00:01:53.240 --> 00:01:54.240
keep changing.

00:01:54.240 --> 00:01:55.240
Yeah.

00:01:55.240 --> 00:02:00.120
Things have been changing at a more rapid pace than they were a few years ago as well.

00:02:00.120 --> 00:02:02.560
So that maybe makes it more challenging.

00:02:02.560 --> 00:02:08.280
Yeah, recently, I've been doing some more research as well.

00:02:08.280 --> 00:02:13.480
So I just finished my master's a few months ago, and I started my PhD.

00:02:13.480 --> 00:02:17.760
And I'm looking at parallelism in Python as one of the topics.

00:02:17.760 --> 00:02:24.320
So I've been quite involved in subinterpreters and the free threading project and some other

00:02:24.320 --> 00:02:25.320
stuff as well.

00:02:25.320 --> 00:02:26.320
Awesome.

00:02:26.320 --> 00:02:28.000
Congratulations on the master's degree.

00:02:28.000 --> 00:02:29.000
That's really great.

00:02:29.000 --> 00:02:31.240
And I didn't realize you were going further.

00:02:31.240 --> 00:02:32.240
So cool.

00:02:32.240 --> 00:02:33.240
Eric.

00:02:33.240 --> 00:02:34.240
Yeah, Eric Snow.

00:02:34.240 --> 00:02:44.480
So I've been working on Python as a core developer for over 10 years now.

00:02:44.480 --> 00:02:48.760
But I've been participating for even longer than that.

00:02:48.760 --> 00:02:50.600
And it's been good.

00:02:50.600 --> 00:02:58.560
I've worked on a variety of things, a lot of stuff down in the core runtime of CPython.

00:02:58.560 --> 00:03:07.040
And I've been working on this, trying to find a solution for multi-core Python since, really

00:03:07.040 --> 00:03:08.040
since 2014.

00:03:08.040 --> 00:03:09.040
Yeah.

00:03:09.040 --> 00:03:16.360
So I've been slowly, ever so slowly working towards that goal.

00:03:16.360 --> 00:03:19.480
And we made it with 3.12, and there's more work to do.

00:03:19.480 --> 00:03:22.520
But that's a lot of the stuff that I've been working on.

00:03:22.520 --> 00:03:28.360
I'm at Microsoft, but don't work with Anthony a whole lot.

00:03:28.360 --> 00:03:36.120
I work on the Python performance team with Guido and Brent Bueger and Mark Shannon, Eric

00:03:36.120 --> 00:03:37.120
Cuttrell.

00:03:37.120 --> 00:03:40.960
And we're just working generally to make Python faster.

00:03:40.960 --> 00:03:44.600
So my part of that has involved subinterpreters.

00:03:44.600 --> 00:03:52.680
But interestingly enough, it's only really this year that I've been able to work on all

00:03:52.680 --> 00:03:54.560
the subinterpreter stuff full time.

00:03:54.560 --> 00:03:58.160
Before that, I was working mostly on other stuff.

00:03:58.160 --> 00:04:01.360
So this year's been a good year for me.

00:04:01.360 --> 00:04:02.360
Yeah, I would say.

00:04:02.360 --> 00:04:05.520
That must be really exciting to get the, like, you know what?

00:04:05.520 --> 00:04:07.120
Why don't you just do that?

00:04:07.120 --> 00:04:08.520
That'd be awesome for us.

00:04:08.520 --> 00:04:10.600
Yeah, it's been awesome.

00:04:10.600 --> 00:04:16.840
Well, maybe since you're on the team, a quick check in on faster CPython.

00:04:16.840 --> 00:04:19.800
It's made a mega difference over the last couple of releases.

00:04:19.800 --> 00:04:23.160
Yeah, it's kind of interesting.

00:04:23.160 --> 00:04:25.920
Mark Shannon definitely has a vision.

00:04:25.920 --> 00:04:30.400
He's developed a plan as of, like, years ago.

00:04:30.400 --> 00:04:36.040
But we finally were able to get him, put him in a position where he could do something

00:04:36.040 --> 00:04:37.040
about it.

00:04:37.040 --> 00:04:39.840
And we've all been kind of pitching in.

00:04:39.840 --> 00:04:46.240
A lot of it has to do with just applying some of the general ideas that are out there regarding

00:04:46.240 --> 00:04:48.400
dynamic languages and optimization.

00:04:48.400 --> 00:04:57.040
Things have been applied to other things like HHVM or various of the JavaScript runtimes.

00:04:57.040 --> 00:05:06.480
And so a lot of specialization, adaptive specialization, a few other techniques.

00:05:06.480 --> 00:05:14.920
But right now, so a lot of that stuff we're able to get in for 3.11.

00:05:14.920 --> 00:05:21.000
In 3.12, there wasn't quite as much impactful stuff.

00:05:21.000 --> 00:05:25.320
We're kind of gearing up to effectively add a JIT into CPython.

00:05:25.320 --> 00:05:30.920
And that's required a lot of kind of behind the scenes work to get things in the right

00:05:30.920 --> 00:05:32.640
places.

00:05:32.640 --> 00:05:37.920
So we're somewhat targeting 3.13 for that.

00:05:37.920 --> 00:05:47.480
So right now, I think with where things are at, we're kind of break even performance wise.

00:05:47.480 --> 00:05:52.200
But there's a lot of stuff that we can do.

00:05:52.200 --> 00:05:57.520
A lot of optimization work that really hasn't even been done yet that'll take that performance

00:05:57.520 --> 00:06:01.200
improvement up pretty drastically.

00:06:01.200 --> 00:06:03.080
It's kind of hard to say where we're going to be.

00:06:03.080 --> 00:06:09.920
But for 3.13, it's looking pretty good for at least some performance improvement because

00:06:09.920 --> 00:06:13.960
of the JITing and optimization work.

00:06:13.960 --> 00:06:14.960
That's exciting.

00:06:14.960 --> 00:06:18.560
We have no real JIT at the moment, right?

00:06:18.560 --> 00:06:19.560
Not in CPython.

00:06:19.560 --> 00:06:20.560
Yeah.

00:06:20.560 --> 00:06:21.560
I mean, I know.

00:06:21.560 --> 00:06:24.560
In Anthony, I did his best.

00:06:24.560 --> 00:06:26.400
I know.

00:06:26.400 --> 00:06:31.400
Well, that's actually super exciting because I feel like that could be another big boost

00:06:31.400 --> 00:06:32.400
potentially.

00:06:32.400 --> 00:06:38.280
With the JIT, it comes all sorts of things like inlining of small methods and optimization

00:06:38.280 --> 00:06:40.400
based on type information.

00:06:40.400 --> 00:06:41.800
Yeah.

00:06:41.800 --> 00:06:42.800
All that stuff.

00:06:42.800 --> 00:06:48.760
One of the most exciting parts for me is that a lot of this work, not long after I joined

00:06:48.760 --> 00:06:56.240
the team-- so what, two years ago, two and a half years ago, somewhere in there-- pretty

00:06:56.240 --> 00:07:02.080
early on, we started reaching out to other folks, other projects that were interested

00:07:02.080 --> 00:07:06.160
in performance and performance of Python code.

00:07:06.160 --> 00:07:10.740
And we've worked pretty hard to collaborate with them.

00:07:10.740 --> 00:07:19.560
So like the team over at Meta, they have a lot of interest in making sure Python is very

00:07:19.560 --> 00:07:21.480
efficient.

00:07:21.480 --> 00:07:26.160
And so we've actually worked pretty closely with them.

00:07:26.160 --> 00:07:30.600
And they're able to take advantage of a lot of the work that we've done, which is great.

00:07:30.600 --> 00:07:31.600
Yeah.

00:07:31.600 --> 00:07:36.480
There seems to be some synergy between the Cinder team and the faster CPython team.

00:07:36.480 --> 00:07:37.880
So awesome.

00:07:37.880 --> 00:07:47.800
But let's focus on a part that is there but not really utilized very much yet, which is

00:07:47.800 --> 00:07:49.000
the subinterpreter.

00:07:49.000 --> 00:07:54.560
So back on-- when is this-- 2019, Eric, I had you on.

00:07:54.560 --> 00:08:00.240
We talked about, can subinterpreters free us from Python's gill?

00:08:00.240 --> 00:08:04.340
And since then, this PEP has been accepted.

00:08:04.340 --> 00:08:07.080
But it's Anthony's fault that we're here.

00:08:07.080 --> 00:08:11.800
Because Anthony posted over on Mastodon, hey, here's a new blog post, me running Python

00:08:11.800 --> 00:08:14.460
parallel applications with subinterpreters.

00:08:14.460 --> 00:08:19.800
How about we use Flask and FastAPI and subinterpreters and make that go fast?

00:08:19.800 --> 00:08:29.080
And that sounded more available in the Python level than I realized the subinterpreter stuff

00:08:29.080 --> 00:08:30.080
was.

00:08:30.080 --> 00:08:33.120
So that's super exciting, both of you.

00:08:33.120 --> 00:08:38.560
Yeah, it's been fun to play with it and try and build applications on it and stuff like

00:08:38.560 --> 00:08:39.560
that.

00:08:39.560 --> 00:08:46.000
And working with Eric probably over the last couple of months on things that we've discovered

00:08:46.000 --> 00:08:50.920
in that process, especially with C extensions.

00:08:50.920 --> 00:08:55.580
Yeah, that's one.

00:08:55.580 --> 00:08:56.580
With C extensions.

00:08:56.580 --> 00:09:02.820
And I think that some of those challenges are going to be the same with free threading

00:09:02.820 --> 00:09:03.820
as well.

00:09:03.820 --> 00:09:12.040
So it's how C extensions have state, where they put it, whether that's thread safe.

00:09:12.040 --> 00:09:19.060
And as soon as you kind of open up the possibility of having multiple gills in one process, then

00:09:19.060 --> 00:09:23.060
what challenges does that create?

00:09:23.060 --> 00:09:25.860
Yeah, absolutely.

00:09:25.860 --> 00:09:29.540
Well, I guess maybe some nomenclature first.

00:09:29.540 --> 00:09:34.140
Not no GIL Python or subinterpreter, free threaded.

00:09:34.140 --> 00:09:36.460
Is that what we're calling it?

00:09:36.460 --> 00:09:37.460
What's the name?

00:09:37.460 --> 00:09:39.860
How do we how do we speak about this?

00:09:39.860 --> 00:09:41.860
It's not quite settled.

00:09:41.860 --> 00:09:48.260
But I think a lot of people have taken to referring to it as free threaded.

00:09:48.260 --> 00:09:49.260
I can go with that.

00:09:49.260 --> 00:09:52.220
People still talk about no gill, but it's free threaded.

00:09:52.220 --> 00:09:54.260
It's probably the best bet.

00:09:54.260 --> 00:09:57.940
Are you describing what it does and why you care?

00:09:57.940 --> 00:09:59.700
Or are you describing the implementation?

00:09:59.700 --> 00:10:04.300
The implementation is it has no gill, so it can be free threaded or it has subinterpreters,

00:10:04.300 --> 00:10:05.300
so it can be free threaded.

00:10:05.300 --> 00:10:08.060
But really, what you want is the free threaded part.

00:10:08.060 --> 00:10:10.660
You don't care actually about the GIL too much.

00:10:10.660 --> 00:10:11.660
Right?

00:10:11.660 --> 00:10:12.660
Kind of.

00:10:12.660 --> 00:10:14.660
Well, it's interesting with subinterpreters.

00:10:14.660 --> 00:10:20.500
It really isn't necessarily a free threaded model.

00:10:20.500 --> 00:10:29.300
It's kind of free threaded only in the part at which you're like moving between interpreters.

00:10:29.300 --> 00:10:33.060
So you only have to care about it when you're interacting between interpreters.

00:10:33.060 --> 00:10:36.260
The rest of the time, you don't have to worry about it.

00:10:36.260 --> 00:10:43.100
With the no GIL is more kind of what we think of as free threading work.

00:10:43.100 --> 00:10:44.100
Everything is unsafe.

00:10:44.100 --> 00:10:45.100
Right.

00:10:45.100 --> 00:10:47.900
And for people who don't know, the no GIL stuff is what's coming out of the sender team

00:10:47.900 --> 00:10:48.900
and from Sam Gross.

00:10:48.900 --> 00:10:55.380
And that was also approved, but with the biggest caveat I've ever seen on an approved PEP.

00:10:55.380 --> 00:10:56.980
Yeah.

00:10:56.980 --> 00:10:58.060
We approved this.

00:10:58.060 --> 00:11:01.860
We also reserved the right to completely undo it and not approve it anymore.

00:11:01.860 --> 00:11:02.860
But fair.

00:11:02.860 --> 00:11:06.900
And it's also a compiler flag that is an optional off by default situation.

00:11:06.900 --> 00:11:08.900
So it should be interesting.

00:11:08.900 --> 00:11:13.700
Yeah, we can maybe compare and contrast them a bit later as well.

00:11:13.700 --> 00:11:14.700
Yeah, absolutely.

00:11:14.700 --> 00:11:19.660
Well, let's start with what is an interpreter?

00:11:19.660 --> 00:11:21.460
So then how do we get to sub interpreters?

00:11:21.460 --> 00:11:23.900
And then what work did you have to do?

00:11:23.900 --> 00:11:28.700
I heard there was a few global variables that are being shared here, Eric.

00:11:28.700 --> 00:11:34.060
Maybe let's give people a quick rundown of what is this and how is it this new feature

00:11:34.060 --> 00:11:36.300
in 3.12 changing things?

00:11:36.300 --> 00:11:37.300
Yeah.

00:11:37.300 --> 00:11:45.900
So there's a in a Python process when you run Python, all of everything that happens,

00:11:45.900 --> 00:11:52.860
all the machinery that's running your Python code is running with a certain amount of global

00:11:52.860 --> 00:11:54.580
state.

00:11:54.580 --> 00:12:00.500
And and historically, you can think of it as, you know, across the whole process, you've

00:12:00.500 --> 00:12:01.980
got a bunch of global state.

00:12:01.980 --> 00:12:09.460
And if you look at all the stuff like in the module, system modules or system, whatever,

00:12:09.460 --> 00:12:14.740
all those things are shared across the whole the whole runtime.

00:12:14.740 --> 00:12:19.620
So if you have different threads, for instance, running, they all share that stuff, even though

00:12:19.620 --> 00:12:22.940
you're going to have different code running in each thread.

00:12:22.940 --> 00:12:30.300
So all of that runtime state is everything that Python needs in order to run.

00:12:30.300 --> 00:12:34.800
But what's interesting is that the vast majority of it.

00:12:34.800 --> 00:12:40.580
You can think of as the actual interpreter.

00:12:40.580 --> 00:12:43.780
And so that state.

00:12:43.780 --> 00:12:48.420
If we treat it as isolated and we're very careful about it, then we can have multiple

00:12:48.420 --> 00:12:50.380
of them.

00:12:50.380 --> 00:12:56.380
That means that when your Python code runs, that it can run with a different set of this

00:12:56.380 --> 00:13:02.740
global state, different modules imported, different things going on, different threads

00:13:02.740 --> 00:13:07.620
that are unrelated and really don't affect each other at all.

00:13:07.620 --> 00:13:15.140
And then with that in mind, you can take it one step farther and say, well, let's completely

00:13:15.140 --> 00:13:19.660
isolate those and like not even have them share a gill.

00:13:19.660 --> 00:13:20.660
Right.

00:13:20.660 --> 00:13:23.420
And then at that point, that's where the magic happens.

00:13:23.420 --> 00:13:31.340
So that's kind of the my first goal in this whole project was to get to that point, because

00:13:31.340 --> 00:13:36.980
once you get there, then it opens up a lot of possibilities when it comes to concurrency

00:13:36.980 --> 00:13:37.980
and parallelism.

00:13:37.980 --> 00:13:38.980
Yeah.

00:13:38.980 --> 00:13:44.500
Then Anthony can start running with his blog posts and showing off things.

00:13:44.500 --> 00:13:46.820
Yeah, absolutely.

00:13:46.820 --> 00:13:55.500
So one thing I don't know the answer to, but might be interesting is Python has a memory

00:13:55.500 --> 00:13:59.460
management story in front of the operating system, virtual memory that's assigned to

00:13:59.460 --> 00:14:05.500
the process with pools, arenas, blocks, those kinds of things.

00:14:05.500 --> 00:14:07.940
What's that look like with regard to subinterpreters?

00:14:07.940 --> 00:14:13.820
Does each subinterpreter have its own chunk or set of those for the memory it allocates,

00:14:13.820 --> 00:14:19.540
or is it still a shared one thing per process?

00:14:19.540 --> 00:14:21.340
It's per interpreter.

00:14:21.340 --> 00:14:25.100
This is something that was very global.

00:14:25.100 --> 00:14:30.700
And like you pointed out earlier, this whole project was all about taking all sorts of

00:14:30.700 --> 00:14:36.020
global state that was actually stored in like C global variables all over the place, right?

00:14:36.020 --> 00:14:43.620
And pulling those in together into one place and moving those down from kind of the process

00:14:43.620 --> 00:14:48.060
global state down into each interpreter.

00:14:48.060 --> 00:14:56.020
So one of those things was all of the allocator state that we have for objects.

00:14:56.020 --> 00:15:01.100
And Python has this idea of different levels of allocators.

00:15:01.100 --> 00:15:06.700
The object allocator is what's used heavily for Python objects, of course, but some other

00:15:06.700 --> 00:15:08.580
state as well.

00:15:08.580 --> 00:15:15.140
And the object allocator is the part that has all the arenas and everything like you're

00:15:15.140 --> 00:15:17.500
saying.

00:15:17.500 --> 00:15:23.380
So part of what I did before we could make the GIL per interpreter, we had to make the

00:15:23.380 --> 00:15:28.340
allocator state per interpreter.

00:15:28.340 --> 00:15:33.420
Well the reason I think that it's interesting asking about it is one, because of the gill

00:15:33.420 --> 00:15:38.020
obviously, but the other one is it seems to me like these subinterpreters could be used

00:15:38.020 --> 00:15:41.100
for a little bit of stability or isolation.

00:15:41.100 --> 00:15:46.940
You want to run some kind of code and when that line exits, I want the memory freed,

00:15:46.940 --> 00:15:50.620
I want modules unloaded, I want it to go back to the way it was.

00:15:50.620 --> 00:15:51.980
You know what I mean?

00:15:51.980 --> 00:15:56.060
Whereas normally in Python, even if the memory becomes free, right, it's still got some of

00:15:56.060 --> 00:15:59.780
that like, well, we allocate this stuff, now we're holding it to refill it and then you

00:15:59.780 --> 00:16:01.740
don't unimport modules.

00:16:01.740 --> 00:16:07.740
And the modules can be pretty intense actually if they start allocating a bunch of stuff

00:16:07.740 --> 00:16:09.100
themselves and so on.

00:16:09.100 --> 00:16:13.060
What do you guys think about this as an idea, as an aspect of it?

00:16:13.060 --> 00:16:20.780
Yeah, there's one example I've been coming across recently and this is a pattern, I think

00:16:20.780 --> 00:16:29.340
it's a bit of an anti-pattern actually, but some Python packages, they store like some

00:16:29.340 --> 00:16:32.260
state information in the module level.

00:16:32.260 --> 00:16:39.120
So an example is a SDK that I've been working with, which has just been rewritten to stop

00:16:39.120 --> 00:16:45.300
it from doing this, but you would put the API key of the SDK, you would import it.

00:16:45.300 --> 00:16:50.740
So you'd import X and then do like X.API_KEY=.

00:16:50.740 --> 00:17:00.140
So it basically stores the API key in the module object, which is fine if you've imported

00:17:00.140 --> 00:17:04.260
the module once and you're using it once.

00:17:04.260 --> 00:17:10.760
But what you see is that if you put that in a web application, it just assumes that everyone

00:17:10.760 --> 00:17:12.460
uses the same key.

00:17:12.460 --> 00:17:19.780
So you can't import that module and then connect to it with different API keys, like you'd

00:17:19.780 --> 00:17:21.500
have different users or something.

00:17:21.500 --> 00:17:22.500
Right.

00:17:22.500 --> 00:17:25.500
So you have some kind of multi-tenancy, right?

00:17:25.500 --> 00:17:31.700
Where like they would say, enter their ChatGPT, open AI key, and then they could work

00:17:31.700 --> 00:17:33.260
on behalf of that, right?

00:17:33.260 --> 00:17:34.860
Potentially something like that, right?

00:17:34.860 --> 00:17:35.980
Yeah, exactly.

00:17:35.980 --> 00:17:41.060
So that's kind of like an API example, but there are other examples where let's say you're

00:17:41.060 --> 00:17:46.860
loading data or something and it stores some temporary information somewhere in like a

00:17:46.860 --> 00:17:52.780
class attribute or even like a module attribute like that, then if you've got one piece of

00:17:52.780 --> 00:17:59.380
code loading data and then in another thread in a web app or just in another thread generally,

00:17:59.380 --> 00:18:04.900
you're reading another piece of data and they're sharing state somehow, and you've got no isolation.

00:18:04.900 --> 00:18:10.520
Some of that is due to the way that people have written the Python code or the extension

00:18:10.520 --> 00:18:18.220
code has kind of been built around, oh, we'll just put this information here and they haven't

00:18:18.220 --> 00:18:21.220
really thought about the isolation.

00:18:21.220 --> 00:18:29.180
Sometimes it's because on the C level especially, that because the GIL was always there, they've

00:18:29.180 --> 00:18:30.940
never had to worry about it.

00:18:30.940 --> 00:18:36.340
So you could just have a counter for example, or there's an object, which is a dictionary

00:18:36.340 --> 00:18:44.340
that is a cache of something, and you just put that as a static variable and you just

00:18:44.340 --> 00:18:45.340
read and write from it.

00:18:45.340 --> 00:18:48.940
You've never had to worry about thread safety because the GIL was there to kind of protect

00:18:48.940 --> 00:18:49.940
you.

00:18:49.940 --> 00:18:53.820
You probably shouldn't have built it that way, but it didn't really matter because it

00:18:53.820 --> 00:18:54.820
worked.

00:18:54.820 --> 00:18:56.580
But what about this, Anthony?

00:18:56.580 --> 00:18:59.980
What if we can write it on one line, it'll probably be safe, right?

00:18:59.980 --> 00:19:04.860
If we can fit it on just one line of Python code, it'll be okay?

00:19:04.860 --> 00:19:05.860
Yeah.

00:19:05.860 --> 00:19:08.860
Dictionary.add, what's wrong there?

00:19:08.860 --> 00:19:10.860
Dictionary.get is fine.

00:19:10.860 --> 00:19:11.860
Yeah.

00:19:11.860 --> 00:19:19.540
So yeah, what we're saying, within subinterpreters, I think what's the concept that people will

00:19:19.540 --> 00:19:25.780
need to kind of understand is where the isolation is because there are different models for

00:19:25.780 --> 00:19:27.220
running parallel code.

00:19:27.220 --> 00:19:35.800
And at the moment we've got coroutines, which is asynchronous, so it can run concurrently.

00:19:35.800 --> 00:19:41.180
So that's if you do async and a wait, or if you use the old coroutine decorator.

00:19:41.180 --> 00:19:47.500
You've also got things like generators, which are kind of like a concurrent pattern.

00:19:47.500 --> 00:19:50.620
You've got threads that you can create.

00:19:50.620 --> 00:19:55.820
All of those live within the same interpreter and they share the same information.

00:19:55.820 --> 00:20:02.500
So you don't have to, if you create a thread, inside that thread, you can read a variable

00:20:02.500 --> 00:20:05.020
from outside of that thread.

00:20:05.020 --> 00:20:06.740
And it doesn't complain.

00:20:06.740 --> 00:20:09.500
You don't need to create a lock at the moment.

00:20:09.500 --> 00:20:14.180
Although in some situations you probably should.

00:20:14.180 --> 00:20:19.780
And you don't need to re-import modules and stuff like that, which can be fine.

00:20:19.780 --> 00:20:23.820
And then at the other extreme, you've got multiprocessing, which is a module in the

00:20:23.820 --> 00:20:30.040
standard library that allows you to create extra Python processes and then kind of gives

00:20:30.040 --> 00:20:33.660
you like an API to talk to them and share information between them.

00:20:33.660 --> 00:20:39.380
And that's kind of like the other extreme, which is you've got, you know, the ultimate

00:20:39.380 --> 00:20:40.380
level of isolation.

00:20:40.380 --> 00:20:44.140
You've got a whole separate Python process.

00:20:44.140 --> 00:20:47.540
But instead of interacting with it via the command line, you've kind of got this nicer

00:20:47.540 --> 00:20:53.620
API where you can almost treat it like it's in the same process as the one you're running

00:20:53.620 --> 00:20:54.620
from.

00:20:54.620 --> 00:20:55.620
Yeah.

00:20:55.620 --> 00:20:58.940
It's kind of magical actually that at all, you get a return value from a process, for

00:20:58.940 --> 00:20:59.940
example.

00:20:59.940 --> 00:21:00.940
Right.

00:21:00.940 --> 00:21:01.940
Yeah.

00:21:01.940 --> 00:21:06.740
But the thing is, if you peel back the covers a little bit, then like how it sends information

00:21:06.740 --> 00:21:12.140
to the other Python process involves a lot of pickles.

00:21:12.140 --> 00:21:14.440
And it's not particularly efficient.

00:21:14.440 --> 00:21:20.940
And also a Python process has a lot of extra stuff that you maybe necessarily didn't even

00:21:20.940 --> 00:21:21.940
need.

00:21:21.940 --> 00:21:25.300
Like you get all this isolation from having it, but you have to import all the modules

00:21:25.300 --> 00:21:26.300
again.

00:21:26.300 --> 00:21:29.620
You have to create the arenas again or the memory allocation.

00:21:29.620 --> 00:21:33.020
You have to do all the startup process again, which takes a lot of time.

00:21:33.020 --> 00:21:34.020
It's like at least 200 milliseconds.

00:21:34.020 --> 00:21:36.260
Parting the Python code again, right?

00:21:36.260 --> 00:21:37.260
At least the PYC.

00:21:37.260 --> 00:21:38.260
Yeah.

00:21:39.260 --> 00:21:40.260
Yeah, exactly.

00:21:40.260 --> 00:21:43.540
So you basically created like a whole separate Python.

00:21:43.540 --> 00:21:49.860
And if you do that just to run a small chunk of code, then it's not probably the best model

00:21:49.860 --> 00:21:50.860
at all.

00:21:50.860 --> 00:21:51.860
Yeah.

00:21:51.860 --> 00:21:58.900
You have a nice graph as well that shows like sort of the rate as you add more work and

00:21:58.900 --> 00:21:59.900
you need more parallelism.

00:21:59.900 --> 00:22:03.580
We'll get to that, I'm sure.

00:22:03.580 --> 00:22:10.380
One thing that struck me coming to Python from other languages like C, C++, C#, there's

00:22:10.380 --> 00:22:17.980
very little locks and events, threading, coordinating stuff in Python.

00:22:17.980 --> 00:22:23.820
And I think that there's probably a ton of Python code that actually is not actually

00:22:23.820 --> 00:22:28.780
thread safe, but people kind of get away with it because the context switching is so coarse

00:22:28.780 --> 00:22:29.780
grained.

00:22:29.780 --> 00:22:30.780
Right?

00:22:30.780 --> 00:22:34.460
Like you say, well, the gills there, so you only run one instruction at a time, but like

00:22:34.460 --> 00:22:39.220
this temporary invalid state you entered to as part of like, just your code running, like

00:22:39.220 --> 00:22:42.860
took money out of this account, and then I'm going to put it into that account.

00:22:42.860 --> 00:22:46.620
Those are multiple Python lines and there's nothing saying they couldn't get interrupted

00:22:46.620 --> 00:22:47.780
between one to the other.

00:22:47.780 --> 00:22:49.260
And then things are busted.

00:22:49.260 --> 00:22:50.260
Right?

00:22:50.260 --> 00:22:55.020
I feel there's some concern about adding this concurrency, like, oh, we're going to worry

00:22:55.020 --> 00:22:56.020
about it.

00:22:56.020 --> 00:22:58.580
Like you probably should be worrying about it now.

00:22:58.580 --> 00:23:04.700
Not as much necessarily, but I feel like people are getting away with it because it's so rare,

00:23:04.700 --> 00:23:06.700
but it's a non-zero possibility.

00:23:06.700 --> 00:23:10.060
What do you guys think?

00:23:10.060 --> 00:23:11.060
>> Yeah.

00:23:11.060 --> 00:23:14.460
I mean, those are real concerns.

00:23:14.460 --> 00:23:23.780
It's as there's been lots of discussion with the no-gil work about really what matters,

00:23:23.780 --> 00:23:28.140
what we need to care about, really what impact it's going to have.

00:23:28.140 --> 00:23:36.940
And I mean, it's probably going to have some impact on people with Python code, but it'll

00:23:36.940 --> 00:23:41.700
especially have impact on people that maintain extension modules.

00:23:41.700 --> 00:23:50.340
But it really is all the pain that comes with free threading.

00:23:50.340 --> 00:23:56.660
That's what it introduces with the benefits as well, of course.

00:23:56.660 --> 00:24:04.180
>> What's interesting, I like to think of subinterpreters kind of provide the same facility,

00:24:04.180 --> 00:24:10.980
but they force you to be explicit about what gets shared, and they force you to do it in

00:24:10.980 --> 00:24:12.460
a thread safe way.

00:24:12.460 --> 00:24:17.060
So you can't do it without thread safety.

00:24:17.060 --> 00:24:20.300
And so it's not an issue.

00:24:20.300 --> 00:24:28.260
It doesn't hurt that people really haven't used subinterpreters extensively up till now,

00:24:28.260 --> 00:24:31.980
whereas threads are kind of something that's been around for quite a while.

00:24:31.980 --> 00:24:33.660
>> Yeah, it has been.

00:24:33.660 --> 00:24:38.340
Well, subinterpreters have traditionally just been a thing you can do from C extensions

00:24:38.340 --> 00:24:45.260
or the C API, which really limits them from being used in just a standard, like, I'm working

00:24:45.260 --> 00:24:49.820
on my web app, so let's just throw in a couple of subinterpreters, you know?

00:24:49.820 --> 00:24:56.500
But in 3.13, is that when we're looking at having a Python level API for creating, interacting

00:24:56.500 --> 00:24:57.500
with?

00:24:57.500 --> 00:25:00.620
>> Yeah, I've been working on a PEP for that, PEP 554.

00:25:00.620 --> 00:25:08.580
I recently created a new PEP to replace that one, which is PEP 734.

00:25:08.580 --> 00:25:11.700
That's the one.

00:25:11.700 --> 00:25:16.660
So that's the one that I'm targeting for 3.13.

00:25:16.660 --> 00:25:19.620
And it's pretty straightforward.

00:25:19.620 --> 00:25:29.700
Create interpreters and kind of look at them and with an interpreter, run some code.

00:25:29.700 --> 00:25:31.260
You know, pretty basic stuff.

00:25:31.260 --> 00:25:36.900
And then also, because subinterpreters aren't quite so useful if you can't cooperate between

00:25:36.900 --> 00:25:45.940
them, there's also a queue type that, you know, you push stuff on and you pop stuff

00:25:45.940 --> 00:25:48.980
off and just pretty basic.

00:25:48.980 --> 00:25:54.540
>> So you could write something like, oh, wait, queue.pop or something like that.

00:25:54.540 --> 00:25:55.540
Excellent.

00:25:55.540 --> 00:25:57.180
>> Yeah, so.

00:25:57.180 --> 00:25:58.980
>> Yeah, this is really cool.

00:25:58.980 --> 00:26:03.500
And the other thing that I wanted to talk about here, looks like you already have it

00:26:03.500 --> 00:26:04.700
in the PEP, which is excellent.

00:26:04.700 --> 00:26:10.180
Somehow I missed that before, is that there's an, we have thread pool executors, we have

00:26:10.180 --> 00:26:16.140
multi-processing pool executors, and this would be an interpreter pool executor.

00:26:16.140 --> 00:26:19.260
What's the thinking there?

00:26:19.260 --> 00:26:22.420
>> People are already familiar with using concurrent futures.

00:26:22.420 --> 00:26:30.180
So if we can present the same API for subinterpreters, it makes it really easy because you can set

00:26:30.180 --> 00:26:35.540
it up with multi-processing or threads and switch it over to one of the other pool types

00:26:35.540 --> 00:26:37.020
without a lot of fuss.

00:26:37.020 --> 00:26:41.660
>> Right, basically with a clever import statement, you're good to go, right?

00:26:41.660 --> 00:26:47.100
From whatever import, like multi-processing pool executor as pool executor or interpreter

00:26:47.100 --> 00:26:50.860
pool executor as pool executor, and then the rest of the code could stay potentially.

00:26:50.860 --> 00:26:51.860
>> Yeah.

00:26:51.860 --> 00:26:52.860
What I expect.

00:26:52.860 --> 00:26:56.780
>> The communication, like you gotta, it's gotta kind of be a basic situation.

00:26:56.780 --> 00:26:57.780
>> Yeah.

00:26:57.780 --> 00:26:58.780
>> Right?

00:26:58.780 --> 00:26:59.780
Because there are assumptions.

00:26:59.780 --> 00:27:00.780
>> Yeah.

00:27:00.780 --> 00:27:07.860
And it should, it should work mostly the same way that you already use it with threads and

00:27:07.860 --> 00:27:08.860
multi-processing.

00:27:08.860 --> 00:27:11.740
But we'll see.

00:27:11.740 --> 00:27:18.660
There's some, some limitations with subinterpreters currently that I'm sure we'll work on solving

00:27:18.660 --> 00:27:21.700
as we can.

00:27:21.700 --> 00:27:23.380
So we'll see.

00:27:23.380 --> 00:27:29.580
It may not be quite as efficient as I'd like at first with the interpreter pool executor

00:27:29.580 --> 00:27:34.740
because we'll probably end up doing some pickling stuff, kind of like multi-processing does.

00:27:34.740 --> 00:27:35.740
>> Yeah.

00:27:35.740 --> 00:27:37.860
>> Although I expect it'll be a little more efficient.

00:27:37.860 --> 00:27:40.460
>> I was gonna, I was gonna save this for later, but I think maybe it's worth talking

00:27:40.460 --> 00:27:41.460
about now.

00:27:41.460 --> 00:27:45.620
So first of all, Anthony, you wrote a lot about and have actually had some recent influence

00:27:45.620 --> 00:27:50.900
on what you can pass across, say the starting code and then the running interpreter that's

00:27:50.900 --> 00:27:53.020
kind of like the subinterpreter doing extra work.

00:27:53.020 --> 00:27:56.660
Want to talk about like what data exchange there is?

00:27:56.660 --> 00:28:04.540
>> Yeah, so when you're using any of these models, multi-processing, subinterpreters

00:28:04.540 --> 00:28:09.860
or threading, I guess you've got two, three things to worry about.

00:28:09.860 --> 00:28:12.140
One is how do you create it in the first place?

00:28:12.140 --> 00:28:14.500
So how do you create a process?

00:28:14.500 --> 00:28:15.780
How do you create an interpreter?

00:28:15.780 --> 00:28:17.620
How do you create a thread?

00:28:17.620 --> 00:28:20.620
The second thing is how do you send data to it?

00:28:20.620 --> 00:28:25.480
Because normally the reason you've created them is because you need it to do some work.

00:28:25.480 --> 00:28:30.380
So you've got the code, which is, you know, when you spawn it, when you create it, the

00:28:30.380 --> 00:28:33.660
code that you want it to run, but that code needs some sort of input.

00:28:33.660 --> 00:28:37.380
And that's probably going to be Python objects.

00:28:37.380 --> 00:28:41.300
It might be reading files, for example, or listening to a network socket.

00:28:41.300 --> 00:28:45.060
So it might be getting the input from somewhere else.

00:28:45.060 --> 00:28:49.060
But typically you need to give it parameters.

00:28:49.060 --> 00:28:55.340
Now the way that works in multi-processing is mostly reliant on pickle.

00:28:55.340 --> 00:29:02.500
So if you start a process and you give it some data, either as a parameter or you create

00:29:02.500 --> 00:29:11.040
a queue and you send data down the queue or the pipe, for example, it pickles the data.

00:29:11.040 --> 00:29:15.580
So you can put a Python object in, it uses the pickle module, it converts that into a

00:29:15.580 --> 00:29:19.780
byte string and then it basically converts the byte string on the other end back into

00:29:19.780 --> 00:29:22.100
objects.

00:29:22.100 --> 00:29:27.420
That's got its limitations because not everything can be pickled.

00:29:27.420 --> 00:29:32.660
And also some objects, especially if you've got like an object which has got objects in

00:29:32.660 --> 00:29:38.100
it and, you know, it's deeply nested or you've got a big complicated dictionary or something

00:29:38.100 --> 00:29:43.540
that's got all these strange types in it, which can't necessarily be re-hydrated from

00:29:43.540 --> 00:29:46.980
just from a byte string.

00:29:46.980 --> 00:29:50.740
An alternative actually I do want to point out, because for people who come across this

00:29:50.740 --> 00:29:55.860
issue quite a lot, there's another package called dill on PyPI.

00:29:55.860 --> 00:30:00.100
So if you think of pickle, think of dill.

00:30:00.100 --> 00:30:02.060
Dill is very similar to pickle.

00:30:02.060 --> 00:30:10.420
It has the same interface, but it can pickle slightly more exotic objects than pickle can.

00:30:10.420 --> 00:30:14.860
So often if you find that you've tried to pickle something, you try to share it with

00:30:14.860 --> 00:30:20.580
a process or a sub-interpreter and it comes back and says this can't be pickled, you can

00:30:20.580 --> 00:30:25.260
try dill and see if that works.

00:30:25.260 --> 00:30:30.380
So yeah, that's the typical way of doing it is that you would pickle an object and then

00:30:30.380 --> 00:30:35.300
on the other end you would basically unpickle it back into another object.

00:30:35.300 --> 00:30:38.580
The downside of that is that it's pretty slow.

00:30:38.580 --> 00:30:44.460
It's equivalent like if you use the JSON module in Python, it's kind of similar, I guess,

00:30:44.460 --> 00:30:49.700
to converting something into JSON and then converting it from JSON back into a dictionary

00:30:49.700 --> 00:30:51.700
on the other end.

00:30:51.700 --> 00:30:54.820
Like it's not a super efficient way of doing it.

00:30:54.820 --> 00:30:58.260
So sub-interpreters have another mechanism.

00:30:58.260 --> 00:31:02.660
I haven't read PEP 734 yet.

00:31:02.660 --> 00:31:09.580
So I don't know how much of this is in the new PEP, Eric, or if it's in the queue.

00:31:09.580 --> 00:31:10.580
But there's a-

00:31:10.580 --> 00:31:11.580
It's much the same.

00:31:11.580 --> 00:31:12.580
Okay.

00:31:12.580 --> 00:31:13.580
It's much the same.

00:31:13.580 --> 00:31:20.980
So there's another mechanism with sub-interpreters because they share the same process, whereas

00:31:20.980 --> 00:31:23.460
multi-processing doesn't, they're separate processes.

00:31:23.460 --> 00:31:28.580
Because they share the same process, you can basically put some data in a memory space

00:31:28.580 --> 00:31:31.780
which can be read from a separate interpreter.

00:31:31.780 --> 00:31:35.000
Now you need to be, well, Python needs to be really careful.

00:31:35.000 --> 00:31:40.360
You don't need to worry too much about it because that complexity is done for you.

00:31:40.360 --> 00:31:43.600
But there are certain types of objects that you can put in as parameters.

00:31:43.600 --> 00:31:50.320
You can send either a startup variables for your sub-interpreter, or you can send via

00:31:50.320 --> 00:31:55.520
like a pipe basically backwards and forwards between the interpreters.

00:31:55.520 --> 00:32:02.600
And these are essentially all the immutable types for Python, which is like string Unicode

00:32:02.600 --> 00:32:09.840
strings, byte strings, bool, none, integer, float, and tuples.

00:32:09.840 --> 00:32:12.560
And you can do tuples of tuples as well.

00:32:12.560 --> 00:32:18.840
And it seems like the tuple part had to something that you added recently, right?

00:32:18.840 --> 00:32:22.160
It says I implemented tuple sharing just last week.

00:32:22.160 --> 00:32:24.320
Yeah, that was that's in now.

00:32:24.320 --> 00:32:26.000
I really wanted to use it.

00:32:26.000 --> 00:32:29.760
So I thought, well, instead of keep, I kept complaining that it wasn't there.

00:32:29.760 --> 00:32:35.120
So I thought instead of complaining, I might as well talk to Eric and work out how to implement

00:32:35.120 --> 00:32:36.120
it.

00:32:36.120 --> 00:32:40.000
But yeah, you can't share dictionaries as well.

00:32:40.000 --> 00:32:41.000
Yeah, exactly.

00:32:41.000 --> 00:32:44.440
So one thing that I thought that might be awesome, are you familiar with message spec?

00:32:44.440 --> 00:32:47.560
Have you guys seen message spec?

00:32:47.560 --> 00:32:55.440
It's like Pydantic in the sense that you create a class with types, but the parsing performance

00:32:55.440 --> 00:33:04.360
is quite a bit like much, much faster, 80 times faster than Pydantic, 10 times faster

00:33:04.360 --> 00:33:08.680
than Marshiro and Seatters and so on.

00:33:08.680 --> 00:33:13.920
And faster still even than say JSON or UJSON.

00:33:13.920 --> 00:33:20.040
So maybe it makes sense to use this turn into its serialization formats bytes, send the

00:33:20.040 --> 00:33:21.560
bytes over and then pull it back.

00:33:21.560 --> 00:33:23.000
I don't know, might give you a nice.

00:33:23.000 --> 00:33:24.520
Yeah, you can share byte string.

00:33:24.520 --> 00:33:31.640
So you can stick something into pickle, you can use like, msgspec or something like

00:33:31.640 --> 00:33:36.760
that to serialize something into a byte string and then receive it on the other end and rehydrate

00:33:36.760 --> 00:33:37.760
it.

00:33:37.760 --> 00:33:40.640
Or even Pydantic, like Pydantic is awesome as well.

00:33:40.640 --> 00:33:44.640
This is meant to be super fast with a little bit of less behavior, right?

00:33:44.640 --> 00:33:47.040
Yeah, so this is a kind of a design thing.

00:33:47.040 --> 00:33:51.720
I think people need to consider when they're like, great, I can run everything in parallel

00:33:51.720 --> 00:33:53.680
now.

00:33:53.680 --> 00:33:57.600
But you have to kind of unwind and think about how you designed your application.

00:33:57.600 --> 00:34:02.080
Like at which point do you fork off the work?

00:34:02.080 --> 00:34:05.760
And how do you split the data?

00:34:05.760 --> 00:34:09.680
You can't just kind of go into it assuming, oh, we'll just have a pool of workers.

00:34:09.680 --> 00:34:14.960
And we've kind of got this shared area of data that everybody just waits for.

00:34:14.960 --> 00:34:19.760
Right, I'll pass it a pointer to a million entry list and I'll just run with it.

00:34:19.760 --> 00:34:25.160
Yeah, because I mean, in any language, you're going to get issues if you do that.

00:34:25.160 --> 00:34:29.040
Even if you've got shared memory, and it's easier to read and write to the different

00:34:29.040 --> 00:34:31.400
spaces, you're going to get issues with locking.

00:34:31.400 --> 00:34:36.160
And I think it's also important with free threading, if you read the spec or kind of

00:34:36.160 --> 00:34:42.880
follow what's happening with free threading, it's not like the GILs disappeared.

00:34:42.880 --> 00:34:46.200
The GILs been replaced with other locks.

00:34:46.200 --> 00:34:50.920
So there are still going to be locks, you can't just have no locks.

00:34:50.920 --> 00:34:51.920
If you've got things running in parallel.

00:34:51.920 --> 00:34:52.920
Especially cross threads, right?

00:34:52.920 --> 00:34:57.760
So it moves some of the reference counting stuff into like, well, it's fast on the default

00:34:57.760 --> 00:35:01.440
thread, the same thread, but if it goes to another, it has to kick in another more thread

00:35:01.440 --> 00:35:04.520
safe case that potentially is slower and so on.

00:35:04.520 --> 00:35:05.520
Yeah.

00:35:05.520 --> 00:35:09.120
So yeah, the really important thing with sub interpreters is that they have their own,

00:35:09.120 --> 00:35:11.280
well, have their own GIL.

00:35:11.280 --> 00:35:14.120
So each one has its own lock.

00:35:14.120 --> 00:35:18.320
So they can run fully in parallel just as they could with multi processing.

00:35:18.320 --> 00:35:23.280
So I feel like a closer comparison with sub interpreters is multi processing.

00:35:23.280 --> 00:35:27.020
Yeah, because they basically run fully in parallel.

00:35:27.020 --> 00:35:31.000
If you start four of them and you have four cores, each core is going to be busy doing

00:35:31.000 --> 00:35:32.560
work.

00:35:32.560 --> 00:35:39.320
You start them, you give them data, you can interact with them whilst they're running.

00:35:39.320 --> 00:35:44.400
And then when they're finished, they can close and they can be destroyed and cleaned up.

00:35:44.400 --> 00:35:47.560
So it's much closer to multi processing.

00:35:47.560 --> 00:35:53.880
But the big, kind of the big difference is that the overhead both on the memory and CPU

00:35:53.880 --> 00:35:56.720
side of things is much smaller.

00:35:56.720 --> 00:35:59.240
Separate processes with multi processing are pretty heavyweight.

00:35:59.240 --> 00:36:02.060
They're big workers.

00:36:02.060 --> 00:36:07.760
And then the other thing that's pretty significant is the time it takes to start one.

00:36:07.760 --> 00:36:12.280
So starting a process with multi processing takes quite a lot of time.

00:36:12.280 --> 00:36:17.440
And it's significantly I think it's like 20 or 30 times faster to start a sub interpreter.

00:36:17.440 --> 00:36:20.480
You have a bunch of graphs for it somewhere.

00:36:20.480 --> 00:36:21.480
There we go.

00:36:21.480 --> 00:36:22.480
Yeah.

00:36:22.480 --> 00:36:27.200
So I scrolled past it.

00:36:27.200 --> 00:36:30.160
There we go.

00:36:30.160 --> 00:36:35.340
It's not exactly the same, but kind of captures a lot of it there.

00:36:35.340 --> 00:36:42.460
So one thing that I think is exciting, Eric, is the interpreter pool, sub interpreter pool,

00:36:42.460 --> 00:36:47.420
because a lot of the difference between the threading and the sub interpreter performance

00:36:47.420 --> 00:36:53.180
is that startup of the new arenas and like importing the standard library, all that kind

00:36:53.180 --> 00:36:55.140
of stuff that still is going to happen.

00:36:55.140 --> 00:36:59.700
But once that those things are loaded up in the process, they could be handed work easily.

00:36:59.700 --> 00:37:00.700
Right.

00:37:00.700 --> 00:37:04.020
And so if you've got a pool of, you know, like say that you have 10 cores, you've got

00:37:04.020 --> 00:37:07.620
10 of them just chilling or however many, you know, you've sort of done enough work

00:37:07.620 --> 00:37:13.780
to like do in parallel, then you could have them laying around and just send like, okay,

00:37:13.780 --> 00:37:14.940
now I want you to run this function.

00:37:14.940 --> 00:37:19.180
And now once you run this and that one means go call that API and then process it.

00:37:19.180 --> 00:37:23.840
And I think you could get the difference between threading and sub interpreters a lot lower

00:37:23.840 --> 00:37:28.140
by having them kind of reuse basically.

00:37:28.140 --> 00:37:30.140
Yep.

00:37:30.140 --> 00:37:32.140
Absolutely.

00:37:32.140 --> 00:37:34.140
Yeah.

00:37:34.140 --> 00:37:41.140
It's there's some of the, the, the key difference I think is mostly that when you have mutable

00:37:41.140 --> 00:37:45.820
data, whereas with threads, you can share it.

00:37:45.820 --> 00:37:51.060
So threads can kind of talk to each other through the data that they share with each

00:37:51.060 --> 00:37:52.700
other.

00:37:52.700 --> 00:37:57.620
Whereas with sub interpreters, there are a lot of restrictions and I expect we'll work

00:37:57.620 --> 00:38:00.140
on that to an extent.

00:38:00.140 --> 00:38:02.820
But it's also part of the programming model.

00:38:02.820 --> 00:38:08.060
And like Anthony was saying, if you really want to take advantage of parallelism, you

00:38:08.060 --> 00:38:09.060
need to think about it.

00:38:09.060 --> 00:38:15.420
You need to actually be careful about your data and how you're splitting up your work.

00:38:15.420 --> 00:38:19.820
I think there's going to be design patterns that we come to know or conventions we come

00:38:19.820 --> 00:38:25.620
to know, like, let's suppose I need some calculation and I'm, I'm going to use it in a for loop.

00:38:25.620 --> 00:38:28.900
You don't run the calculation if it's the same over and over every time through the

00:38:28.900 --> 00:38:30.980
loop, you run it and then you use the result.

00:38:30.980 --> 00:38:31.980
Right.

00:38:31.980 --> 00:38:35.420
So in this, you know, a similar thing here would be like, well, if you're going to process

00:38:35.420 --> 00:38:39.580
a bunch of data and the data comes from a say, a database, don't do the query and hand

00:38:39.580 --> 00:38:41.660
it all the records.

00:38:41.660 --> 00:38:45.020
Just tell it, go get that data from the database.

00:38:45.020 --> 00:38:47.180
That way it's already serialized in the right process.

00:38:47.180 --> 00:38:53.420
And there's not this, this cross serialization to either pickling or whatever mechanism you

00:38:53.420 --> 00:38:54.420
come up with.

00:38:54.420 --> 00:38:55.420
Right.

00:38:55.420 --> 00:38:58.300
So it's just a way to think about when you get the data, can you delay it until it's

00:38:58.300 --> 00:39:02.660
in the sub process and our sub interpreter rather and so on.

00:39:02.660 --> 00:39:03.660
Right.

00:39:03.660 --> 00:39:06.180
Yeah, definitely.

00:39:06.180 --> 00:39:16.140
One interesting thing is that PEP 734 I've included memory view as one of the types that's

00:39:16.140 --> 00:39:17.140
supported.

00:39:17.140 --> 00:39:23.260
So basically you can take a memory view of any kind of object that implements the buffer

00:39:23.260 --> 00:39:24.380
protocol.

00:39:24.380 --> 00:39:29.020
So like NumPy arrays and stuff like that.

00:39:29.020 --> 00:39:33.860
And pass that memory view through to another interpreter and you can use it and doesn't

00:39:33.860 --> 00:39:35.260
make a copy or anything.

00:39:35.260 --> 00:39:38.140
It actually uses the same underlying data.

00:39:38.140 --> 00:39:39.660
They actually get shared.

00:39:39.660 --> 00:39:41.100
Oh, that's interesting.

00:39:41.100 --> 00:39:42.100
Yeah.

00:39:42.100 --> 00:39:48.820
So there's, and I think there's even more room for that with other types, but we're

00:39:48.820 --> 00:39:50.700
starting small.

00:39:50.700 --> 00:40:00.500
But the key thing there is that, like you're saying, I mean, with coming up with different

00:40:00.500 --> 00:40:09.260
models and patterns and libraries, I'm sure they'll come up as people feel out really

00:40:09.260 --> 00:40:11.900
what's the easiest way to take advantage of these features.

00:40:11.900 --> 00:40:18.220
And that's the sort of thing that will apply not just to general free threaded like no

00:40:18.220 --> 00:40:20.740
kill, but also subinterpreters.

00:40:20.740 --> 00:40:21.740
Definitely.

00:40:21.740 --> 00:40:24.700
It's going to be exciting.

00:40:24.700 --> 00:40:29.860
So I guess I want to move on and talk about working with this in Python and the stuff

00:40:29.860 --> 00:40:35.140
that you've done, Anthony, but maybe a quick comment from the audience is Jazzy asks, is

00:40:35.140 --> 00:40:38.420
this built on top of a queue, which is built on top of a linked list because I'm building

00:40:38.420 --> 00:40:42.260
this and this, my research led me to these data structures.

00:40:42.260 --> 00:40:47.860
I guess that's the communication across subinterpreter cross interpreter communication.

00:40:47.860 --> 00:40:55.100
Yeah, it was subinterpreters like in PEP seven, three, four, it's a queue implements the same

00:40:55.100 --> 00:41:00.220
interfaces as the queue from the queue module.

00:41:00.220 --> 00:41:04.700
But there's, there's no reason why people couldn't implement whatever data structure

00:41:04.700 --> 00:41:08.100
they want for communicating between subinterpreters.

00:41:08.100 --> 00:41:14.140
And then that data structure is in charge of, of preserving thread safety and so forth.

00:41:14.140 --> 00:41:15.140
Yep.

00:41:15.140 --> 00:41:16.140
Excellent.

00:41:16.140 --> 00:41:17.500
Yeah, it's not a standard queue.

00:41:17.500 --> 00:41:20.740
It's like a concurrent queue or something along those lines.

00:41:20.740 --> 00:41:21.740
Yeah.

00:41:22.740 --> 00:41:23.740
All right.

00:41:23.740 --> 00:41:28.340
So all of this we've been talking about here is, you know, we're looking at this, this

00:41:28.340 --> 00:41:35.020
cool interpretable executor stuff that's in draft format, Anthony for three 13.

00:41:35.020 --> 00:41:39.980
And somehow I'm looking at this running Python parallel applications and subinterpreters

00:41:39.980 --> 00:41:40.980
that you wrote.

00:41:40.980 --> 00:41:41.980
What's going on here?

00:41:41.980 --> 00:41:42.980
How do you do this magic?

00:41:42.980 --> 00:41:49.100
You need to know the secret, secret password.

00:41:49.100 --> 00:42:01.580
So in Python, in Python three 12, the C API for creating subinterpreters was included.

00:42:01.580 --> 00:42:08.500
And a lot of the mechanism for, for creating subinterpreters was included.

00:42:08.500 --> 00:42:14.740
So there's also a, in, in CPython, there's a standard library, which I think everybody

00:42:14.740 --> 00:42:17.060
kind of knows.

00:42:17.060 --> 00:42:23.780
And then there are some like hidden modules, which are mostly used for testing.

00:42:23.780 --> 00:42:28.140
So not all of them get bundled, I think in the distribution, I think the, the, a lot

00:42:28.140 --> 00:42:31.740
of the test modules get taken out.

00:42:31.740 --> 00:42:35.860
But there are some hidden modules you can use for testing or, cause a lot of the test

00:42:35.860 --> 00:42:41.500
suite for CPython has to test C APIs and nobody really wants to write unit tests in

00:42:41.500 --> 00:42:47.700
C. So they write the tests in Python and then they kind of create this, these modules that

00:42:47.700 --> 00:42:49.620
basically just call the C functions.

00:42:49.620 --> 00:42:54.220
And so you can get the test coverage and do the testing from Python code.

00:42:54.220 --> 00:43:00.500
So I guess what was from PEP six, I can't remember.

00:43:00.500 --> 00:43:09.140
I look at too many PEPs six, six, Eric will probably know what is now PEP 734.

00:43:09.140 --> 00:43:16.780
But the Python interface to create subinterpreters, a version of that was included in three 12.

00:43:16.780 --> 00:43:22.420
So you can import this module called underscore X, X subinterpreters.

00:43:22.420 --> 00:43:27.180
And it's called underscore X, X cause it kind of indicates that it's experimental and it's

00:43:27.180 --> 00:43:30.300
underscore cause you probably shouldn't be using it.

00:43:30.300 --> 00:43:32.020
It's not safe for work to me.

00:43:32.020 --> 00:43:35.540
I mean, yeah, I don't know.

00:43:35.540 --> 00:43:43.860
But it provides a good way of people actually testing this stuff and seeing what happens

00:43:43.860 --> 00:43:48.420
if I import my C extension from a subinterpreter.

00:43:48.420 --> 00:43:54.780
So that's kind of some of what I've been doing is looking at, okay, what can we try and do

00:43:54.780 --> 00:43:56.940
in parallel?

00:43:56.940 --> 00:44:05.980
And this blog post, I wanted to try a, a WSGI or an ASGI web app.

00:44:05.980 --> 00:44:10.420
And the typical pattern that you have at the moment, and I guess how a lot of people would

00:44:10.420 --> 00:44:16.300
be using parallel code, but without really realizing it is when you deploy a web app

00:44:16.300 --> 00:44:24.820
for Django flask or FastAPI, you can't have one gil per web server because if you've got

00:44:24.820 --> 00:44:31.780
one gil per web server, you can only have one user per website, which is not great.

00:44:31.780 --> 00:44:38.700
So the way that most web servers implement this is that they have a pool of workers.

00:44:38.700 --> 00:44:45.340
G unicorn does that by spawning Python processes and then using the multi processing module.

00:44:45.340 --> 00:44:51.460
So it basically creates multiple Python processes, all listening to the same socket.

00:44:51.460 --> 00:44:56.060
And then when a web request comes in, one of them takes that request.

00:44:56.060 --> 00:45:00.420
It also then inside that has a thread pool.

00:45:00.420 --> 00:45:06.260
So even basically a thread pool is, is better for concurrent code.

00:45:06.260 --> 00:45:11.020
So G unicorn normally is used in a multi worker, multi thread model.

00:45:11.020 --> 00:45:13.100
That's how we kind of talk about it.

00:45:13.100 --> 00:45:17.780
So you'd have the number of workers that you have CPU cores, and then inside that you'd

00:45:17.780 --> 00:45:21.300
have multiple threads.

00:45:21.300 --> 00:45:25.060
So it kind of means you can handle more requests at a time.

00:45:25.060 --> 00:45:29.860
If you've got eight cores, you can handle at least eight requests at a time.

00:45:29.860 --> 00:45:36.440
However, because most web code can be concurrent on the back end, like you're making a database

00:45:36.440 --> 00:45:41.900
query or you're reading some stuff from a file like that, that doesn't necessarily need

00:45:41.900 --> 00:45:43.080
to hold the gill.

00:45:43.080 --> 00:45:47.980
So you can run it concurrently, which is why you have multiple threads.

00:45:47.980 --> 00:45:54.300
So even if you've only got eight CPU cores, you can actually handle 16 or 32 web requests

00:45:54.300 --> 00:45:55.940
at once.

00:45:55.940 --> 00:46:00.100
Because some of them will be waiting for the database server to finish running at SQL query

00:46:00.100 --> 00:46:05.640
or the API that it called to actually reply.

00:46:05.640 --> 00:46:10.500
So what I wanted to do with this experiment was to look at the multi worker, multi thread

00:46:10.500 --> 00:46:18.060
model for web apps and say, okay, could the worker be a sub interpreter?

00:46:18.060 --> 00:46:20.480
And like, what difference would that make?

00:46:20.480 --> 00:46:24.980
So instead of using multi processing for the workers, could I use sub interpreters for

00:46:24.980 --> 00:46:27.060
the workers?

00:46:27.060 --> 00:46:32.620
So even though this, the Python interface in 3.12 is experimental, they basically wanted

00:46:32.620 --> 00:46:40.740
to adapt Hypercorn, which is a web server for ASCII and WSGI apps in Python.

00:46:40.740 --> 00:46:45.660
Wanted to adapt Hypercorn and basically start Hypercorn workers from a sub interpreter pool

00:46:45.660 --> 00:46:51.760
and then seeing if I can run Django, Flask and FastAPI in a sub interpreter.

00:46:51.760 --> 00:46:57.500
So a single process, single Python process, but running across multiple cores and listening

00:46:57.500 --> 00:47:04.180
to web requests and basically running and serving web requests with multiple gills.

00:47:04.180 --> 00:47:05.180
So that was the, that was the task.

00:47:05.180 --> 00:47:09.180
>> In your article, you said you had started with a G unicorn and they just made too many

00:47:09.180 --> 00:47:15.340
assumptions about the multi processing, the web workers being truly sub processes, but

00:47:15.340 --> 00:47:17.860
Hypercorn was a better fit, you said, from Phil.

00:47:17.860 --> 00:47:22.420
>> Yeah, it was easier to implement this, this experiment in Hypercorn.

00:47:22.420 --> 00:47:25.860
It had like a single entry point.

00:47:25.860 --> 00:47:30.020
Because when you start an interpreter, when you start a sub interpreter, you need to import

00:47:30.020 --> 00:47:32.220
the modules that you want to use.

00:47:32.220 --> 00:47:37.100
You can't just, you can't just say, run this function over here.

00:47:37.100 --> 00:47:42.500
You can, but if that function relies on something else that you've imported, you need to import

00:47:42.500 --> 00:47:45.260
that from the new sub interpreter.

00:47:45.260 --> 00:47:52.300
So what I did with this experiment was basically start a sub interpreter that imports Hypercorn,

00:47:52.300 --> 00:47:55.820
listens to the sockets, and then is ready to serve web requests.

00:47:55.820 --> 00:47:56.820
>> Interesting.

00:47:56.820 --> 00:47:57.820
Okay.

00:47:57.820 --> 00:48:02.140
And at a minimum, you got it working, right?

00:48:02.140 --> 00:48:05.780
>> Yeah, it did a hello world.

00:48:05.780 --> 00:48:07.580
So we got that working.

00:48:07.580 --> 00:48:10.820
So I was pleased with that.

00:48:10.820 --> 00:48:13.700
And then kind of started doing some more testing of it.

00:48:13.700 --> 00:48:17.800
So how many concurrent requests can I make at once?

00:48:17.800 --> 00:48:18.940
How does it handle that?

00:48:18.940 --> 00:48:21.020
What does my CPU core load look like?

00:48:21.020 --> 00:48:24.860
Is it distributing it well?

00:48:24.860 --> 00:48:31.380
And then kind of some of the questions are, you know, how do you share data between the

00:48:31.380 --> 00:48:33.980
sub interpreters?

00:48:33.980 --> 00:48:41.200
So the minimum I had to do was each sub interpreter needs to know which web socket should I be

00:48:41.200 --> 00:48:42.200
listening to.

00:48:42.200 --> 00:48:47.780
So like which network socket, once I started, what port is it running on?

00:48:47.780 --> 00:48:49.180
And is it running on multiple ports?

00:48:49.180 --> 00:48:50.500
And which one should I listen to?

00:48:50.500 --> 00:48:54.140
So yeah, that's the first thing I had to do.

00:48:54.140 --> 00:48:55.140
>> Nice.

00:48:55.140 --> 00:49:00.420
Yeah, maybe just tell people real quick about just like what are the commands like at the

00:49:00.420 --> 00:49:05.700
Python level that you look at in order to create an interpreter, run some code on it,

00:49:05.700 --> 00:49:06.700
and so on?

00:49:06.700 --> 00:49:09.100
What's this weird world look like?

00:49:09.100 --> 00:49:14.340
>> Derek, do you want to cover that?

00:49:14.340 --> 00:49:16.660
>> Yeah, there is a whole lot.

00:49:16.660 --> 00:49:25.460
And if we talk about PEP 734, you have an interpreter's module with a create function

00:49:25.460 --> 00:49:29.700
in it that returns you an interpreter object.

00:49:29.700 --> 00:49:35.660
And then once you have the interpreter object, you'll have -- it has a function called run.

00:49:35.660 --> 00:49:41.340
Or a method -- the interpreter object also has a method called exec.

00:49:41.340 --> 00:49:45.780
I'm trying to remember what it is.

00:49:45.780 --> 00:49:50.740
Exec sync, because it's synchronous with the current thread.

00:49:50.740 --> 00:49:56.620
And whereas exec run will create a new thread for you and run things in that there.

00:49:56.620 --> 00:49:58.860
So they're kind of different use cases.

00:49:58.860 --> 00:50:00.420
But it's basically the same thing.

00:50:00.420 --> 00:50:02.020
You have some code.

00:50:02.020 --> 00:50:10.180
Currently supports -- just you give it a string with all your code on it, like you load it

00:50:10.180 --> 00:50:12.060
from a file or something.

00:50:12.060 --> 00:50:16.140
Basically it's a script that's going to run in that subinterpreter.

00:50:16.140 --> 00:50:20.300
Alternately, you can give it a function.

00:50:20.300 --> 00:50:25.420
And as long as that function isn't a closure, doesn't have any arguments and stuff like

00:50:25.420 --> 00:50:26.420
that.

00:50:26.420 --> 00:50:31.900
So it's just like really basic -- basically a script, right?

00:50:31.900 --> 00:50:36.220
If you got something like that, you can also pass that through.

00:50:36.220 --> 00:50:38.180
And then it runs it.

00:50:38.180 --> 00:50:41.060
And that's just about it.

00:50:41.060 --> 00:50:44.460
If you want to get some results back, you're going to have to manually pass them back,

00:50:44.460 --> 00:50:46.180
kind of like you do with threads.

00:50:46.180 --> 00:50:48.940
But that's something people already understand pretty well.

00:50:48.940 --> 00:50:49.940
>> Right.

00:50:49.940 --> 00:50:50.940
And create one of those channels.

00:50:50.940 --> 00:50:54.140
And then you just wait for it to exit and then read from the channel, something like

00:50:54.140 --> 00:50:55.140
that.

00:50:55.140 --> 00:50:56.140
>> Yeah.

00:50:57.140 --> 00:51:00.140
And so there's a way to say things like just run.

00:51:00.140 --> 00:51:02.780
And there's also a way to say create an interpreter.

00:51:02.780 --> 00:51:06.060
And then you could use the interpreter to do things.

00:51:06.060 --> 00:51:11.860
>> And that lets you only pay the process-like startup cost once, right?

00:51:11.860 --> 00:51:12.860
>> Yeah.

00:51:12.860 --> 00:51:13.860
Yeah.

00:51:13.860 --> 00:51:18.700
And you can also -- you can call that the run multiple times.

00:51:18.700 --> 00:51:22.340
And each time, it kind of adds on to what ran before.

00:51:22.340 --> 00:51:29.540
So if you run some code that modifies things or imports some modules and that sort of thing,

00:51:29.540 --> 00:51:33.820
those will still be there the next time you run some code in that interpreter.

00:51:33.820 --> 00:51:37.940
Which is nice, because then if you've got some startup stuff that you need to do one

00:51:37.940 --> 00:51:42.140
time, you can do that ahead of time right after you create the interpreter.

00:51:42.140 --> 00:51:47.660
But then in kind of your loop in your worker, then you run again, and all that stuff is

00:51:47.660 --> 00:51:48.660
ready to go.

00:51:48.660 --> 00:51:49.660
>> Oh, that's interesting.

00:51:49.660 --> 00:51:56.100
Because when I think about, say, my web apps, a lot of them talk to MongoDB and use Beanie.

00:51:56.100 --> 00:52:02.120
And you go to Beanie, and you tell it to create a connection -- or a MongoDB client pool.

00:52:02.120 --> 00:52:03.580
And it does all that stuff.

00:52:03.580 --> 00:52:05.340
And then you just talk to it.

00:52:05.340 --> 00:52:07.180
Like, go to that -- kind of like Django or whatever.

00:52:07.180 --> 00:52:10.080
Go to that class and do a query on it.

00:52:10.080 --> 00:52:14.440
You could run that startup code once, potentially, and have that pool just hanging around for

00:52:14.440 --> 00:52:16.300
subsequent work.

00:52:16.300 --> 00:52:17.300
Nice.

00:52:17.300 --> 00:52:20.180
All right.

00:52:20.180 --> 00:52:21.180
Let's see.

00:52:21.180 --> 00:52:22.700
Some more stuff.

00:52:22.700 --> 00:52:27.860
So you said you got it working pretty well, Anthony.

00:52:27.860 --> 00:52:31.900
And you said one of the challenges was trying to get it to shut down, right?

00:52:31.900 --> 00:52:32.900
>> Yeah.

00:52:32.900 --> 00:52:41.300
So in Python, when you start a Python process, you can press Control C to quit, which is

00:52:41.300 --> 00:52:44.300
a keyboard interrupt.

00:52:44.300 --> 00:52:49.660
That kind of sends the interrupt in that process.

00:52:49.660 --> 00:52:55.620
All of these web servers have got like a mechanism for cleanly shutting down.

00:52:55.620 --> 00:52:59.020
Because you don't want to just -- if you press Control C, you don't want to just terminate

00:52:59.020 --> 00:53:00.960
the processes.

00:53:00.960 --> 00:53:06.140
Because when you write an ASCII app in particular, you can have events that you can do.

00:53:06.140 --> 00:53:11.980
So people who have done FastAPI probably know the on event decorator that you can put and

00:53:11.980 --> 00:53:16.780
say when my app starts up, create a database connection pool.

00:53:16.780 --> 00:53:19.980
And when it shuts down, then go and clean up all this stuff.

00:53:19.980 --> 00:53:25.380
So if the web servers decided to shut down for whatever reason, whether you've pressed

00:53:25.380 --> 00:53:31.340
Control C or it just decided to close for whatever reason, it needs to tell all the

00:53:31.340 --> 00:53:34.740
workers to shut down cleanly.

00:53:34.740 --> 00:53:42.320
So signals, like the signals module doesn't work between subinterpreters because it kind

00:53:42.320 --> 00:53:47.020
of sits in the interpreter state, if you want to understand.

00:53:47.020 --> 00:53:54.260
So what I did was basically use a channel so that the main worker, like the coordinator,

00:53:54.260 --> 00:53:59.680
when that had a shutdown request, it would send a message to all of the subinterpreters

00:53:59.680 --> 00:54:03.020
to say, okay, can you stop now?

00:54:03.020 --> 00:54:06.500
And then it would kick off a job.

00:54:06.500 --> 00:54:12.060
Basically tell Hypercorn, in this case, to shut down cleanly, call any shutdown functions

00:54:12.060 --> 00:54:14.360
that you might have.

00:54:14.360 --> 00:54:16.780
And then log a message to say that it's shutting down as well.

00:54:16.780 --> 00:54:22.180
Because the other thing is, with web servers, if it just terminated immediately and then

00:54:22.180 --> 00:54:27.560
you looked at your logs and you were like, okay, why did the website suddenly stop working

00:54:27.560 --> 00:54:33.580
and there was no log entries, it just went from I'm handling requests to just absolute

00:54:33.580 --> 00:54:34.580
silence.

00:54:34.580 --> 00:54:36.500
That also wouldn't be very helpful.

00:54:36.500 --> 00:54:41.660
So it needs to write log messages, it needs to call shutdown functions and stuff.

00:54:41.660 --> 00:54:48.780
So what I did was, and this is I guess where it's kind of a bit of a turtles all the way

00:54:48.780 --> 00:54:54.900
down, but inside the subinterpreter, I start another thread.

00:54:54.900 --> 00:54:59.800
Because if you have a polar which listens to a signal on a channel, that's a blocking

00:54:59.800 --> 00:55:02.740
operation.

00:55:02.740 --> 00:55:08.380
So at the bottom of my subinterpreter code, I've got, okay, run Hypercorn.

00:55:08.380 --> 00:55:11.300
So it's going to run, it's going to listen to the socket, serve web requests.

00:55:11.300 --> 00:55:16.680
But I need to also be able to run concurrently in the subinterpreter, a loop which listens

00:55:16.680 --> 00:55:24.900
to the communication channel and sees if a shutdown request has been sent.

00:55:24.900 --> 00:55:30.860
So this is kind of maybe an implementation detail of how interpreters work in Python,

00:55:30.860 --> 00:55:33.780
but interpreters have threads as well.

00:55:33.780 --> 00:55:39.040
So you can start threads inside interpreters.

00:55:39.040 --> 00:55:43.580
So similar to what I said with gUnicorn and Hypercorn, how you've got multi-worker, multi-thread,

00:55:43.580 --> 00:55:46.080
like each worker has its own threads.

00:55:46.080 --> 00:55:49.140
In Python, interpreters have the threads.

00:55:49.140 --> 00:55:54.260
So you can start a subinterpreter, and then inside that subinterpreter, you can also start

00:55:54.260 --> 00:55:56.820
multiple threads.

00:55:56.820 --> 00:56:00.560
And you can do coroutines and all that kind of stuff as well.

00:56:00.560 --> 00:56:05.260
So basically what I did is to start a subinterpreter which also starts a thread, and that thread

00:56:05.260 --> 00:56:09.500
listens to the communication channel and then waits for a shutdown request.

00:56:09.500 --> 00:56:10.500
>> Right.

00:56:10.500 --> 00:56:13.500
Tells Hypercorn, all right, you're done.

00:56:13.500 --> 00:56:14.500
We're out of here.

00:56:14.500 --> 00:56:15.500
>> Yeah.

00:56:15.500 --> 00:56:16.500
>> So interesting.

00:56:16.500 --> 00:56:20.220
Here's an interesting question from the audience, from Chris as well.

00:56:20.220 --> 00:56:25.420
It says, "We talked about the global kind of startup, like if you run that one, is it

00:56:25.420 --> 00:56:27.180
already beset?

00:56:27.180 --> 00:56:33.140
And does that make code somewhat non-deterministic in a subinterpreter?

00:56:33.140 --> 00:56:34.860
If you explicitly work with it, no.

00:56:34.860 --> 00:56:37.040
But if you're doing the pool, which one do you get?

00:56:37.040 --> 00:56:39.100
Is it initialized or not?"

00:56:39.100 --> 00:56:46.820
Eric, do you have an idea of a startup function that runs in the interpreter pool executor

00:56:46.820 --> 00:56:47.820
type thing?

00:56:47.820 --> 00:56:55.220
Or is it just they get doled out and they run what they run?

00:56:55.220 --> 00:57:00.540
>> With concurrent features, it's already kind of a pattern.

00:57:00.540 --> 00:57:07.900
You have an initialized function that you can call that'll do the right thing.

00:57:07.900 --> 00:57:15.620
And then you have your work that actually -- your task that the worker's actually running.

00:57:15.620 --> 00:57:21.260
So with the -- I don't know.

00:57:21.260 --> 00:57:27.380
I wouldn't say it's non-deterministic unless you have no control over it.

00:57:27.380 --> 00:57:36.460
I mean, it's -- if you want to make sure that state progresses in an expected way, then

00:57:36.460 --> 00:57:39.620
you're going to run your own subinterpreters.

00:57:39.620 --> 00:57:43.260
But if you have no control over the subinterpreters, you're just handing off to some library that's

00:57:43.260 --> 00:57:51.700
using subinterpreters, I would think it would be somewhat not quite so important about whether

00:57:51.700 --> 00:57:54.140
it's deterministic or not.

00:57:54.140 --> 00:58:01.140
Each time it runs, there are a variety of things.

00:58:01.140 --> 00:58:08.820
The whole thing could be kind of reset or you could make sure that anything that runs

00:58:08.820 --> 00:58:17.940
-- any part of your code that runs is careful to keep its state self-contained and therefore

00:58:17.940 --> 00:58:21.620
preserve deterministic behavior that way.

00:58:21.620 --> 00:58:26.580
>> One thing I do a lot is I'll write code that'll say, you know, if this is already

00:58:26.580 --> 00:58:29.100
initialized, don't do it again.

00:58:29.100 --> 00:58:31.020
I talked about the database connection thing.

00:58:31.020 --> 00:58:35.180
If somebody were to call it twice, it'll say, well, looks like the connection's already

00:58:35.180 --> 00:58:38.260
not none, so we're good.

00:58:38.260 --> 00:58:42.220
You could just always run the startup code with one of these short circuit things that

00:58:42.220 --> 00:58:48.540
says, hey, it looks like on this interpreter, this is already done, we're good.

00:58:48.540 --> 00:58:53.500
That would probably handle a good chunk of it right there.

00:58:53.500 --> 00:58:55.780
But we're back to this thing that Anthony said, right?

00:58:55.780 --> 00:58:59.500
We're going to learn some new programming patterns potentially.

00:58:59.500 --> 00:59:02.820
Yeah, quite interesting.

00:59:02.820 --> 00:59:07.820
So we talked at the beginning about how subinterpreters have their own memory and their own module

00:59:07.820 --> 00:59:12.980
loads and all those kinds of things, and that might be potentially interesting for isolation.

00:59:12.980 --> 00:59:17.640
Also kind of tying back to Chris's comment here, this isolation is pretty interesting

00:59:17.640 --> 00:59:21.140
for testing, right, Anthony?

00:59:21.140 --> 00:59:23.020
Like pytest?

00:59:23.020 --> 00:59:28.660
So another thing you've been up to is working with trying to run pytest sessions in subinterpreters.

00:59:28.660 --> 00:59:31.780
Tell people about that.

00:59:31.780 --> 00:59:36.980
Yeah, so I started off with a web worker.

00:59:36.980 --> 00:59:43.260
One of the things I hit with a web worker was that I couldn't start Django applications

00:59:43.260 --> 00:59:49.580
and realized the reason was the datetime module.

00:59:49.580 --> 00:59:55.900
So the Python standard library, some of the modules are implemented in Python, some of

00:59:55.900 --> 01:00:01.340
them are implemented in C, some of them are a combination of both.

01:00:01.340 --> 01:00:06.900
So some modules you import in the standard library have like a C part that's been implemented

01:00:06.900 --> 01:00:11.740
in C for performance reasons typically, or because it needs some special operating system

01:00:11.740 --> 01:00:15.340
API that you can't access from Python.

01:00:15.340 --> 01:00:18.820
And then the front end is Python.

01:00:18.820 --> 01:00:26.300
So there is a list basically of standard library modules that are written in C that have some

01:00:26.300 --> 01:00:28.740
sort of global state.

01:00:28.740 --> 01:00:34.100
And then the core developers have been going down that list and fixing them up so that

01:00:34.100 --> 01:00:37.700
they can be imported from a subinterpreter.

01:00:37.700 --> 01:00:42.500
Or just marking them as not compatible with subinterpreters.

01:00:42.500 --> 01:00:48.740
One such example was the read line module that Eric and I were kind of working on last

01:00:48.740 --> 01:00:51.460
week and the week before.

01:00:51.460 --> 01:00:54.500
Read line is used for I guess listening to like user input.

01:00:54.500 --> 01:01:00.100
So if you run the input built in, read line is one of the utilities it uses to listen

01:01:00.100 --> 01:01:02.980
to keyboard input.

01:01:02.980 --> 01:01:07.740
If you start, let's say you started five subinterpreters at the same time and all of them did a read

01:01:07.740 --> 01:01:12.860
line listen for input, what would you expect the behavior to be?

01:01:12.860 --> 01:01:17.780
Which when you type in the keyboard, where would you expect the letters to come out?

01:01:17.780 --> 01:01:20.940
So it kind of poses an interesting question.

01:01:20.940 --> 01:01:26.500
So read line is not compatible with subinterpreters.

01:01:26.500 --> 01:01:29.700
But we discovered it was actually sharing a global state.

01:01:29.700 --> 01:01:35.860
So when it initialized, it would install like a callback.

01:01:35.860 --> 01:01:39.620
And what that meant was that even though it said it's not compatible, if you started multiple

01:01:39.620 --> 01:01:46.620
subinterpreters that imported read line, it would crash Python itself.

01:01:46.620 --> 01:01:52.100
The date time module is another one that needs fixing.

01:01:52.100 --> 01:01:55.180
It installs a bunch of global state.

01:01:55.180 --> 01:01:56.980
So yeah, date time was another one.

01:01:56.980 --> 01:02:03.740
So what I wanted to do is to try and test some other C extensions that I had and just

01:02:03.740 --> 01:02:12.140
basically write a pytest extension, pytest plugin, I guess, which you've got an existing

01:02:12.140 --> 01:02:17.220
pytest suite, but you want to run all of that in a subinterpreter.

01:02:17.220 --> 01:02:23.420
And the goal of this is really that you're developing a C extension, you've written a

01:02:23.420 --> 01:02:30.180
test suite already for pytest, and you want to run that inside a subinterpreter.

01:02:30.180 --> 01:02:36.500
So I'm looking at this from a couple of different angles, but I want to really try and use subinterpreters

01:02:36.500 --> 01:02:42.140
in other ways, import some C extensions that have never even considered the idea of subinterpreters

01:02:42.140 --> 01:02:45.740
and just see how they respond to it.

01:02:45.740 --> 01:02:48.100
Like read line was a good example.

01:02:48.100 --> 01:02:52.740
Like I think it was a, this won't work, but the fact that it crashed is-

01:02:52.740 --> 01:02:54.460
How is it going to crash, right?

01:02:54.460 --> 01:02:55.780
Like what's happening there?

01:02:55.780 --> 01:03:00.220
Yeah, so it should have kind of just said, this is not compatible.

01:03:00.220 --> 01:03:04.860
And I was kind of uncovered, and this is all super experimental as well.

01:03:04.860 --> 01:03:11.380
So like, this is not, you know, you've had to import the underscore XX module to even

01:03:11.380 --> 01:03:13.940
try this.

01:03:13.940 --> 01:03:18.900
So yeah, there's read line, date time was another one.

01:03:18.900 --> 01:03:23.100
And so I put this sort of pytest extension together so that I could run some existing

01:03:23.100 --> 01:03:26.380
test suites inside subinterpreters.

01:03:26.380 --> 01:03:34.620
And then the next thing that I looked at doing was, CPython has a huge test suite.

01:03:34.620 --> 01:03:45.300
So basically how all of Python itself is tested, the parser, the compiler, the evaluation loop,

01:03:45.300 --> 01:03:48.820
all the standard library modules have got pretty good test coverage.

01:03:48.820 --> 01:03:55.300
So like, when you compile Python from source, or you make changes on GitHub, like it runs

01:03:55.300 --> 01:04:00.060
the test suite to make sure that your changes didn't break anything.

01:04:00.060 --> 01:04:06.820
Now the next thing I kind of wanted to look at was, okay, can we, to try and kind of get

01:04:06.820 --> 01:04:11.060
ahead of the curve really on subinterpreter adoption.

01:04:11.060 --> 01:04:18.540
So in 3.13, when PEP 7.3.4 lands, can we try and test all of the standard library inside

01:04:18.540 --> 01:04:24.460
of subinterpreter and see if it has any other weird behaviors.

01:04:24.460 --> 01:04:29.780
And this test will probably apply to free threading as well, to be honest, because I

01:04:29.780 --> 01:04:36.580
think anything that you're doing like this, you're importing these C extensions, which

01:04:36.580 --> 01:04:39.620
always assumed that there was a big GIL in place.

01:04:39.620 --> 01:04:45.100
If you take away that assumption, then you get these strange behaviors.

01:04:45.100 --> 01:04:49.100
So yeah, the next thing I've been working on is basically running the CPython test

01:04:49.100 --> 01:04:55.220
suite inside subinterpreters and then seeing what kind of weird behaviors pop up.

01:04:55.220 --> 01:05:01.340
I think it's a great idea because obviously CPython is going to need to run code in a

01:05:01.340 --> 01:05:03.100
subinterpreter, run our code, right?

01:05:03.100 --> 01:05:08.820
So at a minimum, the framework, interpreter, all the runtime bits, that should all hang

01:05:08.820 --> 01:05:10.820
together, right?

01:05:10.820 --> 01:05:14.580
Yeah, there are some modules that it doesn't make sense to run in subinterpreters.

01:05:14.580 --> 01:05:17.180
Readline was an example.

01:05:17.180 --> 01:05:19.180
Some TKDrip maybe.

01:05:19.180 --> 01:05:20.180
Yeah, possibly.

01:05:20.180 --> 01:05:25.180
That may be not how it's heading out.

01:05:25.180 --> 01:05:30.700
If you think about like, when you're doing GUI programming, right, you're going to have

01:05:30.700 --> 01:05:37.660
kind of your core stuff running the main thread, right?

01:05:37.660 --> 01:05:42.980
And then you hand off, you may have subthreads doing some other work, but the core of the

01:05:42.980 --> 01:05:45.820
application, think of it as running in the main thread.

01:05:45.820 --> 01:05:48.420
I think of applications in that way.

01:05:48.420 --> 01:05:54.420
And there are certain things that you do in Python, standard library modules that really

01:05:54.420 --> 01:05:58.240
only make sense with that main thread.

01:05:58.240 --> 01:06:03.540
So supporting those in subinterpreters isn't quite as meaningful.

01:06:03.540 --> 01:06:04.540
Yeah.

01:06:04.540 --> 01:06:09.580
I can't remember all the details, but I feel like there are some parts of Windows itself,

01:06:09.580 --> 01:06:13.780
some UI frameworks there that required that you access them on the main program thread,

01:06:13.780 --> 01:06:18.300
not on some background thread as well, because it'd freak things out.

01:06:18.300 --> 01:06:19.980
So it seems like not unreasonable.

01:06:19.980 --> 01:06:21.420
Yeah, same is true.

01:06:21.420 --> 01:06:27.580
Like the signal module, I remember at exit, a few others.

01:06:27.580 --> 01:06:28.580
Excellent.

01:06:28.580 --> 01:06:30.580
All right.

01:06:30.580 --> 01:06:33.700
Well, I guess let's, we're getting short on time.

01:06:33.700 --> 01:06:35.500
Let's wrap it up with this.

01:06:35.500 --> 01:06:43.260
So the big thing to keep an eye on really here is PEP 734, because that's when this

01:06:43.260 --> 01:06:44.260
would land.

01:06:44.260 --> 01:06:49.700
This is, you're no longer with the underscore XX subinterpreter.

01:06:49.700 --> 01:06:52.180
You're just working with the interpreter's submodule.

01:06:52.180 --> 01:06:53.740
Yeah, 3.13.

01:06:53.740 --> 01:06:54.740
Yeah.

01:06:54.740 --> 01:06:57.020
So right now it's in draft.

01:06:57.020 --> 01:06:59.020
What's it looking like?

01:06:59.020 --> 01:07:04.020
If it'll be in 3.13, it'll be in 3.13 alpha something, some beta something.

01:07:04.020 --> 01:07:09.340
Like when is this going to start looking like a thing that is ready for people to play with?

01:07:09.340 --> 01:07:16.340
So I, yeah, this PEP, I went through and did a massive cleanup of PEP 554, which is why

01:07:16.340 --> 01:07:18.740
I made a new PEP for it.

01:07:18.740 --> 01:07:23.780
And simplified a lot of things, clarified a lot of points, had lots of good feedback

01:07:23.780 --> 01:07:30.220
from people and ended up with what I think is a good API, but it was a little different

01:07:30.220 --> 01:07:31.220
in some ways.

01:07:31.220 --> 01:07:37.940
So I've had the implementation for PEP 554 mostly done and ready to go for years.

01:07:37.940 --> 01:07:44.540
And so it was a matter, it's been a matter of now that I have this updated PEP up, going

01:07:44.540 --> 01:07:51.060
back to the implementation, tweaking it to match, and then making sure everything still

01:07:51.060 --> 01:07:52.940
feels right.

01:07:52.940 --> 01:07:55.200
Try and use it in a few cases.

01:07:55.200 --> 01:08:00.620
And if everything looks good, then go ahead and I'll start a discussion on that.

01:08:00.620 --> 01:08:06.140
I'm hoping within the next week or two to start up a round of discussion about this

01:08:06.140 --> 01:08:07.140
PEP.

01:08:07.140 --> 01:08:12.300
And hopefully we won't have a whole lot of back and forth so I can get this over to the

01:08:12.300 --> 01:08:15.460
steering councils in the near future.

01:08:15.460 --> 01:08:18.900
>> Well, the hard work has been done already, right?

01:08:18.900 --> 01:08:23.060
The C layer is there and it's accepted and it's in there.

01:08:23.060 --> 01:08:27.980
Now it's just a matter of what's the right way to look at it from Python, right?

01:08:27.980 --> 01:08:34.940
>> And one thing to keep in mind is that I'm planning on backporting the module to Python

01:08:34.940 --> 01:08:39.900
3.12 just so that we have a per interpreter GIL in 3.12.

01:08:39.900 --> 01:08:42.780
So it'd be nice if people could really take advantage of it.

01:08:42.780 --> 01:08:43.780
>> I see.

01:08:43.780 --> 01:08:47.020
So for that one, we'd have to pip install it or would it be added as?

01:08:47.020 --> 01:08:48.020
>> Yeah, pip install.

01:08:48.020 --> 01:08:49.020
>> Okay.

01:08:49.020 --> 01:08:52.540
>> I probably won't support before 3.12.

01:08:52.540 --> 01:08:58.740
I mean, subinterpreters have been around for decades, but only through the C API.

01:08:58.740 --> 01:09:04.620
But that said, I doubt I'll backport this module past 3.12.

01:09:04.620 --> 01:09:08.100
So just 3.12 and up.

01:09:08.100 --> 01:09:09.380
>> That's more than I expected anyway.

01:09:09.380 --> 01:09:11.060
So that's pretty cool.

01:09:11.060 --> 01:09:14.700
All right, final thoughts, you guys.

01:09:14.700 --> 01:09:19.140
What do you want to tell people about this stuff?

01:09:19.140 --> 01:09:24.140
>> Personally, I'm excited for where everything's going.

01:09:24.140 --> 01:09:29.540
It's taken a while, but I think we're getting to a good place.

01:09:29.540 --> 01:09:34.020
It's interesting with all the discussion about no GIL, it's easy to think, oh, then why do

01:09:34.020 --> 01:09:36.500
we need subinterpreters?

01:09:36.500 --> 01:09:39.020
Or if we have subinterpreters, why do we need no GIL?

01:09:39.020 --> 01:09:42.700
But they're kind of different needs.

01:09:42.700 --> 01:09:48.940
The most interesting thing for me is that what's good for no GIL is good for subinterpreters

01:09:48.940 --> 01:09:50.660
and vice versa.

01:09:50.660 --> 01:09:57.220
That no GIL probably really wouldn't be possible without a lot of the work that we've done

01:09:57.220 --> 01:10:00.460
to make a per-interpreter GIL possible.

01:10:00.460 --> 01:10:04.740
So I think that's one of the neat things.

01:10:04.740 --> 01:10:09.220
The future's looking bright for Python multi-core.

01:10:09.220 --> 01:10:14.740
And I'm excited to see where people go with all these things that we're adding.

01:10:14.740 --> 01:10:25.580
>> Anthony, when's the subinterpreter programming design patterns book coming out?

01:10:25.580 --> 01:10:30.740
>> My thoughts are -- subinterpreters are mentioned in my book, actually, when it was

01:10:30.740 --> 01:10:36.820
like Python 3.9, I think.

01:10:36.820 --> 01:10:42.380
Because it was possible then, but it's changed quite a lot since.

01:10:42.380 --> 01:10:49.460
I guess some thoughts to leave people with, I think if you're a maintainer of a Python

01:10:49.460 --> 01:10:56.100
package or a C extension module in a Python package, there's going to be a lot more exotic

01:10:56.100 --> 01:11:01.900
scenarios for you to test coming in the next year or so.

01:11:01.900 --> 01:11:07.820
And some of those uncover things that you might have done or just kind of relied on

01:11:07.820 --> 01:11:13.580
the GIL with global state, where that's not really desirable anymore and you're going

01:11:13.580 --> 01:11:15.820
to get bugs down the line.

01:11:15.820 --> 01:11:19.900
So I think with any of that stuff as a package maintainer, you want to test as many scenarios

01:11:19.900 --> 01:11:25.660
as you can so that you can catch bugs and fix them before your users find them.

01:11:25.660 --> 01:11:29.220
So if you are a package maintainer, there's definitely some things that you can start

01:11:29.220 --> 01:11:32.620
to look at now to test.

01:11:32.620 --> 01:11:40.700
It's available in 3.13 alpha 2 is at least probably the one I've tried, to be honest.

01:11:40.700 --> 01:11:46.580
And if you're a developer, not necessarily a maintainer, then I think this is a good

01:11:46.580 --> 01:11:56.740
time to start reading up on parallel programming and how you need to design parallel programs.

01:11:56.740 --> 01:12:00.740
And those kinds of concepts are the same across all languages.

01:12:00.740 --> 01:12:02.820
And Python would be no different.

01:12:02.820 --> 01:12:07.700
We just have different mechanisms for starting parallel work and joining it back together.

01:12:07.700 --> 01:12:13.180
But if you're interested in this and you want to run more code in parallel, and there's

01:12:13.180 --> 01:12:21.380
definitely some stuff to read and some stuff to learn about in terms of signals, pipes,

01:12:21.380 --> 01:12:28.780
queues, sharing data, how you have locks and where you should put them, how deadlocks can

01:12:28.780 --> 01:12:30.740
occur, things like that.

01:12:30.740 --> 01:12:33.140
So all of that stuff is the same in Python as anywhere else.

01:12:33.140 --> 01:12:35.820
We just have different mechanisms for doing it.

01:12:35.820 --> 01:12:37.100
All right.

01:12:37.100 --> 01:12:40.020
Well, people have some research work.

01:12:40.020 --> 01:12:45.060
And I guess a really, really quick final question, Eric, and then we'll wrap this up.

01:12:45.060 --> 01:12:48.800
Following up on what Anthony said, like test your stuff, make sure it works in a subinterpreter.

01:12:48.800 --> 01:12:53.300
If for some reason you're like, my code will not work in a subinterpreter and I'm not ready

01:12:53.300 --> 01:12:57.860
yet, is there a way to determine that your code is being run in a subinterpreter rather

01:12:57.860 --> 01:13:00.980
than regularly from your Python code?

01:13:00.980 --> 01:13:01.980
Yeah.

01:13:01.980 --> 01:13:10.340
If you have an extension module that supports subinterpreters, then you will have updated

01:13:10.340 --> 01:13:15.140
your module to use what's called multi-phase init.

01:13:15.140 --> 01:13:18.860
And that's something that shouldn't be too hard to look up.

01:13:18.860 --> 01:13:22.420
I think I talked about it in the pep.

01:13:22.420 --> 01:13:27.380
If you implement multi-phase init, then you've already done most of the work to support a

01:13:27.380 --> 01:13:34.020
subinterpreter.

01:13:34.020 --> 01:13:39.460
If you haven't, then your module can't be imported in a subinterpreter.

01:13:39.460 --> 01:13:44.500
It'll actually fail with an import error if you try and import it in a subinterpreter,

01:13:44.500 --> 01:13:48.300
or at least a subinterpreter that has its own GIL.

01:13:48.300 --> 01:13:53.020
There are ways to create subinterpreters that still share a GIL and that sort of thing.

01:13:53.020 --> 01:13:58.460
But you just won't be able to import it at all.

01:13:58.460 --> 01:14:04.900
So like the readline module can't be imported in subinterpreters.

01:14:04.900 --> 01:14:13.540
The issue that Anthony ran into is kind of a subtle side effect of the check that we're

01:14:13.540 --> 01:14:17.060
doing.

01:14:17.060 --> 01:14:24.060
But really it boils down to if you don't implement multi-phase init, then you won't

01:14:24.060 --> 01:14:26.300
be able to import the module.

01:14:26.300 --> 01:14:27.500
You'll just get an import error.

01:14:27.500 --> 01:14:30.100
So that's-- I mean, it makes it kind of straightforward.

01:14:30.100 --> 01:14:32.040
>>Victor: Yeah, sounds good.

01:14:32.040 --> 01:14:33.040
More opt-in than opt-out.

01:14:33.040 --> 01:14:34.040
>>Michael: Yep.

01:14:34.040 --> 01:14:35.040
>>Victor: Right on.

01:14:35.040 --> 01:14:36.040
All right, guys.

01:14:36.040 --> 01:14:38.660
Thank you both for coming back on the show.

01:14:38.660 --> 01:14:40.020
And awesome work.

01:14:40.020 --> 01:14:43.460
This is looking close to the finish line and exciting.

01:14:43.460 --> 01:14:44.860
>>Anthony: Thanks, Michael.

01:14:44.860 --> 01:14:45.860
>>Victor: Yep.

01:14:45.860 --> 01:14:46.860
See y'all.

01:14:46.860 --> 01:14:47.620
>> Okay.

