WEBVTT

00:00:01.300 --> 00:00:03.320
Sydney, welcome back to Talk Python to Me.

00:00:03.800 --> 00:00:04.480
Fantastic to have you here.

00:00:05.200 --> 00:00:05.320
Thanks.

00:00:05.840 --> 00:00:06.840
Super excited to be back.

00:00:07.740 --> 00:00:08.980
I'm excited to have you back.

00:00:09.480 --> 00:00:11.100
We always get to talk about cool things.

00:00:12.000 --> 00:00:14.840
And for a long time, that's been Pydantic.

00:00:15.280 --> 00:00:19.120
And the last time you were on, we talked about Pydantic performance tips.

00:00:20.640 --> 00:00:24.080
And now it's the topic du jour, really.

00:00:25.359 --> 00:00:44.140
The building with LLMs, which I think is really, really different than saying I use an LLM to build my code or I use LLMs to answer questions or kind of an alternative search but actually using it as a library style in your code right

00:00:44.140 --> 00:00:59.500
yeah definitely I think you can kind of turn things inside out right and like thinking about all the things that you can build with LLM specifically with link graph which is what we're going to chat about today is really exciting.

00:01:00.140 --> 00:01:28.720
Yeah, it is. Also tricky because as you know, as everyone knows who's played with them, LLMs are weirdly not deterministic. And so programs typically like determinism. They like, if I call this function, it's going to do a thing. If I don't call the function, it's not going to do the thing or whatever, right? So that's a big challenge. And I think that's part of what your old toolchain is meant to do.

00:01:29.440 --> 00:01:37.180
Now, just because you've been on the show a couple times, there's probably still people out there who don't know who you are, and your role has changed a little bit.

00:01:37.640 --> 00:01:41.880
How about a quick introduction and see yourself again and what you're doing these days?

00:01:41.960 --> 00:01:44.820
I know you've made a career transition recently.

00:01:45.500 --> 00:01:45.900
Yeah.

00:01:47.280 --> 00:01:48.480
So my name is Sydney.

00:01:48.720 --> 00:01:49.200
Nice to meet folks.

00:01:50.300 --> 00:01:52.500
I am a passionate open source developer.

00:01:53.380 --> 00:02:00.420
So on previous episodes, I was on talking about Pydantic and doing kind of open source maintenance there.

00:02:01.060 --> 00:02:06.220
So Pydantic is both an open source tool, and now there's a company built around that open source tool.

00:02:07.820 --> 00:02:10.520
And I recently switched over to LangChain.

00:02:10.820 --> 00:02:14.680
I'm excited to be working with an open source team in person in Boston.

00:02:15.480 --> 00:02:16.760
So that's quite exciting.

00:02:18.400 --> 00:02:21.360
And I'm mostly working on maintaining LangGraph now.

00:02:21.540 --> 00:02:26.500
So many folks are probably familiar with LangChain, the open source package.

00:02:26.650 --> 00:02:28.300
We'll probably get into that a little bit more soon.

00:02:29.240 --> 00:02:33.160
But LangGraph is kind of our newer agent orchestration framework.

00:02:34.620 --> 00:02:43.400
Other than that, yeah, I love just engaging with folks in the open source community, getting excited about building, you know, cooler AI applications now.

00:02:43.640 --> 00:02:47.240
And I don't know, in my free time, I'm quite the podcast addict.

00:02:47.540 --> 00:02:50.540
So it's always nice to listen to your other podcasts.

00:02:50.980 --> 00:03:00.800
Oh, thank you. Well, I know you worked on some cool projects, and this certainly is another one of them. I didn't realize they were based in Boston. That's cool.

00:03:01.260 --> 00:03:13.720
Yeah, so the main office is actually in San Francisco, but the company's growing pretty fast. So we now have a Boston office as of last week. So I've got my nice little call booth background here, and then a New York office as well.

00:03:14.360 --> 00:03:18.580
Is it off that 128 highway there where a lot of the tech companies are?

00:03:19.820 --> 00:03:24.780
I'm actually not super familiar with San Francisco geography.

00:03:25.760 --> 00:03:26.740
No, no, I'm taking Boston

00:03:26.740 --> 00:03:26.960
up

00:03:26.960 --> 00:03:27.720
by like Woburn.

00:03:28.520 --> 00:03:29.180
Oh, oh, gotcha.

00:03:29.270 --> 00:03:30.820
Yeah, we're actually in Harvard Square.

00:03:31.540 --> 00:03:31.780
Okay.

00:03:32.220 --> 00:03:32.680
Oh, very cool.

00:03:33.420 --> 00:03:34.700
Yeah, proper Boston.

00:03:35.380 --> 00:03:35.440
Yes.

00:03:35.820 --> 00:03:38.620
Not Boston suburbs where a lot of those companies are out there.

00:03:39.120 --> 00:03:39.240
Yeah.

00:03:39.480 --> 00:03:40.240
Yeah, really cool.

00:03:40.370 --> 00:03:45.240
So let's just start by talking about AI a little bit.

00:03:46.900 --> 00:03:50.720
I know some people are AI accelerators.

00:03:51.620 --> 00:03:58.420
People who believe that AI should be just pushed as fast as it can be, and I suspect a lot of folks like that can be found on the playing chain.

00:03:59.660 --> 00:04:02.200
Also, people can be very hesitant to it.

00:04:02.700 --> 00:04:05.200
I know some people are like, I'm not using it.

00:04:05.210 --> 00:04:06.040
I won't use it.

00:04:06.540 --> 00:04:13.280
I'm not sending my data to the big cloud companies or whatever to train their models to take my job or however people perceive this.

00:04:13.880 --> 00:04:16.900
I'm certainly pretty, I see a ton of value in it.

00:04:17.000 --> 00:04:20.980
I think it's really quite incredible.

00:04:21.739 --> 00:04:27.260
But, you know, first, what are your thoughts on kind of using LLMs in general?

00:04:27.470 --> 00:04:30.200
And then two, what is an agent?

00:04:31.320 --> 00:04:32.560
Yeah, great question.

00:04:32.750 --> 00:04:37.300
So I think I definitely fall more in the, like, accelerator side of things.

00:04:37.960 --> 00:04:53.580
But with, you know, guardrails and constraints and rules and boundaries, which is sort of what we're trying to do with LandGraph, I think it can be really helpful as to use, you know, LLMs as a development tool.

00:04:54.210 --> 00:05:01.060
So I personally do that when I'm writing code. I also think it's really helpful to brainstorm with the help of LLMs.

00:05:02.680 --> 00:05:04.560
And yeah, so I think they're like increasingly valuable.

00:05:04.610 --> 00:05:09.800
And as long as we're like being careful about usage, then it's great to kind of lean into their strengths.

00:05:11.790 --> 00:05:16.480
Yeah, Reuven Lerner, I just had him on the show a couple episodes ago.

00:05:16.630 --> 00:05:21.300
And he says he uses the reverse Socratic method with LLMs where

00:05:21.300 --> 00:05:21.720
he writes

00:05:21.720 --> 00:05:27.720
the code, then he asks it to, he sort of interrogates it like, well, what's wrong with my code?

00:05:27.870 --> 00:05:28.620
What could be better?

00:05:28.800 --> 00:05:29.600
And when would I use it?

00:05:29.600 --> 00:05:30.520
And what am I overlooking?

00:05:32.240 --> 00:05:33.340
But there's so many uses.

00:05:33.580 --> 00:05:33.840
It's wild.

00:05:34.380 --> 00:05:35.640
Yeah, it's crazy.

00:05:35.840 --> 00:05:41.140
I think sometimes I'll talk to my parents and be like, oh, I used ChatGPT to help me come up with a recipe.

00:05:41.200 --> 00:05:42.000
And they're like, you did what?

00:05:42.360 --> 00:05:43.560
I didn't even know you could do that.

00:05:45.020 --> 00:05:45.660
Whereas folks

00:05:45.660 --> 00:05:50.600
at work are interacting with AI on the hour.

00:05:52.240 --> 00:05:54.860
So I'll talk a little bit more about what are our agents.

00:05:55.340 --> 00:05:56.720
Agents are a little bit hard to define.

00:05:57.220 --> 00:05:59.360
I think it's kind of an overloaded term.

00:06:00.200 --> 00:06:05.060
But many people are kind of leading into that as the future of AI from a developer perspective.

00:06:05.380 --> 00:06:24.380
So I think about agents as tools that can reason and make decisions for you, specifically LLMs that are powered with things like tool calling and memory and context and things like that.

00:06:24.390 --> 00:06:27.180
So it's basically a super powered LLM.

00:06:27.220 --> 00:06:35.160
Like you're jumping from a simple like chat based model for an LOM to how can you use an LOM in an application?

00:06:37.160 --> 00:06:37.240
Yeah.

00:06:38.190 --> 00:06:38.380
Yeah.

00:06:39.370 --> 00:06:39.520
Okay.

00:06:39.870 --> 00:06:49.240
So it can call tools like you could give it a request and maybe it's going to go and call just like web searches or read web pages.

00:06:49.370 --> 00:06:54.560
You can tell it go read the docs and then do this thing or something along those lines.

00:06:55.340 --> 00:06:55.500
Right.

00:06:55.580 --> 00:07:00.140
So, you know, LLMs are trained on older data.

00:07:00.300 --> 00:07:08.480
And so if you're not enabling your LLMs to use kind of real world data and information, then their utility is limited.

00:07:09.400 --> 00:07:14.380
But agents have the power to use tools, which are basically like API calls.

00:07:15.660 --> 00:07:23.380
Or it can even be more simple than that, like running some code in order to incorporate your like real time relevant data.

00:07:24.260 --> 00:07:24.460
Yeah.

00:07:24.680 --> 00:07:29.480
And it can also kind of make decisions about like, which tools do I want to use and that sort of thing.

00:07:31.100 --> 00:07:32.060
Yeah, so interesting.

00:07:33.979 --> 00:07:37.100
I guess it's related to deep research a little bit, right?

00:07:37.300 --> 00:07:50.020
Like a lot of the deep research things seem to be kind of using tools and using the modern web and other stuff that they can find rather than just whatever the model was when it was constructed.

00:07:50.840 --> 00:07:51.460
Yeah, definitely.

00:07:51.860 --> 00:07:55.520
I think about perplexity is kind of gaining in popularity.

00:07:55.780 --> 00:08:11.380
And it's really good at you can ask it a question, and then it does real-time research against looking things up on the internet and then provides you back with citations, which is a really exciting feature, I think, for folks who are like, you know, LLMs can just make anything up.

00:08:11.420 --> 00:08:14.640
Like, how do I know this is being pulled from reasonable sources?

00:08:14.840 --> 00:08:16.880
And that's kind of an exciting jump in that

00:08:16.880 --> 00:08:17.100
space.

00:08:17.600 --> 00:08:17.900
It is.

00:08:18.240 --> 00:08:30.960
And I think it also goes a little ways, not that far, but a little ways to fill that gap where people said, you know, these search engines, they just take our data and they give us no credit.

00:08:31.730 --> 00:08:31.860
Right?

00:08:32.360 --> 00:08:32.599
Yeah.

00:08:33.219 --> 00:08:35.060
Which I'm sympathetic to.

00:08:35.450 --> 00:08:42.820
However, you know, I do think it says here's the four places I found and here's the video you should watch if you want to go deeper.

00:08:42.930 --> 00:08:44.540
I think that's actually super valuable.

00:08:45.040 --> 00:08:47.860
I just asked perplexity a historical question.

00:08:49.520 --> 00:09:13.000
like what is what is the age of these people who did this thing you know hundreds of years ago and it's it said in their work and it wrote Python code and then it gave me the answer it was probably doing I think it was doing you know pandas type stuff but it was thinking thinking it shows you what it's doing and it just says writing Python and then boom out comes the answer I'm like okay that's weird

00:09:13.000 --> 00:09:14.580
oh man was it correct

00:09:14.980 --> 00:09:16.280
yeah it was correct I

00:09:16.280 --> 00:09:16.680
think Wow.

00:09:19.560 --> 00:09:19.920
Wild.

00:09:20.500 --> 00:09:25.280
But that's kind of the stuff we're talking about a little bit, right, in this agentic sense.

00:09:25.330 --> 00:09:27.120
It's not just like, well, I went to my database.

00:09:27.290 --> 00:09:28.260
These are answers I have.

00:09:28.290 --> 00:09:34.600
But it's like, well, if I can make this web query and I could write some pandas, then I could format it the way you asked for it, right?

00:09:35.260 --> 00:09:35.800
Yeah, yeah.

00:09:35.920 --> 00:09:39.140
There's definitely a lot more kind of reasoning and thinking.

00:09:40.680 --> 00:09:44.260
It is a bit of a black box right behind what is the LLM doing.

00:09:44.500 --> 00:09:49.440
So you got a bit of transparency there with the like writing Python code clue.

00:09:49.760 --> 00:09:51.080
But yeah.

00:09:51.460 --> 00:09:52.700
I wish there was a view source.

00:09:53.150 --> 00:09:53.700
You know what I mean?

00:09:53.800 --> 00:09:53.980
Yeah.

00:09:54.780 --> 00:09:54.860
Yeah.

00:09:55.980 --> 00:10:24.700
I think one of the things that we're trying to do at LinkChain right now with LandGraph development is to kind of bridge the gap between like, what are the parts of your system that you really need to like understand and kind of be guaranteed are reliable and will always run in the same way versus is like where can you afford to introduce non-determinism in favor of like a bit more maybe agency or utility, but also introducing risk like who knows what your LLM is doing in the research phase.

00:10:25.100 --> 00:10:25.560
Yeah.

00:10:26.720 --> 00:10:27.780
It's just so powerful.

00:10:28.380 --> 00:10:34.580
But it's also so scary as a developer to think, I can't debug this.

00:10:34.600 --> 00:10:37.280
I can't prove that it's going to do the right thing all the time.

00:10:38.020 --> 00:10:39.820
And I guess it really depends, right?

00:10:40.040 --> 00:10:45.540
Like Siri, for example, on the iPhone, sorry if that made anybody's phones go off, it's so bad.

00:10:46.700 --> 00:10:51.260
It's just so weirdly not capable of doing stuff.

00:10:51.370 --> 00:10:56.440
And you know that if that were backed by an LLM instead, sure, it might make mistakes periodically.

00:10:57.180 --> 00:11:01.720
But as a net, it would be so much better that it's probably worth it.

00:11:01.880 --> 00:11:02.320
You know what I mean?

00:11:03.040 --> 00:11:03.600
On the other hand,

00:11:03.720 --> 00:11:15.220
you wouldn't want to say, I'm going to create an agentic workflow to determine if you get a mortgage or things like that would be a little bit too much, I think, right?

00:11:15.270 --> 00:11:17.860
It would deny people for weird random reasons.

00:11:18.900 --> 00:11:19.000
Right.

00:11:19.200 --> 00:11:26.540
You definitely have to consider the stakes and risk and value of a decision that you're making, right?

00:11:26.610 --> 00:11:32.800
And I think generally I wouldn't empower LLMs to make the most important decisions in my life on a day-to-day basis.

00:11:33.940 --> 00:11:41.500
but like you said I do find myself like you know laughing at series responses more than actually like valuing the response so

00:11:41.500 --> 00:12:03.200
it's incredible it's setting timers and you know telling you what the temperature is but boy you're very off very much no no it doesn't it just doesn't do it okay so when would you use I mean I talked about examples you wouldn't use give us examples of when you might really embrace these things?

00:12:03.920 --> 00:12:04.300
Yeah.

00:12:05.370 --> 00:12:12.540
So I think, you know, we talked a little bit about the, we've talked about the non-determinism of LLM behavior, right?

00:12:13.340 --> 00:12:17.200
But also kind of the power of an agent is in making decisions for you.

00:12:17.310 --> 00:12:29.760
And if you can give your large language model the appropriate context and descriptions of tools so that it can do a good job making decisions for you, then that can save you a lot of time.

00:12:30.760 --> 00:12:37.260
So one classic example I'll probably touch on a couple times today is you might build an email agent.

00:12:38.840 --> 00:12:44.200
And so the value of that email agent is that it can categorize emails for you.

00:12:45.020 --> 00:12:52.720
Maybe some things are spam, some things need immediate response, some things can even be automatically responded to.

00:12:53.940 --> 00:12:57.600
And then said agent could learn from your preferences over time.

00:12:58.000 --> 00:13:09.140
So maybe before an automatic reply is sent out, you, the user, get to approve that reply, but you didn't have to do all the work of drafting and that sort of thing.

00:13:10.100 --> 00:13:20.580
And so I think the greatest value that I see in developing workflows with agents is if you can come up with systems that we call ambient workflows.

00:13:21.120 --> 00:13:25.840
Basically, instead of responding to some user prompt like, hey, what's the weather?

00:13:26.460 --> 00:13:29.100
They're running in response to external triggers.

00:13:29.420 --> 00:13:33.240
So in this example, like the email that you received.

00:13:34.280 --> 00:13:38.620
And so basically you're trying to create a system that does work and automates work for you.

00:13:40.240 --> 00:13:40.340
Yeah.

00:13:41.280 --> 00:13:41.900
Yeah, that's cool.

00:13:42.660 --> 00:13:44.960
Could determine if it's, is this an emergency?

00:13:45.860 --> 00:13:48.460
Is this something, a tech support type thing?

00:13:48.840 --> 00:13:59.760
is it an accounting thing and then it could just you know maybe do a little bit of work through your docs give a response and then forward it to the right person or something like that right

00:14:00.180 --> 00:14:21.120
right exactly um you know i think we think about like chatbots um on you know like websites that help users as another common example um and you know when you're talking with the chatbot at what point does it decide like you know what we need to escalate this and like actually uh you know provide them with a phone number to call with a real person or that sort of thing.

00:14:23.200 --> 00:14:37.320
Right. Awesome. I definitely see this as being one of the more powerful, more common ways that LLMs actually interact with people. It's kind of weird that it's a chat, you know what I mean?

00:14:37.860 --> 00:14:47.500
It is pretty useful, but it's also just weird that it's like, okay, we're going to have this chat with this thing, whereas it could, it could do so much more, you know?

00:14:48.480 --> 00:14:52.720
Yeah. You mean with the like chat bot example, like on a help page?

00:14:53.420 --> 00:14:53.680
Yeah.

00:14:53.700 --> 00:14:58.980
Yeah. Or yeah, exactly. Or, you know, even ChatGPT itself or something like that. Right.

00:15:00.060 --> 00:15:15.080
Yeah. I think that's kind of, why we're so excited about the like ambient, or like background system pattern is like a chat is super helpful if I'm like, you know, I need, I have these five ingredients in my fridge like what what should i make for dinner um

00:15:15.080 --> 00:15:15.640
that's like

00:15:15.640 --> 00:15:30.400
a very simplistic pattern and you know we think you should kind of build a system that is only as simple as you need um but then uh i think the real value comes when you're yeah responding to those external like real-time triggers

00:15:30.400 --> 00:15:43.940
can one of them be like a cron job like every hour check this stock price or every hour evaluate the sentiment of this community and then do something yeah

00:15:44.220 --> 00:15:56.040
definitely um i think as we get more into discussing how you can build workflows with lane graph it'll become pretty evident how you can like schedule things and that's like a very predictable um pattern right um

00:15:56.040 --> 00:15:57.460
and right okay maybe

00:15:57.460 --> 00:16:08.120
as useful for you i can imagine for the case where you're like running something on a schedule the end result might be like okay you've analyze sentiment, do I need to like send an alert to my user or things like that?

00:16:10.140 --> 00:16:27.400
Yeah, for sure. All right. Well, let's talk a bit about, I guess, Langchain first. So there's a lot of things going on here, right? There's Langchain, the company. There's Langchain, the open source project. There's Langgraph. Tell us about all these things.

00:16:28.300 --> 00:16:48.580
Yeah, yeah. So happy to give kind of an overview. So LangChain, the open source tool, is basically a tool for developers to use LLMs and really increase the value of LLMs or, you know, kind of augment the power of LLMs with your data.

00:16:50.460 --> 00:16:56.300
And so some of the things that we've built in LinkChain include abstractions around chat models.

00:16:56.990 --> 00:17:11.160
So you can decide you're going to use the 4.1 model and you want to use different settings, like maybe set the temperature to zero or only send some maximum number of tokens or things like that.

00:17:12.520 --> 00:17:20.040
And then specifically, a lot of the value from LinkChain, the open source tool, comes from introducing this model agnostic interface.

00:17:21.480 --> 00:17:35.800
So if I want to chain a bunch of model responses together, it's kind of hard to do that if I am separately querying the OpenAI API and then the Anthropic API and then trying to work with Gemini as well.

00:17:35.920 --> 00:17:39.480
There are kind of different response formats and features, etc.

00:17:39.940 --> 00:17:46.440
And so Langchain makes it really easy for developers to use all of those different model providers in a single

00:17:46.440 --> 00:17:46.980
application.

00:17:48.340 --> 00:17:48.980
That's interesting.

00:17:49.480 --> 00:17:54.080
So if I wanted, I could bring my own LLM?

00:17:54.480 --> 00:17:54.920
Is that possible?

00:17:55.480 --> 00:17:56.240
Yes, yeah.

00:17:56.360 --> 00:18:01.940
Right, like I could use Olama or LM Studio or one of those things rather than the full cloud version?

00:18:02.620 --> 00:18:08.200
Yes, yep, definitely, which is nice just from an extensibility point of view.

00:18:08.230 --> 00:18:14.220
And then if you want to use that LLM and then start to use some other more commercially available ones as well.

00:18:14.410 --> 00:18:15.440
You can combine those.

00:18:16.120 --> 00:18:21.100
And so the name Langchain comes together when we think about chains of systems.

00:18:21.360 --> 00:18:25.000
One common example that we use is RAG, right?

00:18:25.420 --> 00:18:38.840
So you have your vector store, some user asks a question, and you fetch the relevant data from the vector store, and then you kind of feed that into the LLM.

00:18:39.720 --> 00:18:43.480
And so that was a very common pattern like folks, LangChain users would implement.

00:18:46.140 --> 00:18:47.180
Okay. Yeah, very cool.

00:18:47.530 --> 00:18:48.640
And then LangGraph.

00:18:49.680 --> 00:18:56.620
LangGraph is more about building workflows out of this, right?

00:18:57.380 --> 00:18:57.600
Yes.

00:18:57.940 --> 00:19:05.800
So LangGraph is kind of the evolution of LangChain and it has two main things that we focus on.

00:19:05.880 --> 00:19:10.820
So it's got a low-level system for building graphs.

00:19:11.130 --> 00:19:17.040
So you can think of that, you know, the very like computer science folks will be excited to hear the terms like nodes and edges, right?

00:19:17.120 --> 00:19:19.760
You're building this low-level system.

00:19:20.410 --> 00:19:25.440
You can control the flow of data or, you know, the flow of your application.

00:19:26.910 --> 00:19:28.120
You can have conditional branches.

00:19:28.310 --> 00:19:32.900
You can run nodes in parallel, all of those kind of exciting graph features.

00:19:34.200 --> 00:19:42.200
And then we also have, and you can see here, what we call pre-builds, which help you build agents on top of that graph framework.

00:19:42.940 --> 00:19:54.860
So the promise of LaneGraph is that it's extensible because we have those low-level building blocks, but then also very easy to get started with an agent application.

00:19:55.980 --> 00:20:03.320
Yeah, like one of the examples you have on the GitHub page is create React agent from the pre-built graphs.

00:20:04.560 --> 00:20:07.920
Is that React as in JavaScript's React framework or

00:20:07.920 --> 00:20:09.260
just a reactive

00:20:09.260 --> 00:20:09.760
thing?

00:20:10.400 --> 00:20:10.980
Good question.

00:20:11.120 --> 00:20:16.000
So that's React is kind of a common term in the agent building space.

00:20:16.120 --> 00:20:19.180
So it's reasoning and action agent.

00:20:20.020 --> 00:20:20.340
I see.

00:20:20.740 --> 00:20:22.760
So it has nothing to do with JavaScript necessarily.

00:20:23.600 --> 00:20:24.180
No, it does not.

00:20:24.340 --> 00:20:30.520
We actually will be probably changing this name soon to just create agent to be a little bit more clear for folks.

00:20:30.560 --> 00:20:31.520
Sure, sure, sure.

00:20:32.420 --> 00:20:32.540
Okay.

00:20:32.560 --> 00:20:53.280
Okay. Yeah, super cool. So I guess give us a sense of maybe, well, let me circle back to some stuff you talked about. Then I want to talk about why not just call OpenAI or something like that. It's API directly. But you mentioned tokens and, oh gosh, what was the other thing?

00:20:54.380 --> 00:20:55.660
I think I mentioned temperature.

00:20:56.460 --> 00:20:57.540
Temperature, that was the one.

00:20:57.680 --> 00:20:58.120
Yes, thank you.

00:20:58.640 --> 00:20:58.960
Temperature.

00:21:01.559 --> 00:21:16.700
So LLMs can have a certain amount of creativity, maybe the right word, but not really, a certain amount of flexibility in, I guess, almost randomness in the amount of the way that it works its way through finding the answers.

00:21:17.060 --> 00:21:23.920
And the higher the temperature, the more you let it be creative and wonder, whereas the lower the temperature, you're like, no.

00:21:23.980 --> 00:21:26.920
So she gave me the closest answer with the least amount of variability.

00:21:27.780 --> 00:21:27.860
Yeah.

00:21:28.120 --> 00:21:36.880
I imagine that that is really an important consideration in building these types of things because it probably significantly influences how deterministic a thing is.

00:21:37.640 --> 00:21:37.840
Yeah.

00:21:38.220 --> 00:21:38.280
Yeah.

00:21:38.320 --> 00:21:43.420
I think a great way to think about it is if your temperature is low, you know, let's say we're going on a scale of like 0 to 1.

00:21:43.520 --> 00:21:48.380
So if your temperature is like 0 to 0.3, it's, you know, more deterministic.

00:21:48.500 --> 00:21:51.940
We can't say like guaranteed to be fully deterministic.

00:21:52.200 --> 00:22:04.100
But I think, you know, when you're building a system that's going to be deployed for users or basically a production ready system, you're certainly going to lean on the side of lower temperature.

00:22:04.460 --> 00:22:14.460
But, you know, for more creative applications, it's nice to have that setting be pretty configurable just so you can, you know, really see what kind of ideas your LLM can help you with.

00:22:14.960 --> 00:22:15.340
Right, right.

00:22:15.440 --> 00:22:19.920
I want to you help me think creatively about doing this thing and what are my options, right?

00:22:20.000 --> 00:22:22.580
Like that kind of thing, you probably don't want to set temperature to zero.

00:22:22.660 --> 00:22:28.500
But when you're building workflows, I imagine it's usually more tending lower.

00:22:29.260 --> 00:22:29.840
Yep, definitely.

00:22:30.200 --> 00:22:40.960
Sort of a silly example of maybe when you would set the temperature to higher is I was doing some shopping this weekend and stumbled upon a listing on Facebook Marketplace.

00:22:41.560 --> 00:22:45.780
And the listing had a riddle as the description.

00:22:45.860 --> 00:22:49.900
And it was like, if you solve this riddle, you can have this bedside table for free.

00:22:50.840 --> 00:22:56.280
And so, you know, plugging the riddle into ChatGPT with a higher temperature might be more valuable there.

00:22:56.520 --> 00:22:57.180
Obviously more of a

00:22:57.180 --> 00:22:57.940
like-to-way example,

00:22:58.160 --> 00:22:59.580
but kind of shows the utility there.

00:23:00.500 --> 00:23:00.660
Yeah.

00:23:01.000 --> 00:23:02.900
I've been doing a bunch of stuff on Facebook Marketplace.

00:23:03.060 --> 00:23:04.660
That is in a crazy place.

00:23:05.280 --> 00:23:05.540
Yes.

00:23:05.940 --> 00:23:06.620
It's the new Craigslist.

00:23:06.820 --> 00:23:07.260
It's really weird.

00:23:07.760 --> 00:23:08.520
Yeah, indeed.

00:23:08.660 --> 00:23:09.160
Yeah,

00:23:09.240 --> 00:23:10.380
that's a wild, wild example.

00:23:11.140 --> 00:23:15.700
And the other one that I think is probably important to define would be just tokens.

00:23:16.050 --> 00:23:21.700
Because a lot of the models, especially the local models, you know, I don't want to send my data off.

00:23:21.700 --> 00:23:22.680
I want to run Olama.

00:23:22.730 --> 00:23:23.900
I want to run LM Studio.

00:23:25.560 --> 00:23:27.600
Those are quite limited, right?

00:23:27.740 --> 00:23:31.920
Like 8,000 tokens versus half a million for some of the cloud models, right?

00:23:32.220 --> 00:23:34.940
So that can be really a significant consideration as well.

00:23:35.920 --> 00:23:36.180
What are they?

00:23:37.060 --> 00:23:39.300
They're not exactly words, but they're kind of words, but they're not words.

00:23:39.800 --> 00:23:40.100
Yeah.

00:23:41.480 --> 00:23:48.660
So it's basically just a way to quantify the amount of data that you're sending to your LLM.

00:23:50.000 --> 00:23:57.980
And so they also can be used to quantify the amount of data or text that your LLM is sending out as well.

00:23:58.400 --> 00:24:01.120
So you'll have input tokens and output tokens.

00:24:02.360 --> 00:24:04.880
Generally, you can kind of equate them to characters.

00:24:05.240 --> 00:24:11.020
I think there's a little bit of nuance there, but I think the simple explanation, that's good enough.

00:24:12.080 --> 00:24:22.000
And the value of such a wide, what we call a context window, is that the more context and direction your LLM has, the better it's going to perform.

00:24:23.620 --> 00:24:23.940
And so

00:24:23.940 --> 00:24:24.440
if you have

00:24:24.440 --> 00:24:32.980
those bigger windows, you can write more verbose instructions and just write more clear guidelines for your LLM to follow.

00:24:34.440 --> 00:24:46.180
You can also put more relevant context for if your system is answering questions, maybe you need to provide just relevant context there.

00:24:47.040 --> 00:24:52.580
And we'll talk a little bit more down the line about the value of memory and long-term storage in agentic applications.

00:24:53.720 --> 00:24:58.900
And so we'll often feed back in some of the important things logged in memory.

00:24:58.980 --> 00:25:02.740
And so if you have that bigger context window, you can send in more of that data as well.

00:25:03.100 --> 00:25:03.300
So that

00:25:03.300 --> 00:25:04.040
might be like user

00:25:04.040 --> 00:25:10.020
preferences or previous responses or previous messages, that sort of thing.

00:25:10.380 --> 00:25:26.320
Yeah, and if you run out of that space, then it's kind of as if the first part of the conversation, which might include important instructions like we are working on this project, here is our goal, you know, would just kind of either drop off or it would be a runtime error or something.

00:25:26.640 --> 00:25:28.200
So, yeah, it super matters.

00:25:28.980 --> 00:25:35.300
Yep. And I think, I guess, one other important note around tokens is that's often how you're charged for LLM use.

00:25:35.500 --> 00:25:35.640
Yes, exactly.

00:25:35.640 --> 00:25:36.700
Like if you are using more

00:25:36.700 --> 00:25:40.940
tokens on input and more tokens on output is a higher cost.

00:25:41.020 --> 00:25:45.280
But that higher cost is often worth the, you know, significant performance improvement.

00:25:46.000 --> 00:25:47.500
It is. It definitely is.

00:25:47.540 --> 00:25:52.840
Yeah, so that's for the cloud usage-based ones, certainly that's what you get charged.

00:25:52.920 --> 00:25:55.740
And that's some of the appeal of the local ones.

00:25:55.860 --> 00:26:00.880
But the local ones, you know, they can't answer the questions as well.

00:26:00.960 --> 00:26:01.940
They don't have as much data.

00:26:02.400 --> 00:26:05.800
Are there local models that do agentic stuff?

00:26:05.980 --> 00:26:14.700
Or would I do something like Lengraph talking to, I don't know, Mistral or something running, like a 7 billion parameter Mistral running locally?

00:26:15.180 --> 00:26:21.520
Would I use Lengraph to get that agentic behavior and things like that?

00:26:22.100 --> 00:26:31.900
Yeah. So I think some of the like one of the main features that we think about when we think about agentic behavior is that like tool calling ability.

00:26:32.580 --> 00:26:38.760
And so I think if a model doesn't have support for that, then its utility is more limited.

00:26:38.980 --> 00:26:50.160
But that being said, you know, as long as the model can reason and provide guidance, then you can certainly kind of build an agentic application around it with LaneGraph.

00:26:50.860 --> 00:27:05.480
Okay. Yeah, very neat. All right. So my other question before we got distracted on this terminology thing, it was, why not just call the LLM directly? What's wrong with that? Which

00:27:05.480 --> 00:27:05.920
I

00:27:05.920 --> 00:27:08.120
can think of a couple things, but lay it out for us.

00:27:08.480 --> 00:27:24.820
Yeah, so I think the most important thing to think about is like controllability and reliability. So one of the main kind of hallmark features of Langraph is this human in the loop support.

00:27:26.280 --> 00:27:31.100
So you can send a request to your LLM and then it's going to return a response.

00:27:31.300 --> 00:27:38.580
And maybe before you do anything with that response, you probably want to check that, you know, maybe the tool call that it made is okay.

00:27:39.100 --> 00:27:48.500
Or, you know, whatever expensive or risky operation that is about to be run, you want to confirm that or maybe edit the parameters or that sort of thing.

00:27:50.140 --> 00:27:55.220
And so I think for very, you know, simple chat based applications, just calling the LLM is okay.

00:27:55.760 --> 00:28:01.760
You can use LangChain to specify some of those model setting parameters easily or use different LLMs.

00:28:02.150 --> 00:28:11.779
But anytime you want to build something with just more reliability, I think you definitely want more of a framework around it that provides those kind of guardrails.

00:28:12.940 --> 00:28:13.540
Sure.

00:28:14.920 --> 00:28:30.340
There's also things like persistence and durability, which if you've had a long, drawn-out conversation with these APIs, that can be a big deal, right?

00:28:30.860 --> 00:28:31.540
Yeah, definitely.

00:28:32.500 --> 00:28:33.320
And I

00:28:33.320 --> 00:28:37.540
figure we'll probably chat a little bit more about both human in the loop and persistence.

00:28:37.940 --> 00:28:44.240
But one of the nice things about Lingraph is that we offer kind of short-term and long-term persistence.

00:28:44.460 --> 00:28:48.740
So you can have persistence across just a conversation.

00:28:49.400 --> 00:28:53.400
So like kind of one thread of discussion or one application run.

00:28:53.700 --> 00:28:54.880
And then also across runs.

00:28:55.640 --> 00:28:56.040
So going back

00:28:56.040 --> 00:28:57.900
to the email assistant

00:28:57.900 --> 00:29:12.020
example, you probably want your email assistant to learn from your long-term preferences rather than just your single human-in-the-loop edit on one email that came in, for example.

00:29:12.100 --> 00:29:22.580
Right. You might say, I know you flagged this to be forward over here, but when it looks like this, I need you to take that into account and it means something different. You want it to know that forever.

00:29:23.560 --> 00:29:24.800
Yep. Yep, exactly.

00:29:25.320 --> 00:29:38.380
At the same time, people get freaked out. I think ChatGPT started identifying your name and addressing you by name without you saying what your name is and it freaked people out. It's like, well, you've been giving it your secrets for six months, you know?

00:29:39.060 --> 00:29:39.160
Yeah.

00:29:39.280 --> 00:29:39.580
But all of

00:29:39.580 --> 00:29:42.040
a sudden, yeah, yeah.

00:29:42.360 --> 00:29:42.460
Yeah.

00:29:42.740 --> 00:29:54.220
I think my equivalent of being a little freaked out by ChatGPT was I was asking it for some like birthday gift ideas for my partner.

00:29:54.310 --> 00:30:03.360
And it was like, oh, I think based on your previous, you know, commentary about your hobbies and things you like to do with your partner, then like here's some ideas.

00:30:03.530 --> 00:30:04.280
And I was like, wait a minute.

00:30:04.370 --> 00:30:05.200
I didn't tell you that today.

00:30:05.760 --> 00:30:05.880
Like

00:30:05.880 --> 00:30:06.660
I didn't mention

00:30:06.660 --> 00:30:08.880
that I, you know, like to run and go on hikes.

00:30:09.120 --> 00:30:09.740
but you know that.

00:30:11.100 --> 00:30:12.060
It's spying on me.

00:30:12.200 --> 00:30:13.720
No, wait, I told it this over

00:30:13.720 --> 00:30:14.100
and over and

00:30:14.100 --> 00:30:14.220
over.

00:30:14.280 --> 00:30:15.620
But yeah, it's very interesting.

00:30:16.580 --> 00:30:18.100
Mostly useful, somewhat weird.

00:30:18.860 --> 00:30:24.720
All right, let's maybe talk through, to give people a sense how this might work, let's maybe just talk through a quick start example.

00:30:24.840 --> 00:30:33.040
I know talking about code, it's a bit troublesome on audio formats, but we'll just keep it high level.

00:30:33.240 --> 00:30:36.140
But give us a sense of what is it like to write some code here.

00:30:36.220 --> 00:30:38.560
Maybe we could just talk through the quick start for Lengraph.

00:30:38.880 --> 00:30:49.080
yeah that sounds great if you don't mind we actually just made a docs update so I sent you a link with our new quick start that is much quicker so it might be a

00:30:49.080 --> 00:30:53.760
little bit breaking news oh it says I have to log in with Vercel

00:30:54.400 --> 00:30:57.120
oh okay no worries hold

00:30:57.120 --> 00:31:00.140
on I'm just going to hack it real quick I don't know how to get to it

00:31:00.799 --> 00:31:03.140
yeah it's on our working preview branch why

00:31:03.140 --> 00:31:05.700
don't you just talk us through it on your side

00:31:06.400 --> 00:31:37.300
yeah sounds great So our quick start is now kind of focused around creating a simple agent. So you can use that create react agent method that we talked about to create a weather agent. And so what that looks like is you call the function with your preferred model. So in this case, we're using Claude 3.7. And then we give it a get weather tool, our kind of, yeah, it just calls a weather API, maybe with a weather agent.

00:31:37.340 --> 00:31:38.460
with the city and state.

00:31:39.780 --> 00:31:42.220
And then you can provide it with a system prompt as well.

00:31:42.460 --> 00:31:51.460
So we give it some information about, you know, you're a helpful assistant that specializes in weather forecasting and you have access to this Git weather tool.

00:31:51.720 --> 00:31:55.440
And, you know, just trying to provide it with helpful context to get it started.

00:31:57.940 --> 00:31:58.720
Yeah, very cool.

00:31:58.970 --> 00:32:00.000
How much do you think that matters?

00:32:01.380 --> 00:32:10.820
That you kind of set the stage, like you're a helpful weather assistant or you're an expert in this field before I ask you questions?

00:32:11.680 --> 00:32:22.660
Yeah, I think for a really simple application, like just providing one weather tool that has a great description, I think it maybe doesn't matter quite as much.

00:32:23.070 --> 00:32:35.000
But I think as you start to build out those customer-facing chatbots or the email agent that we've been discussing, I think the prompt and guidance becomes really important.

00:32:35.740 --> 00:32:45.840
And I think updating that with those memory stores like we've talked about helps your system be the most useful that it could be for you.

00:32:46.440 --> 00:32:54.700
Yeah, that's true because the long-term persistence probably serves a little bit of that role of like setting the context more deeply.

00:32:55.520 --> 00:32:56.100
Yeah, definitely.

00:32:56.560 --> 00:33:04.120
So like just to give kind of an abbreviated example here, let's say my email assistant drafts me an email response.

00:33:04.620 --> 00:33:20.420
And then I edit that and I, you know, make it more concise and maybe a little bit more friendly or something like that. My agent, I can build in logic so that it updates the long term memory store with that kind of guidance.

00:33:21.220 --> 00:33:32.380
And then the next time that the agent is called with an email stimulus, the prompt now has like, keep things concise and a bit more friendly.

00:33:32.920 --> 00:33:36.640
And I didn't necessarily tell it that I wanted it to be shorter and more friendly.

00:33:36.910 --> 00:33:41.720
I just made that relevant edit and then it summarized that into the prompt.

00:33:41.900 --> 00:33:48.580
So all that being said, I think my analysis here would be that context and guidance is really, really important.

00:33:49.240 --> 00:33:49.780
For the

00:33:49.780 --> 00:33:53.000
Sway example, I think it would be okay without as

00:33:53.000 --> 00:33:53.120
much.

00:33:53.440 --> 00:34:00.140
I do think one of the things that people, I would like to hear your thoughts on this as well and the guidance that you all give.

00:34:00.300 --> 00:34:12.940
But one of the things I think people who are new to LLMs in general, I think they just, they kind of assume that it has too much knowledge or too much context.

00:34:12.940 --> 00:34:19.440
and you'll see like a one sentence, you know, please analyze this document for me.

00:34:20.190 --> 00:34:28.740
Whereas if you gave it a two-page write-up of the request you want, you might get dramatically better results.

00:34:29.149 --> 00:34:34.080
And I also think that that's where a lot of people say, oh, LLMs make mistakes and they do all these crazy things.

00:34:34.600 --> 00:34:36.460
It's like, well, you gave it so little information.

00:34:37.100 --> 00:34:38.159
What's it supposed to go on?

00:34:38.179 --> 00:34:48.520
And I guess in general, my comment is I think people give way too short of prompts and background information to these things.

00:34:49.139 --> 00:34:49.700
What do you guys think?

00:34:50.620 --> 00:34:52.399
Yeah, I think that's definitely true.

00:34:52.780 --> 00:34:55.300
I'll send a link here to OpenAI.

00:34:55.320 --> 00:34:58.540
I just released a GPT 4.1 prompting guide.

00:34:59.420 --> 00:35:04.900
And if you look at any of the guide, you'll see these huge context

00:35:04.900 --> 00:35:05.780
blocks.

00:35:06.780 --> 00:35:12.540
And so obviously these guys really know how these models work and are going to give us the best guidance.

00:35:13.860 --> 00:35:15.140
But, yeah, I think.

00:35:15.220 --> 00:35:15.440
Yeah, let

00:35:15.440 --> 00:35:17.120
me just give people listening a sense.

00:35:17.120 --> 00:35:21.120
It says a sample prompt for a software engineering benchmark verified.

00:35:22.660 --> 00:35:25.160
So here's a prompt you might get.

00:35:25.620 --> 00:35:28.640
And it says you'll be tasked to fix an issue in an open source repository.

00:35:29.600 --> 00:35:34.960
And then it just, it doesn't even word wrap, but it still goes on both

00:35:34.960 --> 00:35:37.240
horizontally and

00:35:37.240 --> 00:35:37.540
vertically.

00:35:38.000 --> 00:35:38.600
Hundreds of lines.

00:35:39.040 --> 00:35:45.800
Another thing that I think is super interesting is the use of markdown when you talk to these things.

00:35:46.240 --> 00:35:50.500
Because it can convey a little bit of formatting, like, oh, this is a link.

00:35:51.040 --> 00:35:55.160
Or this is bolded, so it means something different than if it's not bolded.

00:35:56.120 --> 00:35:56.880
Yes, yeah.

00:35:57.160 --> 00:36:05.760
It's really interesting kind of trying to play the game of how can I optimize my context so that I'm optimizing the behavior of the LLM.

00:36:06.960 --> 00:36:13.100
You know, I think we talk a lot when we're talking about Lingraph about, like, the structure of your application.

00:36:13.740 --> 00:36:18.760
But then really all that boils down to is, like, getting your context right so

00:36:18.760 --> 00:36:19.180
that your

00:36:19.180 --> 00:36:20.840
agent can do the right thing.

00:36:21.620 --> 00:36:22.620
Yeah, well, thanks for the diversion.

00:36:22.660 --> 00:36:33.800
I do think to be successful with this stuff, you've really got to have your talking to LLM's game really strong because that's the atomic units of this, right?

00:36:34.380 --> 00:36:34.540
Yep.

00:36:35.140 --> 00:36:35.620
Yeah, definitely.

00:36:36.280 --> 00:36:37.140
Okay, back to the quick start.

00:36:37.780 --> 00:36:38.280
Give it a prompt.

00:36:39.080 --> 00:36:39.180
Yes.

00:36:39.240 --> 00:36:40.020
Maybe a long, good one.

00:36:40.620 --> 00:36:40.820
Yeah.

00:36:41.720 --> 00:36:46.100
If you go on our docs, there should be like an agent section.

00:36:46.400 --> 00:36:54.280
And I think there's, yeah, so if you go to lane graph and then up at the, let's see.

00:36:54.410 --> 00:36:56.060
Oh, yeah, the agents tab on the top.

00:36:58.620 --> 00:36:58.940
What did I do?

00:37:00.420 --> 00:37:01.860
Like the top nav if you go to

00:37:01.860 --> 00:37:02.040
agents.

00:37:02.040 --> 00:37:02.240
There we go.

00:37:02.380 --> 00:37:02.460
Yeah.

00:37:02.900 --> 00:37:03.220
So we

00:37:03.220 --> 00:37:03.360
should

00:37:03.360 --> 00:37:08.800
have a, like, getting started with agents section that's maybe a little bit more relevant for users.

00:37:11.220 --> 00:37:12.740
And, oh, yeah, there you go.

00:37:15.800 --> 00:37:17.880
and it doesn't want to that is so bizarre

00:37:17.880 --> 00:37:21.920
maybe the agents oh

00:37:21.920 --> 00:37:23.160
i think it's just in here yeah

00:37:23.780 --> 00:37:59.660
gotcha um so yeah so what that would look like you have your create react agent um and then you can invoke it with uh with a set of messages so maybe i want to say what's the weather like in San Francisco. And then it returns a response. And then you can invoke it again and say like, what about New York or other questions like that? And that's kind of the like, out of the box, how do you build, you know, just 10 lines of code, you've got an agent working for you with context and tools, et cetera.

00:38:01.079 --> 00:38:11.520
Okay. Yeah. Super neat. So when is your new getting started tutorial walkthrough thing coming out?

00:38:12.140 --> 00:38:18.040
Yeah. Good question. So we have it coming out, I believe at the end of the week.

00:38:19.420 --> 00:38:24.380
We have our interrupt conference next week. So we want to have our docs kind of revamped for that.

00:38:25.240 --> 00:38:33.440
Interrupt. Okay. Is that like, cause that's the call for human in the loop callback effectively, right? Is that what, is that the origin of that name?

00:38:33.960 --> 00:38:42.640
Yes. Yeah, it is. I think we're like really, emphasizing kind of the value of that. I just sent you the, the link as well for that.

00:38:43.000 --> 00:38:44.520
Uh, it should be viewable publicly

00:38:44.520 --> 00:38:45.820
as kind of our old

00:38:45.820 --> 00:38:49.260
agent's guide. but yeah.

00:38:50.120 --> 00:38:50.820
okay, cool.

00:38:51.660 --> 00:38:54.580
Happy to talk more about, human in the loop if that's helpful.

00:38:54.980 --> 00:38:58.560
Yeah, yeah, let's talk about that because that sounds pretty interesting, right?

00:39:00.460 --> 00:39:06.180
We've got chat, which is obviously human in the loop, although when you press deep research, less in the loop.

00:39:07.260 --> 00:39:14.080
Then you've got these Python programs, which maybe just completely run by themselves.

00:39:14.400 --> 00:39:17.060
But yeah, what is this human in the loop and how does that work?

00:39:17.880 --> 00:39:18.720
Yeah, great question.

00:39:18.800 --> 00:39:32.200
So the human in the loop, we are trying to basically add human approval or edits for cases that are basically sensitive decisions that are being made.

00:39:33.180 --> 00:39:34.300
So very simple example.

00:39:34.680 --> 00:39:38.900
Let's say you have a hotel booking agent and, you know, you get a request in.

00:39:39.000 --> 00:39:42.980
That's like, I'd like to book a stay at Hilton in Boston.

00:39:43.400 --> 00:39:53.720
You might want a user to just kind of approve that sensitive, you know, financial, like significant financial burden before the booking actually goes through.

00:39:54.680 --> 00:40:01.760
And so we make it really easy with this interrupt function anywhere in a tool or node to say interrupt.

00:40:02.030 --> 00:40:19.320
I want to get human input here that can review tool calls, validate outputs from an LLM or like augment existing data or context so that you can make sure your LLM is doing the right thing.

00:40:19.680 --> 00:40:30.920
I see. And it's like, well, we found these things. What would you like me to focus on? Or is it okay to make this call to this API? Maybe it's a paid API, like

00:40:30.920 --> 00:40:31.920
we're accessing

00:40:31.920 --> 00:40:39.680
this expensive stock data. But I think if I knew this, I could answer your question. You're like, yeah, spend the dollar. Let's see. That kind of thing.

00:40:39.760 --> 00:40:39.840
Exactly.

00:40:40.480 --> 00:41:04.780
Or like with the hotel booking agent, you know, let's say I'm like, I'd like to book the Hilton in Boston. And then it comes up with a proposal for booking not the right hotel. I can say, like, actually, can you clarify with the user which hotel they'd like to book, like maybe provide an address or something. And then that like cycles back through and then maybe it gets the proper tool called a second time.

00:41:05.900 --> 00:41:12.900
Right, right. They didn't mind driving, so we put them on the opposite end with the maximum traffic.

00:41:13.600 --> 00:41:14.400
It only looks like

00:41:14.400 --> 00:41:20.180
10 miles, but it actually turns out to be two hours to get from here to there when they need to be there. They don't want that, right?

00:41:20.780 --> 00:41:29.980
Yep. Yep, definitely. We, one kind of exciting other open source project we have is called Agent Inbox. And so if you have a lot of

00:41:29.980 --> 00:41:30.760
interrupts,

00:41:31.700 --> 00:41:42.560
you know, going in your application, the Agent Inbox, you can hook up to that application. And it basically provides you almost looking like this email inbox of all the interrupts for you to

00:41:42.560 --> 00:41:43.260
address.

00:41:43.260 --> 00:41:44.400
Interesting. Okay.

00:41:45.460 --> 00:42:00.600
So maybe if you had like a social media agent or something that was helping you with your posts and it, you know, interrupted right before the final post, you can go through and like accept all those tool calls or, you know, edit posts right before they get sent out.

00:42:01.310 --> 00:42:07.540
Just making it easier to kind of deal with, you know, multiple runs, things like that.

00:42:07.760 --> 00:42:16.500
Yeah, I guess that makes a lot of sense because otherwise we're going to just have a bunch of programs that are executing but just stuck there waiting for people until they go and just start typing back to it.

00:42:16.960 --> 00:42:17.780
Right, exactly.

00:42:18.800 --> 00:42:23.000
And how human in the loop ties into this persistence layer is sort of twofold.

00:42:24.200 --> 00:42:28.480
One, an application can be suspended for any indefinite amount of time.

00:42:28.900 --> 00:42:39.900
So the guarantee with our persistence is that you can just restart back where the interrupt was, whether that's a minute later, five minutes later, 10 days later, a year later, whatever that may be.

00:42:41.300 --> 00:42:50.300
And, you know, so just making sure that, like, you're guaranteed to get that human input before you resume, but then you have a kind of resumable state.

00:42:51.380 --> 00:42:54.720
The second thing is, like we've talked about, you want to learn from that human input.

00:42:54.980 --> 00:43:01.080
And so you can kind of analyze that in a separate step and then store that in the long-term memory.

00:43:02.600 --> 00:43:02.740
Okay.

00:43:02.980 --> 00:43:07.300
And is that a coding thing that you do or is that just the way you set up the workflow?

00:43:08.200 --> 00:43:09.000
Yeah, great question.

00:43:09.180 --> 00:43:18.180
So we provide kind of support for the fundamentals in LandGraph, but we think kind of extensibility and customizability is pretty important.

00:43:18.360 --> 00:43:22.760
So we have like data structures for those short-term and long-term stores.

00:43:23.360 --> 00:43:26.820
But we also provide interfaces so that you could customize your store.

00:43:27.020 --> 00:43:33.060
And, you know, we have like in-memory stores or Postgres stores or things like that.

00:43:33.100 --> 00:43:36.540
But if you wanted to use some custom location, you could do that as well.

00:43:37.020 --> 00:43:37.440
Sure.

00:43:38.540 --> 00:43:39.400
Yeah, super interesting.

00:43:39.700 --> 00:43:49.560
So with this ability to suspend these conversations, does that depend upon the LLM keeping that information around?

00:43:49.660 --> 00:43:56.460
Like, you know, if you make a call to an API, do you get like a session ID sort of thing for that conversation?

00:43:56.900 --> 00:44:04.760
Or does it somehow store it, like you say, in Postgres or wherever, and then provide that back to the LLM to keep talking to it?

00:44:05.460 --> 00:44:07.240
Yeah, so it's the latter of those.

00:44:07.780 --> 00:44:13.160
You're kind of storing the conversation state and message history and any other relevant state to your application.

00:44:13.880 --> 00:44:18.920
And then once you get that signal from the user, you can pick all that up.

00:44:19.100 --> 00:44:20.840
It's kind of unique to the thread.

00:44:21.090 --> 00:44:23.920
We call it like the thread ID or the run ID.

00:44:24.910 --> 00:44:26.100
And yeah, start back from there.

00:44:26.840 --> 00:44:30.660
And then you would, if you're going to call the LLM again, just feed in that context.

00:44:31.260 --> 00:44:31.520
Right.

00:44:32.580 --> 00:44:34.940
Yeah, because that's kind of what subsequent chats are, right?

00:44:35.740 --> 00:44:38.480
It's see everything again, plus the new information.

00:44:39.160 --> 00:44:40.600
Now answer that, right?

00:44:41.560 --> 00:44:41.760
It's

00:44:41.760 --> 00:44:42.280
a little weird.

00:44:42.550 --> 00:44:42.700
Yeah.

00:44:43.640 --> 00:44:43.740
Yeah.

00:44:43.880 --> 00:44:48.420
So that's really helpful for things like local LLMs that might run or restart.

00:44:48.580 --> 00:44:53.780
more frequently, unlike some of the cloud ones where they kind of keep that data.

00:44:53.830 --> 00:44:55.640
But I don't know how long they keep it for, you know?

00:44:56.320 --> 00:44:57.580
Yeah, yeah.

00:44:57.780 --> 00:45:07.900
And a new kind of exciting feature in the persistence space generally is we're going to very soon introduce caching on the kind of node-by-node level.

00:45:08.240 --> 00:45:19.120
So you can imagine if you have a node, which is really just a function running some expensive operation, for simplicity, we can just say it's like a pre-processing step, you know,

00:45:19.140 --> 00:45:19.480
not even

00:45:19.480 --> 00:45:21.440
involved with an LLM.

00:45:22.740 --> 00:45:31.580
You pass certain inputs to that node, and you could enable some sort of cache policy there so that if you're starting up with the same inputs, you get to skip that step.

00:45:32.700 --> 00:45:41.760
And so really just trying to optimize for, you know, how can lots of different types of users use this same low-level system for a variety of agentic workflows.

00:45:42.060 --> 00:45:42.600
Yeah, nice.

00:45:44.620 --> 00:45:51.580
So you have these nodes and graphs and then you put it together in this workflow and you've got this cool persistence of memory.

00:45:52.100 --> 00:46:00.280
Can some of those operations be just pure Python workflow type things rather than always LLM, right?

00:46:00.400 --> 00:46:09.040
Like can it be call this LLM, use this tool, call this LLM, now just make an entry in our database or just send an email or whatever?

00:46:09.800 --> 00:46:10.880
Yeah, yeah, definitely.

00:46:11.120 --> 00:46:13.800
I think that's the kind of value of that low-level framework.

00:46:14.000 --> 00:46:22.540
Like other, you know, what we call agent frameworks are really just focusing on the operations involved with LLMs.

00:46:22.780 --> 00:46:25.540
Right. Or orchestrating multiple LLM calls more or less. Yeah.

00:46:26.120 --> 00:46:26.260
Right.

00:46:27.040 --> 00:46:27.300
So like

00:46:27.300 --> 00:46:39.380
the OpenAI Agents SDK, for example, helps you, you know, query OpenAI LLMs and orchestrate many agents together and maybe have guardrails.

00:46:39.620 --> 00:46:45.660
But there's no kind of built-in support for just those standard Python operations, functions, processing, etc.

00:46:46.530 --> 00:46:50.440
And so Node really just can contain any function.

00:46:51.490 --> 00:46:59.380
So calling some API or doing any other thing you might possibly want it to do in just pure Python.

00:46:59.860 --> 00:47:00.180
Yeah.

00:47:01.010 --> 00:47:08.140
So we talked about using different LLMs like Anthropic or OpenAI or a local one that you might download.

00:47:08.780 --> 00:47:12.020
Can I mix and match LLMs within a single workflow?

00:47:12.550 --> 00:47:12.780
Can I

00:47:12.780 --> 00:47:13.580
say, this part

00:47:13.580 --> 00:47:19.960
I want to run on a local model and that part I want to run on O3 Pro or whatever.

00:47:20.780 --> 00:47:21.240
Yeah, definitely.

00:47:21.240 --> 00:47:22.000
I don't know if that's even out on

00:47:22.000 --> 00:47:22.280
the API.

00:47:22.380 --> 00:47:24.920
It's certainly not out in chat, but where's O3 Pro?

00:47:25.100 --> 00:47:25.320
Come on.

00:47:26.960 --> 00:47:29.980
Yeah, and they have O4 and 4.0 as well.

00:47:30.680 --> 00:47:32.820
Oh, my gosh, it's even nice.

00:47:34.340 --> 00:47:35.000
But yeah, absolutely.

00:47:35.150 --> 00:47:38.320
You can mix and match as your heart desires.

00:47:38.380 --> 00:47:52.240
So LingChain, that first open source tool that we have, makes it really easy to pass messages from one OM provider to another in that kind of model agnostic format.

00:47:53.070 --> 00:48:00.960
And so if you're using LingChain's model abstractions from one node to the next, it's quite easy to

00:48:00.960 --> 00:48:02.300
use.

00:48:02.330 --> 00:48:02.700
That's cool.

00:48:03.070 --> 00:48:03.620
Yeah, that's really cool.

00:48:03.660 --> 00:48:13.260
And you could even do things, I imagine, such as I want to call a certain model that's really good at coding, maybe cloud three seven or something like that.

00:48:13.780 --> 00:48:13.920
Right.

00:48:13.920 --> 00:48:14.200
I want

00:48:14.200 --> 00:48:19.640
to call now I need to call another one that's better at tool usage and is more creative now that I have the answer.

00:48:19.900 --> 00:48:27.300
Right. A little bit like my my weirdo perplexity thing went and wrote Python code to answer my my history question.

00:48:27.480 --> 00:48:27.560
Right.

00:48:27.700 --> 00:48:28.220
Yeah,

00:48:28.600 --> 00:48:52.320
totally. And it's really interesting to think about the pattern of using a model as a router. So you can imagine like a user asks a question and you give some model the context that's like, I have three model providers that I can use. Model provider A is really great at research. Model provider B is really good at solving coding problems, etc.

00:48:52.900 --> 00:49:05.100
And so first the user question gets routed through that router node, and then that can send and basically delegate to one of the other nodes based on that model's reasoning decision.

00:49:06.480 --> 00:49:09.900
So that is kind of a cool multi-agent architecture

00:49:09.900 --> 00:49:11.080
that you can design

00:49:11.080 --> 00:49:12.860
with LANGRAPH pretty easily.

00:49:14.360 --> 00:49:30.940
One other thing that I wanted to note when you were asking about running Python code in nodes, you know, kind of LLM or non-agentic patterns, is that we have this main graph API, which is what most folks use.

00:49:31.000 --> 00:49:38.880
but we've also introduced an imperative API that allows you to decorate functions with kind of two key things.

00:49:39.060 --> 00:49:45.300
One is an entry point decorator that's like, you know, kind of indicative of the start node in your graph.

00:49:45.790 --> 00:49:48.040
And then another one is a task decorator.

00:49:48.920 --> 00:50:00.440
And so if you already have kind of a workflow-like system or functions already written, you can easily turn that into a lane graph workflow just with a couple of decorators, which is very nice.

00:50:00.880 --> 00:50:14.640
that's super cool yeah I like it I like it a lot let's talk about if we look inside Langraph surely it's written in Rust right that has to be

00:50:14.640 --> 00:50:20.700
written in Rust yeah still in Python that's awesome

00:50:21.320 --> 00:50:35.300
I think it's great that you know the library itself is written in Python so it's easy to understand And, you know, do you all have external contributors and take pull across and stuff like that?

00:50:36.340 --> 00:50:37.000
Yeah, we do.

00:50:37.190 --> 00:50:49.620
I think we see a bit more engagement on the LangChain side because LangChain itself is more involved with, you know, new features released by model providers or things like that.

00:50:49.690 --> 00:50:51.300
We see more activity there.

00:50:51.570 --> 00:50:53.920
And LangChain is where all of our integrations lie.

00:50:54.120 --> 00:50:57.880
So like the LangChain OpenAI package or LangChain Anthropic.

00:50:58.620 --> 00:51:00.100
So that's where most of

00:51:00.100 --> 00:51:00.520
the community

00:51:00.520 --> 00:51:01.160
engagement is.

00:51:01.340 --> 00:51:08.120
But like any other good open source project, we've got open issues and discussions and a Slack help

00:51:08.120 --> 00:51:09.940
channel for LangGraph

00:51:09.940 --> 00:51:10.580
users as

00:51:10.580 --> 00:51:10.660
well.

00:51:10.700 --> 00:51:11.440
Right, a community.

00:51:11.610 --> 00:51:12.020
Okay, cool.

00:51:12.500 --> 00:51:19.420
Well, I was going to say you had 174 contributors in LangGraph, but I thought that was pretty good in terms of number of people.

00:51:19.600 --> 00:51:23.500
But you've got 3,600 in LangChain.

00:51:23.510 --> 00:51:24.800
So yeah, that's pretty wild.

00:51:25.170 --> 00:51:25.280
You

00:51:25.280 --> 00:51:26.560
used by 237

00:51:26.560 --> 00:51:27.720
,000 projects.

00:51:28.560 --> 00:51:47.840
Yeah, yeah. It is really impressive kind of the scope of usage there. I think it's the biggest like developer community in the AI space. So it's really nice to have so many eager contributors and get to use that as kind of the foundation for the more model driven operations in LandGraph.

00:51:48.020 --> 00:51:56.040
Yeah, I always like to use the Python web frameworks as measuring sticks or or whatever for these things.

00:51:56.780 --> 00:52:01.680
So, for example, if you look at search for Flask on GitHub, right?

00:52:04.960 --> 00:52:08.820
It's got 70,000 stars, which is pretty awesome.

00:52:08.980 --> 00:52:12.980
You guys have 107, just to give people a sense of how popular this is.

00:52:13.120 --> 00:52:19.960
And I also bring this up because there are people out there who go, oh, I can't believe you're doing an AI topic.

00:52:20.320 --> 00:52:21.020
I'm so tired of AI.

00:52:21.020 --> 00:52:23.740
I can't take any more AI, which I get on one hand.

00:52:25.080 --> 00:52:36.900
on the other that this is how significant it is. It's like literally revolutionizing tech and non-tech before our eyes. It's like history in the making.

00:52:37.940 --> 00:52:39.780
It's like the invention of the web a little bit I think.

00:52:40.780 --> 00:52:41.260
What

00:52:41.260 --> 00:52:41.600
are your thoughts?

00:52:43.060 --> 00:52:51.060
I understand kind of the things are so novel and I understand wanting to push back against such big change.

00:52:52.160 --> 00:53:02.680
But I think one of the reasons that I like LaneGraph as a tool so much is that it really lets you balance the, you know, AI when you need it and not when you don't.

00:53:04.320 --> 00:53:09.400
We talk a lot about kind of finding yourself on this, like, workflow versus agents curve.

00:53:10.240 --> 00:53:20.000
The more, like, pure agent is much less predictable and pure workflow is predictable, but maybe not quite as useful.

00:53:20.660 --> 00:53:23.840
And I think it lets you pick like wherever you want to be on that curve.

00:53:24.940 --> 00:53:30.460
And so hopefully that's also relevant for listeners who are like, man, I'm not, you know, super interested in AI applications.

00:53:30.830 --> 00:53:32.980
But like maybe I have this one

00:53:32.980 --> 00:53:33.980
huge workflow

00:53:33.980 --> 00:53:37.360
where one step could really use the help of an LLM.

00:53:38.220 --> 00:53:43.440
And so I think it's a nice like on ramp into AI usage for developers.

00:53:45.260 --> 00:53:47.900
And just the customizability is very valuable.

00:53:48.180 --> 00:53:51.020
But yeah, I mean, I understand like the general hesitation.

00:53:51.560 --> 00:53:56.540
You know, I think it's kind of revolutionizing our industry and it's unpredictable.

00:53:57.600 --> 00:53:58.740
Yeah, I agree.

00:53:59.180 --> 00:54:08.440
One other thing I'd like to throw out there is I think rightly so a lot of people are concerned about the environmental impacts of heavy AI usage.

00:54:10.060 --> 00:54:14.180
On the other hand, I think there's a lot of interesting things you can do with local models.

00:54:14.480 --> 00:54:20.820
And the local models, they really don't make any difference running them, not necessarily training them.

00:54:21.520 --> 00:54:29.120
Like, for example, I was running a 7 billion parameter model for this Git project that I'm doing and this Git tool.

00:54:29.800 --> 00:54:37.000
And I think I have a little iStats menu thing for my computer's energy usage.

00:54:37.200 --> 00:54:42.640
Right now it's using between 13, it says 13 watts, but it bumps around like 13, 14 watts.

00:54:43.040 --> 00:54:48.380
When the LLM was running, it jumped all the way up to 20, which is like one LED light bulb for four seconds, right?

00:54:49.020 --> 00:54:49.660
So people

00:54:49.660 --> 00:55:04.060
who are concerned about those things, you can run these models locally and still get a huge amount of benefit even if you don't get the full deep research, massive, massive type of stuff you get from the cloud.

00:55:04.120 --> 00:55:06.220
It's still really, really impactful, I think.

00:55:06.540 --> 00:55:07.200
Yeah, definitely.

00:55:07.580 --> 00:55:11.560
I think that really leans into the idea of build simple when you can.

00:55:12.700 --> 00:55:19.600
like a lot of these systems can be relatively simple and, you know, context windows are widening.

00:55:19.740 --> 00:55:33.500
And I think if you can construct a solution that just meets your needs but doesn't do anything like crazy outside of it, it's both going to be better environmentally and like more reliable of a system.

00:55:34.500 --> 00:55:45.620
Yeah, I was talking to Ines Montani about this whole topic, But mostly about the open source or at least open weights models and

00:55:45.620 --> 00:55:46.140
things like

00:55:46.140 --> 00:55:46.300
that.

00:55:47.600 --> 00:55:56.320
And one of the things she really focused on was about task-specific models or models that are really focused on solving a certain problem.

00:55:56.480 --> 00:56:08.020
And it sounds to me like combining that idea with lane graph and sort of bringing in multiple models to the model router, I think you called it, could be really cool.

00:56:08.180 --> 00:56:10.960
So instead of running huge models, I run five small ones.

00:56:11.560 --> 00:56:14.760
But in coordination, they actually become very powerful.

00:56:15.460 --> 00:56:15.780
Yeah.

00:56:16.380 --> 00:56:23.360
Yeah, I think one of the challenges of using LLMs is, like, reliability if tasks are ambiguous.

00:56:24.190 --> 00:56:24.300
Right?

00:56:24.370 --> 00:56:27.940
Like, it's not going to do a good job if you don't tell it what you want it to do.

00:56:28.010 --> 00:56:38.980
And so, yeah, if you have these really specialized tools and you only route things to, or I say tools, what I really mean is models, to clarify when tools is also overloaded.

00:56:40.300 --> 00:56:54.540
But yeah, if you have very specialized models and you can kind of properly route queries and questions and tasks and demands to them, I think that's the best way to have a reliable system.

00:56:54.840 --> 00:56:55.960
Yeah, it's probably faster too.

00:56:56.640 --> 00:56:59.180
Yeah, yeah, and lower environmental footprints.

00:57:00.740 --> 00:57:09.480
The one bonus, the one good thing I think here is that the negative environmental impact is directly correlated with cost.

00:57:09.840 --> 00:57:20.260
So people's incentives are aligned for trending towards less environmental impact, even if they don't care about it, right?

00:57:20.300 --> 00:57:21.080
They just want to save money.

00:57:21.580 --> 00:57:22.340
That's still going

00:57:22.340 --> 00:57:23.460
to get kind of the same output.

00:57:24.020 --> 00:57:24.420
Definitely.

00:57:24.810 --> 00:57:26.540
I think I'm actually really excited.

00:57:26.630 --> 00:57:31.500
I won't be able to go to PyCon this year, but I'm excited to watch some of the talks after the fact.

00:57:31.760 --> 00:57:35.260
And I helped kind of review the talk submissions this year.

00:57:35.260 --> 00:57:46.160
And it seems like there's some really great ones focusing on environmental impacts of your Python code and how to reduce those and, you know, how to think about that on an individual versus like large scale.

00:57:46.960 --> 00:57:51.360
And I think it's really great that that's kind of driving more public conversation now.

00:57:51.560 --> 00:57:51.940
Yeah, yeah.

00:57:52.620 --> 00:57:54.280
Don't tell your LLM things at the end.

00:57:54.460 --> 00:57:54.700
Come on.

00:57:54.800 --> 00:57:55.140
Yeah.

00:57:58.840 --> 00:58:02.840
So let's wrap this up with maybe a bit of a showcase.

00:58:03.390 --> 00:58:14.920
Have you got some example apps or things you've seen build or things people are doing that you're like, there's really cool stuff you can do with LinkGraph and LinkChain or LLMs in general?

00:58:15.960 --> 00:58:16.760
What's some cool stuff

00:58:16.760 --> 00:58:18.120
you've seen out there people building?

00:58:18.820 --> 00:58:21.160
Yeah, that's a good question.

00:58:22.840 --> 00:58:27.720
I think the, I mean, I mentioned the email assistant as a very, like, simple one.

00:58:28.840 --> 00:58:53.920
that I think is satisfying just in that the utility is really high and that's saving folks a lot of time. I think the social media agent is a cool idea if you're like, I don't want to spend a bunch of time, you know, working on social media engagement, but I do want to kind of keep up with the most important updates in my space and then also, you know, kind of contribute.

00:58:55.580 --> 00:59:05.200
I think hopefully I'll have some better answers in the short term, still kind of onboarding at BlaineChain and trying to better understand customer use cases.

00:59:06.780 --> 00:59:07.800
Yeah, very cool.

00:59:08.510 --> 00:59:22.280
You know, one of the things that I think, just thinking out loud of the examples of things that'd be really amazing is you look at research labs and academia in general, especially the research heavy ones.

00:59:22.600 --> 00:59:30.220
how much those folks have to read long detailed papers and assess whether they even make any sense.

00:59:30.980 --> 00:59:46.640
You know, you could set up some kind of agentic workflow that just watches for new things being published in your field, looks to summarize them, determines the impact and relevance, and then brings you like a digest, a weekly digest. Well, here's what's happened

00:59:46.640 --> 00:59:47.160
in

00:59:47.160 --> 00:59:48.140
academia for you.

00:59:48.900 --> 00:59:53.480
And you do have other questions that could be like your interrupt or your inbox sort of thing.

00:59:53.590 --> 00:59:55.920
Like, well, which of these would you like us to dive into?

00:59:56.180 --> 00:59:56.240
Right.

00:59:56.340 --> 00:59:59.940
And like maybe, you know, really go in and analyze these further.

01:00:00.300 --> 01:00:00.940
I don't know.

01:00:01.140 --> 01:00:08.020
It seems like when you're really trying to summarize text or process it or correlate, I don't know.

01:00:08.120 --> 01:00:10.220
It just there's so many possibilities there.

01:00:10.880 --> 01:00:11.280
Yeah, definitely.

01:00:11.590 --> 01:00:14.380
And it's really cool to think about how extensible that is.

01:00:14.420 --> 01:00:20.440
You know, you could kind of build this like general system for a research agent that's going to provide you with a summary.

01:00:20.630 --> 01:00:27.640
And then different users can be like, OK, here are the three topics that I'm most interested in, like provide me with the summary for those, you know.

01:00:28.940 --> 01:00:29.400
Nice to

01:00:29.400 --> 01:00:37.480
basically think about like individual users specifying things that they want to narrow in on or like specialties that they want to emphasize.

01:00:38.040 --> 01:00:40.740
Yeah, super, super fun.

01:00:40.760 --> 01:00:54.100
All right. How about a last thing here, a roadmap? Where are things going? What should people maybe have been asking for that might be showing up they should be on the lookout for?

01:00:54.800 --> 01:01:08.300
Yeah, so one thing I mentioned a little bit earlier is that kind of node level cache. I think that'll just be a nice performance boost for folks, especially folks looking to, you know, really scale with their lane graph applications.

01:01:09.780 --> 01:01:31.820
The thing that we're focusing on the most now is kind of this happy path for those pre-built. So the like create agent functions or some other frameworks have classes. I think, for example, with human in the loop, I'm working on a feature right now that makes it really easy to use kind of default patterns for human in the loop with those pre-built agents.

01:01:32.120 --> 01:01:48.700
So maybe you have like tools A, B, and C, and for all of them, you want default logic such that, you know, when the tools run, a human has to accept the tool call before, or when the tools are recommended by an LLM before the tool actually runs.

01:01:49.320 --> 01:02:04.600
So basically just emphasizing really making it easier to get started with those like create agent pre-builts because we think there's going to be, you know, increasing demand on the like, how do I in 10 lines of code get started with a really functional agent?

01:02:05.040 --> 01:02:05.420
Yeah.

01:02:05.820 --> 01:02:07.080
Oh, this is very Pythonic, right?

01:02:07.260 --> 01:02:18.320
Like it has a lot of, you can put, you can do all the things with it if you want, but then you're down in the details and you're doing advanced programming type stuff.

01:02:18.570 --> 01:02:18.700
Right.

01:02:18.940 --> 01:02:24.260
but what's the import library, do the few things, have magic happen, right?

01:02:24.980 --> 01:02:25.860
Yep, yep, definitely.

01:02:26.310 --> 01:02:35.940
I think those quick start guides are great if they're truly quick, and so I think just kind of shooting for that on our end.

01:02:36.220 --> 01:02:36.800
Yeah, that's cool.

01:02:37.110 --> 01:02:41.660
I would also, this is pure speculation, but maybe informed speculation, I don't know.

01:02:42.760 --> 01:02:53.580
If I were to guess, people coming to do web stuff in Python are pretty CS-like folks, maybe not pure.

01:02:53.780 --> 01:02:57.880
They don't necessarily have CS degrees, but they've got that way of thinking.

01:02:58.160 --> 01:03:00.160
They know what a unit test is, even if they don't do them.

01:03:00.580 --> 01:03:03.620
They know about SQL, even if they use an ORM or whatever.

01:03:04.140 --> 01:03:15.780
Then we've got the data science folks, where before the AI revolution, that was like, well, this is where people who are not really programmers sort of get their first step into Python, and then they kind of get hooked.

01:03:15.920 --> 01:03:21.200
But they start by doing these real simple things like the projects you're talking about or the features you're talking about.

01:03:21.620 --> 01:03:23.640
And then they realize, oh, well, I can do a little more.

01:03:23.740 --> 01:03:27.180
And then they sort of work their way backwards into becoming Python programmers.

01:03:27.980 --> 01:03:34.680
I would say that people coming to AI are maybe even less formal than the data science folks.

01:03:34.840 --> 01:03:37.420
It's a little even further along that spectrum there.

01:03:37.500 --> 01:03:45.180
And so really making those people feel welcome until they become a little more programmer-focused is probably really important.

01:03:45.780 --> 01:03:58.260
Yeah, I recall you chatted on your kind of developer trends for 2025 forecast podcast about the value of being able to get started in Python if you've never written a single line of code and it

01:03:58.260 --> 01:03:58.640
being

01:03:58.640 --> 01:03:59.200
pretty

01:03:59.200 --> 01:03:59.480
easy.

01:03:59.660 --> 01:04:13.200
And then, you know, also folks like the, or teams like the folks at Astral developing tools for, you know, really advanced users and people who really want to make Python sophisticated system.

01:04:13.710 --> 01:04:33.680
And I think we basically with LangRaph want to provide that same benefit of you can get started with a couple lines of code and then also like you want to deploy you know tons of parallel instances running parallel processes and streaming responses with all of these features like you can do that too.

01:04:34.060 --> 01:04:34.920
Yeah that's awesome.

01:04:35.600 --> 01:04:39.060
Yeah you start with one, go to the other if that's the path you're on right?

01:04:39.720 --> 01:04:51.680
Yep, yep. And I guess one other thing that I'll mention roadmap-wise is both LangChain and LangGraph integrate with LangSmith, which is our commercial

01:04:51.680 --> 01:04:52.600
product.

01:04:53.280 --> 01:05:03.380
And we didn't talk a ton about observability and more of the prompt engineering or evaluation side of things that is often tied into agentic workflows.

01:05:05.100 --> 01:05:12.900
But yeah, so if you're building applications with LangGraph, using LangSmith for observability can be super valuable, especially when you're at scale.

01:05:14.300 --> 01:05:14.460
Okay.

01:05:15.080 --> 01:05:17.860
Yeah, monitoring for AI app performance, super cool.

01:05:18.240 --> 01:05:24.620
I guess maybe one more is LangChain Studio or LangStudio, the studio for debugging?

01:05:25.580 --> 01:05:40.520
Yeah, so if you're developing your LangGraph application, You can just run lane graph dev in your terminal, and then that pops up with kind of a local studio, and you can, like, step through your graph.

01:05:41.620 --> 01:05:46.620
Yeah, that's a great screenshot there, or a GIF, I guess.

01:05:46.800 --> 01:05:53.940
But that makes it really easy to debug things locally and just kind of understand the flow of your system in real time.

01:05:55.520 --> 01:05:59.780
And then we also offer a lane graph platform where you can do this kind of thing as well.

01:05:59.980 --> 01:06:01.480
but deployed locally.

01:06:02.400 --> 01:06:02.880
Okay.

01:06:03.760 --> 01:06:03.940
Awesome.

01:06:04.280 --> 01:06:05.280
Well, final call to action.

01:06:05.780 --> 01:06:08.380
People are interested in getting started with all these ideas.

01:06:08.600 --> 01:06:09.120
What do you tell them?

01:06:10.140 --> 01:06:10.380
Yeah.

01:06:11.060 --> 01:06:15.760
I would say look forward to our new docs drop this week.

01:06:15.900 --> 01:06:22.380
I think it's going to really emphasize helping folks get started with those create agent pre-builds.

01:06:23.480 --> 01:06:26.940
Can they just go to linkchain.com and click on docs and go from there?

01:06:27.020 --> 01:06:27.140
Yes.

01:06:27.580 --> 01:06:27.700
Okay.

01:06:29.480 --> 01:06:36.360
Yeah, so we have our link chain docs there, our link graph docs there, and it should be quite easy to get started.

01:06:36.660 --> 01:06:42.860
And obviously I mentioned our kind of community forums earlier, but we have a community help slack and open issues and discussions, et cetera.

01:06:42.940 --> 01:06:44.720
So feel free to ask questions there.

01:06:45.800 --> 01:06:53.100
And then this interrupt conference you talked about, is this going to be live streamed or recorded and videos available?

01:06:53.660 --> 01:07:00.800
Yes. Yeah, we should have talk recordings and demo recordings available as well.

01:07:01.500 --> 01:07:08.880
One other resource that I should mention is that we are kind of trying to grow in the education space.

01:07:09.460 --> 01:07:18.680
And so we also offer a Langraph course and we'll be releasing a new course next week, kind of focusing on those ambient agents.

01:07:19.200 --> 01:07:26.980
And that's a really great way to get started if you're maybe not quite ready to write code, but just kind of want to learn more about the LandGraph features.

01:07:27.640 --> 01:07:31.800
Or if you are ready to write code, it has some awesome resources and notebooks as well.

01:07:32.660 --> 01:07:33.060
Awesome.

01:07:33.560 --> 01:07:35.020
Well, very interesting.

01:07:35.360 --> 01:07:39.660
And it's an exciting time to be working in code.

01:07:40.500 --> 01:07:41.500
It's changing fast.

01:07:42.380 --> 01:07:42.960
Yeah, definitely.

01:07:44.160 --> 01:07:47.680
It's great to talk about these cool new systems that we get to use and explore.

01:07:48.360 --> 01:07:51.180
Yeah, well, looking forward to seeing you all you keep creating.

01:07:51.500 --> 01:07:52.620
Thanks for being back on the show, Sydney.

01:07:53.440 --> 01:07:54.560
Yeah, thank you so much for having me.

01:07:55.160 --> 01:07:55.600
Yeah, bye.

