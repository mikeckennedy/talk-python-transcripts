WEBVTT

00:00:00.001 --> 00:00:02.400
>> Hey, Stan.

00:00:02.400 --> 00:00:03.680
>> Hello.

00:00:03.680 --> 00:00:06.380
>> Hello. Welcome back to Talk Python To Me.

00:00:06.380 --> 00:00:09.320
>> I'm glad to be here. Glad to talk performance.

00:00:09.320 --> 00:00:11.660
>> I know. I'm excited to talk performance.

00:00:11.660 --> 00:00:14.300
It's one of those things that I just never get tired

00:00:14.300 --> 00:00:16.520
of thinking about and focusing on.

00:00:16.520 --> 00:00:19.240
It's just so multifaceted.

00:00:19.240 --> 00:00:23.800
As we will see, even for a language like Python

00:00:23.800 --> 00:00:26.240
that is not primarily performance-focused,

00:00:26.240 --> 00:00:27.840
there's a lot to talk about.

00:00:27.840 --> 00:00:30.680
>> Yeah, there's an endless bag of tricks.

00:00:30.680 --> 00:00:33.320
>> Yeah. I would say,

00:00:33.320 --> 00:00:36.240
to undercut my own comment there,

00:00:36.240 --> 00:00:38.840
Python is increasingly focusing on performance

00:00:38.840 --> 00:00:42.400
since 3.10 or so, right?

00:00:42.400 --> 00:00:44.760
>> The secret is that it's

00:00:44.760 --> 00:00:47.680
because Python integrates so well with other languages,

00:00:47.680 --> 00:00:50.200
it always cared about performance in some way.

00:00:50.200 --> 00:00:52.200
Just sometimes you had to leave Python to do it.

00:00:52.200 --> 00:00:55.320
But you still got to keep the Python interface.

00:00:55.320 --> 00:00:58.560
>> There's been such an easy high-performance escape hatch

00:00:58.560 --> 00:01:04.200
that making Python itself faster is obviously not unimportant,

00:01:04.200 --> 00:01:05.880
but maybe not the primary focus, right?

00:01:05.880 --> 00:01:09.560
Like usability, standard library, etc.

00:01:09.560 --> 00:01:14.800
All right. For people who have not heard your previous episode,

00:01:14.800 --> 00:01:18.360
let's maybe just do a quick introduction. Who's Stan?

00:01:18.360 --> 00:01:20.800
>> Yeah. I'm Stan Siebert.

00:01:20.800 --> 00:01:24.560
I am a manager at Anaconda.

00:01:24.560 --> 00:01:27.760
Well-known purveyor of Python packages and such.

00:01:27.760 --> 00:01:30.080
My day-to-day job is actually

00:01:30.080 --> 00:01:33.440
managing most of our open-source developers at Anaconda.

00:01:33.440 --> 00:01:34.680
That includes the Numba team,

00:01:34.680 --> 00:01:35.840
and so we'll be talking about Numba today,

00:01:35.840 --> 00:01:38.240
but other things like we have people working on

00:01:38.240 --> 00:01:43.120
Jupyter and Beware for mobile Python and other projects like that.

00:01:43.120 --> 00:01:45.320
That's what I do mostly is focus on,

00:01:45.320 --> 00:01:48.040
how do we have an impact on the open-source community,

00:01:48.040 --> 00:01:52.560
and what does Python need to stay relevant and keep evolving?

00:01:52.560 --> 00:01:54.880
>> I love it. What a cool job as well.

00:01:54.880 --> 00:01:57.720
>> I'm really grateful. It's a rare position,

00:01:57.720 --> 00:01:59.800
so I'm really glad I've been able to do it for so long.

00:01:59.800 --> 00:02:02.760
>> Yeah. I would also throw out in the list of things

00:02:02.760 --> 00:02:05.520
that you've given a shout out to,

00:02:05.520 --> 00:02:09.120
I would point out PyScript as well.

00:02:09.120 --> 00:02:11.120
>> Oh, yes. Of course. I just

00:02:11.120 --> 00:02:13.080
started managing the PyScript team again actually,

00:02:13.080 --> 00:02:15.320
and so I forgot about that one too.

00:02:15.320 --> 00:02:17.520
Yes, PyScript. Python in your web browser,

00:02:17.520 --> 00:02:19.200
and Python everywhere, on your phone,

00:02:19.200 --> 00:02:21.440
in your browser, all the places.

00:02:21.440 --> 00:02:24.120
>> Yeah. I mean, it's a little bit out of left field

00:02:24.120 --> 00:02:26.160
compared to the other things that you all are working on,

00:02:26.160 --> 00:02:29.440
but it's also a super important piece I think.

00:02:29.440 --> 00:02:30.840
>> Yeah.

00:02:30.840 --> 00:02:34.120
>> Yeah. Really cool. Really cool there.

00:02:34.120 --> 00:02:38.280
So I think we set the stage a bit,

00:02:38.280 --> 00:02:43.400
but maybe let's start with Numba.

00:02:43.400 --> 00:02:45.440
That one's been around for a while.

00:02:45.440 --> 00:02:47.960
Some people know about it, others don't.

00:02:47.960 --> 00:02:52.040
What is Numba and how do we make Python code faster with Numba?

00:02:52.040 --> 00:02:54.040
>> Yeah. So there have been a lot of

00:02:54.040 --> 00:02:56.640
Python compilation projects over the years.

00:02:56.640 --> 00:03:00.240
Again, Numba is very fortunate that it's now 12 years old.

00:03:00.240 --> 00:03:02.680
So we've been doing it a long time,

00:03:02.680 --> 00:03:04.680
and I've been involved with it

00:03:04.680 --> 00:03:07.600
probably almost 10 of those years now.

00:03:07.600 --> 00:03:10.840
I think one of Numba's success points is trying to stay

00:03:10.840 --> 00:03:13.240
focused on an area where we can have a big impact,

00:03:13.240 --> 00:03:16.040
and that is trying to speed up numerical code.

00:03:16.040 --> 00:03:17.680
So there's a lot of, again,

00:03:17.680 --> 00:03:20.360
data science and other sciences,

00:03:20.360 --> 00:03:24.680
there's a lot of need to write custom algorithms that do math.

00:03:24.680 --> 00:03:28.360
Numba's sweet spot is really helping you to speed those up.

00:03:28.360 --> 00:03:30.720
So we see Numba used in a lot of places where

00:03:30.720 --> 00:03:33.080
maybe the algorithm you're looking for isn't

00:03:33.080 --> 00:03:35.680
already in NumPy or already in JAX or something like that.

00:03:35.680 --> 00:03:37.640
You need to do something new.

00:03:37.640 --> 00:03:42.280
Projects like UMAP, which do really novel clustering algorithms,

00:03:42.280 --> 00:03:43.520
or I just at SciPy,

00:03:43.520 --> 00:03:46.280
I learned more about a project called Stumpy,

00:03:46.280 --> 00:03:49.280
which is for time series analysis.

00:03:49.280 --> 00:03:54.440
Those authors were able to use Numba to take the numerical core of that project,

00:03:54.440 --> 00:03:56.320
that was the time bottleneck,

00:03:56.320 --> 00:03:58.720
and speed it up without having to leave Python.

00:03:58.720 --> 00:04:00.800
So that is, I think,

00:04:00.800 --> 00:04:03.840
really where Numba's most effective.

00:04:03.840 --> 00:04:07.440
>> Sure. If you look at a lot of programs,

00:04:07.440 --> 00:04:10.840
there might be 5,000 lines of code or more,

00:04:10.840 --> 00:04:15.160
but even just something only as big as 5,000 lines,

00:04:15.160 --> 00:04:17.200
there's a lot of code,

00:04:17.200 --> 00:04:20.840
but only a little bit of it really actually matters, right?

00:04:20.840 --> 00:04:24.840
>> Yeah. That's what we find a lot is when you sit down and measure your code,

00:04:24.840 --> 00:04:29.360
you'll spot some hotspots where 60 or 70 or

00:04:29.360 --> 00:04:32.680
80 percent of your time is spent in just like three functions or something.

00:04:32.680 --> 00:04:35.200
That's great. If that's the case,

00:04:35.200 --> 00:04:38.640
that's great because you can just zero in on that section for speeding things up,

00:04:38.640 --> 00:04:41.960
and not ruin the readability of the rest of your program.

00:04:41.960 --> 00:04:45.600
Sometimes optimization can make it harder to read the result,

00:04:45.600 --> 00:04:48.920
and so there's always a balance of you have to keep maintaining this project.

00:04:48.920 --> 00:04:52.800
You don't want to make it unreadable just to get five percent more speed.

00:04:52.800 --> 00:04:55.720
>> Yeah, absolutely.

00:04:55.720 --> 00:04:58.840
Not just the readability,

00:04:58.840 --> 00:05:03.520
but the ability to evolve it over time.

00:05:03.520 --> 00:05:08.440
Maybe you're like, "Oh, we're going to compile this section

00:05:08.440 --> 00:05:12.240
here using Numba or Cython or something like that."

00:05:12.240 --> 00:05:17.120
Well, maybe I was going to use this cool new IPI package I found,

00:05:17.120 --> 00:05:20.000
but I can't just jam it in there where it's compiled.

00:05:20.000 --> 00:05:24.400
That's unlikely to work well, and things like that.

00:05:24.400 --> 00:05:29.080
A lot of times, there's these big sections that look complicated,

00:05:29.080 --> 00:05:31.720
they look slow, they're not actually.

00:05:31.720 --> 00:05:36.080
>> Yeah. One thing I also often emphasize for people is that,

00:05:36.080 --> 00:05:38.160
when you think about the time your program takes,

00:05:38.160 --> 00:05:40.200
think about the time you spent working on it,

00:05:40.200 --> 00:05:42.360
as well as the time you spent running it.

00:05:42.360 --> 00:05:45.360
Because we've heard from a lot of projects who said,

00:05:45.360 --> 00:05:47.680
they were able to get major speedups,

00:05:47.680 --> 00:05:52.520
not necessarily because Numba compiled their code to be incredibly fast,

00:05:52.520 --> 00:05:56.800
but it compiled it to be fast enough that they could try new ideas quicker.

00:05:56.800 --> 00:05:59.200
They got to the real win,

00:05:59.200 --> 00:06:01.440
which was a better way to solve their problem,

00:06:01.440 --> 00:06:05.240
because they weren't mired in boilerplate coding.

00:06:05.240 --> 00:06:11.440
>> Right. It turns out I learned I should use a dictionary and not a list,

00:06:11.440 --> 00:06:13.680
and now it's 100 times faster.

00:06:13.680 --> 00:06:15.760
That wasn't actually a compiling thing,

00:06:15.760 --> 00:06:17.920
that was a visibility thing or something, right?

00:06:17.920 --> 00:06:20.680
>> Yeah. Try more things is always helpful.

00:06:20.680 --> 00:06:23.400
A tool that lets you do that is really valuable.

00:06:23.400 --> 00:06:29.640
>> A hundred percent. What tools do you recommend for knowing?

00:06:29.640 --> 00:06:32.880
Because our human intuition sometimes is good,

00:06:32.880 --> 00:06:35.320
but sometimes is really off the mark in terms of

00:06:35.320 --> 00:06:37.680
thinking about what parts are slow, what parts are fast.

00:06:37.680 --> 00:06:40.520
>> Yeah. That's something I definitely when I've talked to people,

00:06:40.520 --> 00:06:43.040
everyone thinks they know where the slow part is,

00:06:43.040 --> 00:06:45.080
but sometimes they're surprised.

00:06:45.080 --> 00:06:47.640
You definitely, before you do anything,

00:06:47.640 --> 00:06:49.040
this is not just Numba advice,

00:06:49.040 --> 00:06:52.920
this is anytime before you're going to speed up your program, measure something.

00:06:52.920 --> 00:06:56.480
What you want is you want a representative benchmark,

00:06:56.480 --> 00:06:58.320
something that's not going to run too fast,

00:06:58.320 --> 00:07:01.360
because often unit tests run too

00:07:01.360 --> 00:07:05.080
quickly to exercise the program in a realistic way.

00:07:05.080 --> 00:07:06.880
You want a benchmark that doesn't run too long,

00:07:06.880 --> 00:07:08.880
but maybe five minutes or something.

00:07:08.880 --> 00:07:13.120
Then you're going to want to run that through profiling tool.

00:07:13.120 --> 00:07:14.760
There are several options.

00:07:14.760 --> 00:07:17.040
I just usually tell people to use CProfile.

00:07:17.040 --> 00:07:19.420
It's built into the standard library in Python.

00:07:19.420 --> 00:07:22.600
It's a great tool, it does the job for most stuff.

00:07:22.600 --> 00:07:25.200
Sometimes there may be other tools,

00:07:25.200 --> 00:07:27.000
things like SnakeViz and other things that help you

00:07:27.000 --> 00:07:28.400
interpret the results of the profile,

00:07:28.400 --> 00:07:31.480
but often you'll use CProfile to collect the data.

00:07:31.480 --> 00:07:37.320
What this does is it records as the program is running,

00:07:37.320 --> 00:07:39.120
what are all the functions that are being

00:07:39.120 --> 00:07:42.200
called and how much time are they taking.

00:07:42.200 --> 00:07:44.640
There are different strategies for how to do this,

00:07:44.640 --> 00:07:46.480
but fundamentally what you get out is

00:07:46.480 --> 00:07:49.760
a essentially a dataset that says,

00:07:49.760 --> 00:07:52.720
2 percent of the time in your program,

00:07:52.720 --> 00:07:55.320
this function was running and 3 percent of this function was running.

00:07:55.320 --> 00:07:57.880
You can just sort that in descending order and

00:07:57.880 --> 00:08:00.920
look and see what pops out at the top.

00:08:00.920 --> 00:08:03.480
Sometimes you're surprised, sometimes you find out it's,

00:08:03.480 --> 00:08:05.160
actually it wasn't my numerical code,

00:08:05.160 --> 00:08:08.200
it's that I spent 80 percent of my time doing

00:08:08.200 --> 00:08:09.800
some string operation that I didn't

00:08:09.800 --> 00:08:11.720
realize I needed to do over and over again.

00:08:11.720 --> 00:08:13.320
>> Right. Exactly.

00:08:13.320 --> 00:08:15.880
Some weird plus equals with a string was

00:08:15.880 --> 00:08:18.440
just creating a thousand strings

00:08:18.440 --> 00:08:20.520
to get to the endpoint or something like that.

00:08:20.520 --> 00:08:22.640
>> Yeah. I could have just done that once up front.

00:08:22.640 --> 00:08:25.240
It's good to do the profiling just to make sure

00:08:25.240 --> 00:08:28.000
there isn't an obvious problem

00:08:28.000 --> 00:08:30.760
before you get into the more detailed optimization.

00:08:30.760 --> 00:08:33.520
>> Before you start changing your code completely,

00:08:33.520 --> 00:08:35.840
it's execution method or whatever.

00:08:35.840 --> 00:08:36.440
>> Yeah.

00:08:36.440 --> 00:08:39.360
>> Yeah. Shout out to the PyCharm folks.

00:08:39.360 --> 00:08:41.080
They've got to push the button to

00:08:41.080 --> 00:08:42.640
profile and they've got to visualize it,

00:08:42.640 --> 00:08:44.400
and they just run CProfile right in there.

00:08:44.400 --> 00:08:47.440
So that's like CProfile on easy mode.

00:08:47.440 --> 00:08:49.520
You get a spreadsheet and you get a graph.

00:08:49.520 --> 00:08:53.440
What about other ones like Bill,

00:08:53.440 --> 00:08:55.400
FIL or anything else?

00:08:55.400 --> 00:08:56.600
Any other recommendations?

00:08:56.600 --> 00:08:59.120
>> Yeah. That's an interesting point.

00:08:59.120 --> 00:09:04.160
CProfile is for compute time profiling.

00:09:04.160 --> 00:09:08.360
An interesting problem you run into is what this tool does,

00:09:08.360 --> 00:09:10.680
which is memory profiling,

00:09:10.680 --> 00:09:14.240
which is often a problem when you're scaling up.

00:09:14.240 --> 00:09:15.720
That's actually one of the other good things

00:09:15.720 --> 00:09:16.920
to keep in mind when you're optimizing,

00:09:16.920 --> 00:09:19.400
is what am I trying to do? Am I trying to get done faster?

00:09:19.400 --> 00:09:21.800
Am I trying to save on compute costs?

00:09:21.800 --> 00:09:23.120
Am I trying to go bigger?

00:09:23.120 --> 00:09:24.560
I have to speed things up so that I

00:09:24.560 --> 00:09:26.160
have room to put more data in.

00:09:26.160 --> 00:09:28.400
If that's where you're going, you might want to look.

00:09:28.400 --> 00:09:29.440
>> Or am I just out of memory?

00:09:29.440 --> 00:09:33.000
>> Yeah. Or am I already stuck?

00:09:33.000 --> 00:09:35.720
There it is very easy in Python to

00:09:35.720 --> 00:09:39.320
not recognize when you have temporary arrays and things.

00:09:39.320 --> 00:09:41.240
Because again, it's also very compact

00:09:41.240 --> 00:09:42.960
and you're not seeing what's getting allocated.

00:09:42.960 --> 00:09:46.480
You can accidentally blow up your memory quite a lot.

00:09:46.480 --> 00:09:50.840
This kind of a profiler is a great option for it.

00:09:50.840 --> 00:09:55.240
What it can often show you is a line by line.

00:09:55.240 --> 00:09:58.560
This is how much memory

00:09:58.560 --> 00:10:01.240
was allocated in each line of your program.

00:10:01.240 --> 00:10:03.040
You can see, oh, that one line of pandas,

00:10:03.040 --> 00:10:05.960
oops, that did it.

00:10:05.960 --> 00:10:08.400
>> Yeah. I can't remember all the details.

00:10:08.400 --> 00:10:09.720
I talked to Ipmar about this one,

00:10:09.720 --> 00:10:14.520
but I feel like it also keeps track of the memory used,

00:10:14.520 --> 00:10:19.120
even down into NumPy and below,

00:10:19.120 --> 00:10:21.120
not just Python memory where it says,

00:10:21.120 --> 00:10:25.000
now there's some opaque blob of data science stuff.

00:10:25.000 --> 00:10:29.080
>> Yeah. Actually, even on the compute profiling,

00:10:29.080 --> 00:10:30.280
there's two approaches.

00:10:30.280 --> 00:10:34.400
CProfile is focused on counting function time.

00:10:34.400 --> 00:10:36.160
But sometimes you have a long function

00:10:36.160 --> 00:10:38.440
and if you're making a bunch of NumPy calls,

00:10:38.440 --> 00:10:41.920
you might actually care line by line how much time is being taken.

00:10:41.920 --> 00:10:44.160
That can be a better way to think about it.

00:10:44.160 --> 00:10:46.640
I think the tool is called LineProf.

00:10:46.640 --> 00:10:49.600
I forget the exact URL,

00:10:49.600 --> 00:10:54.720
but it's an excellent tool in Python for,

00:10:54.720 --> 00:10:58.360
there's one in R and there's an equivalent one.

00:10:58.360 --> 00:10:59.520
Yes.

00:10:59.520 --> 00:11:00.640
>> LineProfile.

00:11:00.640 --> 00:11:02.080
>> LineProfile. There you go.

00:11:02.080 --> 00:11:02.960
>> It's archived.

00:11:02.960 --> 00:11:04.160
>> Oh, it's archived. Okay.

00:11:04.160 --> 00:11:04.600
>> Yeah.

00:11:04.600 --> 00:11:05.680
>> I have to find another tool now.

00:11:05.680 --> 00:11:07.480
This is my go-to for so long.

00:11:07.480 --> 00:11:10.240
I didn't realize it had already been archived.

00:11:10.240 --> 00:11:13.280
>> If it still works, it's all good.

00:11:13.280 --> 00:11:15.120
>> It's been transferred to a new location,

00:11:15.120 --> 00:11:16.680
so that's where it lives now.

00:11:16.680 --> 00:11:18.480
Line profiling is another.

00:11:18.480 --> 00:11:20.760
I often use them complimentary tools.

00:11:20.760 --> 00:11:24.080
Is I zero in on one function with CProfile,

00:11:24.080 --> 00:11:26.360
and then I'll go line profile that function.

00:11:26.360 --> 00:11:27.240
>> Oh, interesting.

00:11:27.240 --> 00:11:29.080
>> Drill in further.

00:11:29.080 --> 00:11:31.320
>> Yeah. This is the general area.

00:11:31.320 --> 00:11:33.040
Now let's really focus on it.

00:11:33.040 --> 00:11:34.720
Memray is another one.

00:11:34.720 --> 00:11:37.240
I talked to the folks from Bloomberg about that.

00:11:37.240 --> 00:11:39.360
>> Oh, okay. I have not used this one.

00:11:39.360 --> 00:11:43.280
>> Yeah. This is a pretty new one and it's quite neat the way it works.

00:11:43.280 --> 00:11:53.800
This one actually tracks C and C++ and other aspects of allocations as well.

00:11:53.800 --> 00:11:58.280
One of the problems you can run into with profiling is,

00:11:58.280 --> 00:12:00.440
especially memory profiling I think,

00:12:00.440 --> 00:12:01.600
although if you just want to know about memory,

00:12:01.600 --> 00:12:04.200
but the more you monitor it,

00:12:04.200 --> 00:12:09.360
the more it becomes a Heisenberg quantum mechanics type thing.

00:12:09.360 --> 00:12:11.320
Once you observe it, you change it.

00:12:11.320 --> 00:12:15.280
The answers you get by observing it are not actually what are happening.

00:12:15.280 --> 00:12:19.280
You got to keep a little bit of an open mind towards that as well.

00:12:19.280 --> 00:12:25.160
>> Yeah. That's even a risk with the compute side of the profiling.

00:12:25.160 --> 00:12:28.840
You're using some compute time to actually observe the program,

00:12:28.840 --> 00:12:30.160
which means that it can,

00:12:30.160 --> 00:12:32.840
and these tools try to subtract out that bias,

00:12:32.840 --> 00:12:35.400
but it does impact things.

00:12:35.400 --> 00:12:41.280
You may want to have a benchmark that you can run as your real source of truth.

00:12:41.280 --> 00:12:45.240
That you run without the profiler turned on just to see a final runtime.

00:12:45.240 --> 00:12:47.040
Run with the profiler to break it down,

00:12:47.040 --> 00:12:48.480
and then when you're all done, you're going to want to run

00:12:48.480 --> 00:12:50.840
that program again with the profiler off to

00:12:50.840 --> 00:12:54.960
see if you've actually improved it while clock time-wise.

00:12:54.960 --> 00:12:56.720
>> Yeah. Yeah, absolutely.

00:12:56.720 --> 00:13:00.440
That's a really good point. Maybe do a percent time type of thing.

00:13:00.440 --> 00:13:01.280
>> Yeah.

00:13:01.280 --> 00:13:03.760
>> Something along those lines.

00:13:03.760 --> 00:13:08.680
Anything else we're talking about that?

00:13:09.120 --> 00:13:13.120
>> That was a little bit of a side deep dive into profiling.

00:13:13.120 --> 00:13:18.440
Because before you apply some of these techniques like Numba and others,

00:13:18.440 --> 00:13:23.400
you certainly want to know where to apply it.

00:13:23.400 --> 00:13:28.360
Part of that is you might need to rewrite your code a little bit to make it

00:13:28.360 --> 00:13:33.800
more optimizable by Numba or these things.

00:13:33.800 --> 00:13:36.920
First of all, what do you do to use Numba?

00:13:36.920 --> 00:13:39.560
You just put a decorator on there and off you go?

00:13:39.560 --> 00:13:40.760
>> At the very simplest level,

00:13:40.760 --> 00:13:43.320
Numba's interface is supposed to be just one decorator.

00:13:43.320 --> 00:13:45.440
Now, there's some nuance obviously and other things you can do,

00:13:45.440 --> 00:13:47.240
but we tried to get it down to,

00:13:47.240 --> 00:13:49.960
for most people, it's just that.

00:13:49.960 --> 00:13:52.280
The end in NGIT means no Python,

00:13:52.280 --> 00:13:57.320
meaning this code is not calling the Python interpreter anymore at all.

00:13:57.320 --> 00:14:01.120
It is purely machine code, no interpreter access.

00:14:01.120 --> 00:14:05.760
>> Interesting. Some of these compiled,

00:14:05.760 --> 00:14:10.040
do a thing and compile your Python code to machine instructions.

00:14:10.040 --> 00:14:12.680
I feel like they still interact with like

00:14:12.680 --> 00:14:19.680
high object pointers and they still work with the API of the Python data types.

00:14:19.680 --> 00:14:20.400
>> Yeah.

00:14:20.400 --> 00:14:26.320
>> Which is nice, but it's a whole lot slower of an optimization.

00:14:26.320 --> 00:14:32.480
Then now it's int32 and it's float32 and these are on registers.

00:14:32.480 --> 00:14:36.320
>> This is part of the reason why Numba focuses on numerical code,

00:14:36.320 --> 00:14:39.720
is that NumPy arrays and actually other arrays in

00:14:39.720 --> 00:14:43.520
PyTorch and other things that support the buffer protocol.

00:14:43.520 --> 00:14:45.680
Really when Numba compiles this,

00:14:45.680 --> 00:14:47.240
it compiles two functions.

00:14:47.240 --> 00:14:49.400
One is a wrapper that handles the transition from

00:14:49.400 --> 00:14:52.800
the interpreter into no Python land as we call it.

00:14:52.800 --> 00:14:55.080
Then there's a core function that is like

00:14:55.080 --> 00:14:57.240
you could have written in C or Fortran or something.

00:14:57.240 --> 00:15:00.680
That wrapper is actually doing all the Py object stuff.

00:15:00.680 --> 00:15:02.480
It's reaching in and saying,

00:15:02.480 --> 00:15:05.200
this integer I'm going to pull out the actual number.

00:15:05.200 --> 00:15:08.240
This NumPy array, I'm going to reach in and grab the data pointer

00:15:08.240 --> 00:15:13.720
and pass those down into the core where all the math happens.

00:15:13.720 --> 00:15:18.840
The only time you interact with the interpreter is really at the edge.

00:15:18.840 --> 00:15:20.560
Then once you get in there,

00:15:20.560 --> 00:15:21.920
you try not to touch it ever again.

00:15:21.920 --> 00:15:27.040
Now Numba does have a feature that we added some years ago called an object mode block,

00:15:27.040 --> 00:15:29.160
which lets you in the middle of your no Python code,

00:15:29.160 --> 00:15:32.600
go back and actually start talking to the interpreter again.

00:15:32.600 --> 00:15:35.480
>> Maybe use a standard library feature or something.

00:15:35.480 --> 00:15:37.320
>> Yeah. The most common use we've seen is

00:15:37.320 --> 00:15:40.360
you want a progress bar to update or something.

00:15:40.360 --> 00:15:43.200
That's not in your hot loop.

00:15:43.200 --> 00:15:45.600
You don't want to be going back to

00:15:45.600 --> 00:15:48.000
the interpreter in something that's really performance critical,

00:15:48.000 --> 00:15:49.080
but inside of a function,

00:15:49.080 --> 00:15:51.320
you might have parts that are more or less,

00:15:51.320 --> 00:15:53.080
one out of a million iterations I want to

00:15:53.080 --> 00:15:54.840
go update the progress bar or something.

00:15:54.840 --> 00:15:58.200
That's totally valid and you can do that with Numba.

00:15:58.200 --> 00:16:01.080
There's a way to get back to the interpreter if you really need to.

00:16:01.080 --> 00:16:04.920
>> Okay. Yeah. It says,

00:16:04.920 --> 00:16:10.520
it takes and translates Python functions to optimize machine code at runtime,

00:16:10.520 --> 00:16:13.280
which is cool. That makes deploying it super easy.

00:16:13.280 --> 00:16:16.320
You don't have to have compiled wheels for it and stuff.

00:16:16.320 --> 00:16:20.080
Using industry standard LLVM compilers and

00:16:20.080 --> 00:16:24.680
then similar speeds to C is in Fortran.

00:16:24.680 --> 00:16:28.320
Which is awesome,

00:16:28.320 --> 00:16:32.920
but also has implications, if I can speak.

00:16:32.920 --> 00:16:36.200
For example, when I came to Python,

00:16:36.200 --> 00:16:41.120
I was blown away that I could just have integers as big as I want.

00:16:41.120 --> 00:16:43.400
I keep adding to them, they just get bigger and bigger,

00:16:43.400 --> 00:16:49.680
and billions, bazillions of bits of accuracy.

00:16:49.680 --> 00:16:53.040
It came from C++ and C#,

00:16:53.040 --> 00:16:56.240
where you explicitly said it's an N32,

00:16:56.240 --> 00:16:58.400
it's an N64, it's a double.

00:16:58.400 --> 00:17:01.960
These all had ranges of valid numbers and then you've got

00:17:01.960 --> 00:17:05.200
weird wraparounds and maybe you create an unsigned one,

00:17:05.200 --> 00:17:06.640
so you can get a little bit bigger.

00:17:06.640 --> 00:17:12.240
I suspect that you may fall victim or be subjected to

00:17:12.240 --> 00:17:16.120
these types of limitations without realizing them in Python,

00:17:16.120 --> 00:17:19.400
if you add and get it because you're back in that land, right?

00:17:19.400 --> 00:17:22.640
Or do you guys magic to allow it?

00:17:22.640 --> 00:17:24.600
>> We do not handle the big integer,

00:17:24.600 --> 00:17:28.920
which is what you're describing as that integer that can grow without bound.

00:17:28.920 --> 00:17:32.520
Because our target audience is very familiar with NumPy,

00:17:32.520 --> 00:17:37.680
NumPy looks at numbers the way you describe from C++ and other languages.

00:17:37.680 --> 00:17:39.200
>> The detail and all that stuff, right?

00:17:39.200 --> 00:17:40.840
>> Yeah. NumPy arrays always have

00:17:40.840 --> 00:17:43.280
a fixed size integer and you get to pick what that is,

00:17:43.280 --> 00:17:46.680
but it has to be 8, 16, 32, 64.

00:17:46.680 --> 00:17:50.200
Some machines can handle bigger, but that it is fixed.

00:17:50.200 --> 00:17:52.840
Once you've locked that in,

00:17:52.840 --> 00:17:54.880
if you go too big,

00:17:54.880 --> 00:17:57.360
you'll just wrap around and overflow.

00:17:57.360 --> 00:18:00.440
Yeah, so that limitation is definitely present again in Numba,

00:18:00.440 --> 00:18:04.080
but fortunately, NumPy users are already familiar with thinking that way.

00:18:04.080 --> 00:18:07.880
It isn't an additional constraint on them too much.

00:18:07.880 --> 00:18:15.120
>> Right. Okay. Then one thing you said was that you should focus on using arrays.

00:18:15.120 --> 00:18:16.200
>> Yes.

00:18:16.200 --> 00:18:23.440
>> That kind of data structures before you apply Numba JIT compilation to it.

00:18:23.440 --> 00:18:26.280
Does that mean list as in bracket,

00:18:26.280 --> 00:18:30.120
bracket or these NumPy type vector things?

00:18:30.120 --> 00:18:33.560
We all have different definitions of what an array is.

00:18:33.560 --> 00:18:35.480
>> Yes, that's right. Array to big. I would say,

00:18:35.480 --> 00:18:38.920
generally, yeah, usually the go-to I talked about is a NumPy array,

00:18:38.920 --> 00:18:40.760
so it has a shape.

00:18:40.760 --> 00:18:43.440
The nice thing, NumPy arrays can be multidimensional,

00:18:43.440 --> 00:18:45.880
so you can represent a lot of complex data that way,

00:18:45.880 --> 00:18:47.920
but within an array,

00:18:47.920 --> 00:18:50.000
there's a fixed element size.

00:18:50.000 --> 00:18:52.240
That element could be a record.

00:18:52.240 --> 00:18:54.520
If you want for every cell in your array to

00:18:54.520 --> 00:18:56.480
store maybe a set of numbers or a pair of numbers,

00:18:56.480 --> 00:18:59.760
you can do that with custom D types and things,

00:18:59.760 --> 00:19:01.600
and Numba will understand that.

00:19:01.600 --> 00:19:04.680
But basically, that's the ideal data structure.

00:19:04.680 --> 00:19:08.640
Numba does have, we added support a couple of years ago for

00:19:08.640 --> 00:19:13.400
other data structures because the downside to a NumPy array is that it's fixed size.

00:19:13.400 --> 00:19:17.600
Once you make it, you can't append to it like you can a Python list.

00:19:17.600 --> 00:19:22.760
Numba does have support for both what we call typed lists and typed dictionaries.

00:19:22.760 --> 00:19:26.920
These are special cases of lists and dictionaries in Python,

00:19:26.920 --> 00:19:29.400
where in the case of a list,

00:19:29.400 --> 00:19:31.680
every element in the list has the same type,

00:19:31.680 --> 00:19:33.720
or in the case of a dictionary,

00:19:33.720 --> 00:19:36.800
the keys are all the same type and the values are all the same type.

00:19:36.800 --> 00:19:41.400
Those cover a lot of the cases where, again,

00:19:41.400 --> 00:19:45.520
when users want to make things where they don't know how long it's going to be,

00:19:45.520 --> 00:19:47.640
you're going to append in the algorithm.

00:19:47.640 --> 00:19:50.040
A list is a much more natural thing than a NumPy array,

00:19:50.040 --> 00:19:52.680
where you might over-allocate or something.

00:19:52.680 --> 00:19:56.120
Dictionaries, our dictionary implementation

00:19:56.120 --> 00:19:59.640
is basically taken straight from CPython's dictionary implementation.

00:19:59.640 --> 00:20:03.480
It's very tuned and very fast in the same way CPython's is.

00:20:03.480 --> 00:20:06.840
We just had to modify it a little bit to add this type information.

00:20:06.840 --> 00:20:12.840
But it's really good for lookup random items kind of stuff.

00:20:12.840 --> 00:20:16.120
Those are available as additional data structures in addition to the array.

00:20:16.120 --> 00:20:23.520
>> To use those, I would say from Numba import something like this?

00:20:23.520 --> 00:20:27.720
>> Yeah. There are new type in the docs.

00:20:27.720 --> 00:20:29.320
I'll show you, you can import

00:20:29.320 --> 00:20:32.960
a typed list as a special class that you can create.

00:20:32.960 --> 00:20:34.200
The downside, by the way,

00:20:34.200 --> 00:20:38.080
is that the reason we have those and we don't just take,

00:20:38.080 --> 00:20:42.000
historically Numba used to try and let you pass in a Python list,

00:20:42.000 --> 00:20:44.380
is that wrapper function would have to go

00:20:44.380 --> 00:20:47.280
recursively through the list of list of list of whatever you might have,

00:20:47.280 --> 00:20:53.280
and pop out all of the elements into some format that wasn't all Py objects,

00:20:53.280 --> 00:20:56.160
so that the new Python could manipulate them quickly.

00:20:56.160 --> 00:21:01.040
Then how do you put it all back if you modify that shadow data structure?

00:21:01.040 --> 00:21:03.880
What we realized is that was confusing people,

00:21:03.880 --> 00:21:06.920
and actually added a lot of overhead and calling functions took too long.

00:21:06.920 --> 00:21:09.200
We instead went up a level and said, "Okay,

00:21:09.200 --> 00:21:11.200
we're going to make a new list that you at

00:21:11.200 --> 00:21:14.240
the interpreter level can opt into for your algorithm."

00:21:14.240 --> 00:21:18.000
Accessing that list from Python is slower than a Python list.

00:21:18.000 --> 00:21:22.360
But accessing it from Numba is like 100 times faster.

00:21:22.360 --> 00:21:26.680
You have to decide while I'm in this mode,

00:21:26.680 --> 00:21:29.520
I'm optimizing for Numba's performance,

00:21:29.520 --> 00:21:32.000
not for the Python interpreter performance.

00:21:32.000 --> 00:21:34.520
>> Which is reasonable often I'd imagine,

00:21:34.520 --> 00:21:36.640
because this is the part you found to be slow.

00:21:36.640 --> 00:21:39.160
>> Yeah. That's the trade-off you make.

00:21:39.160 --> 00:21:43.960
We would not suggest people use type list just in random places in their program.

00:21:43.960 --> 00:21:45.520
>> I heard this is fast.

00:21:45.520 --> 00:21:48.360
We're just going to replace all like new rule,

00:21:48.360 --> 00:21:49.880
bracket bracket is disallowed.

00:21:49.880 --> 00:21:51.120
We're not using this one, right?

00:21:51.120 --> 00:21:53.160
>> Yeah. When you're working with Python objects,

00:21:53.160 --> 00:21:54.880
Python's data structures can't be beat.

00:21:54.880 --> 00:21:58.800
They are so well-tuned that it's

00:21:58.800 --> 00:22:02.920
very hard to imagine something that could be faster than them.

00:22:02.920 --> 00:22:07.880
>> All right. Maybe one more thing on Numba we could talk about is,

00:22:07.880 --> 00:22:11.880
so far I imagine people have been in their mind thinking of,

00:22:11.880 --> 00:22:15.400
I have at least running on CPUs,

00:22:15.400 --> 00:22:20.760
be that on Apple Silicon or Intel chips or AMD or whatever.

00:22:20.760 --> 00:22:24.640
But there's also support for graphics cards, right?

00:22:24.640 --> 00:22:27.560
>> Yes. For a very long,

00:22:27.560 --> 00:22:30.600
I mean, we've had this again for 10 plus years.

00:22:30.600 --> 00:22:33.280
We were very early adopters of CUDA,

00:22:33.280 --> 00:22:37.280
which is the programming interface for NVIDIA GPUs.

00:22:37.280 --> 00:22:39.520
CUDA is supported by every NVIDIA GPU,

00:22:39.520 --> 00:22:43.720
whether it's a low-end gamer card or a super high-end data center card,

00:22:43.720 --> 00:22:45.240
they all support CUDA.

00:22:45.240 --> 00:22:48.240
That was really nice for people who are trying to get into GPU programming,

00:22:48.240 --> 00:22:51.560
you could use inexpensive hardware to learn.

00:22:51.560 --> 00:22:54.680
On both Windows and Linux,

00:22:54.680 --> 00:22:57.920
Macs don't have NVIDIA GPUs for a long, long time now.

00:22:57.920 --> 00:22:59.680
But on Windows and Linux,

00:22:59.680 --> 00:23:05.520
you can basically write what they call a CUDA kernel in pure Python.

00:23:05.520 --> 00:23:08.560
You can pass up arrays,

00:23:08.560 --> 00:23:11.000
either NumPy arrays which then have to be sent to the card

00:23:11.000 --> 00:23:14.800
or special GPU arrays that are already on the card.

00:23:14.800 --> 00:23:20.560
That is a great way for people to learn a bit more about GPU programming.

00:23:20.560 --> 00:23:22.320
I will say, Numba might not be

00:23:22.320 --> 00:23:24.680
the best place to start with GPU programming in Python

00:23:24.680 --> 00:23:27.840
because there's a great project called Cupy,

00:23:27.840 --> 00:23:34.360
C-U-P-Y, that is literally a copy of NumPy,

00:23:34.360 --> 00:23:37.120
but does all the computation on the GPU.

00:23:37.120 --> 00:23:39.200
Cupy works great with Numba.

00:23:39.200 --> 00:23:41.360
I often tell people if you're curious,

00:23:41.360 --> 00:23:42.800
start with Cupy,

00:23:42.800 --> 00:23:46.440
use some of those NumPy functions to get a sense of

00:23:46.440 --> 00:23:49.960
when is an array big enough to matter on the GPU, that sort of thing.

00:23:49.960 --> 00:23:53.480
Then when you start wanting to do more custom algorithms,

00:23:53.480 --> 00:23:57.160
Numba is where you turn to for that second level.

00:23:57.160 --> 00:24:01.880
>> Yeah. I feel like I'm referencing a lot of

00:24:01.880 --> 00:24:04.200
Atomars work over here,

00:24:04.200 --> 00:24:10.320
but what if we didn't have a NVIDIA GPU?

00:24:10.320 --> 00:24:11.560
Is there anything we could do?

00:24:11.560 --> 00:24:13.440
>> Yeah. There are other projects.

00:24:13.440 --> 00:24:15.680
Things like I mentioned here,

00:24:15.680 --> 00:24:22.080
like PyTorch and things have been ported to a number of different backends.

00:24:22.080 --> 00:24:23.920
This is one thing the Numba team,

00:24:23.920 --> 00:24:28.800
we are frequently talking about is how do we add non-NVIDIA GPU support.

00:24:28.800 --> 00:24:31.560
But I don't have any TA on that.

00:24:31.560 --> 00:24:34.200
That's something that we just still are thinking about.

00:24:34.200 --> 00:24:35.840
But PyTorch definitely,

00:24:35.840 --> 00:24:38.240
and you can use PyTorch as an array library.

00:24:38.240 --> 00:24:40.280
You don't have to be doing machine learning necessarily.

00:24:40.280 --> 00:24:43.200
You can use it just for fast arrays.

00:24:43.200 --> 00:24:45.760
It's just most popular because it supports,

00:24:45.760 --> 00:24:48.200
JAX is a very similar thing because it

00:24:48.200 --> 00:24:50.800
adds the extra features you want for those machine learning models.

00:24:50.800 --> 00:24:52.600
But at the core of every machine learning model,

00:24:52.600 --> 00:24:54.080
it's just array math.

00:24:54.080 --> 00:24:56.840
You could choose to just do that if that's what you want.

00:24:56.840 --> 00:24:58.400
Then you can even still pass those arrays

00:24:58.400 --> 00:25:00.360
off the Numba at some point in the future.

00:25:00.360 --> 00:25:02.760
>> I'm interested. Yeah, I didn't realize there was integration.

00:25:02.760 --> 00:25:03.360
>> Because we all understand each other's protocols.

00:25:03.360 --> 00:25:06.400
>> Yeah, I didn't realize there was integration with that as well.

00:25:06.400 --> 00:25:08.680
>> Yeah. A while back,

00:25:08.680 --> 00:25:10.920
we worked with a number of projects to define

00:25:10.920 --> 00:25:13.680
a GPU array interface that's used by a number of them,

00:25:13.680 --> 00:25:16.920
so that we can see each other's arrays without having to copy the data,

00:25:16.920 --> 00:25:18.400
which is very helpful.

00:25:18.400 --> 00:25:20.680
>> Yeah. We have a lot more topics than Numba,

00:25:20.680 --> 00:25:21.840
but I'm still fascinated with it.

00:25:21.840 --> 00:25:23.000
>> Yeah.

00:25:23.000 --> 00:25:28.720
>> One of the big, all the rage now is vector databases,

00:25:28.720 --> 00:25:33.800
obviously, because I want to query my way through LLM outputs.

00:25:33.800 --> 00:25:39.240
Where in 100,000 dimensional space does this question live?

00:25:39.240 --> 00:25:39.760
>> Yeah.

00:25:39.760 --> 00:25:44.480
>> Or whatever. Is there any integration with that kind of stuff?

00:25:44.480 --> 00:25:48.040
>> There's Numba, not directly.

00:25:48.040 --> 00:25:50.240
Although Numba does have interfaces,

00:25:50.240 --> 00:25:52.840
an easy way to call out to C functions.

00:25:52.840 --> 00:25:57.640
A lot of these vector databases are implemented in C or C++ or something.

00:25:57.640 --> 00:26:00.840
If you did have a use case where you needed to call out to one of them,

00:26:00.840 --> 00:26:02.920
if there was a C function call to make

00:26:02.920 --> 00:26:06.880
directly to the underlying library that bypass the interpreter,

00:26:06.880 --> 00:26:08.760
you can do that from Numba.

00:26:08.760 --> 00:26:10.680
I haven't seen anyone do that yet,

00:26:10.680 --> 00:26:14.440
but it's a generic C interface.

00:26:14.440 --> 00:26:19.440
>> Yeah. Maybe there's a database driver written in C, in which case,

00:26:19.440 --> 00:26:21.440
I don't know all the different databases.

00:26:21.440 --> 00:26:23.520
I know there are some that are specifically built for it.

00:26:23.520 --> 00:26:26.200
Maybe DuckDB has got something going on here.

00:26:26.200 --> 00:26:30.520
But also MongoDB has added vector stuff to it,

00:26:30.520 --> 00:26:32.560
and I know they have a C library as well.

00:26:32.560 --> 00:26:36.280
>> Yeah. I believe that LanceDB is one I've seen

00:26:36.280 --> 00:26:40.520
mentioned by used by a couple of projects that's just for vector.

00:26:40.520 --> 00:26:41.720
>> Which one is it called?

00:26:41.720 --> 00:26:43.640
>> LanceDB.

00:26:43.640 --> 00:26:45.440
>> LanceDB. Okay.

00:26:45.440 --> 00:26:50.360
>> Yeah. I heard about it in the context of another Python LLM project.

00:26:50.360 --> 00:26:54.000
>> Sure. Okay. Cool.

00:26:54.000 --> 00:26:55.960
Well, that's news to me,

00:26:55.960 --> 00:26:59.160
but it is a developer-friendly open-source database for AI.

00:26:59.160 --> 00:27:01.880
Okay. Brilliant. All right.

00:27:01.880 --> 00:27:03.080
Well, like I said,

00:27:03.080 --> 00:27:05.000
we have more things to talk about.

00:27:05.000 --> 00:27:06.640
>> So many things.

00:27:06.640 --> 00:27:08.960
>> So many things. This is super.

00:27:08.960 --> 00:27:11.640
Okay. One more thing I want to ask you about here before we go on.

00:27:11.640 --> 00:27:15.680
This has a Cython feel to it.

00:27:15.680 --> 00:27:19.000
Can you compare and contrast Numba to Cython?

00:27:19.000 --> 00:27:22.000
>> Yeah. So Cython

00:27:22.000 --> 00:27:26.360
requires you to put in some type information

00:27:26.360 --> 00:27:29.720
in order to be able to generate C code that is more efficient.

00:27:29.720 --> 00:27:33.080
Numba is mainly focused on type inference.

00:27:33.080 --> 00:27:35.920
So we try to figure out all the types in

00:27:35.920 --> 00:27:39.280
your function based on the types of the inputs.

00:27:39.280 --> 00:27:43.440
In general, although Numba has places where you can put type annotations,

00:27:43.440 --> 00:27:47.000
we generally discourage people from doing it because we find that

00:27:47.000 --> 00:27:49.320
it adds work and is error-prone

00:27:49.320 --> 00:27:51.120
and doesn't really help the performance in the end.

00:27:51.120 --> 00:27:55.520
Numba will figure out all the types directly.

00:27:55.520 --> 00:27:57.920
>> When it JIT compiles it, if it comes in,

00:27:57.920 --> 00:27:59.800
if you call it twice with different types,

00:27:59.800 --> 00:28:01.200
does it just say, "Well,

00:28:01.200 --> 00:28:03.720
now we're going to need this version of

00:28:03.720 --> 00:28:07.280
the function underscore strings list rather than integers list or something?"

00:28:07.280 --> 00:28:10.720
>> Yeah. Every Numba-compiled function actually contains

00:28:10.720 --> 00:28:14.200
a dispatcher that will look at the argument types and pick the right one.

00:28:14.200 --> 00:28:16.520
It's pretty high granularity.

00:28:16.520 --> 00:28:20.320
For example, people who are familiar with multi-dimensional arrays

00:28:20.320 --> 00:28:24.160
and like Fortran and C know that they lay out the rows and columns in

00:28:24.160 --> 00:28:28.000
a different order which has impact on how you do loops and stuff,

00:28:28.000 --> 00:28:30.480
for to maximize locality.

00:28:30.480 --> 00:28:33.200
Numba can tell the difference between those two cases and

00:28:33.200 --> 00:28:35.560
will generate different code for those two cases.

00:28:35.560 --> 00:28:36.080
>> Wow.

00:28:36.080 --> 00:28:39.840
>> So you as the user don't want to even know.

00:28:39.840 --> 00:28:41.400
>> No, you don't want to worry about that.

00:28:41.400 --> 00:28:42.600
That's a whole another level.

00:28:42.600 --> 00:28:44.240
So you were like, "Okay, well,

00:28:44.240 --> 00:28:46.000
if it's laid out in this order,

00:28:46.000 --> 00:28:52.000
it's probably this appears on the local cache for the CPU in this way.

00:28:52.000 --> 00:28:53.280
So if we loop in that direction,

00:28:53.280 --> 00:28:55.360
we'll iterate through the cache instead of

00:28:55.360 --> 00:28:57.520
blow through it every loop or something like that."

00:28:57.520 --> 00:28:59.880
>> Basically, we want to make sure that LLVM knows

00:28:59.880 --> 00:29:02.160
when the step size is one and that's

00:29:02.160 --> 00:29:05.160
either on the row or the column axis depending on that.

00:29:05.160 --> 00:29:07.680
Because compilers in general are magic.

00:29:07.680 --> 00:29:13.240
We are grateful that LLVM exists because they can do so many tricks at that level.

00:29:13.240 --> 00:29:15.840
I mean, this is the same thing that powers Clang and other stuff.

00:29:15.840 --> 00:29:20.200
So all of macOS compilers are built on LLVM.

00:29:20.200 --> 00:29:23.000
So we can leverage all of the tricks they've

00:29:23.000 --> 00:29:25.480
figured out in decades of development.

00:29:25.480 --> 00:29:28.880
>> Yeah, that's cool. Python itself is compiled with that,

00:29:28.880 --> 00:29:31.760
at least on macOS.

00:29:31.760 --> 00:29:35.680
I just saw it last night that I have Clang some version,

00:29:35.680 --> 00:29:38.960
whatever, and I was just looking at the version for my Python.

00:29:38.960 --> 00:29:40.600
It was compiled with that. Cool.

00:29:40.600 --> 00:29:45.520
Okay. So what is next?

00:29:45.520 --> 00:29:50.320
Let's talk about, we've been on the Numba JIT.

00:29:50.320 --> 00:29:53.040
Anthony Shaw wrote an article,

00:29:53.040 --> 00:29:54.960
Python 3.13 gets a JIT.

00:29:54.960 --> 00:30:01.800
This is a pretty comprehensive and interesting article on what's going on here.

00:30:01.800 --> 00:30:06.600
What's your experience with this JIT coming to Python 3.13?

00:30:06.600 --> 00:30:09.680
>> Yeah. They've definitely tried to set

00:30:09.680 --> 00:30:13.360
expectations here that this first release is really planting a flag.

00:30:13.360 --> 00:30:17.840
It's to say, we're going to start building on top of this base.

00:30:17.840 --> 00:30:19.520
So as far as I've seen,

00:30:19.520 --> 00:30:22.200
the benchmarks for 3.13 are not going to

00:30:22.200 --> 00:30:25.760
be like the world on fire kind of stuff.

00:30:25.760 --> 00:30:28.040
>> See, we're not throwing away our C code and

00:30:28.040 --> 00:30:30.640
rewriting operating systems and JIT in Python.

00:30:30.640 --> 00:30:32.640
>> But you have to take a first step.

00:30:32.640 --> 00:30:36.120
This is honestly pretty,

00:30:36.120 --> 00:30:43.400
it impressed me because Numba as a library,

00:30:43.400 --> 00:30:47.960
we can take a lot of risks and do things that I know we can depend on LLVM.

00:30:47.960 --> 00:30:51.520
We can do all sorts of stuff that maybe not work for everyone because if

00:30:51.520 --> 00:30:53.800
Numba doesn't solve your problem, you just don't use it.

00:30:53.800 --> 00:30:55.560
You can just leave it out of your environment.

00:30:55.560 --> 00:30:56.960
You don't have to install it.

00:30:56.960 --> 00:31:02.480
It's easy to us to zero in on just the problems that we're good at and say,

00:31:02.480 --> 00:31:04.880
if this doesn't solve your problem, just leave us out.

00:31:04.880 --> 00:31:08.200
When you actually are putting a JIT into the core interpreter,

00:31:08.200 --> 00:31:09.840
everyone gets it.

00:31:09.840 --> 00:31:11.480
You have to consider,

00:31:11.480 --> 00:31:15.880
and Python is so broad that you could grab

00:31:15.880 --> 00:31:20.680
two Python experts and they may actually have nothing in common with each other.

00:31:20.680 --> 00:31:24.520
But are both equal claim to being experts at using Python,

00:31:24.520 --> 00:31:26.080
but they might use them in different domains

00:31:26.080 --> 00:31:28.000
and have different libraries they care about and all of that.

00:31:28.000 --> 00:31:29.880
>> I feel that way when I talk to

00:31:29.880 --> 00:31:35.160
pandas people and I think about me doing web development APIs and stuff.

00:31:35.160 --> 00:31:39.200
I'm like, I think I'm really good at Python generally,

00:31:39.200 --> 00:31:40.600
good enough to write good apps.

00:31:40.600 --> 00:31:41.640
But then I look at this, I'm like,

00:31:41.640 --> 00:31:48.280
I don't even really, some of these expressions that go into the filter brackets,

00:31:48.280 --> 00:31:52.880
I'm like, I didn't even know that that was possible or really how to apply.

00:31:52.880 --> 00:31:57.840
It's weird to both feel really competent and understand it,

00:31:57.840 --> 00:32:00.120
but also have it no idea.

00:32:00.120 --> 00:32:01.760
I think what you're getting at is,

00:32:01.760 --> 00:32:04.880
those are two really different use cases and they're getting the same JIT,

00:32:04.880 --> 00:32:06.120
and it has to work for both of them.

00:32:06.120 --> 00:32:09.600
But combinatorially explode that problem.

00:32:09.600 --> 00:32:12.000
>> Yeah. All the different hardware,

00:32:12.000 --> 00:32:13.840
I mean, Numba supports a lot of different computers,

00:32:13.840 --> 00:32:15.960
but not everyone that Python supports.

00:32:15.960 --> 00:32:17.960
>> Like Micro Python?

00:32:17.960 --> 00:32:23.160
>> Yeah. We don't work on HP UX or anything like that necessarily.

00:32:23.160 --> 00:32:27.080
Python has an enormous range of supported platforms,

00:32:27.080 --> 00:32:28.560
an enormous set of use cases,

00:32:28.560 --> 00:32:32.920
and anything you do is going to affect everyone.

00:32:32.920 --> 00:32:36.560
This approach, which I would say this copy and patch JIT approach is really

00:32:36.560 --> 00:32:39.480
clever because one of the, again,

00:32:39.480 --> 00:32:41.160
Numba has to bring,

00:32:41.160 --> 00:32:44.080
we build a custom version of LLVM,

00:32:44.080 --> 00:32:46.480
it's a little stripped down, but it's mostly still there.

00:32:46.480 --> 00:32:48.240
We have to bring that along for the ride.

00:32:48.240 --> 00:32:52.360
That's a heavy dependency to put on the core interpreter for everyone.

00:32:52.360 --> 00:32:56.480
The clever bit here is they figured out how to have a JIT,

00:32:56.480 --> 00:32:59.440
but still do all the compiler stuff at build time.

00:32:59.440 --> 00:33:01.680
When you build this copy and patch JIT,

00:33:01.680 --> 00:33:03.240
you actually need LLVM,

00:33:03.240 --> 00:33:04.840
but only at build time,

00:33:04.840 --> 00:33:06.240
and then it can go away.

00:33:06.240 --> 00:33:09.680
The person who receives the interpreter doesn't need LLVM anymore.

00:33:09.680 --> 00:33:15.240
They've basically built for themselves a bunch of little template fragments.

00:33:15.240 --> 00:33:18.320
This is the patching part is basically you're saying,

00:33:18.320 --> 00:33:20.520
I've got a bunch of fragments that implement

00:33:20.520 --> 00:33:23.360
different opcodes in the bytecode different ways,

00:33:23.360 --> 00:33:25.680
and I'm going to string them together,

00:33:25.680 --> 00:33:28.200
and then go in and there's a bunch of fill in

00:33:28.200 --> 00:33:31.800
the blank spots that I can go in and swap in.

00:33:31.800 --> 00:33:34.680
You get your value from here and then you put it over here and all that,

00:33:34.680 --> 00:33:37.480
but the actual machine code was generated by

00:33:37.480 --> 00:33:41.760
LLVM by the person who built Python in the first place.

00:33:41.760 --> 00:33:42.840
>> I see.

00:33:42.840 --> 00:33:44.880
>> It just amazes me this works.

00:33:44.880 --> 00:33:47.760
I'm excited to see where they go with it.

00:33:47.760 --> 00:33:52.640
Because it was a clever way to avoid adding a huge heavy dependency to Python,

00:33:52.640 --> 00:33:55.720
but start to get some of that JIT benefit.

00:33:55.720 --> 00:33:59.920
>> It looks at some of the common patterns.

00:33:59.920 --> 00:34:03.320
I see. We're looping over a list of loads,

00:34:03.320 --> 00:34:08.480
or something and replaces that with more native code or something along those lines.

00:34:08.480 --> 00:34:13.800
>> Yeah. You essentially have a compiler for a bunch of little recipes.

00:34:13.800 --> 00:34:15.560
If I do this pattern,

00:34:15.560 --> 00:34:17.960
sub in this machine code, fill in these blanks,

00:34:17.960 --> 00:34:20.440
and you just have a table of them.

00:34:20.440 --> 00:34:25.000
The challenge there is that there is a combinatorial explosion again of how

00:34:25.000 --> 00:34:30.960
many different full-blown compiler like LLVM has a bunch of rules.

00:34:30.960 --> 00:34:32.960
It's rule-based. It's saying,

00:34:32.960 --> 00:34:35.040
if I see this pattern, I do this replacement,

00:34:35.040 --> 00:34:36.480
and it keeps doing all of this.

00:34:36.480 --> 00:34:37.400
Then at the end, it says, "Okay,

00:34:37.400 --> 00:34:41.200
now I'm going to generate my machine code from those transformations."

00:34:41.200 --> 00:34:43.280
If I don't have LLVM at runtime,

00:34:43.280 --> 00:34:46.800
I have to figure out what are the best templates up front,

00:34:46.800 --> 00:34:49.080
put them in this table.

00:34:49.080 --> 00:34:52.080
This is where honestly looking at

00:34:52.080 --> 00:34:54.960
different usage patterns will probably be a huge help.

00:34:54.960 --> 00:34:58.200
In practice, you could have any sequence of byte codes,

00:34:58.200 --> 00:35:00.720
but in reality, you're probably going to have certain ones a lot,

00:35:00.720 --> 00:35:03.360
and those are the ones you want to focus on.

00:35:03.360 --> 00:35:06.640
I don't know, once we start getting this out in the community

00:35:06.640 --> 00:35:08.200
and start getting feedback on it,

00:35:08.200 --> 00:35:11.480
I'm curious to see how rapidly it can evolve.

00:35:11.480 --> 00:35:13.480
That'll be really interesting.

00:35:13.480 --> 00:35:17.080
>> Yeah, and this whole copy and patching jet is,

00:35:17.080 --> 00:35:20.440
we often hear people say,

00:35:20.440 --> 00:35:23.080
"I have a computer science degree,"

00:35:23.080 --> 00:35:26.360
and I think what that really means is I have a software engineering degree,

00:35:26.360 --> 00:35:28.880
or I am a software engineering person.

00:35:28.880 --> 00:35:33.280
They are not often creating new science,

00:35:33.280 --> 00:35:36.000
computer science of theories.

00:35:36.000 --> 00:35:37.640
They're more like, "I really understand how

00:35:37.640 --> 00:35:39.880
operating system works in programmers and compilers,

00:35:39.880 --> 00:35:44.840
and I write JSON APIs or I talk to databases."

00:35:44.840 --> 00:35:50.280
But this is like true new research out of legitimate computer science.

00:35:50.280 --> 00:35:52.480
>> Yeah, they mentioned,

00:35:52.480 --> 00:35:54.680
they cite a paper from 2021.

00:35:54.680 --> 00:35:57.480
In computer science, going from paper to

00:35:57.480 --> 00:36:01.040
implementation in one of the most popular languages on Earth,

00:36:01.040 --> 00:36:03.360
in three years, seems pretty fast.

00:36:03.360 --> 00:36:04.880
>> It does seem pretty fast.

00:36:04.880 --> 00:36:06.760
It definitely seems pretty fast.

00:36:06.760 --> 00:36:08.880
The reason I bring this up is I imagine

00:36:08.880 --> 00:36:10.840
that dropping it into one of

00:36:10.840 --> 00:36:12.960
the most popular programming languages with

00:36:12.960 --> 00:36:16.520
a super diverse set of architectures and use cases,

00:36:16.520 --> 00:36:19.600
will probably push that science forward.

00:36:19.600 --> 00:36:23.480
>> Yeah. This will be tested

00:36:23.480 --> 00:36:27.800
in one of the most intense environments you could imagine.

00:36:27.800 --> 00:36:29.720
>> Whatever they did for

00:36:29.720 --> 00:36:33.160
their research paper or their dissertation or whatever,

00:36:33.160 --> 00:36:35.360
this is another level of putting it

00:36:35.360 --> 00:36:38.600
into a test and experimentation to put it into Python.

00:36:38.600 --> 00:36:39.680
>> Yeah.

00:36:39.680 --> 00:36:42.360
>> Yeah, wild. Okay.

00:36:42.360 --> 00:36:44.440
Hopefully, this speeds things up.

00:36:44.440 --> 00:36:46.840
This is going to be interesting because it just happens.

00:36:46.840 --> 00:36:50.560
It just happens. If you have Python 3.13,

00:36:50.560 --> 00:36:54.880
it's going to be looking at its patterns and possibly swapping out.

00:36:54.880 --> 00:36:56.160
>> This is-

00:36:56.160 --> 00:36:57.160
>> These recipes.

00:36:57.160 --> 00:37:00.680
>> Yeah. This is complementary to all of the techniques

00:37:00.680 --> 00:37:03.120
the Faster CPython folks have been doing

00:37:03.120 --> 00:37:05.760
all along for many releases now.

00:37:05.760 --> 00:37:06.480
>> Yeah.

00:37:06.480 --> 00:37:08.640
>> They've been looking at other ways to speed up

00:37:08.640 --> 00:37:12.080
the interpreter without going all the way to a full-blown compiler,

00:37:12.080 --> 00:37:16.040
which this is getting you the final step.

00:37:16.040 --> 00:37:18.320
That's again another interesting place is,

00:37:18.320 --> 00:37:20.360
how does this complement those?

00:37:20.360 --> 00:37:22.360
I don't know those details,

00:37:22.360 --> 00:37:24.680
but it's another tool in the toolbox.

00:37:24.680 --> 00:37:25.840
To go back to the beginning,

00:37:25.840 --> 00:37:28.200
it's speed is about having a bunch of tools,

00:37:28.200 --> 00:37:31.200
and you pick up five percent here and five percent there,

00:37:31.200 --> 00:37:33.600
and you pile up enough five percent,

00:37:33.600 --> 00:37:36.040
and pretty soon you have something substantial.

00:37:36.040 --> 00:37:38.600
>> Yeah. Absolutely. That was the plan, right?

00:37:38.600 --> 00:37:44.200
The Faster CPython was to make it multiple times faster by adding

00:37:44.200 --> 00:37:47.400
20 percent improvements release over release over release

00:37:47.400 --> 00:37:52.560
and compounding percentages basically.

00:37:52.560 --> 00:37:57.320
The one thing I don't know about this is,

00:37:57.320 --> 00:38:00.160
we've had the specializing adaptive interpreter

00:38:00.160 --> 00:38:03.520
that was one of those Faster CPython things that came along.

00:38:03.520 --> 00:38:07.120
Is this just the next level of that,

00:38:07.120 --> 00:38:08.560
or is this a replacement for that?

00:38:08.560 --> 00:38:10.400
I don't know. I'm sure people can.

00:38:10.400 --> 00:38:13.360
>> Yeah. I don't know what their roadmap is for that.

00:38:13.360 --> 00:38:15.440
Because I think part of this is, this is so new,

00:38:15.440 --> 00:38:17.560
I think they got to see how it works in

00:38:17.560 --> 00:38:20.480
practice before they start figuring out.

00:38:20.480 --> 00:38:22.360
>> I agree. It feels like

00:38:22.360 --> 00:38:25.800
an alternative to that specialized adaptive interpreter.

00:38:25.800 --> 00:38:28.320
But I don't know.

00:38:28.320 --> 00:38:29.960
Maybe some of the stuff they've learned from one,

00:38:29.960 --> 00:38:33.520
made it possible or even is just an extension of it.

00:38:33.520 --> 00:38:37.720
What do we want to talk about next here?

00:38:37.720 --> 00:38:39.040
I think-

00:38:39.040 --> 00:38:40.920
>> You want to talk about threads?

00:38:40.920 --> 00:38:45.000
>> No. I want to talk about Rust really quick before we talk about it.

00:38:45.000 --> 00:38:47.480
Then because that'll be quick and then I want to talk about threads,

00:38:47.480 --> 00:38:50.920
because threads will not be as quick and it's super interesting.

00:38:50.920 --> 00:38:53.880
It's been a problem that people have been

00:38:53.880 --> 00:38:58.320
chipping at for years and years and years, the threads thing.

00:38:58.320 --> 00:39:02.160
But what do you think about all this Rust mania?

00:39:02.160 --> 00:39:04.200
It's shown some real positive results,

00:39:04.200 --> 00:39:07.760
things like Ruff and Pydantic and others,

00:39:07.760 --> 00:39:11.280
but it's actually a little bizarrely controversial,

00:39:11.280 --> 00:39:14.640
or maybe not bizarre, non-obviously controversial.

00:39:14.640 --> 00:39:17.640
>> Yeah. My take on the Rust stuff is I view it in

00:39:17.640 --> 00:39:20.280
the same light as when we use C and Fortran historically,

00:39:20.280 --> 00:39:23.680
it's just Rust is a nicer language in many ways.

00:39:24.200 --> 00:39:29.360
If being a nicer language means it's certainly,

00:39:29.360 --> 00:39:33.040
you could have taken any of these things and rewritten them in C a long time ago,

00:39:33.040 --> 00:39:34.240
and they would have been faster.

00:39:34.240 --> 00:39:36.280
You just didn't want to write that C code.

00:39:36.280 --> 00:39:42.720
>> Exactly. We could do this in assembler and you would fly guys.

00:39:42.720 --> 00:39:48.160
>> Rust lowering the bar to saying, "Okay,

00:39:48.160 --> 00:39:52.720
maybe I'll implement the core of my algorithm outside of Python entirely."

00:39:52.720 --> 00:39:54.840
It's interesting and honestly,

00:39:54.840 --> 00:39:57.520
I would happily see Rust completely replace

00:39:57.520 --> 00:40:01.560
C as the dominant extension language in Python.

00:40:01.560 --> 00:40:07.280
The trade-off here, and this is one of those things that's sometimes easy to forget,

00:40:07.280 --> 00:40:09.360
again, because the Python community is so diverse,

00:40:09.360 --> 00:40:11.880
is when you do switch something to Rust,

00:40:11.880 --> 00:40:17.160
you do reduce the audience who can contribute effectively in some cases.

00:40:17.160 --> 00:40:24.320
That using Python to implement things has a benefit for the maintainers if it

00:40:24.320 --> 00:40:28.320
lets them get more contributions, more easily onboard new people.

00:40:28.320 --> 00:40:33.860
I hear this a lot actually from academic software,

00:40:33.860 --> 00:40:38.520
where you have this constant rotating students and postdocs and things.

00:40:38.520 --> 00:40:42.200
How quickly you can onboard someone who isn't a professional software developer

00:40:42.200 --> 00:40:46.280
into a project to contribute to it is relevant.

00:40:46.280 --> 00:40:50.080
But I think it's different for every project.

00:40:50.080 --> 00:40:51.800
There are some things like, again,

00:40:51.800 --> 00:40:54.440
Rust in cryptography makes total sense to me

00:40:54.440 --> 00:40:57.160
because that's also a very security conscious thing.

00:40:57.160 --> 00:41:02.000
You really don't want to be dealing with C buffer overflows in that code.

00:41:02.000 --> 00:41:06.120
The guarantees Rust offers are valuable also.

00:41:06.120 --> 00:41:12.720
>> Well, I think that that also makes sense even outside of just security directly.

00:41:12.720 --> 00:41:15.120
You're going to build a web server.

00:41:15.120 --> 00:41:23.640
It's a nerve-wracking thing to run other people's code on an open port on the Internet.

00:41:23.640 --> 00:41:29.880
One of the things I switched to is I recently switched to Granian.

00:41:29.880 --> 00:41:36.240
For a lot of my websites,

00:41:36.240 --> 00:41:38.880
which is a Rust ACP server.

00:41:38.880 --> 00:41:40.880
It's comparable in performance,

00:41:40.880 --> 00:41:42.240
slightly faster than other things.

00:41:42.240 --> 00:41:45.600
But it's way more,

00:41:45.600 --> 00:41:53.160
its deviation from its average is way, way better.

00:41:53.160 --> 00:41:55.200
>> So it's just more consistent.

00:41:55.200 --> 00:41:57.640
>> More consistent, but also,

00:41:57.640 --> 00:41:59.520
yeah, like the average, for example,

00:41:59.520 --> 00:42:02.280
the average, where's the versus third-party server?

00:42:02.280 --> 00:42:03.400
That's the one I want.

00:42:03.400 --> 00:42:07.520
So for against MicroWizKey, for example,

00:42:07.520 --> 00:42:10.520
right, it's six milliseconds versus 17 milliseconds.

00:42:10.520 --> 00:42:11.840
It's like, whatever.

00:42:11.840 --> 00:42:15.080
But then you look at the max latency of 60 versus three seconds.

00:42:15.080 --> 00:42:17.000
It's like, oh, hold on.

00:42:17.000 --> 00:42:19.040
But the fact it's written in Rust,

00:42:19.040 --> 00:42:23.960
I think it's a little bit of extra safety.

00:42:23.960 --> 00:42:26.600
All other things being equal.

00:42:26.600 --> 00:42:29.520
I mean, obviously, a lot of caveats there.

00:42:29.520 --> 00:42:32.680
>> Yeah. Actually, the interesting point about,

00:42:32.680 --> 00:42:33.920
and this is not unique to Rust,

00:42:33.920 --> 00:42:35.480
this is again the same problem with C and other things,

00:42:35.480 --> 00:42:37.240
is that it's a little bit interesting.

00:42:37.240 --> 00:42:40.240
On one hand, we're pushing the Python interpreter and JITs and all

00:42:40.240 --> 00:42:42.440
this other stuff at the same time as you're

00:42:42.440 --> 00:42:45.120
thinking about whether to pull code entirely out of Python.

00:42:45.120 --> 00:42:50.400
It creates a barrier where the JIT can't see what's in the Rust code.

00:42:50.400 --> 00:42:53.920
If there was an optimization that could have crossed that boundary,

00:42:53.920 --> 00:42:57.280
it's no longer available to the compilers.

00:42:57.280 --> 00:43:01.640
This is a problem the Numba team has been thinking about a lot

00:43:01.640 --> 00:43:04.040
because our number one request,

00:43:04.040 --> 00:43:06.360
aside from other GPUs,

00:43:06.360 --> 00:43:09.040
is can Numba be an ahead-of-time compiler

00:43:09.040 --> 00:43:11.280
instead of a just-in-time compiler?

00:43:11.280 --> 00:43:15.720
We were like, superficially, yes, that's straightforward.

00:43:15.720 --> 00:43:17.600
But then we started thinking about the user experience

00:43:17.600 --> 00:43:18.440
and the developer experience,

00:43:18.440 --> 00:43:21.160
and there are some things that you lose when you go ahead of

00:43:21.160 --> 00:43:25.480
time that you have with the JIT and how do you bridge that gap?

00:43:25.480 --> 00:43:27.440
>> Yeah, it's tricky.

00:43:27.440 --> 00:43:30.920
>> We've been trying to figure out some tooling to try and bridge that.

00:43:30.920 --> 00:43:34.880
At SciPy, we did a talk on a project we just started called Pixie,

00:43:34.880 --> 00:43:38.320
which is a sub-project of Numba that is trying

00:43:38.320 --> 00:43:40.360
to which doesn't have Rust support yet,

00:43:40.360 --> 00:43:42.200
but that's been one of the requests.

00:43:42.200 --> 00:43:47.600
If you go to github.com/numba/pixie.

00:43:47.600 --> 00:43:51.360
>> Search engines are pure magic. They really are.

00:43:51.360 --> 00:43:53.320
>> Awesome. Founder issue tracker.

00:43:53.320 --> 00:43:58.360
But Pixie, we gave a talk about it at SciPy.

00:43:58.360 --> 00:43:59.880
It's very early stages.

00:43:59.880 --> 00:44:01.800
But what we're trying to do is figure out how to,

00:44:01.800 --> 00:44:03.000
in the ahead-of-time compilation,

00:44:03.000 --> 00:44:06.000
whether that's C or Rust or even Numba eventually,

00:44:06.000 --> 00:44:11.040
capturing enough info that we can feed that back into a future JIT,

00:44:11.040 --> 00:44:13.760
so that the JIT can still see what's going on in

00:44:13.760 --> 00:44:18.680
the compiled code as a future-proofing ecosystem.

00:44:18.680 --> 00:44:21.560
>> Yeah, that's cool. I know some compilers

00:44:21.560 --> 00:44:25.680
have profiling-based optimization type things.

00:44:25.680 --> 00:44:28.840
You can compile it with some instrumentation,

00:44:28.840 --> 00:44:31.240
run it, and then take that output and feed it back into it.

00:44:31.240 --> 00:44:34.600
I don't know if I've ever practically done anything with that,

00:44:34.600 --> 00:44:38.160
but I go, "That's a neat idea to try it,

00:44:38.160 --> 00:44:39.800
let it see what it does, and then feed it back."

00:44:39.800 --> 00:44:41.960
Is this like that or what do you think?

00:44:41.960 --> 00:44:48.000
>> This is different. This is basically capturing in the library file.

00:44:48.000 --> 00:44:50.000
You compiled ahead of time to a library,

00:44:50.000 --> 00:44:53.960
capturing the LLVM bitcode so that you could pull it out and

00:44:53.960 --> 00:44:57.360
embed it into your JIT which might have other LLVM bitcodes.

00:44:57.360 --> 00:44:59.480
Then you can optimize.

00:44:59.480 --> 00:45:02.560
You can have a function you wrote in Python that calls a function in C,

00:45:02.560 --> 00:45:05.840
and you could actually optimize them together even though

00:45:05.840 --> 00:45:08.080
they were compiled at different times,

00:45:08.080 --> 00:45:09.440
implemented in different languages,

00:45:09.440 --> 00:45:11.160
you could actually cross that boundary.

00:45:11.160 --> 00:45:13.600
>> One's like a head of time compilation,

00:45:13.600 --> 00:45:16.520
just standard compilation, and one is like a JIT thing,

00:45:16.520 --> 00:45:18.720
but it's like we're going to click it together in the right ways.

00:45:18.720 --> 00:45:22.520
>> Yeah. Because JITs are nice in that they can see everything that's going on,

00:45:22.520 --> 00:45:24.520
but then they have to compile everything that's going on,

00:45:24.520 --> 00:45:26.920
and that adds time and latency and things.

00:45:26.920 --> 00:45:28.640
Can you have it both ways?

00:45:28.640 --> 00:45:30.880
It's really what we're trying to do.

00:45:30.880 --> 00:45:34.040
>> It's nice when you can have your cake and eat it too, right?

00:45:34.040 --> 00:45:35.840
>> Yes.

00:45:35.840 --> 00:45:40.200
>> My cake before my vegetables and it'd be fine.

00:45:40.200 --> 00:45:44.000
I said that this Rust thing was a little bit controversial.

00:45:44.000 --> 00:45:47.240
I think there's some just,

00:45:47.240 --> 00:45:53.800
hey, you're stepping on my square of Python space with a new tool.

00:45:53.800 --> 00:45:58.000
I don't think that has anything to do with Rust per se.

00:45:58.000 --> 00:46:02.480
It's just somebody came along and made a tool that is now doing something,

00:46:02.480 --> 00:46:07.240
maybe in better ways or I don't want to start up a whole debate about that.

00:46:07.240 --> 00:46:09.440
But I think the other one is what you touched on is,

00:46:09.440 --> 00:46:14.520
if we go and write a significant chunk of this stuff in this new language,

00:46:14.520 --> 00:46:15.720
regardless what language it is,

00:46:15.720 --> 00:46:21.000
Rust is a relatively not popular language compared to others.

00:46:21.000 --> 00:46:23.600
Then people who contribute to that,

00:46:23.600 --> 00:46:25.280
either from the Python side were like,

00:46:25.280 --> 00:46:28.560
well, there's this big chunk of Rust code now that I don't understand anything about,

00:46:28.560 --> 00:46:30.760
so I can't contribute to that part of it.

00:46:30.760 --> 00:46:32.680
You might even say, well,

00:46:32.680 --> 00:46:36.800
what about the pro developers or the experienced core developers and stuff?

00:46:36.800 --> 00:46:40.440
They're experienced in pro at C in Python,

00:46:40.440 --> 00:46:41.960
which is also not Rust.

00:46:41.960 --> 00:46:46.640
It's this new area that is more opaque to most of the community,

00:46:46.640 --> 00:46:48.760
which I think that's part of the challenge.

00:46:48.760 --> 00:46:54.360
>> Yeah. Some people like learning new programming languages and some don't.

00:46:54.360 --> 00:46:57.240
On some hand, Rust can be,

00:46:57.240 --> 00:46:59.560
this is a new intellectual challenge and it

00:46:59.560 --> 00:47:02.160
fixes practically some problems you have with C.

00:47:02.160 --> 00:47:05.320
In other cases, I wanted to worry about what

00:47:05.320 --> 00:47:07.760
this project does and not another programming language.

00:47:07.760 --> 00:47:09.040
>> Right.

00:47:09.040 --> 00:47:13.160
>> Have to look at your communities and decide what's the right trade-off for you.

00:47:13.160 --> 00:47:14.840
>> Maybe in 10 years,

00:47:14.840 --> 00:47:19.040
CPython will be our Python and it'll be written in Rust.

00:47:19.600 --> 00:47:23.600
>> If we move to WebAssembly and PyScript,

00:47:23.600 --> 00:47:25.840
Pyodide land a lot, having that right,

00:47:25.840 --> 00:47:29.360
there's a non-zero probability,

00:47:29.360 --> 00:47:31.320
but it's not a high number, I suppose.

00:47:31.320 --> 00:47:36.000
Speaking of something I also thought was going to have a very near zero probability.

00:47:36.000 --> 00:47:37.120
>> Yeah.

00:47:37.120 --> 00:47:39.120
>> PEP 703 is accepted.

00:47:39.120 --> 00:47:40.560
Oh my goodness. What is this?

00:47:40.560 --> 00:47:43.320
>> Yeah. This was, again,

00:47:43.320 --> 00:47:45.440
a couple of years ago now or I guess a year ago,

00:47:45.440 --> 00:47:47.600
it was finally accepted.

00:47:47.600 --> 00:47:50.640
For since very long time,

00:47:50.640 --> 00:47:53.120
the Python interpreter has,

00:47:53.120 --> 00:47:55.720
because again, threads are an operating system feature that let you

00:47:55.720 --> 00:47:58.680
do something in a program concurrently.

00:47:58.680 --> 00:48:02.320
Now that all of our computers have four,

00:48:02.320 --> 00:48:05.600
eight, even more cores depending on what kind of machine you have,

00:48:05.600 --> 00:48:09.000
even your cell phone has more than one core.

00:48:09.000 --> 00:48:12.440
Using those cores requires you have

00:48:12.440 --> 00:48:16.240
some parallel computing in your program.

00:48:16.240 --> 00:48:20.920
The problem is that once you start doing things in parallel,

00:48:20.920 --> 00:48:22.920
you have the potential for race conditions.

00:48:22.920 --> 00:48:28.160
You have the two threads might do the same thing at the same time or touch the same data,

00:48:28.160 --> 00:48:30.240
get it inconsistent, and then

00:48:30.240 --> 00:48:32.640
your whole program starts to crash and other bad things happen.

00:48:32.640 --> 00:48:35.560
Historically, the global interpreter lock has been

00:48:35.560 --> 00:48:40.360
the sledgehammer protection of the CPython interpreter.

00:48:40.360 --> 00:48:44.240
But the net result was that threads that were running

00:48:44.240 --> 00:48:47.840
pure Python code basically got no performance benefits.

00:48:47.840 --> 00:48:49.720
You might get other benefits like you could have

00:48:49.720 --> 00:48:51.840
one block on IO while the other one does stuff.

00:48:51.840 --> 00:48:54.600
It was easier to manage that concurrency.

00:48:54.600 --> 00:48:57.680
But if you were trying to do compute on two cores at the same time in

00:48:57.680 --> 00:49:00.600
pure Python code it was just not going to happen because

00:49:00.600 --> 00:49:03.320
every operation touches a Python object,

00:49:03.320 --> 00:49:06.320
has to lock the interpreter while you make that modification.

00:49:06.320 --> 00:49:09.680
>> Now you could write all the multi-threaded code with locks and stuff you want,

00:49:09.680 --> 00:49:12.520
but it's really just going to run one at a time anyway.

00:49:12.520 --> 00:49:13.160
>> Yeah.

00:49:13.160 --> 00:49:16.960
>> A little bit like preemptive multi-threading on a single core CPU.

00:49:16.960 --> 00:49:19.280
I don't know, it's weird. I've added all this complexity,

00:49:19.280 --> 00:49:21.000
but I haven't got much out of it.

00:49:21.000 --> 00:49:24.920
>> The secret of course is that if your Python program contained not Python,

00:49:24.920 --> 00:49:26.720
like C or Cython or Fortran,

00:49:26.720 --> 00:49:29.280
as long as you weren't touching Python objects directly,

00:49:29.280 --> 00:49:31.760
you could release the GIL.

00:49:31.760 --> 00:49:37.880
Especially in the scientific and computing and data science space,

00:49:37.880 --> 00:49:41.480
multi-threaded code has been around for a long time and we've been using it,

00:49:41.480 --> 00:49:47.200
and it's fine. Dask, you can use workers with threads or processes or both.

00:49:47.200 --> 00:49:50.080
I frequently will use Dask with four threads,

00:49:50.080 --> 00:49:52.520
and that's totally fine because most of the codes in

00:49:52.520 --> 00:49:54.760
NumPy and Pandas that release the GIL.

00:49:54.760 --> 00:49:56.600
But that's only a few use cases.

00:49:56.600 --> 00:49:59.920
If you want to expand that to the whole Python interpreter,

00:49:59.920 --> 00:50:01.120
you have to get rid of the GIL.

00:50:01.120 --> 00:50:04.640
You have to have a more fine-grained approach to concurrency.

00:50:04.640 --> 00:50:10.000
This proposal from Sam Gross at Meta was

00:50:10.000 --> 00:50:13.240
basically one of many historical attempts

00:50:13.240 --> 00:50:17.840
to get rid of that global interpreter lock.

00:50:17.840 --> 00:50:20.360
Many have been proposed and failed historically.

00:50:20.360 --> 00:50:25.680
Getting this all the way through the approval process is a real triumph.

00:50:25.680 --> 00:50:29.720
At the point where it was really being hotly contested,

00:50:29.720 --> 00:50:32.040
my maybe slightly cynical take is,

00:50:32.040 --> 00:50:36.120
we have between zero and one more chance to get this right in Python.

00:50:36.120 --> 00:50:40.080
Either it's already too late or this is it.

00:50:40.080 --> 00:50:42.080
I don't know which it is.

00:50:42.080 --> 00:50:46.480
I think there were two main complaints against this change.

00:50:46.480 --> 00:50:49.320
Complaint number one was,

00:50:49.320 --> 00:50:54.480
you theoretically have opened up a parallel compute thing.

00:50:54.480 --> 00:50:58.040
For example, on my Apple M2 Pro,

00:50:58.040 --> 00:51:01.400
I have 10 cores, so I could leverage all of those cores,

00:51:01.400 --> 00:51:05.160
maybe get a five times improvement.

00:51:05.160 --> 00:51:10.760
But single-core regular programming is now 50 percent slower.

00:51:10.760 --> 00:51:13.360
That's what most people do and we don't accept it.

00:51:13.360 --> 00:51:15.320
That's the one of the size,

00:51:15.320 --> 00:51:18.640
the Gilectomy and all that was in that realm, I believe.

00:51:18.640 --> 00:51:21.600
The other is yet to be determined,

00:51:21.600 --> 00:51:25.800
I think, is much like the Python 2-3 shift.

00:51:25.800 --> 00:51:29.800
The problem with Python 2-3 wasn't that the code of Python changed,

00:51:29.800 --> 00:51:34.720
it was that all the libraries I like and need don't work here.

00:51:34.720 --> 00:51:37.880
>> Right. What is going to happen when we take

00:51:37.880 --> 00:51:40.760
half a million libraries that were written in a world that

00:51:40.760 --> 00:51:44.840
didn't know or care about threading and are now subjected to it?

00:51:44.840 --> 00:51:47.680
>> Yeah. There's two levels of problem there.

00:51:47.680 --> 00:51:51.800
There's one that there's work that has to be done to libraries.

00:51:51.800 --> 00:51:57.800
It's usually with C extensions that they assumed a global interpreter lock,

00:51:57.800 --> 00:51:59.800
and they'll have to do some changes to change that.

00:51:59.800 --> 00:52:04.560
But the other one is a much more cultural thing.

00:52:04.560 --> 00:52:06.880
Where the existence of the Guild just meant that

00:52:06.880 --> 00:52:10.320
Python developers just wrote less threaded code.

00:52:10.320 --> 00:52:12.160
>> Yeah. They don't think about locks,

00:52:12.160 --> 00:52:13.080
they don't worry about locks,

00:52:13.080 --> 00:52:15.520
they just assume it's all going to be fine.

00:52:15.520 --> 00:52:23.120
>> Because again, the race condition doesn't protect threaded code from race conditions,

00:52:23.120 --> 00:52:25.760
but it just protects the interpreter from race conditions.

00:52:25.760 --> 00:52:27.680
You and your application logic

00:52:27.680 --> 00:52:29.920
are free to make all the thread mistakes you want.

00:52:29.920 --> 00:52:32.240
But if no one ever ran your code in multiple threads,

00:52:32.240 --> 00:52:33.960
you would never know.

00:52:33.960 --> 00:52:38.360
>> I think that's a super interesting thing,

00:52:38.360 --> 00:52:41.840
that it's a huge cultural issue that people don't think about it.

00:52:41.840 --> 00:52:44.920
Like I said, I used to do a lot of C++ and C#,

00:52:44.920 --> 00:52:47.560
and over there, you're always thinking about threading.

00:52:47.560 --> 00:52:49.880
You're always thinking about, well, what about these three steps?

00:52:49.880 --> 00:52:51.880
Does it go into a temporarily invalid state?

00:52:51.880 --> 00:52:53.560
Do I need to lock this?

00:52:53.560 --> 00:52:56.560
C# even had literally a keyword lock,

00:52:56.560 --> 00:52:58.320
which is like a context manager.

00:52:58.320 --> 00:52:59.600
You say lock curly brace,

00:52:59.600 --> 00:53:01.880
and everything in there is like into a lock and out of it.

00:53:01.880 --> 00:53:05.160
Because it's just so part of that culture.

00:53:05.160 --> 00:53:08.400
Then in Python, you just forget about it and don't worry about it.

00:53:08.400 --> 00:53:13.400
But that doesn't mean that you aren't taking five lines of Python code.

00:53:13.400 --> 00:53:16.440
Each one can run all on its own,

00:53:16.440 --> 00:53:17.920
but taken as a block,

00:53:17.920 --> 00:53:21.280
they may still get into these weird states where if

00:53:21.280 --> 00:53:23.920
another thread after three lines observes the data,

00:53:23.920 --> 00:53:25.600
it's still busted.

00:53:25.600 --> 00:53:29.080
It's just the culture doesn't talk about it very much.

00:53:29.080 --> 00:53:33.080
>> Yeah. If no one ever runs your code in multiple threads,

00:53:33.080 --> 00:53:35.800
all of those bugs are theoretical.

00:53:35.800 --> 00:53:39.400
Now, what's going to shift is,

00:53:39.400 --> 00:53:42.920
all of those C extensions will get fixed and everything will be,

00:53:42.920 --> 00:53:44.880
they'll fix those problems.

00:53:44.880 --> 00:53:47.320
Then we're going to have a second wave of everyone

00:53:47.320 --> 00:53:49.560
seeing their libraries used in

00:53:49.560 --> 00:53:51.560
threaded programs and starting to discover,

00:53:51.560 --> 00:53:52.480
what are the more subtle bugs?

00:53:52.480 --> 00:53:56.720
Do I have global state that I'm not being careful with?

00:53:56.720 --> 00:53:58.680
It's going to be painful,

00:53:58.680 --> 00:54:02.880
but I think it's necessary for Python to stay relevant into the future.

00:54:02.880 --> 00:54:06.280
I'm a little worried. One of the common questions we hear is,

00:54:06.280 --> 00:54:09.680
multiprocessing is fine, why don't we do that?

00:54:09.680 --> 00:54:13.280
Definitely, multiprocessing's big challenge

00:54:13.280 --> 00:54:18.600
is processes don't get to share data directly.

00:54:18.600 --> 00:54:21.360
Even if I have read-only data,

00:54:21.360 --> 00:54:24.120
if I have to load two gigabytes of data in every process,

00:54:24.120 --> 00:54:27.400
and I want to start 32 of them because I have a nice big computer,

00:54:27.400 --> 00:54:31.920
I've just 32X my memory usage just so

00:54:31.920 --> 00:54:36.120
that I can have multiple concurrent computations.

00:54:36.120 --> 00:54:39.360
Now, there are tricks you can play on things like Linux where you load

00:54:39.360 --> 00:54:43.720
the data once and rely on forking to preserve pages of memory.

00:54:43.720 --> 00:54:46.800
Linux does cool copy-on-write stuff when you fork.

00:54:46.800 --> 00:54:49.920
But that's fragile and not necessarily going to work.

00:54:49.920 --> 00:54:50.880
Then the second thing, of course,

00:54:50.880 --> 00:54:53.080
is if any of those have to talk to each other,

00:54:53.080 --> 00:54:55.320
now you're talking about pickling objects and putting them

00:54:55.320 --> 00:54:57.560
through a socket and handing them off.

00:54:57.560 --> 00:55:02.040
That is, again, for certain kinds of applications, just a non-starter.

00:55:02.040 --> 00:55:03.720
>> Yeah, people just start going, "Well,

00:55:03.720 --> 00:55:06.600
we're just going to rewrite this in a language that lets us share pointers."

00:55:06.600 --> 00:55:07.400
>> Yeah.

00:55:07.400 --> 00:55:09.320
>> Or at least memory in process.

00:55:09.320 --> 00:55:13.440
>> Yeah. Again, there are a lot of Python users where they don't need this,

00:55:13.440 --> 00:55:15.480
they don't care about this, this will never impact them.

00:55:15.480 --> 00:55:19.080
Then there's a whole class of Python users who are desperate for this.

00:55:19.080 --> 00:55:20.600
>> Sure.

00:55:20.600 --> 00:55:22.000
>> And really, really want it.

00:55:22.000 --> 00:55:24.400
>> I think there's a couple of interesting things here.

00:55:24.400 --> 00:55:35.440
One, I think this is important for stemming people leaving Python.

00:55:35.440 --> 00:55:37.880
I actually don't hear this that much anymore,

00:55:37.880 --> 00:55:39.600
but I used to hear a lot of,

00:55:39.600 --> 00:55:43.360
"We've left for Go because we need better parallelism,"

00:55:43.360 --> 00:55:45.200
or "We've left for this performance reason."

00:55:45.200 --> 00:55:50.080
And I don't know that's just a success story of the faster CPython initiative,

00:55:50.080 --> 00:55:53.240
or all the people who had been around and decided they needed to leave,

00:55:53.240 --> 00:55:56.440
they're gone and we just don't hear them anymore because they left.

00:55:56.440 --> 00:55:59.360
It's like, I used to hear them say this at the party,

00:55:59.360 --> 00:56:00.480
but then they said they're going to leave,

00:56:00.480 --> 00:56:01.760
and I don't hear anyone say they're leaving.

00:56:01.760 --> 00:56:04.560
Well, it's because everyone's still here who didn't say that.

00:56:04.560 --> 00:56:04.880
I don't know.

00:56:04.880 --> 00:56:12.680
But I do think having this as a capability will be important for people to be able

00:56:12.680 --> 00:56:19.280
to maybe adopt Python where Python was rejected at the proposal stage.

00:56:19.280 --> 00:56:21.960
You know, like, "Should we use Python for this project or something else?

00:56:21.960 --> 00:56:27.640
We need threading, we need computational threading, we've got 128 core, it's out."

00:56:27.640 --> 00:56:27.840
Right?

00:56:27.840 --> 00:56:30.640
And then no one comes and complains about it because they never even started

00:56:30.640 --> 00:56:32.280
that process, right?

00:56:32.280 --> 00:56:36.400
So it'll either allow more people to come into Python or prevent them for leaving

00:56:36.400 --> 00:56:38.720
for that same argument on some projects.

00:56:38.720 --> 00:56:41.720
I think that's a pretty positive thing here.

00:56:41.720 --> 00:56:42.680
- Yeah.

00:56:42.680 --> 00:56:46.760
We don't get to count all of the projects that didn't come into existence

00:56:46.760 --> 00:56:49.440
because of the global interpreter lock.

00:56:49.440 --> 00:56:53.520
It's easy when you're in it to sort of adjust your thinking to not see the

00:56:53.520 --> 00:56:56.360
limitation anymore because you're so used to routing around it.

00:56:56.360 --> 00:56:59.240
You don't even stop and think, "Oh, man, I got to worry about threads."

00:56:59.240 --> 00:57:00.280
You just don't think threads.

00:57:00.280 --> 00:57:01.000
- Yeah.

00:57:01.000 --> 00:57:02.880
I'll give...I totally agree.

00:57:02.880 --> 00:57:08.200
And I'll give people two other examples that maybe resonate more if this

00:57:08.200 --> 00:57:12.520
doesn't resonate with them.

00:57:12.520 --> 00:57:13.880
It's...what have I said?

00:57:13.880 --> 00:57:19.360
Oh, it's a little bit challenging to write this type of mobile phone application

00:57:19.360 --> 00:57:20.360
in Python.

00:57:20.360 --> 00:57:24.880
Like, well, it's nearly impossible to write a mobile phone application in Python.

00:57:24.880 --> 00:57:30.440
So we're not even focusing on that as an issue because no one is...I know,

00:57:30.440 --> 00:57:33.120
beware and a few other things, there's a little bit of work.

00:57:33.120 --> 00:57:36.480
So I don't want to like...I'm not trying to talk bad about them.

00:57:36.480 --> 00:57:40.920
But as a community, there's not like...it's not a React Native sort of thing or a

00:57:40.920 --> 00:57:43.160
Flutter where there's a huge community of people who are just like,

00:57:43.160 --> 00:57:43.960
"And we could do this."

00:57:43.960 --> 00:57:46.800
And then how do we...like, there's just not a lot of talk about it.

00:57:46.800 --> 00:57:52.120
And that doesn't mean that people wouldn't just love to write mobile apps

00:57:52.120 --> 00:57:53.120
in Python.

00:57:53.120 --> 00:57:58.280
It's just...it's so far out of reach that it's just a little whisper in the corner

00:57:58.280 --> 00:58:00.960
for people trying to explore that rather than a big din.

00:58:00.960 --> 00:58:05.160
And I think, you know, same thing about desktop apps, right?

00:58:05.160 --> 00:58:10.040
Wouldn't it be awesome if we could not have Electron but like some really cool,

00:58:10.040 --> 00:58:12.760
super nice UI thing that's almost pure Python?

00:58:12.760 --> 00:58:13.760
It would.

00:58:13.760 --> 00:58:16.880
But people were not focused on it because no one's trying to do it.

00:58:16.880 --> 00:58:20.240
But no one's trying to do it because there weren't good options to do it with, right?

00:58:20.240 --> 00:58:23.760
And I think the same story is going to happen around performance and stuff.

00:58:23.760 --> 00:58:27.120
- I mean, yeah, just to jump in, you know, since I have to talk about the

00:58:27.120 --> 00:58:28.000
beware folks.

00:58:28.000 --> 00:58:30.840
I mean, you've described exactly...the reason why we funded the beware

00:58:30.840 --> 00:58:37.160
development is because, yeah, if we don't work on that now before people sort of...

00:58:37.160 --> 00:58:40.080
there's a lot of work that has to do before you reach that point where it's easy.

00:58:40.080 --> 00:58:47.200
And so recently, the team was able to get sort of tier three support for iOS and

00:58:47.200 --> 00:58:49.880
Android into CPython 3.13.

00:58:49.880 --> 00:58:55.000
So now we're at the first rung of the ladder of iOS and Android support in CPython.

00:58:55.000 --> 00:58:56.080
- That's awesome.

00:58:56.080 --> 00:59:00.680
- And Toga and Briefcase, the two components of beware, are really focused,

00:59:00.680 --> 00:59:03.520
again, on that, yeah, how do I make apps?

00:59:03.520 --> 00:59:05.240
How do I make...for desktop and mobile?

00:59:05.240 --> 00:59:08.640
And so, but it's, yeah, what we ran into people is that they just didn't even

00:59:08.640 --> 00:59:11.040
realize you could even think about doing that.

00:59:11.040 --> 00:59:15.040
And so they just...they never stopped to say, "Oh, I wish I could do this in Python,"

00:59:15.040 --> 00:59:16.480
because they just assumed you couldn't.

00:59:16.480 --> 00:59:17.280
- Yeah.

00:59:17.280 --> 00:59:22.480
And all the people who really needed to, like, were required to leave the ecosystem

00:59:22.480 --> 00:59:24.320
and make another choice, right?

00:59:24.320 --> 00:59:25.040
- Yeah.

00:59:25.040 --> 00:59:29.120
And it will take the same amount of...I was going to say, it'll take the same amount

00:59:29.120 --> 00:59:30.040
of time with this.

00:59:30.040 --> 00:59:34.240
Even once threads are possible in Python, it'll take years to shift the perception.

00:59:34.240 --> 00:59:34.680
- Yeah.

00:59:34.680 --> 00:59:37.040
And probably some of the important libraries.

00:59:37.040 --> 00:59:37.920
- Yeah.

00:59:37.920 --> 00:59:38.640
- Yeah.

00:59:38.640 --> 00:59:39.200
All right.

00:59:39.200 --> 00:59:42.520
So I'm pretty excited about this.

00:59:42.520 --> 00:59:46.560
I was hoping something like this would come, and I didn't know what form it would be.

00:59:46.560 --> 00:59:49.840
I said there were the two limitations, the libraries and the culture,

00:59:49.840 --> 00:59:53.120
which you called out very awesomely, and then also the performance.

00:59:53.120 --> 00:59:59.440
And this one is either neutral or a little bit better in terms of performance.

00:59:59.440 --> 01:00:05.360
So it doesn't have that disqualifying, killing of the single-threaded performance.

01:00:05.360 --> 01:00:09.720
And the person I'm taking here, I will say, again, because you have to be fairly

01:00:09.720 --> 01:00:14.880
conservative with CPython because so many people use it, is that this will be an

01:00:14.880 --> 01:00:18.720
experimental option that by default, Python won't turn this on.

01:00:18.720 --> 01:00:22.680
You will have Python 3.13, when you get it, will still have the global

01:00:22.680 --> 01:00:23.840
interpreter locked.

01:00:23.840 --> 01:00:28.400
But if you build Python 3.13 yourself or you get another kind of experimental build

01:00:28.400 --> 01:00:32.160
of it, there's a flag now at the build stage to turn off the GIL.

01:00:32.160 --> 01:00:36.520
So in this mode, they decided to make, you know, not have to make double negatives.

01:00:36.520 --> 01:00:39.320
This is Python in free-threading mode.

01:00:39.320 --> 01:00:44.240
And that will be an experimental thing for the community to test, to try out,

01:00:44.240 --> 01:00:47.080
to benchmark, and do all these things for a number of years.

01:00:47.080 --> 01:00:50.600
They've taken a very measured approach, and they're saying, "We're not going to force

01:00:50.600 --> 01:00:54.200
the whole community to switch to this until it's proven itself out."

01:00:54.200 --> 01:00:57.920
Everyone's had time to port the major libraries, to try it out,

01:00:57.920 --> 01:01:03.160
to see that it really does meet the promise of not penalizing single-threaded

01:01:03.160 --> 01:01:04.360
stuff too much.

01:01:04.360 --> 01:01:07.040
- Yeah, or breaking the single-threaded code too much, yeah.

01:01:07.040 --> 01:01:07.800
- Yeah.

01:01:07.800 --> 01:01:14.200
And so, the steering council is reserving the right to decide when this becomes,

01:01:14.200 --> 01:01:18.640
or if this becomes the official way for Python, you know, I don't know,

01:01:18.640 --> 01:01:19.840
3.17 or something.

01:01:19.840 --> 01:01:21.960
I mean, it could be several years.

01:01:21.960 --> 01:01:24.840
And so, I just want everyone not to panic.

01:01:24.840 --> 01:01:26.360
- Yeah, exactly.

01:01:26.360 --> 01:01:28.600
- This doesn't get turned on in October.

01:01:28.600 --> 01:01:32.080
- No, and this is super interesting.

01:01:32.080 --> 01:01:39.800
Yeah, so it's accepted, but it only appears in your Python runtime

01:01:39.800 --> 01:01:41.560
if you build it with this.

01:01:41.560 --> 01:01:44.960
So, I imagine, you know, some people will build it themselves,

01:01:44.960 --> 01:01:48.880
but someone also just created a Docker container with Python built with this,

01:01:48.880 --> 01:01:52.040
and you can get the free-threaded Docker version or whatever, right?

01:01:52.040 --> 01:01:54.440
- We've already put out conda packages as well.

01:01:54.440 --> 01:01:55.880
So, if you want to build a conda environment...

01:01:55.880 --> 01:01:56.800
- Okay.

01:01:56.800 --> 01:02:00.720
- Yeah, actually, if you jump over to the Py free-thread page.

01:02:00.720 --> 01:02:02.080
- Yeah, tell people about this.

01:02:02.080 --> 01:02:02.520
- Yeah.

01:02:02.520 --> 01:02:04.520
So, you know, we didn't make this.

01:02:04.520 --> 01:02:08.120
This is the community made this, the scientific Python community put

01:02:08.120 --> 01:02:11.960
this together, and this is a really great resource, again, focused on,

01:02:11.960 --> 01:02:16.680
you know, that community which really wants threading because we have a lot of,

01:02:16.680 --> 01:02:18.640
you know, heavy numerical computation.

01:02:18.640 --> 01:02:21.800
And so, this is a good resource for things like, how do you install it?

01:02:21.800 --> 01:02:24.440
So, there's a link there on what are your options for installing the

01:02:24.440 --> 01:02:25.520
free-threaded CPython?

01:02:25.520 --> 01:02:29.560
You can get it from Ubuntu or PyEnv or conda.

01:02:29.560 --> 01:02:34.320
If you go look at the, you know, and you can build it from source.

01:02:34.320 --> 01:02:36.040
And so, there's also...

01:02:36.040 --> 01:02:37.640
- A container, yeah.

01:02:37.640 --> 01:02:41.440
- Yeah, and there's also a compatibility status tracker which is being updated

01:02:41.440 --> 01:02:46.000
manually to see which of your, you know, again, this is focused,

01:02:46.000 --> 01:02:49.080
I think, yeah, down one more link.

01:02:49.080 --> 01:02:49.640
- Where is the...

01:02:49.640 --> 01:02:50.360
Oh, there it is, yeah.

01:02:50.360 --> 01:02:51.200
- There it is, yeah.

01:02:51.200 --> 01:02:54.200
So, these are, again, this is very focused on the kind of things the scientific

01:02:54.200 --> 01:02:57.760
Python community cares about, but these are things like, you know,

01:02:57.760 --> 01:02:58.800
have we ported Cython?

01:02:58.800 --> 01:03:01.120
Have we ported NumPy?

01:03:01.120 --> 01:03:02.400
Is it being automatically tested?

01:03:02.400 --> 01:03:03.960
Which release has it?

01:03:03.960 --> 01:03:08.800
And the nice thing actually is Pip, as of 24.1, I believe, can tell the difference

01:03:08.800 --> 01:03:12.200
between wheels for regular Python and free-threaded Python.

01:03:12.200 --> 01:03:13.800
- Oh, you can tell by the...

01:03:13.800 --> 01:03:15.040
There's different wheels as well?

01:03:15.040 --> 01:03:18.600
- Yeah, so there's a, you know, Python has always had this thing called an

01:03:18.600 --> 01:03:22.400
ABI tag, which is just a letter that you stick after the version number.

01:03:22.400 --> 01:03:25.160
And T is the one for free-threading.

01:03:25.160 --> 01:03:31.240
And so, now a project can choose to upload wheels for both versions and make it easier

01:03:31.240 --> 01:03:32.760
for people to test out stuff.

01:03:32.760 --> 01:03:36.280
So, for example, I mean, Cython, it looks like there are nightly wheels

01:03:36.280 --> 01:03:38.320
already being built.

01:03:38.320 --> 01:03:40.920
And so, this is, you know, they're moving fast.

01:03:40.920 --> 01:03:44.680
And, you know, definitely, you know, and in our condo, we're also very interested

01:03:44.680 --> 01:03:46.520
in getting into this as well.

01:03:46.520 --> 01:03:49.320
So, that's why we built the condo package for free-threading, and we're going to start

01:03:49.320 --> 01:03:52.200
looking at building more condo packages for these things in order to be able

01:03:52.200 --> 01:03:53.360
to facilitate testing.

01:03:53.360 --> 01:03:56.600
Because I think the biggest thing we want to make sure is, if you want to know if your

01:03:56.600 --> 01:04:01.960
code works, you want the quickest way to get an environment to have some place to test.

01:04:01.960 --> 01:04:02.200
- Yeah.

01:04:02.200 --> 01:04:06.360
- And so, making this more accessible to folks is a really high priority.

01:04:06.360 --> 01:04:09.080
- This is cool.

01:04:09.080 --> 01:04:11.960
There was something like this for Python 2 to 3, I remember.

01:04:11.960 --> 01:04:18.520
It showed, like, the top 1,000 packages on PyPI, and then how many of them were compatible

01:04:18.520 --> 01:04:22.680
with Python 3, basically, by expressing their language tag or something like that.

01:04:22.680 --> 01:04:23.720
- Yep, yep.

01:04:23.720 --> 01:04:24.600
- So, this is kind of like that.

01:04:24.600 --> 01:04:26.440
It's also kind of like the "Can I use?"

01:04:26.440 --> 01:04:27.400
I don't know if you're familiar with this.

01:04:27.400 --> 01:04:27.480
- Yes.

01:04:27.480 --> 01:04:32.120
- "Can I use?" from the web dev world, where you go and say...

01:04:32.120 --> 01:04:32.840
- Oh, yeah.

01:04:32.840 --> 01:04:33.640
Oh, awesome.

01:04:33.640 --> 01:04:34.360
Yeah, yeah, I've seen this.

01:04:34.360 --> 01:04:36.120
- You go and say, "I want to use this.

01:04:36.120 --> 01:04:37.160
I want to use this feature."

01:04:37.960 --> 01:04:43.720
If I want to say "web workers" or something like that, it'll show you all the browsers

01:04:43.720 --> 01:04:45.640
and all the versions, and when were they supported.

01:04:45.640 --> 01:04:49.640
And this sounds a little bit like that, but for free-threaded Python.

01:04:49.640 --> 01:04:52.200
Which, by the way, free-threaded Python is the terminology, right?

01:04:52.200 --> 01:04:53.560
Not "no-gil," but "free-threaded."

01:04:53.560 --> 01:04:54.040
- Yes.

01:04:54.040 --> 01:04:55.320
That is what they've decided.

01:04:55.320 --> 01:04:58.680
I think they're worried about people trying to talk about "no-no-gil" or...

01:04:58.680 --> 01:04:59.000
I don't know.

01:04:59.000 --> 01:05:01.480
- "Gill-ful."

01:05:01.480 --> 01:05:02.600
- Yeah, exactly.

01:05:02.600 --> 01:05:03.400
- "Gill-ful Python."

01:05:03.400 --> 01:05:04.440
Are you running on a gill-ful?

01:05:04.440 --> 01:05:05.720
You know.

01:05:05.720 --> 01:05:06.440
Oh, my gosh.

01:05:06.440 --> 01:05:07.480
- Okay.

01:05:07.480 --> 01:05:08.760
Interesting.

01:05:08.760 --> 01:05:12.360
Now, we have a few other things to talk about, but we don't have really much time to talk

01:05:12.360 --> 01:05:12.680
about them.

01:05:12.680 --> 01:05:21.000
But there was one thing that we were maybe going to talk about a bit with compiling.

01:05:21.000 --> 01:05:24.680
You mentioned some talk or something where people were talking about, "Well, what if

01:05:24.680 --> 01:05:27.320
we had a static-languaged Python and we compiled it?"

01:05:27.320 --> 01:05:35.240
And related to that, Mr. Magnetic says, "Could a Python program be compiled into a binary,

01:05:35.240 --> 01:05:38.840
like a JAR or a Go app or whatever?"

01:05:38.840 --> 01:05:44.120
- Yeah, there are other tools that look at that as a standalone executable.

01:05:44.120 --> 01:05:49.080
So, yeah, one of the things I just wanted to shout out a colleague of mine at Anaconda,

01:05:49.080 --> 01:05:53.880
Antonio Cuny, who is a well-known PyPy developer from long ago.

01:05:53.880 --> 01:05:55.160
He's worked on PyPy for 20 years.

01:05:55.160 --> 01:05:55.400
- Right.

01:05:55.400 --> 01:05:55.960
- He's been working...

01:05:55.960 --> 01:05:59.320
- And not the package installing thing, but the JIT compiler.

01:05:59.320 --> 01:05:59.800
- Oh, yes.

01:05:59.800 --> 01:06:00.520
- PYPY.

01:06:00.520 --> 01:06:01.160
- Yes.

01:06:01.160 --> 01:06:04.360
- Sometimes phonetically, like over audio, that's hard to tell.

01:06:04.360 --> 01:06:05.960
- Yes, yeah, yeah.

01:06:05.960 --> 01:06:07.880
So he's been thinking about this stuff for a very long time.

01:06:07.880 --> 01:06:16.200
And his sort of key insight that at least clicked in my head was that Python is hard

01:06:16.200 --> 01:06:17.880
to compile because it is so dynamic.

01:06:17.880 --> 01:06:23.960
I can, in principle, modify the attributes, like even the functions of a class at any

01:06:23.960 --> 01:06:25.640
point in the execution of the program.

01:06:25.640 --> 01:06:27.000
I can monkey-patch anything.

01:06:27.000 --> 01:06:27.480
I can do...

01:06:27.480 --> 01:06:33.160
This dynamicness is really great for making kind of magical metaprogramming libraries

01:06:33.160 --> 01:06:36.760
that do amazing things with very little typing.

01:06:36.760 --> 01:06:38.840
But it makes compiling them really hard.

01:06:38.840 --> 01:06:42.520
Because you don't get to ever say, "Okay, this can't ever change."

01:06:42.520 --> 01:06:48.680
And so what he's been trying to do with a project called Spy, which he gave a talk on

01:06:48.680 --> 01:06:53.080
at PyCon 2024, but I think the recordings aren't up yet for that.

01:06:53.080 --> 01:06:55.560
And so there isn't a...

01:06:55.560 --> 01:06:57.960
I don't think there's a public page on it, but he does have a talk on it.

01:06:57.960 --> 01:07:02.840
Because I think they've got the keynotes up.

01:07:03.560 --> 01:07:10.280
But the key kind of insight for me for Spy was to recognize that in a typical Python

01:07:10.280 --> 01:07:13.880
program, all of the dynamic metaprogramming happens at the beginning.

01:07:13.880 --> 01:07:18.280
You're doing things like data classes generating stuff and all kinds of things like that.

01:07:18.280 --> 01:07:22.200
And then there's a phase where that stops.

01:07:22.200 --> 01:07:29.480
And so if we could define a sort of variant of Python where those two phases were really

01:07:29.480 --> 01:07:35.800
clear, then you would get all of the dynamic expressiveness, almost all the dynamic expressiveness

01:07:35.800 --> 01:07:41.640
of Python, but still have the ability to then feed that into a compiler tool chain and get

01:07:41.640 --> 01:07:42.680
a binary.

01:07:42.680 --> 01:07:49.000
So this is super early R&D experimental work, but I think that's a really great way to approach

01:07:49.000 --> 01:07:54.440
it because often there's always been this tension of, "Well, if I make Python statically

01:07:54.440 --> 01:07:58.200
compilable, is it just C with different keywords?

01:07:58.200 --> 01:08:01.320
Do I lose the thing I loved about Python?"

01:08:01.320 --> 01:08:04.440
Which was how quickly I could express my idea.

01:08:04.440 --> 01:08:07.800
And so this is, again, to having your cake and eating it too.

01:08:07.800 --> 01:08:12.200
This is trying to find a way to split that difference in a way that lets us get most

01:08:12.200 --> 01:08:13.880
of the benefits of both sides.

01:08:13.880 --> 01:08:16.440
That's pretty interesting.

01:08:16.440 --> 01:08:18.280
And hopefully that talk's up soon.

01:08:18.280 --> 01:08:19.080
That'd be really neat.

01:08:19.080 --> 01:08:22.680
Maybe by the time this episode's out.

01:08:22.680 --> 01:08:29.000
I know the PyCon videos are starting to roll, not out on YouTube, but out on the podcast

01:08:29.000 --> 01:08:29.500
channels.

01:08:29.500 --> 01:08:32.840
It'll be interesting.

01:08:32.840 --> 01:08:39.000
It would be fantastic to have, "Here's my binary of Python.

01:08:39.000 --> 01:08:41.320
Take my data science app and run it.

01:08:41.320 --> 01:08:42.840
Take my desktop app and run it."

01:08:42.840 --> 01:08:45.480
I don't care what you have installed on your computer.

01:08:45.480 --> 01:08:52.600
I don't need you to set up Python 3.10 or higher on your machine and set up a virtual

01:08:52.600 --> 01:08:53.320
environment.

01:08:53.320 --> 01:08:54.920
Just, "Here's my binary."

01:08:54.920 --> 01:08:57.240
Do it as you will.

01:08:57.240 --> 01:08:58.520
That's another.

01:08:58.520 --> 01:09:05.960
I throw that in with the mobile apps and the front end, or the desktop apps, or the front

01:09:05.960 --> 01:09:06.600
end Python.

01:09:06.600 --> 01:09:11.080
That's another one of those things that nobody's pushing towards it.

01:09:11.080 --> 01:09:11.720
Well, not nobody.

01:09:11.720 --> 01:09:15.000
Not that many people are pushing towards it because there's not that many use cases for

01:09:15.000 --> 01:09:21.160
it that people are using it for because it was so challenging that people stopped trying

01:09:21.160 --> 01:09:21.660
to do that.

01:09:21.660 --> 01:09:23.880
Yeah, that's one thing I also...

01:09:23.880 --> 01:09:32.280
People probably hear me say this too many times, but most people use apps when they

01:09:32.280 --> 01:09:35.640
use a computer, not packages or environments.

01:09:35.640 --> 01:09:42.600
And so in the Python space, we are constantly grappling with how hard packages and environments

01:09:42.600 --> 01:09:47.880
are to work with, talk with, decide again, what languages are in, do I care about everything

01:09:47.880 --> 01:09:49.320
or just Python or whatever?

01:09:49.320 --> 01:09:52.760
It's all very hard, but that's actually not how most people interact with a computer at

01:09:52.760 --> 01:09:53.400
all.

01:09:53.400 --> 01:09:56.440
And so it really is one of those things.

01:09:56.440 --> 01:10:01.880
Again, this is one of the reasons I'm so interested in Beware is briefcase is like the app packager.

01:10:01.880 --> 01:10:05.160
And the more they can push on that, the more we have a story.

01:10:05.160 --> 01:10:08.040
Again, there are other tools that have been around for a long time, but that's just what

01:10:08.040 --> 01:10:08.680
I think about a lot.

01:10:09.320 --> 01:10:13.960
But we need to focus on tools for making apps because that's how we're going to share our

01:10:13.960 --> 01:10:16.280
work with 99% of the earth.

01:10:16.280 --> 01:10:17.560
Yes.

01:10:17.560 --> 01:10:18.600
Yeah, 100%.

01:10:18.600 --> 01:10:19.240
I totally agree.

01:10:19.240 --> 01:10:25.640
And lots of props to Keith Russell-Magee and the folks over at Beware for doing that and

01:10:25.640 --> 01:10:31.000
for you guys supporting that work because it's one of those things where there's not

01:10:31.000 --> 01:10:33.560
a ton of people trying to do it.

01:10:33.560 --> 01:10:36.600
It's not like, well, we're using Django, but is there another way we could do it?

01:10:36.600 --> 01:10:38.040
It's basically the same thing, right?

01:10:39.000 --> 01:10:43.720
It's creating a space for Python where it kind of, I know there's PyInstaller and Py2App,

01:10:43.720 --> 01:10:45.560
but it's pretty limited, right?

01:10:45.560 --> 01:10:48.040
Yeah, there's not a lot of effort there.

01:10:48.040 --> 01:10:52.600
And so there are a few people who've been doing it for a long time and others are getting

01:10:52.600 --> 01:10:53.240
more into it.

01:10:53.240 --> 01:11:00.280
And yeah, I wish that we could get more focus on it because there are tools, they just don't

01:11:00.280 --> 01:11:01.400
get a lot of attention.

01:11:01.400 --> 01:11:05.400
Yeah, and they're not very polished and there's so many edge cases and scenarios.

01:11:06.520 --> 01:11:09.880
All right, let's close it out with just a final thought on this little topic and then

01:11:09.880 --> 01:11:11.480
I'll let you wrap this up for us.

01:11:11.480 --> 01:11:15.720
Do you think that's maybe a core developer thing?

01:11:15.720 --> 01:11:20.120
I mean, I know it's awesome that Py2App and PyInstaller and PyFreeze are doing their things,

01:11:20.120 --> 01:11:25.160
that Togr are doing their things to try to make this happen, but I feel like they're

01:11:25.160 --> 01:11:30.120
kind of looking in at Python and go, how can we grab what we need out of Python and jam

01:11:30.120 --> 01:11:31.880
it into an executable and make it work?

01:11:33.800 --> 01:11:38.840
Should we be encouraging the core developers to just go like a Python, MyScript, dash dash

01:11:38.840 --> 01:11:43.160
Windows and they're out, you get in .exe or something?

01:11:43.160 --> 01:11:45.880
I don't know, actually, that would be a great question.

01:11:45.880 --> 01:11:47.320
Actually, I would ask Russell that question.

01:11:47.320 --> 01:11:50.440
He would have probably better perspective than I would.

01:11:50.440 --> 01:11:54.600
Because at some level, it is a tool that is dealing with a lot of problems that aren't

01:11:54.600 --> 01:11:56.280
core to the Python language.

01:11:56.280 --> 01:12:03.000
And so maybe having it outside is helpful, but maybe there's other things that the core

01:12:03.000 --> 01:12:04.600
could do to support it.

01:12:04.600 --> 01:12:09.480
I mean, again, a lot of it has to do with the realities of when you drop an application

01:12:09.480 --> 01:12:11.400
onto a system, you need it to be self-contained.

01:12:11.400 --> 01:12:16.760
You need to, sometimes you have to, do you have to trick the import library to know where

01:12:16.760 --> 01:12:18.440
to find things and all of that.

01:12:18.440 --> 01:12:24.200
That's exactly what I was thinking, is if Python itself didn't require operating system

01:12:24.200 --> 01:12:32.040
level fakes to make it think, if it could go like, here is a thing in memory where you

01:12:32.040 --> 01:12:33.960
just import, this is the import space.

01:12:33.960 --> 01:12:36.520
It's this memory address for these things.

01:12:36.520 --> 01:12:40.760
And we just run from the .exe rather than dump a bunch of stuff temporarily on disk

01:12:40.760 --> 01:12:43.480
and port it through, that kind of weirdness that happens sometimes.

01:12:43.480 --> 01:12:47.080
Yeah, there is probably definitely improvements that could be made to the import mechanism

01:12:47.080 --> 01:12:48.920
to support applications.

01:12:48.920 --> 01:12:49.960
Yeah, exactly.

01:12:49.960 --> 01:12:51.960
Well, we've planted that seed.

01:12:51.960 --> 01:12:53.160
Maybe it will grow, we'll see.

01:12:53.160 --> 01:12:56.280
All right, Stan, this has been an awesome conversation.

01:12:56.280 --> 01:13:00.920
Give us a wrap up on all this stuff, just like sort of final call to action and summary

01:13:00.920 --> 01:13:03.960
of what you guys are doing in Anaconda, because there's a bunch of different stuff we talked

01:13:03.960 --> 01:13:05.240
about that are in this space.

01:13:05.240 --> 01:13:10.360
Yeah, I mean, mainly I would say, I would encourage people that if you want to speed

01:13:10.360 --> 01:13:12.760
up your Python program, you don't necessarily have to leave Python.

01:13:12.760 --> 01:13:18.200
Go take a look at some of these tools, go measure what your program's doing, look at

01:13:18.200 --> 01:13:21.400
tools like Numba, but there are other ones out there, PyTorch and Jax and all sorts of

01:13:21.400 --> 01:13:21.640
things.

01:13:21.640 --> 01:13:23.640
There are lots of choices now for speed.

01:13:23.640 --> 01:13:25.480
And so Python doesn't have to be slow.

01:13:25.480 --> 01:13:30.600
You just have to sort of figure out what you're trying to achieve and find the best tool for

01:13:30.600 --> 01:13:31.080
that.

01:13:31.080 --> 01:13:37.560
So one other thing I do want to shout out, I'm teaching a tutorial in a month over at

01:13:37.560 --> 01:13:43.880
the Anaconda sort of live tutorial system, which will be how to use Numba.

01:13:43.880 --> 01:13:52.600
So if something you saw here you want to go deep on, there will be a tutorial, hopefully

01:13:52.600 --> 01:13:53.960
linked in the show notes or something.

01:13:53.960 --> 01:13:55.320
Yeah, I can link that in the show notes.

01:13:55.320 --> 01:13:56.120
No problem.

01:13:56.120 --> 01:13:56.600
Absolutely.

01:13:56.600 --> 01:13:58.600
So I'll be going in.

01:13:58.680 --> 01:14:01.160
Is that the high performance Python with Numba?

01:14:01.160 --> 01:14:02.520
Yes.

01:14:02.520 --> 01:14:03.080
Yes.

01:14:03.080 --> 01:14:06.760
So yeah, we'll be doing worked examples and you'll get to ask questions and all that

01:14:06.760 --> 01:14:07.000
stuff.

01:14:07.000 --> 01:14:07.960
Cool.

01:14:07.960 --> 01:14:10.280
I'll make sure to put that in the show notes so people can check it out.

01:14:10.280 --> 01:14:11.880
Cool.

01:14:11.880 --> 01:14:12.840
All right.

01:14:12.840 --> 01:14:16.840
Well, thanks for sharing all the projects that you guys are working on and just the

01:14:16.840 --> 01:14:19.400
broader performance stuff that you're tracking.

01:14:19.400 --> 01:14:20.920
Yeah, awesome.

01:14:20.920 --> 01:14:21.880
Glad to chat.

01:14:21.880 --> 01:14:23.080
You bet.

01:14:23.080 --> 01:14:24.200
See you later.

01:14:24.200 --> 01:14:25.560
Bye.

