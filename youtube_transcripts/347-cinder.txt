00:00:00 - Dino, welcome to Talk Python To Me.

00:00:03 - Hi Michael, thanks for having me.

00:00:05 - I'm really excited to talk to you.

00:00:07 You've been involved in a lot of projects that I've wanted to talk to you about over the years and haven't yet, so we're gonna touch on a couple of those.

00:00:16 But we've got some really big news around Cinder and some performance stuff that you all over at Instagram are doing to try to make Python faster.

00:00:24 You did a really cool PyCon keynote, or not keynote, a talk on that.

00:00:28 So we're gonna dive deep into this alternate reality runtime of CPython called Cinder that you all have created.

00:00:35 That's gonna be a lot of fun.

00:00:37 - Yeah.

00:00:38 And it's only slightly alternate reality.

00:00:41 - It's not that much of an alternate reality, just a little bit.

00:00:43 - Yeah.

00:00:44 - Before we do though, let's just hear your story.

00:00:48 How'd you get into programming in Python?

00:00:50 - So programming, I started programming when I was a teenager.

00:00:57 I got into computers initially, really through BBSs.

00:01:02 - Oh, yes.

00:01:03 - Which maybe probably a lot of people--

00:01:05 - Was this pre-internet?

00:01:05 This was pre, this is like dial up, oh, you would dial into the BBS?

00:01:10 Oh my God.

00:01:11 - Yep, yeah, like, you know, I had a modem, someone else had a modem sitting in their home, waiting for people to call in.

00:01:18 You'd log in, send emails, post messages, take your turns on games, log out, and someone else could log in and respond to your emails.

00:01:28 - It was so amazing.

00:01:28 It's been email mint, wait for another BBS to dial in to connect to that one to like sync its local batch of emails.

00:01:36 It was like a peer to peer email, it was so weird.

00:01:38 - Yeah, yeah.

00:01:39 I mean, or like there's a lot of local emails, right?

00:01:42 Where it's just on, you're waiting for the other person to have a chance to log in.

00:01:46 But yeah, there's also that network, like a couple of different big networks.

00:01:52 It was such a different time.

00:01:54 - It was such a different time.

00:01:56 I was not super into this as much.

00:01:58 My brother was really into it.

00:02:00 We had two phone lines so that we could do more of this.

00:02:04 Did you ever play Trade Wars or any of the games that were out there?

00:02:07 - Yes, Trade Wars was awesome.

00:02:08 - Real good.

00:02:09 I think I would still enjoy Trade Wars.

00:02:11 It was so good.

00:02:12 - Yeah.

00:02:13 I was still playing Trade Wars in college.

00:02:16 We formed teams and were trying to take over some trade wars that was available over the internet, actually, that you could like, telnet into and play.

00:02:28 (laughs)

00:02:29 - A lot of this BBS stuff had sort of found a home over telnet for a while, hadn't it?

00:02:34 - Yeah.

00:02:35 - Yeah.

00:02:36 - And I think, I think the main BBS software that I used, that was used in St. Louis where I grew up, which was World War IV, with, I think it's still around and like available for you to like, if you really want to host it on some internet server, but who's going to do that?

00:02:58 - Incredible.

00:03:01 Okay, so how's the BBS story fed into the programming side of things?

00:03:06 - So the BBS software that kind of was really popular, you could get a license to it for 50 bucks and you got the source code for it along with it.

00:03:17 and there's a big active modding community.

00:03:21 And so, you know, I started off like taking people's mods and applying them and then trying to make my own mods and like just ended up teaching myself C.

00:03:31 And initially very poorly taught myself C, but then, you know, finally got good at it at some point.

00:03:40 - How fun.

00:03:43 - Yeah.

00:03:45 Did other people use your mods or were you running your own BBS or anything like that?

00:03:50 I did a really bad job at running my own BBS.

00:03:57 I petitioned my parents for a second phone line, but I also wanted to use it for phone calls.

00:04:04 So to call my BBS, you had to dial in, and then I had this device where you could punch four extra codes and it would connect you to the modem.

00:04:14 So that was kind of annoying and didn't make it the world's most popular BBS.

00:04:17 And it was rather short lived.

00:04:20 - Hurt some of the automation, yeah.

00:04:22 - Yeah.

00:04:24 But I published my mods.

00:04:26 My friends ran BBSes.

00:04:28 They'd pick up some of the mods.

00:04:30 I don't know that I was the most popular modder out there.

00:04:33 I should go and see if I can find them.

00:04:35 That might be terrifying, though.

00:04:37 - Yeah, that might be terrifying.

00:04:38 But it could also be amazing.

00:04:39 All right, let's wrap up the BBS side of things.

00:04:43 putting some bookends on the timeframe here.

00:04:44 What was the beginning baud rate and end baud rate of your BBS time?

00:04:50 - 2400 to 57.6K.

00:04:54 - Yeah, so you took it all the way to the end there.

00:04:57 2400 probably meant it didn't require putting the phone on.

00:05:01 - No, no coupler.

00:05:02 - Like the war games.

00:05:04 - Yeah, never that bad.

00:05:07 - Fantastic.

00:05:09 All right, how about Python?

00:05:12 So I got into Python in a very weird way 'cause I started working on a Python implementation having really never touched or used Python before.

00:05:23 Obviously I'd heard about it and I was kind of like, "Significant white space, "that sounds weird." But ended up really loving working on it on IronPython, really loving the language and the way it was designed.

00:05:40 It's a very, it gave me a very weird outlook on Python, I think, just because I knew all sorts of weird corner cases about Python and the language and all the details there, but then didn't really know much about libraries and things like that.

00:05:59 And to some extent that continues today, but I get to write a lot more Python code today too.

00:06:07 but like always having been on the implementation side is a little strange.

00:06:12 - It is strange.

00:06:15 And it is, I guess it would be a weird way to get to know the language.

00:06:18 So I feel like one of the real big powers of Python is that you can be really effective with it with a super partial understanding.

00:06:26 Like you could have literally no idea how to create a function and you could still do useful things with Python.

00:06:32 Whereas if you're gonna jump in and create Iron Python, which we'll talk about in a second, you have to start out, what are these meta classes and how do I best implement dynamic objects and all this stuff?

00:06:48 That's like the opposite of starting with a partial understanding.

00:06:51 - Well, and how do imports work?

00:06:54 That was a big thing.

00:06:57 - I remember when I learned about them, I'm like, wait, this is like running code.

00:07:01 It's not like an include file or a statically linked file or adding a reference in .NET or something like that.

00:07:10 It's, nope, it just runs whatever's in the script and it happens to be most of the time it defines behaviors but it doesn't have to.

00:07:17 - Yeah, and like how do you pick what's going to get imported?

00:07:21 And yeah, the semantics there are so complicated.

00:07:26 - Yep, there are some oddities of Python but in general, it seems to be working well for people.

00:07:36 But I can see as I'm implementing it, it could, you could definitely be pulling some hair out.

00:07:41 - And I mean, so many things implementing it are just, they're super sane.

00:07:47 Like they make a lot of sense.

00:07:50 There's just some weird corner cases that you run into that are, it's like, what's going on here?

00:07:57 And when I worked on higher end Python, we couldn't look at the source code of CPython.

00:08:02 which made things really interesting.

00:08:04 - Okay, because this predates .NET being open source and all that kind of stuff, right?

00:08:09 - Yep, yep.

00:08:11 - You don't want to be poisoned by the ideas.

00:08:15 - Yeah, and Iron Python was open source, but this was when Microsoft was still very much figuring out what, how they wanted to approach open source, and were still very cagey about it.

00:08:31 It was very interesting.

00:08:33 - Yeah, they've come a long way.

00:08:34 Many companies have, I would say.

00:08:36 It's still, there's some, - Yeah.

00:08:39 - video synchronicities, I guess, there, but certainly it's a different time now than it was then.

00:08:44 This was like, what, 2008, 2009 timeframe-ish?

00:08:49 Or 2005, maybe?

00:08:51 - It's, yeah, 2005, 2006, I think it was around Iron Python 1.0.

00:09:01 2006 sounds about right.

00:09:02 Yeah.

00:09:03 Yeah. So that's a while ago.

00:09:06 Yes.

00:09:07 Doesn't sound that long ago to me, but honestly, it's a while ago.

00:09:10 It's yeah.

00:09:12 It's kind of, yeah.

00:09:14 It's like remembering that the nineties is not 10 years ago.

00:09:17 It's true.

00:09:19 All right.

00:09:22 how about day to day?

00:09:24 What are you doing now?

00:09:24 Your Instagram, right?

00:09:25 Yeah.

00:09:27 So basically I work on our fork of CPython, which we call Sender, and my job is to help make, and my entire team's job is to make Instagram run more efficiently.

00:09:40 I mean obviously Instagram's a very large website that has a lot of traffic, and it's a very large Django app.

00:09:51 So we just spend our time trying to improve CPython and very specifically trying to improve CPython for Instagram's workload.

00:10:03 We're very driven by kind of that is our sole direction.

00:10:08 And so it lets us make some interesting decisions and drive some interesting decisions, but it's just really spending the day looking at what we can do to improve performance and going off and implementing that.

00:10:25 and making Instagram a little bit faster.

00:10:29 - So when we talk about Python and Django running Instagram, I put up a little post here of something I did yesterday, just to have some Instagram stuff to show.

00:10:39 Is that talking about the website?

00:10:41 Is that the APIs behind the scenes?

00:10:43 Like when you say Django runs Instagram, what are we talking about here?

00:10:49 - So it's the website, it's the APIs.

00:10:51 There's obviously some parts that aren't Django, but kind of everything that people's devices are interacting with is going through the Django front end.

00:11:04 And there's also a bunch of like, if we have asynchronous processes that need to take off and run in the background, that's kind of all handled by a Django tier as well.

00:11:14 So it's a good chunk of what's going on.

00:11:20 - Yeah, nice.

00:11:21 This is probably one of the, if not the largest, Django deployment there is, right?

00:11:26 This is a lot of servers we're talking about, right?

00:11:29 - I would assume so.

00:11:30 I don't know.

00:11:31 I don't know.

00:11:33 There might be something else pretty big out there.

00:11:35 - Yeah, I feel like the talk at the 2017 PyCon, remember that one we used to go to places where there are other people and we'd just go and be in the same room and stuff?

00:11:44 Read the same mail? - That was so nice.

00:11:45 - I know, it sure was.

00:11:46 And there was a cool Instagram talk about, I believe that one was about disabling the GC or something like that.

00:11:56 I feel like they said in that talk, at least at that time, that was one of the largest, not the largest Django deployment.

00:12:01 - Yeah, and we no longer disable the GC.

00:12:05 We fixed the memory leak, so that's good.

00:12:07 - Okay.

00:12:08 (laughing)

00:12:09 We're gonna talk a lot about memory and honestly, this whole conversation is gonna be a bit of a test, an assessment of my CPython internals, but I think that's okay because a lot of people out there don't know super in-depth details about CPython and I can play the person who asked the questions for them.

00:12:32 - Awesome.

00:12:33 - Awesome.

00:12:34 - I can try to answer questions.

00:12:36 (laughing)

00:12:37 - Well, we'll keep it focused on the part that you've been doing.

00:12:40 But during your talk, you mentioned a couple of things.

00:12:43 First, you said, okay, well, when we're running over on Django, we're running on, you say UWSGI, I feel like it's a micro, like it used to be a like a--

00:12:57 - Micro whiskey.

00:12:58 - Yeah, micro whiskey, so I don't know, UWSGI, micro whiskey, whatever it is.

00:13:02 - Yeah.

00:13:03 - I feel like all these projects that have interesting names should have a, press here to hear how it should be pronounced.

00:13:08 Should it be WSGI or whiskey?

00:13:10 Anyway, this micro whiskey you guys are running on and understanding how it creates child processes and forks out the work.

00:13:21 It's really important for understanding some of the improvements that you've made and some of the areas you've focused on.

00:13:28 So maybe we could start a little bit by talking about just the infrastructure and how actually the execution of Python code happens over at Instagram.

00:13:39 - Yeah.

00:13:40 So in addition to uWSGI, it's running on Linux.

00:13:45 and which is probably not surprising to anyone.

00:13:49 And one of the common things--

00:13:51 - Literally zero people are surprised now.

00:13:53 - Yeah.

00:13:54 - Hot-toast Windows server, come on.

00:13:55 (laughing)

00:13:58 - Or Solaris.

00:13:59 - Or a Raspberry Pi cluster, come on.

00:14:03 - Yeah, that'd be awesome.

00:14:05 So like one of the common things that people take advantage on Linux is fork and exec, where you start up a master process and then you fork off some trial processes and they can share all of the memory of that master process.

00:14:24 So it's a relatively cheap operation to go off and spawn those trial processes and you get a lot of sharing between those two processes, which reduces kind of the memory that you need to use and all of that good stuff.

00:14:39 And so the way uWSGI is working is that we are spawning our master process, going off importing kind of all of the website, like we try to make sure that everything gets loaded initially and then spawn off a whole bunch of worker processes which are going to actually be serving the traffic.

00:15:05 And if something happens to one of those worker processes, then the master will come in and spawn a new worker to replace it.

00:15:14 And that kind of goes on and on and on.

00:15:17 - It's also not just about durability.

00:15:21 It's also about scalability, right?

00:15:24 If one of the worker processes is busy working on a request, well, there might be nine others and the supervisor process can look and say, okay, well I got some requests gotta be processed.

00:15:36 Here, this one's not busy and sort of scale it out.

00:15:39 And that also helps a lot with Python's GIL and stuff.

00:15:43 You can just throw more of these worker processes at it to get more scalability.

00:15:47 And at some point that kind of hits the database limits anyway, so it doesn't really matter that much, right?

00:15:54 - Yeah, and I think like you can auto tune.

00:15:56 I don't know exactly all the details of our settings.

00:15:59 - But it can also be like settings in there, yeah.

00:16:02 Yeah, like it can tune for memory for stalled workers.

00:16:07 It's pretty smart.

00:16:13 - Yeah.

00:16:15 - But yeah.

00:16:17 - There's actually a really interesting, I don't know, have you, maybe you've seen this.

00:16:20 There's a really interesting post called configuring USG for production deployment over on Bloomberg Tech, talking about all these knobs that they turn to make it work better and do these different things.

00:16:34 And it's super interesting if these tuning knobs are unfamiliar to Python people.

00:16:39 Yeah, but the important takeaway here is when we're talking about running your code on a single server we're talking about five, 10, 20 copies of the same process running the same code.

00:16:52 - 40, 60.

00:16:53 (laughing)

00:16:55 - Exactly.

00:16:56 You guys pay for bigger cloud instances.

00:16:58 I mean, you have your own data centers, right?

00:17:00 So you probably get a lot of VMs.

00:17:02 - Yeah, and so that impacts a lot of the decisions that we make.

00:17:09 And we can talk about those more later.

00:17:14 I think another interesting thing about our UWSD and our deployments in general is that we're also redeploying every 10 minutes.

00:17:25 - I saw that, that blows my mind.

00:17:28 So tell me about this rapid redeployment.

00:17:32 - It blows my mind too.

00:17:34 And when I started at Facebook, I guess now Meta, but it was Facebook back then, you go through a process called bootcamp where you spend your first several weeks just learning about Facebook.

00:17:48 And one of the first things you learn is like, Facebook.com redeploys every three to four hours.

00:17:53 I'm like, that's insanely fast.

00:17:55 and then land an Instagram.

00:17:58 We redeploy every 10 minutes.

00:18:00 It's like, what?

00:18:01 (laughs)

00:18:03 So-- - Yeah, that's incredible.

00:18:06 - It like-- - Can you talk about why that is?

00:18:08 Is there just that many improvements in code changes going on or is there some other balancing reason that this happens?

00:18:17 - I don't know what all the original reasoning is.

00:18:21 It has a very nice.

00:18:25 So one of the nice things about deploying a lot is when something goes wrong, it's not hard to figure out what caused things to go wrong.

00:18:33 - Right, a bunch of small changes and each one gets deployed.

00:18:37 So you're not going back to the last six months or whatever, right?

00:18:39 - Yeah, exactly.

00:18:41 Or I mean, even, I mean, each of those deployments has a good number of changes in it.

00:18:48 And, you know, if even if it was like four hours, there would be a huge number of changes that you'd have to track things down through.

00:18:58 And it's also like, it's really satisfying from a developer standpoint, in that, you know, you land your change and it's rolling out in half an hour.

00:19:09 And so I don't think anyone, again, I don't know all the original reasoning, but I don't think anyone would really want to change it just because it actually has some significant benefits.

00:19:22 It makes things interesting and challenging in some ways too, but otherwise I think it's really nice.

00:19:28 Yeah, it just never ceases to frustrate me or blow my mind how there's these companies just have extended downtime.

00:19:39 Like, I'm not talking, we pushed out a new version and in order to switch things in and out of the load balancer, there's five seconds of downtime or database migration and it creates a new index and that's going to take one or two minutes.

00:19:53 I'm talking, we're going to be down for six hours on Sunday, so please schedule your work around.

00:19:59 I'm just like, what is wrong with these companies?

00:20:01 Like, how is it possible that it takes so long to deploy these things?

00:20:08 And if they had put in some mechanism to ship small amounts of code with automation, then they would just not be in this situation, right?

00:20:20 Like they would get pushed somewhere and then something would happen and then they would have a new version of the site, right?

00:20:28 - It always baffles me when I end up at a website like, and it's like, we're currently down for service.

00:20:36 It's like, what?

00:20:37 Like, this is a website, you're not supposed to do that.

00:20:41 - The most insane thing, I'll cut off this thing, but it drives me crazy.

00:20:46 The most insane thing is I've seen websites that were closed on Sunday.

00:20:50 I'm like, "What do you mean it's closed on Sunday?" Yeah.

00:20:52 Just go and turn it off when you go home?

00:20:56 It's open Monday to Friday sort of thing.

00:20:58 It was like a government website and I don't know why it had to be closed, but apparently it had to be closed.

00:21:05 We have engineers standing by Monday through Friday to process your requests by hand.

00:21:10 Exactly.

00:21:11 We got to push the button.

00:21:12 No one's there to push the button.

00:21:13 - Okay, so I guess one more setting the stage stories here or things to know is that you run these servers quite close to their limits in terms of like CPU usage and stuff like that.

00:21:29 And then also you said one of the areas that you focus on is request per second as your important metric.

00:21:37 Do you wanna talk about those for a moment?

00:21:39 - Sure, so I don't know what the overall numbers under normal load are.

00:21:47 I don't think the CPU load is necessarily super high, but what we wanna know at the end of the day is like how many requests can we serve under peak load?

00:22:01 And so what we can actually do is take traffic and route it to a set of servers and drive that traffic up to where the server is under peak load, and we see how many requests per second a server is able to serve at that point, which gives us a pretty good idea of kind of what the overall level of efficiency is.

00:22:27 So when we make a change, we can basically run an A/B test where we take one set of servers that don't have the change, drive them up to peak load, and compare it against another set of servers that have the change, and drive those set of servers up to peak load, and then compare between the two and see how many requests per second we end up getting and what the change is.

00:22:56 And we can do that to a decent amount of accuracy.

00:23:01 I think kind of like when we kick off a manual test, we try to strive for within 0.25%.

00:23:08 When we're doing releases of Sender, I think we try to push it a little bit further by doing more runs.

00:23:14 So we get down to like 0.1% or something like that.

00:23:18 So we have a pretty good idea of what the performance impact of what those changes are gonna end up looking like.

00:23:24 - I think that makes a ton of sense.

00:23:26 You could do profiling and obviously-

00:23:29 - We do that too.

00:23:30 - Yeah.

00:23:31 But in the end of the day, there's a bunch of different things, right?

00:23:36 If I profile against some process and say, well, this went this much faster in terms of CPU, maybe it took more memory.

00:23:45 And at production scale, it turns into swap, which means it's dramatically--

00:23:51 there's a bunch of pushes and pulls in there.

00:23:54 And this pragmatic, just like, let's just see what it can take now, is interesting.

00:23:59 You all are in this advantaged situation where you have more traffic than any given server can handle, I would imagine.

00:24:08 Yes.

00:24:10 So you can tune...

00:24:12 We actually work on one server.

00:24:14 Exactly.

00:24:16 It hasn't been rebooted in seven years.

00:24:18 Yeah.

00:24:20 you have the ability to say, well, let's just tune some of our traffic over to this one particular server to sort of see this limit.

00:24:29 Whereas a lot of companies and products don't, right?

00:24:34 Like I use this thing called locust.io, which is just a fantastic Python framework for doing load testing, to actually know the upper bound of what my servers can handle, because we get a lot of traffic, but we don't get 30,000 requests a second lots of traffic.

00:24:53 (laughing)

00:24:54 And so I think this is really neat that you can actually test in production sort of beyond integration tests, not test that it works right, but send real traffic and actually see how it responds.

00:25:06 'Cause really that's the most important thing, right?

00:25:08 Does it do more or does it do less than before?

00:25:11 - And you brought up profiling and we still have to use profiling sometimes to like, 0.25%, 0.1%, that's still a lot of noise.

00:25:21 And so if there's some little micro-optimization, we can still be like, okay, well, what's this function using after the change, kind of across some percentage of the entire fleet, which is kind of amazing, 'cause the profiling's just running on production traffic sampled.

00:25:42 So for smaller things, that ends up becoming super important.

00:25:46 - Right, and you're making a ton of changes as we're about to dive into, but there are additive or multiplicative or something like that, right?

00:25:54 So if you make this thing 1% faster, that 5% faster, this 3% faster, all of a sudden you could end up at 20 to 30% faster in production, right?

00:26:02 - Yep.

00:26:03 - All right.

00:26:05 - And how you do that math, we just add up the percents.

00:26:07 (laughing)

00:26:08 - Exactly.

00:26:09 Oh, where's Cinder?

00:26:10 Here we go.

00:26:11 All right, so when I saw this come out, when did you all make this public?

00:26:16 - Shortly before PyCon?

00:26:18 - Yeah, that's right.

00:26:22 - Yeah.

00:26:23 - Put it like February, March, something like that.

00:26:26 - Yeah, something like that.

00:26:27 - That sounds exactly right with this eight months ago.

00:26:29 So this is under the Facebook incubator.

00:26:35 You guys got to rename this.

00:26:36 Should be meta.

00:26:37 - Yeah, I wonder who's job it is.

00:26:39 - I mean, come on, come on.

00:26:41 So it doesn't matter all that much.

00:26:44 It's Instagram, I guess.

00:26:46 but let me just read the first opening bit here.

00:26:50 I think there's a lot to take away just from the first sentence.

00:26:53 Cinder is Instagram's internal performance oriented production version of CPython 3.8.

00:26:59 So performance oriented, we've been talking about performance.

00:27:02 We're gonna get into a lot of the cool things you've done.

00:27:05 Production version.

00:27:07 So you guys are running on this on Cinder?

00:27:09 Fantastic.

00:27:12 and we redeploy like once a week.

00:27:16 - You redeploy the Cinder or CPython runtime, right?

00:27:20 - Yep.

00:27:21 - Yeah, yeah, yeah.

00:27:22 - So like the source that's up here is, yeah, if you go back and look at maybe a week ago is what we're probably running in production at any given time.

00:27:32 - Right, okay, fantastic.

00:27:34 And then CPython 3.8, because you've made a lot of changes to this that can't really move forward.

00:27:42 So you picked the one, I'm guessing that was the most current when you first started, most current and stable, and just started working on that, right?

00:27:50 - So we do upgrade.

00:27:52 It was previously built on CPython 3.7.

00:27:56 - Oh, cool, okay.

00:27:57 - There's hundreds or, I don't know if we're yet up into thousands of changes yet, But there's a lot of diffs that we've applied.

00:28:10 It's been, I mean, we've been working on it for, I've been working on it for three years now and it predates me.

00:28:20 So we've upgraded to 3.7, we're gonna upgrade to 3.10 next, which we're actually planning on starting early next year.

00:28:32 So it's just a big involved process.

00:28:35 - And you've also contributed some stuff from Cinder to 3.10, that'll be interesting as well.

00:28:43 That probably actually makes it harder to merge rather than easier.

00:28:46 - We hope that makes it easier.

00:28:49 That is one of the things that--

00:28:51 - Yeah, I guess you could drop that whole section, right?

00:28:53 You could just say, you know what, we don't even need this whole enhancement because that's part of Python now, right?

00:28:58 Okay.

00:28:58 - Yeah, that is the incentive for us, one of the incentives for us to contribute.

00:29:04 - Yeah, yeah.

00:29:05 Itamar out in the live stream and the audience says-

00:29:09 - 2,000.

00:29:10 - I did commit.

00:29:11 Oh my gosh.

00:29:12 Yeah, that's awesome.

00:29:13 So-

00:29:13 - Itamar is going to be help.

00:29:17 He is now our kind of full-time dedicated resource to help us upstream things.

00:29:23 - Oh, that's spoke to upstream.

00:29:26 Itamar's job is to take the work you're doing here and then work on getting that into C5 Lump properly.

00:29:31 - Yeah.

00:29:32 Yeah.

00:29:33 We could have been doing such a better job, I think.

00:29:38 We've upstreamed some little things, some slightly more significant things, but it's something that we really need to be working on more.

00:29:46 And so now we've got someone who's dedicated to it and obviously he's not just doing it in a vacuum, we're gonna help him.

00:29:56 But having someone drive that and make sure it actually happens is super important.

00:30:01 - Yeah, that's really cool.

00:30:02 I suspect that he and Lucas Lenga will become friends.

00:30:05 (all laughing)

00:30:08 Lucas will be on the receiving end of that a lot.

00:30:10 I bet.

00:30:11 The developer in residence over at CPython.

00:30:15 Cool.

00:30:15 All right, so I guess let's talk about this.

00:30:20 Is it supported?

00:30:22 So right now the story is you guys have put this out here as sort of a proof of concept and by the way, we're using it, but not we expect other teams and companies to take this and then like just run on it as well, right?

00:30:36 This is probably more to like work on the upstreaming side.

00:30:40 Is that the story?

00:30:41 - Yeah, and like let people know what we're doing.

00:30:44 If someone wants to pick it up and try it, that's great.

00:30:49 It's just mainly, you know, we're focused on our workload and making it faster and can't commit to helping people people out and making it work for them.

00:31:02 - Right, but as you just said, you are working on bringing these changes up to CPython and you already have to some degree.

00:31:08 So that's pretty good.

00:31:12 I guess it also lets you all take a more specialized focused view and say, you know what?

00:31:18 We wanna make micro-WSGI when it's forks off child processes we wanna make it that happen better and use less memory.

00:31:26 And we're gonna focus on that.

00:31:28 And if it makes sense to move that to main Python, good.

00:31:31 If not, then we're just gonna keep those changes there.

00:31:34 - Yeah, and that's happened, I think, with like we've done some work around immortalization of the GCE, which is kind of a big improvement over not collecting that we were talking about earlier.

00:31:49 And that didn't make sense for upstream CPython.

00:31:54 And so it was just, okay, that's something that we just have to maintain.

00:31:57 I was so excited when I saw this come out.

00:31:59 I'm like, wow, this is the biggest performance story I've seen around CPython for quite a while.

00:32:04 And now there have been some other things as well.

00:32:06 We'll touch on at the end on how they come together.

00:32:09 But maybe walk us through what is Sender?

00:32:13 What is this work?

00:32:15 And we can dive into some of the areas maybe.

00:32:18 - Sure.

00:32:19 You have immortal instances highlighted.

00:32:20 So we could start talking about that.

00:32:22 - Let's start with JIT first.

00:32:23 I think, yeah, yeah.

00:32:24 - Okay, sure.

00:32:25 - JIT. - Yeah, that's not fair.

00:32:28 - So the JIT isn't what I work on day to day.

00:32:31 We have several other team members who are working on that full time, but it's obviously a huge part of the performance story.

00:32:40 So the JIT right now is, it's a method at a time JIT, so it compiles each individual method.

00:32:48 It's, again, very tuned for our workload.

00:32:52 kind of you can see here, like some of the descriptions of how to use this thing, and it's mentioning this JIT list file.

00:33:01 So when we're using this in production, what happens is we compile all of the functions ahead of time inside of the master process before we fork off all those worker processes, because we want all that memory to be served, shared between the different processes.

00:33:20 So that's kind of a unusual mode for a chip to work in.

00:33:25 - Right, you don't normally think about filtering processes and forking, they just do their own thing, right?

00:33:32 - Yeah, it's just like, okay, I have this method, it's gone hot, it's time to chip it.

00:33:36 So, yeah, so it's used in this weird way.

00:33:43 At some point we need to, I think, add support for kind of normal, normal JITting methods when they get hot.

00:33:53 Like we were at the point where we're talking about using Cinder a little bit beyond Instagram within meta.

00:34:02 And so at that point, people are gonna need something that isn't so heavily tuned to uWSGI.

00:34:12 The JIT does, it's entirely, we kind of own the full stack.

00:34:21 So it uses, I think, is it AsmJIT?

00:34:26 It uses a library to do the x64 code generation.

00:34:32 But other than that, we go from a high level representation where--

00:34:40 - How close is the high level representation to just Python's bytecode, the PYC's bytecode?

00:34:46 - A pretty good set of overlap.

00:34:51 There are also a lot of opcodes which kind of turn into multiple smaller things.

00:34:58 So like off the top of my head, I think like making a function involves setting several different attributes on it at the end.

00:35:08 So there's something that says, make me this function, which is just a single op code in CPython.

00:35:15 And there's several different op codes which are setting those fields on it.

00:35:19 So it's pretty close, but maybe slightly lower level.

00:35:24 There's also a lot of op codes in there for just kind of super low level operations.

00:35:31 So one of the things, the thing that I spend most of my time working on is static Python.

00:35:37 And so we've added a bunch of things that support primitive math and simple loads and stores of fields and lower level things like that.

00:35:46 So it's a mix.

00:35:48 - Yeah, the static Python that we're gonna talk about is super cool.

00:35:51 And is that possible because the JIT, like you can do whatever you want and then the JIT will see that and then adapt.

00:35:58 - The JIT's really important to it because like it takes things that are usually tons of instructions and turns them into a single instruction or a couple of instructions.

00:36:11 It's not 100% required, like we support it in the interpreter loop and kind of our goal is to do no harm and generally like at least get the normal performance, but legit being able to resolve things statically and turn them into simple loads is super important.

00:36:31 So from HIR, we turn that into an SSA form and run a bunch of optimizations over it.

00:36:40 I think one really interesting optimization is ref count removal.

00:36:46 So we can see that these objects are either borrowed or just that we'd have extra ref counts happening on them that we don't need to actually insert and we can just collide all of those, which is--

00:37:02 - There's a lot of interesting stuff happening around memory that you all are doing.

00:37:05 - Yes.

00:37:07 - One of them is this ref count, and you make assumptions that are reasonable.

00:37:13 Like when I'm in a method call of a class, I don't need to increment and then decrement the self object because guess what?

00:37:22 The thing must be alive because it's doing stuff, right?

00:37:24 And then it sounds like also maybe with constants, like the number one doesn't need its ref count to change and stuff like that, you notice that and go, you know what, we're just gonna skip that.

00:37:33 - Yeah, yeah, and one of the things we've done is the immortalization of objects.

00:37:40 And so we can also, like the number one is gonna be an immortal instance.

00:37:47 And so in that case, we can be like, okay, yeah, we don't need to deal with ref counts on this.

00:37:53 Unless of course, like that number one ends up going off to somewhere that maybe doesn't understand the ref counting semantics of the JIT, in which case maybe we do have to end up inserting them.

00:38:06 Or like it's going through like an if else or something where one of the branches we have to end up ref counting.

00:38:13 So it's smart.

00:38:16 And it's important because within mortal instances, our ref counts are a little bit more expensive than normal ref counts, 'cause we have to check to see if the object is immortal too.

00:38:30 - Right, instead of just doing an increment on a number.

00:38:33 - Yeah. (laughs)

00:38:34 - Okay, so this immortal instances, this comes back to that memory thing that comes back to the turning off the GC, which you stopped turning it off.

00:38:42 It sounds like immortal instances are a more nuanced way to solve that same problem.

00:38:47 - So this is really about that fork and exec model.

00:38:51 So when we fork off those worker processes, they're initially sharing all of the memory with the master process, unless they happen to go off and write to it.

00:39:03 And ref counts are a really big source of writing to that shared memory.

00:39:11 And so what this does is it takes all of the objects that are present inside of the master process and runs through, marks them all as immortal.

00:39:21 And then from then on out, the trial process will be like, oh, this thing's immortal.

00:39:27 I'm not gonna change the ref count.

00:39:29 And so that makes it--

00:39:30 - Okay, so this happens, you basically just scan the whole heap right before you do the fork, and you're like, everything, we're just gonna clone this, and it becomes unchangeable.

00:39:39 And then we'll just, at least with regard to its ref count, and we'll go from there.

00:39:44 - Yep, yeah.

00:39:45 And then as long as, ideally, we also don't, we shouldn't have a lot of global mutable state.

00:39:52 People shouldn't be, like, you know, if you think about what's in the master process, it's like classes and functions, and people shouldn't be really going off and mutating those things inside of the worker processes.

00:40:05 It seems like something strange is happening if that's going on.

00:40:09 - I guess maybe let me ask you really quick or let you talk about really quickly this.

00:40:16 The real benefit here is if on Linux, when you fork off these processes, if the memory itself hasn't been changed, that can be shared across the 40 or 60 processes.

00:40:30 But as soon as that memory change, it has like a local copy has to be dedicated to that one worker process.

00:40:36 So if it's silly stuff, simple stuff, like I want to pass this string around that happens to be global, And then it says, well, it's passed.

00:40:47 Excuse me.

00:40:47 So you've got to add ref to it, which means you get 60 copies of it all of a sudden, right?

00:40:56 Those really simple things, you were able to get lots better memory sharing, which then leads to cache hits versus cache mists and misses.

00:41:03 And there's like all these knock-on effects, right?

00:41:06 - Yeah.

00:41:06 And it's not just the string itself, right?

00:41:09 It's the entire page that the string lives on.

00:41:11 So, you might have a 15 byte string with a 16 byte object header and you end up copying 4K of memory.

00:41:25 - Because you changed a six reference number to a seven or to a five.

00:41:31 - Yeah.

00:41:32 - Fascinating.

00:41:34 Okay.

00:41:35 Do you think that Python, CPython itself could adopt this?

00:41:38 - So we tried.

00:41:41 We tried to upstream it and there is resistance to it.

00:41:44 I mean, it is touching something that's very core.

00:41:48 It's gonna be a bit of a maintenance burden.

00:41:50 There are other reasons, I think, that people are now talking about wanting to have immortal instances.

00:41:58 So Eric Snow has been working on subinterpreters for a long time and I think he has been interested in them recently for sharing objects between interpreters.

00:42:11 And I think Sam Gross's work on NoGil might have some form of immortal instances as well.

00:42:19 So maybe the core immortal instances support could land upstream at some point, but maybe the code that actually is walking the heap and is freezing everything, maybe that's very Instagram specific and doesn't have much value in upstreaming.

00:42:41 It seems to me that there's probably a set of things that would be good immortal instances for almost any Python process that starts up, right?

00:42:53 Like before your code runs everything there probably would be a good candidate for that.

00:42:59 And you know, there's potential, Like it's kind of scary because ref counts are so frequent and so adding extra code in the ref count process seems risky.

00:43:14 But if you can freeze enough stuff that was kind of there before the program started up, that's super core and happening a lot, then maybe it does actually end up making sense for other workloads too.

00:43:33 Yeah, perhaps. Okay, so these immortal instances are one of the things you all have done.

00:43:37 That's pretty fascinating.

00:43:39 And also a huge win, something like 5%.

00:43:44 Yeah, yeah, that's right. It says right here.

00:43:46 Big win in production, 5%.

00:43:48 Does that mean 5% request per second?

00:43:52 Is that when you say 5%? Is that the metric you're talking about here?

00:43:55 Yep.

00:43:56 Yeah.

00:43:58 Have you thought about or tested, I'm sure you've thought about, like if this lets you run more worker processes off of, you know, increment that, that spawn worker process number.

00:44:10 - I think the developer who worked on this was doing, did look at that number and was looking at tweaking the number of worker processes.

00:44:22 If I recall, he got a little bit of pushback from people who were nervous about increasing it.

00:44:29 - Don't mess with this number.

00:44:30 We never mess with this number.

00:44:31 What are you doing?

00:44:32 (laughing)

00:44:33 - Yeah.

00:44:34 Yeah.

00:44:37 (laughing)

00:44:38 So I don't--

00:44:39 - I hear you.

00:44:40 I'm just thinking, if it really does create more shared memory, maybe it creates more space on the same hardware for you to actually create more.

00:44:48 And then that would just possibly allow even a bigger gain in requests per second because there's more parallelism.

00:44:55 - And given that it was such a big win, it could have just been that we were already under significant memory pressure and it got us out of significant memory pressure.

00:45:05 Maybe we had the right number.

00:45:06 Maybe we had too many hosts, I don't know.

00:45:08 - Yeah, yeah, perhaps, perhaps.

00:45:10 But still 5% is, as one of the changes, is still a pretty big deal.

00:45:16 All right, the next one on deck is strict modules.

00:45:20 Let's talk about strict modules.

00:45:22 - So, I mean, we've talked about a little bit of things that are kind of related to this.

00:45:28 You know, I was saying, like, if you have things that are going off and mutating your things in the master process, it's like, what?

00:45:34 That's kind of crazy.

00:45:35 So strict modules weren't about performance, actually.

00:45:42 There's a little bit of performance thought behind it, but now they're really, we're not considering them as a performance feature at all.

00:45:50 They're more about a reliability feature.

00:45:53 And so you brought up early on how, like, Python modules, just going off, executing some code, who knows what that code's gonna do.

00:46:02 So strict modules is a attempt to tame that process.

00:46:08 And what we do is we run static analysis over the code.

00:46:15 I mean, we are basically interpreting the code in a safe interpreter.

00:46:19 And if the module has any external side effects or depends upon any external side effects, we don't allow it to be imported.

00:46:29 And so we know that all the modules are side effect free that are strict.

00:46:35 And we've-

00:46:40 - When you say they're side effect free, does that mean that the importing of them is side effect free or all of their functions are also side effect free?

00:46:48 - The importing of them.

00:46:49 Their functions can do whatever they want.

00:46:52 they can call functions from other modules, they can call functions from themselves.

00:46:58 If they call those modules at the top level while doing the import, then those functions need to be side effect free.

00:47:04 - Where does this lead you?

00:47:10 What do you get out of this?

00:47:12 - We get additional reliability.

00:47:14 So like Instagram, as I think maybe we mentioned this, being a big monolithic application.

00:47:25 Maybe we didn't get to that.

00:47:26 - Yeah, I don't think we talked about that, but this is not a 100 microservices type of thing, is it?

00:47:31 - No, it's one giant application.

00:47:34 The thing that gets redeployed every 10 minutes is that giant application.

00:47:38 - That makes the redeployment even more impressive, by the way, right?

00:47:43 - Yeah, I mean, maybe.

00:47:49 - It's nice in that it's one giant application 'cause you just have to redeploy one thing.

00:47:53 (laughs)

00:47:55 - Things you gotta keep in sync all at the same time, right?

00:48:00 - Yeah, our PEs make that happen and it just happens behind the scenes as far as I'm concerned.

00:48:05 (laughs)

00:48:07 So, if you have, like, if you import one module and it depends on side effects from another module and then something changes the import order, whether that's like state that things are depending upon, suddenly things blow up in production and your site doesn't work and everyone's really sad.

00:48:29 So this is like, we wanna get to a world where our modules are completely safe.

00:48:36 We've used this, we've experimented doing other things with this.

00:48:40 So like adding a hop reload capability, We know the modules are completely side effect free.

00:48:47 Why not just patch the module in place and like let developers move on without restarting the website?

00:48:54 It has the potential to kind of really change the way we store modules, although we haven't gone down this route yet, where instead of storing modules is a bunch of Python code that needs to run off and execute.

00:49:09 Could we store modules as like, here's a class definition, here's a function and can we lazily load portions of the modules out of there?

00:49:20 But we also have a really other different take on lazy loading that's in Cinder now too.

00:49:26 - Yeah, that's interesting.

00:49:30 'Cause normally you can't re-import something because maybe you've set up some kind of static value on a class, you've set some module level variable, and that'll get wiped away, right?

00:49:47 - I mean, you can call reload on a module, but whether or not that's a safe thing to do, who knows?

00:49:56 - Exactly, exactly.

00:49:58 All right, cool.

00:49:59 So I think one of the more interesting areas, probably the two that really stood out to me are the JIT and StaticPython, with the mortal objects being right behind it.

00:50:09 But StaticPython, this is your area, right?

00:50:11 What is this?

00:50:12 - Yep.

00:50:13 So this is a attempt to leverage the types that we already have throughout our entire code base.

00:50:22 So Instagram is a hundred percent typed, although there are still some any types flowing around, but you can't add code that isn't typed.

00:50:34 So we know the types of things.

00:50:39 - Right, you're talking traditional, just colon int, colon str, optional str, that type of typing, yeah?

00:50:47 - Yeah, so why not add a compilation step when we're compiling things to PYCs, instead of just ignoring the types?

00:51:00 Why don't we pay attention to the types?

00:51:02 So we have a compiler that's written in Python.

00:51:08 There's actually this old compiler package that started in Python 2.

00:51:14 There's this external, there's this developer on GitHub, PF Falcon, who upgraded it to Python 3 at some point, and we upgraded it to Python 3.8, and made it match CPython identical for bytecode generation.

00:51:36 So we have this great Python code base to work in, to write a compiler in, and we analyze the type annotations, and then we have runtime support and a set of new opcodes that can much more efficiently dispatch the things.

00:51:56 There's a great, my coworker, Karl Meyer, had this awesome slide of calling a function during a PyCon talk and it was just like pages, well, it was one page and a very, very tiny font of the assembly of what it takes for CPython to invoke a function.

00:52:17 And then we're able to just directly call the function using the x64 calling convention.

00:52:24 So shuffle a few registers around and emit a call instruction.

00:52:28 - That's awesome.

00:52:29 It surprised me when I first got into Python, how expensive calling a function was, not regardless of what it does, just the act of calling it.

00:52:39 You know, coming from C# and C++ where you think you'll get inline by either the compiler or the JIT compiler and all sorts of interesting things.

00:52:46 You're like, wait, this is expensive.

00:52:48 I should consider whether or not I'm calling a function in a tight loop.

00:52:52 - There's so many things it has to deal with.

00:52:55 Like it has to deal with adding the default values in and you don't know whether you're gonna have to do that until you get to the function.

00:53:03 It's gotta deal with taking keyword arguments and mapping those onto the correct keywords.

00:53:09 And like, that's one thing in static Python, we do that at compile time.

00:53:14 Like we, if you're calling with keyword arguments, they turn into positional arguments because we know what we're going to, and we can just shuffle those around at compile time and just save a whole bunch of overhead.

00:53:26 - Yeah, that's fantastic.

00:53:27 And the way people should think of this is maybe like mypyc or Cython where it looks like regular Python, but then out the other side comes better stuff, except for the difference here is you guys do it at JIT, not as some sort of ahead of time pre-deployment type of thing.

00:53:45 - Yeah, and so the first thing we did with it was actually we had, I don't know, 40 Cython modules that were inside of the Instagram code base.

00:53:55 And that was a big developer pain point and that those things had to be rebuilt.

00:54:00 The tooling for editing them wasn't as good because you don't get syntax color highlighting.

00:54:06 And so we were able to just get rid of all of those.

00:54:09 And those were heavily tuned, like using a bunch of Cython features.

00:54:15 And so that really kind of proved things out that like, okay, if we need to use low-level features, 'cause we support things like permanent events if you wanna use them, instead of like having boxed variable size.

00:54:28 And so that was a good proving that it worked.

00:54:33 And now I think it's more close to my PySE at runtime as we've been going through and converting other modules to a static Python within the Instagram code base.

00:54:47 - Yeah, fantastic.

00:54:48 You guys say that static Python plus sender JIT achieve seven times performance improvements over CPython on the type version of Richard's benchmark.

00:54:58 I mean, obviously you gotta be specific, right?

00:55:00 But still that's a huge--

00:55:00 - Yeah, yep.

00:55:02 And some of that's like the ability to use primitive integers.

00:55:08 Some of that's the ability to use V tables for invoking functions instead of having to do the dynamic lookup, which is something that both mypyc and Cython support.

00:55:21 So lots of little things end up adding up a lot and some of that's just legit.

00:55:26 - Yeah, it's fantastic.

00:55:27 You've talked about using primitive integers and I've always thought that Python should support this idea somehow.

00:55:37 Like if you're doing some operation like computing the square root or something, you take two numbers, two integers and do some math, maybe multiply, square them and then subtract them or something like that.

00:55:51 And all of that stuff goes through a really high overhead version of what a number is, right?

00:55:58 Like instead of being a four or eight byte thing on a register, it's 50 bytes or something like that as a high object long thing that gets ref counted.

00:56:13 And then like somewhere in there is the number bit.

00:56:16 And that's awesome because it supports having huge numbers.

00:56:19 Like you don't ever see negative 2.1 billion when you add an increment of a number by one in Python, which is great.

00:56:25 But it also means that at certain times you're doing math is just so much slower 'cause you can't use registers.

00:56:33 You've gotta use like complex math, right?

00:56:36 It sounds like you're doing this, this like let's treat this number as a small number rather than a high object pointer drive thing.

00:56:46 And JITs can handle this to some degree, right?

00:56:50 And they can recognize that things are small numbers and generate more efficient code.

00:56:58 I think when you had Anthony on, he was talking about Pigeon doing this.

00:57:01 There's still some overhead there for dealing with the cases where you have to bail out and it's not that case.

00:57:12 It's nice just having the straight line code that's there.

00:57:16 You can also do tag pointers, which again, kind of handle that.

00:57:20 Tag pointers are kind of difficult on CPython because things expect PyObject stars, and if that PyObject star ever escapes to something that's not your CPython code, it's going to be very unhappy.

00:57:33 - Yeah.

00:57:34 - So this is, I mean, the nice thing is it's a relatively straightforward way to allow it.

00:57:39 It was actually a little bit controversial in that like, is this really what Python developers are going to expect and are we going to have the right semantics there?

00:57:51 And I think we have a to-do item to actually make things raise overflow errors if they do overflow instead of flowing over to negative 2 billion.

00:58:01 - That would be fantastic.

00:58:03 (laughing)

00:58:05 I would personally rather see an overflow error but then have it wrap around to the negative side or go back to zero if it's unsigned or whatever terrible outcome you're gonna get.

00:58:14 - Yeah, yeah, it's a much more reasonable behavior.

00:58:17 We've just, I guess we haven't been very motivated to actually go and fix that.

00:58:21 - Well, you're probably not doing the type of processing that would lead to that, right?

00:58:26 You're probably not doing like scientific stuff where all of a sudden, you took a factorial too big or you did some insane thing like that.

00:58:34 There's probably not a single factorial in the entire code base, I would guess.

00:58:37 - Yeah, there's not a lot of math.

00:58:39 There was like some, like the only place where you've used primitive integers really was in the existing conversion, in the conversion of the existing Cython code.

00:58:49 Where people had--

00:58:49 - Because it probably started as an int32 or an int64, right?

00:58:53 - Yeah, yeah.

00:58:54 Like they had that option available to them.

00:58:56 They used it.

00:58:58 It's not like something that we're going through and sprinkling in a random Python code.

00:59:02 'Cause like, yeah, we don't do much math.

00:59:04 It's very object oriented, lots of function calls, lots of classes.

00:59:10 - Yeah, absolutely.

00:59:11 All right, there's a lot of other good things that you talked about, but they're not necessarily listed right here.

00:59:18 Like, it's sort of kind of stuff with async and await.

00:59:23 It sounds like you guys use async and await a lot.

00:59:25 Is that right?

00:59:26 - Yes.

00:59:27 So the, yeah, the entire code base is basically async.

00:59:32 there was a big conversion, a big push to convert it right as I was starting.

00:59:38 And now everything basically is async, unless obviously it's not--

00:59:43 - Yeah, I heard that async await is slow, why would you ever use that?

00:59:46 - Because it allows additional parallelization, because multiple requests can be served by the same worker.

00:59:54 - Sure, well, you know, whenever I hear those, I see examples of like, we're just calling something as fast as you can.

01:00:02 And it doesn't really provide, there's not an actual waiting, right?

01:00:07 Like the async and await is really good to scale the time when you're waiting, do something else.

01:00:12 And a lot of the examples, this is slower, there's like no waiting period, but you know what is a really good slow thing?

01:00:17 An external API and a database.

01:00:19 And it sounds like you guys probably talk to those things.

01:00:22 - And yes, and I mean, the no waiting case is actually what this eager coroutine evaluation is all about.

01:00:30 Like, yeah, sometimes we're talking to a database, but sometimes you have a function that's like, have I fetched this from the database?

01:00:38 Okay, here it is.

01:00:40 I don't have to wait for it.

01:00:41 Otherwise I'll go off and fetch it from the database.

01:00:44 - Right, if there's an early return before the first await.

01:00:47 - Exactly.

01:00:48 - There's not a huge value to calling this, right?

01:00:51 - Yeah.

01:00:51 - So tell us about this eager coroutine evaluation, which deals with that, right?

01:00:55 - Yeah, so this lets us run the function up to the first await and only go off and kind of, so A, like normally what happens is you produce your coroutine object, schedule that on your event loop, and then eventually it'll get called.

01:01:15 And now when you call the function, it's gonna run, it's gonna immediately run up to the first await.

01:01:21 And if it doesn't hit that first await, it's just gonna have the value that's produced and you're not gonna have to go through this big churn of going through the event loop with this whole coroutine object.

01:01:33 So, yeah. - That's fantastic.

01:01:36 - It is slightly different semantics because now you could have some CPU heavy thing, which is just like not sharing the CPU with other workers, which isn't great.

01:01:54 And I think it can end up kind of, I think there can be some slight differences on what the scheduling happens, like where you could have observable differences, but we haven't had any issues with that.

01:02:07 So I think it's might be a little bit controversial, but it is such a big win that it makes a lot of sense for us.

01:02:14 - It certainly could change the order.

01:02:18 If you were doing, here's a whole bunch of coroutines and a bunch of awaits and stuff, and then you ran them in one mode, the sort of standard mode versus this, you would get a different order.

01:02:29 But, you know, I mean, it sounds like you're gonna ultimately put the same amount of CPU load on, I mean, async and await runs on one thread anyway, generally.

01:02:40 - Yeah.

01:02:41 - Unless you do something funky to like wrap some kind of thread or something, but in general, it still runs there.

01:02:47 - I would hope that most people aren't super dependent upon the order.

01:02:52 (laughs)

01:02:53 - If you're dependent upon the order and you're doing threading or something like that, you're doing it wrong.

01:02:59 - Yeah.

01:03:00 The fairness issue might be a bigger issue.

01:03:03 But yeah, for us it makes a lot of sense.

01:03:10 - Yeah, that's really cool.

01:03:11 All right, another one was shadow code or shadow byte code.

01:03:17 - Yeah, so this is our inline caching implementation.

01:03:21 We've had this for a few years.

01:03:26 Python 3.11 is getting something very similar.

01:03:31 So we kind of expect that our version will be going away.

01:03:36 We'll have to see if there's any cases that aren't covered or if there's any performance differences.

01:03:43 But basically it's nearly identical.

01:03:47 We have a extra copy of the byte code, which is why it's called shadow byte code, which we can mutate in the background and replace the normal opcodes with specialized ones.

01:04:00 So if we're doing a load adder, and that load adder is against an instance of a specific type, we can just say, okay, well, we know that this load adder doesn't have a type descriptor associated with it, a descriptor associated with it, like a get set data descriptor.

01:04:25 We know that the instance has a split dictionary, which is the way CPython shares dictionaries, dictionary layout between instances of classes.

01:04:39 We know this attribute is at offset two within the split dictionary.

01:04:44 So we just do a simple type check and make sure that the type's still compatible and go off and look in the instance dictionary and pull the value out.

01:04:53 Instead of going through and looking up all of those other things that I've just described, which is kind of what you have to do every single time on a normal load adder.

01:05:03 - Yeah, that's really cool.

01:05:06 Is this something that could come back to CPython?

01:05:10 - Yeah, I think the fact that they've gone off and built their own version in 3.11 means that's not gonna happen. (laughs)

01:05:16 - The idea lives there.

01:05:18 - Yes.

01:05:20 - Awesome.

01:05:22 What else?

01:05:24 We're getting short on time here.

01:05:28 I think maybe you could just highlight really quickly, stepping back one feature point on the async I/O stuff.

01:05:37 Is the send receive without stop iteration stuff that you all did, and that getting upstreamed as well already?

01:05:45 >> Yeah. So that was adding -- so I didn't work on this. Developer Vladimir Mative worked on this. And that was adding in a -- I think he added in a new set of slots for actually achieving this at the end of the day. And in Sender, we have a type flag that says this type has these additional slots.

01:06:16 And so we can call the send function and the receive function and get back an enum that's kind of, did this thing return a result?

01:06:25 Did this thing throw an exception?

01:06:28 And here's the result.

01:06:31 So that instead of producing the stop iteration on every single result, we just return the result.

01:06:41 And that is obviously big with coroutines 'cause coroutines are generators at the end of the day.

01:06:47 - Yeah, that's fantastic.

01:06:48 Yeah, everything can get more efficient by not allocating on sort of hidden behind the scene exceptions, right?

01:06:58 - Yeah.

01:06:59 - All right, well, there's a bunch of cool stuff here and I'm really happy to hear that you and your team and Hidem are out there are working on bringing this stuff over because I was so excited when I saw it and then I saw, is it supported?

01:07:10 "Ah, not really, you really shouldn't use this." I'm like, "Oh, but it's so good, "like I want so much of this stuff to be moved over." So that's cool.

01:07:18 - And I think some of it will be difficult to move over.

01:07:21 Like moving the entire JIT over, the JIT's written in C++.

01:07:25 Obviously the CPython core developers were open to C++ for a JIT at one point in time with unladen swallow.

01:07:34 Whether or not that feeling has changed, who knows?

01:07:38 but it's a big piece of code to drop in.

01:07:40 So one thing that we really want to do going forward is actually get to the point where the big pieces of Sender are actually just pip installable.

01:07:49 So we'll work on getting the hooks that we need upstreamed.

01:07:54 One thing that the JIT relies on a lot is dictionary watchers so that we can do really super fast global loads.

01:08:01 And we have a bunch of hooks into like type modification and function modification that aren't super onerous by any means.

01:08:11 So if we can get those upstream, then we can make the JIT just be here, pip install this.

01:08:17 And so hopefully we can get those upstream in 3.11 and have pip install sender start working.

01:08:29 - Yeah, that'd be awesome.

01:08:30 So yeah, really good work on these.

01:08:33 I guess let's wrap up our conversation here 'cause we're definitely short on time.

01:08:37 But you know, there's the other projects, which I'm gonna start calling the Shannon plan that Mark and Guido are working on.

01:08:45 They've been working on for a year.

01:08:47 And then there's Pigeon, which by the way, Anthony Shaw's taken over, but you created Pigeon, right?

01:08:55 - Yep.

01:08:56 (laughing)

01:08:58 - Well done on that.

01:08:59 - On a week at a PyCon.

01:09:00 - Exactly.

01:09:03 And Sam Gross's work on the Nogel stuff, all of this seems to be independent, but in the same area as those things.

01:09:12 Where do you see the synergies?

01:09:13 Do you see any chance for those to come together?

01:09:17 Is that through some kind of pip, putting the right hooks in there and other people plugging in what they want?

01:09:21 Or what do you see there?

01:09:23 Be great if this can come together a little bit.

01:09:26 - Yeah, in a lot of places, we're working on independent things.

01:09:32 Obviously, Pidgin is a JIT and we're a JIT.

01:09:35 So what the future of JITs--

01:09:37 - Different goals to some degree, right?

01:09:39 - Yeah, but I mean, also very similar and overlapping goals.

01:09:44 I think there'll probably have to be discussion of what the future of JITs look like in CPython.

01:09:52 Like, is that something that's part of the core?

01:09:54 Or is that something that should live on as being external?

01:09:58 Or is there gonna be a grand competition and at one point one of the JITs will win.

01:10:03 Who knows?

01:10:06 It's a good discussion that should probably take place.

01:10:10 The hooks for JITs are there.

01:10:12 And between what Brett and I added for Pigeon and Mark Shannon's vector call work that happened several releases ago, I think JITs have a pretty good foundation for hooking and replacing code execution.

01:10:30 They probably need other hooks to get into other things like the dictionary watchers that I mentioned.

01:10:39 But we can keep working on hooks.

01:10:43 Other things have less overlap, so hopefully we can all kind of work in our own streams and work to improve things and make those available to Python developers in the best way that's available and not be stomping on each other's shoes or duplicating work too much.

01:11:04 - Yeah, absolutely.

01:11:05 Well, it's an exciting time.

01:11:07 I feel like a lot of stuff is sort of coming back to the forefront and feels like--

01:11:12 - So much performance work.

01:11:13 - Yeah, for sure.

01:11:14 Feels like the core developers are open to hearing about it and taking on some of the disruption and complexity that might come from it, but still could be valuable, right?

01:11:25 I guess there's probably enough--

01:11:28 - It's absolutely gonna be valuable.

01:11:29 - Yeah, I feel like there's enough pressure from other languages like Go and Rust and stuff.

01:11:34 Oh, you should come over to our world and forget that Python stuff.

01:11:37 And like, hold on, hold on, hold on.

01:11:38 We can do that too, but we've got to do it.

01:11:41 - We can get faster.

01:11:42 - Yeah.

01:11:44 Well, this is awesome work.

01:11:45 Thanks for coming on and sharing.

01:11:46 - Thank you for having me.

01:11:48 - Yeah, you and your team are doing.

01:11:49 Now, before you get out of here, Got the final two questions.

01:11:51 Let's do notable PyPI package first.

01:11:55 So is there some library or notable package out there that you come across like, oh, this thing's awesome.

01:12:00 People should know about whatever.

01:12:03 - So does it have to be PyPI?

01:12:05 - No, any project.

01:12:08 - Okay, so as I said, I have a very weird relationship with Python, right, as using mainly from the implementation side.

01:12:19 So I think my favorite package is the standard library.

01:12:23 And if I had to pick something out of the standard library, I think one of the coolest parts is mock.

01:12:30 It's been an interesting integration with static Python, but like seeing the way people use it and drive their tests, it's kind of really kind of amazing.

01:12:45 - Yeah, I agree.

01:12:46 It's definitely a very cool that people should certainly be using.

01:12:50 And now if you're gonna write some Python code, you might also have special requirements that shift you in one way or the other, but what editor are you using?

01:12:56 - Oh, I use VS Code pretty much.

01:12:59 Well, I use VS Code and I use Nano when I need to make a quick edit from the command prompt.

01:13:04 - Yeah, I'm a fan of Nano as well.

01:13:06 Like, let's just keep it simple.

01:13:07 It's just give me a Nano and I'll edit this thing over the shell.

01:13:10 - Yeah.

01:13:11 It has syntax color highlighting now.

01:13:13 (laughing)

01:13:14 - Oh, advanced, that's awesome.

01:13:17 - Cool, no, I use it as well.

01:13:18 All right, well, Dino, thank you so much for being here.

01:13:21 Final call to action, people are excited about these ideas, maybe they wanna contribute back or try them out.

01:13:25 What do you say?

01:13:26 - I mean, try out Sender, yeah, it's unsupported, but if you have thoughts on it, that's cool.

01:13:34 I think we'll be in a better place.

01:13:37 - You have instructions on how to build it right here, so you could check it out.

01:13:40 - There's a Docker container.

01:13:41 - Yeah, okay.

01:13:42 - Yeah, so it's pretty easy to give it a shot.

01:13:46 You know, like, it might be harder to get it up and running in a perf sensitive environment.

01:13:55 If you want to try out static Python, that'd be cool or strict modules and give us any feedback you have on those.

01:14:03 Fantastic.

01:14:04 All right.

01:14:05 Well, thanks for being on the show.

01:14:07 Great to chat with you.

01:14:08 Thank you, Michael.

01:14:09 Yeah, you bet.

01:14:10 Bye.

01:14:11 See you.

01:14:12 Yeah.

01:14:13 Bye.

01:14:13 [BLANK_AUDIO]

