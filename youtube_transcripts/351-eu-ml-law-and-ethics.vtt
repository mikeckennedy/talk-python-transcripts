WEBVTT

00:00:00.000 --> 00:00:05.000
- Catherine Ines, welcome to Talk Python to Me.


00:00:05.000 --> 00:00:08.480
- Hey, hi Michael.


00:00:08.480 --> 00:00:09.480
- It's great to be back.


00:00:09.480 --> 00:00:11.400
- Yeah, it's great to have you here.


00:00:11.400 --> 00:00:12.800
Ines, you've been here a bunch of times.


00:00:12.800 --> 00:00:14.840
Catherine, have I had you on before?


00:00:14.840 --> 00:00:15.680
- Yeah, I think so.


00:00:15.680 --> 00:00:17.280
A while ago now. - I think so as well,


00:00:17.280 --> 00:00:18.920
but it's been a really long time, hasn't it?


00:00:18.920 --> 00:00:20.200
- Yeah, yeah.


00:00:20.200 --> 00:00:22.560
- Yeah, well, it's great to have you back.


00:00:22.560 --> 00:00:27.560
I feel like this is a very Berlin-focused podcast today.


00:00:27.560 --> 00:00:29.980
(laughing)


00:00:29.980 --> 00:00:36.420
unrelated, both of you happen to be there. So that's really cool. Thank you for taking time out of your evening to be part of the show.


00:00:36.420 --> 00:00:48.940
Yeah. All right. Well, we're going to talk about machine learning, some of the rules and regulations come coming around there, especially in


00:00:48.940 --> 00:00:58.460
Europe, we're going to talk about fairness, we're going to talk even a little bit about interesting indirect implications like Google, or


00:00:58.460 --> 00:01:02.100
and how to go GitHub Copilot and these types of things.


00:01:02.100 --> 00:01:05.620
We'll sort of go through the whole ML space


00:01:05.620 --> 00:01:06.780
and talk about some of these ideas.


00:01:06.780 --> 00:01:09.940
But you both are doing very interesting things


00:01:09.940 --> 00:01:11.540
in the machine learning space.


00:01:11.540 --> 00:01:13.540
Let's sort of just get a little bit of your background.


00:01:13.540 --> 00:01:14.820
Catherine, tell people a bit


00:01:14.820 --> 00:01:17.180
about what you're doing these days.


00:01:17.180 --> 00:01:21.540
- Yeah, I'm here in Berlin and focused on


00:01:21.540 --> 00:01:23.300
how do we think about data privacy


00:01:23.300 --> 00:01:25.820
and data security concerns in machine learning.


00:01:25.820 --> 00:01:29.260
So for the past five years, I've worked on the space of


00:01:29.260 --> 00:01:31.620
how do we think about problems like anonymization,


00:01:31.620 --> 00:01:35.020
differential privacy, as well as how do we think about


00:01:35.020 --> 00:01:38.060
solutions like encrypted learning and building ways


00:01:38.060 --> 00:01:40.220
that we can learn from encrypted data.


00:01:40.220 --> 00:01:43.240
So it's been really fun and I'm excited,


00:01:43.240 --> 00:01:45.900
but first to publicly announce here


00:01:45.900 --> 00:01:49.900
that I'll be joining ThoughtWorks in January.


00:01:49.900 --> 00:01:50.860
- Yeah, that's awesome.


00:01:50.860 --> 00:01:54.540
- Yeah, as a principal data scientist,


00:01:54.540 --> 00:01:56.980
their focus exactly on this problem,


00:01:56.980 --> 00:01:59.700
which they've been noticing here in Europe is,


00:01:59.700 --> 00:02:01.420
how do we think about data privacy


00:02:01.420 --> 00:02:02.780
and data security problems


00:02:02.780 --> 00:02:05.180
when we think about machine learning?


00:02:05.180 --> 00:02:08.160
It's a growing concern, so it should be pretty exciting.


00:02:08.160 --> 00:02:11.060
- Yeah, a company like ThoughtWorks


00:02:11.060 --> 00:02:13.880
is one of these companies that works


00:02:13.880 --> 00:02:15.940
with other companies a lot,


00:02:15.940 --> 00:02:18.820
and it's sort of this consulting side of things.


00:02:18.820 --> 00:02:22.560
And I feel like you can have a pretty wide-ranging impact


00:02:22.560 --> 00:02:23.700
through those channels.


00:02:23.700 --> 00:02:24.540
- Yeah, yeah.


00:02:24.540 --> 00:02:29.540
- Yeah, do you think that being in Germany,


00:02:29.540 --> 00:02:33.780
there's more focus on these kinds of things in Europe,


00:02:33.780 --> 00:02:35.900
but especially in Germany, it seems like,


00:02:35.900 --> 00:02:39.740
than a pair, like say in the US?


00:02:39.740 --> 00:02:42.820
Does the US have more of a YOLO attitude towards privacy


00:02:42.820 --> 00:02:46.060
and machine learning stuff?


00:02:46.060 --> 00:02:50.280
- Yeah, I mean, I think just from a regulatory aspect,


00:02:50.280 --> 00:02:53.180
since we saw passage of the GDPR,


00:02:53.180 --> 00:02:57.260
which is the big European privacy law in 2018,


00:02:57.260 --> 00:02:58.420
that it went into effect.


00:02:58.420 --> 00:03:02.660
We definitely saw kind of a growing trend here in Europe.


00:03:02.660 --> 00:03:05.340
Overall, I would say like actually France


00:03:05.340 --> 00:03:08.340
and the Netherlands have done quite a lot of good work,


00:03:08.340 --> 00:03:13.340
even Ireland at questioning, let's say larger tech usage.


00:03:13.340 --> 00:03:17.300
But the activism, I would say on the ground activism


00:03:17.300 --> 00:03:20.820
here in Germany from the chaos communication club


00:03:20.820 --> 00:03:23.460
and other types of activists that are here


00:03:23.460 --> 00:03:26.740
has been quite strong, which is exciting to see.


00:03:26.740 --> 00:03:29.660
And therefore, I think Leeds kind of ends up


00:03:29.660 --> 00:03:34.060
being in the headlines maybe a bit more internationally.


00:03:34.060 --> 00:03:35.620
- Also actually a fun fact that the story


00:03:35.620 --> 00:03:37.420
I always like to tell Americans is that like,


00:03:37.420 --> 00:03:39.660
if you go on Google Street View here in Berlin,


00:03:39.660 --> 00:03:41.900
it's an awesome, I don't know,


00:03:41.900 --> 00:03:43.580
kind of, it's the day to day,


00:03:43.580 --> 00:03:45.340
it's like over 10 years old now.


00:03:45.340 --> 00:03:47.340
So you can be there, and Berlin's heavily genderfied now.


00:03:47.340 --> 00:03:49.700
So you can really see, wow, how did my neighborhood look


00:03:49.700 --> 00:03:52.280
10, 12 years ago, because Google did it once,


00:03:52.280 --> 00:03:55.300
they never came back because everyone wanted their buildings


00:03:55.300 --> 00:03:58.540
pixelated, and they were like, okay, fuck this.


00:03:58.540 --> 00:03:59.540
Germany is too difficult.


00:03:59.540 --> 00:04:01.820
We're never gonna send our cars through here again.


00:04:01.820 --> 00:04:03.860
So, you know, I definitely encourage you


00:04:03.860 --> 00:04:05.740
to use Google Street View in Berlin.


00:04:05.740 --> 00:04:08.740
It's really fun from like the historical perspective.


00:04:08.740 --> 00:04:09.620
- How funny, yeah.


00:04:09.620 --> 00:04:11.580
So you can go and basically say,


00:04:11.580 --> 00:04:14.380
I want my house fuzzed out


00:04:14.380 --> 00:04:17.220
so you can't see details about my personal business.


00:04:17.220 --> 00:04:18.940
- Yeah, and a lot of buildings will look like that.


00:04:18.940 --> 00:04:20.300
- Yeah, yeah.


00:04:20.300 --> 00:04:23.300
- Yeah, if I went to my place on Google Maps,


00:04:23.300 --> 00:04:24.780
you can see it evolve over time.


00:04:24.780 --> 00:04:27.100
And I go, that was when I still had that other car


00:04:27.100 --> 00:04:29.940
before it broke down or crashed or whatever.


00:04:29.940 --> 00:04:35.300
And I could sort of judge how old the pictures are by,


00:04:35.300 --> 00:04:37.580
you know, is it, what season is it?


00:04:37.580 --> 00:04:40.860
What's in the driveway or what's the porch look like?


00:04:40.860 --> 00:04:43.000
What kind of, you know, chairs do we have?


00:04:43.000 --> 00:04:44.380
There's all sorts of detail.


00:04:44.380 --> 00:04:46.820
Like none of it's obscured, right?


00:04:46.820 --> 00:04:51.220
There's a fun fact that some researchers worked on in the US


00:04:51.220 --> 00:04:55.300
of could they do the census just via Google Street View?


00:04:55.300 --> 00:04:57.500
And they found there was a heavy correlation


00:04:57.500 --> 00:05:00.180
between census data and the makes of cars


00:05:00.180 --> 00:05:02.260
that people had in their driveway.


00:05:02.260 --> 00:05:02.760
So--


00:05:02.760 --> 00:05:03.300
Oh my goodness.


00:05:03.300 --> 00:05:03.800
Whoa.


00:05:03.800 --> 00:05:05.660
It's a good-- it's an interesting paper.


00:05:05.660 --> 00:05:08.640
Yeah, I think actually Timnit Gebru might have been


00:05:08.640 --> 00:05:13.540
on that paper as well, the kind of very well-known machine


00:05:13.540 --> 00:05:15.100
learning ethics researcher who's now


00:05:15.100 --> 00:05:17.740
running her own organization in the space.


00:05:17.740 --> 00:05:19.420
Anyways, it's a really cool paper.


00:05:19.420 --> 00:05:21.060
I'll see if I can find it and send it to you


00:05:21.060 --> 00:05:21.900
for the show notes.


00:05:21.900 --> 00:05:23.380
- Yeah, put it in the show notes, awesome.


00:05:23.380 --> 00:05:25.860
All right, well, congratulations on the ThoughtWorks thing.


00:05:25.860 --> 00:05:26.700
That's really cool.


00:05:26.700 --> 00:05:29.620
Ines, how about you tell people about yourself as well?


00:05:29.620 --> 00:05:31.800
It's been almost a year, I think,


00:05:31.800 --> 00:05:33.860
maybe since I had you on Talk Python.


00:05:33.860 --> 00:05:36.020
- Yeah, I think it was the year in review.


00:05:36.020 --> 00:05:37.580
I was in Australia at the time.


00:05:37.580 --> 00:05:38.420
It was summer.


00:05:38.420 --> 00:05:41.300
Now I'm in Berlin, now it's winter.


00:05:41.300 --> 00:05:43.820
- Cool, yeah.


00:05:43.820 --> 00:05:47.660
But yeah, I'm still the co-founder of Explosion.


00:05:47.660 --> 00:05:50.500
We're probably most known for our open source library,


00:05:50.500 --> 00:05:52.700
spaCy, which is an open source library


00:05:52.700 --> 00:05:54.100
for natural language processing.


00:05:54.100 --> 00:05:57.540
And one of the main things people do with our stack


00:05:57.540 --> 00:06:01.460
is build NLP and machine learning applications.


00:06:01.460 --> 00:06:03.940
We also publish an annotation tool called Prodigy,


00:06:03.940 --> 00:06:06.540
which allows creating training data


00:06:06.540 --> 00:06:07.580
for machine learning models.


00:06:07.580 --> 00:06:09.620
And all our work and everything we do


00:06:09.620 --> 00:06:12.280
is very focused on running things on your own hardware,


00:06:12.280 --> 00:06:15.960
data privacy, and that's also something that's very important


00:06:15.960 --> 00:06:18.940
and something that we see our users and customers do.


00:06:18.940 --> 00:06:20.620
So people want to train their own models


00:06:20.620 --> 00:06:23.860
and then actually think about how do I create my data?


00:06:23.860 --> 00:06:26.240
What do I do to make my model good?


00:06:26.240 --> 00:06:28.960
What do I do to make my application work?


00:06:28.960 --> 00:06:31.840
And so this all ties in quite well with other questions


00:06:31.840 --> 00:06:34.380
about like, okay, what should I do?


00:06:34.380 --> 00:06:36.800
How should I reason about my data?


00:06:36.800 --> 00:06:40.680
Which we also see as a very, very important point.


00:06:40.680 --> 00:06:43.180
And I actually think this can even prevent a lot of problems


00:06:43.180 --> 00:06:46.500
that we see if you actually just look at your data,


00:06:46.500 --> 00:06:49.660
think about what do I want my system and my pipeline to do,


00:06:49.660 --> 00:06:51.500
how do I break down my problem.


00:06:51.500 --> 00:06:56.100
And that's exactly what the tools we're building


00:06:56.100 --> 00:06:59.140
are hopefully helping people to do.


00:06:59.140 --> 00:07:00.820
Yeah, fantastic.


00:07:00.820 --> 00:07:02.780
So you have a spacey, and then you have some--


00:07:02.780 --> 00:07:04.020
that's the open source thing.


00:07:04.020 --> 00:07:06.260
You also have some products on top of that, right?


00:07:06.260 --> 00:07:06.900
Yeah, exactly.


00:07:06.900 --> 00:07:09.500
So we have a Prodigy, if you scroll down a bit here,


00:07:09.500 --> 00:07:12.420
you're pulling up the page.


00:07:12.420 --> 00:07:15.060
Yeah, exactly, Prodigy, that's an annotation tool.


00:07:15.060 --> 00:07:17.940
And we're currently working on an extension for it


00:07:17.940 --> 00:07:22.100
that it's a bit more like a SaaS cloud tool,


00:07:22.100 --> 00:07:23.980
but has a private cloud aspect.


00:07:23.980 --> 00:07:25.580
So you can bring your own hardware,


00:07:25.580 --> 00:07:30.140
you can run your code, your data on your own cloud resources.


00:07:30.140 --> 00:07:33.100
So no data, nothing has to leave your service.


00:07:33.100 --> 00:07:34.900
And that's something that people already find


00:07:34.900 --> 00:07:35.860
very appealing about Prodigy.


00:07:35.860 --> 00:07:38.100
You can just download it, run it.


00:07:38.100 --> 00:07:43.100
It doesn't send anything across over the internet.


00:07:43.100 --> 00:07:45.660
- I really love that you all are.


00:07:45.660 --> 00:07:47.200
Yeah, I love that you all are embracing that


00:07:47.200 --> 00:07:50.860
because there's such, we'll get into this later,


00:07:50.860 --> 00:07:52.980
not the first topic, but it's related.


00:07:52.980 --> 00:07:54.620
So I'll just talk a bit about it.


00:07:54.620 --> 00:07:57.300
I really like that you're not sending


00:07:57.300 --> 00:07:58.980
the people's data back


00:07:58.980 --> 00:08:01.700
because if they're gonna trust your tools,


00:08:01.700 --> 00:08:04.580
they need to know that you're not sharing data


00:08:04.580 --> 00:08:07.720
that is either part of their competitive advantage


00:08:07.720 --> 00:08:10.100
or they have to protect for privacy reasons.


00:08:10.100 --> 00:08:15.100
I recently started, got into the GitHub Copilot trial


00:08:15.100 --> 00:08:17.500
and I installed that and it said,


00:08:17.500 --> 00:08:19.540
or preview, whatever it's called,


00:08:19.540 --> 00:08:22.180
and they said, "Oh, you just have to accept this agreement


00:08:22.180 --> 00:08:27.180
"where it says, if you generate code from us,


00:08:27.180 --> 00:08:28.820
"we're gonna get some analytics."


00:08:28.820 --> 00:08:29.920
I'm like, "All right, that's fine."


00:08:29.920 --> 00:08:32.020
Whatever, I ask it how to connect to SQLAlchemy


00:08:32.020 --> 00:08:33.700
'cause I forgot, it'll just tell me.


00:08:33.700 --> 00:08:36.900
Oh, and if you make any edits, we're gonna send those back.


00:08:36.900 --> 00:08:39.420
Wait a minute, what if one of the edits


00:08:39.420 --> 00:08:42.060
is put my AWS access key in there


00:08:42.060 --> 00:08:43.220
'cause it needs to be there


00:08:43.220 --> 00:08:45.660
or for a little not thing that I'm gonna publish,


00:08:45.660 --> 00:08:47.940
but it's still going back, right?


00:08:47.940 --> 00:08:50.580
So there's a lot of things and I've just uninstalled them.


00:08:50.580 --> 00:08:51.500
I'm like, you know what?


00:08:51.500 --> 00:08:54.380
No, this is just too much to risk


00:08:54.380 --> 00:08:57.440
or too little benefit for me in my world.


00:08:57.440 --> 00:08:59.160
- Yeah, I think we also see a lot of it


00:08:59.160 --> 00:09:00.460
is also kind of pointless.


00:09:00.460 --> 00:09:02.260
I think there used to be this idea that like,


00:09:02.260 --> 00:09:05.140
Oh, you can just like collect all the data.


00:09:05.140 --> 00:09:07.780
And then at some point you can do some magical AI with it.


00:09:07.780 --> 00:09:10.900
And I think for years, this used to be the classic pitch of every startup.


00:09:10.900 --> 00:09:11.460
Like, I don't know.


00:09:11.460 --> 00:09:14.860
It's almost used to be what we work pitched in some weird way.


00:09:14.860 --> 00:09:18.540
It's usually like, oh, we do X and then we collect data and then it's dot, dot,


00:09:18.540 --> 00:09:21.140
dot, and then it's AI and then it's profit.


00:09:21.140 --> 00:09:26.300
Um, and that, that used to be people like map out their business plan.


00:09:26.300 --> 00:09:29.180
I think this has changed a bit, but I think you can still see some of the


00:09:29.180 --> 00:09:31.220
where companies are like,


00:09:31.220 --> 00:09:33.220
oh, we might as well get as much data as possible


00:09:33.220 --> 00:09:35.300
because maybe there's something we can do with it.


00:09:35.300 --> 00:09:38.380
And we've always seen working in the field


00:09:38.380 --> 00:09:41.380
that like, nah, I don't want new annotations.


00:09:41.380 --> 00:09:45.340
Like there's literally no advantage I get from that.


00:09:45.340 --> 00:09:47.500
So I might as well set up the tools


00:09:47.500 --> 00:09:49.300
so that you can keep them.


00:09:49.300 --> 00:09:50.700
- That's perfect.


00:09:50.700 --> 00:09:54.500
And Catherine, Mr. Hypermagnetic says,


00:09:54.500 --> 00:09:57.300
hi, I thought KJM and Stan was a hip new tech stack.


00:09:57.300 --> 00:09:58.620
(all laughing)


00:09:58.620 --> 00:10:00.860
- No, it's my company name.


00:10:00.860 --> 00:10:02.660
So yeah, yeah, yeah, yeah.


00:10:02.660 --> 00:10:04.920
So it's like a good inside joke


00:10:04.920 --> 00:10:08.060
that lives on many, many decades later.


00:10:08.060 --> 00:10:08.880
- I love it.


00:10:08.880 --> 00:10:12.140
All right, well, let's kick things off with some regulation


00:10:12.140 --> 00:10:15.980
and then we can go and talk more about


00:10:15.980 --> 00:10:17.820
maybe some other laws we can talk about,


00:10:17.820 --> 00:10:20.000
things like co-pilot and other stuff.


00:10:20.000 --> 00:10:24.500
But I did say this was a European focused,


00:10:24.500 --> 00:10:26.240
at least kickoff to the show here.


00:10:26.240 --> 00:10:31.240
So I think one of the biggest tech stories out of the EU


00:10:31.240 --> 00:10:34.720
in the last couple of years has got to be GDPR, right?


00:10:34.720 --> 00:10:38.400
Like that's, I still heard just yesterday


00:10:38.400 --> 00:10:40.680
people talking about, well, this is because of GDPR


00:10:40.680 --> 00:10:43.360
and because we're doing that and this company is not, right?


00:10:43.360 --> 00:10:46.720
There's still just so much awareness of privacy


00:10:46.720 --> 00:10:47.800
because of GDPR.


00:10:47.800 --> 00:10:51.280
And I do think there are some benefits


00:10:51.280 --> 00:10:52.840
and I think there are some drawbacks to it


00:10:52.840 --> 00:10:55.960
but it certainly is making a difference, right?


00:10:55.960 --> 00:11:02.200
And so now there's something that might be sort of an equivalent on the AI side of things, right?


00:11:02.200 --> 00:11:05.320
I mean, not exactly the same, but that kind of regulation.


00:11:05.320 --> 00:11:13.480
Yeah, yeah, it's interesting because it's been a work in progress for some years, like the GDPR was.


00:11:13.480 --> 00:11:22.680
So the initial talks for the GDPR started, I think, in like 2014, 2015, didn't get written until 2016, went into effect in 2018.


00:11:23.680 --> 00:11:27.380
and still a topic of conversation now many years later.


00:11:27.380 --> 00:11:30.880
This is some pluses, some minuses, right?


00:11:30.880 --> 00:11:33.400
We can talk about how GDPR was intended


00:11:33.400 --> 00:11:35.820
versus how we've seen it rolled out across the web,


00:11:35.820 --> 00:11:39.480
which is quite different than what was intended, obviously.


00:11:39.480 --> 00:11:41.560
- I think that's always a problem


00:11:41.560 --> 00:11:44.640
with a lot of regulations and the EU in general.


00:11:44.640 --> 00:11:47.360
You see, like, I'm very pro-regulation.


00:11:47.360 --> 00:11:49.280
I think the idea of GDPR is great,


00:11:49.280 --> 00:11:55.280
But of course, once a large organization like the EU


00:11:55.280 --> 00:11:59.800
rolls things out, it can go a bit wrong here and there.


00:11:59.800 --> 00:12:01.920
Well, let me just set a little foundation


00:12:01.920 --> 00:12:05.120
about why I sort of said I think there are some negatives.


00:12:05.120 --> 00:12:07.480
I think the privacy stuff is all great.


00:12:07.480 --> 00:12:09.840
I think the ability to export your data is great.


00:12:09.840 --> 00:12:12.400
The ability to have it erased, to know what's being done,


00:12:12.400 --> 00:12:14.800
these are all good things.


00:12:14.800 --> 00:12:17.960
I feel, though, that it disproportionately


00:12:17.960 --> 00:12:20.280
was difficult for small businesses,


00:12:20.280 --> 00:12:26.860
but was aimed at places like Facebook and Google and stuff.


00:12:26.860 --> 00:12:29.780
So for example, for like my courses and stuff,


00:12:29.780 --> 00:12:33.060
I had to stop doing anything else for almost two weeks


00:12:33.060 --> 00:12:35.740
and rewrite a whole bunch of different things


00:12:35.740 --> 00:12:39.500
to make, to comply to the best of my knowledge,


00:12:39.500 --> 00:12:42.000
I'm doing it right, but who knows?


00:12:42.000 --> 00:12:46.300
Whereas Facebook didn't shut down for two weeks.


00:12:46.300 --> 00:12:50.300
- Yeah, well, no, they had quite a bit of--


00:12:50.300 --> 00:12:52.620
- Only as a percentage, only as a percentage


00:12:52.620 --> 00:12:54.220
of total employees, I mean small.


00:12:54.220 --> 00:12:56.620
- But they actually had to shut down several products


00:12:56.620 --> 00:12:58.620
that are no longer available in Europe


00:12:58.620 --> 00:13:01.020
that are available in other jurisdictions.


00:13:01.020 --> 00:13:03.500
And also when we look at who's been fined,


00:13:03.500 --> 00:13:06.220
it's been predominantly the FANGs and other large--


00:13:06.220 --> 00:13:08.060
- Yeah, I do think the enforcement


00:13:08.060 --> 00:13:11.500
is focused on the FANG side of things, yeah.


00:13:11.500 --> 00:13:12.860
- Yeah, so-- - Which is fair.


00:13:12.860 --> 00:13:14.700
- It's like, you know, we're not gonna be


00:13:14.700 --> 00:13:16.460
- Yeah, so it's like, you know,


00:13:16.460 --> 00:13:18.780
which is basically what most folks said


00:13:18.780 --> 00:13:21.460
when it went into enforcement is like,


00:13:21.460 --> 00:13:23.040
yes, we believe these are things


00:13:23.040 --> 00:13:24.340
that everybody should be doing


00:13:24.340 --> 00:13:27.220
to better look after the security of sensitive data,


00:13:27.220 --> 00:13:30.460
regardless of, you know, the provenance, so to speak.


00:13:30.460 --> 00:13:35.120
But also we intend to employ this legislation


00:13:35.120 --> 00:13:37.300
to look after these problems, right?


00:13:37.300 --> 00:13:40.880
And everything that Max Scrums has been doing,


00:13:40.880 --> 00:13:43.220
he's based here in Germany


00:13:43.220 --> 00:13:47.000
He's been filing quite a lot of amazing lawsuits


00:13:47.000 --> 00:13:48.880
against a variety of the things


00:13:48.880 --> 00:13:52.000
and been getting some interesting rulings,


00:13:52.000 --> 00:13:54.300
let's say from the European courts.


00:13:54.300 --> 00:13:56.080
- Yeah, good.


00:13:56.080 --> 00:13:59.540
Ines, how was the GPR for you


00:13:59.540 --> 00:14:01.280
before we get to the next law?


00:14:01.280 --> 00:14:03.080
- Yeah, I mean--


00:14:03.080 --> 00:14:04.480
- Explosion, was it a big deal?


00:14:04.480 --> 00:14:07.820
- Not so much because I think actually already


00:14:07.820 --> 00:14:09.840
our standards were such that like,


00:14:09.840 --> 00:14:15.800
We weren't really doing anything that was violating or intended to violate


00:14:15.800 --> 00:14:17.320
what then became GDPR.


00:14:17.320 --> 00:14:21.120
Like I think it was just, we had to go through some things to make, I don't


00:14:21.120 --> 00:14:25.240
know, we've always intended to not have any cookies on our sites.


00:14:25.240 --> 00:14:27.440
So we don't, in the first place.


00:14:27.440 --> 00:14:31.440
And then, it's actually a lot of work to like, make sure that like nothing you


00:14:31.440 --> 00:14:33.800
use tries to sneak some cookies in there.


00:14:33.800 --> 00:14:36.320
And then you're like, ah, I used the wrong URL here.


00:14:36.320 --> 00:14:38.640
Now I have all these YouTube cookies again.


00:14:38.680 --> 00:14:43.320
But I think in general, we were already,


00:14:43.320 --> 00:14:48.320
even before it came up, or before GDPR really came out,


00:14:48.320 --> 00:14:51.360
we realized that, oh, we're actually pretty compliant,


00:14:51.360 --> 00:14:53.720
or at least we already aim to be compliant,


00:14:53.720 --> 00:14:56.200
so we didn't have to do very much.


00:14:56.200 --> 00:15:00.400
- I think I was too, in terms of principle,


00:15:00.400 --> 00:15:02.720
but not exactly in practice.


00:15:02.720 --> 00:15:04.000
There were those types of things.


00:15:04.000 --> 00:15:06.880
Like, for example, I have the discuss comment section


00:15:06.880 --> 00:15:09.360
at the bottom of all the pages.


00:15:09.360 --> 00:15:14.160
And then I realized they were putting double-click cookies


00:15:14.160 --> 00:15:16.640
and Facebook cookies and all sorts of stuff.


00:15:16.640 --> 00:15:19.320
I'm like, wait a minute, I don't want people


00:15:19.320 --> 00:15:20.600
who come to my page to get that,


00:15:20.600 --> 00:15:21.440
but I'm not trying to use it.


00:15:21.440 --> 00:15:23.560
It's like this cascading chain


00:15:23.560 --> 00:15:25.680
and yeah, like embedding YouTube videos.


00:15:25.680 --> 00:15:26.880
We go to a lot of work to make sure


00:15:26.880 --> 00:15:30.120
that it doesn't actually embed YouTube.


00:15:30.120 --> 00:15:31.560
It does a thing that then allows you


00:15:31.560 --> 00:15:33.920
to get to the YouTube without the, yeah.


00:15:33.920 --> 00:15:36.280
Anyway, it's like that kind of stuff, right?


00:15:36.280 --> 00:15:37.800
But still, I think it's good.


00:15:37.800 --> 00:15:40.040
I think it's pretty good in general.


00:15:40.040 --> 00:15:43.360
But let's talk about machine learning and AI stuff.


00:15:43.360 --> 00:15:46.360
So I pulled out some highlights here.


00:15:46.360 --> 00:15:51.520
Let me maybe throw them out, and you all can react to them.


00:15:51.520 --> 00:15:56.400
So we've got this article on techmonitor.ai called


00:15:56.400 --> 00:15:59.600
"The EU's Leaked AI Regulation is Ambitious


00:15:59.600 --> 00:16:01.640
but Disappointingly Vague.


00:16:01.640 --> 00:16:04.160
New Rules for AI in Europe are Simultaneously


00:16:04.160 --> 00:16:06.240
Bold and Underwhelming."


00:16:06.240 --> 00:16:07.800
I think they interviewed different people


00:16:07.800 --> 00:16:10.920
who probably have those opinions,


00:16:10.920 --> 00:16:12.120
as you could see through the article.


00:16:12.120 --> 00:16:13.440
It's not the same person necessarily


00:16:13.440 --> 00:16:14.940
that holds both those at once.


00:16:14.940 --> 00:16:20.440
So this is leaked, but this was leaked in April 15th


00:16:20.440 --> 00:16:23.080
of this year, and I think seven days later or something,


00:16:23.080 --> 00:16:24.280
the actual thing was published.


00:16:24.280 --> 00:16:26.300
So it's not so much about the leak,


00:16:26.300 --> 00:16:29.040
just that the article kind of covers the details, right?


00:16:29.040 --> 00:16:32.120
This is still not unknown, is it?


00:16:32.120 --> 00:16:35.960
- No, no, no, the full text is available


00:16:35.960 --> 00:16:41.280
there's been a lot of good kind of deeper analysis from variety perspectives.


00:16:41.280 --> 00:16:41.560
Yeah.


00:16:41.560 --> 00:16:42.480
Yeah.


00:16:42.480 --> 00:16:47.640
So, Catherine, you want to give us a quick summary of what the goal of this is?


00:16:47.640 --> 00:16:49.400
Yeah.


00:16:49.400 --> 00:16:55.480
So I think, I mean, when I first got wind that this was going to be happening, I was


00:16:55.480 --> 00:17:01.400
talking with some folks at the Bundesministerium Intern, which is basically the


00:17:01.400 --> 00:17:04.200
Intern German administration here.


00:17:04.200 --> 00:17:11.000
So think of like the, if we had a, in the U S sorry, U S centric in the U S, if we


00:17:11.000 --> 00:17:15.380
had like an office of Homeland Security and the interior, and they were all


00:17:15.380 --> 00:17:19.620
together and they like also did like FTC like things, that's what it would be.


00:17:19.620 --> 00:17:20.340
Anyways.


00:17:20.340 --> 00:17:25.220
So, and they have a group called the Dutton Ethic Commission, which


00:17:25.220 --> 00:17:26.620
is data ethics commission.


00:17:26.620 --> 00:17:31.680
And they had built several large reports on thinking and analyzing about the


00:17:31.680 --> 00:17:38.400
risk of algorithmic-based systems and algorithmic-based decision making, which has been a topic of


00:17:38.400 --> 00:17:43.680
conversation obviously for a long time. Eventually all of that what I found out was that they were


00:17:43.680 --> 00:17:50.240
talking then with other groups in the EU about forming a regulation like this and if anybody


00:17:50.240 --> 00:17:56.560
wants to read the German Datten Ethic Commission report, which also is available in English,


00:17:56.560 --> 00:18:02.160
you can see that a lot of the ideas are kind of taken and transferred there, which is basically


00:18:02.160 --> 00:18:09.120
like when we think about AI systems, can we analyze the level of risk that they would have,


00:18:09.120 --> 00:18:16.080
let's say, in use in society? So you can think of very high risk being like bombing people and


00:18:16.080 --> 00:18:24.400
very low risk like, people like drones or, or self-flying planes or absolutely. I mean,


00:18:24.400 --> 00:18:26.680
I mean, we have drones that bomb people.


00:18:26.680 --> 00:18:28.520
That's the thing that happens in the world.


00:18:28.520 --> 00:18:32.960
But what is less common is that you just send the drone out


00:18:32.960 --> 00:18:35.500
and say, go find the quote bad people


00:18:35.500 --> 00:18:36.500
and take care of them, right?


00:18:36.500 --> 00:18:39.560
There's still usually a person somewhere


00:18:39.560 --> 00:18:41.440
that makes a decision.


00:18:41.440 --> 00:18:44.640
And so, I mean, I don't think we want a world


00:18:44.640 --> 00:18:47.520
where we just send out robots to take care of stuff.


00:18:47.520 --> 00:18:49.760
- I think some people want that


00:18:49.760 --> 00:18:54.640
because it can be very nice to absolve yourself


00:18:54.640 --> 00:18:56.360
of that responsibility.


00:18:56.360 --> 00:18:57.720
You're the one pressing the button,


00:18:57.720 --> 00:18:59.120
you have to answer for that,


00:18:59.120 --> 00:19:01.320
and you have to take accountability.


00:19:01.320 --> 00:19:03.360
If the machine did that, well, I mean,


00:19:03.360 --> 00:19:05.560
it's kind of like this problem of, okay,


00:19:05.560 --> 00:19:09.680
whose fault is it if a self-driving car kills a pedestrian?


00:19:09.680 --> 00:19:12.080
- Yeah, that's true.


00:19:12.080 --> 00:19:14.040
- There's a really great psychological theorem


00:19:14.040 --> 00:19:18.240
around that too, called the moral crumple zone,


00:19:18.240 --> 00:19:21.880
which is basically talks about how the nearest human


00:19:21.880 --> 00:19:24.680
to an automated system gets blamed.


00:19:24.680 --> 00:19:27.120
So, yeah.


00:19:27.120 --> 00:19:30.040
So, you know, it's like, well, why don't you do something?


00:19:30.040 --> 00:19:31.960
I don't know, the computer said yes.


00:19:31.960 --> 00:19:35.480
So, you know, it's interesting psychology


00:19:35.480 --> 00:19:37.600
that we use to like judge people.


00:19:37.600 --> 00:19:40.120
Like you should have done something.


00:19:40.120 --> 00:19:41.080
- Yeah, and I do think actually


00:19:41.080 --> 00:19:42.280
in the self-driving car example,


00:19:42.280 --> 00:19:44.000
I think a lot of people would actually say,


00:19:44.000 --> 00:19:46.680
yeah, the developer who built that system


00:19:46.680 --> 00:19:50.040
that made the decision to drive forward and not stop


00:19:50.040 --> 00:19:51.040
is to blame.


00:19:51.040 --> 00:19:53.840
So, you know, that does check out, yeah.


00:19:53.840 --> 00:19:56.020
- It could be, I really like the crumple zone analogy.


00:19:56.020 --> 00:19:57.860
I don't know if I'm receiving it correctly,


00:19:57.860 --> 00:20:00.560
but you know, if you're in a car crash,


00:20:00.560 --> 00:20:03.480
like the radiator is gonna get smashed straight away.


00:20:03.480 --> 00:20:05.040
Like that's the first thing when it caves in,


00:20:05.040 --> 00:20:07.320
but the driver back in the middle


00:20:07.320 --> 00:20:08.640
might be the one who did it.


00:20:08.640 --> 00:20:11.040
In the software world, maybe the equivalent is,


00:20:11.040 --> 00:20:13.000
yeah, the developer made that choice,


00:20:13.000 --> 00:20:16.360
but they made that choice 'cause the CEO and the manager


00:20:16.360 --> 00:20:19.960
said, we're optimizing for this and we don't care.


00:20:19.960 --> 00:20:22.640
We want to ship faster or we want to make sure


00:20:22.640 --> 00:20:24.720
this scenario is perfect.


00:20:24.720 --> 00:20:26.600
And they're like, you know what, that's gonna have a problem


00:20:26.600 --> 00:20:28.400
when it gets snowy and we can't see this line.


00:20:28.400 --> 00:20:30.360
Like, you know what, this is what we're aiming for.


00:20:30.360 --> 00:20:32.620
And like, they don't necessarily make them do it,


00:20:32.620 --> 00:20:35.200
but they say, this is really where you gotta go.


00:20:35.200 --> 00:20:37.020
So CrumpleZone, I like it.


00:20:37.020 --> 00:20:40.520
- Yeah, yeah, and actually I think that just to make it


00:20:40.520 --> 00:20:42.840
clear, the CrumpleZone was like that the driver


00:20:42.840 --> 00:20:45.000
would get blamed rather than the company


00:20:45.000 --> 00:20:46.640
that produced the software.


00:20:46.640 --> 00:20:50.080
And so like, or the operator of the radiology machine


00:20:50.080 --> 00:20:52.760
would get blamed like rather than the producers


00:20:52.760 --> 00:20:55.440
because you kind of create this like trust,


00:20:55.440 --> 00:20:58.620
inherent trust, like, well, you know,


00:20:58.620 --> 00:21:00.720
like they're building a self-driving car,


00:21:00.720 --> 00:21:03.080
clearly it's not their fault, it's the driver's fault.


00:21:03.080 --> 00:21:05.200
Why weren't you paying attention or something like this?


00:21:05.200 --> 00:21:06.020
Right?


00:21:06.020 --> 00:21:06.920
- Yeah. - Yeah.


00:21:06.920 --> 00:21:08.100
- Yeah, absolutely.


00:21:08.100 --> 00:21:13.960
So, really quickly, I just wanna say,


00:21:13.960 --> 00:21:16.040
None of us are lawyers.


00:21:16.040 --> 00:21:19.120
So don't take any of this advice and go do legal things.


00:21:19.120 --> 00:21:20.280
Talk to real lawyers.


00:21:20.280 --> 00:21:22.640
But I do want to talk about the law.


00:21:22.640 --> 00:21:24.640
So one of the things the article points out


00:21:24.640 --> 00:21:27.800
is these rules apply to EU companies and those


00:21:27.800 --> 00:21:29.960
that operate in the EU.


00:21:29.960 --> 00:21:33.400
And then what is way more broadly for tech companies


00:21:33.400 --> 00:21:35.520
or impact EU citizens, right?


00:21:35.520 --> 00:21:38.640
So if you have a website and EU citizens use it,


00:21:38.640 --> 00:21:41.080
or you have an app and EU citizens use your API


00:21:41.080 --> 00:21:43.660
and it makes decisions, probably this applies to you as well.


00:21:43.660 --> 00:21:44.660
I'm guessing.


00:21:44.660 --> 00:21:49.300
We'll see in practice how it gets rolled out.


00:21:49.300 --> 00:21:52.580
But yeah, it's always about the case law afterwards.


00:21:52.580 --> 00:21:54.060
But in theory, yes.


00:21:54.060 --> 00:21:55.260
Yeah, yeah, yeah.


00:21:55.260 --> 00:22:00.540
And it's mainly about the documenting risk, is I guess what I would say.


00:22:00.540 --> 00:22:02.660
Like documenting and addressing risks.


00:22:02.660 --> 00:22:07.980
So one interesting thing about it, and I'd be curious to hear both of y'all's thoughts


00:22:07.980 --> 00:22:14.620
around it is kind of bringing to the forefront the idea of auditing AI systems and what should


00:22:14.620 --> 00:22:22.700
be done to better audit and document problems in automated systems like AI systems or machine


00:22:22.700 --> 00:22:29.020
learning systems. That I find quite interesting. Would be curious to hear y'all's take.


00:22:29.020 --> 00:22:34.540
Yeah, I mean I think even fundamentally I think that's also something I pointed out in the article


00:22:34.540 --> 00:22:41.100
is that there's already this problem of how do you even define where an AI system starts,


00:22:41.100 --> 00:22:48.060
where it ends, what is the system, is it just a component, is it the model, but by itself the


00:22:48.060 --> 00:22:52.620
same model can be used in all kinds of different use cases and some of them can be bad and some of


00:22:52.620 --> 00:22:58.460
them can be good, so it has to be the larger component. But then also even where does AI


00:22:58.460 --> 00:23:02.220
really start, you could have a role-based system that does the exact same thing,


00:23:03.020 --> 00:23:07.180
Is that exempt for even if the outcomes are pretty much identical?


00:23:07.180 --> 00:23:10.140
I think that's already where it gets pretty problematic.


00:23:10.140 --> 00:23:18.780
And where I think you'll probably see a lot of people being able to get away with things.


00:23:18.780 --> 00:23:25.100
Yeah, the law seems to try to characterize the behavior of the software rather than the


00:23:25.100 --> 00:23:32.780
implementation or the technology of it. It doesn't say when a neural network comes up with an outcome


00:23:32.780 --> 00:23:35.420
or something along those lines.


00:23:35.420 --> 00:23:39.080
And they talked about how the idea is to try to make it


00:23:39.080 --> 00:23:41.620
more long-lived and more broadly applicable,


00:23:41.620 --> 00:23:46.160
but also that could result in ways to sneak around it.


00:23:46.160 --> 00:23:51.280
Technically, it's still doing what the law's trying to cover


00:23:51.280 --> 00:23:55.240
but somehow we built software that doesn't quite line up.


00:23:55.240 --> 00:23:58.900
- Yeah, and actually to get back to this auditing question,


00:23:58.900 --> 00:24:00.700
I do think this is, it's definitely


00:24:00.700 --> 00:24:02.040
a very interesting part of it.


00:24:02.040 --> 00:24:05.520
and I do think that it also shows that stuff like interpretability


00:24:05.520 --> 00:24:08.800
will become much more relevant and interesting,


00:24:08.800 --> 00:24:11.400
or even more relevant going forward,


00:24:11.400 --> 00:24:15.240
because I do think if we end up with laws like that,


00:24:15.240 --> 00:24:19.840
companies will have to be able to either just explain internally


00:24:19.840 --> 00:24:23.040
why did my system make a certain decision,


00:24:23.040 --> 00:24:25.880
or maybe even this has to be communicated to the user,


00:24:25.880 --> 00:24:30.840
or to citizen in a similar way of how with GDPR


00:24:30.840 --> 00:24:37.200
you can request your data, maybe you can be able to request more information on why a


00:24:37.200 --> 00:24:44.080
certain outcome was produced for you or which features of you the system is using.


00:24:44.080 --> 00:24:47.080
And I think that all that does require, you know.


00:24:47.080 --> 00:24:48.080
Sure.


00:24:48.080 --> 00:24:50.080
The feature seems completely doable.


00:24:50.080 --> 00:24:54.960
It seems entirely reasonable to say, well, we used your age, your gender and your income


00:24:54.960 --> 00:24:57.660
and your education level to make this decision.


00:24:57.660 --> 00:25:02.660
I think maybe more tricky is why did it make a decision?


00:25:02.660 --> 00:25:06.700
I mean, you two know better than I do,


00:25:06.700 --> 00:25:09.500
but it seems really, I can sit down and read code


00:25:09.500 --> 00:25:10.660
and say, well, there's an if statement


00:25:10.660 --> 00:25:13.340
that if this number is greater than this, we do that.


00:25:13.340 --> 00:25:15.980
But it's really hard to do that with neural networks, right?


00:25:15.980 --> 00:25:17.260
- Yeah, machine learning gets tricky


00:25:17.260 --> 00:25:19.100
because machine learning is always code and data.


00:25:19.100 --> 00:25:22.420
And yeah, so it's kind of, it's this software 2.0 idea


00:25:22.420 --> 00:25:25.300
where even testing a system is much more difficult.


00:25:25.300 --> 00:25:27.340
Like if you're just writing straightforward


00:25:27.340 --> 00:25:30.620
Python code, you can then write a test and you can say, oh, if this goes in, this should


00:25:30.620 --> 00:25:31.620
come out.


00:25:31.620 --> 00:25:33.500
And if that's true, then yeah, you have a green test.


00:25:33.500 --> 00:25:34.500
And then...


00:25:34.500 --> 00:25:35.500
Yeah, exactly.


00:25:35.500 --> 00:25:40.540
But I mean, it kind of is a testament to the fact that we don't really test machine learning


00:25:40.540 --> 00:25:42.380
systems today very well.


00:25:42.380 --> 00:25:48.300
Like we have very early in the whole ML ops side of the equation.


00:25:48.300 --> 00:25:53.460
And I think one of the things, so first off is a lot of these audits, people think they're


00:25:53.460 --> 00:25:55.020
going to be self-assessments.


00:25:55.020 --> 00:25:59.580
leave that to question mark of how a self-assessment is going to work at any


00:25:59.580 --> 00:26:04.340
type of scale. But then also they put forward one thing that I really


00:26:04.340 --> 00:26:07.500
liked about this law is they actually put forward things that people should be


00:26:07.500 --> 00:26:12.320
testing for like security of the models, like the privacy of the models, like the


00:26:12.320 --> 00:26:16.500
interpretability of the models and so forth. And I would say that most places


00:26:16.500 --> 00:26:20.580
that are throwing machine learning into production today do not test for any of


00:26:20.580 --> 00:26:21.580
those things before.


00:26:21.580 --> 00:26:29.460
No, I think it's mostly like, oh, some accuracy score and then maybe some feedback from users.


00:26:29.460 --> 00:26:31.460
Does it seem like a reasonable outcome?


00:26:31.460 --> 00:26:32.460
Okay, good.


00:26:32.460 --> 00:26:33.460
It's working.


00:26:33.460 --> 00:26:34.460
Yeah.


00:26:34.460 --> 00:26:36.180
And I think that there's also a lot of, you know, we see this kind of disconnect if you


00:26:36.180 --> 00:26:41.220
go from academia to industry where like, you know, you can't, like academia works differently.


00:26:41.220 --> 00:26:42.900
You have very different objectives and that's great.


00:26:42.900 --> 00:26:45.020
You're trying to explore new algorithms.


00:26:45.020 --> 00:26:49.500
You're trying to solve problems that are really hard and then see what works best on them.


00:26:49.500 --> 00:26:53.420
you know, a brand different technical solutions.


00:26:53.420 --> 00:26:55.620
And in industry, you're actually shipping something


00:26:55.620 --> 00:26:56.500
that affects people.


00:26:56.500 --> 00:27:00.060
And yeah, if you just apply the exact same metrics,


00:27:00.060 --> 00:27:02.380
that's, you know, that's just not enough.


00:27:02.380 --> 00:27:07.140
- What do you think about testing by coming up with


00:27:07.140 --> 00:27:11.340
scenarios that should pass or not pass?


00:27:11.340 --> 00:27:15.220
For example, if you're testing some kind of algorithm


00:27:15.220 --> 00:27:21.620
that says yes or no on a mortgage for a house or something like that, just say, "Look, okay,


00:27:21.620 --> 00:27:29.220
I would expect that a single mom would still be able to get a mortgage. So let's have a single


00:27:29.220 --> 00:27:33.300
mom apply to the model and see what..." You come up with these scenarios, but if it fails any of


00:27:33.300 --> 00:27:39.700
these, it's unfair. Try to give an example. Is it possible? I mean, I don't want to call it cute.


00:27:40.580 --> 00:27:45.140
it's a very idealistic view and that's all very nice but I do think I think I personally I see


00:27:45.140 --> 00:27:49.780
two problems with this one is that a lot of AI systems are usually quite different not everything


00:27:49.780 --> 00:27:53.300
is as straightforward as like oh I have a pipeline here that predicts whether you should get a


00:27:53.300 --> 00:27:57.300
mortgage it's often like lots of different components every company tries to solve very


00:27:57.300 --> 00:28:02.100
very different problems so you can't like easily develop a framework where you have one input and


00:28:02.100 --> 00:28:08.420
one output usually predicting the thing is one tiny part of a much larger application and then also


00:28:08.420 --> 00:28:12.980
okay if you have like you know things like oh should someone get a mortgage, should a private


00:28:12.980 --> 00:28:17.220
company give you a mortgage or not, I think a lot of you know companies would find that oh maybe


00:28:17.220 --> 00:28:24.260
it's up to us whether you know you get a mortgage or not, there's no you know there's no general


00:28:24.260 --> 00:28:28.260
framework for and you know maybe with a mortgage it's a bit different but there are so many


00:28:28.260 --> 00:28:34.340
applications where yes you can say it's really unfair but it's still you know within the round


00:28:34.340 --> 00:28:40.180
of what a company would argue is up to their discretion.


00:28:40.180 --> 00:28:41.780
And I'm not defending that.


00:28:41.780 --> 00:28:44.940
I'm just saying it's very, very difficult to say, oh, you're being unfair.


00:28:44.940 --> 00:28:47.700
- This is not as important as my example.


00:28:47.700 --> 00:28:54.580
- At least in the US, there's actually laws for this,


00:28:54.580 --> 00:28:57.580
so like equal treatment or disparate treatment, we would say.


00:28:57.580 --> 00:29:01.420
So there's actual mathematical, it's a statistical relationship,


00:29:01.420 --> 00:29:08.220
like 70, 70, 30 or 80, 20, I think that you can show that there's disparate treatment. So for


00:29:08.220 --> 00:29:13.500
example, if you could prove that there's that much of a difference if you're a single mother,


00:29:13.500 --> 00:29:17.500
let's say, versus other groups, and you actually have a legal court case, you can take the bank


00:29:17.500 --> 00:29:23.660
to court and you can sue them. So there's some precedence for equal treatment, at least in some


00:29:23.660 --> 00:29:30.140
countries and some jurisdictions. And I think from, but from thinking about the mathematical


00:29:30.140 --> 00:29:35.180
problem of fairness. I mean, in all of the research that a lot of really amazing, intelligent


00:29:35.180 --> 00:29:40.980
researchers have done is they've shown that fairness definitions and the choice of fairness


00:29:40.980 --> 00:29:46.300
and how you define it can actually be mathematically diametrically opposed. So depending on what


00:29:46.300 --> 00:29:53.020
definition you choose, and there's a whole bunch. So Arvind Narayanan and his group in


00:29:53.020 --> 00:29:56.580
the US have been doing a ton of research on this. There's a bunch of folks that have been


00:29:56.580 --> 00:29:59.000
doing research for more than a decade on this.


00:29:59.000 --> 00:30:03.680
All the fat ML people, the fairness, accountability, and transparency and ML


00:30:03.680 --> 00:30:07.520
conferences that run every year have been doing again, two decades,


00:30:07.520 --> 00:30:09.340
nearly of research on this stuff.


00:30:09.340 --> 00:30:11.560
But it's not a solved problem.


00:30:11.560 --> 00:30:17.200
Even if let's say you choose a fairness definition, mathematically, you measure


00:30:17.200 --> 00:30:19.320
the model, you have met that requirement.


00:30:19.320 --> 00:30:23.240
It doesn't mean that the actually what you're trying to show in the world or


00:30:23.240 --> 00:30:24.640
or what you're trying to do in the world


00:30:24.640 --> 00:30:27.880
from like how we humans would define fairness


00:30:27.880 --> 00:30:29.820
is what you've met, right?


00:30:29.820 --> 00:30:31.360
So yeah.


00:30:31.360 --> 00:30:34.240
- Statistics and intuition are not necessarily the same


00:30:34.240 --> 00:30:35.080
for sure.


00:30:35.080 --> 00:30:37.440
Catherine, you had an interesting comment


00:30:37.440 --> 00:30:41.640
about fairness is not the only metric


00:30:41.640 --> 00:30:43.680
before we started recording.


00:30:43.680 --> 00:30:44.880
- Oh yeah.


00:30:44.880 --> 00:30:48.680
Yeah, I mean, the question that I like to ask people


00:30:48.680 --> 00:30:52.520
is let's say you're building a computer vision system


00:30:52.520 --> 00:30:57.520
to identify people and bomb the right targets.


00:30:57.520 --> 00:30:59.640
If you said it performed fairly,


00:30:59.640 --> 00:31:02.200
let's say in relation to gender


00:31:02.200 --> 00:31:04.000
or in relation to skin color,


00:31:04.000 --> 00:31:06.080
to the darkness of your skin color,


00:31:06.080 --> 00:31:08.540
would that be an ethical or fair system?


00:31:08.540 --> 00:31:11.000
- Yeah.


00:31:11.000 --> 00:31:13.400
It's hard.


00:31:13.400 --> 00:31:15.260
It's certainly not an easy answer.


00:31:15.260 --> 00:31:17.080
- Yeah.


00:31:17.080 --> 00:31:17.920
- Yeah.


00:31:17.920 --> 00:31:18.760
- That's it.


00:31:18.760 --> 00:31:20.320
- There's more to it.


00:31:20.320 --> 00:31:22.760
So one of the things that's interesting about this law


00:31:22.760 --> 00:31:26.680
is that it talks about high risk AI systems


00:31:26.680 --> 00:31:30.660
and it refers to those pretty frequently through there.


00:31:30.660 --> 00:31:33.260
So high risk AI systems include those


00:31:33.260 --> 00:31:35.880
used to manipulate human behavior,


00:31:35.880 --> 00:31:40.880
conduct social scoring or for indiscriminate surveillance.


00:31:40.880 --> 00:31:44.120
Those are actually banned in the EU


00:31:44.120 --> 00:31:45.560
according to this law, right?


00:31:45.560 --> 00:31:48.640
- And I guess you could almost read,


00:31:48.640 --> 00:31:52.040
Like you can read who this was written for


00:31:52.040 --> 00:31:56.800
and who they had in mind when they wrote that.


00:31:56.800 --> 00:32:01.800
I think it's quite clear what types of applications


00:32:01.800 --> 00:32:03.400
and what types of companies.


00:32:03.400 --> 00:32:05.560
- Yeah.


00:32:05.560 --> 00:32:07.120
- Yeah.


00:32:07.120 --> 00:32:09.320
- The social scoring stuff is really creepy.


00:32:09.320 --> 00:32:14.800
But yeah, indiscriminate surveillance also.


00:32:14.800 --> 00:32:18.220
And then it also talks about how special authorization


00:32:18.220 --> 00:32:22.380
will be required for remote biometric identification.


00:32:22.380 --> 00:32:26.540
This is, I'm guessing, types of biometric identification


00:32:26.540 --> 00:32:29.100
that you don't actively participate in, right?


00:32:29.100 --> 00:32:31.020
You don't put your fingerprint on something,


00:32:31.020 --> 00:32:32.620
but you just happen to be there.


00:32:32.620 --> 00:32:36.180
They call out specifically facial recognition,


00:32:36.180 --> 00:32:38.460
but I've also heard things like gait,


00:32:38.460 --> 00:32:42.620
the way that you walk, and weird stuff like that.


00:32:42.620 --> 00:32:47.420
So it's not being special authorization will be required.


00:32:47.420 --> 00:32:48.700
- Oh, you're typing too, right?


00:32:48.700 --> 00:32:51.020
- Your typing pattern is quite,


00:32:51.020 --> 00:32:52.700
or is more unique than you think.


00:32:52.700 --> 00:32:54.700
Yeah, yeah, yeah.


00:32:54.700 --> 00:32:57.620
- Yeah, actually, even more sort of old school fingerprinting


00:32:57.620 --> 00:33:00.800
I'm always like amazed at what like can be done


00:33:00.800 --> 00:33:03.220
to like uniquely identify you on the internet


00:33:03.220 --> 00:33:07.600
even without having any personal identifiable information.


00:33:07.600 --> 00:33:08.440
Yeah.


00:33:08.440 --> 00:33:12.900
- Another thing that stood out to me I thought was fun


00:33:12.900 --> 00:33:17.020
is that people have to be told,


00:33:17.020 --> 00:33:19.220
when they're interacting with an AI system.


00:33:19.220 --> 00:33:22.800
So you have to explicitly say,


00:33:22.800 --> 00:33:27.580
hey, this thing that you're talking through here,


00:33:27.580 --> 00:33:31.000
this one, you're not talking to a human right now,


00:33:31.000 --> 00:33:32.400
you're talking to a machine.


00:33:32.400 --> 00:33:36.540
- Yeah, we'll see if it gets rolled out like cookies.


00:33:36.540 --> 00:33:39.120
(all laughing)


00:33:39.120 --> 00:33:42.060
A big blurb of text that says,


00:33:42.060 --> 00:33:44.500
there may or may not be automated systems


00:33:44.500 --> 00:33:46.420
that you interact with on this product.


00:33:46.420 --> 00:33:48.540
- Yeah, because they're also like, yeah, yeah.


00:33:48.540 --> 00:33:50.180
I think it's almost like, I don't know,


00:33:50.180 --> 00:33:51.320
like a classic disclaimer,


00:33:51.320 --> 00:33:52.980
because I think the way it's written


00:33:52.980 --> 00:33:54.380
probably makes people think more


00:33:54.380 --> 00:33:56.380
about like conversational AI,


00:33:56.380 --> 00:33:58.860
but I do think this also covers everything else.


00:33:58.860 --> 00:34:00.820
And if you use some component


00:34:00.820 --> 00:34:04.260
that does some arbitrary predictions in order to,


00:34:04.260 --> 00:34:06.260
I don't know, this can go like 20 levels deep,


00:34:06.260 --> 00:34:07.660
then you need this disclaimer on it.


00:34:07.660 --> 00:34:09.540
And then unfortunately, I think,


00:34:09.540 --> 00:34:11.500
and the unfortunate side effect of that will be that,


00:34:11.500 --> 00:34:13.260
okay, people are less likely to notice it


00:34:13.260 --> 00:34:15.780
because it will have to be on everything.


00:34:15.780 --> 00:34:17.560
like, you know, they even really small features


00:34:17.560 --> 00:34:19.940
you might wanna have on your website


00:34:19.940 --> 00:34:22.920
that do use AI in some form or another.


00:34:22.920 --> 00:34:25.000
- It seems totally reasonable,


00:34:25.000 --> 00:34:27.760
maybe unnecessary, but certainly reasonable.


00:34:27.760 --> 00:34:28.860
But yeah, you're right.


00:34:28.860 --> 00:34:31.640
It's gonna be like the cookie notices, you know?


00:34:31.640 --> 00:34:33.520
So if you go to say, on Netflix,


00:34:33.520 --> 00:34:36.480
and you go to watch a movie,


00:34:36.480 --> 00:34:38.960
well, that list I've recommended for you,


00:34:38.960 --> 00:34:40.920
do you have to like, okay this?


00:34:40.920 --> 00:34:43.880
I'm not sure if I can find it.


00:34:43.880 --> 00:34:45.000
- Yeah, or maybe,


00:34:45.000 --> 00:34:47.240
Maybe it will just be a Netflix,


00:34:47.240 --> 00:34:48.400
and it's like terms and conditions.


00:34:48.400 --> 00:34:49.960
When you accept those terms and conditions


00:34:49.960 --> 00:34:52.120
that most people probably don't even read,


00:34:52.120 --> 00:34:53.400
you will accept that yes,


00:34:53.400 --> 00:34:55.680
everything you're interacting with here is AI.


00:34:55.680 --> 00:34:57.760
And yeah.


00:34:57.760 --> 00:35:00.880
- By the way, here's a very long 20-page document


00:35:00.880 --> 00:35:03.920
about how we may or may not use automated systems


00:35:03.920 --> 00:35:05.560
in your use of this website.


00:35:05.560 --> 00:35:07.320
- Yeah, exactly.


00:35:07.320 --> 00:35:09.680
(laughing)


00:35:09.680 --> 00:35:10.520
Exactly.


00:35:10.520 --> 00:35:12.280
- Have fun.


00:35:12.280 --> 00:35:13.760
Let me know how the reading goes.


00:35:13.760 --> 00:35:14.960
(laughing)


00:35:14.960 --> 00:35:16.720
I said, I thought I got, there was a lot of stuff


00:35:16.720 --> 00:35:19.080
that came out of the GDPR that was pretty good.


00:35:19.080 --> 00:35:22.280
The little, this website may use cookies.


00:35:22.280 --> 00:35:23.120
That to me,


00:35:23.120 --> 00:35:24.880
- It's the worst.


00:35:24.880 --> 00:35:26.040
- Is the worst.


00:35:26.040 --> 00:35:28.140
It's like, do you wanna be able to log in?


00:35:28.140 --> 00:35:30.600
Or do you wanna not be able to log in?


00:35:30.600 --> 00:35:31.440
Well, I wanna be able to log in.


00:35:31.440 --> 00:35:33.240
Okay, we've gotta use cookies.


00:35:33.240 --> 00:35:35.120
So I actually got this thing called,


00:35:35.120 --> 00:35:38.540
I don't care about cookies as a browser extension.


00:35:38.540 --> 00:35:41.920
And if it sees that, it tries to agree to it automatically.


00:35:41.920 --> 00:35:46.920
like every side just so to cut down on the cookie, okay.


00:35:46.920 --> 00:35:50.280
- This was by no means the intention of the law


00:35:50.280 --> 00:35:52.000
just to make it clear to everyone.


00:35:52.000 --> 00:35:54.200
- No, I think it's important to bring that up


00:35:54.200 --> 00:35:56.280
because also, you know, especially I think, yeah,


00:35:56.280 --> 00:35:57.600
from the European perspective, you know,


00:35:57.600 --> 00:36:00.560
I'm genuinely a fan of GDPR and then often, yeah,


00:36:00.560 --> 00:36:02.480
people go like, "Oh, it's all these cookie pop-ups."


00:36:02.480 --> 00:36:05.280
And it's like, yeah, no, that's not GDPR.


00:36:05.280 --> 00:36:08.720
- The cookie pop-ups predated the GDPR, didn't it?


00:36:09.800 --> 00:36:11.000
It was mainly role.


00:36:11.000 --> 00:36:16.260
So some people did it before, but it was deeply rolled out for GDPR


00:36:16.260 --> 00:36:17.600
because there's all this compliance.


00:36:17.600 --> 00:36:24.040
Now a tip for folks, is a, somebody got sued.


00:36:24.040 --> 00:36:30.400
It was Google or Facebook that it was too hard to just, do the least possible.


00:36:30.400 --> 00:36:34.800
So now if you're, if you haven't installed this extension, there's


00:36:34.800 --> 00:36:38.360
usually a big button that says legitimate interest, and you're going


00:36:38.360 --> 00:36:40.680
just press that one is the least amount.


00:36:40.680 --> 00:36:41.720
Um, yes.


00:36:41.720 --> 00:36:44.660
Does it usually now involve two clicks rather than one?


00:36:44.660 --> 00:36:45.200
Yes.


00:36:45.200 --> 00:36:47.320
Um, but the whole,


00:36:47.320 --> 00:36:51.640
does it, because I think as far as I know, it's also, if you get to a set, if


00:36:51.640 --> 00:36:55.480
it offers you to go to a settings page, everything has to be unchecked by default.


00:36:55.480 --> 00:36:58.520
So yeah, it's actually quite convenient.


00:36:58.520 --> 00:37:00.000
You just go to the button.


00:37:00.000 --> 00:37:01.320
That's not except all.


00:37:01.320 --> 00:37:05.040
And then you accept whatever, and then you get nothing.


00:37:05.560 --> 00:37:09.120
It might be this big and the same color as the page.


00:37:09.120 --> 00:37:10.760
So just be like really looking for it.


00:37:10.760 --> 00:37:14.160
- You gotta highlight it and see if there's like a shadow.


00:37:14.160 --> 00:37:15.400
- Which just talks about,


00:37:15.400 --> 00:37:16.880
I don't know if you've talked about this


00:37:16.880 --> 00:37:18.080
on the podcast recently,


00:37:18.080 --> 00:37:20.680
but I've been reading a lot about dark patterns


00:37:20.680 --> 00:37:23.960
and like dark patterns and privacy are like


00:37:23.960 --> 00:37:28.480
in a very deep love relationship on the internet of like,


00:37:28.480 --> 00:37:30.720
no, you really wanna give us all your data?


00:37:30.720 --> 00:37:31.560
You're gonna be so sad if you don't.


00:37:31.560 --> 00:37:33.720
- The dark patterns and the lack of privacy.


00:37:33.720 --> 00:37:34.560
Yeah, yeah, yeah.


00:37:34.560 --> 00:37:37.440
At least with the same color foreground and background,


00:37:37.440 --> 00:37:39.240
that ties into another compliance thing,


00:37:39.240 --> 00:37:40.360
which is accessibility.


00:37:40.360 --> 00:37:41.920
And at least if like in the US,


00:37:41.920 --> 00:37:45.100
you can get sued for having a not accessible website,


00:37:45.100 --> 00:37:47.680
even companies will at least not do this.


00:37:47.680 --> 00:37:48.720
Even if it's only,


00:37:48.720 --> 00:37:50.800
if they don't care about anyone accessing their website


00:37:50.800 --> 00:37:52.880
and all they care about is not getting sued,


00:37:52.880 --> 00:37:55.400
you won't have buttons with the same foreground


00:37:55.400 --> 00:37:57.320
and background color anymore.


00:37:57.320 --> 00:37:58.720
- Yeah, indeed.


00:37:58.720 --> 00:38:00.400
And I'm okay with my,


00:38:00.400 --> 00:38:03.180
I'm okay with Cookie's little clicker thing,


00:38:03.180 --> 00:38:08.180
because I also have like network level tracking blocking.


00:38:08.180 --> 00:38:12.260
So, then they say, sure, well, fine,


00:38:12.260 --> 00:38:13.220
here's your Facebook cookie.


00:38:13.220 --> 00:38:14.260
It's like, no, it is blocked.


00:38:14.260 --> 00:38:17.020
So like, anyway, it's my weird setup.


00:38:17.020 --> 00:38:20.900
Vincent out in the audience says,


00:38:20.900 --> 00:38:22.900
just to mention Rasa, a Python tool


00:38:22.900 --> 00:38:25.820
for open source chatbots took the effort


00:38:25.820 --> 00:38:28.580
or writing down some ethical principles to good design.


00:38:28.580 --> 00:38:31.100
And one of those is that it lists


00:38:31.100 --> 00:38:34.340
a conversational assistant should identify itself as one.


00:38:34.340 --> 00:38:36.300
- Hi Vincent.


00:38:36.300 --> 00:38:37.120
(laughing)


00:38:37.120 --> 00:38:37.960
Yeah, no, Raza is definitely,


00:38:37.960 --> 00:38:40.940
Raza is a great open source library as well.


00:38:40.940 --> 00:38:43.980
We kind of, you know, friends with Spacey.


00:38:43.980 --> 00:38:46.300
It's kind of the same ecosystem.


00:38:46.300 --> 00:38:47.580
And no, I think it's, yeah,


00:38:47.580 --> 00:38:48.860
I think this is a good principle.


00:38:48.860 --> 00:38:51.380
Actually, often when I use like a, like a chat,


00:38:51.380 --> 00:38:53.620
a bot or whatever, and I'm not sure it's a chat bot,


00:38:53.620 --> 00:38:55.780
they're like a lot of, you know, things and ways of,


00:38:55.780 --> 00:38:58.380
things you can write to check if it's a human or not,


00:38:58.380 --> 00:39:00.060
because they're like, you know, there's certain things


00:39:00.060 --> 00:39:01.940
And usually these things are quite bad


00:39:01.940 --> 00:39:03.280
at resolving references.


00:39:03.280 --> 00:39:07.380
So if you use a pronoun or to refer to something


00:39:07.380 --> 00:39:10.420
you previously said or a person or something,


00:39:10.420 --> 00:39:11.620
there are a lot of things that often


00:39:11.620 --> 00:39:13.020
these things are quite bad at.


00:39:13.020 --> 00:39:16.420
And if you vague enough, a human will always get it,


00:39:16.420 --> 00:39:18.980
but a machine might not.


00:39:18.980 --> 00:39:20.700
So if you--


00:39:20.700 --> 00:39:25.420
- Use your text processing ML skills and experience.


00:39:25.420 --> 00:39:27.740
- Yeah, because there was a case where I was like,


00:39:27.740 --> 00:39:30.220
this person, this agent is so incompetent,


00:39:30.220 --> 00:39:32.340
it must be like a machine.


00:39:32.340 --> 00:39:34.540
And then it's actually, it would be a pretty good chatbot


00:39:34.540 --> 00:39:38.180
because the chatbot passed as an incompetent human,


00:39:38.180 --> 00:39:41.140
but no, it turned out it was just an incompetent human.


00:39:41.140 --> 00:39:45.100
- Yeah, that's true.


00:39:45.100 --> 00:39:48.380
The chatbots are very bad at retaining,


00:39:48.380 --> 00:39:50.960
like building up a state of the conversation.


00:39:50.960 --> 00:39:53.500
It's like they see the message and then they respond to it.


00:39:53.500 --> 00:39:56.780
It's like you ask a question


00:39:56.780 --> 00:40:01.780
And then it does, you say, well, what exactly is this about?


00:40:01.780 --> 00:40:03.820
Well, I said it was about this above.


00:40:03.820 --> 00:40:05.740
So what do you think it's, you know, it's.


00:40:05.740 --> 00:40:09.740
- Yeah, I mean, these systems are getting better at this,


00:40:09.740 --> 00:40:11.100
but there's just some like, you know,


00:40:11.100 --> 00:40:13.800
if you really try to be as vague as possible,


00:40:13.800 --> 00:40:15.900
you can like trick them


00:40:15.900 --> 00:40:18.060
and then you find out if it's a human or not.


00:40:18.060 --> 00:40:19.780
- Yeah, exactly.


00:40:19.780 --> 00:40:22.220
So let's see, some more things about the law


00:40:22.220 --> 00:40:24.440
is over here, the military.


00:40:25.420 --> 00:40:30.420
AI in the military is exempt, so that's not a surprise.


00:40:30.420 --> 00:40:33.420
I mean, there's probably a top secret stuff.


00:40:33.420 --> 00:40:35.300
How are you going to submit that?


00:40:35.300 --> 00:40:36.140
I don't know.


00:40:36.140 --> 00:40:37.980
- Yeah, but then it's like, oh, you know,


00:40:37.980 --> 00:40:39.460
a lot of the worst things that happen


00:40:39.460 --> 00:40:40.580
happen in this context.


00:40:40.580 --> 00:40:44.260
So it's like, you know, as a little anecdote,


00:40:44.260 --> 00:40:45.780
for example, for a long time,


00:40:45.780 --> 00:40:48.040
we've always had an exposure with this policy


00:40:48.040 --> 00:40:50.460
that we do not sell to organizations


00:40:50.460 --> 00:40:54.820
who are primarily engaged in government, military,


00:40:54.820 --> 00:40:56.560
for intelligence national security,


00:40:56.560 --> 00:41:00.000
because, and our reasoning for that has always been that,


00:41:00.000 --> 00:41:03.240
well, in the free market, you have a lot of other ways


00:41:03.240 --> 00:41:05.480
that companies and applications can be regulated


00:41:05.480 --> 00:41:07.200
by regulations like this,


00:41:07.200 --> 00:41:09.040
but also just by market pressures


00:41:09.040 --> 00:41:10.960
and by things just being more public.


00:41:10.960 --> 00:41:12.640
All of these things you do not have


00:41:12.640 --> 00:41:16.720
if the work is military intelligence


00:41:16.720 --> 00:41:18.760
or certain government work.


00:41:18.760 --> 00:41:22.280
So we see that as very problematic


00:41:22.280 --> 00:41:23.680
because you have absolutely no idea


00:41:23.680 --> 00:41:25.460
what the software is used for,


00:41:25.460 --> 00:41:28.260
and there's absolutely no way to regulate it ever.


00:41:28.260 --> 00:41:31.120
And then we'd say, okay, that's not


00:41:31.120 --> 00:41:33.520
what we want to sell our software to.


00:41:33.520 --> 00:41:35.280
Like the other use cases, you know,


00:41:35.280 --> 00:41:36.840
some government things are fine,


00:41:36.840 --> 00:41:40.440
you know, we'd happily sell to the IRS and equivalents,


00:41:40.440 --> 00:41:44.400
or, you know, Federal Reserves.


00:41:44.400 --> 00:41:46.680
There are a lot of things that are like not terrible


00:41:46.680 --> 00:41:47.680
that are government adjacent,


00:41:47.680 --> 00:41:50.280
or just a lot of research labs as well, but,


00:41:50.280 --> 00:41:52.440
- Yeah. - Yeah, military.


00:41:52.440 --> 00:41:53.440
That's quite obvious.


00:41:53.440 --> 00:41:55.120
- Yeah.


00:41:55.120 --> 00:41:57.960
- Well, and then you think like how many companies


00:41:57.960 --> 00:41:59.480
that work on machine learning today


00:41:59.480 --> 00:42:03.680
that focus on selling explicitly into the military.


00:42:03.680 --> 00:42:05.680
And it's like, well, are they exempt?


00:42:05.680 --> 00:42:08.320
Because they're just, you know, basically an exempt,


00:42:08.320 --> 00:42:10.360
like is Palantir exempt from this?


00:42:10.360 --> 00:42:11.560
I'm very curious. - Oh, interesting.


00:42:11.560 --> 00:42:15.440
Right, because, yeah, the law would otherwise apply to them,


00:42:15.440 --> 00:42:17.760
but sort of indirectly.


00:42:17.760 --> 00:42:21.520
So you're asking about transitive property, basically.


00:42:21.520 --> 00:42:25.200
- Yeah, yeah, like, well, it's only in use in military use,


00:42:25.200 --> 00:42:26.920
so it's probably okay.


00:42:26.920 --> 00:42:29.120
It's probably exempt or whatever.


00:42:29.120 --> 00:42:30.760
- Yeah, well, I guess if you can make the case


00:42:30.760 --> 00:42:32.880
that it's classified,


00:42:32.880 --> 00:42:34.680
that that's probably what companies like that


00:42:34.680 --> 00:42:36.680
who have the means,


00:42:36.680 --> 00:42:39.480
they would make sure that every project they're taking on


00:42:39.480 --> 00:42:41.120
is classified in some way,


00:42:41.120 --> 00:42:43.640
and then they get a roundup.


00:42:43.640 --> 00:42:45.440
- Yeah, that's probably true.


00:42:45.440 --> 00:42:47.880
All right, another thing that I thought was interesting.


00:42:47.880 --> 00:42:50.480
So all the stuff we talked about so far


00:42:50.480 --> 00:42:58.480
sort of just laying out the details. On the imprecision and subjectivity side, one of


00:42:58.480 --> 00:43:03.040
the quotes was, "One area that raised eyebrows is part of the report which reads, 'AI systems


00:43:03.040 --> 00:43:07.560
designed or used in a manner that exploits information or prediction about a person or


00:43:07.560 --> 00:43:13.400
group of persons in order to target their vulnerabilities or special circumstances,


00:43:13.400 --> 00:43:18.320
causing a person to behave or form an opinion or take a decision to their detriment.'"


00:43:18.320 --> 00:43:23.320
Yeah, that sounds like a lot of big tech, honestly.


00:43:23.320 --> 00:43:26.960
Like a lot of the social networks,


00:43:26.960 --> 00:43:28.400
maybe they even suggest,


00:43:28.400 --> 00:43:31.420
maybe that's even like Amazon shopping recommendations,


00:43:31.420 --> 00:43:32.260
right?


00:43:32.260 --> 00:43:35.380
Encourage you to buy something that you don't need


00:43:35.380 --> 00:43:36.600
or whatever.


00:43:36.600 --> 00:43:37.860
What do you think about that?


00:43:37.860 --> 00:43:40.020
- Yeah, I mean, I guess it's quite vague


00:43:40.020 --> 00:43:41.700
and it's like, okay, how do you define,


00:43:41.700 --> 00:43:44.060
you know, we'd have to wait for like the actual cases


00:43:44.060 --> 00:43:46.020
to come up and from making the case that like,


00:43:46.020 --> 00:43:53.740
"Oh, I don't know, my wife divorced me because X happened and that was the outcome and it's


00:43:53.740 --> 00:44:01.220
clearly that company's fault and then someone can decide whether that's true or not."


00:44:01.220 --> 00:44:06.740
Of course, these are not cases that this was designed for or written for, but it is vague


00:44:06.740 --> 00:44:10.860
to this extent where like, "Yes, this is probably the illegit case that the judge has to decide


00:44:10.860 --> 00:44:12.420
over and maybe the person would win."


00:44:12.420 --> 00:44:16.100
Wouldn't it be great if they gave examples?


00:44:16.100 --> 00:44:21.220
I didn't want to accept the cookies, so I'm suing under the new law.


00:44:21.220 --> 00:44:23.940
Yeah, that made me feel bad.


00:44:23.940 --> 00:44:31.060
But I mean, I think some of it is like really, I feel like the conversation here, and I'm


00:44:31.060 --> 00:44:36.620
being curious to the opinion on the conversation in the US is around kind of the political


00:44:36.620 --> 00:44:42.140
ad manipulation and the amount, let's say, of when we think about topics like disinformation


00:44:42.140 --> 00:44:50.220
misinformation, the amount of kind of algorithmic use of, let's say, opinion pieces to kind of push


00:44:50.220 --> 00:44:57.420
particular agendas. When I read this, I'm guessing that's like one of the things they had in mind.


00:44:57.420 --> 00:45:05.100
I had misinformation and fake news and all that kind of stuff is what brought me to this.


00:45:05.100 --> 00:45:09.980
Yeah, and I was also thinking of recommendation systems and like even, I mean, I don't know,


00:45:09.980 --> 00:45:13.720
not even to fake news, but like, okay, you can, you know,


00:45:13.720 --> 00:45:18.020
manipulate people into, you know, joining certain groups. yeah.


00:45:18.020 --> 00:45:22.080
Yeah. Buying things. Yeah.


00:45:22.080 --> 00:45:27.440
Yeah. You're a relatively just stable, normal person. Yeah.


00:45:27.440 --> 00:45:31.000
And then, then you, you read some posts,


00:45:31.000 --> 00:45:34.880
they suggest you join a group three months later, you'll, you know,


00:45:34.880 --> 00:45:38.000
you're in the wilderness training with a gun or something like it's just,


00:45:38.160 --> 00:45:43.120
It's so easy to send people down these holes, I think.


00:45:43.120 --> 00:45:45.920
On a much more relatable note, I would say,


00:45:45.920 --> 00:45:48.040
even though I really love YouTube,


00:45:48.040 --> 00:45:50.200
one of the sort of sayings,


00:45:50.200 --> 00:45:51.120
I think I heard it somewhere,


00:45:51.120 --> 00:45:52.840
I don't know where it came from, so I can't attribute it,


00:45:52.840 --> 00:45:55.660
but you're never extreme enough for YouTube.


00:45:55.660 --> 00:45:59.240
If you watch three YouTube videos on some topic,


00:45:59.240 --> 00:46:01.200
let's suppose your washer broke,


00:46:01.200 --> 00:46:04.080
and so you need to figure out how does my dishwasher work?


00:46:04.080 --> 00:46:06.520
And so you watch several videos to try to fix it,


00:46:06.520 --> 00:46:08.660
well, your feed is full of dishwasher stuff.


00:46:08.660 --> 00:46:12.260
And if you watch a few more, it's nothing but dishwashers.


00:46:12.260 --> 00:46:14.240
There's a lot of other videos besides dishwashers.


00:46:14.240 --> 00:46:17.820
So any little, like, it's almost like the butterfly effect,


00:46:17.820 --> 00:46:19.200
the chaos theory effect of like,


00:46:19.200 --> 00:46:20.400
I washed a little bit this way,


00:46:20.400 --> 00:46:23.760
and then, whoosh, you just, you end up down that channel.


00:46:23.760 --> 00:46:26.140
- Yeah, and I think like,


00:46:26.140 --> 00:46:28.700
one of the interesting things I think about that is,


00:46:28.700 --> 00:46:31.140
I've been talking with a few folks at like,


00:46:31.140 --> 00:46:33.480
where, you know, a friend's, you know,


00:46:33.480 --> 00:46:35.920
family has been kind of like radicalized


00:46:35.920 --> 00:46:40.060
around some of the topics that I'd say are very radical online right now.


00:46:40.060 --> 00:46:42.760
And they're like, I just don't know how it happens.


00:46:42.760 --> 00:46:46.980
And it's kind of like, well, the internet that they're experiencing


00:46:46.980 --> 00:46:50.600
is incredibly different than the internet that you're experiencing.


00:46:50.600 --> 00:46:55.240
And so it's kind of like, when we think about lockdown or where the


00:46:55.240 --> 00:46:59.200
internet is going to be like a major source of people's life, and then


00:46:59.200 --> 00:47:03.840
their internet is just a completely different experience than yours based


00:47:03.840 --> 00:47:09.120
off of some related search terms across maybe four or five different sites that


00:47:09.120 --> 00:47:12.200
have been linked via cookies or other types of information.


00:47:12.200 --> 00:47:19.160
I mean, it's like, yeah, you can say, well, I have this experience, but if your


00:47:19.160 --> 00:47:23.400
entire world online was different, maybe you wouldn't have the same experience.


00:47:23.400 --> 00:47:27.640
I think it would be very hard to say what, how you would think and feel if your


00:47:27.640 --> 00:47:31.120
entire information experience was completely different.


00:47:31.120 --> 00:47:34.080
Don't make me think about weird alternate realities of myself.


00:47:34.080 --> 00:47:34.580
[LAUGHTER]


00:47:34.580 --> 00:47:36.920
What if just one decision was made differently?


00:47:36.920 --> 00:47:39.080
What world would you be in?


00:47:39.080 --> 00:47:40.960
It could be really different, honestly.


00:47:40.960 --> 00:47:42.920
No, I mean, you wouldn't even necessarily know.


00:47:42.920 --> 00:47:45.280
But I think that's also kind of a problem.


00:47:45.280 --> 00:47:47.440
In that sense, I do like that it's relatively vague.


00:47:47.440 --> 00:47:49.560
And I think laws can be vague because you don't


00:47:49.560 --> 00:47:50.400
know what's going to happen.


00:47:50.400 --> 00:47:52.520
And you might have people who are in a situation


00:47:52.520 --> 00:47:54.520
where they don't necessarily feel like, oh, I've


00:47:54.520 --> 00:47:58.120
been tricked or treated badly here.


00:47:58.120 --> 00:48:01.880
and maybe the outcomes of their behavior are bad,


00:48:01.880 --> 00:48:06.880
but maybe what the platform did wasn't necessarily illegal.


00:48:06.880 --> 00:48:08.120
That's also the problem.


00:48:08.120 --> 00:48:12.200
It's like a lot of the content you can watch on YouTube


00:48:12.200 --> 00:48:16.440
is legal and it's your right as a free citizen,


00:48:16.440 --> 00:48:19.420
especially in the US where people take this


00:48:19.420 --> 00:48:23.360
even more seriously to some degree than people in Europe.


00:48:23.360 --> 00:48:26.360
You can watch anti-vax videos all day


00:48:26.360 --> 00:48:27.520
and that's your right.


00:48:27.520 --> 00:48:29.880
and nobody can keep you from that.


00:48:29.880 --> 00:48:30.720
- It's not good for you, you can do it.


00:48:30.720 --> 00:48:33.720
- No, it's not good for you, but you can.


00:48:33.720 --> 00:48:37.080
And so otherwise I think, yeah,


00:48:37.080 --> 00:48:43.480
with terms that are maybe less vague in that respect,


00:48:43.480 --> 00:48:46.720
it would be much harder to actually go after cases


00:48:46.720 --> 00:48:50.240
where yes, it's clearly, the platform is clearly to blame


00:48:50.240 --> 00:48:51.640
or the platform should be regulated,


00:48:51.640 --> 00:48:53.880
which obviously it's very clear


00:48:53.880 --> 00:48:55.520
that this is what they had in mind.


00:48:55.520 --> 00:48:56.360
- Right.


00:48:56.360 --> 00:49:00.640
I really like the part here that's like exploit information, target


00:49:00.640 --> 00:49:05.560
vulnerabilities, because it's kind of like, okay, I know these, you know, I


00:49:05.560 --> 00:49:08.640
mean, what we saw with Cambridge Analytica and then a bunch of the targeted


00:49:08.640 --> 00:49:14.120
stuff after that was like, we can figure out exactly how to target undecided


00:49:14.120 --> 00:49:18.480
voters of these different racial groups in these counties, and we can like feed


00:49:18.480 --> 00:49:20.920
them as many Facebook ads as possible.


00:49:20.920 --> 00:49:22.360
And it's just like, wow.


00:49:22.360 --> 00:49:22.600
Okay.


00:49:22.600 --> 00:49:28.800
I don't think people realize that that was so easy to put together and do given


00:49:28.800 --> 00:49:32.740
like a fairly small amount of information about a person.


00:49:32.740 --> 00:49:38.480
So, I mean, and, and it's not, it's not personal information, right.


00:49:38.480 --> 00:49:43.480
Because usually it's what we would call, you know, profiles of individuals.


00:49:43.480 --> 00:49:49.060
So you fit a profile because, you know, you like these three brands on Facebook


00:49:49.820 --> 00:49:54.860
and you live in these districts and you're this age and this race or you report this race


00:49:54.860 --> 00:49:58.660
or we can infer your race because of these other things that you've liked.


00:49:58.660 --> 00:50:04.180
It means, you know, it adds a lot of information that I don't think most people know that,


00:50:04.180 --> 00:50:08.260
you know, that you can get that specific in the advertising world.


00:50:08.260 --> 00:50:09.560
Yeah.


00:50:09.560 --> 00:50:14.860
How do you ladies feel about the whole flock thing?


00:50:14.860 --> 00:50:15.580
Oh, yeah.


00:50:15.980 --> 00:50:18.100
Chrome was doing to replace cookies.


00:50:18.100 --> 00:50:20.180
I mean, we wouldn't even have to have those little buttons


00:50:20.180 --> 00:50:21.020
or my add-in.


00:50:21.020 --> 00:50:22.220
It'd be a good world.


00:50:22.220 --> 00:50:24.460
(laughing)


00:50:24.460 --> 00:50:31.460
- So I think that there's been a lot of important writing


00:50:31.460 --> 00:50:36.460
about flocks and vulnerabilities in the design.


00:50:36.460 --> 00:50:37.300
- Sorry, if they don't know.


00:50:37.300 --> 00:50:38.140
- Yeah, I'm sorry.


00:50:38.140 --> 00:50:41.660
- Federated learning of cohorts.


00:50:41.660 --> 00:50:43.100
- Yeah, yeah, yeah.


00:50:43.100 --> 00:50:50.140
And federated learning is essentially a tool that can be privacy preserving, but doesn't have to be.


00:50:50.140 --> 00:51:02.620
And it basically means that the data stays on device and the things that we send to a centralized location or several centralized location are usually gradient updates.


00:51:02.620 --> 00:51:04.460
So these are small updates to the model.


00:51:04.460 --> 00:51:08.180
The model is then shared amongst participants and the process repeats.


00:51:09.100 --> 00:51:17.020
The exact design of how Phlox was rolled out and is rolled out is, I think, not fully clear.


00:51:17.020 --> 00:51:35.020
And in general, I'm a fan of some parts of federated learning, but there's a lot of loopholes in Phlox design that would still involve the ability for people to both reverse engineer the models, but also to fingerprint people.


00:51:35.020 --> 00:51:40.860
So to take your cohort plus your browser fingerprint, you combine the two,


00:51:40.860 --> 00:51:44.860
it becomes fairly easy to re-identify individuals.


00:51:44.860 --> 00:51:48.380
Yeah, and I think the more underlying problem is also that, well, you know,


00:51:48.380 --> 00:51:51.900
are you going to trust something that comes out of Google that's marketed as like,


00:51:51.900 --> 00:51:57.340
"Oh, it will like, you know, preserve your privacy and be like, really great for you and the internet."


00:51:57.340 --> 00:52:01.820
And I mean, that's just like screens of like red flags.


00:52:02.780 --> 00:52:07.780
It feels like we've been presented a false dichotomy.


00:52:07.780 --> 00:52:11.100
We could either have this creepy cookie world


00:52:11.100 --> 00:52:13.940
or because we must still have tracking


00:52:13.940 --> 00:52:15.300
or we could have this flock.


00:52:15.300 --> 00:52:17.020
It's like, well, or we could just not have tracking.


00:52:17.020 --> 00:52:19.060
Like that's also a possible future.


00:52:19.060 --> 00:52:20.540
We don't have to have tracking


00:52:20.540 --> 00:52:22.580
and here's a better tracking mechanism.


00:52:22.580 --> 00:52:23.820
We just have not have tracking.


00:52:23.820 --> 00:52:25.180
How about that?


00:52:25.180 --> 00:52:29.620
- I was reading a wonderful article about IE6.


00:52:29.620 --> 00:52:31.180
Okay, I don't know.


00:52:31.180 --> 00:52:32.020
- Wow, that's almost a history.


00:52:32.020 --> 00:52:35.100
- Young children, I'm sorry, I'm sorry


00:52:35.100 --> 00:52:37.060
that I'm bringing up ancient history,


00:52:37.060 --> 00:52:38.620
but there was a browser once


00:52:38.620 --> 00:52:40.740
and it was called Internet Explorer 6.


00:52:40.740 --> 00:52:43.580
It was the bane of every web developer's existence


00:52:43.580 --> 00:52:45.020
for a long time.


00:52:45.020 --> 00:52:47.620
But one thing that I didn't know about it until recently


00:52:47.620 --> 00:52:52.380
is it actually had privacy standards built into the browser.


00:52:52.380 --> 00:52:54.700
You could set up certain privacy preferences


00:52:54.700 --> 00:52:57.020
and it would like block cookies and websites


00:52:57.020 --> 00:52:59.220
and stuff for you automatically.


00:52:59.220 --> 00:53:01.140
- Way ahead of its time.


00:53:01.140 --> 00:53:05.580
And there was this whole standard called P3P


00:53:05.580 --> 00:53:08.860
that with the WC3 put together around like,


00:53:08.860 --> 00:53:12.500
everybody's gonna have your local stored privacy preferences


00:53:12.500 --> 00:53:14.120
and then when you browse the web,


00:53:14.120 --> 00:53:16.500
it's just gonna automatically block stuff and all this stuff


00:53:16.500 --> 00:53:20.500
and I was like, we figured this out during IE6,


00:53:20.500 --> 00:53:21.340
what happened?


00:53:21.340 --> 00:53:25.020
So yeah, just let you know, a little bit of history.


00:53:25.020 --> 00:53:27.780
Look up P3P, so yeah.


00:53:27.780 --> 00:53:29.620
- Absolutely, all right.


00:53:29.620 --> 00:53:30.980
I guess to close out the flock thing,


00:53:30.980 --> 00:53:34.780
The thing that scares me about this is if I really wanted to, I could open up a private


00:53:34.780 --> 00:53:40.520
window and I could go, I could even potentially fire up a VPN on a different location and


00:53:40.520 --> 00:53:41.520
go visit a place.


00:53:41.520 --> 00:53:47.700
And when I show up there, no matter how creepy tracking that place happens to be, I am basically


00:53:47.700 --> 00:53:50.180
an unknown to that location.


00:53:50.180 --> 00:53:53.940
Whereas this stuff, if your browser constantly puts you into this category, well, you show


00:53:53.940 --> 00:53:55.900
up already in that category.


00:53:55.900 --> 00:54:00.260
There's really no way to sort of have a fresh start, I guess.


00:54:00.260 --> 00:54:07.140
All right, so one thing, Ines, maybe you could speak to this since you're right in the middle of it, is


00:54:07.140 --> 00:54:13.700
they talk about how one of the things that's not mentioned in here is,


00:54:13.700 --> 00:54:21.340
you know, how does it, basically they say the regulation does little to incentivize or support EU innovation and entrepreneurship in this space.


00:54:21.340 --> 00:54:31.340
There's nothing in here in this law that specifically is to promote EU-based ML companies.


00:54:31.340 --> 00:54:35.340
I mean, I guess, I think also even...


00:54:35.340 --> 00:54:37.340
Does it even belong there or is it okay?


00:54:37.340 --> 00:54:40.340
I mean, I don't know. I was actually, I was a bit confused by that.


00:54:40.340 --> 00:54:43.340
It does remind me of like, well, in general, for a long time, a lot of people have said,


00:54:43.340 --> 00:54:46.340
"Oh, the EU is like a bad place for startups."


00:54:46.340 --> 00:54:50.340
I think actually regulation is a big part of that, which is sort of, you know,


00:54:50.340 --> 00:54:54.540
full circle, like a lot of people find that, well, the EU is more difficult.


00:54:54.540 --> 00:54:57.980
You have to stick to all of these rules and people actually enforce them and


00:54:57.980 --> 00:55:00.420
you're less free and you can't do whatever the fuck you want.


00:55:00.420 --> 00:55:03.420
So you should go to the US where people are a bit more like chill.


00:55:03.420 --> 00:55:08.380
And it's a bit more common to like, I don't know, ask for forgiveness later.


00:55:08.380 --> 00:55:11.320
And just like, so I think, I think that is definitely kind of


00:55:11.320 --> 00:55:12.920
a mentality that people have.


00:55:12.920 --> 00:55:16.380
So I'm like, I'm not sure, honestly, I'm not sure what like,


00:55:17.140 --> 00:55:22.500
incentivizing EU entrepreneurship could be.


00:55:22.500 --> 00:55:28.460
I mean, for me personally, it was a very conscious decision for us to start a company in Berlin


00:55:28.460 --> 00:55:31.700
and the EU was like a big part in that.


00:55:31.700 --> 00:55:37.340
I know that maybe I'm not the typical entrepreneur and we're doing things quite differently with our company as well.


00:55:37.340 --> 00:55:43.940
We're not like your typical startup, but being in the EU was actually very attractive to us.


00:55:43.940 --> 00:55:57.420
And even recently, as we sold some shares in the company, it was incredibly important to us to stay a German company and be a company paying taxes to the country that we actually incorporated in and not just become a US company.


00:55:57.420 --> 00:56:02.620
But I know that's not necessarily true for everyone.


00:56:02.620 --> 00:56:05.100
But are you maximizing shareholder value?


00:56:05.100 --> 00:56:05.900
No, just kidding.


00:56:05.900 --> 00:56:10.300
That leads to so many wrongs.


00:56:10.700 --> 00:56:13.740
this short, short sighted thinking. I think that's, that's great that you


00:56:13.740 --> 00:56:19.900
capitalism's gonna capitalism, like, you know, and then I think that as like,


00:56:19.900 --> 00:56:24.660
you know, someone who is also participating in capitalism. Sure. And


00:56:24.660 --> 00:56:28.860
yeah, it's like, I don't know, I do think, you know, Europe is becoming


00:56:28.860 --> 00:56:33.140
more attractive as a location for companies, I think Berlin is becoming


00:56:33.140 --> 00:56:38.580
more attractive as a location to be based in and start a company. But it is


00:56:38.580 --> 00:56:43.340
also true that there are a lot of, I think, more general things that make it


00:56:43.340 --> 00:56:47.500
harder to actually run a business here, especially if you directly compare it to


00:56:47.500 --> 00:56:50.820
the US and yes, a lot of that is also the bureaucracy.


00:56:50.820 --> 00:56:56.260
It is a lot of the structures not being as developed.


00:56:56.260 --> 00:57:00.880
It's also, yeah, if you are looking to get investment in your company, it


00:57:00.880 --> 00:57:05.500
often makes a lot more sense to look in the US for that, which then causes


00:57:05.500 --> 00:57:08.900
other difficulties, if you're, you know, especially a young


00:57:08.900 --> 00:57:11.500
company and you don't, you know, you can't have as many months,


00:57:11.500 --> 00:57:14.420
like in our case, we could be like, okay, here's what we want


00:57:14.420 --> 00:57:18.860
to do. If you're not in that position, you can't do that. And


00:57:18.860 --> 00:57:22.380
so I, like, I agree with like the problems here, but I don't


00:57:22.380 --> 00:57:25.620
know how this law and this proposal.


00:57:25.620 --> 00:57:27.820
What was it supposed to do? Right?


00:57:27.820 --> 00:57:31.980
Yeah, I mean, say, oh, you're sort of you're exempt from some


00:57:31.980 --> 00:57:37.700
of the things if you are like, I don't know, a startup coming to the EU.


00:57:37.700 --> 00:57:44.780
Here's how it advantages the EU. All companies that are not EU-based have to follow this


00:57:44.780 --> 00:57:49.580
law and no rules for the EU. That wouldn't be with the principles of it.


00:57:49.580 --> 00:57:54.620
Yeah, if you're a startup or a company moving to the EU, you don't only have to follow half


00:57:54.620 --> 00:58:01.660
of these things and then everyone's back in, I don't know, having their mailbox companies


00:58:01.660 --> 00:58:14.660
And this also surprises me a little bit, is that there's nothing in here about climate change and model training and sort of the cost of operation of these things.


00:58:14.660 --> 00:58:18.660
Does that surprise you? Would it belong here? What do you all think?


00:58:18.660 --> 00:58:23.660
nothing in here about climate change and model training


00:58:23.660 --> 00:58:27.900
and sort of the cost of operation of these things.


00:58:27.900 --> 00:58:30.140
Does that surprise you?


00:58:30.140 --> 00:58:31.340
Would it belong here?


00:58:31.340 --> 00:58:32.460
What do you all think?


00:58:32.460 --> 00:58:36.580
- I mean, is it a high risk?


00:58:36.580 --> 00:58:37.620
This is my ask.


00:58:37.620 --> 00:58:39.900
It's like when I saw it wasn't in there at all,


00:58:39.900 --> 00:58:42.660
not even lightly mentioned, I was like,


00:58:42.660 --> 00:58:46.380
how many like carbon emissions do we have to go


00:58:46.380 --> 00:58:51.260
until it's high risk. But evidently, the thinking of human side of high risk,


00:58:51.260 --> 00:58:52.980
actually, climate change is also humans.


00:58:52.980 --> 00:58:55.580
I direct high risk.


00:58:55.580 --> 00:59:01.180
When is the AI gonna kill me eventually? Or tomorrow?


00:59:01.180 --> 00:59:04.140
Is it arms or is it just


00:59:04.140 --> 00:59:08.980
is it like just 30 years from now when it floods or something? Yeah, yeah.


00:59:08.980 --> 00:59:13.780
So yeah, I think like it was I was definitely curious to see that they didn't


00:59:13.780 --> 00:59:19.060
included despite all of the kind of work here from the Greens and other parties like them


00:59:19.060 --> 00:59:24.900
for climate change awareness when we talk about what is a risk, right? Obviously,


00:59:24.900 --> 00:59:31.300
there's a huge risk for the entire world, right? Yeah. Yeah. I guess it also seems like maybe it


00:59:31.300 --> 00:59:36.500
was too difficult to implement in terms of how do we police that? Would this then imply, I don't


00:59:36.500 --> 00:59:40.340
I don't know, would AWS have to report to the EU


00:59:40.340 --> 00:59:43.740
about who's using what compute?


00:59:43.740 --> 00:59:45.620
And, or I don't know,


00:59:45.620 --> 00:59:48.580
report if the compute exceeds a certain limit


00:59:48.580 --> 00:59:50.260
so that then you can be audited.


00:59:50.260 --> 00:59:52.660
Like these could all be potential implications,


00:59:52.660 --> 00:59:57.260
which again, then tie into other privacy concerns


00:59:57.260 --> 01:00:01.900
because yeah, I wouldn't necessarily want AWS


01:00:01.900 --> 01:00:03.980
to snoop around my compute,


01:00:04.860 --> 01:00:07.260
Maybe they have to, if the EU needs it.


01:00:07.260 --> 01:00:14.620
Like, wait a minute, we just had to reveal that this company did $2 million worth of GPU training,


01:00:14.620 --> 01:00:16.380
and we thought they were just a little small company.


01:00:16.380 --> 01:00:17.300
What's going on, right?


01:00:17.300 --> 01:00:19.900
Like some, something like that could come out.


01:00:19.900 --> 01:00:29.980
But you know, I don't know, something I had in mind is maybe if you create ML models for European citizens,


01:00:29.980 --> 01:00:34.420
those models must be trained with renewable energy or something to that effect, right?


01:00:34.420 --> 01:00:38.540
that you don't have to report it, but that has to be the case.


01:00:38.540 --> 01:00:38.860
I don't know.


01:00:38.860 --> 01:00:39.260
Yeah.


01:00:39.260 --> 01:00:40.140
But I mean, I don't know.


01:00:40.140 --> 01:00:40.740
It's interesting.


01:00:40.740 --> 01:00:44.580
It's an interesting question because the thing is, if you had like, you know,


01:00:44.580 --> 01:00:47.820
too many restrictions around this, this would encourage people to like, I don't


01:00:47.820 --> 01:00:52.420
know, train less, which then in turn is quite bad, I think actually what's quite


01:00:52.420 --> 01:00:55.900
important is that like, if you are developing these systems, you should


01:00:55.900 --> 01:00:59.980
train, you know, you should care about like what you're training and you


01:00:59.980 --> 01:01:03.620
shouldn't like, you know, constantly train these large language models for no


01:01:03.620 --> 01:01:07.880
reason just so you can say, Oh, look at my, my model that's bigger than yours.


01:01:07.880 --> 01:01:12.620
But it is on a smaller scale, it's very important to keep training your


01:01:12.620 --> 01:01:17.180
model, to keep collecting data and to keep improving it and to also train


01:01:17.180 --> 01:01:21.540
models that are really specific for your problems and not just find something


01:01:21.540 --> 01:01:24.380
that you download off the internet or that, you know, someone gives you by


01:01:24.380 --> 01:01:29.100
an API that kind of, sort of does what you do and then you're like, ah, that's


01:01:29.100 --> 01:01:32.540
good enough because that's how you end up with a lot more problems, like being


01:01:32.540 --> 01:01:34.580
- Right. - To like creating data


01:01:34.580 --> 01:01:36.340
and being able to train a system


01:01:36.340 --> 01:01:38.140
for your really, really specific use case.


01:01:38.140 --> 01:01:40.860
That's an advantage that's not like, you know,


01:01:40.860 --> 01:01:43.980
a disadvantage that you're trying to avoid.


01:01:43.980 --> 01:01:45.740
- That's a really good point.


01:01:45.740 --> 01:01:46.580
- Yeah.


01:01:46.580 --> 01:01:48.700
- It could be absolutely in contrast


01:01:48.700 --> 01:01:50.460
with some of the other things.


01:01:50.460 --> 01:01:51.980
Like it has to be fair,


01:01:51.980 --> 01:01:55.600
but if it uses too much training,


01:01:55.600 --> 01:01:57.700
that's gonna go to the other one.


01:01:57.700 --> 01:01:58.900
So let's do less trainings.


01:01:58.900 --> 01:02:01.500
It's kind of close enough to be unfair, right?


01:02:01.500 --> 01:02:04.060
Yeah, and then that encourages people to use, I don't know,


01:02:04.060 --> 01:02:08.580
just like some arbitrary API that they can find,


01:02:08.580 --> 01:02:13.420
which again is also, you know, not great or like,


01:02:13.420 --> 01:02:15.540
you know, I don't know, I think the bigger takeaway


01:02:15.540 --> 01:02:17.460
or the very important takeaway


01:02:17.460 --> 01:02:19.460
from these really large language models,


01:02:19.460 --> 01:02:21.380
in my opinion, it's not necessarily that like,


01:02:21.380 --> 01:02:23.220
whoa, if we just make it bigger and bigger,


01:02:23.220 --> 01:02:25.580
we can get a system that then is pretty good


01:02:25.580 --> 01:02:27.660
at pretty much everything considering


01:02:27.660 --> 01:02:30.020
it's never learned anything about any of these things.


01:02:30.020 --> 01:02:32.700
I think the takeaway, no, and I think that many people


01:02:32.700 --> 01:02:33.820
are still seeing it this way.


01:02:33.820 --> 01:02:36.500
And I think the more reasonable takeaway is,


01:02:36.500 --> 01:02:39.960
if a model that was just trained on like tons of texts


01:02:39.960 --> 01:02:42.800
can do pretty good things with stuff


01:02:42.800 --> 01:02:44.140
it's never seen before,


01:02:44.140 --> 01:02:47.260
how well could a much smaller, more specific system do


01:02:47.260 --> 01:02:52.020
if we actually trained it on maybe a small subset


01:02:52.020 --> 01:02:54.740
of only what we wanna do, and that will be more efficient.


01:02:54.740 --> 01:02:57.540
And I think people, we should stop like hoping


01:02:57.540 --> 01:03:04.260
there'll be one model that can magically do, I don't know, your arbitrary accounting task and also


01:03:04.260 --> 01:03:10.980
decide whether Michael should get a mortgage or not. I think that's kind of a weird idea. It's


01:03:10.980 --> 01:03:15.140
like you want a specific system. That requires training. I think training is good.


01:03:15.140 --> 01:03:19.300
Yeah. Put a good word in with the mortgage AI for me, will you?


01:03:19.300 --> 01:03:24.820
I think you want to have a quick comment on this and maybe we should wrap it up after that.


01:03:25.540 --> 01:03:31.940
Yeah, I mean, I guess I was just going to reference the opportunity and risks of foundation models,


01:03:31.940 --> 01:03:38.580
which I think touches on some of these things. So it's this mega paper, and it's exactly,


01:03:38.580 --> 01:03:44.500
well, some of the sections are about exactly this problem of, like, why do we believe that we need


01:03:44.500 --> 01:03:50.580
to have these foundational models or these large, extremely large, even larger than the last largest


01:03:51.140 --> 01:03:56.180
models to do all of the things when it also has all these other implications,


01:03:56.180 --> 01:04:00.260
environmental factors being one of them, because obviously when you train one of these models,


01:04:00.260 --> 01:04:04.740
it's like driving your car around for like 10 years or something like this. So, you know,


01:04:04.740 --> 01:04:10.660
there's big implications and I think the point of can you build a smaller targeted model to do the


01:04:10.660 --> 01:04:15.060
same thing and then the other point of if we need these big models, what are their ways for us to


01:04:15.060 --> 01:04:22.500
hook in and do small bits of training rather than to retrain from the start from the very beginning.


01:04:22.500 --> 01:04:30.980
I mean, these are like the hard problems that I think need solving, maybe not always


01:04:30.980 --> 01:04:35.460
building a better recommendation machine. So yeah, if you're looking for a problem,


01:04:35.460 --> 01:04:41.300
solve some of these problems. Yeah, fantastic. This is a big article. The PDF is published.


01:04:41.300 --> 01:04:44.500
people can check it out. We'll link to it in the show notes.


01:04:44.500 --> 01:04:48.580
Yeah, also actually on the foundation, sorry, no, I just wanted to say,


01:04:48.580 --> 01:04:51.540
sorry, I've been referring to these as language models. I've been trying to


01:04:51.540 --> 01:04:55.140
train myself to use the more explicit term,


01:04:55.140 --> 01:04:58.180
because I think foundation models are a much better way


01:04:58.180 --> 01:05:01.060
to express this. And I'm so happy this term was introduced, because it finally


01:05:01.060 --> 01:05:03.060
solves a lot of these problems of everything


01:05:03.060 --> 01:05:06.500
being a model that I think causes a lot of confusion


01:05:06.500 --> 01:05:10.580
when talking about machine learning. Yeah, excellent.


01:05:10.580 --> 01:05:14.020
whoops, moving around.


01:05:14.020 --> 01:05:16.940
Haley in the audience asked, what does climate change


01:05:16.940 --> 01:05:17.980
have to do with this?


01:05:17.980 --> 01:05:19.480
The reason I brought it up, one, is


01:05:19.480 --> 01:05:21.380
because Europe seems to be leading at least


01:05:21.380 --> 01:05:25.980
in the consensus side of things trying


01:05:25.980 --> 01:05:27.180
to address climate change.


01:05:27.180 --> 01:05:29.500
I feel like there's a lot of citizens there


01:05:29.500 --> 01:05:32.420
where it's on their mind, and they


01:05:32.420 --> 01:05:35.140
want the government to do something about it and stuff.


01:05:35.140 --> 01:05:36.580
The governments do a lot there.


01:05:36.580 --> 01:05:40.100
So as a law, I thought maybe it would touch on that.


01:05:40.100 --> 01:05:42.780
because Catherine, you pointed out some crazy number.


01:05:42.780 --> 01:05:45.620
You want to like just reemphasize some of the cost


01:05:45.620 --> 01:05:46.460
of some of these changes.


01:05:46.460 --> 01:05:48.180
It's not just like, oh, well,


01:05:48.180 --> 01:05:49.700
it's like leaving a few lights on.


01:05:49.700 --> 01:05:50.540
It's a lot.


01:05:50.540 --> 01:05:52.860
- No, it's just huge.


01:05:52.860 --> 01:05:54.540
Yeah, and they keep getting bigger.


01:05:54.540 --> 01:05:56.820
I forget who released the newest one.


01:05:56.820 --> 01:05:58.660
I don't know if you remember Ines,


01:05:58.660 --> 01:06:01.260
but it's like, they keep getting bigger and bigger.


01:06:01.260 --> 01:06:03.620
So some of these have like billions and billions


01:06:03.620 --> 01:06:05.140
and billions of parameters.


01:06:05.140 --> 01:06:09.580
They sometimes have extremely large amounts of data,


01:06:09.580 --> 01:06:12.780
either as external reference or in the model itself.


01:06:12.780 --> 01:06:17.820
And yeah, Timnit Gebru's paper that essentially she


01:06:17.820 --> 01:06:22.940
was basically fired from Google for researching was around--


01:06:22.940 --> 01:06:24.820
or one of the parts of the paper was


01:06:24.820 --> 01:06:27.700
around how much carbon emissions come


01:06:27.700 --> 01:06:28.940
from training these models.


01:06:28.940 --> 01:06:32.220
They've only gotten bigger since that paper.


01:06:32.220 --> 01:06:35.380
And yeah, I may have the statistic wrong,


01:06:35.380 --> 01:06:40.380
but it is almost as bad as driving a car around,


01:06:40.380 --> 01:06:43.300
with the motor on every day,


01:06:43.300 --> 01:06:46.060
with your normal commute for like 10 plus years


01:06:46.060 --> 01:06:48.020
to just train one model.


01:06:48.020 --> 01:06:50.940
And it's really absurd because some of these models


01:06:50.940 --> 01:06:53.620
are just training to prove that we can train them.


01:06:53.620 --> 01:06:55.540
And so it's like, what?


01:06:55.540 --> 01:06:56.820
- Yeah, and often it's not,


01:06:56.820 --> 01:06:58.860
the artifact isn't even as useful where it's like,


01:06:58.860 --> 01:07:02.100
okay, with a lot of the bird models, we can at least,


01:07:02.100 --> 01:07:04.160
I think it's good that we just reuse these weights.


01:07:04.160 --> 01:07:10.000
I think often in practice that's what's done. You take some of these weights that someone else has


01:07:10.000 --> 01:07:13.520
trained or use these embeddings and then you train something else on top of that.


01:07:13.520 --> 01:07:21.520
Yeah or even just like you use these embeddings to initialize your model and then you train


01:07:21.520 --> 01:07:28.160
different components using these embeddings and that is efficient. But it also means that


01:07:28.160 --> 01:07:31.960
"Okay, we're kind of stuck with a lot of these artifacts


01:07:31.960 --> 01:07:35.560
"that are getting stale all the time."


01:07:35.560 --> 01:07:38.280
- Yeah, yeah, so the comment in the audience was,


01:07:38.280 --> 01:07:40.560
"I could train one on my laptop and use electricity."


01:07:40.560 --> 01:07:44.240
Like, true, but it's like 50,000 laptops,


01:07:44.240 --> 01:07:46.120
or I mean, it's a much--


01:07:46.120 --> 01:07:47.400
- I mean, it's a lot more, exactly.


01:07:47.400 --> 01:07:48.960
No, and I think training on a laptop is great.


01:07:48.960 --> 01:07:50.680
Like, for example, we recently did some work


01:07:50.680 --> 01:07:54.520
to be able to hook into the Accelerate library


01:07:54.520 --> 01:07:57.640
on the new M1 MacBooks,


01:07:57.640 --> 01:08:00.480
which make things a lot faster in spacey.


01:08:00.480 --> 01:08:01.960
And that was quite cool to see.


01:08:01.960 --> 01:08:03.560
And we want to do a bit more there because like,


01:08:03.560 --> 01:08:05.240
oh, you can really, you know,


01:08:05.240 --> 01:08:06.560
if we optimize this further,


01:08:06.560 --> 01:08:09.720
you can actually train a model on your MacBook.


01:08:09.720 --> 01:08:12.240
And this can be really accurate


01:08:12.240 --> 01:08:15.120
and you don't necessarily need like all this computer power.


01:08:15.120 --> 01:08:18.880
And yep, training on laptop is good.


01:08:18.880 --> 01:08:19.800
- It is, and you can do it.


01:08:19.800 --> 01:08:22.160
But a lot of the ones that we're actually talking about


01:08:22.160 --> 01:08:25.120
use these huge, huge modules that take a lot.


01:08:25.120 --> 01:08:28.280
So you can say you don't really care about climate change


01:08:28.280 --> 01:08:32.720
or whatever, but if you do, the ML training side


01:08:32.720 --> 01:08:34.240
has a pretty significant impact.


01:08:34.240 --> 01:08:37.040
And I was unsure whether or not to see it,


01:08:37.040 --> 01:08:40.400
but I guess it makes sense that it's not there.


01:08:40.400 --> 01:08:40.960
Who knows?


01:08:40.960 --> 01:08:45.400
It's also-- they said this is a foundation for potentially


01:08:45.400 --> 01:08:48.720
future AI laws in Europe.


01:08:48.720 --> 01:08:52.000
Yeah, and I also appreciate that, OK, they didn't want to tie


01:08:52.000 --> 01:08:56.160
everything together. Like I can't even I think from a political perspective, if you are proposing


01:08:56.160 --> 01:09:04.720
this pretty bold framework for regulation, tying it into too many other topics can easily, I don't


01:09:04.720 --> 01:09:08.960
know, distract from like the core point that they want to make. So I think it might have actually


01:09:08.960 --> 01:09:14.960
been like a, you know, strategic decision. - Yep, absolutely. All right, ladies, this has


01:09:14.960 --> 01:09:21.280
been a fantastic conversation. I've learned a lot and really enjoyed having you here. Now,


01:09:21.280 --> 01:09:22.180
Now, before we get out of here,


01:09:22.180 --> 01:09:24.200
maybe since there's two of you and we're sort of over time,


01:09:24.200 --> 01:09:27.080
I'll just ask you one question instead of the two.


01:09:27.080 --> 01:09:28.700
So if you're gonna write some Python code,


01:09:28.700 --> 01:09:30.060
what editor are you using these days?


01:09:30.060 --> 01:09:31.260
Catherine, you go first.


01:09:31.260 --> 01:09:35.820
- I am still in Vim.


01:09:35.820 --> 01:09:36.900
Am I old?


01:09:36.900 --> 01:09:37.820
I think I'm old.


01:09:37.820 --> 01:09:39.740
(laughing)


01:09:39.740 --> 01:09:41.400
- Oh, you're classic, come on.


01:09:41.400 --> 01:09:43.780
(laughing)


01:09:43.780 --> 01:09:44.800
- Classic model.


01:09:44.800 --> 01:09:47.040
(laughing)


01:09:48.340 --> 01:09:51.340
- No, I'm quite boring, Visual Studio Code.


01:09:51.340 --> 01:09:52.980
I've been using that for years.


01:09:52.980 --> 01:09:54.880
It's pretty nice.


01:09:54.880 --> 01:09:56.900
I think it's probably the most common answer you get


01:09:56.900 --> 01:09:58.580
and it's quite, yeah.


01:09:58.580 --> 01:09:59.420
- It certainly lies--


01:09:59.420 --> 01:10:01.620
- I mean, actually using Vim is a lot edgier and cooler.


01:10:01.620 --> 01:10:04.260
I wish, you know, even for, maybe for that reason alone,


01:10:04.260 --> 01:10:06.540
I should just like, you know, switch to that.


01:10:06.540 --> 01:10:08.140
- She edits code without even a window.


01:10:08.140 --> 01:10:10.860
It just appears on this black surface.


01:10:10.860 --> 01:10:14.220
- (laughs) I mean, my co-founder, he programs that way


01:10:14.220 --> 01:10:17.060
and I'm like, okay, if it makes you happy.


01:10:17.060 --> 01:10:17.900
- Yeah, yeah, yeah, you do you.


01:10:17.900 --> 01:10:20.100
(laughing)


01:10:20.100 --> 01:10:22.540
- Some people just like to suffer, that's okay.


01:10:22.540 --> 01:10:23.380
- Oh, damn.


01:10:23.380 --> 01:10:25.620
(laughing)


01:10:25.620 --> 01:10:26.460
- All right. - No, sorry.


01:10:26.460 --> 01:10:28.260
No offense, like, I don't know,


01:10:28.260 --> 01:10:29.860
this was a joke, no offense to anyone


01:10:29.860 --> 01:10:30.740
who's programming in Vim.


01:10:30.740 --> 01:10:33.020
I know lots of great people who do that.


01:10:33.020 --> 01:10:34.540
I don't wanna get any hate messages.


01:10:34.540 --> 01:10:36.780
(laughing)


01:10:36.780 --> 01:10:41.100
- All right, all right, well, Catherine Ines,


01:10:41.100 --> 01:10:42.740
thanks for coming back on the show


01:10:42.740 --> 01:10:44.060
and sharing your thoughts here.


01:10:44.060 --> 01:10:45.860
- Yeah, thanks for having me.


01:10:45.860 --> 01:10:48.200
Thanks Rose!

