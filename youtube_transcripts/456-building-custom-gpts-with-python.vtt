WEBVTT

00:00:00.000 --> 00:00:04.600
- Ian, welcome to Talk Python to Me.

00:00:04.600 --> 00:00:06.400
- Hey, Michael, good to see you again.

00:00:06.400 --> 00:00:08.700
- Yeah, great to see you again.

00:00:08.700 --> 00:00:10.320
It has been a little while.

00:00:10.320 --> 00:00:11.960
It seems like not so long ago,

00:00:11.960 --> 00:00:16.160
and yet when I pull up the episode that we did together,

00:00:16.160 --> 00:00:21.040
sure enough, it says March 7th, 2018.

00:00:21.040 --> 00:00:22.400
Wow.

00:00:22.400 --> 00:00:23.560
- Years are short.

00:00:23.560 --> 00:00:24.400
The years are short.

00:00:24.400 --> 00:00:25.920
They go by really fast.

00:00:25.920 --> 00:00:27.160
- They sure do.

00:00:27.160 --> 00:00:30.560
So back then, we were talking about Python

00:00:30.560 --> 00:00:32.520
and biology and genomics,

00:00:32.520 --> 00:00:36.340
and it sounds like you're still doing genetic-type things

00:00:36.340 --> 00:00:40.200
and still doing Python and all that kind of stuff.

00:00:40.200 --> 00:00:41.760
- For sure, yeah, definitely.

00:00:41.760 --> 00:00:43.720
I work for a company called Genome Oncology.

00:00:43.720 --> 00:00:46.520
We do precision oncology software,

00:00:46.520 --> 00:00:49.120
helping folks make sense of genomics

00:00:49.120 --> 00:00:50.960
and trying to help cancer patients.

00:00:50.960 --> 00:00:51.800
- That's awesome.

00:00:51.800 --> 00:00:55.920
There's different levels of helping people with software.

00:00:56.840 --> 00:01:01.840
On one level, we probably have ad retargeting.

00:01:01.840 --> 00:01:06.760
On the other, we've got medical benefits

00:01:06.760 --> 00:01:09.840
and looking for helping people

00:01:09.840 --> 00:01:12.240
who are suffering socially or whatever.

00:01:12.240 --> 00:01:14.600
So it's gotta feel good to write software

00:01:14.600 --> 00:01:17.560
that is making a difference in people's lives.

00:01:17.560 --> 00:01:18.400
- That's right.

00:01:18.400 --> 00:01:21.000
I did spend a lot of the 2000s making e-commerce websites,

00:01:21.000 --> 00:01:23.120
and that wasn't exactly the most fulfilling thing.

00:01:23.120 --> 00:01:25.440
I learned a lot, but it wasn't as exciting

00:01:25.440 --> 00:01:26.280
as what I'm doing now,

00:01:26.280 --> 00:01:29.080
or at least as fulfilling as what I'm doing now.

00:01:29.080 --> 00:01:30.280
- Nice.

00:01:30.280 --> 00:01:33.080
Were those earlier websites in Python?

00:01:33.080 --> 00:01:33.920
- No.

00:01:33.920 --> 00:01:37.040
So yeah, I mean, that was all Java for the most part.

00:01:37.040 --> 00:01:39.360
And finally, with this company,

00:01:39.360 --> 00:01:43.520
I knocked out a prototype in Django a few years ago,

00:01:43.520 --> 00:01:47.080
and my boss at the time was like, "You did that so fast.

00:01:47.080 --> 00:01:48.720
"You should do some more stuff in Python."

00:01:48.720 --> 00:01:51.640
So that's kinda how it evolved.

00:01:51.640 --> 00:01:55.200
And now, basically, most of our core back end is Python,

00:01:55.200 --> 00:01:58.680
and we use a lot of Svelte for the user interfaces.

00:01:58.680 --> 00:02:00.040
- Okay, beautiful.

00:02:00.040 --> 00:02:01.360
It's easy to forget.

00:02:01.360 --> 00:02:06.880
Like five years ago, 10 years ago,

00:02:06.880 --> 00:02:09.200
people were questioning whether Python

00:02:09.200 --> 00:02:10.280
is something you should use.

00:02:10.280 --> 00:02:11.240
Is it a real language?

00:02:11.240 --> 00:02:12.080
Do you really use it?

00:02:12.080 --> 00:02:13.400
Is it safe to use?

00:02:13.400 --> 00:02:17.000
Maybe you should use a Java or a C# or something like that,

00:02:17.000 --> 00:02:19.760
because this is a real project.

00:02:19.760 --> 00:02:20.600
It's interesting.

00:02:20.600 --> 00:02:22.680
I don't hear that nearly as much anymore, do you?

00:02:22.680 --> 00:02:23.520
- No, no.

00:02:23.520 --> 00:02:25.160
I grew up a Boston sports fan,

00:02:25.160 --> 00:02:27.520
and it was like being a Boston sports fan was terrible

00:02:27.520 --> 00:02:28.360
for the longest time.

00:02:28.360 --> 00:02:29.200
And now it's like, okay,

00:02:29.200 --> 00:02:32.240
we don't wanna hear about your problems right now.

00:02:32.240 --> 00:02:33.160
And same thing with Python.

00:02:33.160 --> 00:02:34.440
It's like, I like Python.

00:02:34.440 --> 00:02:35.320
It's like, yeah, great.

00:02:35.320 --> 00:02:36.680
So does everybody else in the world.

00:02:36.680 --> 00:02:39.120
So yeah, it's really not the issue anymore.

00:02:39.120 --> 00:02:40.560
Now it's not the cool thing to play with,

00:02:40.560 --> 00:02:43.360
so now you gotta go to Rust or something else.

00:02:43.360 --> 00:02:44.800
- You know what's shiny?

00:02:44.800 --> 00:02:46.320
LLMs are shiny.

00:02:46.320 --> 00:02:48.040
- LLMs are very shiny, for sure.

00:02:48.040 --> 00:02:50.120
- Yeah, we can talk about them today.

00:02:50.120 --> 00:02:51.240
- Yeah, that sounds great.

00:02:51.240 --> 00:02:52.080
Let's do it.

00:02:52.080 --> 00:02:53.080
- All right, before we do that, though,

00:02:53.080 --> 00:02:55.920
just, you know, it has been a good five years

00:02:55.920 --> 00:02:56.760
since you've been on the show.

00:02:56.760 --> 00:02:57.600
It's helped people.

00:02:57.600 --> 00:03:00.640
I guess you already introduced yourself.

00:03:00.640 --> 00:03:02.280
- Yeah, okay.

00:03:02.280 --> 00:03:05.320
- So let's jump in.

00:03:05.320 --> 00:03:09.520
So first of all, we're gonna talk about

00:03:09.520 --> 00:03:12.600
building applications that are basically

00:03:12.600 --> 00:03:16.520
powered by LLMs that you plug into, right?

00:03:16.520 --> 00:03:17.360
- Yep, yep.

00:03:17.360 --> 00:03:20.520
- Before we get into creating LLMs,

00:03:20.520 --> 00:03:25.520
just for you, where do LLMs play a role for you

00:03:25.520 --> 00:03:28.960
in software development these days?

00:03:28.960 --> 00:03:31.320
- Sure, so, you know, like everybody else,

00:03:31.320 --> 00:03:33.360
I had been playing with,

00:03:33.360 --> 00:03:35.560
so I do natural language processing as part of my job,

00:03:35.560 --> 00:03:37.800
right, so using spaCy was a big part

00:03:37.800 --> 00:03:41.040
of the information extraction stack that we use,

00:03:41.040 --> 00:03:42.840
'cause we have to deal with a lot of medical data,

00:03:42.840 --> 00:03:44.560
and medical data's just unstructured

00:03:44.560 --> 00:03:48.040
and has to be cleaned up before it can be used.

00:03:48.040 --> 00:03:49.480
So that was my exposure.

00:03:49.480 --> 00:03:53.880
I had seen GPTs and the idea of generating text,

00:03:53.880 --> 00:03:54.880
just starting from that,

00:03:54.880 --> 00:03:56.880
didn't really make much sense to me at the time.

00:03:56.880 --> 00:03:58.200
But then obviously, like everybody else,

00:03:58.200 --> 00:04:00.880
when ChatGPT came out, I was like, oh, I get this now.

00:04:00.880 --> 00:04:05.520
This thing does, it can basically learn in the context,

00:04:05.520 --> 00:04:07.360
and it can actually produce something that's interesting,

00:04:07.360 --> 00:04:09.960
and you can use it for things like information extraction.

00:04:09.960 --> 00:04:13.160
So just like everybody else, I kind of woke up to 'em

00:04:13.160 --> 00:04:15.600
around that time that they got released,

00:04:15.600 --> 00:04:17.080
and I use 'em all the time, right?

00:04:17.080 --> 00:04:19.520
So ChatGPT 4 is really what I use.

00:04:19.520 --> 00:04:22.040
I would recommend, if you can afford the $20 a month,

00:04:22.040 --> 00:04:26.040
it's still the best model that there is as of January 2024,

00:04:26.040 --> 00:04:27.160
and I use that for coding.

00:04:27.160 --> 00:04:30.680
I don't really like the coding tools, the copilots,

00:04:30.680 --> 00:04:34.000
but there's definitely folks that swear by them.

00:04:34.000 --> 00:04:37.320
My workflow is more of I have a problem,

00:04:37.320 --> 00:04:39.680
work with the chatbot to try to think

00:04:39.680 --> 00:04:40.720
through all the edge cases,

00:04:40.720 --> 00:04:44.280
and then think through the test case, the tests,

00:04:44.280 --> 00:04:45.720
and then I think through the code, right?

00:04:45.720 --> 00:04:47.880
And then the actual typing of the code, yeah,

00:04:47.880 --> 00:04:49.680
I'll have it do a lot of the boilerplate stuff,

00:04:49.680 --> 00:04:52.760
but then kind of shaping the APIs and things like that.

00:04:52.760 --> 00:04:54.280
I kind of like to do that myself still.

00:04:54.280 --> 00:04:55.640
I'm kind of old school. - Yeah.

00:04:55.640 --> 00:04:57.120
- Old school.

00:04:57.120 --> 00:04:58.360
- I guess I'm old school as well,

00:04:58.360 --> 00:05:00.200
'cause I'm like right there with you.

00:05:00.200 --> 00:05:01.200
- Yeah, yeah.

00:05:01.200 --> 00:05:03.880
- For me, I don't generally run copilot

00:05:03.880 --> 00:05:06.620
or those kinds of things in my editors.

00:05:06.620 --> 00:05:10.480
I do have some features turned on,

00:05:10.480 --> 00:05:13.680
but primarily it's just really nice autocomplete.

00:05:13.680 --> 00:05:14.520
You know what I mean?

00:05:14.520 --> 00:05:17.000
It seems like it almost just knows

00:05:17.000 --> 00:05:19.640
what I want to type anyway, and that's getting better.

00:05:19.640 --> 00:05:22.240
I don't know if anyone's noticed recently,

00:05:22.240 --> 00:05:24.080
one of the recent releases of PyCharm,

00:05:24.080 --> 00:05:27.600
it starts to autocomplete whole lines,

00:05:27.600 --> 00:05:28.840
and I don't know where it's getting this from,

00:05:28.840 --> 00:05:31.480
and I think I have the AI features turned off.

00:05:31.480 --> 00:05:33.400
At least it says I have no license.

00:05:33.400 --> 00:05:35.080
Guessing that means they're turned off.

00:05:35.080 --> 00:05:38.120
So it must be something more built into it,

00:05:38.120 --> 00:05:40.560
and that's pretty excellent.

00:05:40.560 --> 00:05:42.920
But for me, I find I'm pretty content

00:05:42.920 --> 00:05:44.720
to just sit and write code.

00:05:44.720 --> 00:05:49.720
However, the more specific the unknowns are,

00:05:49.720 --> 00:05:51.640
the more willing I'm like,

00:05:51.640 --> 00:05:53.840
oh, I need to go to ChatGPT for this.

00:05:53.840 --> 00:05:57.000
Like for example, how do you use Pydantic?

00:05:57.000 --> 00:06:00.620
Well, I'll probably just go look at a quick code sample

00:06:00.620 --> 00:06:02.480
and see that so I can understand it.

00:06:02.480 --> 00:06:07.240
But if I have this time string

00:06:07.240 --> 00:06:10.600
with the date like this, the month like this,

00:06:10.600 --> 00:06:12.520
and then it has the time zone like that,

00:06:12.520 --> 00:06:13.640
how do I parse that?

00:06:13.640 --> 00:06:16.680
Or how do I generate another one like that in Python?

00:06:16.680 --> 00:06:18.080
And here's the answer.

00:06:18.080 --> 00:06:20.360
Or I have this giant weird string,

00:06:20.360 --> 00:06:23.240
and I want this part of it as extracted

00:06:23.240 --> 00:06:24.720
with a regular expression.

00:06:24.720 --> 00:06:26.440
- Right, regular expression, I was just gonna say that.

00:06:26.440 --> 00:06:27.520
- Oh my gosh.

00:06:27.520 --> 00:06:28.840
- You don't have to write another one of those.

00:06:28.840 --> 00:06:29.800
Yeah, it's great.

00:06:29.800 --> 00:06:30.840
- Yeah, it's pretty much like,

00:06:30.840 --> 00:06:32.400
do you need it to detect the end of a line

00:06:32.400 --> 00:06:33.680
straight to ChatGPT?

00:06:33.680 --> 00:06:36.120
Not really, but it's like almost any level

00:06:36.120 --> 00:06:37.800
of regular expression.

00:06:37.800 --> 00:06:39.400
I'm like, I need some AI for this,

00:06:39.400 --> 00:06:42.320
'cause this is not time well spent for me.

00:06:42.320 --> 00:06:44.360
But yeah, it's great.

00:06:44.360 --> 00:06:45.920
- Yeah, one big tip I would give people though

00:06:45.920 --> 00:06:48.400
is that these chatbots, they wanna please you.

00:06:48.400 --> 00:06:51.400
So you have to ask it to criticize you.

00:06:51.400 --> 00:06:53.040
You have to say, here's some piece of code,

00:06:53.040 --> 00:06:55.040
tell me all the ways it's wrong, right?

00:06:55.040 --> 00:06:59.280
And you have to also ask for lots of different examples,

00:06:59.280 --> 00:07:01.600
because it just starts to get more creative,

00:07:01.600 --> 00:07:02.640
more things that it says.

00:07:02.640 --> 00:07:04.400
It really thinks by talking,

00:07:04.400 --> 00:07:06.680
which is a really weird thing to consider.

00:07:06.680 --> 00:07:08.560
It's definitely some things to keep in mind

00:07:08.560 --> 00:07:09.400
when you're working with these things.

00:07:09.560 --> 00:07:12.880
- And they do have these really weird things.

00:07:12.880 --> 00:07:15.720
Like if you compliment them, or if you ask it,

00:07:15.720 --> 00:07:18.480
you sort of tell it, like, I really want you to tell me.

00:07:18.480 --> 00:07:20.000
It actually makes a difference, right?

00:07:20.000 --> 00:07:21.320
It's not just like a search engine.

00:07:21.320 --> 00:07:22.520
Like, well, of course, what does it care?

00:07:22.520 --> 00:07:24.320
Just put these keywords in and they come out.

00:07:24.320 --> 00:07:26.440
Like, no, you've kind of got to like,

00:07:26.440 --> 00:07:27.600
you gotta talk to it a little bit.

00:07:27.600 --> 00:07:29.280
- I've seen people threatening them,

00:07:29.280 --> 00:07:32.200
or like saying that someone's being held ransom,

00:07:32.200 --> 00:07:35.240
or, you know, I like to say, my boss is really mad at me.

00:07:35.240 --> 00:07:36.400
Like, help me out here, right?

00:07:36.400 --> 00:07:38.600
And like, see if it'll generate some better code.

00:07:39.560 --> 00:07:41.520
- Look, you're not being a good user.

00:07:41.520 --> 00:07:45.320
You're trying to trick me.

00:07:45.320 --> 00:07:47.720
I've been a good chatbot, and you've been a bad user,

00:07:47.720 --> 00:07:49.840
and I'm not gonna help you anymore.

00:07:49.840 --> 00:07:52.520
That was actually basically a conversation

00:07:52.520 --> 00:07:54.200
from being in the early days.

00:07:54.200 --> 00:07:56.560
- Yeah, the Sydney episode, yeah, that was crazy, right?

00:07:56.560 --> 00:07:57.400
Super funny.

00:07:57.400 --> 00:07:59.240
- How funny?

00:07:59.240 --> 00:08:00.880
All right, well, I'm sure a lot of people out there

00:08:00.880 --> 00:08:02.760
are using AI these days.

00:08:02.760 --> 00:08:05.600
I think I saw a quote from, I think it was from GitHub,

00:08:05.600 --> 00:08:09.440
saying that over 50% of developers are using Copilot,

00:08:09.440 --> 00:08:14.280
which, crazy, but I mean, not that surprising.

00:08:14.280 --> 00:08:16.240
50% of the people are using autocomplete,

00:08:16.240 --> 00:08:19.400
so I guess it's kind of like that, right?

00:08:19.400 --> 00:08:21.200
- They're great tools, they're gonna keep evolving.

00:08:21.200 --> 00:08:22.440
There's some other ones I'm keeping an eye on.

00:08:22.440 --> 00:08:24.040
There's one called Console,

00:08:24.040 --> 00:08:25.360
which just takes a different approach.

00:08:25.360 --> 00:08:27.040
They use some stronger models.

00:08:27.040 --> 00:08:29.640
And then there's a website called Find, P-H-I-N-D,

00:08:29.640 --> 00:08:31.640
that allows you to do some searching,

00:08:31.640 --> 00:08:33.400
that they've built their own custom model.

00:08:33.400 --> 00:08:34.480
Really interesting companies

00:08:34.480 --> 00:08:37.120
that are doing some really cool things.

00:08:37.120 --> 00:08:40.120
And then Perplexity is like the search replacement

00:08:40.120 --> 00:08:42.160
that a lot of folks are very excited about using

00:08:42.160 --> 00:08:43.000
instead of Google.

00:08:43.000 --> 00:08:45.200
So there's a lot of different tools out there.

00:08:45.200 --> 00:08:47.440
You could spend all your day just kind of playing around

00:08:47.440 --> 00:08:48.360
and learning these things,

00:08:48.360 --> 00:08:50.480
but you gotta actually kind of get some stuff done, too.

00:08:50.480 --> 00:08:52.160
- Yeah, you gotta pick something and go, right?

00:08:52.160 --> 00:08:56.680
Because with all the churn and growth

00:08:56.680 --> 00:08:57.680
and experimentation we got,

00:08:57.680 --> 00:08:59.240
you probably could try a new tool every day

00:08:59.240 --> 00:09:01.880
and still not try them all, you know?

00:09:01.880 --> 00:09:02.720
- Exactly, right.

00:09:02.720 --> 00:09:03.540
- Just keep falling farther behind,

00:09:03.540 --> 00:09:05.400
but you gotta pick something and go.

00:09:05.400 --> 00:09:06.240
- And go, yep.

00:09:06.240 --> 00:09:08.720
- Let's talk about writing some code.

00:09:08.720 --> 00:09:11.800
- Yeah, the next thing you're gonna do

00:09:11.800 --> 00:09:16.480
after you use a chatbot is to hit an API.

00:09:16.480 --> 00:09:18.160
Like if you're gonna program an app

00:09:18.160 --> 00:09:20.160
and that app is gonna have LLM inside of it,

00:09:20.160 --> 00:09:22.080
large language models inside of it,

00:09:22.080 --> 00:09:24.540
APIs are pretty much the next step, right?

00:09:24.540 --> 00:09:28.160
So OpenAI has different models that are available.

00:09:28.160 --> 00:09:30.240
This is a webpage that I just saw recently

00:09:30.240 --> 00:09:32.340
that'll actually compare the different models

00:09:32.340 --> 00:09:33.180
that are out there.

00:09:33.180 --> 00:09:35.400
And there's obviously the big guy, which is OpenAI,

00:09:35.400 --> 00:09:37.640
and you can get that through Azure as well

00:09:37.640 --> 00:09:39.200
if you have a Microsoft arrangement.

00:09:39.200 --> 00:09:42.280
And there's some security reasons or HIPAA compliance

00:09:42.280 --> 00:09:44.440
and some other reasons that you might want

00:09:44.440 --> 00:09:47.800
to talk through Azure instead of going directly to OpenAI.

00:09:47.800 --> 00:09:50.540
I defer to your IT department about that.

00:09:50.540 --> 00:09:54.780
Google has Gemini, which they just released the pro version,

00:09:54.780 --> 00:09:58.780
which I believe is as strong as 3.5, roughly.

00:09:58.780 --> 00:10:02.000
That is interesting because if you don't care

00:10:02.000 --> 00:10:03.480
about them training on your data,

00:10:03.480 --> 00:10:05.960
if like whatever you're doing is just like

00:10:05.960 --> 00:10:07.440
not super proprietary or something

00:10:07.440 --> 00:10:09.400
you're trying to keep secret,

00:10:09.400 --> 00:10:12.320
they're offering free API access

00:10:12.320 --> 00:10:14.680
at I believe 60 words per minute, right?

00:10:14.680 --> 00:10:17.100
So basically one a second, you can call this thing

00:10:17.100 --> 00:10:18.320
and there's no charge.

00:10:18.320 --> 00:10:20.800
So I don't know how long that's gonna last.

00:10:20.800 --> 00:10:21.920
So if you have an interesting project

00:10:21.920 --> 00:10:24.600
that you wanna use in a large language model for,

00:10:24.600 --> 00:10:26.280
you might wanna look at that.

00:10:26.280 --> 00:10:28.100
- Especially if it's already open data

00:10:28.100 --> 00:10:29.600
that you're playing with.

00:10:29.600 --> 00:10:30.440
- Exactly right.

00:10:30.440 --> 00:10:32.200
- Data you've somehow published to the web

00:10:32.200 --> 00:10:35.520
that just certainly been consumed by these things.

00:10:35.520 --> 00:10:37.800
- Yep, and these models are gonna train on it, right?

00:10:37.800 --> 00:10:38.720
That's the trade, right?

00:10:38.720 --> 00:10:40.640
They're trying to get more tokens,

00:10:40.640 --> 00:10:41.480
is what they call it, right?

00:10:41.480 --> 00:10:43.760
The tokens are what they need to actually

00:10:43.760 --> 00:10:44.840
make these models smarter.

00:10:44.840 --> 00:10:47.580
So everyone's just hunting for more tokens

00:10:47.580 --> 00:10:49.920
and I think this is part of their strategy for that.

00:10:49.920 --> 00:10:52.720
And then there's also a CLAUDE by Anthropic.

00:10:52.720 --> 00:10:54.280
And then after that, you get into the,

00:10:54.280 --> 00:10:56.640
kind of the open source APIs as well.

00:10:56.640 --> 00:11:00.000
- There's some really powerful open source ones out there.

00:11:00.000 --> 00:11:02.040
- Yep, and if you scroll down,

00:11:02.040 --> 00:11:02.880
yeah, scroll down here.

00:11:02.880 --> 00:11:04.840
- Yeah, this is DocSpot, for people listening,

00:11:04.840 --> 00:11:06.200
DocSpot.ai and it,

00:11:06.200 --> 00:11:10.200
is its sole purpose just to tell you

00:11:10.200 --> 00:11:11.640
price comparisons and stuff like that?

00:11:11.640 --> 00:11:12.480
Or does it have more?

00:11:12.480 --> 00:11:15.560
- Yeah, I assume this company has got some product,

00:11:15.560 --> 00:11:16.880
unfortunately I don't know what it is,

00:11:16.880 --> 00:11:19.720
but I saw this link that they put out there

00:11:19.720 --> 00:11:20.880
and it's a calculator.

00:11:20.880 --> 00:11:22.960
So you basically can put your, what tokens,

00:11:22.960 --> 00:11:24.520
how many tokens, there's input tokens

00:11:24.520 --> 00:11:25.720
and there's output tokens, right?

00:11:25.720 --> 00:11:28.520
So they're gonna charge more on the output tokens.

00:11:29.760 --> 00:11:31.520
That's for the most part, some of the libraries,

00:11:31.520 --> 00:11:33.680
some of the models are more equal.

00:11:33.680 --> 00:11:35.240
And then they, what they do is,

00:11:35.240 --> 00:11:37.320
if you can figure out like roughly how big a message

00:11:37.320 --> 00:11:39.600
is gonna be, both the input and the output,

00:11:39.600 --> 00:11:40.720
how many calls you're gonna make,

00:11:40.720 --> 00:11:44.560
you can use that to then calculate basically the cost.

00:11:44.560 --> 00:11:46.400
And the cost is always at like,

00:11:46.400 --> 00:11:47.560
tokens per thousand,

00:11:47.560 --> 00:11:50.120
or dollars or pennies really,

00:11:50.120 --> 00:11:51.800
pennies per thousand tokens.

00:11:51.800 --> 00:11:53.680
And then it's just a math equation at that point.

00:11:53.680 --> 00:11:55.120
And what you'll find is,

00:11:55.120 --> 00:11:57.800
calling GPT-4 is gonna be super expensive

00:11:57.800 --> 00:12:00.480
and then calling, a small seven,

00:12:00.480 --> 00:12:02.280
what's called a 7B model from Mistral

00:12:02.280 --> 00:12:03.560
is gonna be the cheapest.

00:12:03.560 --> 00:12:06.160
And you're just gonna look for these different providers.

00:12:06.160 --> 00:12:07.000
And there was really a--

00:12:07.000 --> 00:12:08.320
- Those really are different.

00:12:08.320 --> 00:12:12.580
Like for example, opening an Azure GPT-4 is,

00:12:12.580 --> 00:12:16.480
three, a little over 3 cents per call.

00:12:16.480 --> 00:12:21.480
Whereas GPT-5 Turbo is 1/10 of 1 cent.

00:12:21.480 --> 00:12:25.240
It's a big difference there.

00:12:25.240 --> 00:12:28.800
11 cents versus $3 to have a conversation with it.

00:12:28.800 --> 00:12:29.640
That's just--

00:12:29.640 --> 00:12:31.480
- It's crazy.

00:12:31.480 --> 00:12:33.080
It's a very wide difference.

00:12:33.080 --> 00:12:34.760
And it's all based on,

00:12:34.760 --> 00:12:36.680
how much compute do these models take, right?

00:12:36.680 --> 00:12:40.240
'Cause the bigger the model, the more accurate it is.

00:12:40.240 --> 00:12:42.200
But also the more expensive it is for them to run it.

00:12:42.200 --> 00:12:44.620
So that's why there's such a cost difference.

00:12:44.620 --> 00:12:47.840
- Yeah, recently interviewed,

00:12:47.840 --> 00:12:49.320
just released a while ago,

00:12:49.320 --> 00:12:52.840
interviewed because of time shifting on podcast,

00:12:52.840 --> 00:12:54.960
Mark Russinovich, CTO of Azure.

00:12:54.960 --> 00:12:57.800
And we talked about all the crazy stuff that they're doing

00:12:57.800 --> 00:13:00.160
for coming up with the,

00:13:00.160 --> 00:13:03.000
just running these computers that handle all of this compute

00:13:03.000 --> 00:13:05.880
and it's really a lot.

00:13:05.880 --> 00:13:08.000
- Yep, for sure, for sure.

00:13:08.000 --> 00:13:11.840
It's, there's so many,

00:13:11.840 --> 00:13:13.800
and there was a GPU shortage for a while.

00:13:13.800 --> 00:13:15.040
I don't know if that's still going on.

00:13:15.040 --> 00:13:18.120
And obviously, the big companies are buying

00:13:18.120 --> 00:13:19.720
hundreds of thousands of these GPUs

00:13:19.720 --> 00:13:22.360
to get the scale they need.

00:13:22.360 --> 00:13:23.640
- Yeah.

00:13:23.640 --> 00:13:25.640
- And so once you figure out which API you wanna use,

00:13:25.640 --> 00:13:27.640
then you wanna talk about the library.

00:13:27.640 --> 00:13:29.920
So now, most of these providers,

00:13:29.920 --> 00:13:32.320
they have a Python library that they offer.

00:13:32.320 --> 00:13:36.320
I know OpenAI does and Google Gemini does,

00:13:36.320 --> 00:13:38.200
but there's also open source ones, right?

00:13:38.200 --> 00:13:41.160
'Cause they're not very complicated to talk to.

00:13:41.160 --> 00:13:43.720
It's just basically HTTP requests.

00:13:43.720 --> 00:13:45.080
So it's just really a matter of like,

00:13:45.080 --> 00:13:47.200
what's the ergonomics you're looking for as a developer

00:13:47.200 --> 00:13:49.640
to interact with these things.

00:13:49.640 --> 00:13:50.880
And most importantly,

00:13:50.880 --> 00:13:52.520
make sure you're maintaining optionality, right?

00:13:52.520 --> 00:13:55.480
Like, it's great to do a prototype

00:13:55.480 --> 00:13:58.240
with one of these models,

00:13:58.240 --> 00:13:59.560
but recognize you might wanna switch,

00:13:59.560 --> 00:14:01.800
either for cost reasons or performance reasons

00:14:01.800 --> 00:14:02.920
or what have you.

00:14:02.920 --> 00:14:05.960
And, you know, LangChain, for instance,

00:14:05.960 --> 00:14:08.840
has a ton of the providers as part of,

00:14:08.840 --> 00:14:12.200
you basically are just switching a few arguments

00:14:12.200 --> 00:14:14.360
when you're switching between them.

00:14:14.360 --> 00:14:16.880
And then Simon Willison has, you know,

00:14:16.880 --> 00:14:19.000
a Python fame, has an LLM project

00:14:19.000 --> 00:14:22.240
where he's defined, you know, basically a set of,

00:14:22.240 --> 00:14:24.720
and it's really clean just the way he's organized it,

00:14:24.720 --> 00:14:26.800
because you can just add plugins as you need them, right?

00:14:26.800 --> 00:14:28.440
So you don't have to install

00:14:28.440 --> 00:14:29.800
all the different libraries that are out there.

00:14:29.800 --> 00:14:31.160
And I think LangChain is kind of following

00:14:31.160 --> 00:14:32.800
a similar approach there.

00:14:32.800 --> 00:14:35.800
I think they're coming up with a LangChain core capability

00:14:35.800 --> 00:14:38.280
where you can just kind of bring in things as you need them.

00:14:38.280 --> 00:14:43.120
And so the idea is you're now coding against these libraries

00:14:43.120 --> 00:14:47.760
and you're trying to bring together, you know,

00:14:47.760 --> 00:14:49.400
the text you need to have analyzed

00:14:49.400 --> 00:14:51.000
or whatever your use case is,

00:14:51.000 --> 00:14:53.400
and then it'll come back with the generation.

00:14:53.400 --> 00:14:55.240
And you can also not just use them on the cloud,

00:14:55.240 --> 00:14:56.840
you can use open source ones as well

00:14:56.840 --> 00:14:59.040
and run them locally on your local computer.

00:14:59.040 --> 00:15:02.360
- I'd never really thought about

00:15:02.360 --> 00:15:06.120
my architectural considerations,

00:15:06.120 --> 00:15:08.400
I guess, of these sorts of things.

00:15:08.400 --> 00:15:10.120
But of course you want to set up

00:15:10.120 --> 00:15:11.960
some kind of abstraction layer

00:15:11.960 --> 00:15:16.960
so you're not completely tied into some provider.

00:15:16.960 --> 00:15:20.240
It could be that it becomes too expensive.

00:15:20.240 --> 00:15:21.560
It could be that it becomes too slow,

00:15:21.560 --> 00:15:25.040
but it also might just be something that's better.

00:15:25.040 --> 00:15:27.000
It could be something else that comes along that's better

00:15:27.000 --> 00:15:30.200
and you're like, "Eh, we could switch, it's 25% better."

00:15:30.200 --> 00:15:34.120
But it's like a week to pull all the details

00:15:34.120 --> 00:15:36.320
of this one LLM out and put the new ones in,

00:15:36.320 --> 00:15:37.600
and so it's not worth it, right?

00:15:37.600 --> 00:15:41.960
So you like having, being tied to a particular database

00:15:41.960 --> 00:15:43.360
rather than more general--

00:15:43.360 --> 00:15:45.600
- Especially at this moment in time, right?

00:15:45.600 --> 00:15:47.360
Every couple months, something,

00:15:47.480 --> 00:15:49.000
so something from the bottom up

00:15:49.000 --> 00:15:50.720
is getting better and better,

00:15:50.720 --> 00:15:54.240
meaning LLAMA came out a year ago

00:15:54.240 --> 00:15:56.880
and then LLAMA 2 and Mistral and Mixtral,

00:15:56.880 --> 00:15:59.280
and LLAMA 3 is gonna be coming out later

00:15:59.280 --> 00:16:00.360
this year, we believe.

00:16:00.360 --> 00:16:02.760
And so those models, which are smaller

00:16:02.760 --> 00:16:04.880
and cheaper and easier to use,

00:16:04.880 --> 00:16:07.680
or not easier to use, but they're just cheaper,

00:16:07.680 --> 00:16:12.320
is, those things are happening all the time.

00:16:12.320 --> 00:16:14.320
So being able to be flexible and nimble

00:16:14.320 --> 00:16:15.840
and kind of change where you are

00:16:15.840 --> 00:16:19.320
is gonna be crucial, at least for the next couple years.

00:16:19.320 --> 00:16:21.360
- Yeah, the example that I gave was databases, right?

00:16:21.360 --> 00:16:25.760
And databases have been kind of a known commodity

00:16:25.760 --> 00:16:28.280
since the '80s, or what, 1980s?

00:16:28.280 --> 00:16:30.200
And of course, there's new ones that come along,

00:16:30.200 --> 00:16:31.680
but they're kind of all the same,

00:16:31.680 --> 00:16:34.640
and we got, there was MySQL,

00:16:34.640 --> 00:16:38.080
now there's Postgres that people love, right?

00:16:38.080 --> 00:16:41.480
So that is changing way, way slower than this.

00:16:41.480 --> 00:16:42.960
And people are like, "Well, we gotta think about

00:16:42.960 --> 00:16:44.960
those kinds of, don't get tied into that."

00:16:44.960 --> 00:16:46.640
Well, sure, right?

00:16:46.640 --> 00:16:48.800
- It's way less stable.

00:16:48.800 --> 00:16:53.800
- Right, and people, and create layers of abstraction

00:16:53.800 --> 00:16:55.960
there too, is where you got SQLAlchemy,

00:16:55.960 --> 00:16:59.600
and then Sebastian from FastAPI has SQLModel,

00:16:59.600 --> 00:17:01.960
that's a layer on top of SQLAlchemy.

00:17:01.960 --> 00:17:04.120
And then there's also folks that just like

00:17:04.120 --> 00:17:07.400
writing clean ANSI SQL, and you can hopefully

00:17:07.400 --> 00:17:09.360
be able to port that from database to database as well.

00:17:09.360 --> 00:17:12.960
So yeah, it's the same principles,

00:17:12.960 --> 00:17:16.520
separation of concerns, so you can kind of be flexible.

00:17:16.520 --> 00:17:18.560
- All right, so you talked about LangChain.

00:17:18.560 --> 00:17:21.200
Just give us a sense real quick of what LangChain is.

00:17:21.200 --> 00:17:25.120
- Yeah, I mean, this was a great project

00:17:25.120 --> 00:17:26.280
from a timing perspective.

00:17:26.280 --> 00:17:28.320
I believe they invented it and released it

00:17:28.320 --> 00:17:30.160
right around the time Chats GPT came out.

00:17:30.160 --> 00:17:33.560
It's a very comprehensive library with lots of,

00:17:33.560 --> 00:17:35.400
I mean, the best part about LangChain to me

00:17:35.400 --> 00:17:37.640
is the documentation and the code samples, right?

00:17:37.640 --> 00:17:39.520
Because if you want to learn how to interact

00:17:39.520 --> 00:17:42.040
with a different large language model,

00:17:42.040 --> 00:17:44.000
or work with a vector database,

00:17:44.000 --> 00:17:45.720
and there's another library called LamaIndex

00:17:45.720 --> 00:17:47.720
that does a really good job at this as well.

00:17:47.720 --> 00:17:50.080
They have tons and tons of documentation and examples.

00:17:50.080 --> 00:17:53.680
So you can kind of look at those and try to understand it.

00:17:53.680 --> 00:17:56.160
The chaining part really came from the idea of like,

00:17:56.160 --> 00:17:59.000
okay, prompt the large language model gives a response,

00:17:59.000 --> 00:18:01.600
and I'm gonna take that response and prompt,

00:18:01.600 --> 00:18:04.720
again, with a new prompt using that output.

00:18:04.720 --> 00:18:07.680
The challenge with that is the reliability

00:18:07.680 --> 00:18:08.520
of these models, right?

00:18:08.520 --> 00:18:10.360
They're not gonna get close,

00:18:10.360 --> 00:18:13.880
they're not close to 100% accurate on these types of tasks.

00:18:13.880 --> 00:18:17.560
The idea of agents as well as another thing

00:18:17.560 --> 00:18:19.680
that you might build with a LangChain,

00:18:19.680 --> 00:18:22.640
and the idea there is basically the agent is,

00:18:22.640 --> 00:18:25.920
getting a task, coming up with a plan for that task,

00:18:25.920 --> 00:18:28.640
and then kind of stepping through those tasks

00:18:28.640 --> 00:18:30.400
to get the job done.

00:18:30.400 --> 00:18:31.720
Once again, we're just not there yet

00:18:31.720 --> 00:18:34.800
as far as those technologies,

00:18:34.800 --> 00:18:36.920
just because of the reliability.

00:18:36.920 --> 00:18:39.000
And then there's also a bunch of security concerns

00:18:39.000 --> 00:18:40.800
that are out there too,

00:18:40.800 --> 00:18:42.200
that you should definitely be aware of,

00:18:42.200 --> 00:18:44.160
like one term to Google

00:18:44.160 --> 00:18:46.800
and make sure you understand is prompt injection.

00:18:46.800 --> 00:18:48.960
So Simon, once again, he's got a great blog,

00:18:48.960 --> 00:18:51.200
and he's got a great blog article,

00:18:51.200 --> 00:18:54.120
or just even that tag on his blog is,

00:18:54.120 --> 00:18:56.760
tons of articles around prompt injection.

00:18:56.760 --> 00:18:59.040
And prompt injection is basically the idea

00:18:59.040 --> 00:19:02.320
of you have an app, a user says something in the app,

00:19:02.320 --> 00:19:05.600
or like types into the whatever the input is,

00:19:05.600 --> 00:19:07.440
and whatever text that they're sending through,

00:19:07.440 --> 00:19:08.600
just like with SQL injection,

00:19:08.600 --> 00:19:10.320
it kind of hijacks the conversation

00:19:10.320 --> 00:19:12.360
and causes the large language model

00:19:12.360 --> 00:19:14.120
to kind of do a different thing.

00:19:14.120 --> 00:19:17.480
- Yeah, you'll hear a little Bobby Llama, we call him.

00:19:17.480 --> 00:19:21.280
- Right, exactly right.

00:19:21.280 --> 00:19:24.440
And then the other wild one is like,

00:19:24.440 --> 00:19:26.400
people are putting stuff up on the internet,

00:19:26.400 --> 00:19:29.600
so that when the large language model browses for webpages

00:19:29.600 --> 00:19:30.680
and brings back text,

00:19:30.680 --> 00:19:34.000
it's reading the HTML or reading the text in the HTML,

00:19:34.000 --> 00:19:35.360
and it's causing the large language model

00:19:35.360 --> 00:19:37.080
to behave in some unexpected way.

00:19:37.080 --> 00:19:40.800
So there's lots of crazy challenges out there.

00:19:40.800 --> 00:19:45.600
- I'm sure there's a lot of adversarial stuff

00:19:45.600 --> 00:19:46.760
happening to these things,

00:19:46.760 --> 00:19:49.240
as they're both trying to gather data

00:19:49.240 --> 00:19:51.080
and then trying to run, right?

00:19:51.080 --> 00:19:53.140
I saw the most insane,

00:19:53.140 --> 00:19:56.680
I don't know, I guess it was an article,

00:19:56.680 --> 00:19:58.840
I saw it on RSS somewhere.

00:19:58.840 --> 00:20:02.220
It was saying that on Amazon,

00:20:02.220 --> 00:20:03.760
there's all these knockoff brands

00:20:03.760 --> 00:20:05.840
that are trying to, you know,

00:20:05.840 --> 00:20:09.080
instead of Gucci, you have a Gucci or I don't know,

00:20:09.080 --> 00:20:10.320
whatever, right?

00:20:10.320 --> 00:20:13.560
And they're getting so lazy.

00:20:13.560 --> 00:20:14.440
I don't know what the right word is,

00:20:14.440 --> 00:20:17.940
that they're using LLMs to try to write a description

00:20:17.940 --> 00:20:21.400
that is sort of in the style of Gucci, let's say.

00:20:21.400 --> 00:20:22.380
- Right.

00:20:22.380 --> 00:20:23.360
- And it'll come back and say,

00:20:23.360 --> 00:20:26.200
"I'm sorry, I'm a large language model.

00:20:26.200 --> 00:20:28.840
I'm not, my rules forbid me

00:20:28.840 --> 00:20:32.680
from doing brand trademark violation."

00:20:32.680 --> 00:20:33.520
- Right, right.

00:20:33.520 --> 00:20:35.920
- That's what the Amazon listing says on Amazon.

00:20:35.920 --> 00:20:37.800
They just take it and they just straight pump it straight.

00:20:37.800 --> 00:20:39.960
Whatever it says, it just goes straight into Amazon.

00:20:39.960 --> 00:20:40.800
- Yep, yep, yep.

00:20:40.800 --> 00:20:41.620
You have to like Google like,

00:20:41.620 --> 00:20:43.400
"Sorry, I'm not, sorry as a large language model,"

00:20:43.400 --> 00:20:44.280
or "Sorry as a whatever."

00:20:44.280 --> 00:20:45.120
Yeah, you can find all.

00:20:45.120 --> 00:20:45.960
- Exactly, there's like,

00:20:45.960 --> 00:20:48.120
the product listings are full of that.

00:20:48.120 --> 00:20:49.320
It's amazing.

00:20:49.320 --> 00:20:50.320
It's amazing. - It's crazy.

00:20:50.320 --> 00:20:51.160
It's crazy.

00:20:51.160 --> 00:20:53.600
- Yeah, but so certainly the reliability of that is,

00:20:53.600 --> 00:20:56.200
you know, they could probably use some testing

00:20:56.200 --> 00:20:58.160
and those kinds of things.

00:20:58.160 --> 00:20:59.920
- For sure.

00:20:59.920 --> 00:21:00.740
- Owen out there asks like,

00:21:00.740 --> 00:21:02.760
"I wonder if for local LLM models,

00:21:02.760 --> 00:21:04.520
there's a similar site as DocSpots

00:21:04.520 --> 00:21:06.720
that show you like what you need to run it locally?"

00:21:06.720 --> 00:21:08.800
So that's an interesting question also,

00:21:08.800 --> 00:21:12.000
segue to maybe talk about like some local stuff.

00:21:12.000 --> 00:21:15.600
- Yeah, so yeah, LLM Studio, this is a new product.

00:21:15.600 --> 00:21:17.800
I honestly haven't had a chance to like really dig in

00:21:17.800 --> 00:21:19.200
and understand who created this

00:21:19.200 --> 00:21:21.840
and you know, make sure that the privacy stuff

00:21:21.840 --> 00:21:26.080
is up to snuff, but I've played around with it locally.

00:21:26.080 --> 00:21:27.480
It seems to work great.

00:21:27.480 --> 00:21:29.160
It's really slick, really nice user interface.

00:21:29.160 --> 00:21:31.340
So if you're just wanting to get your feet wet

00:21:31.340 --> 00:21:33.200
and try to understand some of these models,

00:21:33.200 --> 00:21:34.920
I'd download that and check it out.

00:21:34.920 --> 00:21:37.640
There's a ton of models up on Hugging Face.

00:21:37.640 --> 00:21:39.680
This product seems to just basically link right

00:21:39.680 --> 00:21:44.680
into the Hugging Face interface and so, and grabs models.

00:21:44.680 --> 00:21:47.800
And so some of the models you wanna look for

00:21:47.800 --> 00:21:49.720
are right now as in January, right?

00:21:49.720 --> 00:21:54.720
There's Mistral 7B, you know, M-I-S-T-R-A-L.

00:21:54.720 --> 00:21:57.520
There's another one called Phi 2.

00:21:57.520 --> 00:21:59.220
Those are two of the smaller models

00:21:59.220 --> 00:22:01.280
that should run pretty well on, you know,

00:22:01.280 --> 00:22:05.320
like a commercial grade GPU or an M1 or an M2 Mac,

00:22:05.320 --> 00:22:09.440
if that's what you have and start playing with them.

00:22:09.440 --> 00:22:12.440
And they're quantized, which means they're just kind of made

00:22:12.440 --> 00:22:14.960
a little to take a little bit less space,

00:22:14.960 --> 00:22:17.240
which is good from like a virtual RAM

00:22:17.240 --> 00:22:19.240
with regards to these GPUs.

00:22:19.240 --> 00:22:22.680
And you know, there's a account on Hugging Face

00:22:22.680 --> 00:22:23.720
called the Bloke.

00:22:23.720 --> 00:22:26.560
If you look for him, you'll see all his different fine,

00:22:26.560 --> 00:22:29.120
there's different fine tunes and things like that.

00:22:29.120 --> 00:22:30.900
And there's a group called Noose,

00:22:30.900 --> 00:22:33.220
I think is how you pronounce it, N-O-U-S.

00:22:33.220 --> 00:22:35.300
And they've got some of the fine tunes

00:22:35.300 --> 00:22:37.980
that are basically the highest performing ones

00:22:37.980 --> 00:22:38.980
that are out there.

00:22:38.980 --> 00:22:40.740
So if you're really looking for a high performing

00:22:40.740 --> 00:22:42.700
local model that can actually, you know,

00:22:42.700 --> 00:22:44.960
help you with code or reasoning,

00:22:44.960 --> 00:22:48.020
those are definitely the way to get started.

00:22:48.020 --> 00:22:49.640
- Excellent.

00:22:49.640 --> 00:22:51.300
Yeah, this one seems pretty nice.

00:22:51.300 --> 00:22:53.800
I also haven't played with it, I just learned about it,

00:22:53.800 --> 00:22:55.500
but it's looking really good.

00:22:55.500 --> 00:22:59.020
I had played with, what was it, GPT for All?

00:22:59.020 --> 00:23:00.820
I think it was. - Yep, yep, yep.

00:23:00.820 --> 00:23:02.780
- Was the one that I played with.

00:23:02.780 --> 00:23:05.260
Sometimes it looks a little bit nicer than that,

00:23:05.260 --> 00:23:07.820
for some reason, I don't know how different it really is.

00:23:07.820 --> 00:23:10.860
- No, I mean, it's all the idea of like downloading

00:23:10.860 --> 00:23:13.060
these files and running them locally.

00:23:13.060 --> 00:23:14.300
And these are just user interfaces

00:23:14.300 --> 00:23:16.380
that make it a little easier.

00:23:16.380 --> 00:23:18.060
You know, the original project,

00:23:18.060 --> 00:23:20.580
the original project that made this stuff kind of possible

00:23:20.580 --> 00:23:22.900
was a project called Llama CPP.

00:23:22.900 --> 00:23:26.860
There's a Python library that can work with that directly.

00:23:26.860 --> 00:23:28.980
There's another project called Llama File,

00:23:28.980 --> 00:23:31.020
where if you download the whole thing,

00:23:31.020 --> 00:23:33.100
it actually runs no matter where you are.

00:23:33.100 --> 00:23:36.340
I think it runs on Mac and Linux and Windows and BSD

00:23:36.340 --> 00:23:37.300
or whatever it is.

00:23:37.300 --> 00:23:39.180
- Nice. - And it's, I mean,

00:23:39.180 --> 00:23:42.900
it's an amazing technology that this one put together.

00:23:42.900 --> 00:23:44.660
It's really impressive.

00:23:44.660 --> 00:23:46.140
And then, you know,

00:23:46.140 --> 00:23:48.540
you can actually just use Google Colab too, right?

00:23:48.540 --> 00:23:51.080
So Google Colab has some GPUs with it.

00:23:51.080 --> 00:23:53.640
I think if you upgrade it to the $10 a month version,

00:23:53.640 --> 00:23:57.060
I think you get some better GPUs access.

00:23:57.060 --> 00:23:59.500
So if you actually wanna get a hand of like running,

00:23:59.500 --> 00:24:01.180
instead of, and so this is a little bit different, right?

00:24:01.180 --> 00:24:03.140
So instead of calling an API,

00:24:03.140 --> 00:24:04.600
when you're using Google Colab,

00:24:04.600 --> 00:24:06.940
you can actually use a library called Hugging Face,

00:24:06.940 --> 00:24:08.500
and then you can actually load these things

00:24:08.500 --> 00:24:10.260
directly into your memory

00:24:10.260 --> 00:24:13.340
and then into your actual, you know, Python environment,

00:24:13.340 --> 00:24:14.940
and then you're working with it directly.

00:24:14.940 --> 00:24:17.100
So it just takes a little bit of work

00:24:17.100 --> 00:24:19.340
to make sure you're running it on the GPU,

00:24:19.340 --> 00:24:20.620
'cause if you're running it on the CPU,

00:24:20.620 --> 00:24:21.980
it's gonna be a lot slower.

00:24:21.980 --> 00:24:24.660
- Yeah, it definitely makes a big difference.

00:24:24.660 --> 00:24:27.860
There's a tool that I use that for a long time,

00:24:27.860 --> 00:24:30.660
right on the CPU, and they rewrote it to run on the GPU.

00:24:30.660 --> 00:24:32.620
Even on my M2 Pro,

00:24:32.620 --> 00:24:34.980
it was like three times faster or something, you know?

00:24:34.980 --> 00:24:36.660
- Yep, for sure, for sure.

00:24:36.660 --> 00:24:39.020
- Yeah, it makes a big difference.

00:24:39.020 --> 00:24:40.860
So with this, let's see here,

00:24:40.860 --> 00:24:42.520
says some things that you can do.

00:24:42.520 --> 00:24:45.580
Which one?

00:24:45.580 --> 00:24:46.860
I've read a bunch of these all at the same time,

00:24:46.860 --> 00:24:47.940
so I've gotta remember.

00:24:47.940 --> 00:24:49.980
So with the LM Studio,

00:24:50.060 --> 00:24:54.660
plus you run the LLMs offline and use models

00:24:54.660 --> 00:24:56.700
through an open AI, that's what I was looking for,

00:24:56.700 --> 00:25:00.020
the open AI compatible local server.

00:25:00.020 --> 00:25:00.860
- Right, yeah, so--

00:25:00.860 --> 00:25:03.740
- You could basically get an API for any of these

00:25:03.740 --> 00:25:06.100
and then start programming against it, right?

00:25:06.100 --> 00:25:09.540
- Exactly right, and it's basically the same interface,

00:25:09.540 --> 00:25:13.580
right, so same APIs for posting in response

00:25:13.580 --> 00:25:16.660
of the JSON schema that's going back and forth.

00:25:16.660 --> 00:25:18.620
So you're programming against that interface,

00:25:18.620 --> 00:25:21.420
and then you basically port it and move it to another,

00:25:21.420 --> 00:25:24.180
to the open AI models if you wanted to as well.

00:25:24.180 --> 00:25:26.300
So everyone's kind of coalescing around open AI

00:25:26.300 --> 00:25:28.620
as kind of like the quote unquote standard,

00:25:28.620 --> 00:25:31.020
but there's nothing, you know, there's really no,

00:25:31.020 --> 00:25:33.460
there's no mode around that standard as well, right?

00:25:33.460 --> 00:25:36.780
'Cause anybody can kind of adopt it and use it.

00:25:36.780 --> 00:25:39.680
- There's not like a W3C committee choosing.

00:25:39.680 --> 00:25:42.620
- Correct, correct.

00:25:42.620 --> 00:25:43.460
And yeah, so--

00:25:43.460 --> 00:25:46.140
- The market will choose for us, let's go.

00:25:46.140 --> 00:25:47.980
- Yep, yeah, and it seems to be,

00:25:47.980 --> 00:25:49.020
it seems to be working out well.

00:25:49.020 --> 00:25:52.700
And that's another benefit of Simon's LLM project, right?

00:25:52.700 --> 00:25:54.700
You can, he's got the ability to kind of switch back

00:25:54.700 --> 00:25:59.700
and forth between these different libraries and APIs as well.

00:25:59.700 --> 00:26:03.500
- Yeah, this LM Studio says,

00:26:03.500 --> 00:26:06.500
"This app does not collect data nor monitor your actions.

00:26:06.500 --> 00:26:08.580
Your data stays local on your machine,

00:26:08.580 --> 00:26:09.420
free for personal use."

00:26:09.420 --> 00:26:10.580
All that sounds great.

00:26:10.580 --> 00:26:12.340
"For business use, please get in touch."

00:26:12.340 --> 00:26:13.820
I always just like these, like,

00:26:13.820 --> 00:26:16.500
if you gotta ask, it's too much to type.

00:26:16.500 --> 00:26:17.860
- Probably, yeah.

00:26:17.860 --> 00:26:20.260
- So I'm not using it for personal use,

00:26:20.260 --> 00:26:21.900
just so if anybody's watching, yes.

00:26:21.900 --> 00:26:23.300
Just plain out.

00:26:23.300 --> 00:26:27.540
- Yeah, and it's, you know,

00:26:27.540 --> 00:26:28.980
either they just haven't thought it through

00:26:28.980 --> 00:26:30.820
and they just don't wanna talk about it yet.

00:26:30.820 --> 00:26:32.420
- Sure, I think that's part of it.

00:26:32.420 --> 00:26:33.780
- I just probably imagine it's probably,

00:26:33.780 --> 00:26:35.300
it's like, "Ah, we haven't figured out a business model.

00:26:35.300 --> 00:26:37.780
Just, I don't know, shoot us a note."

00:26:37.780 --> 00:26:39.660
- Yep, they're concentrating on the product,

00:26:39.660 --> 00:26:40.860
which makes sense.

00:26:40.860 --> 00:26:43.300
- Yeah, so then the other one is Llamafile,

00:26:43.300 --> 00:26:45.820
llamafile.ai that you mentioned.

00:26:45.820 --> 00:26:46.880
This packages it up.

00:26:46.880 --> 00:26:50.740
I guess, going back to the LM Studio real quick,

00:26:50.740 --> 00:26:52.900
one of the things that's cool about this

00:26:52.900 --> 00:26:57.020
is if it's the OpenAI API, right,

00:26:57.020 --> 00:26:58.940
for this little local server that you can play with,

00:26:58.940 --> 00:27:03.060
but then you can pick LLMs such as Llama, Falcon,

00:27:03.060 --> 00:27:07.780
Replit, all the different ones, right,

00:27:07.780 --> 00:27:09.340
Starcoder and so on.

00:27:09.340 --> 00:27:12.420
It would let you write an app

00:27:12.420 --> 00:27:13.980
as if it was going to OpenAI

00:27:13.980 --> 00:27:16.380
and then just start swapping in models.

00:27:16.880 --> 00:27:18.400
And go like, "Oh, if we switch to this model,

00:27:18.400 --> 00:27:19.240
how'd that work?"

00:27:19.240 --> 00:27:20.560
But you don't even have to change any code, right?

00:27:20.560 --> 00:27:22.720
Just probably maybe a string that says

00:27:22.720 --> 00:27:25.120
which model to initialize.

00:27:25.120 --> 00:27:28.080
- Correct, well, you're getting to one of the tricks though

00:27:28.080 --> 00:27:30.320
is then the prompts themselves, right?

00:27:30.320 --> 00:27:32.240
So the models themselves, yeah,

00:27:32.240 --> 00:27:35.080
the models themselves act differently.

00:27:35.080 --> 00:27:37.600
And part of this whole world

00:27:37.600 --> 00:27:40.120
is what they call prompt engineering, right?

00:27:40.120 --> 00:27:42.480
So prompt engineering is really just exploring

00:27:42.480 --> 00:27:44.920
how to interact with these models,

00:27:44.920 --> 00:27:47.660
how to make sure that they're kind of

00:27:47.660 --> 00:27:50.700
in the right mind space to tackle your problem.

00:27:50.700 --> 00:27:52.740
A lot of the times that people get,

00:27:52.740 --> 00:27:53.860
when they struggle with these things,

00:27:53.860 --> 00:27:55.900
it's really just, they'd really got to think

00:27:55.900 --> 00:27:58.860
more like a psychiatrist when they're working with a model.

00:27:58.860 --> 00:28:01.980
Basically getting them kind of prepared.

00:28:01.980 --> 00:28:04.900
One of the tricks people did, figured out early was,

00:28:04.900 --> 00:28:07.660
you're a genius at software development,

00:28:07.660 --> 00:28:09.420
like compliment the thing, make it feel like,

00:28:09.420 --> 00:28:14.420
"Oh, I'm gonna behave like I'm a world rockstar programmer."

00:28:15.400 --> 00:28:16.480
- Well, it's gonna give you average,

00:28:16.480 --> 00:28:17.800
but if you tell them, "I'm genius,"

00:28:17.800 --> 00:28:19.360
then let's start, we'll do that.

00:28:19.360 --> 00:28:20.920
- Right, right, 'cause I mean,

00:28:20.920 --> 00:28:23.920
'cause, and there was also a theory like that in December

00:28:23.920 --> 00:28:26.320
that the large language models were getting dumber

00:28:26.320 --> 00:28:27.360
because it was the holidays

00:28:27.360 --> 00:28:29.680
and people don't work as hard, right?

00:28:29.680 --> 00:28:30.720
It's really hard to know

00:28:30.720 --> 00:28:33.840
which of these kind of things are true or not,

00:28:33.840 --> 00:28:36.120
but it's definitely true that each model

00:28:36.120 --> 00:28:36.940
is a little bit different.

00:28:36.940 --> 00:28:39.600
And if you write a prompt that works really well

00:28:39.600 --> 00:28:42.560
on one model, even if it's a stronger model

00:28:42.560 --> 00:28:44.700
or a weaker model, and then you port it to another model

00:28:44.700 --> 00:28:49.700
and it's, and that's the stronger model works worse, right?

00:28:49.700 --> 00:28:52.460
It can be very counterintuitive at times

00:28:52.460 --> 00:28:55.420
and you've got to test things out.

00:28:55.420 --> 00:28:57.720
And that really gets to the idea of evals, right?

00:28:57.720 --> 00:29:01.380
So evaluation is really a key problem, right?

00:29:01.380 --> 00:29:03.540
Making sure that if you're gonna be writing prompts

00:29:03.540 --> 00:29:06.900
and you're gonna be building different retrieval,

00:29:06.900 --> 00:29:09.220
augmented generation solutions,

00:29:09.220 --> 00:29:11.840
you need to know about prompt injection

00:29:11.840 --> 00:29:13.700
and you need to know about prompt engineering

00:29:13.700 --> 00:29:17.060
and you need to know what these things can and can't do.

00:29:17.060 --> 00:29:17.940
You know, there's, you know,

00:29:17.940 --> 00:29:21.560
one trick is what they call few shot prompting,

00:29:21.560 --> 00:29:24.140
which is, you know, if you want it to do data extraction,

00:29:24.140 --> 00:29:26.300
you can say, oh, okay, I want you to extract data

00:29:26.300 --> 00:29:29.620
from texts that I give you in JSON.

00:29:29.620 --> 00:29:31.060
If you give it a few examples,

00:29:31.060 --> 00:29:33.000
like wildly different examples,

00:29:33.000 --> 00:29:35.540
because the giving it a bunch of similar stuff,

00:29:35.540 --> 00:29:37.580
it might kind of cause it to just coalesce

00:29:37.580 --> 00:29:38.940
around those similar examples.

00:29:38.940 --> 00:29:42.000
But if you can give it a wildly different set of examples,

00:29:42.000 --> 00:29:45.080
that's called in context learning or few shot prompting,

00:29:45.080 --> 00:29:48.040
and it will do a better job at that specific task for you.

00:29:48.040 --> 00:29:51.520
- Okay, that's super neat.

00:29:51.520 --> 00:29:54.040
When you're creating your apps,

00:29:54.040 --> 00:29:58.440
do you do things like, here's the input from the program

00:29:58.440 --> 00:30:00.600
or from the user or wherever it came from,

00:30:00.600 --> 00:30:03.640
but maybe before that you give it like three or four prompts

00:30:03.640 --> 00:30:05.760
and then let it have the question, right?

00:30:05.760 --> 00:30:07.360
Instead of just taking the text,

00:30:07.360 --> 00:30:11.320
like I'm gonna ask you questions about biology and genetics,

00:30:11.320 --> 00:30:13.060
and it's gonna be under this context.

00:30:13.060 --> 00:30:14.940
And I want you to favor these data sources.

00:30:14.940 --> 00:30:17.300
Now ask your question, something like this.

00:30:17.300 --> 00:30:19.420
- For sure, all those types of strategies

00:30:19.420 --> 00:30:20.740
are worth experimenting with, right?

00:30:20.740 --> 00:30:23.320
Like what actually will work for your scenario?

00:30:23.320 --> 00:30:24.160
I can't tell you, right?

00:30:24.160 --> 00:30:26.380
You gotta dig in, you gotta figure it out

00:30:26.380 --> 00:30:28.740
and you gotta try different things.

00:30:28.740 --> 00:30:29.580
And you can--

00:30:29.580 --> 00:30:30.900
- You're about to win the Nobel Prize

00:30:30.900 --> 00:30:32.420
in genetics for your work.

00:30:32.420 --> 00:30:34.920
Now I'm gonna ask you some questions.

00:30:34.920 --> 00:30:36.740
- For sure, for sure, that'll definitely work.

00:30:36.740 --> 00:30:39.460
And then threatening it that your boss is mad at you

00:30:39.460 --> 00:30:40.660
is also gonna help you too, right?

00:30:40.660 --> 00:30:41.500
For sure.

00:30:41.500 --> 00:30:44.180
- If I don't solve this problem, I'm gonna get fired.

00:30:44.180 --> 00:30:46.220
As a large language model, I can't tell you,

00:30:46.220 --> 00:30:47.380
but I'm gonna be fired.

00:30:47.380 --> 00:30:49.020
All right, well, then the answer is.

00:30:49.020 --> 00:30:50.520
- Right, right, exactly right.

00:30:50.520 --> 00:30:54.420
- Okay, so for these, they run, like you said,

00:30:54.420 --> 00:30:56.340
they run pretty much locally,

00:30:56.340 --> 00:30:59.940
these different models on LM Studio

00:30:59.940 --> 00:31:02.100
and others like the Llama file and so on.

00:31:02.100 --> 00:31:03.500
- Yep, yep, and Lama.

00:31:03.500 --> 00:31:05.160
- I don't need a cluster.

00:31:05.160 --> 00:31:06.020
- Correct, correct.

00:31:06.020 --> 00:31:08.540
Yeah, that's, I mean, Llama CPP is really the project

00:31:08.540 --> 00:31:09.700
that should get all the credit

00:31:09.700 --> 00:31:12.600
for making this work on your laptops.

00:31:12.600 --> 00:31:17.300
And then Llama file and Llama CPP all have servers.

00:31:17.300 --> 00:31:21.260
So I'm guessing LM Studio is just exposing that server.

00:31:21.260 --> 00:31:25.060
And that's in the base Llama CPP project.

00:31:25.060 --> 00:31:27.540
And that's really what it is.

00:31:27.540 --> 00:31:32.060
It's really just about now you can post your requests.

00:31:32.060 --> 00:31:33.800
It's handling all of the work

00:31:33.800 --> 00:31:36.440
with regards to the token generation

00:31:36.440 --> 00:31:38.000
on the backend using Llama CPP.

00:31:38.000 --> 00:31:39.060
And then it's returning it to you

00:31:39.060 --> 00:31:41.760
using the HTTP kind of processes.

00:31:41.760 --> 00:31:46.580
- Is Llama originally from Meta?

00:31:46.580 --> 00:31:48.020
Is that where that came from?

00:31:48.020 --> 00:31:49.540
- Yeah, so yeah, I don't,

00:31:49.540 --> 00:31:53.340
I think there were people that were kind of using that LLM.

00:31:53.340 --> 00:31:55.060
I think people were kind of keying off

00:31:55.060 --> 00:31:57.280
the Llama thing at one point.

00:31:57.280 --> 00:32:00.680
I think Llama Index, for instance,

00:32:00.680 --> 00:32:02.900
I think that project was originally called GPT Index

00:32:02.900 --> 00:32:04.580
and they decided, oh, I don't wanna be like,

00:32:04.580 --> 00:32:06.540
I don't wanna confuse myself with OpenAI

00:32:06.540 --> 00:32:07.820
or confuse my project with OpenAI.

00:32:07.820 --> 00:32:09.500
So they switched the Llama Index.

00:32:09.500 --> 00:32:11.420
And then of course, Meta released Llama.

00:32:11.420 --> 00:32:14.260
So, you know, you can't, you kinda,

00:32:14.260 --> 00:32:16.260
and then everything from there has kind of evolved too.

00:32:16.260 --> 00:32:17.540
Right, there's been alpacas

00:32:17.540 --> 00:32:18.660
and a bunch of other stuff as well.

00:32:18.660 --> 00:32:20.780
So, but the original Llama--

00:32:20.780 --> 00:32:21.940
- If you don't know your animals, yeah.

00:32:21.940 --> 00:32:22.820
If you don't know your animals,

00:32:22.820 --> 00:32:25.940
you can't figure out the heritage of these projects.

00:32:25.940 --> 00:32:26.780
- Correct.

00:32:26.780 --> 00:32:31.140
But Llama from Meta was the first open source,

00:32:31.140 --> 00:32:33.380
I'd say large language model of note,

00:32:33.380 --> 00:32:34.900
I guess, since ChatGPT.

00:32:34.900 --> 00:32:37.500
There were certainly other, you know,

00:32:37.500 --> 00:32:39.480
I'm not a, so one thing to caveat,

00:32:39.480 --> 00:32:40.780
I am not a researcher, right?

00:32:40.780 --> 00:32:42.660
So there's lots of folks in the ML research community

00:32:42.660 --> 00:32:44.100
that know way more than I do.

00:32:44.100 --> 00:32:48.180
But, 'cause there was like Bloom and T5

00:32:48.180 --> 00:32:49.420
and a few other large, you know,

00:32:49.420 --> 00:32:50.660
quote unquote large language models.

00:32:50.660 --> 00:32:54.460
But Llama, after ChatGPT, Llama was the big release

00:32:54.460 --> 00:32:57.060
that came from Meta in, I think, March.

00:32:57.060 --> 00:32:58.780
And then, and that was from Meta.

00:32:58.780 --> 00:33:00.860
And then they had it released under just like

00:33:00.860 --> 00:33:03.680
research use terms.

00:33:03.680 --> 00:33:05.900
And then only certain people could access to it.

00:33:05.900 --> 00:33:07.900
And then someone put a, I guess,

00:33:07.900 --> 00:33:11.060
put like a BitTorrent link or something on GitHub.

00:33:11.060 --> 00:33:13.060
And then basically the world had it.

00:33:13.060 --> 00:33:14.980
And then they did end up releasing Llama 2

00:33:14.980 --> 00:33:17.580
a few months later with more friendly terms.

00:33:17.580 --> 00:33:21.500
So that, and it was a much stronger model as well.

00:33:21.500 --> 00:33:22.340
- Nice.

00:33:22.340 --> 00:33:23.160
It's kind of the realization, like,

00:33:23.160 --> 00:33:24.300
well, if it's gonna be out there anyway,

00:33:24.300 --> 00:33:26.620
let's at least get credit for it then.

00:33:26.620 --> 00:33:27.580
- Yep, for sure.

00:33:28.580 --> 00:33:29.420
- For sure.

00:33:29.420 --> 00:33:32.220
And I did read something where like basically Facebook

00:33:32.220 --> 00:33:34.860
approached OpenAI for access to their models

00:33:34.860 --> 00:33:35.700
to help them write code.

00:33:35.700 --> 00:33:37.220
But the cost was so high that they decided

00:33:37.220 --> 00:33:38.560
to just go build their own, right?

00:33:38.560 --> 00:33:43.100
So it's kind of interesting how this stuff has evolved.

00:33:43.100 --> 00:33:46.060
- Like, you know, we've got a big cluster of computers.

00:33:46.060 --> 00:33:47.180
- Right.

00:33:47.180 --> 00:33:48.700
Metaverse thing doesn't seem to be working yet.

00:33:48.700 --> 00:33:51.540
So let's go ahead and train a bunch of large language models.

00:33:51.540 --> 00:33:56.300
- Pass it over in the Metaverse data center.

00:33:56.300 --> 00:33:58.940
So one of the things that people will maybe talk about

00:33:58.940 --> 00:34:03.060
in this space is RAG or retrieval augmented generation.

00:34:03.060 --> 00:34:03.900
What's this?

00:34:03.900 --> 00:34:07.140
- Yeah, so I think one thing to recognize

00:34:07.140 --> 00:34:10.300
is that the large language models,

00:34:10.300 --> 00:34:13.740
if it's not in the training set and it's not in the prompt,

00:34:13.740 --> 00:34:15.340
it really doesn't know about it.

00:34:15.340 --> 00:34:18.620
And the question of like, what's reasoning

00:34:18.620 --> 00:34:20.920
and what's generalizing and things like that,

00:34:20.920 --> 00:34:22.660
those are big debates that people are having.

00:34:22.660 --> 00:34:24.420
What's intelligence, what have you.

00:34:24.420 --> 00:34:26.820
But recognizing the fact that you have this prompt

00:34:26.820 --> 00:34:28.300
and things you put in the prompt,

00:34:28.300 --> 00:34:29.820
the large language model can understand

00:34:29.820 --> 00:34:31.660
and extrapolate from is really powerful.

00:34:31.660 --> 00:34:34.300
So, and that's called in context learning.

00:34:34.300 --> 00:34:36.900
So retrieval augmented generation is the idea of,

00:34:36.900 --> 00:34:40.220
okay, I'm gonna go, I'm going to maybe ask,

00:34:40.220 --> 00:34:41.780
allow a person to ask a question.

00:34:41.780 --> 00:34:45.900
This is kind of like the common use case that I see.

00:34:45.900 --> 00:34:48.620
User asks a question, we're gonna take that question,

00:34:48.620 --> 00:34:52.660
find the relevant content, put that content in the prompt

00:34:52.660 --> 00:34:54.100
and then do something with it, right?

00:34:54.100 --> 00:34:55.700
So it might be something like,

00:34:55.700 --> 00:34:56.940
ask a question about,

00:34:56.940 --> 00:34:59.860
how tall is the leaning tower of Pisa, right?

00:34:59.860 --> 00:35:01.060
And so now it's gonna go off

00:35:01.060 --> 00:35:04.100
and find that piece of content from Wikipedia

00:35:04.100 --> 00:35:04.940
or what have you,

00:35:04.940 --> 00:35:06.380
and then put that information in the prompt

00:35:06.380 --> 00:35:10.700
and then now the model can then respond

00:35:10.700 --> 00:35:12.220
to that question based on that text.

00:35:12.220 --> 00:35:13.500
Obviously that's a pretty simple example,

00:35:13.500 --> 00:35:14.700
but you can get more complicated

00:35:14.700 --> 00:35:16.860
and it's going out and bringing back

00:35:16.860 --> 00:35:19.020
lots of different content, slicing it up,

00:35:19.020 --> 00:35:20.700
putting in the prompt and asking a question.

00:35:20.700 --> 00:35:22.780
So now the trick is, okay,

00:35:22.780 --> 00:35:24.300
how do you actually get that content

00:35:24.300 --> 00:35:25.860
and how do you do that?

00:35:25.860 --> 00:35:29.260
Well, information retrieval, search engines

00:35:29.260 --> 00:35:31.860
and things like that, that's obviously the technique,

00:35:31.860 --> 00:35:34.300
but one of the key techniques that people have been

00:35:34.300 --> 00:35:37.140
kind of discovering, rediscovering, I guess,

00:35:37.140 --> 00:35:40.060
is this idea of word embeddings or vectors.

00:35:40.060 --> 00:35:42.340
And so Word2Vec was this project that came out,

00:35:42.340 --> 00:35:44.180
I think 11 years ago or so.

00:35:44.180 --> 00:35:47.580
And there was a big, the big meme around that was,

00:35:47.580 --> 00:35:49.900
you could take the embedding for the word king,

00:35:49.900 --> 00:35:52.740
you could then subtract the embedding for the word man

00:35:52.740 --> 00:35:54.620
add the word embedding for woman,

00:35:54.620 --> 00:35:57.580
and then the end math result would actually be close

00:35:57.580 --> 00:35:59.700
to the embedding for the word queen.

00:35:59.700 --> 00:36:00.980
And so what is an embedding?

00:36:00.980 --> 00:36:01.820
What's a vector?

00:36:01.820 --> 00:36:04.180
It's basically this large floating point number

00:36:04.180 --> 00:36:08.740
that has semantic meaning inferred into it.

00:36:08.740 --> 00:36:10.980
And it's built just by training a model.

00:36:10.980 --> 00:36:12.740
So just like you train a large language model,

00:36:12.740 --> 00:36:14.500
they can trade these embedding models

00:36:14.500 --> 00:36:18.060
to basically take a word and then take a sentence

00:36:18.060 --> 00:36:21.740
and then take a document is what OpenAI can do

00:36:21.740 --> 00:36:23.740
and turn that into this big giant,

00:36:23.740 --> 00:36:30.580
200, 800, 1500, depending on the size of the embedding,

00:36:30.580 --> 00:36:33.820
the floating point numbers,

00:36:33.820 --> 00:36:35.820
and then use that as a what's called

00:36:35.820 --> 00:36:37.580
semantic similarity search.

00:36:37.580 --> 00:36:38.780
So you're basically going off

00:36:38.780 --> 00:36:40.660
and asking for similar documents.

00:36:40.660 --> 00:36:42.140
And so you get those documents

00:36:42.140 --> 00:36:43.700
and then you make your prompt.

00:36:43.700 --> 00:36:45.860
- Yeah, wild.

00:36:45.860 --> 00:36:46.740
It's really wild.

00:36:46.740 --> 00:36:50.260
So we're gonna make an 800 dimensional space

00:36:50.260 --> 00:36:53.300
and each concept gets a location in that space.

00:36:53.300 --> 00:36:55.980
And then you're gonna get another concept as a prompt.

00:36:55.980 --> 00:36:59.180
You say, what other things in this space are near it?

00:36:59.180 --> 00:37:00.420
Right? - Right.

00:37:00.420 --> 00:37:01.380
Right. - Wild.

00:37:01.380 --> 00:37:02.900
- And then what you're,

00:37:02.900 --> 00:37:06.300
you know, the hard problems that remain are,

00:37:06.300 --> 00:37:07.140
well, first you gotta figure out

00:37:07.140 --> 00:37:07.980
what you're trying to solve.

00:37:07.980 --> 00:37:09.540
So once you figure out what you're actually trying to solve,

00:37:09.540 --> 00:37:12.020
then you can start asking yourself questions like,

00:37:12.020 --> 00:37:15.660
okay, well, how do I chunk up the documents that I have?

00:37:15.660 --> 00:37:17.140
Right, and there's all these different,

00:37:17.140 --> 00:37:18.300
and there's another great place

00:37:18.300 --> 00:37:19.700
for Lama Index and LangChain.

00:37:19.700 --> 00:37:21.900
They have chunking strategies

00:37:21.900 --> 00:37:23.540
where they'll take a big giant document

00:37:23.540 --> 00:37:25.940
and break it down into sections.

00:37:25.940 --> 00:37:27.380
And then you chunk each section

00:37:27.380 --> 00:37:30.940
and then you do the embedding on just that small section.

00:37:30.940 --> 00:37:32.340
Because the idea being,

00:37:32.340 --> 00:37:36.460
can you get finer and finer sets of texts

00:37:36.460 --> 00:37:38.980
that you can then, when you do your retrieval,

00:37:38.980 --> 00:37:40.580
you get the right information back.

00:37:40.580 --> 00:37:41.740
And then the other challenge

00:37:41.740 --> 00:37:43.860
is really like the question answer problem, right?

00:37:43.860 --> 00:37:45.900
If a person's asking a question,

00:37:45.900 --> 00:37:47.420
how do you turn that question

00:37:47.460 --> 00:37:50.300
into the same kind of embedding space as the answer, right?

00:37:50.300 --> 00:37:52.140
And so there's lots of different strategies

00:37:52.140 --> 00:37:53.100
that are out there for that.

00:37:53.100 --> 00:37:55.220
And then another problem is

00:37:55.220 --> 00:37:56.780
if you're looking at the Wikipedia page

00:37:56.780 --> 00:37:58.540
for the Tower of Pisa,

00:37:58.540 --> 00:38:00.140
it might actually have like a sentence in here

00:38:00.140 --> 00:38:04.380
that says it is X number of meters tall or feet tall,

00:38:04.380 --> 00:38:06.860
but it won't actually have the word Tower of Pisa in it.

00:38:06.860 --> 00:38:09.140
So there's another chunking strategy

00:38:09.140 --> 00:38:11.340
where they call propositional trunking,

00:38:11.340 --> 00:38:14.300
where they basically use a large language model

00:38:14.300 --> 00:38:18.940
to actually redefine each sentence

00:38:18.940 --> 00:38:21.460
so that it actually has those proper nouns baked into it

00:38:21.460 --> 00:38:23.140
so that when you do the embedding,

00:38:23.140 --> 00:38:27.100
it doesn't lose some of the detail with propositions.

00:38:27.100 --> 00:38:28.460
- It's this tall, but...

00:38:28.460 --> 00:38:32.260
It's something that replaces this tall

00:38:32.260 --> 00:38:34.660
with its actual height and things like that.

00:38:34.660 --> 00:38:35.580
- Correct, correct.

00:38:35.580 --> 00:38:37.060
- Okay, crazy.

00:38:37.060 --> 00:38:39.060
- But fundamentally, you're working with unstructured data

00:38:39.060 --> 00:38:40.180
and it's kind of messy

00:38:40.180 --> 00:38:43.140
and it's not always gonna work the way you want.

00:38:43.140 --> 00:38:45.580
And there's a lot of challenges

00:38:45.580 --> 00:38:46.860
and people are trying lots of different things

00:38:46.860 --> 00:38:47.940
to make it better.

00:38:47.940 --> 00:38:48.780
- That's cool.

00:38:48.780 --> 00:38:50.860
It's not always deterministic or exactly the same,

00:38:50.860 --> 00:38:52.940
so that can be tricky as well.

00:38:52.940 --> 00:38:56.940
One of the big parts of at least this embedding stuff

00:38:56.940 --> 00:38:59.620
you're talking about are vector databases.

00:38:59.620 --> 00:39:01.060
And they used to be really rare

00:39:01.060 --> 00:39:03.520
and kind of their own specialized thing.

00:39:03.520 --> 00:39:05.700
Now they're starting to show up in lots of places.

00:39:05.700 --> 00:39:09.200
And you've shared with us this link of VectorDB comparison.

00:39:09.200 --> 00:39:10.780
I just saw that MongoDB added it.

00:39:10.780 --> 00:39:13.940
I'm like, I didn't know that it had anything to do with that.

00:39:13.940 --> 00:39:15.180
I'm probably not gonna mess with it,

00:39:15.180 --> 00:39:17.380
but it's interesting that it's just finding its way

00:39:17.380 --> 00:39:19.340
in all these different spaces.

00:39:19.340 --> 00:39:22.500
- Yeah, it was weird there for a couple of years

00:39:22.500 --> 00:39:24.940
where people were basically talking about vector databases

00:39:24.940 --> 00:39:26.620
like they're their own separate thing.

00:39:26.620 --> 00:39:28.540
The vector databases are now becoming their own

00:39:28.540 --> 00:39:30.900
fully fledged either relational database

00:39:30.900 --> 00:39:32.980
or a graph database or a search engine.

00:39:32.980 --> 00:39:35.420
Those are kind of the three categories for...

00:39:35.420 --> 00:39:37.060
I mean, I guess Redis is its own thing too.

00:39:37.060 --> 00:39:41.660
But for the most part, those new databases,

00:39:41.660 --> 00:39:44.060
quote unquote, are now kind of trying to become

00:39:44.060 --> 00:39:45.460
more fully fledged.

00:39:45.460 --> 00:39:47.620
And vectors and semantic search

00:39:47.620 --> 00:39:49.420
is really just one feature, right?

00:39:49.420 --> 00:39:51.340
And yeah.

00:39:51.340 --> 00:39:52.580
- I was just thinking that.

00:39:52.580 --> 00:39:53.900
Is this thing that you're talking about,

00:39:53.900 --> 00:39:56.540
is it a product or is it a feature

00:39:56.540 --> 00:39:58.100
of a bigger product, right?

00:39:58.100 --> 00:39:58.940
- Correct.

00:39:58.940 --> 00:40:00.020
- If you already got a database,

00:40:00.020 --> 00:40:01.540
it's already doing a bunch of things.

00:40:01.540 --> 00:40:03.100
Could it just answer the vector question?

00:40:03.100 --> 00:40:04.140
Maybe, maybe not.

00:40:04.140 --> 00:40:05.020
- Exactly right.

00:40:05.020 --> 00:40:05.860
Exactly right.

00:40:05.860 --> 00:40:08.220
So the one thing to recognize is that...

00:40:08.220 --> 00:40:09.380
And then the other thing people do

00:40:09.380 --> 00:40:10.980
is they just take NumPy or what have you

00:40:10.980 --> 00:40:12.300
and just load them all into memory.

00:40:12.300 --> 00:40:13.260
And if you don't have that much data,

00:40:13.260 --> 00:40:15.660
that's actually probably gonna be the fastest

00:40:15.660 --> 00:40:17.860
and simplest way to work.

00:40:17.860 --> 00:40:20.180
But the thing you gotta recognize is the fact

00:40:20.180 --> 00:40:24.340
that there is a precision and recall and cost trade-off

00:40:24.340 --> 00:40:25.980
that happens as well.

00:40:25.980 --> 00:40:29.220
So they have to index these vectors

00:40:29.220 --> 00:40:31.740
and there's different algorithms that are used

00:40:31.740 --> 00:40:35.020
and different algorithms do better than others.

00:40:35.020 --> 00:40:36.660
So you gotta make sure you understand that as well.

00:40:36.660 --> 00:40:39.580
So, and one thing that you can do is,

00:40:39.580 --> 00:40:40.660
for instance, pgVector,

00:40:40.660 --> 00:40:42.940
which is an extension for Postgres,

00:40:42.940 --> 00:40:45.500
you can start off by not indexing at all.

00:40:45.500 --> 00:40:47.700
And you should get, I believe,

00:40:47.700 --> 00:40:49.140
hopefully I'm not misspeaking,

00:40:49.140 --> 00:40:50.300
you should get perfect recall,

00:40:50.300 --> 00:40:51.540
meaning you'll get the right answer.

00:40:51.540 --> 00:40:54.460
You'll get the, if you ask for the five closest vectors

00:40:54.460 --> 00:40:57.700
to your query, you'll get the five closest,

00:40:57.700 --> 00:41:00.140
but it'll be slower than you probably want.

00:41:00.140 --> 00:41:01.460
So then you have to index it.

00:41:01.460 --> 00:41:02.940
And then what ends up happening is,

00:41:02.940 --> 00:41:04.940
the next time you might only get four of those five.

00:41:04.940 --> 00:41:08.660
You'll get something else that snuck into that list.

00:41:08.660 --> 00:41:13.100
- Yeah, that's, if you got time,

00:41:13.100 --> 00:41:15.980
you're willing to spend unlimited time,

00:41:15.980 --> 00:41:20.580
then you can get the right answer, the exact answer.

00:41:20.580 --> 00:41:22.540
But I guess that's all sorts of heuristics, right?

00:41:22.540 --> 00:41:25.260
You're like, I could spend three days

00:41:25.260 --> 00:41:27.100
or I could do a Monte Carlo thing

00:41:27.100 --> 00:41:29.460
and I can give you the answer in a fraction of a second.

00:41:29.460 --> 00:41:30.500
- Right, right, right.

00:41:30.500 --> 00:41:33.300
- But it's not deterministic.

00:41:33.300 --> 00:41:34.820
All right, so then I won't go with my camera.

00:41:34.820 --> 00:41:36.100
So I turn it off, I don't know what's up.

00:41:36.100 --> 00:41:37.140
- Yeah, no worries.

00:41:37.140 --> 00:41:39.940
- Yeah, so you wrote a cool blog post

00:41:39.940 --> 00:41:42.580
called "What is a Custom GPT?"

00:41:42.580 --> 00:41:45.740
And we wanna talk some about building custom GPTs

00:41:45.740 --> 00:41:48.260
and with SAPI and so on.

00:41:48.260 --> 00:41:49.220
So let's talk about this.

00:41:49.220 --> 00:41:52.820
Like one of the, I think one of the challenges

00:41:52.820 --> 00:41:55.620
in why it takes so much compute for these systems

00:41:55.620 --> 00:41:57.140
is like they're open-ended.

00:41:57.140 --> 00:42:00.700
You can ask me any question about any knowledge in the world

00:42:00.700 --> 00:42:03.020
in the humankind, right?

00:42:03.020 --> 00:42:05.860
You can ask about that, let's start talking.

00:42:05.860 --> 00:42:09.660
Or it could be, you can ask me about genetics.

00:42:09.660 --> 00:42:11.860
- Right. - Right?

00:42:11.860 --> 00:42:14.620
That seems like you could both get better answers

00:42:14.620 --> 00:42:17.460
if you actually only care about genetic responses,

00:42:17.460 --> 00:42:20.660
not how tall is the Leaning Tower

00:42:20.660 --> 00:42:22.540
and probably make it smaller, right?

00:42:22.540 --> 00:42:25.140
So is that kind of the idea of these custom GPTs

00:42:25.140 --> 00:42:26.420
or what is it?

00:42:26.420 --> 00:42:31.220
- No, so custom GPTs are a new capability from OpenAI.

00:42:31.220 --> 00:42:34.700
And basically they are a wrapper around,

00:42:34.700 --> 00:42:37.060
a very small subset,

00:42:37.060 --> 00:42:41.180
but it's still using the OpenAI ecosystem, okay?

00:42:41.180 --> 00:42:43.340
And so what you do is you give it a name,

00:42:43.340 --> 00:42:45.660
you give it a logo, you give it a prompt.

00:42:45.660 --> 00:42:49.500
And then from there, you can also give it knowledge.

00:42:49.500 --> 00:42:51.180
You can upload PDF documents to it

00:42:51.180 --> 00:42:53.660
and it will actually slice and dice those PDF documents

00:42:53.660 --> 00:42:55.500
using some sort of vector search.

00:42:55.500 --> 00:42:57.700
We don't know how it actually works.

00:42:57.700 --> 00:43:00.180
The GPT, the cool thing is the GPT will work on your phone.

00:43:00.180 --> 00:43:01.380
Right, so I have my phone,

00:43:01.380 --> 00:43:02.740
I can have a conversation with my phone,

00:43:02.740 --> 00:43:04.660
I can take a picture, upload a picture

00:43:04.660 --> 00:43:07.220
and it will do vision analysis on it.

00:43:07.220 --> 00:43:10.540
So I get all the capabilities of OpenAI GPT-4.

00:43:10.540 --> 00:43:13.540
But a custom GPT is one that I can construct

00:43:13.540 --> 00:43:15.100
and give a custom prompt to,

00:43:15.100 --> 00:43:17.380
which basically then says, okay, now you're,

00:43:17.380 --> 00:43:18.660
and to your point, I think maybe this is

00:43:18.660 --> 00:43:19.500
where you're going with it,

00:43:19.500 --> 00:43:21.020
like, hey, now you're an expert in genomics

00:43:21.020 --> 00:43:22.380
or you're an expert in something

00:43:22.380 --> 00:43:27.220
and you're basically coaching the language model

00:43:27.220 --> 00:43:29.380
in what it can and can't do.

00:43:29.380 --> 00:43:34.380
And so basically, it's a targeted experience

00:43:34.380 --> 00:43:36.900
within the large language,

00:43:36.900 --> 00:43:39.780
within the ChatGPT ecosystem.

00:43:39.780 --> 00:43:41.900
It has access to also the OpenAI tools,

00:43:41.900 --> 00:43:44.220
like so OpenAI has the ability to do code interpreter

00:43:44.220 --> 00:43:47.300
and Dolly, and it can also hit the web browser.

00:43:47.300 --> 00:43:49.100
So you have access to everything.

00:43:49.100 --> 00:43:51.540
But the interesting thing to me is the fact

00:43:51.540 --> 00:43:52.660
that you can actually tie this thing

00:43:52.660 --> 00:43:54.220
to what are called actions.

00:43:54.220 --> 00:43:56.620
So March, I think of last year,

00:43:56.620 --> 00:43:57.660
they actually had this capability

00:43:57.660 --> 00:43:59.580
called plugins that they announced.

00:43:59.580 --> 00:44:02.500
And plugins have kind of faded to the background.

00:44:02.500 --> 00:44:04.540
I don't know if they're gonna deprecate them officially,

00:44:04.540 --> 00:44:07.100
but the basic gist with plugins is what was,

00:44:07.100 --> 00:44:09.900
you could turn that on and it can then call your API.

00:44:09.900 --> 00:44:11.100
And the cool thing about it was

00:44:11.100 --> 00:44:13.460
that it read your OpenAPI spec, right?

00:44:13.460 --> 00:44:16.580
So you write an OpenAPI spec, which is Swagger,

00:44:16.580 --> 00:44:18.300
if you're familiar with Swagger,

00:44:18.300 --> 00:44:20.100
and it basically defines what all the endpoints are,

00:44:20.100 --> 00:44:23.620
what the path is, what the inputs and outputs are,

00:44:23.620 --> 00:44:27.660
including all the classes or field level information

00:44:27.660 --> 00:44:29.220
and any constraints or what have you.

00:44:29.220 --> 00:44:32.660
So you can fully define your OpenAPI spec.

00:44:32.660 --> 00:44:34.340
It can then call that OpenAPI spec,

00:44:34.340 --> 00:44:36.820
and it's basically giving it tools.

00:44:36.820 --> 00:44:38.860
So like the example that they say in the documentation

00:44:38.860 --> 00:44:39.820
is get the weather, right?

00:44:39.820 --> 00:44:41.860
So if you say, what's the weather in Boston?

00:44:41.860 --> 00:44:43.860
Well, Chetchibt doesn't know the weather in Boston.

00:44:43.860 --> 00:44:45.140
All it knows how to do is call it,

00:44:45.140 --> 00:44:47.660
but you can call an API and it figures out

00:44:47.660 --> 00:44:49.460
how to call the API, get that information,

00:44:49.460 --> 00:44:51.620
and then it can use that to redisplay.

00:44:51.620 --> 00:44:54.060
And that's a very basic example.

00:44:54.060 --> 00:44:57.100
You can do way more complicated things than that,

00:44:57.100 --> 00:44:59.740
and it's pretty powerful.

00:44:59.740 --> 00:45:02.900
- Okay, that sounds really pretty awesome.

00:45:02.900 --> 00:45:05.800
I thought a lot about different things that I might build.

00:45:05.800 --> 00:45:10.740
Yeah, I guess on your blog post here,

00:45:10.740 --> 00:45:14.220
you've got some key benefits and you've got some risks.

00:45:14.220 --> 00:45:16.860
You maybe wanna talk a bit about that?

00:45:16.860 --> 00:45:20.140
- Yeah, so the first part with plugins that was wrong,

00:45:20.140 --> 00:45:21.580
that didn't work as well,

00:45:21.580 --> 00:45:25.180
is that there was no kind of overarching custom instruction

00:45:25.180 --> 00:45:28.140
that could actually teach it how to work with your plugin.

00:45:28.140 --> 00:45:30.420
So if you couldn't put it in the API spec,

00:45:30.420 --> 00:45:31.660
then you couldn't like integrate it

00:45:31.660 --> 00:45:35.020
with a bunch of other stuff or other capabilities, right?

00:45:35.020 --> 00:45:37.140
So the custom instruction is really a key thing

00:45:37.140 --> 00:45:39.460
for making these custom APIs strong.

00:45:39.460 --> 00:45:41.380
But one warning about the custom instruction,

00:45:41.380 --> 00:45:43.820
whatever you put in there, anybody can download, right?

00:45:43.820 --> 00:45:45.620
Not just the folks at OpenAI, anybody.

00:45:45.620 --> 00:45:48.020
Like basically there's GitHub projects

00:45:48.020 --> 00:45:49.880
where like thousands of these,

00:45:49.880 --> 00:45:54.500
the custom prompts that people have put into their GPT.

00:45:54.500 --> 00:45:56.220
And there are now knockoffs on GPT.

00:45:56.220 --> 00:45:59.900
So it's all kind of a mess right now in the OpenAI store.

00:45:59.900 --> 00:46:00.820
I'm sure they'll clean it up,

00:46:00.820 --> 00:46:03.780
but just recognize the custom instruction is not protected

00:46:03.780 --> 00:46:05.160
and neither is the knowledge.

00:46:05.160 --> 00:46:06.860
So if you upload a PDF,

00:46:06.860 --> 00:46:08.380
there have been people that have been figuring out

00:46:08.380 --> 00:46:10.060
how to like download those PDFs.

00:46:10.060 --> 00:46:13.020
And I think that that might be a solved problem now

00:46:13.020 --> 00:46:15.540
or they're working on it, but it's something to know.

00:46:15.540 --> 00:46:17.700
The other problem with plugins was

00:46:17.700 --> 00:46:19.000
I can get a plugin working,

00:46:19.000 --> 00:46:20.940
but if they didn't approve my plugin

00:46:20.940 --> 00:46:23.040
and put it in their plugin store,

00:46:23.040 --> 00:46:24.820
I couldn't share it with other people.

00:46:24.820 --> 00:46:27.940
The way it works now is I can actually make a GPT

00:46:27.940 --> 00:46:30.080
and I can give it to you and you can use it directly,

00:46:30.080 --> 00:46:33.140
even if it's not in the OpenAPI store or OpenAI store.

00:46:33.140 --> 00:46:35.460
It is super easy to get started.

00:46:35.460 --> 00:46:37.500
They have like a tool to like,

00:46:37.500 --> 00:46:39.160
help you generate your DALI picture.

00:46:39.160 --> 00:46:40.660
And actually you don't even have to figure out

00:46:40.660 --> 00:46:42.140
how to do the custom instructions yourself.

00:46:42.140 --> 00:46:44.940
You can just kind of chat that into existence.

00:46:44.940 --> 00:46:46.860
But the thing that I'm really excited about

00:46:46.860 --> 00:46:48.780
is that this is like free playing.

00:46:48.780 --> 00:46:52.380
Like you could do, so the hosting cost

00:46:52.380 --> 00:46:54.020
is basically all on the client side.

00:46:54.020 --> 00:46:56.500
You have to be a ChatGPT plus user right now

00:46:56.500 --> 00:46:58.100
to create these and use these.

00:46:58.100 --> 00:47:01.060
But the cool thing as a developer,

00:47:01.060 --> 00:47:02.540
I don't have to pay those API fees

00:47:02.540 --> 00:47:03.900
that we were talking about, right?

00:47:03.900 --> 00:47:06.180
And if I need to use GPT four,

00:47:06.180 --> 00:47:07.940
which I kind of do for my business right now,

00:47:07.940 --> 00:47:10.260
just because of how complicated it is,

00:47:10.260 --> 00:47:12.100
I don't have to pay those token fees

00:47:12.820 --> 00:47:16.820
for folks using my custom GPT at this moment.

00:47:16.820 --> 00:47:18.620
So, and then the risks go.

00:47:18.620 --> 00:47:21.460
- Where's like the billing or whatever you call it

00:47:21.460 --> 00:47:23.300
for the custom GPT lie?

00:47:23.300 --> 00:47:24.980
Is that in the person who's using it?

00:47:24.980 --> 00:47:26.540
Does it have to, it goes onto their account

00:47:26.540 --> 00:47:28.140
and whatever their account?

00:47:28.140 --> 00:47:29.420
- Yeah, so everyone just, you know,

00:47:29.420 --> 00:47:34.420
right now OpenAPI, OpenAI ChatGPT plus is $20 a month.

00:47:34.420 --> 00:47:36.980
And then there's a team's version,

00:47:36.980 --> 00:47:38.500
which I think is either 25 or 30,

00:47:38.500 --> 00:47:42.080
depending on the number of users or how you pay for it.

00:47:42.080 --> 00:47:45.220
And then, and that's the cost.

00:47:45.220 --> 00:47:48.420
So right now, if you wanna use custom GPTs,

00:47:48.420 --> 00:47:51.300
everyone needs to be a ChatGPT plus user.

00:47:51.300 --> 00:47:54.700
There's no extra cost based on usage or anything like that.

00:47:54.700 --> 00:47:59.340
In fact, there's talk about revenue sharing

00:47:59.340 --> 00:48:02.100
between OpenAI and developers of custom GPTs,

00:48:02.100 --> 00:48:03.780
but that has not come out yet

00:48:03.780 --> 00:48:05.740
as far as like what those details are.

00:48:05.740 --> 00:48:09.060
- It does have an app store feel to it, doesn't it?

00:48:09.060 --> 00:48:10.940
- Mm-hmm, yep, for sure, for sure.

00:48:10.940 --> 00:48:14.240
And then, but there's risks too, right?

00:48:14.240 --> 00:48:15.180
Obviously anybody can,

00:48:15.180 --> 00:48:18.220
there's already been like tons of copies up there.

00:48:18.220 --> 00:48:20.160
OpenAI, you know,

00:48:20.160 --> 00:48:23.200
they're looking for their business model too, right?

00:48:23.200 --> 00:48:27.080
So they could, if someone has a very successful custom GPT,

00:48:27.080 --> 00:48:29.020
it's well within their right to kind of add that

00:48:29.020 --> 00:48:30.460
to the base product as well.

00:48:30.460 --> 00:48:34.220
And then prompt injection is still a thing.

00:48:34.220 --> 00:48:36.140
So if you're doing anything in your actions

00:48:36.140 --> 00:48:37.580
that actually changes something,

00:48:37.580 --> 00:48:40.660
that is consequential is what they call it,

00:48:40.660 --> 00:48:42.660
you better think very carefully,

00:48:42.660 --> 00:48:45.380
like what's the worst thing that could happen, right?

00:48:45.380 --> 00:48:47.500
'Cause whatever the worst thing that could happen is,

00:48:47.500 --> 00:48:48.420
that's what's gonna happen.

00:48:48.420 --> 00:48:50.420
'Cause people can figure this stuff out

00:48:50.420 --> 00:48:52.780
and they can confuse the large language models

00:48:52.780 --> 00:48:54.820
into calling them, right?

00:48:54.820 --> 00:48:56.020
- The more valuable it is

00:48:56.020 --> 00:48:57.660
that they can make that thing happen,

00:48:57.660 --> 00:48:59.620
the more effort they're gonna put into it as well.

00:48:59.620 --> 00:49:02.060
Yeah, yeah, yeah. - For sure, for sure.

00:49:02.060 --> 00:49:04.540
And then just in general, you're, oh, go ahead.

00:49:04.540 --> 00:49:06.140
- I was gonna ask, do you think,

00:49:07.820 --> 00:49:10.900
it's easy to solve SQL injection

00:49:10.900 --> 00:49:15.460
and other forms of injection, at least in principle, right?

00:49:15.460 --> 00:49:17.080
There's a education problem,

00:49:17.080 --> 00:49:20.020
there's millions of people coming along as developers

00:49:20.020 --> 00:49:22.780
and they see some demo that says,

00:49:22.780 --> 00:49:26.380
the query is like this plus the name, wait a minute.

00:49:26.380 --> 00:49:31.960
So it kind of recreates itself through not total awareness,

00:49:31.960 --> 00:49:36.140
but there's a very clear thing you do to solve that,

00:49:36.140 --> 00:49:38.060
you use parameters, you don't concatenate strings

00:49:38.060 --> 00:49:40.140
with user input, problem solved.

00:49:40.140 --> 00:49:41.900
What about prompt injection though?

00:49:41.900 --> 00:49:46.420
It's so vague how these AIs know what to do

00:49:46.420 --> 00:49:47.420
in the first place.

00:49:47.420 --> 00:49:51.820
And so then how do you completely block that off?

00:49:51.820 --> 00:49:53.060
- Unsolved problem, right?

00:49:53.060 --> 00:49:56.020
Like, and I'm definitely stealing from Simon on this

00:49:56.020 --> 00:49:57.900
'cause I've heard him say it on a few podcasts

00:49:57.900 --> 00:50:01.640
is just basically there's no solution as far as we know.

00:50:01.640 --> 00:50:03.100
So you have to design,

00:50:04.060 --> 00:50:07.140
and there's no solution to the hallucination problem either

00:50:07.140 --> 00:50:08.980
'cause that's a feature, right?

00:50:08.980 --> 00:50:11.240
That's actually what the thing is supposed to do.

00:50:11.240 --> 00:50:12.660
So when you're building these systems,

00:50:12.660 --> 00:50:15.340
you have to recognize those two facts

00:50:15.340 --> 00:50:18.860
along with some other facts that really limit

00:50:18.860 --> 00:50:20.700
what you can build with these things.

00:50:20.700 --> 00:50:24.540
- So you shouldn't use it for like legal briefs,

00:50:24.540 --> 00:50:26.500
is that what you're saying?

00:50:26.500 --> 00:50:28.620
- Well, once again, I think these things

00:50:28.620 --> 00:50:31.260
are great collaborative tools, right?

00:50:31.260 --> 00:50:32.260
The human in the loop,

00:50:32.260 --> 00:50:33.500
and that's everything that I'm building, right?

00:50:33.500 --> 00:50:34.900
So all the stuff that I'm building

00:50:34.900 --> 00:50:36.820
is assuming that the human's in the loop

00:50:36.820 --> 00:50:41.060
and what I'm trying to do is augment and amplify expertise.

00:50:41.060 --> 00:50:44.460
I'm building tools for people that know about genomics

00:50:44.460 --> 00:50:46.740
and cancer and how to help cancer patients.

00:50:46.740 --> 00:50:48.260
I'm not designing it for cancer patients

00:50:48.260 --> 00:50:50.660
who are gonna go operate on themselves, right?

00:50:50.660 --> 00:50:52.140
Like that's not the goal.

00:50:52.140 --> 00:50:56.040
The idea is there's a lot of information.

00:50:56.040 --> 00:50:59.300
These tools are super valuable

00:50:59.300 --> 00:51:03.300
from like synthesizing a variety of info,

00:51:03.300 --> 00:51:05.940
but you still need to look at the underlying citations

00:51:05.940 --> 00:51:08.340
and ChatGPT by itself can't give you citations.

00:51:08.340 --> 00:51:10.460
Like it'll make some up.

00:51:10.460 --> 00:51:12.540
It'll say, oh, I think there's probably a Wikipedia page

00:51:12.540 --> 00:51:15.420
with this link, but you actually have to,

00:51:15.420 --> 00:51:17.900
you definitely have to have an outside tool,

00:51:17.900 --> 00:51:20.500
either the web, Bing, which is, I would say,

00:51:20.500 --> 00:51:23.100
subpar for a lot of use cases,

00:51:23.100 --> 00:51:24.340
or you have to have actions

00:51:24.340 --> 00:51:26.380
that can actually bring back references

00:51:26.380 --> 00:51:27.300
and give you those links.

00:51:27.300 --> 00:51:28.500
And then the expert will then say,

00:51:28.500 --> 00:51:30.300
oh, okay, great, thanks for synthesizing this,

00:51:30.300 --> 00:51:31.780
giving me this info.

00:51:31.780 --> 00:51:33.580
Let me go validate this myself, right?

00:51:33.580 --> 00:51:35.900
Go click on the link and go validate it.

00:51:35.900 --> 00:51:37.980
And that's really, I think that's really the sweet spot

00:51:37.980 --> 00:51:40.020
for these things, at least for the near future.

00:51:40.020 --> 00:51:42.420
- Yeah, don't ask it for the answer,

00:51:42.420 --> 00:51:45.020
ask it to help you come up with the answer, right?

00:51:45.020 --> 00:51:45.980
- Exactly right.

00:51:45.980 --> 00:51:49.420
And then have it criticize you when you do have something,

00:51:49.420 --> 00:51:50.620
'cause then it'll do a great job

00:51:50.620 --> 00:51:53.000
of telling you everything you've done wrong.

00:51:53.000 --> 00:51:55.540
- I'm feeling too good about myself.

00:51:55.540 --> 00:51:57.220
I need you to insult me a lot.

00:51:57.220 --> 00:51:58.060
Let's get going.

00:51:58.060 --> 00:52:02.300
All right, speaking of talking about ourselves,

00:52:02.300 --> 00:52:04.780
you've got this project called PyPI GPT.

00:52:04.780 --> 00:52:06.220
What's this about?

00:52:06.220 --> 00:52:08.140
- Yeah, so I really wanted to tell people

00:52:08.140 --> 00:52:11.180
that FastAPI and Pydantic, 'cause Python,

00:52:11.180 --> 00:52:12.100
like we were saying earlier,

00:52:12.100 --> 00:52:13.660
I don't know if it was on the call or not,

00:52:13.660 --> 00:52:16.900
but Python is the winning language, right?

00:52:16.900 --> 00:52:19.900
And I think FastAPI and Pydantic are the winning libraries

00:52:19.900 --> 00:52:22.140
in their respective fields, and they're great.

00:52:22.140 --> 00:52:23.740
And they're perfect for this space,

00:52:23.740 --> 00:52:25.820
because you need an open API spec.

00:52:27.100 --> 00:52:28.980
English is the new programming language, right?

00:52:28.980 --> 00:52:31.900
So Andrej Koparthe, who used to work at Tesla

00:52:31.900 --> 00:52:34.220
and now works at OpenAI, has this pinned tweet

00:52:34.220 --> 00:52:35.180
where he's basically like,

00:52:35.180 --> 00:52:36.820
"English is the hottest programming language,"

00:52:36.820 --> 00:52:37.820
or something like that.

00:52:37.820 --> 00:52:39.340
And that's really the truth,

00:52:39.340 --> 00:52:40.740
'cause even in this space

00:52:40.740 --> 00:52:43.380
where I'm building an open API spec,

00:52:43.380 --> 00:52:46.980
99% of the work is thinking about

00:52:46.980 --> 00:52:48.380
the description of the endpoints

00:52:48.380 --> 00:52:50.780
or the description of the fields

00:52:50.780 --> 00:52:54.620
or codifying the constraints on different fields.

00:52:54.620 --> 00:52:56.780
You can use these greater thans and less thans

00:52:56.780 --> 00:52:59.740
and regexes to describe it.

00:52:59.740 --> 00:53:01.100
And so what I did was I said,

00:53:01.100 --> 00:53:03.860
"Okay, let's build this thing in FastAPI,"

00:53:03.860 --> 00:53:05.740
just to get an example out for folks.

00:53:05.740 --> 00:53:08.460
And then I turned it on.

00:53:08.460 --> 00:53:11.700
I actually use ngrok as my service layer,

00:53:11.700 --> 00:53:13.940
'cause you have to have HTTPS to make this thing work.

00:53:13.940 --> 00:53:15.980
So I figured out- - Ngrok is so good.

00:53:15.980 --> 00:53:17.900
- Yep, yep. - Yeah, yeah.

00:53:17.900 --> 00:53:20.260
- I turned that on with an nginx thing in front of it.

00:53:20.260 --> 00:53:22.580
So this library, to actually use it,

00:53:22.580 --> 00:53:25.620
you'll have to actually set that stuff up yourself.

00:53:25.620 --> 00:53:26.580
You have to download it.

00:53:26.580 --> 00:53:27.420
You have to run it.

00:53:27.420 --> 00:53:29.900
You have to either get it on a server with HTTPS,

00:53:29.900 --> 00:53:31.500
with Let's Encrypt or something.

00:53:31.500 --> 00:53:33.860
But then once you've turned it on,

00:53:33.860 --> 00:53:35.780
then you can actually see how it generates

00:53:35.780 --> 00:53:39.780
the open API spec, how to configure the GPT.

00:53:39.780 --> 00:53:41.300
I didn't do much work with regards

00:53:41.300 --> 00:53:43.140
to the custom instructions that I came up with.

00:53:43.140 --> 00:53:45.180
I just said, "Hey, call my API, figure it out."

00:53:45.180 --> 00:53:46.140
And it does.

00:53:46.140 --> 00:53:48.060
And so what this GPT does is it basically says,

00:53:48.060 --> 00:53:49.860
"Okay, given a package name and a version number,

00:53:49.860 --> 00:53:51.700
it's gonna go and grab this data

00:53:51.700 --> 00:53:53.260
from the SQLite database that I found

00:53:53.260 --> 00:53:55.540
that has this information and then bring it back to you."

00:53:55.540 --> 00:53:57.900
It's the least interesting GPT I could come up with, I guess.

00:53:57.900 --> 00:54:00.740
But it shows kind of the mechanics, right?

00:54:00.740 --> 00:54:04.940
The mechanics of setting up the servers

00:54:04.940 --> 00:54:08.140
and the application within FastAPI,

00:54:08.140 --> 00:54:12.380
the kind of the little bits they have to flip

00:54:12.380 --> 00:54:17.380
to make sure that OpenAI can understand your OpenAPI spec.

00:54:17.380 --> 00:54:21.620
I bumble through OpenAI and OpenAPI all the time.

00:54:21.620 --> 00:54:23.580
And make sure that they can talk to each other.

00:54:23.580 --> 00:54:25.300
And then it will then do the right thing

00:54:25.300 --> 00:54:28.700
and call your server and bring the answers back.

00:54:28.700 --> 00:54:32.100
And there's a bunch of little flags and information

00:54:32.100 --> 00:54:33.940
you need to know about actions

00:54:33.940 --> 00:54:37.300
that are on the OpenAPI documentation.

00:54:37.300 --> 00:54:39.340
And so I tried to pull that all together

00:54:39.340 --> 00:54:42.180
into one simple little project for people to look at.

00:54:42.180 --> 00:54:44.300
- It's cool.

00:54:44.300 --> 00:54:45.140
So you can ask it questions like,

00:54:45.140 --> 00:54:47.740
"Tell me about FastAPI, this version."

00:54:47.740 --> 00:54:48.980
And it'll come back and do it.

00:54:48.980 --> 00:54:50.100
- Correct.

00:54:50.100 --> 00:54:51.700
I was hoping to do something a little better like,

00:54:51.700 --> 00:54:53.420
"Hey, here's my requirements file."

00:54:53.420 --> 00:54:56.860
And go, "Tell me, am I on the latest version

00:54:56.860 --> 00:54:58.780
of everything or whatever?"

00:54:58.780 --> 00:54:59.860
Something more interesting.

00:54:59.860 --> 00:55:00.980
I just didn't have time.

00:55:00.980 --> 00:55:01.820
So-

00:55:01.820 --> 00:55:03.420
- Can you ask it questions such as,

00:55:03.420 --> 00:55:05.380
"What's the difference between this version

00:55:05.380 --> 00:55:07.060
and that version?"

00:55:07.060 --> 00:55:08.940
- You could, if that information is in the database.

00:55:08.940 --> 00:55:10.620
I actually don't know if it is.

00:55:10.620 --> 00:55:14.460
So, and then obviously you could also hit the PyPI server.

00:55:14.460 --> 00:55:15.300
And I didn't do that.

00:55:15.300 --> 00:55:18.940
I just wanted to, I don't wanna be hitting anybody's server

00:55:18.940 --> 00:55:20.500
indiscriminately at this point.

00:55:20.540 --> 00:55:23.620
But that would be a great use case, right?

00:55:23.620 --> 00:55:25.260
So like someone could take this

00:55:25.260 --> 00:55:29.180
and certainly add some capabilities.

00:55:29.180 --> 00:55:32.980
The thing that is valuable that I'm trying to showcase

00:55:32.980 --> 00:55:36.340
is the fact that ChatGPT and large language models,

00:55:36.340 --> 00:55:39.340
while they do have the world's information kind of compressed

00:55:39.340 --> 00:55:43.460
at a point in time, they are still not a database, right?

00:55:43.460 --> 00:55:45.940
They don't do well when you're basically trying

00:55:45.940 --> 00:55:48.260
to make sure you have a comprehensive query

00:55:48.260 --> 00:55:49.820
and you've brought back all the information.

00:55:49.820 --> 00:55:50.700
And they're also not good

00:55:50.700 --> 00:55:52.780
from like a up-to-date perspective, right?

00:55:52.780 --> 00:55:53.820
There's a cutoff date.

00:55:53.820 --> 00:55:56.180
Thankfully, they finally updated that recently.

00:55:56.180 --> 00:55:58.300
I think it's now April of 2023,

00:55:58.300 --> 00:56:01.220
but at some point it just doesn't know about newer things.

00:56:01.220 --> 00:56:05.780
And so a GPT is a really interesting way of doing that.

00:56:05.780 --> 00:56:07.380
Like I'm gonna put it out in the universe

00:56:07.380 --> 00:56:08.580
and hopefully someone will do it.

00:56:08.580 --> 00:56:11.220
Make me a modern Python GPT,

00:56:11.220 --> 00:56:14.380
which is basically like get the new version of Pydantic

00:56:14.380 --> 00:56:16.340
and Polars and a few other libraries

00:56:16.340 --> 00:56:18.620
that ChatGPT does a bad job at,

00:56:18.620 --> 00:56:21.540
just because they're in underactive development

00:56:21.540 --> 00:56:24.580
during the time that ChatGPT was getting trained.

00:56:24.580 --> 00:56:28.260
So that's the perfect use cases

00:56:28.260 --> 00:56:31.060
for these types of custom GPTs with knowledge

00:56:31.060 --> 00:56:34.140
in a PDF file or an API backing it up.

00:56:34.140 --> 00:56:37.740
- I think there's a ton of value in being able

00:56:37.740 --> 00:56:40.580
to feed a little bit of your information,

00:56:40.580 --> 00:56:43.660
some of your documents or your code repository

00:56:43.660 --> 00:56:46.500
or something to a GPT

00:56:46.500 --> 00:56:48.340
and then be able to ask it questions.

00:56:48.900 --> 00:56:49.740
- Exactly right.

00:56:49.740 --> 00:56:50.580
- Got it right.

00:56:50.580 --> 00:56:51.420
- Yeah.

00:56:52.260 --> 00:56:54.820
Like, you know, tell me about the security vulnerabilities

00:56:54.820 --> 00:56:56.020
that you see in the code.

00:56:56.020 --> 00:56:59.980
Like, is there anywhere where I'm missing some test

00:56:59.980 --> 00:57:03.900
or I'm calling a function in a way that's known to be bad?

00:57:03.900 --> 00:57:07.260
And you know, like that kind of stuff is really tricky,

00:57:07.260 --> 00:57:09.700
but it's also tricky because it doesn't,

00:57:09.700 --> 00:57:11.340
even if you paste in a little bit of code,

00:57:11.340 --> 00:57:12.940
it's not the whole project, right?

00:57:12.940 --> 00:57:14.820
So, you know, to put a little bit more in there

00:57:14.820 --> 00:57:16.460
is pretty awesome.

00:57:16.460 --> 00:57:17.300
- Yeah, for sure.

00:57:17.300 --> 00:57:20.140
Yeah, being able to give it, you know,

00:57:20.140 --> 00:57:22.940
all the code from some of these code repositories, right?

00:57:22.940 --> 00:57:25.140
Like, and bringing back the relevant information.

00:57:25.140 --> 00:57:26.980
So I think there is a kind of this race.

00:57:26.980 --> 00:57:28.460
There's gonna be other, you know,

00:57:28.460 --> 00:57:31.540
there's another cool project called SourceGraph and Cody

00:57:31.540 --> 00:57:33.740
that we can talk about that will, you know,

00:57:33.740 --> 00:57:36.820
run on your local server and basically indexes

00:57:36.820 --> 00:57:39.340
your code base and will bring back relevant snippets

00:57:39.340 --> 00:57:42.380
from your code base and answer questions kind of in context.

00:57:42.380 --> 00:57:44.300
And, you know, long-term,

00:57:44.940 --> 00:57:46.380
and then there's a new project,

00:57:46.380 --> 00:57:48.260
or I don't know how new, Codeium,

00:57:48.260 --> 00:57:52.540
they had a new paper where they talked about flow engineering

00:57:52.540 --> 00:57:55.260
and flow engineering is just basically that same concept

00:57:55.260 --> 00:57:58.660
of the human in the loop with the LLM, with the code,

00:57:58.660 --> 00:58:00.940
that's the magic combination of kind of those people,

00:58:00.940 --> 00:58:03.740
those entities kind of iterating with each other.

00:58:03.740 --> 00:58:07.740
So, and I think these, you know,

00:58:07.740 --> 00:58:09.060
these tools are definitely gonna evolve

00:58:09.060 --> 00:58:13.580
and you really wanna have the ability to have access

00:58:13.580 --> 00:58:15.900
to your specific information

00:58:15.900 --> 00:58:17.660
to answer your specific questions.

00:58:17.660 --> 00:58:19.880
- Cody is new to me.

00:58:19.880 --> 00:58:22.460
- Yeah. - Cody.dev.

00:58:22.460 --> 00:58:27.140
And it's little subtitle or whatever is,

00:58:27.140 --> 00:58:30.140
"Cody is a coding assistant that uses AI.

00:58:30.140 --> 00:58:31.780
Understand your code base," right?

00:58:31.780 --> 00:58:33.720
It was saying, what was it?

00:58:33.720 --> 00:58:38.220
It was about your entire code base,

00:58:38.220 --> 00:58:40.140
APIs, implementations, and idioms.

00:58:40.140 --> 00:58:42.340
Like that's kind of what I was suggesting,

00:58:42.340 --> 00:58:43.620
at least for code, right?

00:58:43.620 --> 00:58:45.160
- Yeah, and Sourcegraph,

00:58:45.160 --> 00:58:50.160
those folks really understand code indexing and searching.

00:58:50.160 --> 00:58:51.660
Like that's what the first product was.

00:58:51.660 --> 00:58:52.660
They were kind of just teed up,

00:58:52.660 --> 00:58:54.620
ready for this large language model moment.

00:58:54.620 --> 00:58:57.480
And then they said, "Oh, let's just put Cody on top of that."

00:58:57.480 --> 00:58:59.620
So this thing will run, it will understand your code

00:58:59.620 --> 00:59:01.820
and it will kind of bring things together for you.

00:59:01.820 --> 00:59:04.460
So, and these folks do podcasts all the time.

00:59:04.460 --> 00:59:06.500
I'd reach out to them.

00:59:06.500 --> 00:59:08.060
- Okay, yeah, interesting.

00:59:08.060 --> 00:59:09.500
It's quite neat looking.

00:59:09.500 --> 00:59:10.700
I think I'm gonna give it a try.

00:59:10.700 --> 00:59:14.420
It both plugs into PyCharm and VS Code.

00:59:14.420 --> 00:59:15.520
So that's pretty neat.

00:59:15.520 --> 00:59:18.580
All right, well.

00:59:18.580 --> 00:59:20.860
- Very cool.

00:59:20.860 --> 00:59:21.700
- Let's see.

00:59:21.700 --> 00:59:23.460
So I think we're starting to get a little bit short

00:59:23.460 --> 00:59:25.860
on time here, but for people who want to play

00:59:25.860 --> 00:59:28.980
with the PyPI GPT, maybe as an example,

00:59:28.980 --> 00:59:31.440
to just cut the readme and it's easy to get from there,

00:59:31.440 --> 00:59:33.500
what do you need to tell them?

00:59:33.500 --> 00:59:35.200
- Yeah, I put a make file in there,

00:59:35.200 --> 00:59:36.600
so you know exactly like the steps

00:59:36.600 --> 00:59:38.840
to kind of make the environment, download the files,

00:59:38.840 --> 00:59:42.260
and just ping me, follow me on Twitter,

00:59:42.260 --> 00:59:45.420
I'm or, and ping me if you need anything there.

00:59:45.420 --> 00:59:48.580
I'm also on LinkedIn and GitHub, right?

00:59:48.580 --> 00:59:52.340
So you can certainly reach out if you have any challenges.

00:59:52.340 --> 00:59:53.900
- Excellent.

00:59:53.900 --> 00:59:56.060
All right, well, yeah, go ahead.

00:59:56.060 --> 00:59:58.820
- Yeah, I'd say like the last thing that I'm, you know,

00:59:58.820 --> 01:00:00.820
folks that are actually in the medical space, right?

01:00:00.820 --> 01:00:03.620
So the thing that I'm working on right now actively

01:00:03.620 --> 01:00:07.140
is how to integrate this thing with our knowledge base,

01:00:07.140 --> 01:00:07.980
right?

01:00:07.980 --> 01:00:10.660
So I've built a database of hand curated trials,

01:00:10.660 --> 01:00:13.860
hand curated therapies and other information.

01:00:13.860 --> 01:00:17.100
I've, you know, built it so that, you know,

01:00:17.100 --> 01:00:19.380
my custom GPT can actually work with that.

01:00:19.380 --> 01:00:21.340
I've come up with some, I'd say novel,

01:00:21.340 --> 01:00:22.820
I always haven't seen anybody else

01:00:22.820 --> 01:00:24.380
and I haven't seen any research

01:00:24.380 --> 01:00:28.380
approaching things the same way I am

01:00:28.380 --> 01:00:30.180
that handles some of the other challenges

01:00:30.180 --> 01:00:31.140
that are out there, right?

01:00:31.140 --> 01:00:34.220
So for instance, the context window is a challenge.

01:00:34.220 --> 01:00:36.500
So the context window is the amount of text

01:00:36.500 --> 01:00:40.700
that's in there and how it gets processed.

01:00:40.700 --> 01:00:45.140
If you're making decisions and you're changing course,

01:00:45.140 --> 01:00:48.480
the chatbot will lose track of those changes, right?

01:00:48.480 --> 01:00:50.100
So if you're, you know, experimenting

01:00:50.100 --> 01:00:52.300
or going down one path of inquiry

01:00:52.300 --> 01:00:54.620
and then you switch to another path,

01:00:54.620 --> 01:00:57.860
it can get confused and forget that you switched paths.

01:00:57.860 --> 01:00:58.700
And it can also--

01:00:58.700 --> 01:01:00.700
- It's not a database to hold all that information.

01:01:00.700 --> 01:01:03.780
You're like, well, it forgot the last three things,

01:01:03.780 --> 01:01:04.900
the first three things you told it.

01:01:04.900 --> 01:01:06.540
It only knows four and you think it knows seven

01:01:06.540 --> 01:01:09.300
and it's getting incomplete, right?

01:01:09.300 --> 01:01:10.140
- Yep.

01:01:10.140 --> 01:01:12.260
And, you know, one of the key things is

01:01:12.260 --> 01:01:13.420
you actually got to,

01:01:13.420 --> 01:01:15.780
you actually want it to forget some things as well, right?

01:01:15.780 --> 01:01:18.300
So those are all interesting challenges.

01:01:18.300 --> 01:01:22.380
And I'm actually working with these custom GPTs

01:01:22.380 --> 01:01:26.500
to kind of change the way that the collaboration works

01:01:26.500 --> 01:01:29.860
between the human, the expert,

01:01:29.860 --> 01:01:32.460
the large language model or the assistant

01:01:32.460 --> 01:01:35.900
and my backend, my actual retrieval model,

01:01:35.900 --> 01:01:38.580
the API that's actually doing stuff.

01:01:38.580 --> 01:01:42.620
- So are researchers and MDs and PhDs

01:01:42.620 --> 01:01:44.700
at your company talking about this thing

01:01:44.700 --> 01:01:46.780
and making use of it?

01:01:46.780 --> 01:01:47.620
- For sure.

01:01:47.620 --> 01:01:49.860
We're, yeah, I mean, we're in active development right now.

01:01:49.860 --> 01:01:51.140
We have a few key opinion leaders

01:01:51.140 --> 01:01:53.660
that are working with us and collaborating with us,

01:01:53.660 --> 01:01:54.940
but we're always looking for more folks

01:01:54.940 --> 01:01:57.620
that are in the field that actually are.

01:01:57.620 --> 01:02:00.980
And right now you need kind of the cutting edge people

01:02:00.980 --> 01:02:04.100
'cause this stuff's not ready for prime time.

01:02:04.100 --> 01:02:07.420
Clinical decision support is a really hard problem.

01:02:07.420 --> 01:02:09.940
And, but we need the folks that are,

01:02:09.940 --> 01:02:11.220
that want to get ahead of it.

01:02:11.220 --> 01:02:13.060
'Cause we know that there are doctors

01:02:13.060 --> 01:02:14.580
and there are patients that are asking

01:02:14.580 --> 01:02:15.940
ChatGPT questions right now.

01:02:15.940 --> 01:02:18.420
And even if it says I'm not a medical expert, blah, blah,

01:02:18.420 --> 01:02:19.380
blah.

01:02:19.380 --> 01:02:20.580
And at the end of the day,

01:02:20.580 --> 01:02:22.380
we actually don't have enough doctors, right?

01:02:22.380 --> 01:02:23.900
That's the other scary thing

01:02:23.900 --> 01:02:26.860
is we don't have enough doctors, patients want answers.

01:02:26.860 --> 01:02:29.700
How do we build solutions that can allow this expertise

01:02:29.700 --> 01:02:33.740
to get more democratized and more into the folks hands?

01:02:33.740 --> 01:02:37.500
And I'm hoping our tool along with

01:02:37.500 --> 01:02:39.700
these large language models can help

01:02:39.700 --> 01:02:41.660
relieve some of that burden.

01:02:41.660 --> 01:02:46.660
- Well, it might not be as 100% accurate, 100% precise,

01:02:46.660 --> 01:02:49.460
but neither are doctors, right?

01:02:49.460 --> 01:02:51.060
They get stuff wrong.

01:02:51.060 --> 01:02:54.660
You just need to be in the realm of as good as a doctor.

01:02:54.660 --> 01:02:59.140
You don't need to be completely without making a mistake.

01:02:59.140 --> 01:03:01.860
And that's, I think, a challenge

01:03:01.860 --> 01:03:05.940
that we're just gonna have to get used to in general, right?

01:03:05.940 --> 01:03:08.740
I joked about the legal brief thing

01:03:08.740 --> 01:03:11.460
'cause someone got in trouble for submitting a brief

01:03:11.460 --> 01:03:14.140
that hallucinations.

01:03:14.140 --> 01:03:15.780
And there's certain circumstances

01:03:15.780 --> 01:03:17.220
where maybe it's just not acceptable,

01:03:17.220 --> 01:03:21.700
but AI self-driven cars, people crash,

01:03:21.700 --> 01:03:24.620
but that's like a human mistake.

01:03:24.620 --> 01:03:26.140
But when a machine makes it,

01:03:26.140 --> 01:03:29.060
it's a pre-programmed, predetermined mistake.

01:03:29.060 --> 01:03:29.900
Something like that.

01:03:29.900 --> 01:03:33.060
It doesn't feel the same as if the machine made a mistake.

01:03:33.060 --> 01:03:35.300
So if a machine makes a recommendation,

01:03:35.300 --> 01:03:38.220
like you need this cancer treatment or you're fine,

01:03:38.220 --> 01:03:40.140
you don't need it and it was wrong,

01:03:40.140 --> 01:03:43.900
people are not gonna be as forgiving.

01:03:43.900 --> 01:03:46.220
But it doesn't mean there's not value

01:03:46.220 --> 01:03:49.020
to be gained from systems that can help you, right?

01:03:49.020 --> 01:03:51.260
- I always appreciate those machine learning papers

01:03:51.260 --> 01:03:53.940
that'll show the tracking of over time

01:03:53.940 --> 01:03:55.900
of how the models have gotten better and better.

01:03:55.900 --> 01:03:56.980
And they put the human in there

01:03:56.980 --> 01:03:57.940
and then you can see that the human

01:03:57.940 --> 01:04:01.620
has already gotten eclipsed by the models.

01:04:01.620 --> 01:04:02.860
And that's a specific problem, right?

01:04:02.860 --> 01:04:05.100
'Cause it's not, and also recognizing

01:04:05.100 --> 01:04:07.540
that a lot of this stuff, these models that are doing tasks

01:04:07.540 --> 01:04:08.900
are doing one specific task.

01:04:08.900 --> 01:04:09.980
They're not doing a whole job.

01:04:09.980 --> 01:04:13.100
They're not doing an end-to-end process.

01:04:13.100 --> 01:04:14.780
They're answering a medical question

01:04:14.780 --> 01:04:16.580
or they're looking at an image

01:04:16.580 --> 01:04:19.180
and finding all the cats or whatever it's supposed to do.

01:04:19.180 --> 01:04:23.380
So, and to your point though,

01:04:23.380 --> 01:04:26.100
humans aren't perfect at these tasks either.

01:04:26.100 --> 01:04:29.260
- Yeah, I think mostly people are gonna be using

01:04:29.260 --> 01:04:31.220
this kind of stuff to help them

01:04:31.220 --> 01:04:33.140
come up with these answers, right?

01:04:33.140 --> 01:04:35.820
My weird Amazon description example

01:04:35.820 --> 01:04:39.180
is gonna be the edge case, not the go-to like.

01:04:39.180 --> 01:04:40.500
- Agreed, agreed.

01:04:40.500 --> 01:04:42.900
- Yeah, you came in, you spoke to the chat bot,

01:04:42.900 --> 01:04:45.140
here's your diagnosis, have a good day, right?

01:04:45.140 --> 01:04:47.740
Not so much, more like, I need some help

01:04:47.740 --> 01:04:48.580
thinking through this.

01:04:48.580 --> 01:04:52.660
What are some studies that have addressed this, right?

01:04:52.660 --> 01:04:54.300
And like those kinds of questions.

01:04:54.300 --> 01:04:57.500
- Yep, and I hesitate to say it's just a better search

01:04:57.500 --> 01:05:00.100
engine 'cause I actually think it's got way more potential

01:05:00.100 --> 01:05:00.940
than that.

01:05:00.940 --> 01:05:01.780
- I agree.

01:05:01.780 --> 01:05:05.980
- It's a conversation, it can iterate back and forth.

01:05:05.980 --> 01:05:07.540
And what I'm actually trying to do

01:05:07.540 --> 01:05:09.820
is build some state into it, right?

01:05:09.820 --> 01:05:14.140
Some structured way of kind of remembering

01:05:14.140 --> 01:05:18.180
what the conversation was and using a lot of the techniques

01:05:18.180 --> 01:05:19.580
that these large language models are good at

01:05:19.580 --> 01:05:22.060
to actually to make that actually happen.

01:05:22.060 --> 01:05:24.340
And so that you can actually build a system

01:05:24.340 --> 01:05:27.580
so that the human and the assistant and the backend

01:05:27.580 --> 01:05:31.460
all kind of know what the other party is thinking about

01:05:31.460 --> 01:05:33.060
and that they all work together.

01:05:33.060 --> 01:05:35.300
- Nice.

01:05:35.300 --> 01:05:40.300
For your genomics custom GPT thing

01:05:40.300 --> 01:05:42.300
that you're making internally,

01:05:42.300 --> 01:05:45.020
is that gonna become a product eventually

01:05:45.020 --> 01:05:46.180
if other people are interested?

01:05:46.180 --> 01:05:48.220
Is there some way they can keep tabs on it

01:05:48.220 --> 01:05:49.660
or is it just internal only?

01:05:49.660 --> 01:05:51.460
- Definitely reach out to me.

01:05:51.460 --> 01:05:53.420
So we're building different versions of GPTs.

01:05:53.420 --> 01:05:55.700
Like we're gonna have a GPT for our curation team

01:05:55.700 --> 01:05:57.500
that curates knowledge.

01:05:57.500 --> 01:05:59.300
And we're building a GPT that,

01:05:59.300 --> 01:06:01.300
my hope is that it will go to physicians,

01:06:01.300 --> 01:06:03.780
to oncologists and genomic counselors

01:06:03.780 --> 01:06:06.500
and other providers that could actually use this thing.

01:06:06.500 --> 01:06:12.020
Eventually, if it becomes robust enough and stable enough

01:06:12.020 --> 01:06:14.300
and I don't feel like we're doing a disservice,

01:06:14.300 --> 01:06:15.980
we could certainly make a version of that available

01:06:15.980 --> 01:06:17.100
for cancer patients as well.

01:06:17.100 --> 01:06:18.660
I would, I'd love to have that.

01:06:18.660 --> 01:06:21.420
I just wanna make sure that it's done in a responsible way.

01:06:21.820 --> 01:06:23.420
- Yeah, absolutely.

01:06:23.420 --> 01:06:26.540
Well, I honestly hope that you actually do such a good job

01:06:26.540 --> 01:06:28.660
that we don't have to have cancer research anymore,

01:06:28.660 --> 01:06:29.620
but that's-

01:06:29.620 --> 01:06:33.620
- That's really the end goal.

01:06:33.620 --> 01:06:35.140
Yes, that is definitely the end goal.

01:06:35.140 --> 01:06:36.700
And that's really exciting too.

01:06:36.700 --> 01:06:39.380
So is that the new drugs that are coming out,

01:06:39.380 --> 01:06:41.580
new treatments that are coming out.

01:06:41.580 --> 01:06:44.860
It's really just about making sure people are aware of it,

01:06:44.860 --> 01:06:46.620
making sure that they're getting the genetic testing

01:06:46.620 --> 01:06:47.460
that they need, right?

01:06:47.460 --> 01:06:49.900
So if you have a loved one that has,

01:06:50.860 --> 01:06:52.180
unfortunately has cancer,

01:06:52.180 --> 01:06:53.940
make sure that they're at least asking their doctor

01:06:53.940 --> 01:06:55.340
the question about genomic testing

01:06:55.340 --> 01:06:56.980
to make sure that they're getting

01:06:56.980 --> 01:06:58.980
the best possible treatment.

01:06:58.980 --> 01:07:00.180
- Sure.

01:07:00.180 --> 01:07:01.100
Sounds good.

01:07:01.100 --> 01:07:04.740
All right, well, quickly, before we get out of here,

01:07:04.740 --> 01:07:07.900
recommendation on some libraries,

01:07:07.900 --> 01:07:10.740
some project that maybe we haven't talked about yet.

01:07:10.740 --> 01:07:11.580
Something you could possibly,

01:07:11.580 --> 01:07:13.340
like, oh, this would be awesome.

01:07:13.340 --> 01:07:14.980
- Yeah, well, we ran out of time.

01:07:14.980 --> 01:07:16.940
I was gonna talk about some of these pedantic projects.

01:07:16.940 --> 01:07:19.940
So there's Marvin Instructor and Outlines.

01:07:19.940 --> 01:07:22.900
So folks should definitely look at those.

01:07:22.900 --> 01:07:24.500
So basically what you do is you take,

01:07:24.500 --> 01:07:26.740
you can describe stuff as pedantic,

01:07:26.740 --> 01:07:28.140
and then it'll actually just extract it

01:07:28.140 --> 01:07:30.140
right into that Pydantic model for you.

01:07:30.140 --> 01:07:35.220
So Marvin and Outlines and Instructor.

01:07:35.220 --> 01:07:37.220
So check those guys out, they're awesome.

01:07:37.220 --> 01:07:40.020
And then the other one that I actually had teed up

01:07:40.020 --> 01:07:41.140
was Visicalc.

01:07:41.140 --> 01:07:44.460
So Visicalc is like this crazy command line tool.

01:07:44.460 --> 01:07:45.300
It's awesome.

01:07:45.300 --> 01:07:48.060
Like you can basically look at giant CSV files

01:07:48.060 --> 01:07:49.020
all on the command line.

01:07:49.020 --> 01:07:51.340
It has like these hotkeys that you can do.

01:07:51.340 --> 01:07:52.180
- Okay.

01:07:52.180 --> 01:07:54.900
- And it, sorry, not visit count, visit data.

01:07:54.900 --> 01:07:55.820
- Visit data, okay.

01:07:55.820 --> 01:07:56.820
- Visit data.

01:07:56.820 --> 01:07:58.580
And so basically it's just,

01:07:58.580 --> 01:08:00.740
it's basically Excel inside your terminal.

01:08:00.740 --> 01:08:05.620
And this was before Rich and the Textual project.

01:08:05.620 --> 01:08:07.260
And it was just like, it was kind of mind blowing,

01:08:07.260 --> 01:08:09.180
all the stuff that this person was able to figure out

01:08:09.180 --> 01:08:10.260
how to make work.

01:08:10.260 --> 01:08:13.620
- Yeah, that's super amazing.

01:08:13.620 --> 01:08:15.140
I just wanted to give a shout out one more thing,

01:08:15.140 --> 01:08:17.740
'cause your visit data reminded me of something

01:08:17.740 --> 01:08:20.660
I just came across called Btop.

01:08:20.660 --> 01:08:21.500
- Yep, yep, yep.

01:08:21.500 --> 01:08:22.420
- If people have servers out there

01:08:22.420 --> 01:08:25.100
and they need to know what's going on with their server,

01:08:25.100 --> 01:08:27.020
where's my, I need a picture for this.

01:08:27.020 --> 01:08:32.380
But yeah, it's like a nice visualization.

01:08:32.380 --> 01:08:35.080
I don't know why there's no pictures on this homepage.

01:08:35.080 --> 01:08:37.860
Oh well, but yeah, check it out.

01:08:37.860 --> 01:08:39.300
There's also a BPytop.

01:08:39.300 --> 01:08:42.900
It's pretty amazing what people can do in the terminal,

01:08:42.900 --> 01:08:44.340
right?

01:08:44.340 --> 01:08:45.180
- Yeah, it's awesome.

01:08:45.180 --> 01:08:47.540
- It's like, I don't know where the,

01:08:47.540 --> 01:08:49.020
there used to be pictures in this, whatever.

01:08:49.020 --> 01:08:49.860
Oh, there they are.

01:08:49.860 --> 01:08:53.140
They're just responsive design themselves out.

01:08:53.140 --> 01:08:56.140
But yeah, if you want a bunch of live graphs,

01:08:56.140 --> 01:08:57.380
every time I see stuff like this,

01:08:57.380 --> 01:08:59.060
the visit data or this,

01:08:59.060 --> 01:09:01.660
or what the textual folks are working on,

01:09:01.660 --> 01:09:03.660
it's just like, I can't believe they built this, right?

01:09:03.660 --> 01:09:06.220
Like I'm working at the level of Colorama.

01:09:06.220 --> 01:09:08.100
This string is right here.

01:09:08.100 --> 01:09:08.940
- Right, right, right.

01:09:08.940 --> 01:09:09.780
- And they're like, oh yeah.

01:09:09.780 --> 01:09:11.540
- I got an emoji to show up, right?

01:09:11.540 --> 01:09:12.380
I'm excited.

01:09:12.380 --> 01:09:13.820
- Yes, exactly, yes.

01:09:13.820 --> 01:09:15.780
A rocket ship is there, not just text.

01:09:17.340 --> 01:09:18.500
- Yeah, pretty excellent.

01:09:18.500 --> 01:09:22.780
All right, well, Ian, thank you for being here.

01:09:22.780 --> 01:09:24.380
And keep up the good work.

01:09:24.380 --> 01:09:27.420
I know so many people are using LLMs,

01:09:27.420 --> 01:09:30.180
but not that many people are creating LLMs.

01:09:30.180 --> 01:09:32.660
And as developers, we love to create things.

01:09:32.660 --> 01:09:34.540
We already have the tools to do it.

01:09:34.540 --> 01:09:39.540
People can check out your GitHub repo on the PyPI and GPT

01:09:39.540 --> 01:09:42.180
and use it as a starting place, right?

01:09:42.180 --> 01:09:43.020
- Sounds great.

01:09:43.020 --> 01:09:45.780
Yeah, and definitely reach out if you have any questions.

01:09:46.660 --> 01:09:48.620
- Excellent, well, thanks for coming back on the show.

01:09:48.620 --> 01:09:49.460
Catch you later.

01:09:49.460 --> 01:09:51.020
- Great, good to talk to you, bye-bye.

01:09:51.020 --> 01:09:52.060
- Yeah, you bet, bye.

01:09:52.060 --> 01:09:57.100
(No audio)

