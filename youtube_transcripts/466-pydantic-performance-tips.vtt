WEBVTT

00:00:00.001 --> 00:00:05.000
Sidney, welcome back to Talk Python To Me.

00:00:05.000 --> 00:00:10.000
It's awesome to have you here.

00:00:10.000 --> 00:00:15.000
I am too.

00:00:15.000 --> 00:00:20.000
We're going to talk about Pydantic, one of my very favorite libraries that just makes working with Python, data, and data exchange so, so easy, which is awesome.

00:00:20.000 --> 00:00:25.000
It's really cool that you're on the Pydantic team these days.

00:00:25.000 --> 00:00:30.000
Before then, I guess, let's jump back just a little bit.

00:00:30.000 --> 00:00:35.000
A few weeks ago, got to meet up a little bit in Pittsburgh at PyCon.

00:00:35.000 --> 00:00:40.000
How was PyCon for you?

00:00:40.000 --> 00:00:45.000
It was great. It was my first PyCon experience ever. It was a very, very large conference, so it was a cool kind of first introductory conference experience.

00:00:45.000 --> 00:00:50.000
I had just graduated not even a week before, so it was a fun way to kind of roll into full-time work

00:00:50.000 --> 00:00:55.000
and get exposed really to the Python community.

00:00:55.000 --> 00:01:00.000
It was great to just kind of have a mix of getting to give a talk, getting to attend lots of awesome presentations,

00:01:00.000 --> 00:01:05.000
and then most of all just meeting a bunch of really awesome people in the community.

00:01:05.000 --> 00:01:10.000
I always love how many people you get to meet from so many different places and perspectives.

00:01:10.000 --> 00:01:15.000
It just reminds you the world is really big, but also really small.

00:01:15.000 --> 00:01:20.000
You get to meet your friends and new people from all over the place.

00:01:20.000 --> 00:01:25.000
Definitely. I was impressed by the number of international attendees. I didn't really expect that. It was great.

00:01:25.000 --> 00:01:30.000
Yeah, same here.

00:01:30.000 --> 00:01:35.000
Maybe a quick introduction for yourself for those who didn't hear your previous episode,

00:01:35.000 --> 00:01:40.000
and then we'll talk a bit about this Pydantic library.

00:01:40.000 --> 00:01:45.000
Yeah, sure. Sounds great. My name is Sydney. I just graduated from the University of Wisconsin.

00:01:45.000 --> 00:01:50.000
Last time I chatted with you, I was still pursuing my degree in computer science

00:01:50.000 --> 00:01:55.000
and working part-time as an intern at the company Pydantic,

00:01:55.000 --> 00:02:00.000
which is kind of founded around the same ideas that inspired the open source tool,

00:02:00.000 --> 00:02:05.000
and now we're building commercial tools.

00:02:05.000 --> 00:02:10.000
I work with them primarily on the open source side.

00:02:10.000 --> 00:02:15.000
Very excited to be contributing to the open source community,

00:02:15.000 --> 00:02:20.000
but also getting to help with our commercial tools and development there.

00:02:20.000 --> 00:02:25.000
Yeah, awesome. We'll talk a bit about that later.

00:02:25.000 --> 00:02:30.000
Super cool to be able to work on open source as a proper job, right?

00:02:30.000 --> 00:02:35.000
I've kind of encouraged lots of people to contribute to open source as kind of a jumpstart

00:02:35.000 --> 00:02:40.000
into their software development careers, especially young folks who are looking to get started with things

00:02:40.000 --> 00:02:45.000
and maybe don't have an internship or that sort of thing set up yet.

00:02:45.000 --> 00:02:50.000
I think it's a really awesome pipeline for getting exposed to good code and collaborating with others and that sort of thing,

00:02:50.000 --> 00:02:55.000
but it's definitely special to get to do and get paid as well.

00:02:55.000 --> 00:03:00.000
So it's a little bit unbelievable to me, but I'm sure that it is true

00:03:00.000 --> 00:03:05.000
that there are folks out there listening to the podcast that are like, "Pydantic. Maybe I've heard of that.

00:03:05.000 --> 00:03:10.000
What is this Pydantic thing?"

00:03:10.000 --> 00:03:15.000
So Pydantic is the leading data validation library for Python.

00:03:15.000 --> 00:03:20.000
And so Pydantic uses type hints, which are optional in Python,

00:03:20.000 --> 00:03:25.000
but it's kind of generally more and more encouraged to enforce constraints on data

00:03:25.000 --> 00:03:30.000
and kind of validate data structures, etc.

00:03:30.000 --> 00:03:35.000
So we're kind of looking at a very simple example together right now where we're importing things like

00:03:35.000 --> 00:03:40.000
date time and tuple types from typing. And then kind of the core of Pydantic

00:03:40.000 --> 00:03:48.000
is you define these classes that inherit from this class called BaseModel that's in Pydantic.

00:03:48.000 --> 00:03:54.000
And that inheritance is what ends up helping you use methods to validate data,

00:03:54.000 --> 00:04:01.000
build JSON schema, things like that. And so in our case, we have this delivery class that has a timestamp,

00:04:01.000 --> 00:04:08.000
which is of type date time, and then a dimensions tuple, which has two int parts.

00:04:08.000 --> 00:04:13.000
And so then when you pass data into this delivery class to create an instance,

00:04:13.000 --> 00:04:20.000
Pydantic handles validating that data to make sure that it conforms to those constraints we've specified.

00:04:20.000 --> 00:04:25.000
And so it's really a kind of intermediate tool that you can use for deserialization or loading data

00:04:25.000 --> 00:04:30.000
and then serialization, dumping data.

00:04:30.000 --> 00:04:35.000
It's a thing of beauty. I really love the way that it works. If you've got JSON data, nested JSON data,

00:04:35.000 --> 00:04:42.000
if you go to pydantic.dev/opensource, there's an example of here that we're talking about.

00:04:42.000 --> 00:04:47.000
It's got a tuple, but the tuple contains integers, two of them.

00:04:47.000 --> 00:04:51.000
And so if there's a tuple of three things, it'll give you an error.

00:04:51.000 --> 00:04:55.000
If it's a tuple of a date time and an int, it'll give you an error.

00:04:55.000 --> 00:04:58.000
It reaches all the way inside.

00:04:58.000 --> 00:05:02.000
Things I guess it compares against, it's a little bit like data classes.

00:05:02.000 --> 00:05:06.000
Have you done much with data classes and compared them?

00:05:06.000 --> 00:05:11.000
Yeah, that's a great question. So we actually offer support for Pydantic data classes.

00:05:11.000 --> 00:05:18.000
So I think data classes kind of took the first step of really supporting using type hints for model fields

00:05:18.000 --> 00:05:25.000
and things like that. And then Pydantic sort of takes the next jump in terms of validation and schema support.

00:05:25.000 --> 00:05:31.000
And so I think one very common use case is if you're defining API request and response models,

00:05:31.000 --> 00:05:35.000
you can imagine the JSON schema capabilities come in handy there.

00:05:35.000 --> 00:05:42.000
And just ensuring the integrity of your API and the data you're dealing with, very helpful in the validation front.

00:05:42.000 --> 00:05:45.000
Yeah, very cool.

00:05:45.000 --> 00:05:54.000
Okay, well, I guess one more thing for people who are not super familiar with it.

00:05:54.000 --> 00:05:59.000
Pydantic is, I think it's used every now and then. Let's check it out on GitHub here.

00:05:59.000 --> 00:06:03.000
I'm just trying to think of some of the main places people have heard of it.

00:06:03.000 --> 00:06:10.000
Obviously, FastAPI, I think, is the thing that really launched its popularity in the early days, if I had to guess.

00:06:10.000 --> 00:06:16.000
But if we go over to GitHub, GitHub says that for the open source things,

00:06:16.000 --> 00:06:25.000
that Pydantic is a foundational dependency for 412,644 different projects.

00:06:25.000 --> 00:06:28.000
That's unbelievable.

00:06:28.000 --> 00:06:37.000
Yeah, it's very exciting. We just got our May download numbers and heard that we have over 200 million downloads in May.

00:06:37.000 --> 00:06:44.000
So that's both version one and version two, but definitely exciting to see how kind of critical of a tool it's become

00:06:44.000 --> 00:06:47.000
for so many different use cases in Python, which is awesome.

00:06:47.000 --> 00:06:50.000
Yeah, absolutely. It's really, really critical.

00:06:50.000 --> 00:07:00.000
I think we should probably talk a little bit about Pydantic v1, v2 as a way to get into the architecture conversation.

00:07:00.000 --> 00:07:06.000
That was a big thing I talked to Samuel Colvin maybe a year ago or so, I would imagine.

00:07:06.000 --> 00:07:11.000
I think around PyCon, I think we did actually a PyCon last year as well.

00:07:11.000 --> 00:07:20.000
Yeah, for sure. So a lot of the benefit of using Pydantic is we promise some great performance.

00:07:20.000 --> 00:07:25.000
And a lot of those performance gains came during our jump from v1 to v2.

00:07:25.000 --> 00:07:35.000
So v1 was written solely in Python. We had some compiled options, but really it was mostly Pythonic data validation.

00:07:35.000 --> 00:07:40.000
I say Pythonic, it's always Pythonic, but data validation done solely in Python.

00:07:40.000 --> 00:07:47.000
And the big difference with v2 is that we rewrote kind of the core of our code in Rust.

00:07:47.000 --> 00:07:54.000
And so Rust is much faster. And so depending on what kind of code you're running, v2 can be, you know,

00:07:54.000 --> 00:07:58.000
anywhere from two to 20 times faster in certain cases.

00:07:58.000 --> 00:08:03.000
So right now we still have this Python wrapper around everything in v2.

00:08:03.000 --> 00:08:08.000
But then, and that's kind of used to define schemas for models and that sort of thing.

00:08:08.000 --> 00:08:15.000
And then the actual validation and serialization logic occurs in Pydantic core in Rust.

00:08:15.000 --> 00:08:24.000
Right. So I think the team did a really good job to make this major change, this major rewrite,

00:08:24.000 --> 00:08:31.000
and split the whole monolithic thing into a Pydantic core, Pydantic itself, which is Python based,

00:08:31.000 --> 00:08:36.000
in a way that didn't break too many projects, right?

00:08:36.000 --> 00:08:41.000
Yeah, that was the goal. You know, every now and then there are breaking changes that I think

00:08:41.000 --> 00:08:45.000
are generally a good thing for the library moving forward, right?

00:08:45.000 --> 00:08:50.000
Like hopefully whenever we make a breaking change, it's because it's leading to a significant improvement.

00:08:50.000 --> 00:08:53.000
But we definitely do our best to avoid breaking changes.

00:08:53.000 --> 00:09:03.000
And certainly someday we'll launch a v3 and hopefully that'll be an even more seamless transition for v2 users to v3 users.

00:09:03.000 --> 00:09:09.000
Yeah, I would imagine that the switch to Rust probably, that big rewrite,

00:09:09.000 --> 00:09:15.000
it probably caused a lot of thoughts of reconsidering, "How are we doing this?"

00:09:15.000 --> 00:09:19.000
Or, "Now that it's over in Rust, maybe it doesn't make sense this way," or whatever.

00:09:19.000 --> 00:09:24.000
Yeah, and I think just kind of, you know, we got a lot of feedback and usage of Pydantic v1,

00:09:24.000 --> 00:09:28.000
so tried to do our best to incorporate all that feedback into a better v2 version

00:09:28.000 --> 00:09:31.000
in terms of both APIs and performance and that sort of thing.

00:09:31.000 --> 00:09:34.000
Sure, sure.

00:09:34.000 --> 00:09:39.000
John out in the audience asks, "How does the team approach thread safety with this?"

00:09:39.000 --> 00:09:43.000
So Rust can be multiple threads easy.

00:09:43.000 --> 00:09:49.000
Python, not so much really, although maybe soon with free-threaded Python.

00:09:49.000 --> 00:09:51.000
Yeah, that's a good question.

00:09:51.000 --> 00:09:58.000
So our kind of Rust guru on the team is David Hewitt, and he's very in the know about all of the multi-threading

00:09:58.000 --> 00:10:01.000
and things happening on the Rust side of things.

00:10:01.000 --> 00:10:04.000
I myself have some more to learn about that, certainly.

00:10:04.000 --> 00:10:09.000
But I think in general, kind of our approach is that Rust is quite type-safe,

00:10:09.000 --> 00:10:13.000
both performant and type-safe, which is great, and memory-safe as well.

00:10:13.000 --> 00:10:20.000
And I think most of our-- I'll talk a little bit later about some, like, parallelization

00:10:20.000 --> 00:10:23.000
and vectorization that we're looking at for performance improvements,

00:10:23.000 --> 00:10:29.000
but in terms of safety, I think if you have any questions, feel free to open an issue on the Pydantic core repo

00:10:29.000 --> 00:10:32.000
and we can get a conversation going with David Hewitt.

00:10:32.000 --> 00:10:35.000
I would imagine it's not-- you guys haven't had to do too much with it,

00:10:35.000 --> 00:10:44.000
just that Python currently, but soon-- but currently doesn't really let you do much true multi-threading

00:10:44.000 --> 00:10:46.000
because of the GIL.

00:10:46.000 --> 00:10:53.000
The whole-- I think, you know-- yeah, I think Python 3.13 is going to be crazy with free-threaded Python

00:10:53.000 --> 00:10:57.000
and it's going to be interesting to see how that evolves.

00:10:57.000 --> 00:11:01.000
Yeah, I know we definitely do some jumping through hoops and just, you know,

00:11:01.000 --> 00:11:06.000
having to be really conscious of stuff with the GIL in Pydantic core and Py03,

00:11:06.000 --> 00:11:11.000
and Py03 is kind of the library that bridges Python and Rust,

00:11:11.000 --> 00:11:14.000
and so it's heavily used in Pydantic core, as you can imagine.

00:11:14.000 --> 00:11:16.000
So I'm excited to see what changes might look like there.

00:11:16.000 --> 00:11:18.000
Yeah, same.

00:11:18.000 --> 00:11:20.000
All right, well, let's jump into the performance

00:11:20.000 --> 00:11:24.000
because you're here to tell us all about Pydantic performance tips,

00:11:24.000 --> 00:11:25.000
and you've got a whole bunch of these.

00:11:25.000 --> 00:11:27.000
Did you give this talk at PyCon?

00:11:27.000 --> 00:11:28.000
I did, partially.

00:11:28.000 --> 00:11:31.000
It's a little bit different, but some of the tips are the same.

00:11:31.000 --> 00:11:34.000
I don't think the videos are out yet, are they?

00:11:34.000 --> 00:11:37.000
At the time of recording on June 13th.

00:11:37.000 --> 00:11:39.000
Yeah, no, I actually checked a couple of minutes ago.

00:11:39.000 --> 00:11:42.000
I was like, "I said one thing during my talk that I wanted to double-check,

00:11:42.000 --> 00:11:44.000
but the videos are not out yet."

00:11:44.000 --> 00:11:45.000
No, I'm really excited.

00:11:45.000 --> 00:11:46.000
There's going to be a bunch--

00:11:46.000 --> 00:11:49.000
There was actually a bunch of good talks, including yours and some others.

00:11:49.000 --> 00:11:51.000
I want to watch, but they're not out yet.

00:11:51.000 --> 00:11:55.000
All right, let's jump into Pydantic performance.

00:11:55.000 --> 00:11:57.000
Where should we start?

00:11:57.000 --> 00:11:59.000
I can start on the slideshow if we want.

00:11:59.000 --> 00:12:01.000
Yeah, let's do that.

00:12:01.000 --> 00:12:02.000
Awesome.

00:12:02.000 --> 00:12:05.000
So, yeah, I think kind of the categories of performance tips

00:12:05.000 --> 00:12:07.000
that we're going to talk about here

00:12:07.000 --> 00:12:11.000
kind of have some fast, one-liner type performance tips

00:12:11.000 --> 00:12:13.000
that you can implement in your own code.

00:12:13.000 --> 00:12:16.000
And then kind of the meat of the, like,

00:12:16.000 --> 00:12:20.000
how do I improve performance in my application that uses Pydantic?

00:12:20.000 --> 00:12:23.000
We're going to talk a bit about discriminated unions,

00:12:23.000 --> 00:12:25.000
also called tag unions.

00:12:25.000 --> 00:12:29.000
And then kind of finally talk about, on our end of the development,

00:12:29.000 --> 00:12:32.000
how are we continuously improving performance,

00:12:32.000 --> 00:12:36.000
you know, Pydantic internals-wise, et cetera.

00:12:36.000 --> 00:12:41.000
Sure. Do you all have the equivalent of unit tests for performance?

00:12:41.000 --> 00:12:43.000
Yeah, we do.

00:12:43.000 --> 00:12:46.000
We use a library called CodSpeed

00:12:46.000 --> 00:12:48.000
that I'm excited to touch on a bit more later.

00:12:48.000 --> 00:12:50.000
Yeah, all right, let's talk about that later. Perfect.

00:12:50.000 --> 00:12:52.000
Yeah, sure thing.

00:12:52.000 --> 00:12:54.000
So I have this slide up right now

00:12:54.000 --> 00:12:57.000
just kind of talking about why people use Pydantic.

00:12:57.000 --> 00:13:00.000
We've already covered some of these, but just kind of as a general recap,

00:13:00.000 --> 00:13:02.000
it's powered by type hints,

00:13:02.000 --> 00:13:05.000
and one of our biggest promises is speed.

00:13:05.000 --> 00:13:07.000
We also have these other great features

00:13:07.000 --> 00:13:10.000
like JSON schema compatibility and documentation

00:13:10.000 --> 00:13:13.000
comes in particularly handy when we talk about APIs,

00:13:13.000 --> 00:13:17.000
you know, support for custom validation and serialization logic.

00:13:17.000 --> 00:13:21.000
And then as we saw with the GitHub repository observations,

00:13:21.000 --> 00:13:24.000
a very robust ecosystem of libraries and other tools

00:13:24.000 --> 00:13:27.000
that use and depend on Pydantic

00:13:27.000 --> 00:13:30.000
that leads to this kind of extensive and large community,

00:13:30.000 --> 00:13:32.000
which is really great.

00:13:32.000 --> 00:13:36.000
So this all kind of lies on the foundation of like Pydantic is easy to use

00:13:36.000 --> 00:13:38.000
and it's very fast.

00:13:38.000 --> 00:13:42.000
Yeah, and this, yeah, well, the speed is really interesting

00:13:42.000 --> 00:13:45.000
in the multiplier that you all have

00:13:45.000 --> 00:13:49.000
for basically a huge swath of the Python ecosystem, right?

00:13:49.000 --> 00:13:53.000
We just saw the 412,000 things that depend on Pydantic.

00:13:53.000 --> 00:13:56.000
Well, a lot of those,

00:13:56.000 --> 00:14:00.000
their performance depends on Pydantic's performance as well, right?

00:14:00.000 --> 00:14:02.000
Yeah, certainly.

00:14:02.000 --> 00:14:05.000
Yeah, it's nice to have such a large ecosystem of folks

00:14:05.000 --> 00:14:08.000
to also contribute to the library as well, right?

00:14:08.000 --> 00:14:11.000
Because other people are dependent on our performance,

00:14:11.000 --> 00:14:15.000
the community definitely becomes invested in it as well, which is great.

00:14:15.000 --> 00:14:17.000
Yeah, I bet.

00:14:17.000 --> 00:14:19.000
But yeah, so kind of as that first category,

00:14:19.000 --> 00:14:22.000
we can chat about some basic performance tips.

00:14:22.000 --> 00:14:24.000
And I'll do my best here to kind of describe this generally

00:14:24.000 --> 00:14:28.000
for listeners who maybe aren't able to see the screen.

00:14:28.000 --> 00:14:31.000
So when you are--

00:14:31.000 --> 00:14:33.000
Can we share your slideshow later with the audience?

00:14:33.000 --> 00:14:35.000
Can we put it in the show notes?

00:14:35.000 --> 00:14:36.000
Yeah, yeah, absolutely.

00:14:36.000 --> 00:14:38.000
Okay, so people want to go back and check it out.

00:14:38.000 --> 00:14:39.000
But yeah, we'll describe it for everyone.

00:14:39.000 --> 00:14:40.000
Go ahead.

00:14:40.000 --> 00:14:43.000
Yeah, so when you're validating data in Pydantic,

00:14:43.000 --> 00:14:49.000
you can either validate Python objects or like dictionary-type data

00:14:49.000 --> 00:14:53.000
or you can validate JSON-formatted data.

00:14:53.000 --> 00:14:56.000
And so one of these kind of like one-liner tips that we have

00:14:56.000 --> 00:15:01.000
is to use our built-in model validate JSON method

00:15:01.000 --> 00:15:04.000
instead of calling this our model validate method

00:15:04.000 --> 00:15:09.000
and then separately loading the JSON data with the standard libJSON package.

00:15:09.000 --> 00:15:13.000
And the reason that we recommend that is one of the like crux

00:15:13.000 --> 00:15:16.000
of the general performance patterns that we try to follow

00:15:16.000 --> 00:15:20.000
is not materializing things in Python when we don't have to.

00:15:20.000 --> 00:15:23.000
So we've already mentioned that our core is written in Rust,

00:15:23.000 --> 00:15:25.000
which is much faster than Python.

00:15:25.000 --> 00:15:29.000
And so with our model validate JSON built-in method,

00:15:29.000 --> 00:15:32.000
whenever you pass in that string, we send it right to Rust.

00:15:32.000 --> 00:15:34.000
Whereas if you do the JSON loading by yourself,

00:15:34.000 --> 00:15:36.000
you're going to like materialize Python object

00:15:36.000 --> 00:15:38.000
and then have to send it over.

00:15:38.000 --> 00:15:43.000
Right, and so you're going to be using the built-in JSON load S,

00:15:43.000 --> 00:15:47.000
which will then, or load or whatever, and then it'll pull that in,

00:15:47.000 --> 00:15:50.000
turn it into a Python dictionary, then you take it

00:15:50.000 --> 00:15:53.000
and try to convert that back to a Rust data structure

00:15:53.000 --> 00:15:55.000
and then validate it in Rust.

00:15:55.000 --> 00:15:57.000
That's where all the validation lives anyway.

00:15:57.000 --> 00:15:59.000
So just get out of the way, right?

00:15:59.000 --> 00:16:03.000
Exactly, yep. It's like skip the Python step if you can, right?

00:16:03.000 --> 00:16:05.000
And I will note there is one exception here,

00:16:05.000 --> 00:16:09.000
which is I mentioned we support custom validation.

00:16:09.000 --> 00:16:12.000
If you're using what we call like before and wrap validators

00:16:12.000 --> 00:16:18.000
that do something in Python and then call our internal validation logic

00:16:18.000 --> 00:16:21.000
and then maybe even do something after, it's okay.

00:16:21.000 --> 00:16:25.000
You can use model validate and the built-in JSON.load S

00:16:25.000 --> 00:16:27.000
because you're already kind of guaranteed to be materializing

00:16:27.000 --> 00:16:29.000
Python objects in that case.

00:16:29.000 --> 00:16:31.000
But for the vast majority of cases, it's great to just go with

00:16:31.000 --> 00:16:34.000
the built-in model validate JSON.

00:16:34.000 --> 00:16:36.000
Yeah, that's really good advice.

00:16:36.000 --> 00:16:40.000
And they seem kind of equivalent, but once you know the internals,

00:16:40.000 --> 00:16:43.000
right, then it's, well, maybe it's not exactly.

00:16:43.000 --> 00:16:46.000
Yeah, and I think implementing some of these tips is helpful

00:16:46.000 --> 00:16:48.000
in that if you understand some of the kind of like

00:16:48.000 --> 00:16:52.000
Pydantic architectural context, it can also just help you think more

00:16:52.000 --> 00:16:57.000
about like how can I write my Pydantic code better.

00:16:57.000 --> 00:16:58.000
Absolutely.

00:16:58.000 --> 00:17:02.000
So the next tip I have here, very easy one-liner fix,

00:17:02.000 --> 00:17:05.000
which is when you're using type adapter,

00:17:05.000 --> 00:17:10.000
which is this structure you can use to basically validate one type.

00:17:10.000 --> 00:17:14.000
So we have base models, which we've chatted about before,

00:17:14.000 --> 00:17:17.000
which is like if you have a model with lots of fields,

00:17:17.000 --> 00:17:19.000
that's kind of the structure you use to define it.

00:17:19.000 --> 00:17:21.000
Well, type adapter is great if you're like,

00:17:21.000 --> 00:17:24.000
"I just want to validate that this data is a list of integers,"

00:17:24.000 --> 00:17:26.000
for example, as we're seeing on the screen.

00:17:26.000 --> 00:17:29.000
Right, because let me give people an idea.

00:17:29.000 --> 00:17:32.000
Like if you accept, if you've got a JSON,

00:17:32.000 --> 00:17:36.000
well, just JSON data from wherever, but a lot of times it's coming over an API

00:17:36.000 --> 00:17:40.000
or it's provided to you as a file and it's not your data you control, right,

00:17:40.000 --> 00:17:45.000
you're trying to validate it, you could get a dictionary JSON object

00:17:45.000 --> 00:17:47.000
that's got curly braces with a bunch of stuff,

00:17:47.000 --> 00:17:49.000
in which case that's easy to map to a class.

00:17:49.000 --> 00:17:53.000
But if you just have JSON, which is bracket, thing, thing, thing, thing,

00:17:53.000 --> 00:17:58.000
close bracket, well, how do you have a class that represents a list?

00:17:58.000 --> 00:18:02.000
Like it gets really tricky, right, to be able to understand.

00:18:02.000 --> 00:18:04.000
You can't model that with classes.

00:18:04.000 --> 00:18:07.000
And so you all have this type adapter thing, right?

00:18:07.000 --> 00:18:09.000
That's what the role plays generally.

00:18:09.000 --> 00:18:10.000
Is that right?

00:18:10.000 --> 00:18:13.000
Yeah, and I think it's also really helpful in a testing context.

00:18:13.000 --> 00:18:18.000
Like, you know, when we want to check that our validation behavior is right

00:18:18.000 --> 00:18:22.000
for one type, there's no reason to go, like, build an entire model.

00:18:22.000 --> 00:18:26.000
If you're really just validating against one type or structure,

00:18:26.000 --> 00:18:28.000
type adapter is great.

00:18:28.000 --> 00:18:33.000
And so kind of the advice here is you only want to initialize

00:18:33.000 --> 00:18:35.000
your type adapter object once.

00:18:35.000 --> 00:18:39.000
And the reason behind that is we build a core schema in Python

00:18:39.000 --> 00:18:44.000
and then attach that to a class or type adapter, et cetera.

00:18:44.000 --> 00:18:48.000
And so if you can not build that type adapter within your loop,

00:18:48.000 --> 00:18:52.000
but instead do it right before or not build it in your function,

00:18:52.000 --> 00:18:56.000
but instead outside of it, then you can avoid building the core schema

00:18:56.000 --> 00:18:58.000
over and over again.

00:18:58.000 --> 00:19:01.000
Yeah, so basically what you're saying is that the type adapter

00:19:01.000 --> 00:19:07.000
that you create might as well be a singleton because it's stateless, right?

00:19:07.000 --> 00:19:10.000
Like, it doesn't store any data.

00:19:10.000 --> 00:19:13.000
It's kind of slightly expensive to create, relatively.

00:19:13.000 --> 00:19:16.000
And so if you had a function that was called over and over again

00:19:16.000 --> 00:19:19.000
and that function had a loop, and inside the loop you're creating

00:19:19.000 --> 00:19:22.000
the type adapter, that'd be like worst case scenario almost, right?

00:19:22.000 --> 00:19:25.000
Yeah, exactly.

00:19:25.000 --> 00:19:28.000
And I think this kind of goes along with general best programming tips, right?

00:19:28.000 --> 00:19:31.000
Which is like, if you only need to create something once, do that once.

00:19:31.000 --> 00:19:34.000
Exactly.

00:19:34.000 --> 00:19:37.000
And then the other way way way back in time could be like

00:19:37.000 --> 00:19:40.000
a compiled regular expression.

00:19:40.000 --> 00:19:43.000
You wouldn't do that over and over in a loop.

00:19:43.000 --> 00:19:46.000
You would just create a regular compiled regular expression

00:19:46.000 --> 00:19:49.000
and then use it throughout your program, right?

00:19:49.000 --> 00:19:52.000
Because it's kind of expensive to do that, but it's fast once it's created.

00:19:52.000 --> 00:19:55.000
Yeah, exactly. And funny that you mentioned that.

00:19:55.000 --> 00:19:58.000
I actually fixed a bug last week where we were compiling regular expressions

00:19:58.000 --> 00:20:01.000
twice when folks specified that as a constraint on a field.

00:20:01.000 --> 00:20:04.000
So definitely just something to keep in mind and easy to fix

00:20:04.000 --> 00:20:07.000
or implement with type adapters here.

00:20:07.000 --> 00:20:10.000
Yeah, awesome. Okay, I like this one. That's a good one.

00:20:10.000 --> 00:20:13.000
Yeah.

00:20:13.000 --> 00:20:16.000
So this next tip also kind of goes along with general best practices,

00:20:16.000 --> 00:20:19.000
but the more specific you can be with your type hints, the better.

00:20:19.000 --> 00:20:22.000
And so specifically,

00:20:22.000 --> 00:20:25.000
if you know that you have a list of integers,

00:20:25.000 --> 00:20:28.000
it's better and more efficient to specify a type hint

00:20:28.000 --> 00:20:31.000
as a list of integers instead of a sequence of integers, for example.

00:20:31.000 --> 00:20:34.000
Or if you know you have a dictionary

00:20:34.000 --> 00:20:37.000
that maps strings to integers,

00:20:37.000 --> 00:20:40.000
specify that type hint as a dictionary, not a mapping.

00:20:40.000 --> 00:20:43.000
Interesting.

00:20:43.000 --> 00:20:46.000
Yeah, so you could import a sequence from the typing module,

00:20:46.000 --> 00:20:49.000
which is the generic way,

00:20:49.000 --> 00:20:52.000
but I guess you probably have specific code that runs

00:20:52.000 --> 00:20:55.000
that can validate lists more efficiently

00:20:55.000 --> 00:20:58.000
than a general iterable type of thing, right?

00:20:58.000 --> 00:21:01.000
Yeah, exactly. So in the case of a sequence versus a list,

00:21:01.000 --> 00:21:04.000
it's the square and rectangle thing, right?

00:21:04.000 --> 00:21:07.000
A list is a sequence, but there are lots of other types of sequences.

00:21:07.000 --> 00:21:10.000
And so you can imagine for a sequence,

00:21:10.000 --> 00:21:13.000
we have to check lots of other things, whereas if you know

00:21:13.000 --> 00:21:16.000
with certainty this is going to be a list or it should be a list,

00:21:16.000 --> 00:21:19.000
then you can have things be more efficient with specificity there.

00:21:19.000 --> 00:21:22.000
Does it make any difference at all

00:21:22.000 --> 00:21:25.000
whether you use the more modern type specifications?

00:21:25.000 --> 00:21:28.000
Traditionally, people would say

00:21:28.000 --> 00:21:31.000
"from typing import capital LLIST,"

00:21:31.000 --> 00:21:34.000
but now you can just say lowercase LLIST

00:21:34.000 --> 00:21:37.000
with the built-in and no import statement?

00:21:37.000 --> 00:21:40.000
Are those equivalent or is there some minor difference there?

00:21:40.000 --> 00:21:43.000
Do you know?

00:21:43.000 --> 00:21:46.000
Yeah, that's a good question.

00:21:46.000 --> 00:21:49.000
I wouldn't be surprised if there was a minor difference

00:21:49.000 --> 00:21:52.000
in the type specifications.

00:21:52.000 --> 00:21:55.000
I suppose you could import the old capital LLIST in a newer Python version,

00:21:55.000 --> 00:21:58.000
but I think the difference is more related to specificity of a type hint

00:21:58.000 --> 00:22:01.000
rather than versioning.

00:22:01.000 --> 00:22:04.000
If the use of that capital LLIST

00:22:04.000 --> 00:22:07.000
made you write an import statement,

00:22:07.000 --> 00:22:10.000
it would cause the program to start

00:22:10.000 --> 00:22:13.000
ever so slightly slower because there's another import.

00:22:13.000 --> 00:22:16.000
It's got to run, whereas it already knows.

00:22:16.000 --> 00:22:19.000
It's already imported what LLIST is.

00:22:19.000 --> 00:22:22.000
You wouldn't believe how many times I get messages

00:22:22.000 --> 00:22:25.000
on YouTube videos I've done or even from courses

00:22:25.000 --> 00:22:28.000
saying, "Michael, I don't know what you're doing,

00:22:28.000 --> 00:22:31.000
but your code is just wrong.

00:22:31.000 --> 00:22:34.000
I wrote lowercase LLIST bracket something

00:22:34.000 --> 00:22:37.000
and it said LLIST is not sub-indexable

00:22:37.000 --> 00:22:40.000
or something like that.

00:22:40.000 --> 00:22:43.000
Look, you've just done it wrong.

00:22:43.000 --> 00:22:46.000
Or you're on Python 3.7 or something super old

00:22:46.000 --> 00:22:49.000
before these new features were added.

00:22:49.000 --> 00:22:52.000
There's just somewhere in the community we haven't

00:22:52.000 --> 00:22:55.000
communicated this well, I don't know.

00:22:55.000 --> 00:22:58.000
Yeah, for sure. I was writing some code earlier today

00:22:58.000 --> 00:23:01.000
in a meeting and I used the from typing import union

00:23:01.000 --> 00:23:04.000
and then union x and y type.

00:23:04.000 --> 00:23:07.000
My coworker was like, "Cindy, use the pipe.

00:23:07.000 --> 00:23:10.000
What are you doing?"

00:23:10.000 --> 00:23:13.000
Here's the thing, that was introduced in 3.10, I believe.

00:23:13.000 --> 00:23:16.000
If people are in 3.9, that code doesn't run

00:23:16.000 --> 00:23:19.000
or if they're not familiar with the changes.

00:23:19.000 --> 00:23:22.000
There's all these trade-offs.

00:23:22.000 --> 00:23:25.000
I almost feel like it would be amazing to go back

00:23:25.000 --> 00:23:28.000
for any time there's a security release that releases

00:23:28.000 --> 00:23:31.000
say another 3.7 or something and change the error message

00:23:31.000 --> 00:23:34.000
to say, "This feature only works in the future version

00:23:34.000 --> 00:23:37.000
of Python rather than some arbitrary error

00:23:37.000 --> 00:23:40.000
that's wrong." That would be great.

00:23:40.000 --> 00:23:43.000
Yeah, definitely. Some of those errors can be pretty cryptic

00:23:43.000 --> 00:23:46.000
with the syntax stuff.

00:23:46.000 --> 00:23:49.000
Be specific, list tuple, not sequence if you know it's a list

00:23:49.000 --> 00:23:52.000
or a tuple or whatever.

00:23:52.000 --> 00:23:55.000
Then my last minor tip, which great that you brought up

00:23:55.000 --> 00:23:58.000
import statements and adding general time

00:23:58.000 --> 00:24:01.000
to a program.

00:24:01.000 --> 00:24:04.000
I don't have a slide for this one, but if we go back

00:24:04.000 --> 00:24:07.000
to the slide, we talked about the fact that initializing

00:24:07.000 --> 00:24:10.000
this type adapter builds a core schema

00:24:10.000 --> 00:24:13.000
and attaches it to that class.

00:24:13.000 --> 00:24:16.000
That's done at build time,

00:24:16.000 --> 00:24:19.000
at import time. That's already done.

00:24:19.000 --> 00:24:22.000
If you really don't want

00:24:22.000 --> 00:24:25.000
to have that import or build time

00:24:25.000 --> 00:24:28.000
take a long time, you can use the

00:24:28.000 --> 00:24:31.000
defer build flag. What that does

00:24:31.000 --> 00:24:34.000
is defers the core schema build until the first

00:24:34.000 --> 00:24:37.000
validation call. You can also set that on model config

00:24:37.000 --> 00:24:40.000
and things like that. Basically, the idea here is

00:24:40.000 --> 00:24:43.000
striving to be lazier.

00:24:43.000 --> 00:24:46.000
If we don't need to build this core schema right at import time

00:24:46.000 --> 00:24:49.000
because we want our program to start up quickly,

00:24:49.000 --> 00:24:52.000
that's great. We might have a little bit of a delay on the first

00:24:52.000 --> 00:24:55.000
validation, but maybe startup time is more important.

00:24:55.000 --> 00:24:58.000
That's a little bit more of a preferential

00:24:58.000 --> 00:25:01.000
performance tip, but available for folks who need it.

00:25:01.000 --> 00:25:04.000
Let me give you an example.

00:25:04.000 --> 00:25:07.000
I'll give people an example where I think this might be useful.

00:25:07.000 --> 00:25:10.000
In the Talk Python training, the courses site,

00:25:10.000 --> 00:25:13.000
I think we've got 20,000 lines of Python code,

00:25:13.000 --> 00:25:16.000
which is probably more at this point. I checked a long time ago.

00:25:16.000 --> 00:25:19.000
But a lot. It's a package.

00:25:19.000 --> 00:25:22.000
When you import it, it imports all the stuff

00:25:22.000 --> 00:25:25.000
to run the whole web app, but also little utilities

00:25:25.000 --> 00:25:28.000
like, "Oh, I just want to get a quick report.

00:25:28.000 --> 00:25:31.000
I want to just access this model and then use it

00:25:31.000 --> 00:25:34.000
on something real quick." It imports all that stuff

00:25:34.000 --> 00:25:37.000
so that app startup would be potentially slowed down by this.

00:25:37.000 --> 00:25:40.000
Where if you know only sometimes is that type adapter used,

00:25:40.000 --> 00:25:43.000
you don't want to necessarily have it completely created

00:25:43.000 --> 00:25:46.000
until that function gets called.

00:25:46.000 --> 00:25:49.000
Then the first function call might be a little slow,

00:25:49.000 --> 00:25:52.000
but there'd be plenty of times where maybe it never gets called, right?

00:25:52.000 --> 00:25:55.000
- Yeah, definitely. - Yeah, awesome.

00:25:55.000 --> 00:25:58.000
- Okay. Very good. - All right.

00:25:58.000 --> 00:26:01.000
A more complex performance optimization is using tagged unions.

00:26:01.000 --> 00:26:04.000
They're still pretty simple.

00:26:04.000 --> 00:26:07.000
It's just a little bit more than a one-line change.

00:26:07.000 --> 00:26:10.000
Talking about tagged unions,

00:26:10.000 --> 00:26:13.000
we can go through a basic example,

00:26:13.000 --> 00:26:16.000
why we're using tagged unions in the first place,

00:26:16.000 --> 00:26:19.000
and then some more advanced examples.

00:26:19.000 --> 00:26:22.000
- Let's start with what are tagged unions,

00:26:22.000 --> 00:26:25.000
because I honestly have no idea. I know what unions are,

00:26:25.000 --> 00:26:28.000
but tagging them, I don't know. - Yeah, sure thing.

00:26:28.000 --> 00:26:31.000
Tagged unions are a special type of union.

00:26:31.000 --> 00:26:34.000
We also call them discriminated unions.

00:26:34.000 --> 00:26:37.000
They help you specify a member of a model

00:26:37.000 --> 00:26:40.000
that you can use for discrimination in your validation.

00:26:40.000 --> 00:26:43.000
What that means is if you have two models

00:26:43.000 --> 00:26:46.000
that are pretty similar,

00:26:46.000 --> 00:26:49.000
your field can be either one of those types of models,

00:26:49.000 --> 00:26:52.000
model X or model Y.

00:26:52.000 --> 00:26:56.000
But you know that there's one tag or discriminator field that differs.

00:26:56.000 --> 00:26:59.000
You can specifically validate against that field

00:26:59.000 --> 00:27:02.000
and skip some of the other validation.

00:27:02.000 --> 00:27:05.000
I'll move on to an example here in a second,

00:27:05.000 --> 00:27:08.000
but basically it helps you validate more efficiently

00:27:08.000 --> 00:27:11.000
because you get to skip validation of some fields.

00:27:11.000 --> 00:27:14.000
It's really helpful if you have models that have 100 fields,

00:27:14.000 --> 00:27:17.000
but one of them is really indicative of what type it might be.

00:27:17.000 --> 00:27:20.000
- I see. So instead of trying to figure out

00:27:20.000 --> 00:27:23.000
is it all of this stuff, once you know

00:27:23.000 --> 00:27:26.000
it has this aspect or that aspect,

00:27:26.000 --> 00:27:29.000
then you can branch it on a path and just treat it as

00:27:29.000 --> 00:27:32.000
one of the elements of the union. Is that right?

00:27:32.000 --> 00:27:35.000
- Yes, exactly. One other note about

00:27:35.000 --> 00:27:38.000
discriminated unions is you specify this discriminator,

00:27:38.000 --> 00:27:41.000
and it can either be a string literal type or a callable type.

00:27:41.000 --> 00:27:44.000
We'll look at some examples of both.

00:27:44.000 --> 00:27:47.000
Here's a more concrete example so we can

00:27:47.000 --> 00:27:50.000
really better understand this.

00:27:50.000 --> 00:27:53.000
Let's say we have a classic example,

00:27:53.000 --> 00:27:56.000
a cat model and a dog model.

00:27:56.000 --> 00:27:59.000
- Yeah. Cat people and dog people. You're going to start a debate here.

00:27:59.000 --> 00:28:02.000
- Exactly. They both have this

00:28:02.000 --> 00:28:05.000
pet type field. For the

00:28:05.000 --> 00:28:08.000
cat model, it's a literal that is just

00:28:08.000 --> 00:28:11.000
the string cat. For the dog model, it's the

00:28:11.000 --> 00:28:14.000
literal that's the string dog. It's just kind of a flag

00:28:14.000 --> 00:28:17.000
on a model to indicate what type it is.

00:28:17.000 --> 00:28:20.000
You can imagine in this basic case, we only have a couple of

00:28:20.000 --> 00:28:23.000
fields attached to each model, but maybe

00:28:23.000 --> 00:28:26.000
this is like data in a

00:28:26.000 --> 00:28:29.000
vet database. You can imagine there's going to be tons of fields

00:28:29.000 --> 00:28:32.000
attached to this. It'd be pretty helpful

00:28:32.000 --> 00:28:35.000
to just be able to look at it and say, "Oh, the pet type is dog.

00:28:35.000 --> 00:28:38.000
Let's make sure this data is valid for a dog

00:28:38.000 --> 00:28:41.000
type." I'll also note we have a lizard in here.

00:28:41.000 --> 00:28:48.000
What this looks like in terms of validation with Pydantic, then,

00:28:48.000 --> 00:28:51.000
is that when we specify this pet field,

00:28:51.000 --> 00:28:54.000
we just add one extra setting

00:28:54.000 --> 00:28:57.000
which says that the discriminator is that pet type field.

00:28:57.000 --> 00:29:00.000
Then, when we pass in data that

00:29:00.000 --> 00:29:03.000
corresponds to a dog model, Pydantic is smart

00:29:03.000 --> 00:29:06.000
enough to say, "Oh, this is a discriminated union field.

00:29:06.000 --> 00:29:09.000
Let me go look for the pet type

00:29:09.000 --> 00:29:12.000
field on the model and just see what that is,

00:29:12.000 --> 00:29:15.000
and then use that to inform my decision for

00:29:15.000 --> 00:29:18.000
what type I should validate against."

00:29:18.000 --> 00:29:21.000
That's awesome. If we don't set the discriminator

00:29:21.000 --> 00:29:24.000
keyword value in

00:29:24.000 --> 00:29:27.000
the field for the union, it'll still

00:29:27.000 --> 00:29:30.000
work, right? It just has to be more exhaustive

00:29:30.000 --> 00:29:33.000
and slow. Yeah, exactly. It'll still

00:29:33.000 --> 00:29:36.000
validate and it'll say, "Hey, let's take this input data

00:29:36.000 --> 00:29:39.000
and try to validate it against the cat model."

00:29:39.000 --> 00:29:42.000
Then, Pydantic will come back and say, "Oh, that's not a valid cat.

00:29:42.000 --> 00:29:45.000
Let's try the next one." Whereas with this discriminated

00:29:45.000 --> 00:29:48.000
pattern, we can skip right to the dog, which

00:29:48.000 --> 00:29:51.000
you can imagine helps us skip some of the other steps.

00:29:51.000 --> 00:29:54.000
Yeah, absolutely. Okay, that's really cool. I had no idea about this.

00:29:54.000 --> 00:29:57.000
Yeah, it's a cool, I'd say,

00:29:57.000 --> 00:30:00.000
moderate-level feature. I think if you're just starting to

00:30:00.000 --> 00:30:03.000
use Pydantic, you probably haven't touched discriminated unions

00:30:03.000 --> 00:30:06.000
much, but we hope that it's simple enough to implement that

00:30:06.000 --> 00:30:09.000
most folks can use it if they're using unions.

00:30:09.000 --> 00:30:12.000
Yeah, that's cool. I don't use unions very often, which is probably why,

00:30:12.000 --> 00:30:15.000
other than symptom pipe none, which is

00:30:15.000 --> 00:30:18.000
optional. But if I did,

00:30:18.000 --> 00:30:21.000
I'll definitely remember this.

00:30:21.000 --> 00:30:24.000
Alrighty, so as I've mentioned,

00:30:24.000 --> 00:30:27.000
this helps for more efficient validation. And then where this

00:30:27.000 --> 00:30:30.000
really comes and has a lot of value is when you

00:30:30.000 --> 00:30:33.000
are dealing with lots of nested models or models that have

00:30:33.000 --> 00:30:36.000
tons of fields. So let's say you have a union with, like,

00:30:36.000 --> 00:30:39.000
10 members, and each member of the union has 100 fields.

00:30:39.000 --> 00:30:42.000
If you can just do validation against 100 fields instead of

00:30:42.000 --> 00:30:45.000
1,000, that would be great in terms of a

00:30:45.000 --> 00:30:48.000
performance gain. And then, once again, with nested models,

00:30:48.000 --> 00:30:51.000
if you can skip lots of those union

00:30:51.000 --> 00:30:54.000
member validations, also going to boost your performance.

00:30:54.000 --> 00:30:57.000
Yeah, for sure. You know an example where this seems

00:30:57.000 --> 00:31:00.000
very likely would be

00:31:00.000 --> 00:31:03.000
using it with Beanie or some other document

00:31:03.000 --> 00:31:06.000
database where the modeling structure is very hierarchical.

00:31:06.000 --> 00:31:09.000
You end up with a lot of nested

00:31:09.000 --> 00:31:12.000
sub-identic models in there.

00:31:12.000 --> 00:31:15.000
Yeah, very much so.

00:31:15.000 --> 00:31:18.000
So as a little bit of an added benefit, we can talk

00:31:18.000 --> 00:31:21.000
about this improved error handling, which is a great way to

00:31:21.000 --> 00:31:24.000
kind of visualize why the discriminated union pattern

00:31:24.000 --> 00:31:27.000
is more efficient. So right now we're looking at an example

00:31:27.000 --> 00:31:30.000
of validation against a model

00:31:30.000 --> 00:31:33.000
that doesn't use a discriminated union, and the

00:31:33.000 --> 00:31:36.000
errors are not very nice to look at. You basically see

00:31:36.000 --> 00:31:39.000
the errors for every single

00:31:39.000 --> 00:31:42.000
permutation of the different values, and we're using nested

00:31:42.000 --> 00:31:45.000
models, so it's very hard to interpret.

00:31:45.000 --> 00:31:48.000
So we don't have to look at this for too long. It's not

00:31:48.000 --> 00:31:51.000
very nice, but if we look at...

00:31:51.000 --> 00:31:54.000
But basically the error message says, "Look, there's something

00:31:54.000 --> 00:31:57.000
wrong with the union. If it was a string, it is missing these

00:31:57.000 --> 00:32:00.000
things. If it was this kind of thing, it misses those things.

00:32:00.000 --> 00:32:03.000
If it was a dog, it misses this. If it's a cat, it

00:32:03.000 --> 00:32:06.000
misses that." And it doesn't specifically tell you.

00:32:06.000 --> 00:32:09.000
Exactly. It's a dog, so it's missing

00:32:09.000 --> 00:32:12.000
the collar size or whatever, right?

00:32:12.000 --> 00:32:15.000
Right, exactly. But then, and I'll go back and

00:32:15.000 --> 00:32:18.000
kind of explain the discriminated model for this case in a

00:32:18.000 --> 00:32:21.000
second, but if you look at this is the model with

00:32:21.000 --> 00:32:24.000
the discriminated union instead, we have one

00:32:24.000 --> 00:32:27.000
very nice error that says, "Okay, you're trying to

00:32:27.000 --> 00:32:30.000
validate this X field

00:32:30.000 --> 00:32:33.000
and it's the wrong type, right?"

00:32:33.000 --> 00:32:36.000
So, yeah.

00:32:36.000 --> 00:32:39.000
The first example that we were looking at was using string

00:32:39.000 --> 00:32:42.000
type discriminators. So, we just had this pet type thing that

00:32:42.000 --> 00:32:45.000
said, "Oh, this is a cat," or "This is a dog," that sort of

00:32:45.000 --> 00:32:48.000
thing. We also offer some more

00:32:48.000 --> 00:32:51.000
customization in terms of we also

00:32:51.000 --> 00:32:54.000
allow callable discriminators. So, in this

00:32:54.000 --> 00:32:57.000
case, this field can be either

00:32:57.000 --> 00:33:00.000
a string or this instance

00:33:00.000 --> 00:33:03.000
of discriminated model. So, it's kind of a

00:33:03.000 --> 00:33:06.000
recursive pattern, right? And that's where you can imagine

00:33:06.000 --> 00:33:09.000
the nested structures becoming very complex

00:33:09.000 --> 00:33:12.000
very easily. And we use this

00:33:12.000 --> 00:33:15.000
kind of callable to differentiate

00:33:15.000 --> 00:33:18.000
between which model we should validate against

00:33:18.000 --> 00:33:21.000
and then we tag each of the cases.

00:33:21.000 --> 00:33:24.000
So, a little bit more of a complex application here, but

00:33:24.000 --> 00:33:27.000
once again, when you kind of see the benefit in terms of errors

00:33:27.000 --> 00:33:30.000
and interpreting things and performance, I think it's

00:33:30.000 --> 00:33:33.000
generally a worthwhile investment.

00:33:33.000 --> 00:33:36.000
So, if you wanted something like a composite

00:33:36.000 --> 00:33:39.000
key equivalent of a discriminator,

00:33:39.000 --> 00:33:42.000
if it has this field

00:33:42.000 --> 00:33:45.000
and its nested model is of this type,

00:33:45.000 --> 00:33:48.000
it's one thing versus another.

00:33:48.000 --> 00:33:51.000
Like a free user versus a paying user,

00:33:51.000 --> 00:33:54.000
you might have to look and see their total lifetime value plus

00:33:54.000 --> 00:33:57.000
that they're a registered user. I don't know.

00:33:57.000 --> 00:34:00.000
You could write code that would pull that information out and

00:34:00.000 --> 00:34:03.000
then you could use that to validate against

00:34:03.000 --> 00:34:06.000
which thing to validate against, right?

00:34:06.000 --> 00:34:09.000
Yeah, exactly. Yeah, it definitely comes in handy when you have like,

00:34:09.000 --> 00:34:12.000
you're like, "Okay, well, I still want the performance benefits of a

00:34:12.000 --> 00:34:15.000
discriminated union, but I kind of have three fields

00:34:15.000 --> 00:34:18.000
on each model that are indicative of which one I should validate against."

00:34:18.000 --> 00:34:21.000
And it's like, well, you know, taking the time to

00:34:21.000 --> 00:34:24.000
look at those three fields over the hundred is definitely

00:34:24.000 --> 00:34:27.000
worth it. Just a little bit of complexity for the developer.

00:34:27.000 --> 00:34:30.000
One other note here is that

00:34:30.000 --> 00:34:33.000
Can we go back really quick on that previous one?

00:34:33.000 --> 00:34:36.000
So I got a quick question. So for this, you write a function.

00:34:36.000 --> 00:34:39.000
It's given the

00:34:39.000 --> 00:34:42.000
value that comes in, which could be a

00:34:42.000 --> 00:34:45.000
string, it could be a dictionary, etc.

00:34:45.000 --> 00:34:48.000
Could you do a little bit

00:34:48.000 --> 00:34:51.000
further performance improvements and add like

00:34:51.000 --> 00:34:54.000
func tools, LRU cache to

00:34:54.000 --> 00:34:57.000
cache the output so every time it sees the same thing,

00:34:57.000 --> 00:35:00.000
if there's a repeated data through your validation, it goes, "I already know what it is."

00:35:00.000 --> 00:35:03.000
What do you think? Yeah, I do think that would be

00:35:03.000 --> 00:35:06.000
possible. That's definitely an optimization we should try out and put in our

00:35:06.000 --> 00:35:09.000
docs for like the advanced, advanced performance tips.

00:35:09.000 --> 00:35:12.000
Yeah, because if you've got a thousand strings

00:35:12.000 --> 00:35:15.000
and then you know that word like it's

00:35:15.000 --> 00:35:18.000
maybe male, female, male, female, male, male,

00:35:18.000 --> 00:35:21.000
female, like that kind of where the data is repeated a bunch,

00:35:21.000 --> 00:35:24.000
then it could just

00:35:24.000 --> 00:35:27.000
go, "Yep, we already know that answer."

00:35:27.000 --> 00:35:30.000
Potentially, I don't know. Yeah, no, definitely.

00:35:30.000 --> 00:35:33.000
And I will say I don't know if it takes effect.

00:35:33.000 --> 00:35:36.000
I don't think it takes effect with discriminated unions because this logic

00:35:36.000 --> 00:35:39.000
is kind of in Python, but I will say

00:35:39.000 --> 00:35:42.000
we recently added a like string caching setting

00:35:42.000 --> 00:35:45.000
because we have kind of our own JSON parsing

00:35:45.000 --> 00:35:48.000
logic that we use in Pydantic Core. And so we added

00:35:48.000 --> 00:35:51.000
a string caching setting so that you don't have to

00:35:51.000 --> 00:35:54.000
rebuild the exact same strings every time.

00:35:54.000 --> 00:35:57.000
So that's a nice performance boost.

00:35:57.000 --> 00:36:00.000
Caching is awesome. Until it's not.

00:36:00.000 --> 00:36:03.000
Exactly. So one quick note here

00:36:03.000 --> 00:36:06.000
is just that discriminated unions are still JSON schema

00:36:06.000 --> 00:36:09.000
compatible, which is awesome for

00:36:09.000 --> 00:36:12.000
the case where you're once again defining like API requests

00:36:12.000 --> 00:36:15.000
and responses, you want to still have valid JSON schema coming out of

00:36:15.000 --> 00:36:18.000
those models. Yeah, very cool.

00:36:18.000 --> 00:36:21.000
And that might show up in things like

00:36:21.000 --> 00:36:24.000
open API documentation and stuff like that, right?

00:36:24.000 --> 00:36:27.000
Yep, exactly.

00:36:27.000 --> 00:36:30.000
So I'll kind of skip over this. We already touched on the

00:36:30.000 --> 00:36:33.000
callable discriminators, and then

00:36:33.000 --> 00:36:36.000
I'll leave these slides up here as a reference again.

00:36:36.000 --> 00:36:39.000
I don't think this is worth touching in too much detail, but

00:36:39.000 --> 00:36:42.000
just kind of another comment about if you've got

00:36:42.000 --> 00:36:45.000
nested models, that still works well with

00:36:45.000 --> 00:36:48.000
discriminated unions. So we're still on the pet example,

00:36:48.000 --> 00:36:51.000
but let's say this time you have a white cat

00:36:51.000 --> 00:36:54.000
and a black cat model, and then you also have

00:36:54.000 --> 00:36:57.000
your existing dog model. You can still

00:36:57.000 --> 00:37:00.000
create a union of

00:37:00.000 --> 00:37:03.000
your cat union is a union of black cat

00:37:03.000 --> 00:37:06.000
and white cat, and then you can union that with the dogs

00:37:06.000 --> 00:37:09.000
and it still works. And once again, you can kind of imagine

00:37:09.000 --> 00:37:12.000
the exponential blow up that would occur if you

00:37:12.000 --> 00:37:15.000
didn't use some sort of discriminator here in terms of errors.

00:37:15.000 --> 00:37:18.000
Yeah, very interesting. Okay, cool.

00:37:18.000 --> 00:37:21.000
Yeah, so that's kind of

00:37:21.000 --> 00:37:24.000
all in terms of my recommendations for

00:37:24.000 --> 00:37:27.000
discriminated union application. I would encourage folks who are

00:37:27.000 --> 00:37:30.000
interested in this to check out our documentation.

00:37:30.000 --> 00:37:33.000
It's pretty thorough in that regard, and I think we also have those links

00:37:33.000 --> 00:37:36.000
attached to the podcast.

00:37:36.000 --> 00:37:39.000
Yeah, absolutely.

00:37:39.000 --> 00:37:42.000
And then performance improvements in the pipeline. Is this something that

00:37:42.000 --> 00:37:45.000
we can control from the outside? Is this something that you all are just

00:37:45.000 --> 00:37:48.000
adding for us in the next version?

00:37:48.000 --> 00:37:51.000
Yeah, good question. This is hopefully, maybe not all in the next version,

00:37:51.000 --> 00:37:54.000
but just kind of things we're keeping our eyes on in terms of

00:37:54.000 --> 00:37:57.000
requested performance improvements and ideas that we have.

00:37:57.000 --> 00:38:00.000
I'll go a little bit out of order here. We've been talking a bunch

00:38:00.000 --> 00:38:03.000
about core schema and kind of

00:38:03.000 --> 00:38:06.000
how we're trying to optimize that, or

00:38:06.000 --> 00:38:09.000
just trying to optimize that, and that actually happens in Python.

00:38:09.000 --> 00:38:12.000
So one of the biggest things that we're trying to do is

00:38:12.000 --> 00:38:15.000
effectively speed up the core schema building process so that

00:38:15.000 --> 00:38:18.000
import times are faster and just

00:38:18.000 --> 00:38:21.000
Pydantic is more performant in general.

00:38:21.000 --> 00:38:24.000
Cool.

00:38:24.000 --> 00:38:27.000
So one thing that

00:38:27.000 --> 00:38:30.000
I'd like to ask about

00:38:30.000 --> 00:38:33.000
back on the Python side a little bit,

00:38:33.000 --> 00:38:36.000
suppose I've got some really large document,

00:38:36.000 --> 00:38:39.000
really nested document.

00:38:39.000 --> 00:38:42.000
Maybe I've converted some terrible XML thing into JSON

00:38:42.000 --> 00:38:45.000
or I don't know, something. And there's a little bit of

00:38:45.000 --> 00:38:48.000
structured schema that I care about, and then there's a whole bunch

00:38:48.000 --> 00:38:51.000
of other stuff that I could potentially

00:38:51.000 --> 00:38:54.000
create nested models to go to, but I don't

00:38:54.000 --> 00:38:57.000
really care about validating them. It's just whatever it is, it is.

00:38:57.000 --> 00:39:00.000
What if you just said that was a dictionary?

00:39:00.000 --> 00:39:03.000
Would that short-circuit a whole bunch of validation and stuff

00:39:03.000 --> 00:39:06.000
that would make it faster, potentially?

00:39:06.000 --> 00:39:09.000
Could it turn off the validation for a subset

00:39:09.000 --> 00:39:12.000
of the model if it's really big and deep

00:39:12.000 --> 00:39:15.000
and you don't really care for that part?

00:39:15.000 --> 00:39:18.000
Yeah, good question. So we offer an annotation called

00:39:18.000 --> 00:39:21.000
skip validation that you can apply to certain types.

00:39:21.000 --> 00:39:24.000
So that's kind of one approach. I think in the future

00:39:24.000 --> 00:39:27.000
it would be nice to offer a config setting so that you can

00:39:27.000 --> 00:39:30.000
more easily list features that you want to skip

00:39:30.000 --> 00:39:33.000
validation for instead of applying those on a field-by-field basis.

00:39:33.000 --> 00:39:36.000
And then the other thing is, if you only define

00:39:36.000 --> 00:39:39.000
your model in terms of the fields that you really care about

00:39:39.000 --> 00:39:42.000
from that very gigantic amount of data,

00:39:42.000 --> 00:39:45.000
we will just ignore the extra data that you pass in

00:39:45.000 --> 00:39:48.000
and pull out the relevant information.

00:39:48.000 --> 00:39:51.000
Right, okay. Good.

00:39:51.000 --> 00:39:54.000
Back to the pipeline.

00:39:54.000 --> 00:39:57.000
Yeah, back to the pipeline.

00:39:57.000 --> 00:40:00.000
So another improvement, we talked a little bit about

00:40:00.000 --> 00:40:03.000
potential parallelization of things or vectorization.

00:40:03.000 --> 00:40:06.000
One thing that I'm excited to learn more about in the future

00:40:06.000 --> 00:40:09.000
and that we've started working on is this thing called SIMD

00:40:09.000 --> 00:40:12.000
in Jitter, and that's our JSON iterable

00:40:12.000 --> 00:40:15.000
parser library that I was talking about.

00:40:15.000 --> 00:40:18.000
SIMD stands for Single Instruction Multiple Data.

00:40:18.000 --> 00:40:21.000
It basically means that you can do operations faster

00:40:21.000 --> 00:40:24.000
and that's with this kind of vectorization approach.

00:40:24.000 --> 00:40:27.000
I certainly don't claim to be an expert in SIMD

00:40:27.000 --> 00:40:30.000
but I know that it's improving our

00:40:30.000 --> 00:40:33.000
validation speeds in the

00:40:33.000 --> 00:40:36.000
department of JSON parsing, so that's something that we're hoping to

00:40:36.000 --> 00:40:39.000
support for a broader set

00:40:39.000 --> 00:40:42.000
of architectures going forward.

00:40:42.000 --> 00:40:45.000
Yeah, that's really cool. Almost like what Pandas does

00:40:45.000 --> 00:40:48.000
for Python, instead of looping over in validation

00:40:48.000 --> 00:40:51.000
and doing something to each piece, you just go, "This whole column,

00:40:51.000 --> 00:40:54.000
multiply it by two."

00:40:54.000 --> 00:40:57.000
I'm sure it's not implemented the same, but conceptually the same.

00:40:57.000 --> 00:41:00.000
Just to be clear.

00:41:00.000 --> 00:41:03.000
Very much so. And then the other two things in the pipeline that I'm going to mention

00:41:03.000 --> 00:41:06.000
are kind of related once again to the

00:41:06.000 --> 00:41:09.000
avoiding materializing things in Python if we can.

00:41:09.000 --> 00:41:12.000
We're even kind of extending that to avoiding materializing

00:41:12.000 --> 00:41:15.000
things in Rust if we don't have to.

00:41:15.000 --> 00:41:18.000
The first thing is, when we're parsing JSON in Rust, can we just

00:41:18.000 --> 00:41:21.000
do the validation as we kind of chomp through the JSON

00:41:21.000 --> 00:41:24.000
instead of materializing the JSON as

00:41:24.000 --> 00:41:27.000
a Rust object and then doing all the validation?

00:41:27.000 --> 00:41:30.000
Can we just do it in one pass?

00:41:30.000 --> 00:41:33.000
Is that almost like generators

00:41:33.000 --> 00:41:36.000
and iterables rather than loading all into memory

00:41:36.000 --> 00:41:39.000
at once and then processing it one at a time?

00:41:39.000 --> 00:41:42.000
Yeah, exactly. And it's kind of like,

00:41:42.000 --> 00:41:45.000
do you build the tree and then walk it three times

00:41:45.000 --> 00:41:48.000
or do you just do your operations every time you

00:41:48.000 --> 00:41:51.000
add something to the tree?

00:41:51.000 --> 00:41:54.000
And then the last performance improvement in the pipeline that I'll mention is

00:41:54.000 --> 00:41:57.000
this thing called FastModel. It has not been released yet.

00:41:57.000 --> 00:42:00.000
It hasn't really even been significantly developed,

00:42:00.000 --> 00:42:03.000
but this is cool in that it's really approaching

00:42:03.000 --> 00:42:06.000
that kind of laziness concept again.

00:42:06.000 --> 00:42:09.000
So, attributes would remain in Rust after validation until

00:42:09.000 --> 00:42:12.000
they're requested. So this is kind of along the lines of the

00:42:12.000 --> 00:42:15.000
defer/build logic that we were talking about in terms of

00:42:15.000 --> 00:42:18.000
we're not going to send you the data or perform the necessary

00:42:18.000 --> 00:42:21.000
operations until they're requested.

00:42:21.000 --> 00:42:24.000
Right. Okay. Yeah, if you don't ever access the field,

00:42:24.000 --> 00:42:27.000
then why process all that stuff and convert it

00:42:27.000 --> 00:42:30.000
into Python objects?

00:42:30.000 --> 00:42:33.000
Yeah, exactly. But yeah, we're kind of just excited in general

00:42:33.000 --> 00:42:36.000
to be looking at lots of performance improvements

00:42:36.000 --> 00:42:39.000
on our end, even after the big V2 speedup. Still have lots of

00:42:39.000 --> 00:42:42.000
other things to work on and improve.

00:42:42.000 --> 00:42:45.000
Yeah, it sure seems like it. And

00:42:45.000 --> 00:42:48.000
if this free-threaded Python thing takes off,

00:42:48.000 --> 00:42:51.000
who knows? Maybe there's even more craziness

00:42:51.000 --> 00:42:54.000
with parallel processing

00:42:54.000 --> 00:42:57.000
of different branches of the model

00:42:57.000 --> 00:43:00.000
alongside each other.

00:43:00.000 --> 00:43:03.000
Yeah. So I think this kind of

00:43:03.000 --> 00:43:06.000
dovetails nicely into, like, you asked

00:43:06.000 --> 00:43:09.000
earlier, is there a way that we kind of monitor the performance

00:43:09.000 --> 00:43:12.000
improvements that we're making? And

00:43:12.000 --> 00:43:15.000
we're currently using and getting started with

00:43:15.000 --> 00:43:18.000
two tools that are really helpful.

00:43:18.000 --> 00:43:21.000
And I can share some PRs if that's

00:43:21.000 --> 00:43:24.000
helpful and send links after. But one of them is

00:43:24.000 --> 00:43:27.000
CodSpeed, which integrates

00:43:27.000 --> 00:43:30.000
super nicely with CI

00:43:30.000 --> 00:43:33.000
and GitHub. And it basically runs tests

00:43:33.000 --> 00:43:36.000
tagged with this benchmark tag.

00:43:36.000 --> 00:43:39.000
And then it'll run them on

00:43:39.000 --> 00:43:42.000
main compared to on your branch. And then you can see, like,

00:43:42.000 --> 00:43:45.000
"Oh, this made my code 30% slower.

00:43:45.000 --> 00:43:48.000
Maybe let's not merge that right away."

00:43:48.000 --> 00:43:51.000
Or conversely, if there's a 30%

00:43:51.000 --> 00:43:54.000
improvement on some of your benchmarks, it's really nice to kind of track

00:43:54.000 --> 00:43:57.000
that.

00:43:57.000 --> 00:44:00.000
This is at codspeed.io, right?

00:44:00.000 --> 00:44:03.000
Yeah.

00:44:03.000 --> 00:44:06.000
And then it sets up as, say, a GitHub action as part of your

00:44:06.000 --> 00:44:09.000
CI/CD and probably automatically

00:44:09.000 --> 00:44:12.000
runs when a PR is open and things along those lines, right?

00:44:12.000 --> 00:44:15.000
Yep, exactly.

00:44:15.000 --> 00:44:18.000
All right, I've never heard of this.

00:44:18.000 --> 00:44:21.000
But if it just does the performance testing for yourself

00:44:21.000 --> 00:44:24.000
and you're like, "Oh, this is great. I'm going to try this out,"

00:44:24.000 --> 00:44:27.000
then why not, right? Let it do that.

00:44:27.000 --> 00:44:30.000
Yeah. And then I guess another tool

00:44:30.000 --> 00:44:33.000
that I'll mention while

00:44:33.000 --> 00:44:36.000
talking about kind of our, you know,

00:44:36.000 --> 00:44:39.000
continuous optimization is one word for it

00:44:39.000 --> 00:44:42.000
is this tool kind of similarly named

00:44:42.000 --> 00:44:45.000
called CodeFlash.

00:44:45.000 --> 00:44:48.000
So CodeFlash is a new tool that

00:44:48.000 --> 00:44:51.000
we're using to develop potentially more performant versions,

00:44:51.000 --> 00:44:54.000
kind of analyze those in terms of, you know,

00:44:54.000 --> 00:44:57.000
is this new code passing existing tests?

00:44:57.000 --> 00:45:00.000
Is it passing additional tests that we write?

00:45:00.000 --> 00:45:03.000
And then another great thing that it does

00:45:03.000 --> 00:45:06.000
is open PRs for you with those improvements and then

00:45:06.000 --> 00:45:09.000
explain the improvements. So I think it's a

00:45:09.000 --> 00:45:12.000
really pioneering tool in the space and we're

00:45:12.000 --> 00:45:15.000
excited to kind of experiment with it more on our

00:45:15.000 --> 00:45:18.000
own in our repository.

00:45:18.000 --> 00:45:21.000
I love it. Just tell me why is this,

00:45:21.000 --> 00:45:24.000
why did this slow down? Well, here's why.

00:45:24.000 --> 00:45:27.000
Yeah, exactly. And they offer both

00:45:27.000 --> 00:45:30.000
like local runs of the tool and also

00:45:30.000 --> 00:45:33.000
built-in CI support. So

00:45:33.000 --> 00:45:36.000
those are just kind of two tools that we use to use and are

00:45:36.000 --> 00:45:39.000
increasingly using to help us

00:45:39.000 --> 00:45:42.000
kind of check our performance as we continue to develop

00:45:42.000 --> 00:45:45.000
and really inspire us to get those green

00:45:45.000 --> 00:45:48.000
check marks with the performance improved on

00:45:48.000 --> 00:45:51.000
lots of PRs. Yeah, the more you can have it where

00:45:51.000 --> 00:45:54.000
if it passes the automated build,

00:45:54.000 --> 00:45:57.000
it's just ready to go and you don't have to

00:45:57.000 --> 00:46:00.000
worry a little bit and keep testing things and then have

00:46:00.000 --> 00:46:03.000
uncertainty, you know that.

00:46:03.000 --> 00:46:06.000
That's nice, right? Gives you a lot of, lets you rest

00:46:06.000 --> 00:46:09.000
and sleep at night. Yeah, most certainly.

00:46:09.000 --> 00:46:12.000
I mean, I said it before, but the

00:46:12.000 --> 00:46:15.000
number of people who are impacted by

00:46:15.000 --> 00:46:18.000
Pydantic, I don't know what that number is, but it has

00:46:18.000 --> 00:46:21.000
to be tremendous because if there's 400,000 projects that

00:46:21.000 --> 00:46:24.000
use it, like think of the users of those projects, right?

00:46:24.000 --> 00:46:27.000
That multiple has got to be big for,

00:46:27.000 --> 00:46:30.000
I'm sure there's some really popular ones. For example, FastAPI.

00:46:30.000 --> 00:46:33.000
Yeah, and it's just nice

00:46:33.000 --> 00:46:36.000
to know that there are other companies

00:46:36.000 --> 00:46:39.000
and tools out there that can help us to

00:46:39.000 --> 00:46:42.000
really boost the performance benefits for all those users,

00:46:42.000 --> 00:46:45.000
which is great.

00:46:45.000 --> 00:46:48.000
That is really cool. I think, you know, let's talk about one more

00:46:48.000 --> 00:46:51.000
performance benefit for people

00:46:51.000 --> 00:46:54.000
and not so much in how fast your code runs,

00:46:54.000 --> 00:46:57.000
but in how fast you go from

00:46:57.000 --> 00:47:00.000
raw data to Pydantic models.

00:47:00.000 --> 00:47:03.000
So one thing,

00:47:03.000 --> 00:47:06.000
we may have even spoken about this before,

00:47:06.000 --> 00:47:09.000
are you familiar with JSON to Pydantic?

00:47:09.000 --> 00:47:12.000
Yeah, it's a really cool tool.

00:47:12.000 --> 00:47:15.000
Yeah, it's such a cool tool. And if you've got some really complicated data,

00:47:15.000 --> 00:47:18.000
like let's see, I'll pull up some

00:47:18.000 --> 00:47:21.000
weather data that's in JSON format or something, right?

00:47:21.000 --> 00:47:24.000
Like if you just take this

00:47:24.000 --> 00:47:27.000
and you throw it in here, just don't even have to pretty print it,

00:47:27.000 --> 00:47:30.000
it'll just go, "Okay, well, it looks like what we've got is

00:47:30.000 --> 00:47:33.000
this really complicated nested model here."

00:47:33.000 --> 00:47:36.000
And it took, you know, we did this while I was talking,

00:47:36.000 --> 00:47:39.000
it took 10 seconds for me clicking the API to get a response

00:47:39.000 --> 00:47:42.000
to having a pretty decent representation here.

00:47:42.000 --> 00:47:45.000
Yeah, it's great in terms of

00:47:45.000 --> 00:47:48.000
developer agility, especially, right?

00:47:48.000 --> 00:47:51.000
It's like, "Oh, I've heard of this tool called Pydantic.

00:47:51.000 --> 00:47:54.000
I've seen it in places. I don't really know if I want to manually

00:47:54.000 --> 00:47:57.000
go build all these models for my super complicated JSON data."

00:47:57.000 --> 00:48:00.000
And it's like, "Oh, I've got to do this.

00:48:00.000 --> 00:48:03.000
I've got to do this. I've got to do this."

00:48:03.000 --> 00:48:06.000
And it's like, "Oh, I've got to do this. I've got to do this."

00:48:51.000 --> 00:48:54.000
And if you have a piece of JSON data and you say, "Convert that to Pydantic,"

00:48:54.000 --> 00:48:57.000
you'll get really good results.

00:48:57.000 --> 00:49:00.000
You have a little more control over than what you just get with this tool.

00:49:00.000 --> 00:49:05.000
But I think those two things, while not about run-time performance,

00:49:05.000 --> 00:49:08.000
going from, "I have data," to, "I'm working with Pydantic,"

00:49:08.000 --> 00:49:11.000
that's pretty awesome.

00:49:11.000 --> 00:49:14.000
Yeah, definitely.

00:49:14.000 --> 00:49:17.000
And if any passionate open-source contributors are listening

00:49:17.000 --> 00:49:20.000
and want to create a CLI tool for doing this locally,

00:49:20.000 --> 00:49:25.000
I think this is based on something that I don't use,

00:49:25.000 --> 00:49:30.000
but I think it's based on this data model code generator,

00:49:30.000 --> 00:49:33.000
which I think might be a CLI tool or a library.

00:49:33.000 --> 00:49:36.000
Let's see. Yes.

00:49:36.000 --> 00:49:39.000
Oh, yeah. Very nice.

00:49:39.000 --> 00:49:42.000
But here's the problem, though. You go and define a YAML file.

00:49:42.000 --> 00:49:45.000
It's just not as easy as, "There's a text field. I paste in my stuff."

00:49:45.000 --> 00:49:48.000
But it does technically work, I suppose.

00:49:48.000 --> 00:49:51.000
Yeah.

00:49:51.000 --> 00:49:54.000
But no, definitely the LLM approach or just the basic website approach

00:49:54.000 --> 00:49:57.000
is very quick, which is nice.

00:49:57.000 --> 00:50:00.000
Yeah. Speaking of LLMs, just really quick,

00:50:00.000 --> 00:50:03.000
you get some of the Python newsletters and other places,

00:50:03.000 --> 00:50:06.000
like, "Here's the cool new packages."

00:50:06.000 --> 00:50:09.000
A lot of them are like, 9 out of 10 of them are about LLMs these days.

00:50:09.000 --> 00:50:12.000
That feels a little over the top to me,

00:50:12.000 --> 00:50:15.000
but I know there's other things going on in the world.

00:50:15.000 --> 00:50:18.000
I know there's a lot of code development and coding these days.

00:50:18.000 --> 00:50:21.000
I know you write a lot of code and think about it a lot

00:50:21.000 --> 00:50:24.000
and probably use LLM somewhere in there.

00:50:24.000 --> 00:50:27.000
Yeah, no, for sure.

00:50:27.000 --> 00:50:30.000
I'm pretty optimistic and excited about it.

00:50:30.000 --> 00:50:33.000
I think there's a lot of good that can be done

00:50:33.000 --> 00:50:36.000
and a lot of productivity boosting to be had

00:50:36.000 --> 00:50:39.000
from integrating with these tools,

00:50:39.000 --> 00:50:42.000
both in your local development environment and also just in general.

00:50:42.000 --> 00:50:45.000
I think the performance department, like we can see with CodeFlash,

00:50:45.000 --> 00:50:48.000
using LLMs to help you write

00:50:48.000 --> 00:50:51.000
for performance code can also be really useful.

00:50:51.000 --> 00:50:54.000
It's been exciting to see some libraries really leverage

00:50:54.000 --> 00:50:57.000
Pydantic as well in that space in terms of validating LLM outputs

00:50:57.000 --> 00:51:00.000
or even using LLM calls

00:51:00.000 --> 00:51:03.000
in Pydantic validators to validate

00:51:03.000 --> 00:51:06.000
data along constraints that are more

00:51:06.000 --> 00:51:09.000
language model friendly.

00:51:09.000 --> 00:51:12.000
I'm optimistic about it.

00:51:12.000 --> 00:51:15.000
I still have a lot to learn, but it's cool to see

00:51:15.000 --> 00:51:18.000
the variety of applications and where you can plug in Pydantic

00:51:18.000 --> 00:51:21.000
in that process for fun.

00:51:21.000 --> 00:51:24.000
I totally agree. Right now, the context window,

00:51:24.000 --> 00:51:27.000
how much you can give it as information

00:51:27.000 --> 00:51:30.000
then to start asking questions is still a little bit small.

00:51:30.000 --> 00:51:33.000
You can't give it some huge program and say,

00:51:33.000 --> 00:51:36.000
"Find me the bugs where this function is called,"

00:51:36.000 --> 00:51:39.000
and not quite understand enough all at once.

00:51:39.000 --> 00:51:42.000
That thing keeps growing, so eventually, someday, we'll all see.

00:51:42.000 --> 00:51:45.000
Yep.

00:51:45.000 --> 00:51:48.000
All right.

00:51:48.000 --> 00:51:51.000
Let's talk just for a minute, maybe real quick,

00:51:51.000 --> 00:51:54.000
about what you all are doing at Pydantic,

00:51:54.000 --> 00:51:57.000
the company, rather than Pydantic, the open source library.

00:51:57.000 --> 00:52:00.000
What do you all got going on there?

00:52:00.000 --> 00:52:03.000
Yeah, sure.

00:52:03.000 --> 00:52:06.000
The company has released our first commercial tool.

00:52:06.000 --> 00:52:09.000
It's called LogFire, and it's in open beta.

00:52:09.000 --> 00:52:12.000
It's an observability platform,

00:52:12.000 --> 00:52:15.000
and we'd really encourage anyone interested to try it out.

00:52:15.000 --> 00:52:18.000
It's super easy to get started with,

00:52:18.000 --> 00:52:21.000
just the basic pip install of the SDK,

00:52:21.000 --> 00:52:24.000
and then start using it in your code base.

00:52:24.000 --> 00:52:27.000
Then we have the LogFire dashboard

00:52:27.000 --> 00:52:30.000
where you're going to see the observability and results.

00:52:30.000 --> 00:52:33.000
We adopt this needle in the haystack philosophy,

00:52:33.000 --> 00:52:36.000
where we want this to be a very easy-to-use

00:52:36.000 --> 00:52:39.000
observability platform that offers

00:52:39.000 --> 00:52:42.000
very Python-centric insights.

00:52:42.000 --> 00:52:45.000
It's this opinionated wrapper around open telemetry,

00:52:45.000 --> 00:52:48.000
if folks are familiar with that.

00:52:48.000 --> 00:52:51.000
But in the context of performance,

00:52:51.000 --> 00:52:54.000
one of the great things about this tool

00:52:54.000 --> 00:52:57.000
is that it offers this nested logging and profiling structure

00:52:57.000 --> 00:53:00.000
for code.

00:53:00.000 --> 00:53:03.000
It can be really helpful in looking at your code

00:53:03.000 --> 00:53:06.000
and being like, "We don't know where this performance

00:53:06.000 --> 00:53:09.000
slowdown is occurring, but if we integrate with LogFire,

00:53:09.000 --> 00:53:12.000
we can see that very easily in the dashboard."

00:53:12.000 --> 00:53:15.000
You have some interesting approaches,

00:53:15.000 --> 00:53:18.000
specifically targeting popular frameworks,

00:53:18.000 --> 00:53:21.000
like instrument FastAPI or something like that, right?

00:53:21.000 --> 00:53:24.000
Yeah, definitely.

00:53:24.000 --> 00:53:27.000
Trying to build integrations that work very well

00:53:27.000 --> 00:53:30.000
with FastAPI, other tools like that,

00:53:30.000 --> 00:53:33.000
and even also offering custom features in the dashboard.

00:53:33.000 --> 00:53:36.000
If you're using an observability tool,

00:53:36.000 --> 00:53:39.000
you're probably advanced enough to want to add some extra things

00:53:39.000 --> 00:53:42.000
to your dashboard, and we're working on supporting that

00:53:42.000 --> 00:53:45.000
with fast UI, which I know you've chatted with Samuel about as well.

00:53:45.000 --> 00:53:48.000
Yeah, absolutely.

00:53:48.000 --> 00:53:51.000
I got a chance to talk to Samuel about

00:53:51.000 --> 00:53:54.000
LogFire and some of the behind-the-scenes infrastructure.

00:53:54.000 --> 00:53:57.000
It was really interesting.

00:53:57.000 --> 00:54:00.000
But also, speaking of fast UI,

00:54:00.000 --> 00:54:03.000
I did speak to him. When was that? Back in February?

00:54:03.000 --> 00:54:06.000
Yeah.

00:54:06.000 --> 00:54:09.000
This is a really popular project, and even on the...

00:54:09.000 --> 00:54:12.000
I was like...

00:54:12.000 --> 00:54:15.000
Quite a few people decided that they were interested

00:54:15.000 --> 00:54:18.000
in even watching the video on that one.

00:54:18.000 --> 00:54:21.000
Anything with fast UI?

00:54:21.000 --> 00:54:24.000
Sorry, did you say anything with fast UI?

00:54:24.000 --> 00:54:27.000
Yeah.

00:54:27.000 --> 00:54:30.000
Are you doing anything on the fast UI side, or are you on the

00:54:30.000 --> 00:54:33.000
Pydantic side of things?

00:54:33.000 --> 00:54:36.000
Yeah, good question. I've been working mostly on Pydantic.

00:54:36.000 --> 00:54:39.000
Larger user base, more feature requests,

00:54:39.000 --> 00:54:42.000
but I've done a little bit on the fast UI side

00:54:42.000 --> 00:54:45.000
and excited to brush up on my TypeScript

00:54:45.000 --> 00:54:48.000
and build that out as a more robust and supported tool.

00:54:48.000 --> 00:54:51.000
I think, especially as we grow as a company and have more

00:54:51.000 --> 00:54:54.000
open source support in general, that'll be a priority for us,

00:54:54.000 --> 00:54:57.000
which is exciting.

00:54:57.000 --> 00:55:00.000
Yeah, it's an interesting project.

00:55:00.000 --> 00:55:03.000
Basically, a cool way to do

00:55:03.000 --> 00:55:06.000
JavaScript front ends and React and then plug those back into

00:55:06.000 --> 00:55:09.000
Python APIs like

00:55:09.000 --> 00:55:12.000
FastAPI and those types of things, right?

00:55:12.000 --> 00:55:15.000
Yeah, and kind of a similarity with fast UI and LogFire,

00:55:15.000 --> 00:55:18.000
the new tool, is that there's pretty seamless integration with

00:55:18.000 --> 00:55:21.000
Pydantic, which is definitely going to be one of the core

00:55:21.000 --> 00:55:24.000
tenets of any products or open source things that we're

00:55:24.000 --> 00:55:27.000
producing in the future.

00:55:27.000 --> 00:55:30.000
Yeah, I can imagine that's something you want to pay special

00:55:30.000 --> 00:55:33.000
attention to, is how well do these things fit together as a

00:55:33.000 --> 00:55:36.000
whole, rather than just, "Here's something interesting.

00:55:36.000 --> 00:55:39.000
Here's something interesting."

00:55:39.000 --> 00:55:42.000
That pretty much wraps it up for the time that we have to talk

00:55:42.000 --> 00:55:45.000
today.

00:55:45.000 --> 00:55:48.000
Let's close it out for us with maybe a final call to action

00:55:48.000 --> 00:55:51.000
for people who are already using Pydantic

00:55:51.000 --> 00:55:54.000
and they want it to go faster, or maybe they could adopt some

00:55:54.000 --> 00:55:57.000
of these tips. What do you tell them?

00:55:57.000 --> 00:56:00.000
Yeah, I would say

00:56:00.000 --> 00:56:03.000
inform yourself just a little bit about

00:56:03.000 --> 00:56:06.000
the Pydantic architecture, just in terms of

00:56:06.000 --> 00:56:09.000
what is core schema and why are we using Rust

00:56:09.000 --> 00:56:12.000
for validation and serialization?

00:56:12.000 --> 00:56:15.000
Then that can take you to the next steps of

00:56:15.000 --> 00:56:18.000
when do I want to build my core schemas based on the nature of my

00:56:18.000 --> 00:56:21.000
application? Is it okay if imports take a little bit longer, or

00:56:21.000 --> 00:56:24.000
do I want to delay that? Then take a look at

00:56:24.000 --> 00:56:27.000
discriminated unions. Then maybe, if you're

00:56:27.000 --> 00:56:30.000
really interested in improving performance across your application

00:56:30.000 --> 00:56:33.000
that supports Pydantic and other things,

00:56:33.000 --> 00:56:36.000
try out LogFire and just see what sort of benefits you can get there.

00:56:36.000 --> 00:56:39.000
Yeah, see where you're spending your time is

00:56:39.000 --> 00:56:42.000
one of the very,

00:56:42.000 --> 00:56:45.000
not just focused on Pydantic, but in general,

00:56:45.000 --> 00:56:48.000
our intuition is often pretty bad for

00:56:48.000 --> 00:56:51.000
where is your code slow and where is it not slow?

00:56:51.000 --> 00:56:54.000
You're like, "That looks really complicated. That must be slow."

00:56:54.000 --> 00:56:57.000
Nope, it's that one call to some sub-module that you didn't realize

00:56:57.000 --> 00:57:00.000
was terrible.

00:57:00.000 --> 00:57:03.000
I guess that circles back to the LLM

00:57:03.000 --> 00:57:06.000
tools and integrated performance

00:57:06.000 --> 00:57:09.000
analysis with Codd Speed and CodeFlash and even just other

00:57:09.000 --> 00:57:12.000
LLM tools, which is like, "Use the tools you have at hand."

00:57:12.000 --> 00:57:15.000
Sometimes they're better at performance

00:57:15.000 --> 00:57:18.000
improvements than you might be, or it can at least give you good tips that give you

00:57:18.000 --> 00:57:21.000
a launching point, which is great.

00:57:21.000 --> 00:57:24.000
For sure, or even good old C-profile built right in,

00:57:24.000 --> 00:57:27.000
if you want to do it that way.

00:57:27.000 --> 00:57:30.000
Awesome.

00:57:30.000 --> 00:57:33.000
Sydney, thank you for being back on the show and sharing all these tips.

00:57:33.000 --> 00:57:36.000
Congratulations on all the work you and the team are doing.

00:57:36.000 --> 00:57:39.000
What a success Pydantic is.

00:57:39.000 --> 00:57:42.000
Yeah, thank you so much for having me.

00:57:42.000 --> 00:57:45.000
It was wonderful to get to have this discussion with you and excited that I got to

00:57:45.000 --> 00:57:48.000
meet you in person at PyCon recently as well.

00:57:48.000 --> 00:57:51.000
Yeah, that was really great.

00:57:51.000 --> 00:57:54.000
Until next PyCon, see you later.

00:57:54.000 --> 00:57:57.000
Bye.

