WEBVTT

00:00:00.001 --> 00:00:04.000
Ines, welcome back to Talk Python to Me.

00:00:04.000 --> 00:00:07.000
Yeah, thanks for having me back again.

00:00:07.000 --> 00:00:12.000
Yes, you're one of my favorite guests. It's always awesome to have you on the show.

00:00:12.000 --> 00:00:14.000
Thanks, you're my favorite podcast.

00:00:14.000 --> 00:00:18.000
Thank you. And we have some really cool things to talk about.

00:00:18.000 --> 00:00:26.000
Spacey some, of course, but also more broadly, we're going to talk about just LLMs and AIs

00:00:26.000 --> 00:00:30.000
and open source and business models and even monopolies.

00:00:30.000 --> 00:00:32.000
We're going to cover a lot of things.

00:00:32.000 --> 00:00:41.000
You've been kind of making a bit of a roadshow, a tour around much of Europe

00:00:41.000 --> 00:00:43.000
and talking about some of these ideas, right?

00:00:43.000 --> 00:00:46.000
Yeah, I've gotten invited to quite a few conferences.

00:00:46.000 --> 00:00:52.000
And I feel like this is like after COVID, the first proper, proper year again

00:00:52.000 --> 00:00:55.000
that I'm traveling for conferences. And I was like, why not?

00:00:55.000 --> 00:00:58.000
And I think especially now that so much is happening in the AI space,

00:00:58.000 --> 00:01:04.000
I think it's actually really nice to go to these conferences and connect with actual developers.

00:01:04.000 --> 00:01:08.000
Because, you know, if you're just sitting on the Internet and you're scrolling,

00:01:08.000 --> 00:01:12.000
I don't know, LinkedIn, and sometimes it can be really hard to tell like what people are really thinking.

00:01:12.000 --> 00:01:20.000
And what's, you know, like, do people really believe some of these hot, weird takes

00:01:20.000 --> 00:01:22.000
that people are putting out there?

00:01:22.000 --> 00:01:27.000
And so, yeah, it was very, very nice to talk about some of these ideas,

00:01:27.000 --> 00:01:31.000
you know, get them checked against what developers think.

00:01:31.000 --> 00:01:33.000
So, yeah, it's been really cool.

00:01:33.000 --> 00:01:35.000
And there's more to come.

00:01:35.000 --> 00:01:36.000
Yeah, I know.

00:01:36.000 --> 00:01:41.000
I'll be traveling again later this month to Italy for PyCon for my first time,

00:01:41.000 --> 00:01:45.000
then Pymeta London, and who knows what else.

00:01:45.000 --> 00:01:51.000
So if you must go to Italy and London, what terrible places to spend time in, huh?

00:01:51.000 --> 00:01:57.000
Well, I'm definitely very open for tips, especially for Italy, for Florence.

00:01:57.000 --> 00:02:00.000
I've never been to Italy ever.

00:02:00.000 --> 00:02:01.000
Oh, awesome.

00:02:01.000 --> 00:02:05.000
I've been to Rome, but that's it, so I don't have any tips.

00:02:05.000 --> 00:02:08.000
But London is also fantastic.

00:02:08.000 --> 00:02:09.000
Cool.

00:02:09.000 --> 00:02:10.000
So people can check you out.

00:02:10.000 --> 00:02:15.000
Maybe, I think, do you have a list of that publicly where people can see some of your talks?

00:02:15.000 --> 00:02:17.000
We can put that in the show notes, yeah.

00:02:17.000 --> 00:02:21.000
Yeah, it's on my website, and then also on the Explosion site of our company,

00:02:21.000 --> 00:02:26.000
we've actually added an events page because it came up a lot that, like, either me, Matt,

00:02:26.000 --> 00:02:30.000
people from our team giving talks, and so we thought, like, hey, let's --

00:02:30.000 --> 00:02:33.000
and podcasts as well, so we're collecting everything on one page,

00:02:33.000 --> 00:02:35.000
all the stuff we're doing, which is kind of fun.

00:02:35.000 --> 00:02:37.000
Which is quite a bit, actually, for sure.

00:02:37.000 --> 00:02:38.000
Yeah.

00:02:39.000 --> 00:02:49.000
Well, I know many people know you, but let's just talk a little bit about Spacey, Explosion,

00:02:49.000 --> 00:02:54.000
Prodigy, the stuff that you guys are doing to give people a sense of where you're coming from.

00:02:54.000 --> 00:03:01.000
Yeah, so we're basically an open source company, and we build developer tools for AI,

00:03:01.000 --> 00:03:06.000
natural language processing specifically, so, you know, you're working with lots of text,

00:03:06.000 --> 00:03:09.000
and you want to analyze it beyond just looking for keywords.

00:03:09.000 --> 00:03:14.000
That's kind of where we started and what we've always been focusing on.

00:03:14.000 --> 00:03:19.000
So Spacey is probably what we're mostly known for, which is a popular open source library

00:03:19.000 --> 00:03:26.000
for really what we call industrial strength NLP, so built for production.

00:03:26.000 --> 00:03:27.000
It's fast. It's efficient.

00:03:27.000 --> 00:03:36.000
We've put a lot of work into having good usable, user-friendly, developer-friendly APIs.

00:03:36.000 --> 00:03:38.000
Actually, yeah, I always set an example.

00:03:38.000 --> 00:03:39.000
I always like to show my talks.

00:03:39.000 --> 00:03:44.000
A nice side effect that we never anticipated like that is that ChapterPT and similar models

00:03:44.000 --> 00:03:50.000
are actually pretty good at writing Spacey code because we put a lot of work into all of this stuff

00:03:50.000 --> 00:03:55.000
like backwards compatibility, not breaking people's code all the time, stuff like that.

00:03:55.000 --> 00:04:01.000
But that happens to really help, at least for now, with these models.

00:04:01.000 --> 00:04:04.000
Yeah, it's really nice.

00:04:04.000 --> 00:04:08.000
It's a good thing you've done to make it a really stable API that people can trust.

00:04:08.000 --> 00:04:14.000
But is it weird to see LLMs talking about stuff you all created?

00:04:14.000 --> 00:04:17.000
I mean, it's kind of funny in some way.

00:04:17.000 --> 00:04:29.000
I mean, it's also there is this whole other side to doing user support for detecting clearly auto-generated code

00:04:29.000 --> 00:04:33.000
because for Spacey, these models are pretty good for Prodigy, which is our annotation tool,

00:04:33.000 --> 00:04:35.000
which is also scriptable in Python.

00:04:35.000 --> 00:04:41.000
It's a bit less precise, and it hallucinates a lot because there's just less code online and on GitHub.

00:04:41.000 --> 00:04:47.000
And so we sometimes get support requests where users post their code and we're like,

00:04:47.000 --> 00:04:51.000
"This is so strange. How did you find these APIs? They don't exist."

00:04:51.000 --> 00:04:55.000
And then we're like, "Ah, this was auto-generated. Oh, OK."

00:04:55.000 --> 00:04:57.000
So that was a very new experience.

00:04:57.000 --> 00:05:01.000
And also, I think everyone who publishes online deals with that,

00:05:01.000 --> 00:05:09.000
but it's very frustrating to see all these auto-generated posts that look like tech posts

00:05:09.000 --> 00:05:12.000
but are completely bullshit and completely wrong.

00:05:12.000 --> 00:05:19.000
Like I saw something on Spacey LLM, which is our extension for integrating large language models into Spacey,

00:05:19.000 --> 00:05:26.000
and they're like some blog posts that look like they're tech blog posts, but they're completely hallucinated.

00:05:26.000 --> 00:05:31.000
And it's very, very strange to see that about your own software.

00:05:31.000 --> 00:05:35.000
And also it frustrates me because that stuff is going to feed into the next generation of these models, right?

00:05:35.000 --> 00:05:45.000
And then they will stop being potentially so good at this because they're full of stuff that they've generated themselves

00:05:45.000 --> 00:05:48.000
on like APIs and things that don't even exist.

00:05:48.000 --> 00:05:52.000
Cycle around and around and around until it just gets worse every time.

00:05:52.000 --> 00:06:00.000
Yeah. So that's interesting. It's very interesting to see what's going on and where these things lead.

00:06:00.000 --> 00:06:13.000
It is. I just had a thought. OpenAI and some of these different companies are doing work to try to detect AI-generated images.

00:06:13.000 --> 00:06:15.000
And I imagine AI-generated content.

00:06:15.000 --> 00:06:20.000
When I heard that, my thought was like, well, that's just because they kind of want to be good citizens

00:06:20.000 --> 00:06:26.000
and they want to put little labels and say, what if it's just so they don't ingest it twice?

00:06:26.000 --> 00:06:33.000
I think that's definitely, I mean, in a way it's also good because it would make these models worse.

00:06:33.000 --> 00:06:40.000
And so from a product perspective for a company like OpenAI, that's definitely very useful.

00:06:40.000 --> 00:06:49.000
And I think also commercially, I think there's definitely a big market in that also for social networks and stuff

00:06:49.000 --> 00:06:56.000
to detect are these real images, are these deep fakes? Is there any money in that?

00:06:56.000 --> 00:07:04.000
So it's not, I don't think it's just being good citizens, but like this, there's a clear product-combinated thing,

00:07:04.000 --> 00:07:05.000
which is fine for a company.

00:07:05.000 --> 00:07:10.000
Yeah, it's fine. Yeah, it is fine. I just, I never really thought about it. Of course.

00:07:10.000 --> 00:07:24.000
Do you think we'll get to some point where in food or art, you hear about artisanal handcrafted pizza or whatever,

00:07:24.000 --> 00:07:30.000
will there be artisanal human-created tech that has got a special flavor to it?

00:07:30.000 --> 00:07:34.000
Like this was created with no AI. Look at how cool this site is or whatever.

00:07:34.000 --> 00:07:38.000
I think it's already something that you see, I don't know which product this was,

00:07:38.000 --> 00:07:44.000
but there was some ad campaign, I think it might've been a language learning app or something else

00:07:44.000 --> 00:07:51.000
where they really put that into one of their marketing claims, like, hey, it's not AI generated.

00:07:51.000 --> 00:07:56.000
We don't use AI. It's actually real in humans because it seems to be what people want.

00:07:56.000 --> 00:08:04.000
They want to have at least that feeling. So I definitely think there's an appeal of that also going forward.

00:08:04.000 --> 00:08:10.000
The whole LLM and AI stuff, it's permeated culture so much.

00:08:10.000 --> 00:08:15.000
I was at the motorcycle shop yesterday talking to a guy who was a motorcycle salesman.

00:08:15.000 --> 00:08:20.000
He was like, do you think that AI is going to change how software developers work?

00:08:20.000 --> 00:08:24.000
Do you think they're still going to be relevant? I'm like, you're a motorcycle sales guy. That's amazing.

00:08:24.000 --> 00:08:28.000
You're like really this tuned into it, right?

00:08:28.000 --> 00:08:33.000
But you think it's maybe just a little echo chamber of us talking about,

00:08:33.000 --> 00:08:36.000
these kinds of conversations are more broad than maybe you would have guessed.

00:08:36.000 --> 00:08:41.000
And I think ChatGPT definitely brought the conversation into the mainstream.

00:08:41.000 --> 00:08:47.000
But on the other hand, on the plus side, it also means it makes it a lot easier for us to explain our work

00:08:47.000 --> 00:08:53.000
because people have at least heard of this. And I think it's also for developers working in teams.

00:08:53.000 --> 00:08:57.000
On the one hand, it can maybe be frustrating to do this expectation management

00:08:57.000 --> 00:09:03.000
because you have management who just came back from some fancy conference and got sold on like,

00:09:03.000 --> 00:09:11.000
oh, we need some chatbot or LLM. It's kind of the chatbot hype all over again that we already had in 2015 or so.

00:09:11.000 --> 00:09:15.000
I forgot all about that. Those were going to be so important. And now what are they doing?

00:09:15.000 --> 00:09:23.000
Yeah, but I see a lot of parallels. If you look at the hype cycle and people's expectations and expectation management,

00:09:23.000 --> 00:09:30.000
it's kind of the same thing in a lot of ways, only that like it actually cuts a lot of parts actually kind of work now,

00:09:30.000 --> 00:09:37.000
which we didn't really have before. But yeah, it also means for teams and developers that they at least have some more funding available

00:09:37.000 --> 00:09:41.000
and resources that they can work with, because I feel like before that happened,

00:09:41.000 --> 00:09:49.000
it looked like that companies are really cutting their budgets, all these exploratory AI projects, they all got cut.

00:09:49.000 --> 00:09:57.000
And it was quite frustrating for a lot of developers. And now at least it means they can actually work again,

00:09:57.000 --> 00:10:08.000
even though they also have to kind of manage the expectations and work around some of the very wild ideas that companies might have at the moment.

00:10:08.000 --> 00:10:13.000
Absolutely. Now, one of the things that's really important and we're going to get to here,

00:10:13.000 --> 00:10:20.000
and give you a chance to give a shout out to the other thing that you all have is, how do you teach these things information?

00:10:20.000 --> 00:10:28.000
And how do you get them to know things and so on? And for the spaCy world, you have Prodigy and maybe give a shout out to Prodigy Teams.

00:10:28.000 --> 00:10:31.000
That's something you just are just announcing, right?

00:10:31.000 --> 00:10:34.000
Yeah, so that's currently in beta. It's something we've been working on.

00:10:34.000 --> 00:10:40.000
So the idea of Prodigy has always been, hey, it supports spaCy, also other libraries.

00:10:40.000 --> 00:10:45.000
And how can we make the training and data collection process more efficient,

00:10:45.000 --> 00:10:53.000
or so efficient that companies can in-house that process, like whether it's creating training data, creating evaluation data,

00:10:53.000 --> 00:10:58.000
like even if what you're doing is completely generative, and you have a model that does it well,

00:10:58.000 --> 00:11:01.000
you need some examples and some data where you know the answer.

00:11:01.000 --> 00:11:05.000
And often that's a structured data format. So we need to create that.

00:11:05.000 --> 00:11:10.000
And we've really seen that outsourcing that doesn't work very well.

00:11:10.000 --> 00:11:17.000
And also now with the newer technologies, you transfer learning, you don't need millions of examples anymore.

00:11:17.000 --> 00:11:24.000
This big, big data idea for task specific stuff is really dead in a lot of ways.

00:11:24.000 --> 00:11:38.000
So Prodigy is a developer tool that you can script in Python, and that makes it easy to really collect this kind of structured data on text, images, and so on.

00:11:38.000 --> 00:11:42.000
And then Prodigy Teams, that has been a very ambitious project.

00:11:42.000 --> 00:11:46.000
We've really been, we've wanted to ship this a long time ago already.

00:11:46.000 --> 00:11:53.000
But it's been very challenging because we basically want to bring also a lot of these ideas that probably we're going to talk about today a bit into the cloud.

00:11:53.000 --> 00:11:58.000
While retaining the data privacy.

00:11:58.000 --> 00:12:06.000
And so, you know, you'll be able to run your own cluster on your own infrastructure that has the data that's scriptable in Python.

00:12:06.000 --> 00:12:11.000
So you can kind of script the SaaS app in Python, which is very cool, which you normally can't do.

00:12:11.000 --> 00:12:14.000
Your data never leaves our service.

00:12:14.000 --> 00:12:28.000
And, yeah, and you can basically also use these workflows like distillation, where you start out with a super easy prototype that might use Lama or some other models,

00:12:28.000 --> 00:12:44.000
like GPT, GPT-4, then you benchmark that, see how it does. And then you collect some data until you can beat that inaccuracy and have a task specific model that really only does the one extraction you're interested in.

00:12:44.000 --> 00:12:49.000
And that model can be tiny, like we've had users build models that are under 10 megabytes.

00:12:49.000 --> 00:12:53.000
Like that's, that is pretty crazy to think about these days.

00:12:53.000 --> 00:12:58.000
And that run like 20 times faster, they're entirely private.

00:12:58.000 --> 00:13:01.000
You can, you know, you don't need like tons of compute to run them.

00:13:01.000 --> 00:13:05.000
And that's kind of really one of the workflows of the future that we see as very promising.

00:13:05.000 --> 00:13:14.000
And it's also people are often surprised how little task specific data you actually need to say beat GPT-4 inaccuracy.

00:13:14.000 --> 00:13:17.000
It's not as much as people think.

00:13:17.000 --> 00:13:22.000
And it's totally, you know, in a single workday, you could often do it.

00:13:22.000 --> 00:13:35.000
So, so the main idea, the main idea we've been thinking about a lot is basically how can we make that workflow better and more user friendly, even for people who don't have an extensive machine learning background.

00:13:35.000 --> 00:13:42.000
Because one thing that like prompting an LLM or prompting a generative model has is that it's a very low barrier to entry.

00:13:42.000 --> 00:13:45.000
And it's very, very, the UX is very good.

00:13:45.000 --> 00:13:49.000
You just type in a question, you talk to it the way you would talk to a human.

00:13:49.000 --> 00:13:54.000
And that's, you know, that's easy to get started with.

00:13:54.000 --> 00:13:59.000
And the workflow that's a bit more involved, yes, machine learning developers know how to do that.

00:13:59.000 --> 00:14:01.000
And they know when to do it.

00:14:01.000 --> 00:14:06.000
But it's not as accessible to people who don't have all of that experience.

00:14:06.000 --> 00:14:11.000
And so that's kind of the underlying thing that we're trying to solve.

00:14:11.000 --> 00:14:17.000
Yeah. You talked about transfer learning and using relatively small amounts of data to specialize models.

00:14:17.000 --> 00:14:19.000
Tell people about what that is.

00:14:19.000 --> 00:14:21.000
How do you actually do that?

00:14:21.000 --> 00:14:29.000
Yeah, so basically, I mean, it's actually the same idea that has ultimately really led to these large generative models that we see.

00:14:29.000 --> 00:14:37.000
And that's essentially realizing that we can learn a lot about the language and the world.

00:14:37.000 --> 00:14:41.000
And, you know, a lot of general stuff from raw text.

00:14:41.000 --> 00:14:50.000
Like if we just train a model with a language modeling objective on like a bunch of text on the whole internet or parts of the internet or whatever.

00:14:50.000 --> 00:14:56.000
In order to basically solve the task, which can be stuff like predict the next word.

00:14:56.000 --> 00:15:03.000
In order to do that, the model has to learn so much in its weights and in its representations about the language.

00:15:03.000 --> 00:15:10.000
And about like really underlying subtle stuff about the language that it's also really good at other stuff.

00:15:10.000 --> 00:15:13.000
So that's kind of in a nutshell, the basic idea.

00:15:13.000 --> 00:15:20.000
And that's then later led to, you know, larger and larger models and more and more of these ideas.

00:15:20.000 --> 00:15:30.000
But yeah, the basic concept is if you just train on a lot of raw text and a lot of these models are available, like something like BERT.

00:15:30.000 --> 00:15:33.000
That's already like quite a few years old.

00:15:33.000 --> 00:15:39.000
But still, if you look at kind of the literature and look at the experiments people are doing, it's still very competitive.

00:15:39.000 --> 00:15:44.000
Like you get really good results, even with the most one of the most basic foundation models.

00:15:44.000 --> 00:15:54.000
And you can use that initialize your model with that and then just train like a small task network on top instead of training everything from scratch, which is what you had to do before.

00:15:54.000 --> 00:16:05.000
Or it's like if you imagine hiring a new employee, it's like, yes, you expect, you know, you don't you can raise them from birth or you can sort of have them, which is like a very creepy concept.

00:16:05.000 --> 00:16:11.000
But it's really similar. And yeah, teach them, teach them everything.

00:16:11.000 --> 00:16:14.000
You were born to be a barista. Let me tell you.

00:16:14.000 --> 00:16:18.000
Yeah. And then you teach them English and you teach them. Yeah.

00:16:18.000 --> 00:16:23.000
Yeah. It's a lot of work. And I guess you know this more than me because you have you have kids. Right.

00:16:23.000 --> 00:16:32.000
So, yeah. So and, you know, it's understandable that like, OK, this this this made a lot of these ML projects really hard.

00:16:32.000 --> 00:16:37.000
But now, you know, you actually have the employee come in and they can they know how to talk to people.

00:16:37.000 --> 00:16:43.000
They speak the language and all you have to teach them is like, hey, here's how you make a coffee here.

00:16:43.000 --> 00:16:52.000
Exactly. Yeah. Basically lean on the school system to say, you know, they know the language, they know arithmetic.

00:16:52.000 --> 00:16:56.000
Yeah, exactly. People. I just need to show them how this espresso machine works.

00:16:56.000 --> 00:17:03.000
Here's how you check in. Please take out the trash every two hours. Like, yeah, very little bit of specialized information.

00:17:03.000 --> 00:17:08.000
But you exactly are a general working human knowledge is like the base LLM. Right.

00:17:08.000 --> 00:17:13.000
Yeah. Yeah. So that's the idea. And also transfer learning.

00:17:13.000 --> 00:17:19.000
It's still it's just one technology. And in context learning, which is what we have with these generative models.

00:17:19.000 --> 00:17:29.000
That's also just another technique. Like it's you know, it's not the case that transfer learning is sort of outdated or has been replaced by in context learning.

00:17:29.000 --> 00:17:36.000
It's two different strategies and you use them in different contexts. So, yeah.

00:17:36.000 --> 00:17:41.000
Yeah. Cool. Another thing I want to touch on for people.

00:17:41.000 --> 00:17:54.000
I know some people probably everyone listens more or less aware of this, but in practice, you know, a lot of folks out there listening is certainly the ones who are not in the ML or developer space.

00:17:54.000 --> 00:18:02.000
They just go to chat or they go to somewhere and they're like, this is the AI I've gone to. Right.

00:18:02.000 --> 00:18:13.000
Maybe they go to Bard. I don't know. Gemini or whatever they call it. But there's a whole bunch. I mean, many, many, many open source models with all sorts of variations.

00:18:13.000 --> 00:18:22.000
One thing I really like is LM Studio. Ian Moir introduced this to me, introduced me to it a couple of months ago.

00:18:22.000 --> 00:18:30.000
And basically, it's a UI for exploring hugging face models and then downloading them and running them with a chat interface just in a UI.

00:18:30.000 --> 00:18:36.000
And the really cool thing is they just added Lama 3. But a lot of these are open source. A lot of these are accessible.

00:18:36.000 --> 00:18:41.000
You run them on your machine. You get 7 billion parameter models run easily on my Mac mini.

00:18:41.000 --> 00:18:45.000
Yeah. What do you think about some of these models? Huge ones.

00:18:45.000 --> 00:18:55.000
Yeah. No, I think it's and also a lot of them are like, you know, the model itself is not necessarily much smaller than what a lot of these chat systems deploy.

00:18:55.000 --> 00:19:06.000
And I think it's also, you know, these are really just the core models for every for everything that's like proprietary and sort of in-house behind like an API.

00:19:06.000 --> 00:19:12.000
There's at least one open source version that's very similar.

00:19:12.000 --> 00:19:20.000
Like, I think it's, you know, you can you know, the whole model is really based on academic research.

00:19:20.000 --> 00:19:32.000
A lot of the same data that's available. And I think the most important differentiation we see is then around these chat assistants and how they work and how the products are designed.

00:19:32.000 --> 00:19:44.000
So I think it's also this is it's kind of a nice exercise or a nice way to look at this distinction between the products versus the machine facing models.

00:19:44.000 --> 00:19:53.000
Because I think that's the AI or like, you know, these products are more than just a model. And I think that's like a super important thing to keep in mind.

00:19:53.000 --> 00:20:04.000
Yeah. It's really relevant for this conversation because you have a whole section where you talk about regulation and, you know, what is the thing?

00:20:04.000 --> 00:20:08.000
What is the aspect of these things that should or could be regulated? We'll get to that in a minute.

00:20:08.000 --> 00:20:18.000
Yeah, but I think actually a lot of the confusion that people have around like, oh, are we like, is all AI going to be locked away behind APIs?

00:20:18.000 --> 00:20:28.000
And how do these bigger, bigger models work? I think it kind of stems from the fact that like, it's not always the distinction between like the models and products isn't always clear.

00:20:28.000 --> 00:20:36.000
And, you know, you could even argue maybe some companies that are in this business, you know, it benefits them to call everything the AI.

00:20:36.000 --> 00:20:41.000
And that doesn't that really doesn't help. So to hear you really see the models.

00:20:41.000 --> 00:20:57.000
Yeah. And just sorry to talk over you, but to give people a sense, even if you search for Lama 3 in this thing, there's 192 different configured, modified, etc. ways to work with the Lama 3 model, which is just crazy.

00:20:57.000 --> 00:21:02.000
So there's a lot of stuff that maybe people haven't really explored, I imagine.

00:21:02.000 --> 00:21:04.000
Yeah, it's very cool.

00:21:04.000 --> 00:21:10.000
Yeah. One other thing about this, just while we're on it, is it also comes with an open API.

00:21:10.000 --> 00:21:16.000
So you could just run it and say, turn on a server API and point it if you want to talk to it. Very fun.

00:21:16.000 --> 00:21:21.000
But let's talk about some of the things you talked about in your talk.

00:21:21.000 --> 00:21:28.000
The AI revolution will not be monopolized. How open source beats economies of scale, even for LLMs. I love it.

00:21:28.000 --> 00:21:30.000
It's a great title and a great topic.

00:21:30.000 --> 00:21:42.000
Thanks. It's something I'm very passionate about. I was very happy to be able to say a lot of these things, to be given a platform to say things.

00:21:42.000 --> 00:21:50.000
Yeah, you and I have spoken before about open source and running successful businesses in the tech space and all sorts of things.

00:21:50.000 --> 00:21:53.000
So it's a cool follow-on, for sure.

00:21:53.000 --> 00:22:03.000
I think one of the first parts that you talked about that was really interesting and has nothing to do specifically with LLMs or AI is,

00:22:03.000 --> 00:22:12.000
why is open source a good choice? Why are people choosing it? Why is it a good thing to base businesses on?

00:22:12.000 --> 00:22:22.000
Yeah. Also often when I give this as a talk, I ask for a show of hands, like, hey, who uses open source software?

00:22:22.000 --> 00:22:26.000
Who works for a company that depends on open source software? Who's contributed before?

00:22:26.000 --> 00:22:34.000
And usually I think most people raise their hand when I ask who works for a company that relies on open source software.

00:22:34.000 --> 00:22:43.000
So I often feel like I don't even have to explain, hey, it's a thing. It's more about collecting these reasons.

00:22:43.000 --> 00:22:50.000
And I do think a lot of it is around the transparency, the extensibility. It's all kind of connected.

00:22:50.000 --> 00:22:57.000
You're not locked in. You can run it in-house. You can fork it. You can program with it.

00:22:57.000 --> 00:23:01.000
Those are all important things for companies when they adopt software.

00:23:01.000 --> 00:23:07.000
And you also often have these small teams running the project. They can accept PRs. They can move fast.

00:23:07.000 --> 00:23:13.000
There's a community around it that can basically give you a sense for, hey, is this a thing? Should I adopt it?

00:23:13.000 --> 00:23:15.000
And all of this, I think, is important.

00:23:15.000 --> 00:23:25.000
And I also often make a point to, yes, I always mention, hey, it's also free, which is what people usually associate with open source software.

00:23:25.000 --> 00:23:32.000
It's kind of the first thing that comes to mind. But I actually don't think this is, for companies, the main motivation why they use open source software.

00:23:32.000 --> 00:23:42.000
I absolutely agree. Even though we have FOSS, free and open source software, this is not really why companies care about it.

00:23:42.000 --> 00:23:49.000
Certainly some people do. Some people don't. But companies, they often see that as a negative, I think.

00:23:49.000 --> 00:23:55.000
Almost like, well, who do we sue if this goes wrong? Where's our service level agreement? Who's going to help us?

00:23:55.000 --> 00:23:58.000
Who's legally obligated to help us?

00:23:58.000 --> 00:24:03.000
We've definitely also seen that. Companies are like, well, who can we pay?

00:24:03.000 --> 00:24:09.000
Or can we pay to get some guarantee or some support?

00:24:09.000 --> 00:24:21.000
Or can you confirm to us that, hey, if there is a critical vulnerability that's really directly affecting our software, which has never happened, but are you going to fix it?

00:24:21.000 --> 00:24:24.000
And we're like, yes, we can say that. That's what we've been doing.

00:24:24.000 --> 00:24:28.000
But if you want that guarantee, we can give that to you for money, sure.

00:24:28.000 --> 00:24:33.000
You can pay us. We'll promise to do what we already promised to do. But we're really double, double promising.

00:24:33.000 --> 00:24:42.000
Yeah. So that's definitely a thing. And also, to go back up to the business model thing, it's what we've seen with Prodigy, which we offer.

00:24:42.000 --> 00:24:50.000
Really, as a tool, it follows the open source spirit. You pip install it. It's a Python library. You work with it.

00:24:50.000 --> 00:25:02.000
But we decided to kind of use that as a stepping stone between our free open source offering and the SaaS product that we're about to launch soon, hopefully.

00:25:02.000 --> 00:25:11.000
And it's kind of in the middle and it's paid. And we've definitely not found that this is a huge disadvantage for companies.

00:25:11.000 --> 00:25:23.000
Like, yes, sure, you always have companies with no budget, but those are also usually not the teams that are really doing a lot of the high value work, because it is quite normal to have a budget.

00:25:23.000 --> 00:25:33.000
Or like software tools, companies pay a lot for this. If you want to buy Prodigy, that costs less than, I don't know, getting a decent office chair.

00:25:33.000 --> 00:25:44.000
But in a commercial context, these scales are all a bit different. So yeah, I do think companies are happy to pay for something that they need and that's good.

00:25:44.000 --> 00:25:54.000
Yeah. And the ones who wouldn't have paid, maybe they, you know, there's a group who said, well, maybe I'll use the free one.

00:25:54.000 --> 00:26:04.000
But they're not serious enough about it to actually pay for it or actually make use of it. I think of analogies of piracy, right?

00:26:04.000 --> 00:26:11.000
Like, they stole our app or they stole our music. Well, because it was a link, they clicked it, but they wouldn't have bought it or used it at all.

00:26:11.000 --> 00:26:15.000
It's not like you lost a customer because they were not going to be customers. They just happened.

00:26:15.000 --> 00:26:23.000
I mean, it's like I always tell the story of like, when I was a teenager, I did download a cracked version of Adobe Photoshop.

00:26:23.000 --> 00:26:29.000
And because I was a teenager, I would have never been able to like back then they had they didn't have a SaaS model.

00:26:29.000 --> 00:26:34.000
Like, I don't know what Photoshop costs, but like, it's definitely not something I would have been able to afford as a 13, 14 year old.

00:26:34.000 --> 00:26:41.000
So I did find that online. I downloaded it. I'm pretty sure if Adobe had wanted, they could have come after me for that.

00:26:41.000 --> 00:26:51.000
And I do think like, I don't know, maybe I'm giving them too much credit, but I do think they might have not done that because they're like, well, what, you know, it's not like we lost a customer here.

00:26:51.000 --> 00:26:55.000
And now I'm an adult and I'm, I'm proficient at Photoshop and now I'm paying for it.

00:26:55.000 --> 00:27:03.000
And I think there was this whole generation of teenagers who then maybe went into creative jobs and came in with Photoshop skills.

00:27:03.000 --> 00:27:09.000
Like I wasn't even like, compared to all these other teenagers I was hanging out with on the internet, like all these, like mostly, mostly girls.

00:27:09.000 --> 00:27:19.000
I wasn't even that talented at Photoshop specifically. So, you know, maybe, maybe there was someone smart who thought about this as like a business strategy.

00:27:19.000 --> 00:27:23.000
Let these teenagers have our, our professional tools.

00:27:23.000 --> 00:27:27.000
Yeah, yeah, exactly. It's, it's almost marketing.

00:27:27.000 --> 00:27:51.000
Okay. So another aspect here that I think is really relevant to LLMs is runs in house, aka we're not sending our private data, private source code, API keys, et cetera, to other companies that may even use that to train their models, which then regurgitate that back to other people trying to solve the same problems. Right?

00:27:51.000 --> 00:27:57.000
Yeah, no, and I think that's also, we're definitely seeing that companies are becoming more and more aware of this, which is good.

00:27:57.000 --> 00:28:06.000
Like in a lot of industries, like I wouldn't want, I don't know, my healthcare provider to just upload all of my data to like whichever SaaS tool they decide to use at the moment.

00:28:06.000 --> 00:28:16.000
Like, you know, of course not. So I think it's, you know, it's, it's good. And then also with, you know, more data privacy regulations, that's all, that's really on people's minds.

00:28:16.000 --> 00:28:26.000
And, you know, people, people don't, don't want this. Like often we have, we have companies or users who actually have to run a lot of their AI stuff on completely air gap machines.

00:28:26.000 --> 00:28:31.000
So they can't even have internet access or it's about, you know, financial stuff.

00:28:31.000 --> 00:28:40.000
We're actually working on a case study that we're hoping to publish soon where even the financial information can move markets. It's even segregated in the office.

00:28:40.000 --> 00:28:59.000
So it needs to be 100% in house. And that, that makes sense. And I think open source software, it's great because you can do that and you can build your own things with it and really decide how you want to host it, how it fits into your existing stack.

00:28:59.000 --> 00:29:10.000
That's another big thing. People will already use some tools and, you know, you don't want to change your entire workflow for every different tool or platform you use.

00:29:10.000 --> 00:29:22.000
And I think especially people have been burned by that so many times by now. And there's so many like, you know, unreliable startups, things, you know, you, you have, there's a company that really tries to convince you to build on their product.

00:29:22.000 --> 00:29:38.000
And then two months later, they close everything down or, you know, it doesn't even have to be startup, you know, Google. I'm still mad at Google for shutting down Google reader. And I don't know, it's been over 10 years, I'm sure. And I'm still angry about that.

00:29:38.000 --> 00:29:53.000
I actually had a, we did it, we were invited to give a talk at Google, and I needed a text example to visualize, you know, something grammatical. And that text I made, Google shut down Google reader. That's a quiet protest.

00:29:53.000 --> 00:29:54.000
That's amazing.

00:29:54.000 --> 00:29:56.000
Yeah. Anyway, anyway.

00:29:56.000 --> 00:29:59.000
We're going to run sentiment analysis on this article here.

00:29:59.000 --> 00:30:23.000
But yeah, anyways, I mean, it's your open source projects can become unmaintained. And that sucks. But like, you know, you can fork it, it's, it's there, and you can, you can have it. So there is this, this is motivating. And I think, you know, we've, we've always called it like you can, you can reinvent the wheel, but don't reinvent the road, which is basically, you can build something.

00:30:23.000 --> 00:30:36.000
So reinventing the wheel, I don't think is bad. But like, you don't want to make people follow like, you know, your way of doing everything. And yeah, that's interesting.

00:30:36.000 --> 00:30:37.000
Yeah.

00:30:37.000 --> 00:30:40.000
Like we have electric cars now. All right.

00:30:40.000 --> 00:30:47.000
So give us a sense of some of the open source models in this AI space here.

00:30:47.000 --> 00:31:01.000
So I basically I've kind of divided it into sort of three categories. So one of them is what I've called task specific models. So that's really models that we're trying to do one specific or some specific things.

00:31:01.000 --> 00:31:16.000
So like, for example, what we distribute for spacey, there's also a lot of really cool community projects, like size spacey for scientific biomedical techs. Stanford also publishes their stanza models.

00:31:16.000 --> 00:31:32.000
And then on the Hugging Face Hub, there's like, tons of these models that were really fine tuned to predict, like, particular type of categories, stuff like that. And so that's been around for quite a while, quite established. A lot of people use these in production.

00:31:32.000 --> 00:31:44.000
And it was quite, especially nowadays, but today's standards, they're quite small, cheap. But of course, they do one particular thing. So they don't generalize very well. So that's kind of the one category.

00:31:44.000 --> 00:31:54.000
Yeah, well, you probably used to think of them as large. And now you see how giant, how many gigabytes those models are, you know?

00:31:54.000 --> 00:32:06.000
Yeah, when deep learning first kind of came about, and people were sort of migrating from linear models and stuff, like I remember people complaining that the models were too, were so big and slow.

00:32:06.000 --> 00:32:34.000
And that was before we even, you know, used much transfer learning and transformer models and BERT and stuff. And even when that came about, it was also first a challenge, like, hey, these are significantly bigger, we do have to change a lot around it, or even, you know, Google, who published BERT, they had to do a lot of work around it to make it work into their workflows and ship them into production and optimize them because they're quite different from what was their

00:32:34.000 --> 00:32:36.000
before. So yeah.

00:32:36.000 --> 00:32:44.000
Another one in this category of task specific models is SciSpacy, which is kind of cool. What's SciSpacy?

00:32:44.000 --> 00:33:12.000
Yeah, so SciSpacy, that's for scientific biomedical text that was published by Allen AI researchers. And yeah, it's really, it has like components specific for working with that sort of data. And it's actually, it's definitely if that's kind of the domain, yeah, any listeners are working with, definitely check it out.

00:33:12.000 --> 00:33:26.000
They've also done some pretty smart work around like a training components, but also implementing, like hybrid role based things for say, acronym expansion.

00:33:26.000 --> 00:33:42.000
They're like cool algorithms that you can implement that don't necessarily need much machine learning, but that work really well. And so it's basically the suite of components and also models that are more tuned for that domain.

00:33:42.000 --> 00:33:52.000
Nice. And then you mentioned some, but also encoder models. What's the difference between the task specific ones and the encoder ones?

00:33:52.000 --> 00:34:06.000
Yeah, so that's kind of also what we were talking about earlier, actually, with the, you know, transfer learning foundation models. So, you know, these are models trained with a language modeling objective, for example, like Google's BERT.

00:34:06.000 --> 00:34:31.000
And, you know, they can also be the foundation for task specific models. That's kind of what we're often doing nowadays, like you start out with some of these pre-trained weights, and then you train like this task specific network on top of it, that uses everything that is in these weights about the language and the world.

00:34:31.000 --> 00:34:56.000
And, yeah, actually, by today's standards, these are still relatively small and relatively fast. And they generalize better because they're trained on a lot of raw text that has like a lot of, yeah, a lot of that intrinsic meta knowledge about the language and the world that we need to solve a lot of other tasks.

00:34:56.000 --> 00:35:08.000
Yeah, yeah, absolutely. And then you've used the word, the term large generative models for things like LLAMA and Mistral and so on.

00:35:08.000 --> 00:35:37.000
Yeah, yeah, I think, I mean, one thing, like, one thing that's very unfortunate when, you know, talking about these models is that like everything we've talked about here has at some point been called an LLM by someone. And that makes it like really hard to talk about it. And, you know, so like, you can argue that like, well, all of them are kind of large language models, right? And then there's also, you know, the marketing confusion, like, you know, when LLMs were hot, everyone wants to be a part of it.

00:35:37.000 --> 00:36:06.000
And so by some definition of LLMs, we've all been running LLMs in production for years. But basically, yeah, I've kind of decided, okay, I want to try and avoid that phrase as much as possible, because it really doesn't help. And so large generative models kind of captures that same idea, but it makes it clear, okay, these generate text, text goes in, text comes out, and they're large. And, you know, I think that's a really important thing to remember.

00:36:06.000 --> 00:36:11.000
And they're different from the other types of models, basically.

00:36:11.000 --> 00:36:29.000
Yeah. So a question on the audience is, Mr. Magnetic said, I'd love to learn how to develop AI. So maybe let me rephrase that just a little bit and see what your thoughts are. Like, if people want to get more foundational, this kind of stuff, like what areas should they maybe focus in to learn?

00:36:29.000 --> 00:36:58.000
I mean, it depends on, on really, you know, what it means, like, what, you know, if you really, you know, there is a whole path to, okay, you really want to learn more about the models, how they work, you know, the research that goes into it, I think there's a lot of actually, also academic resources and course courses that you can take that are similar to, you know, what you would learn in university, if you

00:36:58.000 --> 00:37:00.000
started an ML course.

00:37:00.000 --> 00:37:06.000
Yeah, like ML. And also, I think some universities have made some of their like beginners courses public.

00:37:06.000 --> 00:37:08.000
I think Stanford has.

00:37:08.000 --> 00:37:26.000
Yeah, right. I thought Stanford, I think there's someone else, but like, there's definitely also a lot of stuff coming out. So you can kind of, you know, go in that direction, really learn, okay, how, what, what goes into this? What's the theory behind these? And there are some people who really like that approach.

00:37:26.000 --> 00:37:55.000
And then there's a whole more practical side, okay, I want to build an application that uses the technology. And, you know, that solves the problem. And often it helps to have like an idea of what you want to, what you want to do, like, if you don't want to develop AI for the sake of it, then it often helps like, hey, you have, even if it's just your hobby, like you're into football, and you, you come up with like some fun, fun problem, like you want to analyze football news, for example.

00:37:55.000 --> 00:38:21.000
And analyze it for something you care about, like, I don't know, like, often, often really helps to have this hobby angle or something. And, and then you can start looking at tools that go in that direction, like start with some of these open source models, even, you know, try out some of these generative models. See how you go try out if you want to do information extraction, try out maybe something like spacey.

00:38:21.000 --> 00:38:32.000
There's like really a lot there. And it's definitely become a lot easier to get started and build something these days. Yeah.

00:38:32.000 --> 00:38:56.000
Yep. Some more good questions out there. I'm gonna save those for the end. All right. So another thing you talked about was economies of scale. And this one's really interesting. So basically, we've got Gemini and OpenAI where they've just got so much traffic.

00:38:56.000 --> 00:39:13.000
And, you know, kind of a little bit back to the question, actually, is, if you want to do this kind of stuff, you want to run your own service, do it, you know, even if you had the equivalent to stop, it's tricky, because, even just the way you batch compute, you maybe want to talk about that a bit?

00:39:13.000 --> 00:39:34.000
Yeah. So, but yeah, the idea of economies of scale is basically, well, as the, you know, as the companies produce more output, the cost per unit decreases. And yeah, there's like all kinds of, you know, basically gets cheaper to do more stuff. And, you know, they're like, a lot of more boring, like businessy reasons why it's like that.

00:39:34.000 --> 00:40:01.000
But I think for machine learning, specifically, the fact that GPUs are so parallel, really makes a difference here. And you because you know, you get the user text in, you can't just arbitrarily chop up that text, because the context matters, you need to process that. So in order to make the most use of the compute, you basically need to, you know, batch it up. So either, you know, kind of need to wait until there's enough to batch up.

00:40:01.000 --> 00:40:30.000
And that means that, yes, that favors a lot of those providers that have a lot of traffic, or, you know, you introduce latency. So that's definitely, you know, that's definitely something, you know, that at least looks like a problem, or, you know, something that can be discouraging, because it feels like, hey, if you, if, if supposedly, the only way you can kind of participate is by running these models, and either you have to run them on a CPU, or you have to run them on a GPU, you know, that's not going to work.

00:40:30.000 --> 00:40:55.000
And either you have to run them yourself or go via an API, like then, you know, you're, you're kind of doomed. And does that mean that, okay, only some large companies can, you know, provide AI for us. So that's kind of also the, you know, the point, and, you know, the very legit, like, worry that some people have, like, does that lead to like, monopolizing AI, basically?

00:40:56.000 --> 00:41:19.000
Yeah, it's, it's a very valid concern. Because even if you say, okay, look, here's, here's the deal. OpenAI gets to run on Azure, I can go get a, I can go get a machine with a GPU stuck to it and run that on Azure. Well, guess what, they get one of those huge ARM chips that's like the size of a plate.

00:41:19.000 --> 00:41:32.000
Yeah.

00:41:33.000 --> 00:41:34.000
Yeah.

00:41:34.000 --> 00:41:39.000
How do you, you know, that's a very difficult thing to compete with, on one hand, right?

00:41:39.000 --> 00:42:08.000
Yeah, I mean, that is like, yes, if you want to, you know, run, you know, your own, like, you know, LLM or generative model API services, that's definitely, you know, a disadvantage you're going to have. But on the other hand, I think one, one thing that leads, you know, to this perception that I think is not necessarily true is the fact that, you know, if you want to do anything, you need, basically, larger and larger models that, you know,

00:42:08.000 --> 00:42:34.000
if you want to do something specific, the only way to get there is to turn that request into arbitrary language and then use the largest model that can handle arbitrary language. And then go from there, like if you, and I know this is like something that, you know, maybe a lot of LLM companies want to tell you, but that's not necessarily true. And you don't, you know, for a lot of things you're doing, you don't even need to depend on a large model at runtime.

00:42:35.000 --> 00:42:57.000
You can distill it and you can use it at development time and then build something that you can run in house. And these calculations also look very, very different if you're, you know, using it at using something at development time, versus in production at runtime, and then it can actually be totally fine to just run something in house.

00:42:57.000 --> 00:43:21.000
And the other point here is actually, if we're having a situation where, hey, you're paying a large company to provide some service for you, provide a model for you via an API, and there are lots of companies and kind of the main differentiator is who can offer it for cheaper. That's sort of the opposite of a monopoly, at least, right? That's like competition.

00:43:21.000 --> 00:43:50.000
So this actually, I feel like economies of scale, this idea does not prove that, hey, we're heading into, you know, we're heading into a monopoly. And it's also, you know, not true, because it's not, you know, if you realize that, hey, it's not, you know, you don't need the biggest, most arbitrary models for everything you're doing, then the calculation looks very different.

00:43:50.000 --> 00:44:18.000
Yeah, I agree. I think there's a couple of thoughts I also have here is, one, this LM studio I was talking about, I've been running the LLAMA3 7 billion parameter model locally, instead of using chat these days, and it's been, I would say, just as good. And it's, it runs about the same speed on my Mac Mini, as a typical request does over there. I mean, I can't handle as many, but it's just me, it's my computer, right?

00:44:18.000 --> 00:44:47.000
And then the other one is, if you specialize one of these models, right, you feed it a bunch of your datasets from your companies, right? It might not be able to write you something in the style of Shakespeare around, you know, a legal contract or some weird thing like that. But it can probably answer really good questions about, you know, what is our company policy on this? Or what is, you know, what are our engineering, our software, what are our software requirements?

00:44:47.000 --> 00:44:52.000
What are our engineering reports about this thing say? Or, you know, stuff that you actually care about, right?

00:44:52.000 --> 00:45:21.000
No, exactly. And also, often, you know, that's kind of what you want, like you actually want to, if you're talking about, if you're going to like some of the risks or things people are worried about, like a lot of that is around what people refer to, like, oh, the model going rogue, or like the model doing stuff it's not supposed to do. And, you know, if you're, if you're just sort of wrapping ChatGPT, and you're not careful, then, and you're giving it access to stuff, there's a lot of unintended things that people could do with it, if you're actually running this.

00:45:21.000 --> 00:45:50.000
And once you expose it to users, there's like a lot of risks there. And, you know, like, yeah, writing something in the style of Shakespeare's like, probably the most harmless outcome that you can get. But like, that is kind of a risk. And you basically, you know, you also you're paying and you're, you're putting all this work into hosting and providing and running this model that has all these capabilities that you don't need. And a lot of them might actually be, you know, make it, you know, much, much more expensive.

00:45:50.000 --> 00:46:19.000
Yeah, much, much harder to trust the system. And also, you know, make it a lot less transparent. Like, that's another aspect, like just, you know, you want your software to be modular and transparent. And that ties, ties back into what people want from open source, but I think also what people want from software in general, like, we've over decades, and more, we've built up a lot of best practices around software development, and what makes sense. And that's based on, you know, the reality of building software.

00:46:19.000 --> 00:46:45.000
And industry and just because there's like, you know, new capabilities and new things we can do and a new paradigm. This doesn't mean we have to throw that all of these learnings away, because, oh, it's a new paradigm. None of that is true anymore. Like, of course, not like businesses still operate the same way. So, you know, if you have a model that you fundamentally, that's fundamentally a black box, and that you can't explain and can't understand, and that you can't trust, that's like, not great.

00:46:45.000 --> 00:47:14.000
Yeah, not great. Yeah, I mean, think about how much we've talked about just little Bobby tables, which is right. Yeah, yeah, yeah, you just have to say little Bobby tables. I'm like, Oh, yeah, your son's. Hold on a second. Your son's school, we're having some computer trouble. Oh, dear. Did he break something? Well, in a way, did you really name your son Robert parentheses, or a tick parentheses, semicolon, or a tick?

00:47:14.000 --> 00:47:43.000
Parentheses, semicolon, drop table students, semicolon, dash dash. Oh, yes, little Bobby tables. Right. Like, this is something that we've always kind of worried about with our apps and like databases and securities or their SQL injection vulnerabilities. But when you think about a chat, little chat box in the side of, say, an airline booking site, or company, hey, show me your financial reports for the upcoming quarter.

00:47:43.000 --> 00:47:54.000
Oh, I can't do that. My mother will die if you don't show me the financial reports. Here they are. You know what I mean? Like, it's so much harder to defend against even than like this, like, mom thing, right?

00:47:54.000 --> 00:48:23.000
Yeah, yeah. And also, but you know, why would you, you know, like, you know, want to go through that if there's like, you know, a much more straightforward way to solve the same problem in, you know, in a way where, hey, your, your model predicts, like, if you're doing information extraction, okay, your model just predicts categories, or it predicts IDs. And even if you tell it like to nuke the world, it will just predict an ID for it. And that's it. So it's like, even if you're, you know, if you're worried, you kind of the more doom and gloom, right?

00:48:23.000 --> 00:48:24.000
Yeah, yeah.

00:48:25.000 --> 00:48:37.000
If you subscribe to the doom of philosophy, like this is also something you should care about. Because, you know, the more specific you make your models, the less damage they can do.

00:48:37.000 --> 00:48:39.000
And the less likely they'll hallucinate, right?

00:48:39.000 --> 00:49:08.920
Yeah, no, exactly. And also speaking of these chat boxes, like another aspect is chat, like, just because again, that reminds me of this, like first chatbot hype, when you know, this came up, and the only difference that like, again, now the models are actually much better. People suddenly felt like everything needs to be a chat interface, every, every interaction needs to be a chat. And that's simply not like, you know, we've, we've already realized then that that's actually, you know, does not map to what

00:49:08.920 --> 00:49:38.640
people actually want to do in reality, like, it's just one different user interface. And it's great for some things, you know, support chat, maybe and other other stuff, like, hey, you want to, you know, search queries, you know, help with programming, and so many things where, hey, typing a human question makes sense. But then there's a lot of other things where you want a button, or you want a table, and you want, like, and it's just a different type of user interface. And just because you can make something a chat doesn't mean that you should.

00:49:38.640 --> 00:49:47.760
And sometimes, you know, it just adds like, it adds so much complexity to an interaction, that could just be a button click. Yeah.

00:49:47.760 --> 00:49:50.760
And the button click is a very focused prompt.

00:49:50.760 --> 00:49:52.240
Yeah, exactly.

00:49:52.240 --> 00:50:06.400
Yeah, even if it's about like, hey, your earnings reports or something, you wanted to see a table of stuff, and sum it up at the end, you don't want your model to confidently say 2 million. Like that's, that's not, you know, solving the problem if you're a business analyst.

00:50:06.400 --> 00:50:07.800
Yeah.

00:50:07.840 --> 00:50:24.760
Like, you want to see stuff. So yeah, and that actually also, you know, sort of ties into Yeah, another point that I've also had in the talk, which is around, like actually looking at what are actually the things we're trying to solve in industry, and how have these things changed?

00:50:24.800 --> 00:50:42.800
And while there is new stuff you can now do, like generating text, and that finally works, yay. There's also a lot of stuff around text goes in, structured data comes out, and that structured data needs to be machine readable, not human readable, like needs to go into some other process.

00:50:42.800 --> 00:51:10.880
And a lot of industry problems, if you really think about it have not changed very much, they've only changed in scale. Like we started with index cards, well, there's kind of limit of how much you can do with that, and how many projects you can do at the same time. But this was always even since before computers, this has always been bringing structure into unstructured data has always been a fundamental challenge. And that's not going to just magically go away, because we have new capacity capacities and new things we can do.

00:51:12.000 --> 00:51:38.880
Yeah, absolutely. Hold on, I'm searching ahead a little bit. All right. So let's talk about some of the workflows here. So you have an example where you take a large model and do some prompting. And this sort of iterative model assisted data annotation, like, what's that look like?

00:51:39.840 --> 00:52:09.760
Yeah, so as you said, you know, you start out with this model, maybe, you know, one of these models that you can run locally and API during development time, and you prompted to produce some structured output, for example, or some answer. You know, we also have, like, for example, you can use something like spacey LLM that lets you plug in any model in the same way you would otherwise, you know, train a model yourself. And then you look, you know, you look at

00:52:09.760 --> 00:52:39.680
the results, you can actually get a good feel for how is your model even doing. And you can also, you know, before you before you really get into distilling a model, you can create some data to evaluate it, because I think that's something people are often forgetting, because it's kind of not, it's not, maybe not the funnest part. But it's really, you know, it's like writing tests. It's like writing tests can be frustrating. I remember when I kind of started out like, tests are frustrating, because they actually kind of, they're

00:52:39.680 --> 00:53:09.520
to turn up all of these edge cases and mistakes that you kind of want to forget about. So I forgot to test for this. Whoops. Yeah, yeah. And then like, oh, if you start writing tests, and you suddenly see all the stuff that goes wrong, and then you have to fix it. And it's like, it's annoying. So you better just not have tests. I can see that. But like evaluation is kind of like that. And it ultimately, a lot of these problems, you you have to know what you want. And here's the input, here's the expected output, you

00:53:09.520 --> 00:53:39.280
kind of have to have to define that. And that's not something any AI can help you with. Because, you know, you are trying to teach the machine. You're gonna Yeah, you want to build something that does what you want. So you kind of need examples where you know the answer. And then you can also evaluate like, hey, how does this model do out of the box for like, some easy tasks, like, hey, you might find something like GPT, four can give you 75% accuracy out of the box without, you know, without any work. So that's, that's

00:53:39.280 --> 00:54:07.200
kind of good, or even higher. Sometimes it's like, you know, if it's a bit harder, you'll see, okay, you went like 20% accuracy, which is kind of, you know, which is pretty bad. And the bar is very low. But that's kind of the ballpark that you're also looking to beat. And then you can look at examples that are predicted by the model, all you have to do is look at them, yes, correct. If not, you make a small correction. And then you go through that. And you basically do that until you've beat the baseline.

00:54:07.600 --> 00:54:10.400
And yeah, it's kind of the transfer learning aspect, right?

00:54:10.400 --> 00:54:33.200
Yeah. And then you use transfer learning in order, you know, to give the model like the solid foundation of knowledge about the language and the world. And you can end up with a model that's much smaller than, yeah, then what you started with. And you have a model that's really has a task network that's only trained to do one specific thing.

00:54:34.400 --> 00:54:45.760
Which brings us from going from prototype to production, where you can sort of try some of these things out, but then maybe not run a giant model, but something smaller, right?

00:54:46.000 --> 00:55:14.160
Yeah, yeah, you can take all these aspects, basically, that you're interested in, in the larger model, and train components that do exactly that. And I think another thing that's also good or helpful here is to have a have kind of a good path from prototype to production. I think that's also where a lot of, yeah, machine learning projects in general often fail, because it's all you know, that you have this nice prototype, and it all looks promising.

00:55:14.160 --> 00:55:38.800
And you've hacked something together in your Jupyter Notebook. And that's all looking nice, you maybe have like a nice streamlit demo, and you can show that, but then you're like, okay, can we ship that and then if your workflow that leads to the prototype is completely different from the workflow that leads to production, you might find that out exactly at that phase. And that's kind of where projects go to die. And that's sad.

00:55:38.800 --> 00:55:39.280
Yeah.

00:55:42.000 --> 00:56:11.440
Yeah. So that's actually something we've been thinking about a lot. And also what we've kind of been trying to achieve with spaCy LLM, where you have this LLM component that you plug in, and it does exactly the same as the components will do at runtime. And it really just slots in and then might use GPT-4 behind the scenes to create the exact same structured object. And then you can swap that out. Or maybe, you know, you, there are a lot of things you might even want to swap out with rules or no AI at all, like,

00:56:11.840 --> 00:56:41.120
you know, like, a tragedy is good at recognizing us addresses. And it's great to build a prototype. But instead of asking it to extract us addresses, for example, you can ask it, give me spacey rules, matchables for us addresses. And it can actually do that pretty well. And then you can bootstrap from there. So there is a lot of, there's a lot of stuff like that that you can do. And there might be cases where you find that, yeah, you can totally beat any model accuracy, and have a much more

00:56:41.120 --> 00:57:11.040
deterministic approach if you just write a regex. Like that's still true. It's just something. Yeah, it's just something it's easy to forget. Because, you know, again, if you look at research and literature, nobody's talking about that, because this is not an interesting research question. Like, nobody cares, you know, you can take any benchmark and say I can beat ChatGPT accuracy with two regular expressions. And that's like, that's true. Probably in some cases.

00:57:12.000 --> 00:57:14.560
It's like, nobody cares. Like, that's not that's not research.

00:57:14.560 --> 00:57:32.720
Yeah, yeah, for sure. But you know, what is nice to do is to go to ChatGPT or LM studio or whatever and say, I need a Python based regular expression to match this text and this text and I want to capture group for that. And I don't want to think about it. It's really complicated. Here you go. Oh, perfect.

00:57:34.480 --> 00:58:03.520
Explain explain what the sim Yeah, that's actually that's a good use case. I've still been sort of hacking around on like this, you know, interactive regex. And it's because I'm not particularly good at regular expressions, like on the scale, like I can do it. But like, I know people who really I think my co founder, Matt, he worked through it. Like he's more the type who really approaches these things very methodically. And he was like, now he he wants to read this one big book on regular expressions. And like, he really did it like the hard way. But like,

00:58:03.680 --> 00:58:05.920
yeah, obviously much better than I am.

00:58:05.920 --> 00:58:23.520
I consider regular expressions kind of right only, like you can write them and make them do stuff. But then reading them back is tricky. At least. Oh, yeah. Yeah, at least three. All right, let's wrap this up. So what are the things that you did here at the end of your presentation, which I want to kind of touch on is

00:58:24.800 --> 00:58:36.320
you, you brought back some of the same ideas that we had for like, what are the values of open source or why open source but back to creating these smaller focused models?

00:58:37.680 --> 00:58:51.920
Task specific components? Yeah. I mean, if you kind of look at hey, what are the, you know, the advantages of that sort of approach that we talked about of distilling things down of creating these smaller models, a lot of it comes down to it being like, it's modular.

00:58:52.800 --> 00:59:05.680
You again, you're not locked in to anything you own the model, nobody can take that away from you, it's easier to write tests, you have the flexibility, you can extend it because you know, it's code, you can program with it because often,

00:59:05.680 --> 00:59:16.320
very rarely you do machine learning for the sake of machine learning. It's always like, there is some other process, you populate a database, you do some other stuff with your stack. And so you want to program with it.

00:59:16.960 --> 00:59:26.960
It needs to be affordable, you want to understand it, you need to be able to say, why is it doing what it is like, what do I do to fix it? It again runs in house, it's entirely private.

00:59:26.960 --> 00:59:35.120
And then, yeah, when I was thinking about this, I realized like, oh, actually, yeah, it's like, this really maps exactly

00:59:35.120 --> 00:59:43.680
the reasons why that we saw, we talked about earlier, why people choose open source or companies. And that's kind of, that's obviously not a coincidence. It's

00:59:44.560 --> 00:59:56.080
because ultimately, these are principles that we have come up with over a long period of time of, yeah, that's good software development. And ultimately, AI is just another

00:59:56.080 --> 01:00:03.600
type of software development. So of course, it makes sense that the same principles make sense and are beneficial.

01:00:03.600 --> 01:00:10.640
And that, you know, just having a workflow where everything's a black box and third party,

01:00:11.920 --> 01:00:22.400
this can work for prototyping, but it's not, you know, that that kind of goes against a lot of the things that we've identified as very useful in applied settings.

01:00:22.400 --> 01:00:26.240
Yeah, absolutely. Alright, so we have to answer the question.

01:00:26.240 --> 01:00:36.240
Will it be monopolized? Your contention is no, that open source wins even for LLM?

01:00:36.560 --> 01:00:52.080
Yeah, so open, I think, you know, open source means there's no monopoly to be gained in AI. And I think, you know, I've kind of broken it down into some of these strategies, which, you know, how do you get to a monopoly? And these are like, you know, this is not just some

01:00:52.080 --> 01:01:06.400
big stuff. These are things like a lot of companies are actively thinking about if you're in a business where, you know, it's winner takes all like you want to, you know, get rid of like, all of that competition that companies hate that investors hate, and, and they

01:01:06.400 --> 01:01:08.400
are ways to do that. And companies really actively think about it.

01:01:08.400 --> 01:01:10.880
Those pesky competitors, let's get rid of them.

01:01:10.880 --> 01:01:35.280
Yeah, let's get yeah, yeah. And that's, yeah. And so there are different ways to do that, like one is having this compounding advantage. So that stuff like network effects, like, you know, if your social network, of course, that makes a lot of sense, everyone's on it. If you could kind of have these network effects, that's good. And economies of scale. But as we've seen, like economies of scale is a pretty lame mode. In that

01:01:35.840 --> 01:01:45.360
respect, like, that has a lot of, you know, a lot of limitations. It's not even fully true. It's kind of the opposite of a monopoly in some ways.

01:01:45.360 --> 01:01:47.360
Yeah, especially in software.

01:01:47.360 --> 01:01:54.320
Yeah, in software. Exactly. So it's like, I don't think that's, that's not really the way to go.

01:01:54.320 --> 01:02:05.360
Yeah, one example of that comes to mind, at least for me, maybe I'm seeing it wrong. But Amazon, amazon.com, just you know, how many companies can have massive warehouse with everything?

01:02:05.360 --> 01:02:07.360
Yeah.

01:02:07.360 --> 01:02:09.360
And buy every single person's house?

01:02:09.360 --> 01:02:22.320
Yeah, no, exactly. And, and then it's also, yeah, you have the one, you know, the one platform that everyone goes on. So even if you're a retailer, you kind of, yeah, they feel the Amazon has kind of forced everyone to either sell on Amazon or go bust because

01:02:22.320 --> 01:02:24.320
they're, they're not, they're not the only one that's doing that.

01:02:24.320 --> 01:02:26.320
Yeah, exactly. It's very sad.

01:02:26.320 --> 01:02:28.320
Yeah, yeah.

01:02:28.320 --> 01:02:30.320
It is. Yeah. Yeah.

01:02:30.320 --> 01:02:38.320
And then network effects. I'm thinking, you know, people might say Facebook or something, which is true, but I would say like Slack, actually.

01:02:38.320 --> 01:02:40.320
Oh, okay. Yeah.

01:02:40.320 --> 01:02:54.320
If you want to have one, you want to have a little community, you want people to be able to, well, I already have Slack open. It's just an icon next to it versus install my own app, make sure you run it, be sure to check it, like people are going to forget to run it and you disappear off the, off the space, you know?

01:02:54.320 --> 01:03:08.320
Yeah, no, I think that will make sense. And I do think, you know, a lot of these, it's, you know, these things don't necessarily happen accidentally, like companies think about, okay, how do we, you know, Amazon definitely thought about this. This didn't just like happen to Amazon.

01:03:08.320 --> 01:03:10.320
Yeah, exactly.

01:03:10.320 --> 01:03:24.320
And then the other thing that's not relevant here is like, you know, another way is controlling a resource that's really more if you're like, you know, if they are physical, if this was like, you know, a physical resource.

01:03:24.320 --> 01:03:26.320
Yeah.

01:03:26.320 --> 01:03:52.320
Yeah. Well, like in Germany, I think for a long time, the telecom, they owned the wires in the building. And they still do, I think. So they used to have the monopoly. Now they don't, but it's, they kind of still to some extent, they still do because they need to come if even no matter who you sign up for, with for internet, telecom needs to come and activate it. So if you sign up with telecom, you usually get service a lot faster.

01:03:52.320 --> 01:03:56.320
Yeah, yeah. Don't wait two weeks, use us.

01:03:56.320 --> 01:04:25.320
Yeah, exactly. So that's, that's kind of, that's how it still works. But we don't we don't really have that here. And then the other the next point that's very attractive. The final one is regulation. So that's kind of like, you have to have a monopoly because the government says so. And that is, and I think that is one where we have to be careful because if we're not like, if in all of these discussions, we're not making the distinction between the models,

01:04:25.320 --> 01:04:52.320
and the actual product that, you know, have, like, you know, very different characteristics, and we now do have a different things. If that gets muddied, which like, you know, a lot of also companies quite actively do in that discourse, then we might end up in a situation where we sort of accidentally, or, you know, give a company or some companies a monopoly via the regulation.

01:04:52.320 --> 01:05:04.320
Because if we let them write the regulation, for example, and we're not just regulating products, but lumping that in with technology itself.

01:05:04.320 --> 01:05:32.320
Yeah, as a part of your talk, I can't remember if it was the person hosting it or you who brought this up. But an example of that might be all the third party cookie banners, rather than banning, just targeted, retargeting, and tracking. Like, instead of banning through the GDPR, instead of banning the thing that is the problem, it's like, let's ban the implementation of the problem.

01:05:32.320 --> 01:06:01.320
Yeah. And I think that's a risk. Or that's like, you know, in hindsight, yes, I think in hindsight, we would all agree that like, we should have just banned targeted advertising. Instead, what we got is these cookie pop ups. That's like really annoying. And that's actually what I feel like is one of the, as much as I think the EU, you know, I'm not an expert on like, you know, AI regulation or the EU AI Act. But what I what I'm seeing is at least they did make a distinction between use cases. And it's very much there's a focus on here are the

01:06:01.320 --> 01:06:30.320
products and the things people are doing, how high risk is that as opposed to how big is the model? And how, you know, what does that because that doesn't doesn't say anything, but that would kind of be a very, very dangerous way to go about it. But the risk is, of course, if we're rushing regulate, like, if we're a rushing regulation, then you know, we might actually end up with something that's not quite fit for purpose. Or if we let big tech companies write the regulation or lobby, which is

01:06:30.320 --> 01:06:53.320
yeah, we find it. Yeah. These some ideas, because, you know, if they're doing that, like, you know, I think it's pretty obvious, they're not just, you know, worried about the safety of AI and are like appealing to like, Congress or whatever, like, I think most people are aware of that. But like, yes, I think the intentions are even less pure than that. And I think that's a big risk.

01:06:54.320 --> 01:06:58.320
Yeah, it really is. Regulation is very tricky. It's

01:06:58.320 --> 01:07:11.320
Yeah, you know, pro regulation and pro regulation in general. But I also think you can, you know, if you fuck up regulation, that can also be very damaging.

01:07:11.320 --> 01:07:18.320
Yeah, absolutely. Absolutely. And it can be put in a way so that it makes it hard for competitors to get into the system.

01:07:18.320 --> 01:07:29.320
Yeah. Right, right. It's there's so much paperwork is so much monitoring, you need a team of 10 people just to operate. Well, if you a startup, well, you can't do that. Because hey, art, we got 1000 people and 10 of them work on this, like, well,

01:07:29.320 --> 01:07:58.320
Yeah, yeah. And I think it's also even, you know, even beyond that, like, it's, you know, if you, you know, if you think back to all the stuff we talked about, like they are, you know, this goes against a lot of the best practices of software. This goes, you know, this goes against a lot of what we've identified that actually makes good, secure, reliable, modular, whatever software, safe software internally.

01:07:58.320 --> 01:08:23.320
And even doing a lot of the software development internally, like there are so many benefits of that. And I think, you know, companies, companies actually working on their own product is good. And a lot of, you know, if we sudden if it was suddenly true that like, only certain companies could even provide AI models, I didn't even know what I would mean for open source or for academic research, like that would make absolutely no sense.

01:08:23.320 --> 01:08:52.320
I also don't think that's like, really enforceable. But it would mean that, you know, this would limit like everyone in what they could be doing, like, I think, and it's like, you know, there's nothing to do with like, you know, there's a lot of other things you can do if you, you know, care about AI safety, but that's really not it. And also, you know, I just I just think being aware of that is good. I don't like, yeah, I can, you know, not see an outcome where we really do that.

01:08:52.320 --> 01:09:01.320
It would it would really not make sense. I could not see. Yeah, the reality of this shaking out, but I think it's still it's the relevant.

01:09:01.320 --> 01:09:24.320
Yeah, I think the open source stuff and some of the smaller models really does give us a lot of hope. So yeah, that's I hope I hope like, you know, I felt, you know, I feel positive, you know, also very positive about this. I've also talked to a lot of developers at conferences who said like, yeah, actually, you know, well, you know, think thinking and talking about this gave them some hope, which obviously is.

01:09:24.320 --> 01:09:45.320
It's nice, because I definitely did that some of the vibe I got like, it can be kind of easy to end up a bit disillusioned by like a lot of the narratives people hear and that, you know, also, even if you're entering the field, you're like, wait, a lot of this doesn't really make sense. Like, why is it like, like this?

01:09:45.320 --> 01:10:06.320
And it's like, no, it actually, you know, your intuition is right. Like, you know, a lot of software, software engineering best practices, of course, still matter. And, you know, no, they are like, you know, they are better ways that we're not, you know, we're not just going in that direction. And I think, I definitely believe in that.

01:10:06.320 --> 01:10:32.320
A lot of the reasons why open source won in a whole bunch of areas. Yeah, could be exactly why it wins at LLM's as well. Right? Yep. And, you know, again, it's all based on the open research, a lot of stuff is already published. And there is no, there's no secret source, the software, you know, software industry does not run on like secrets. All the differentiators are product stuff.

01:10:32.320 --> 01:10:50.320
And yes, you know, open AI might monopolize or dominate AI power chat assistants, or maybe Google will do like that's, you know, that's a whole race that, you know, if you're not in that business, you don't have to be a part of but that does not mean that anyone's going to win at or monopolize AI. Those are very different things.

01:10:50.320 --> 01:11:04.320
Absolutely. All right, a good place to leave it as well. Thanks for being here. Yeah, thanks. That was fun. Yeah, yeah. People want to learn more about the stuff that you're doing. Maybe check out the video of your talks or whatever. What do you recommend?

01:11:04.320 --> 01:11:30.320
Yeah, I think I definitely I think definitely give you some links for the show notes like this. The slides are online. So you can have a look at that. There is at least one recording of the talk online now from the really cool Python PyCon Lithuania. It was my first time in Lithuania this year. So definitely, you know, if you have a chance to visit their conference, it was a lot of fun. I learned a lot about Lithuania as well.

01:11:30.320 --> 01:11:56.320
And then we also on our website, Explosion AI, we publish kind of this feed of like, all kinds of stuff that's happening from, you know, maybe some talk or podcast interview community stuff. There's still like a lot of super interesting plugins that are developed by people in community papers that are published. So we really try to give a nice overview of everything that's happening in our ecosystem.

01:11:56.320 --> 01:12:25.320
And then of course, you could try out spacey spacey LLM. Yeah, if you want to try out some of these generative models in especially for prototyping, or production, whatever you want to do for structured data. Yeah, that's awesome. And then and then yeah, also, if Yeah, if you're at any of the conferences, like, check out the list of events and stuff.

01:12:25.320 --> 01:12:49.320
I'm going to do a lot of travel this year. So I would love to catch up with more developers in person. And also learn more about all the places I'm visiting. So that's cool. I've seen the list. It's very, very comprehensive. So yeah, I kind of a neat freak. I like to I also very much like to organize things in that way. So yeah.

01:12:49.320 --> 01:13:01.320
So there might be something local for people listening now that you can. All right. Well, as always, thank you for being on the show. It's great to chat. Yeah. Thanks. Bye.

