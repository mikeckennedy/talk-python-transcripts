WEBVTT

00:00:00.000 --> 00:00:03.600
>> David, welcome to Talk Python To Me.


00:00:03.600 --> 00:00:04.800
Awesome to have you here.


00:00:04.800 --> 00:00:06.640
>> Yeah. Thank you, Michael.


00:00:06.640 --> 00:00:11.880
>> Yeah. I'm really excited to see what the AIs have to say today.


00:00:11.880 --> 00:00:15.760
>> The AIs. Yeah. Language models, sure.


00:00:15.760 --> 00:00:17.680
>> Yes, exactly.


00:00:17.680 --> 00:00:21.400
Now, you've built a really cool extension for


00:00:21.400 --> 00:00:25.560
Jupyter that plugs in large language models for people.


00:00:25.560 --> 00:00:27.760
It's looking super interesting,


00:00:27.760 --> 00:00:30.080
so I'm excited to talk to you about it.


00:00:30.080 --> 00:00:33.280
>> Yeah. I'm excited to talk about Jupyter AI too.


00:00:33.280 --> 00:00:36.560
I've actually presented this twice at one set,


00:00:36.560 --> 00:00:38.040
actually three times.


00:00:38.040 --> 00:00:43.000
I did a short demo at this tech meetup thing in Seattle.


00:00:43.000 --> 00:00:46.240
That was actually the first time Jupyter AI was shown to the public.


00:00:46.240 --> 00:00:53.040
Then I presented at PyData Seattle at Microsoft's Redmond campus.


00:00:53.040 --> 00:00:57.320
Then I got to present again at JupyterCon in Paris.


00:00:57.320 --> 00:01:00.280
This May, it was a really wonderful experience.


00:01:00.280 --> 00:01:02.920
>> Wow. You're making the rounds.


00:01:02.920 --> 00:01:05.320
>> Yeah. I love to talk about Jupyter AI.


00:01:05.320 --> 00:01:08.560
It happens to get me some plane tickets.


00:01:08.560 --> 00:01:12.680
>> Honestly, that's like half the bonus of conferences,


00:01:12.680 --> 00:01:14.760
is awesome places you get to go.


00:01:14.760 --> 00:01:18.160
The other half probably is the people you meet. It's really cool.


00:01:18.160 --> 00:01:20.920
>> For me, it's almost all the people.


00:01:20.920 --> 00:01:24.680
The people are just so great, especially JupyterCon.


00:01:24.680 --> 00:01:30.520
>> Yeah. The JupyterCon videos are now out for JupyterCon 2023.


00:01:30.520 --> 00:01:33.520
There's a ton of good-looking talks there.


00:01:33.520 --> 00:01:36.880
>> Yeah. Lots of really smart people.


00:01:36.880 --> 00:01:40.360
I was chatting to a few folks there.


00:01:40.360 --> 00:01:44.280
That's the only place where you're going to find people who work at


00:01:44.280 --> 00:01:50.120
these hedge funds and trading firms just lounging so idly and casually.


00:01:50.120 --> 00:01:53.760
>> The markets open and they're chilling. It's all fine.


00:01:53.760 --> 00:01:57.440
>> Yeah. There's a lot of smart people there.


00:01:57.440 --> 00:02:00.600
>> Yeah. I think Jupyter,


00:02:00.600 --> 00:02:04.400
more than a lot of programming technologies,


00:02:04.400 --> 00:02:09.760
bring people from all sorts of different places together, different backgrounds.


00:02:09.760 --> 00:02:15.360
>> Yeah. There's a lot of reasons behind that.


00:02:15.360 --> 00:02:16.920
But long story short,


00:02:16.920 --> 00:02:20.640
Jupyter is pretty awesome and that's why I wanted to contribute to it.


00:02:20.640 --> 00:02:23.520
>> Yeah. Awesome. Well, let's start


00:02:23.520 --> 00:02:26.440
this whole conversation with a bit of background about yourself.


00:02:26.440 --> 00:02:29.720
For people who didn't see your talk and don't know you yet,


00:02:29.720 --> 00:02:31.160
tell us a bit about you.


00:02:31.160 --> 00:02:35.800
>> I mean, I didn't really give much of an intro there either, but sure.


00:02:35.800 --> 00:02:39.920
I've worked for AWS as a software engineer.


00:02:39.920 --> 00:02:42.240
I've been with AWS,


00:02:42.240 --> 00:02:47.280
specifically the AI/ML organization at AWS.


00:02:47.280 --> 00:02:50.600
I've been with them for almost two years now.


00:02:50.600 --> 00:02:56.040
Right now, my manager is actually Brian Granger,


00:02:56.040 --> 00:02:58.600
who's the co-founder of Project Jupyter.


00:02:58.600 --> 00:03:00.600
He also works for AWS.


00:03:00.600 --> 00:03:05.280
He's been offering some technical and product guidance


00:03:05.280 --> 00:03:07.800
for the things that we're building.


00:03:07.800 --> 00:03:11.160
He's a fantastic gentleman to work with.


00:03:11.160 --> 00:03:18.040
>> Yeah. That's really neat to have him available as a resource, as a colleague.


00:03:18.040 --> 00:03:19.640
>> Yeah. It's funny.


00:03:19.640 --> 00:03:23.680
I met him internally.


00:03:23.680 --> 00:03:25.960
When I first joined, I wasn't working for him.


00:03:25.960 --> 00:03:27.440
But at tech companies,


00:03:27.440 --> 00:03:30.000
you can do this internal transfer thing.


00:03:30.000 --> 00:03:33.840
Basically, the team I just joined,


00:03:33.840 --> 00:03:37.200
right after I joined, it started to dissolve a little


00:03:37.200 --> 00:03:40.040
because they just launched a product at reInvent,


00:03:40.040 --> 00:03:43.400
which happens in December.


00:03:43.400 --> 00:03:48.440
I joined in December and it's like, "Oh, hi."


00:03:48.560 --> 00:03:56.360
So then I saw Brian Granger's name somehow and I messaged him.


00:03:56.360 --> 00:03:59.880
I didn't even know that he was the co-founder of Project Jupyter.


00:03:59.880 --> 00:04:03.000
I just wanted to work for him because I used it before.


00:04:03.000 --> 00:04:05.720
Yeah. It's a pretty funny story.


00:04:05.720 --> 00:04:10.640
>> Yeah. Indeed. I imagine that project is really,


00:04:10.640 --> 00:04:13.400
this is Jupyter AI is a great example.


00:04:13.400 --> 00:04:15.960
But I just thinking of being, say,


00:04:15.960 --> 00:04:19.200
a founder of Jupyter or something like that,


00:04:19.200 --> 00:04:22.040
these things take on a life of their own.


00:04:22.040 --> 00:04:27.200
He's probably in awe of all the stuff happening and all the things going on.


00:04:27.200 --> 00:04:30.240
There's probably a lot of stuff in Jupyter he doesn't even know about, right?


00:04:30.240 --> 00:04:31.000
It's like that's happening.


00:04:31.000 --> 00:04:32.680
>> Yeah. It's huge.


00:04:32.680 --> 00:04:35.920
The leadership structure has changed to accommodate that.


00:04:35.920 --> 00:04:40.440
So Brian is no longer the benevolent dictator for life.


00:04:40.440 --> 00:04:43.160
Project Jupyter is now governed by a committee.


00:04:43.160 --> 00:04:47.760
It's decentralized and democratized to allow it to scale.


00:04:47.760 --> 00:04:50.680
>> Yeah, of course.


00:04:50.680 --> 00:05:01.160
So let's start by talking about a bit of the role of AI in data science.


00:05:01.160 --> 00:05:04.800
I don't know how you feel about it.


00:05:04.800 --> 00:05:07.600
Obviously, you must be somewhat of an advocate putting


00:05:07.600 --> 00:05:10.920
this much time and energy into bringing it to Jupyter.


00:05:10.920 --> 00:05:12.240
>> Wow.


00:05:12.240 --> 00:05:14.720
>> However, I personally,


00:05:14.720 --> 00:05:18.320
when I want to know something,


00:05:18.320 --> 00:05:22.240
I don't think there's a great specific search result for it.


00:05:22.240 --> 00:05:26.040
Straight to ChatGPT or friends,


00:05:26.040 --> 00:05:30.120
I think there's such a wealth of information there.


00:05:30.120 --> 00:05:34.680
From I need to take this paragraph and clean it up and make it sound better,


00:05:34.680 --> 00:05:36.400
to I have this program,


00:05:36.400 --> 00:05:38.480
I want to convert to another language,


00:05:38.480 --> 00:05:42.000
or I have this data in this website.


00:05:42.000 --> 00:05:44.560
How do I get it? You can ask


00:05:44.560 --> 00:05:48.480
so many open-ended questions and really get great answers.


00:05:48.480 --> 00:05:51.520
So it seems to me,


00:05:51.520 --> 00:05:53.400
especially coming from, like I mentioned before,


00:05:53.400 --> 00:05:58.960
those diverse backgrounds, people not necessarily being super deep in programming,


00:05:58.960 --> 00:06:00.240
maybe they're deep in finance,


00:06:00.240 --> 00:06:04.040
but they do programming, that having this AI capability to ask,


00:06:04.040 --> 00:06:07.400
like, "Hey, I know I can, but how?"


00:06:07.400 --> 00:06:10.320
What do you think for data science in particular?


00:06:10.320 --> 00:06:14.360
>> Yeah. This is an interesting topic,


00:06:14.360 --> 00:06:19.640
because I think the whole power of language models stems from their ubiquity and


00:06:19.640 --> 00:06:25.840
versatility and how they can be very generally applicable.


00:06:25.840 --> 00:06:28.600
The thing about language models is that they're


00:06:28.600 --> 00:06:32.440
basically statistical models that have been trained on a very,


00:06:32.440 --> 00:06:35.560
very, very large corpus of data.


00:06:35.560 --> 00:06:38.840
To the computer, the computer doesn't really


00:06:38.840 --> 00:06:42.680
understand English, it doesn't really understand natural language.


00:06:42.680 --> 00:06:48.920
It has, when it's trained,


00:06:48.920 --> 00:06:52.400
it has knowledge of the distribution of information and


00:06:52.400 --> 00:06:56.160
how information interacts with other information.


00:06:56.160 --> 00:06:58.240
So because of that,


00:06:58.240 --> 00:07:01.680
it has a very general applicability.


00:07:01.680 --> 00:07:05.480
I don't think that's utilities limited to data science.


00:07:05.480 --> 00:07:09.760
Now, if we're talking about the field of data science specifically,


00:07:09.760 --> 00:07:12.560
I think language models have


00:07:12.560 --> 00:07:17.280
extraordinary utility and explanatory natural language tasks,


00:07:17.280 --> 00:07:19.440
which I think everybody is aware of now,


00:07:19.440 --> 00:07:22.880
now that ChatGPT has been out for almost a year.


00:07:22.880 --> 00:07:23.320
>> Yeah.


00:07:23.320 --> 00:07:28.680
>> But I think in the field of data science and other deep technical fields,


00:07:28.680 --> 00:07:33.800
they're especially applicable because of how complicated some of the work is.


00:07:33.800 --> 00:07:37.080
So yeah.


00:07:37.080 --> 00:07:42.520
Yeah. Chat AI can also help analyze and debug code,


00:07:42.520 --> 00:07:45.800
which JupyterLab also allows you to do.


00:07:45.800 --> 00:07:51.800
>> Yeah. I think I know it's statistics,


00:07:51.800 --> 00:07:53.120
but when you look at it,


00:07:53.120 --> 00:07:55.320
it seems like it understands.


00:07:55.320 --> 00:07:59.440
It seems like it understands my question.


00:07:59.440 --> 00:08:02.920
I think one of the really interesting parts is


00:08:02.920 --> 00:08:06.320
the fact that these LLMs have a context.


00:08:06.320 --> 00:08:09.320
I would like you to write a program in Python.


00:08:09.320 --> 00:08:11.640
Okay, great. Tell it to me.


00:08:11.640 --> 00:08:14.000
I want a program that does X, Y,


00:08:14.000 --> 00:08:15.520
Z, and then it writes it in Python,


00:08:15.520 --> 00:08:16.520
which sounds so simple,


00:08:16.520 --> 00:08:18.360
but up until then,


00:08:18.360 --> 00:08:23.280
things like Siri and all the other voice assistants,


00:08:23.280 --> 00:08:27.120
they seemed so disjointed and so not understand.


00:08:27.120 --> 00:08:29.200
I just asked you about the weather.


00:08:29.200 --> 00:08:30.960
When I ask you how hot it is,


00:08:30.960 --> 00:08:34.560
how do you not understand that that applies to the weather?


00:08:34.560 --> 00:08:40.960
The fact that you converse with them over a series of interactions is pretty special.


00:08:40.960 --> 00:08:47.160
>> Yeah. I mean, the context is basically implemented just by


00:08:47.160 --> 00:08:51.240
passing the whole history appended to your prompt.


00:08:51.240 --> 00:08:55.920
So yeah, it's not like any super staple magic or whatever,


00:08:55.920 --> 00:09:00.840
but it's still very interesting how far you can take it.


00:09:00.840 --> 00:09:02.680
Yeah, definitely.


00:09:02.680 --> 00:09:10.200
Like the context allows you to interact with the AI a lot more conversationally and humanly.


00:09:10.200 --> 00:09:13.200
You don't have to pretend you're talking to an AI.


00:09:13.200 --> 00:09:18.600
You can actually just treat it like a human and it still answers questions very well.


00:09:18.600 --> 00:09:22.760
>> Yeah. It was even at Google,


00:09:22.760 --> 00:09:25.200
there was that engineer who said they thought it had become


00:09:25.200 --> 00:09:28.840
sentient and there was that whole drama around that, right?


00:09:28.840 --> 00:09:31.680
>> This is such a crazy coincidence,


00:09:31.680 --> 00:09:35.840
but my roommate is actually friends with that gentleman.


00:09:35.840 --> 00:09:37.360
>> Oh, really? Okay. Wow.


00:09:37.360 --> 00:09:39.480
>> Absolutely crazy coincidence.


00:09:39.480 --> 00:09:42.480
Like I just thought it was really funny you bring it up.


00:09:42.480 --> 00:09:44.960
It was a Cajun gentleman,


00:09:44.960 --> 00:09:46.400
senior engineer at Google.


00:09:46.400 --> 00:09:47.880
>> Yeah.


00:09:47.880 --> 00:09:49.600
>> Yeah. Very funny.


00:09:49.600 --> 00:09:51.320
>> It's been a little while. I don't remember all the details,


00:09:51.320 --> 00:09:55.400
but yeah, I mean, it's pretty wild and pretty powerful.


00:09:55.400 --> 00:09:56.960
I think I've recently read,


00:09:56.960 --> 00:09:59.600
I'm trying to quick look it up, but I didn't find it.


00:09:59.600 --> 00:10:05.000
I think they just used LLMs to discover a new protein folding.


00:10:05.000 --> 00:10:07.280
It's that kind of stuff that makes me think like,


00:10:07.280 --> 00:10:14.440
okay, how interesting that knowledge wasn't out there in the world necessarily.


00:10:14.440 --> 00:10:18.840
>> Yeah. I have a lot to say on that subject.


00:10:18.840 --> 00:10:23.120
Personally, I don't believe that language models are actually intelligent.


00:10:23.120 --> 00:10:26.040
I think that people are conflating.


00:10:26.040 --> 00:10:28.760
Well, they're certainly not conscious.


00:10:28.760 --> 00:10:30.200
>> Absolutely. Yeah.


00:10:30.200 --> 00:10:31.760
>> As to whether they're intelligent,


00:10:31.760 --> 00:10:33.160
I don't think they are.


00:10:33.160 --> 00:10:35.360
I think that intelligence has,


00:10:35.360 --> 00:10:37.400
like intelligence as we know it,


00:10:37.400 --> 00:10:38.960
as humans know it, has


00:10:38.960 --> 00:10:42.680
some different characteristics that language models don't really exhibit.


00:10:42.680 --> 00:10:44.640
They're best thought of as really,


00:10:44.640 --> 00:10:47.560
really, really good statistical models.


00:10:47.560 --> 00:10:50.680
Are you familiar with the mirror test?


00:10:50.680 --> 00:10:52.600
>> Maybe, but I don't think so.


00:10:52.600 --> 00:10:55.600
>> Yeah. It's like this idea in animal psychology,


00:10:55.600 --> 00:10:57.800
but if a cat sees a mirror,


00:10:57.800 --> 00:10:59.760
it thinks it's another cat because-


00:10:59.760 --> 00:11:03.960
>> Right. You see them all get big and trying to act big and tough,


00:11:03.960 --> 00:11:05.720
to chase it off and it's just them.


00:11:05.720 --> 00:11:09.840
>> Yeah. Language models are like that, but for humans.


00:11:09.840 --> 00:11:14.320
If something mimics human-like quality closely enough,


00:11:14.320 --> 00:11:16.760
it's very tempting to think of it as human.


00:11:16.760 --> 00:11:21.600
>> Yeah. We see faces on Mars when it's really just erosion, stuff like that.


00:11:21.600 --> 00:11:25.080
>> Yeah. Exactly. I could talk about this for a full hour,


00:11:25.080 --> 00:11:28.080
but yeah, we should totally move on to another topic.


00:11:28.080 --> 00:11:30.280
>> Well, when you're saying it's not that intelligent,


00:11:30.280 --> 00:11:33.960
I'm wondering if maybe you've misnamed this project.


00:11:33.960 --> 00:11:36.040
Maybe it should just be Jupyter A,


00:11:36.040 --> 00:11:39.040
like the eye. Do you guys got the eye? I don't know.


00:11:39.040 --> 00:11:41.920
>> I mean, we're just following convention, right?


00:11:41.920 --> 00:11:43.680
So I still use the term AI.


00:11:43.680 --> 00:11:46.080
>> Of course. Of course. You got to talk to people.


00:11:46.080 --> 00:11:46.960
>> Yeah.


00:11:46.960 --> 00:11:49.960
>> Well, emphasize the artificial.


00:11:49.960 --> 00:11:50.600
>> Yeah.


00:11:50.600 --> 00:11:54.160
>> Yeah. Before we get to Jupyter AI,


00:11:54.160 --> 00:11:55.960
what came before?


00:11:55.960 --> 00:12:00.720
How did people work with things like Chats GPT and other LLMs in Jupyter,


00:12:00.720 --> 00:12:03.640
before stuff like Jupyter AI came along?


00:12:03.640 --> 00:12:05.640
>> Yeah. That's a great question.


00:12:05.640 --> 00:12:08.880
So I think initially,


00:12:08.880 --> 00:12:10.640
it was a combination.


00:12:10.640 --> 00:12:14.640
So the initial motivation for this project came through a combination


00:12:14.640 --> 00:12:19.280
of a demo put together by Fernando Perez,


00:12:19.280 --> 00:12:21.560
who is another, I believe,


00:12:21.560 --> 00:12:25.040
another co-founder of the project, Jupyter.


00:12:25.040 --> 00:12:28.640
He put together this demo called JupiT,


00:12:28.640 --> 00:12:30.920
which is spelled like Jupyter,


00:12:30.920 --> 00:12:32.520
except the last letter is an E,


00:12:32.520 --> 00:12:36.120
and it's a pun of like Chat GPT, right?


00:12:36.120 --> 00:12:43.960
JupiT. So it was a combination of that demo project set by Fernando,


00:12:43.960 --> 00:12:47.560
and some motivation from my manager, Brian,


00:12:47.560 --> 00:12:54.320
who also was as a leader in the AWS AI organization.


00:12:54.320 --> 00:12:58.040
He's always trying to think of fancy,


00:12:58.040 --> 00:12:59.400
fancy new ideas, right?


00:12:59.400 --> 00:13:03.160
This is a pretty fun idea to work out.


00:13:03.160 --> 00:13:05.200
So I put together,


00:13:05.200 --> 00:13:07.760
I think this was sometime in early January,


00:13:07.760 --> 00:13:09.600
I put together the first demo.


00:13:09.600 --> 00:13:13.080
It was private, and I showed it off to the team,


00:13:13.080 --> 00:13:14.440
and they were like, "Wow,


00:13:14.440 --> 00:13:15.560
this has a lot of potential.


00:13:15.560 --> 00:13:17.760
Let's see if we can grow it a bit more."


00:13:17.760 --> 00:13:21.640
Then as we worked on it for the next few months,


00:13:21.640 --> 00:13:23.400
it became clear like, "Oh, wow,


00:13:23.400 --> 00:13:25.720
this is actually really significant.


00:13:25.720 --> 00:13:27.280
Let's keep working on this."


00:13:27.280 --> 00:13:29.960
So yeah, it's definitely been


00:13:29.960 --> 00:13:35.000
a collaborative effort to bring Jupyter AI to where it is today.


00:13:35.000 --> 00:13:36.360
>> Yeah. Sounds like it.


00:13:36.360 --> 00:13:39.840
Definitely a lot of contributors over on the GitHub listing.


00:13:39.840 --> 00:13:41.880
So let's get into it.


00:13:41.880 --> 00:13:43.240
What is Jupyter AI?


00:13:43.240 --> 00:13:45.880
People can guess, but it's also different in ways


00:13:45.880 --> 00:13:49.000
than maybe just plug it in a chat window.


00:13:49.000 --> 00:13:51.960
>> Yeah. So Jupyter AI is actually,


00:13:51.960 --> 00:13:54.720
right now it's two packages.


00:13:54.720 --> 00:13:58.280
But it's best thought of as just a set of packages that


00:13:58.280 --> 00:14:02.320
bring generative AI to Project Jupyter as a whole.


00:14:02.320 --> 00:14:03.840
So not just JupyterLab,


00:14:03.840 --> 00:14:07.680
but also Jupyter Notebook and IPython.


00:14:07.680 --> 00:14:12.840
>> Even the shell. I guess


00:14:12.840 --> 00:14:16.360
you invoke it by doing the magic there as well.


00:14:16.360 --> 00:14:18.400
>> Well, it's the IPython shell,


00:14:18.400 --> 00:14:22.440
which is not the same as a Bash shell.


00:14:22.440 --> 00:14:24.080
>> Got it. Yeah, of course.


00:14:24.080 --> 00:14:25.520
>> Right.


00:14:25.520 --> 00:14:27.440
>> Interesting.


00:14:27.440 --> 00:14:29.920
>> Yeah. So I can dive into


00:14:29.920 --> 00:14:33.120
a little bit more detail on what these two packages are.


00:14:33.120 --> 00:14:37.160
So we have the base Jupyter AI package,


00:14:37.160 --> 00:14:39.360
which is spelled exactly as you might imagine it.


00:14:39.360 --> 00:14:41.840
It's Jupyter-AI.


00:14:41.840 --> 00:14:47.960
That is a JupyterLab extension that brings a UI to JupyterLab,


00:14:47.960 --> 00:14:52.120
which is the screenshot that you're showing on your screen,


00:14:52.120 --> 00:14:55.040
but for viewers without a screen,


00:14:55.040 --> 00:14:58.280
it basically is the package that adds


00:14:58.280 --> 00:15:00.800
that chat panel to the left-hand side


00:15:00.800 --> 00:15:04.240
and allows you to speak conversationally with an AI.


00:15:04.240 --> 00:15:07.840
Then the second package is Jupyter-AI magics,


00:15:07.840 --> 00:15:09.800
which is spelled the same except at the end,


00:15:09.800 --> 00:15:12.480
it's spelled hyphen magics.


00:15:12.480 --> 00:15:14.880
That is actually the base library that


00:15:14.880 --> 00:15:19.280
implements some of the AI providers we use


00:15:19.280 --> 00:15:24.440
and brings these things called magic commands to the IPython shell.


00:15:24.440 --> 00:15:30.080
Magic commands basically let you invoke our library code,


00:15:30.080 --> 00:15:34.680
aka using it to call language models, for instance.


00:15:34.680 --> 00:15:38.640
That allows you to do it inside an IPython context.


00:15:38.640 --> 00:15:42.360
What's crazy is that if you run IPython in your terminal shell,


00:15:42.360 --> 00:15:46.440
you can actually run Jupyter AI from your terminal,


00:15:46.440 --> 00:15:49.400
which is pretty cool.


00:15:49.400 --> 00:15:51.000
>> Yeah, I didn't realize that.


00:15:51.000 --> 00:15:52.360
It makes sense, of course,


00:15:52.360 --> 00:15:54.280
but I hadn't really thought about it.


00:15:54.280 --> 00:15:56.960
I thought more of this like a GUI type of


00:15:56.960 --> 00:15:59.200
thing that was alongside what you're doing.


00:15:59.200 --> 00:16:01.720
>> Yeah, we try to make it flexible and there's


00:16:01.720 --> 00:16:03.760
reasons for the magic commands


00:16:03.760 --> 00:16:06.080
which I can talk about that later though.


00:16:06.080 --> 00:16:09.280
>> Yeah, sure. Now, one thing,


00:16:09.280 --> 00:16:13.120
you said this but I want to emphasize it a little bit in that.


00:16:13.120 --> 00:16:14.360
>> Yeah.


00:16:14.360 --> 00:16:18.880
>> This will run anywhere IPython kernel runs,


00:16:18.880 --> 00:16:20.400
which is JupyterLab notebook,


00:16:20.400 --> 00:16:23.040
but also Google Colab, VS Code,


00:16:23.040 --> 00:16:24.560
other places as well, right?


00:16:24.560 --> 00:16:30.000
So pretty much it comes to you wherever your Jupyter type of stuff is, right?


00:16:30.000 --> 00:16:32.760
>> Yeah, and the same goes for the lab extension.


00:16:32.760 --> 00:16:35.960
So the great thing about lab extensions is that they work


00:16:35.960 --> 00:16:42.720
anywhere where the product is just built on top of JupyterLab, right?


00:16:42.720 --> 00:16:47.080
So Google Colab is essentially what Google has is a,


00:16:47.080 --> 00:16:51.480
well, I obviously can't attest to what they're actually doing.


00:16:51.480 --> 00:16:53.880
But most likely, it's like a set of extensions or


00:16:53.880 --> 00:16:56.960
CSS themes that are built on top of JupyterLab,


00:16:56.960 --> 00:17:00.040
but the underlying code is still JupyterLab.


00:17:00.040 --> 00:17:01.400
It's still mostly JupyterLab.


00:17:01.400 --> 00:17:05.000
So you can actually just install extensions and they work just fine,


00:17:05.000 --> 00:17:09.680
which is another reason why JupyterLab is just pretty awesome.


00:17:09.680 --> 00:17:11.480
>> Yeah, it sure is.


00:17:11.480 --> 00:17:15.680
JupyterLab itself is basically a pre-selected,


00:17:15.680 --> 00:17:18.280
pre-configured set of extensions, right?


00:17:18.280 --> 00:17:19.800
>> Yes, that is true.


00:17:19.800 --> 00:17:26.840
>> Yeah. So I've been giving preference to,


00:17:26.840 --> 00:17:29.840
or showing just what I play with mostly,


00:17:29.840 --> 00:17:31.480
which is chat-gpt.


00:17:31.480 --> 00:17:35.240
But there's actually a lot of language models that you can work with, right?


00:17:35.240 --> 00:17:39.400
>> Oh, yes. So one of the big things,


00:17:39.400 --> 00:17:41.640
and this is something I'll circle back to you later,


00:17:41.640 --> 00:17:44.880
is that Jupyter AI is meant to be model agnostic,


00:17:44.880 --> 00:17:49.640
meaning that we don't discriminate against the choice of model or model provider.


00:17:49.640 --> 00:17:51.560
Because as an open-source project,


00:17:51.560 --> 00:17:54.920
it's very imperative that we maintain the trust of our users, right?


00:17:54.920 --> 00:17:57.440
The users have to be sure that this isn't just


00:17:57.440 --> 00:18:00.760
some product that exists to force them,


00:18:00.760 --> 00:18:08.920
or pigeonhole them into using a certain model provider like OpenAI or Anthropic, right?


00:18:08.920 --> 00:18:11.640
The product makes no opinions.


00:18:11.640 --> 00:18:15.200
We simply try to support everything as best as we can.


00:18:15.200 --> 00:18:19.320
We've written a lot of code to make sure that


00:18:19.320 --> 00:18:25.040
all of these models just play nicely together essentially.


00:18:25.040 --> 00:18:31.440
Every model provider, let's say from Anthropic or AI21 or Cohere,


00:18:31.440 --> 00:18:34.800
every one of these APIs has its own quirks,


00:18:34.800 --> 00:18:37.880
every one of these Python SDKs has its own quirks.


00:18:37.880 --> 00:18:42.000
We work very hard to basically iron out


00:18:42.000 --> 00:18:47.000
the surface and make everything have the same interface.


00:18:47.000 --> 00:18:50.440
I think we can talk about that later though.


00:18:50.440 --> 00:18:54.120
>> Sure. It also makes it pretty easy to try them out, right?


00:18:54.120 --> 00:18:55.040
>> Oh, yeah.


00:18:55.040 --> 00:18:56.000
>> Switch from one to the other.


00:18:56.000 --> 00:19:01.280
I wonder how Hugging Face versus OpenAI would do to solve this problem, right?


00:19:01.280 --> 00:19:03.080
>> Absolutely, right?


00:19:03.080 --> 00:19:07.840
That's one of the ideas is while certain model providers,


00:19:07.840 --> 00:19:13.040
they might offer a UI if they're very well funded by their investors.


00:19:13.040 --> 00:19:16.400
For example, OpenAI, they have a UI.


00:19:16.400 --> 00:19:21.520
But that UI only allows you to compare between different models from OpenAI,


00:19:21.520 --> 00:19:28.760
which as independent third party looking to use an AI service,


00:19:28.760 --> 00:19:31.680
that information is obviously a little biased, right?


00:19:31.680 --> 00:19:34.760
You want to see what other providers have to offer.


00:19:34.760 --> 00:19:36.280
What does AI21 have?


00:19:36.280 --> 00:19:38.080
What does Anthropic have?


00:19:38.080 --> 00:19:49.400
Right now, there really is no cross-model provider UI or interface in general.


00:19:49.720 --> 00:19:56.280
That's one of the use cases that Jupyter AI was intended to fit.


00:19:56.280 --> 00:20:02.000
>> Yeah, provides a standard way to interact with all these and compare them.


00:20:02.000 --> 00:20:04.400
It's also a UI for them if you're using JupyterLab.


00:20:04.400 --> 00:20:12.680
Yeah. I'm not familiar with all of these different models and companies.


00:20:12.680 --> 00:20:14.680
Do any of those run locally,


00:20:14.680 --> 00:20:21.640
things like GPT for all where it's like a local model versus some kind of Cloud?


00:20:21.640 --> 00:20:25.160
Where's your key? Where's your billing details and all that?


00:20:25.160 --> 00:20:29.960
>> Yeah. We actually recently just merged


00:20:29.960 --> 00:20:35.920
a PR that adds GPT for all support and that's included in the release.


00:20:35.920 --> 00:20:41.560
However, I think back when we first implemented this a few months ago,


00:20:41.560 --> 00:20:46.160
I had a few issues with platform compatibility.


00:20:46.160 --> 00:20:51.000
Some of the binaries that we downloaded from GPT for


00:20:51.000 --> 00:20:56.120
all didn't seem to work well on my M1 Mac, for instance.


00:20:56.120 --> 00:20:59.560
I'd say, yes, we do have local model support,


00:20:59.560 --> 00:21:01.600
but it's a bit experimental right now.


00:21:01.600 --> 00:21:03.680
We're still ironing out the edges and


00:21:03.680 --> 00:21:07.200
testing, seeing how we can make the experience better.


00:21:07.200 --> 00:21:10.080
Does it sometimes only give bad output because we


00:21:10.080 --> 00:21:12.560
forget to install the shared library?


00:21:12.560 --> 00:21:16.480
Those are the type of questions that our team is wrangling right now.


00:21:16.480 --> 00:21:18.680
>> I see. I just jumped right into it,


00:21:18.680 --> 00:21:22.320
but maybe tell people what GPT for all is just real quickly.


00:21:22.320 --> 00:21:27.720
>> Yeah. I mean, GPT for all offers a few local models.


00:21:27.720 --> 00:21:29.840
Actually, they offer several, I believe.


00:21:29.840 --> 00:21:32.800
>> Yeah. I think they offer maybe like 10 to 15.


00:21:32.800 --> 00:21:34.520
The number is getting quite large.


00:21:34.520 --> 00:21:36.080
>> Yeah.


00:21:36.080 --> 00:21:38.800
>> To the point where I don't know what the right choice is.


00:21:38.800 --> 00:21:40.680
I'm like, which one do I download?


00:21:40.680 --> 00:21:42.440
They say they're all good. Of course,


00:21:42.440 --> 00:21:43.520
they're going to say they're good.


00:21:43.520 --> 00:21:45.960
>> Yeah. I'll be frank.


00:21:45.960 --> 00:21:49.640
I don't actually have that much experience with GPT for all.


00:21:49.640 --> 00:21:52.880
We mainly use them as a provider


00:21:52.880 --> 00:21:56.200
for these free and open source language models.


00:21:56.200 --> 00:21:57.280
>> Yeah.


00:21:57.280 --> 00:22:00.040
>> Yeah. Beyond that,


00:22:00.040 --> 00:22:04.120
I think they offer a UI as well for multiple platforms.


00:22:04.120 --> 00:22:06.040
>> They do. Yeah. Basically,


00:22:06.040 --> 00:22:07.760
I've only played with them a little bit,


00:22:07.760 --> 00:22:08.840
just started checking it out.


00:22:08.840 --> 00:22:11.320
But it's basically a local.


00:22:11.320 --> 00:22:13.720
You download the model, you run it locally,


00:22:13.720 --> 00:22:17.160
you don't pay anything because it's just running on your machine,


00:22:17.160 --> 00:22:18.680
as opposed to say,


00:22:18.680 --> 00:22:23.840
OpenAI and others where you've at least got rate limiting,


00:22:23.840 --> 00:22:26.120
and certain amount of queries before you have to pay,


00:22:26.120 --> 00:22:30.000
and maybe potentially access to better models like chat,


00:22:30.000 --> 00:22:32.720
GPT-4 versus 3.5 and so on.


00:22:32.720 --> 00:22:35.320
>> Well, I think that's also taking


00:22:35.320 --> 00:22:40.720
the characteristics of these service providers for granted.


00:22:40.720 --> 00:22:44.160
Yes, definitely.


00:22:44.160 --> 00:22:49.600
While it does hurt the wallet to pay for usage credits,


00:22:49.600 --> 00:22:52.400
it's also pretty remarkable how


00:22:52.400 --> 00:22:55.760
small the latency has gotten with some of these APIs.


00:22:55.760 --> 00:23:00.200
I've gotten sub 500 millisecond latency on some of these APIs.


00:23:00.200 --> 00:23:04.440
That's really incredible because when I was using GPT for all,


00:23:04.440 --> 00:23:07.080
the latency was a little bit high,


00:23:07.080 --> 00:23:10.560
when running locally with limited compute resources.


00:23:10.560 --> 00:23:14.240
It's really remarkable how fast these APIs are.


00:23:14.240 --> 00:23:18.000
>> It is pretty insane. Sometimes, it drives me crazy.


00:23:18.000 --> 00:23:20.200
I'm just only again referring to ChatGPT


00:23:20.200 --> 00:23:22.440
because I don't have the experience with the others to the degree,


00:23:22.440 --> 00:23:26.680
but it drives me crazy how it artificially limits


00:23:26.680 --> 00:23:30.360
the response based on the speed of the response.


00:23:30.360 --> 00:23:32.320
So it looks like it's chatting with you.


00:23:32.320 --> 00:23:36.600
I'm like, "No, I have four pages of stuff because you just get it out."


00:23:36.600 --> 00:23:39.320
It'll say something like,


00:23:39.320 --> 00:23:44.880
if you can ask, I gave you a five-page program.


00:23:44.880 --> 00:23:47.160
Let's call it X. If you say,


00:23:47.160 --> 00:23:50.240
"What is X?" It'll just start printing it slowly line by line.


00:23:50.240 --> 00:23:52.440
You know you just are echoing it back.


00:23:52.440 --> 00:23:55.280
Just get it out. I want to ask you the next question.


00:23:55.280 --> 00:23:56.480
You know what I mean?


00:23:56.480 --> 00:23:59.120
>> Yeah. In that case,


00:23:59.120 --> 00:24:01.320
that's actually a feature request that we've gotten


00:24:01.320 --> 00:24:03.880
because it doesn't actually slow down.


00:24:03.880 --> 00:24:06.320
It's not just like a pointless animation.


00:24:06.320 --> 00:24:12.080
The servers are streaming essentially token by token,


00:24:12.080 --> 00:24:14.520
as the language model generates output.


00:24:14.520 --> 00:24:17.440
So it's more like a progress indicator


00:24:17.440 --> 00:24:21.520
than a superfluous animation.


00:24:21.520 --> 00:24:25.000
>> Yeah, of course. But if you've got something large,


00:24:25.000 --> 00:24:27.200
large blocks of text you're working with,


00:24:27.200 --> 00:24:29.440
it can be a drag.


00:24:29.440 --> 00:24:34.240
I wanted to touch on some of


00:24:34.240 --> 00:24:37.200
the different features that I pulled out that I thought were cool.


00:24:37.200 --> 00:24:43.400
I mean, obviously, it goes without saying that Jupyter AI is on GitHub.


00:24:43.400 --> 00:24:45.280
I mean, because it's software.


00:24:45.280 --> 00:24:48.560
It's open source, which I don't know if we said that,


00:24:48.560 --> 00:24:51.840
but obviously free open source on GitHub,


00:24:51.840 --> 00:24:54.720
BSD3 license.


00:24:54.720 --> 00:24:58.560
But it's also noteworthy that it's officially under


00:24:58.560 --> 00:25:03.920
the JupyterLab organization, not under the David account.


00:25:03.920 --> 00:25:10.520
>> Yeah. It's officially part of the JupyterLab sub-project.


00:25:10.520 --> 00:25:12.000
Yeah, as you pointed out,


00:25:12.000 --> 00:25:14.840
we're under the JupyterLab GitHub org as well.


00:25:14.840 --> 00:25:17.080
>> Yeah, that's awesome. All right.


00:25:17.080 --> 00:25:20.480
So let's talk about some of the different things you can do with it.


00:25:20.480 --> 00:25:22.120
Some of them will be straightforward,


00:25:22.120 --> 00:25:26.520
like just like how do I write a function in Jupyter AI?


00:25:26.520 --> 00:25:29.320
Others, I think, are going to be a little more interesting.


00:25:29.320 --> 00:25:32.960
So let's start with asking something about your notebook.


00:25:32.960 --> 00:25:35.960
Tell us what people can do here.


00:25:35.960 --> 00:25:39.440
>> Yeah. So asking about your notebook basically means


00:25:39.440 --> 00:25:43.280
like you can actually teach Jupyter AI about certain files.


00:25:43.280 --> 00:25:47.560
So the way you do this is via a slash command in the chat UI.


00:25:47.560 --> 00:25:50.720
So you type slash learn and then file path,


00:25:50.720 --> 00:25:56.560
and it essentially teaches the Jupyter AI about that file.


00:25:56.560 --> 00:26:01.680
Now, it works best with files that are written in natural language.


00:26:01.680 --> 00:26:06.280
So like text files or markdown.


00:26:06.280 --> 00:26:12.040
Yeah. So especially like developer documentation as well.


00:26:12.040 --> 00:26:16.960
It works best with those kind of files.


00:26:16.960 --> 00:26:20.600
After Jupyter AI learns about these files,


00:26:20.600 --> 00:26:24.920
you can then ask questions about the files it's learned by


00:26:24.920 --> 00:26:32.520
prefixing your question with the slash command, which is, yeah.


00:26:32.520 --> 00:26:33.480
>> That is so cool.


00:26:33.480 --> 00:26:36.000
>> It's pretty cool. It is pretty cool, I know.


00:26:36.000 --> 00:26:40.480
>> It's so cool because what I've done a lot of times,


00:26:40.480 --> 00:26:42.560
if I want ChatGPT to help me,


00:26:42.560 --> 00:26:43.720
it's like, all right, well,


00:26:43.720 --> 00:26:45.400
let me copy some code.


00:26:45.400 --> 00:26:46.080
>> Right.


00:26:46.080 --> 00:26:48.400
>> Then I'm going to have a conversation about it.


00:26:48.400 --> 00:26:50.440
But a lot of the context of, well,


00:26:50.440 --> 00:26:53.480
it's actually referencing this other function and what does that do?


00:26:53.480 --> 00:26:59.800
Or just a broader understanding of what am I actually working on is missing.


00:26:59.800 --> 00:27:06.200
Because you can't paste 20 files into ChatGPT and start talking about it.


00:27:06.200 --> 00:27:07.440
But with this, you can.


00:27:07.440 --> 00:27:12.600
You can say learn about different things.


00:27:12.600 --> 00:27:14.480
You can say learn about your notebook,


00:27:14.480 --> 00:27:16.280
but you can also probably tell it like,


00:27:16.280 --> 00:27:20.320
learn about my documentation or learn about my dataset.


00:27:20.320 --> 00:27:22.600
Now, let me talk to you about it.


00:27:22.600 --> 00:27:27.760
>> Yeah. What's interesting is that right now,


00:27:27.760 --> 00:27:30.440
while it works best for natural language documents,


00:27:30.440 --> 00:27:33.760
we are working on improving the experience for code.


00:27:33.760 --> 00:27:39.520
From our testing, the code is mostly like the capabilities of


00:27:39.520 --> 00:27:43.800
Jupyter AI after learning code is right now,


00:27:43.800 --> 00:27:46.720
mostly limited to explaining what code does.


00:27:46.720 --> 00:27:51.480
But it explains it from the doc stream.


00:27:51.480 --> 00:27:54.800
We're working on a way to format the code in a manner


00:27:54.800 --> 00:27:58.920
that is more interpretable to a language model.


00:27:58.920 --> 00:28:00.640
It's more, yeah,


00:28:00.640 --> 00:28:03.760
and we're working on ways to improve the experience for code.


00:28:03.760 --> 00:28:05.240
But yeah, definitely.


00:28:05.240 --> 00:28:09.720
The long-term vision is to have Jupyter AI literally be able to learn from


00:28:09.720 --> 00:28:14.080
a whole directory of files or possibly even a URL,


00:28:14.080 --> 00:28:18.960
like a remote URL to a documentation page.


00:28:18.960 --> 00:28:22.760
Or yeah, we have some big ideas there.


00:28:22.760 --> 00:28:24.560
We're still working on them.


00:28:24.560 --> 00:28:28.680
>> I want to work with a new package XYZ.


00:28:28.680 --> 00:28:30.360
I don't know what XYZ is.


00:28:30.360 --> 00:28:32.940
You know what? Here's where you can learn about it.


00:28:32.940 --> 00:28:34.960
Go and figure it out. Yeah.


00:28:34.960 --> 00:28:38.400
>> Right. Like query the PyPI API,


00:28:38.400 --> 00:28:41.720
get the docs page from the metadata,


00:28:41.720 --> 00:28:44.080
and then go to that URL, scrape it.


00:28:44.080 --> 00:28:44.600
>> Yeah.


00:28:44.600 --> 00:28:47.320
>> Lots of things we're exploring.


00:28:47.320 --> 00:28:48.560
I've been trying.


00:28:48.560 --> 00:28:50.400
>> It's still early days for this, right?


00:28:50.400 --> 00:28:52.280
You've been at it about a year or so?


00:28:52.280 --> 00:28:54.760
>> Yeah. It's been out for a while.


00:28:54.760 --> 00:28:58.600
Recently, I've had to work on a few other things as well,


00:28:58.600 --> 00:29:00.760
like Jupyter AI.


00:29:00.760 --> 00:29:04.400
Unfortunately, I cannot give my entire life to Jupyter AI.


00:29:04.400 --> 00:29:07.640
So I've been working on a few other things these past few months.


00:29:07.640 --> 00:29:13.400
But yes, there are a lot of things that I envision for Jupyter AI.


00:29:13.400 --> 00:29:17.240
I have a much bigger vision for what I want this project to be capable of.


00:29:17.240 --> 00:29:19.600
>> Okay. Exciting.


00:29:19.600 --> 00:29:25.200
Exciting. So what is this screenshot that you got here in the section that I'll link to in


00:29:25.200 --> 00:29:30.280
the show notes is cool because you can select a piece of a portion,


00:29:30.280 --> 00:29:31.440
not even a whole cell,


00:29:31.440 --> 00:29:34.000
but a portion of code in a cell,


00:29:34.000 --> 00:29:35.400
and then you can ask,


00:29:35.400 --> 00:29:37.000
what does this code do?


00:29:37.000 --> 00:29:38.200
>> Right.


00:29:38.200 --> 00:29:39.360
>> That's awesome.


00:29:39.360 --> 00:29:44.400
>> Yeah. We have an integration with the JupyterLab Editor APIs.


00:29:44.400 --> 00:29:49.360
So you can select a block of code and then include that in your prompt,


00:29:49.360 --> 00:29:53.560
and it will be appended to your prompt below.


00:29:53.560 --> 00:29:57.440
It's appended to box, sorry.


00:29:57.440 --> 00:30:01.560
You can basically ask, so you can select a block of code.


00:30:01.560 --> 00:30:03.480
So in this screenshot right here,


00:30:03.480 --> 00:30:10.160
there's this block of code that computes the least common multiple of two integers.


00:30:10.160 --> 00:30:13.600
You can select that and then click "Include


00:30:13.600 --> 00:30:15.440
selection" and then ask Jupyter AI,


00:30:15.440 --> 00:30:18.840
what does this do, which is pretty awesome.


00:30:18.840 --> 00:30:21.640
>> Yeah. Another checkbox is "Replace selection."


00:30:21.640 --> 00:30:22.960
I'm guessing that is like,


00:30:22.960 --> 00:30:28.360
help me rewrite this code to be more efficient or if there's any bugs, fix it.


00:30:28.360 --> 00:30:31.680
>> So the replace selection checkbox is totally independent.


00:30:31.680 --> 00:30:35.040
So you can actually use both at the same time.


00:30:35.040 --> 00:30:37.760
One of the use cases for this is refactoring.


00:30:37.760 --> 00:30:40.640
I've actually applied this in practice a few times,


00:30:40.640 --> 00:30:44.840
where you can basically select a block of code,


00:30:44.840 --> 00:30:48.400
and then click both "Include" and "Replace selection."


00:30:48.400 --> 00:30:50.880
Then you can format your prompt to say,


00:30:50.880 --> 00:30:52.920
refactor this block of code,


00:30:52.920 --> 00:30:55.760
do not include any additional help or text.


00:30:55.760 --> 00:30:57.760
When you send that prompt over,


00:30:57.760 --> 00:31:06.040
it will actually refactor the code for you in your notebook, which is pretty great.


00:31:06.040 --> 00:31:09.880
>> That's pretty awesome. You could do things like,


00:31:09.880 --> 00:31:14.200
refactor this to use guarding clauses so it's less nested,


00:31:14.200 --> 00:31:17.200
less arrow code or whatever.


00:31:17.200 --> 00:31:20.040
>> Yeah, or add a docstring.


00:31:20.040 --> 00:31:27.640
Summarize the purpose of this function and then enclose that in a docstring.


00:31:27.640 --> 00:31:29.080
Add it to the function.


00:31:29.080 --> 00:31:30.680
>> Yeah.


00:31:30.680 --> 00:31:32.880
>> Or this code is Panda's code,


00:31:32.880 --> 00:31:34.280
but I'd like to use Polars.


00:31:34.280 --> 00:31:36.040
Please rewrite it for Polar.


00:31:36.040 --> 00:31:36.360
>> Yes.


00:31:36.360 --> 00:31:38.240
>> It's not a super compatible API.


00:31:38.240 --> 00:31:39.520
It's not like DAST to Panda,


00:31:39.520 --> 00:31:40.760
where it's basically the same.


00:31:40.760 --> 00:31:44.600
>> Yeah. This circles back to that question you had asked earlier.


00:31:44.600 --> 00:31:47.160
I think I went on a tangent there and didn't fully answer,


00:31:47.160 --> 00:31:51.600
but what is the utility of Jupyter AI to data practitioners?


00:31:51.600 --> 00:31:55.040
So data scientists, machine learning engineers.


00:31:55.040 --> 00:31:57.440
The include selection features,


00:31:57.440 --> 00:31:59.760
we've heard great feedback about how helpful it is


00:31:59.760 --> 00:32:03.040
to actually explain a dataset.


00:32:03.040 --> 00:32:06.200
So sometimes you're working with a test set


00:32:06.200 --> 00:32:10.120
and it's not immediately clear what the features of this test set are,


00:32:10.120 --> 00:32:14.160
or what this even does because sometimes it's high-dimensional data.


00:32:14.160 --> 00:32:16.800
They can literally select it and then click


00:32:16.800 --> 00:32:20.160
include selection and say, and tell Jupyter AI,


00:32:20.160 --> 00:32:22.040
explain to me what this does.


00:32:22.040 --> 00:32:24.480
Just like what is this data frame stuff?


00:32:24.480 --> 00:32:26.560
Like, well, we got data frames and data frames.


00:32:26.560 --> 00:32:27.840
What's going on here?


00:32:27.840 --> 00:32:29.880
What even is the structure of this?


00:32:29.880 --> 00:32:31.000
>> Yeah.


00:32:31.000 --> 00:32:31.640
>> Yeah.


00:32:31.640 --> 00:32:33.440
>> That's awesome. I think it's super valuable.


00:32:33.440 --> 00:32:36.720
I think this is a little bit I was getting to before,


00:32:36.720 --> 00:32:38.360
one of the features that I think is cool.


00:32:38.360 --> 00:32:41.880
Whereas if you just go with straight ChatGPT,


00:32:41.880 --> 00:32:42.920
you copy your code,


00:32:42.920 --> 00:32:44.360
you paste it into the chat.


00:32:44.360 --> 00:32:47.720
Hopefully, it doesn't say it's too much text and then you can talk about it.


00:32:47.720 --> 00:32:49.080
But then when you get an answer,


00:32:49.080 --> 00:32:50.280
you've got to grab it,


00:32:50.280 --> 00:32:51.480
move it back over,


00:32:51.480 --> 00:32:55.680
and this fluid back and forth is really nice.


00:32:55.680 --> 00:32:59.840
>> Yeah. That's actually one of the design principles that we


00:32:59.840 --> 00:33:03.160
worked out when first starting this project officially,


00:33:03.160 --> 00:33:06.400
is the idea that Jupyter AI should be human-centered.


00:33:06.400 --> 00:33:11.920
As in, you shouldn't be expected to be a developer to know how to use this tool.


00:33:11.920 --> 00:33:13.480
Like this tool is for humans,


00:33:13.480 --> 00:33:15.920
not for any specific persona,


00:33:15.920 --> 00:33:18.200
just for humans in general.


00:33:18.200 --> 00:33:20.920
>> That's awesome. Yeah. In this case,


00:33:20.920 --> 00:33:24.600
you select the function that does the lowest common denominator bit,


00:33:24.600 --> 00:33:26.080
and you ask it what it does.


00:33:26.080 --> 00:33:27.840
It says, "The code will print out


00:33:27.840 --> 00:33:31.000
the least common multiple of two numbers passed to it."


00:33:31.000 --> 00:33:33.520
Super simple, very concise.


00:33:33.520 --> 00:33:36.680
Okay, great. Now, we can go on to the next thing.


00:33:36.680 --> 00:33:37.560
>> Yeah.


00:33:37.560 --> 00:33:46.120
>> That's awesome. Then there's this LCD function


00:33:46.120 --> 00:33:48.160
that we're talking about here.


00:33:48.160 --> 00:33:56.360
This example is recursive, which I think recursion is pretty insane.


00:33:56.360 --> 00:33:56.680
>> Yeah.


00:33:56.680 --> 00:33:58.760
>> As just a concept for people like-


00:33:58.760 --> 00:34:01.320
>> Wait, no. This is the iterative version.


00:34:01.320 --> 00:34:02.840
So this is after the-


00:34:02.840 --> 00:34:05.880
>> This is after, yeah. So if we go back up.


00:34:05.880 --> 00:34:06.440
>> Yeah.


00:34:06.440 --> 00:34:09.960
>> It's- Anyway.


00:34:09.960 --> 00:34:12.480
But one of the things you ask it is things like,


00:34:12.480 --> 00:34:15.040
an example is rewrite this function to be iterative,


00:34:15.040 --> 00:34:18.560
not recursive, which is actually really, really awesome.


00:34:18.560 --> 00:34:20.720
You're like, "This is breaking my brain.


00:34:20.720 --> 00:34:24.000
Let's see if we can not do that anymore."


00:34:24.000 --> 00:34:26.600
Another thing I wanted to talk about,


00:34:26.600 --> 00:34:31.400
and you've talked a fair amount about this in your presentations that you did.


00:34:31.400 --> 00:34:35.240
I can't remember if it was the JupyterCon or the PyData one that I saw.


00:34:35.240 --> 00:34:35.440
>> Yeah.


00:34:35.440 --> 00:34:38.680
>> But one of those two. You talked about generating


00:34:38.680 --> 00:34:43.840
new notebooks and how it's actually quite a tricky process.


00:34:43.840 --> 00:34:46.720
You've got to break it down into little steps because if you ask too much from


00:34:46.720 --> 00:34:50.360
the AI it doesn't give you a lot of great answers.


00:34:50.360 --> 00:34:52.840
Tell us about making new notebooks.


00:34:52.840 --> 00:34:55.640
Why would I even use- I can go to JupyterLab and say,


00:34:55.640 --> 00:34:58.120
"File new," and it'll make that for me.


00:34:58.120 --> 00:34:59.440
So what's this about?


00:34:59.440 --> 00:35:04.920
>> Yeah. So the generate capability is great because it generates a file that is


00:35:04.920 --> 00:35:09.240
essentially a tutorial that can be used as a tutorial to teach you about new subjects.


00:35:09.240 --> 00:35:12.080
So you could, for example,


00:35:12.080 --> 00:35:17.000
submit a prompt like slash generate a notebook about asyncio,


00:35:17.000 --> 00:35:21.000
or a demonstration of how to use Matplotlib.


00:35:21.000 --> 00:35:23.800
This will take a bit of time,


00:35:23.800 --> 00:35:28.080
but eventually Jupyter AI is done thinking and generates a file.


00:35:28.080 --> 00:35:31.360
It names the file and generates a notebook,


00:35:31.360 --> 00:35:32.640
and the notebook has a name,


00:35:32.640 --> 00:35:35.320
it has a title, it has sections,


00:35:35.320 --> 00:35:38.800
table of contents, and each of the cells within it,


00:35:38.800 --> 00:35:43.840
is tied to some topic that is determined to be


00:35:43.840 --> 00:35:48.280
helpful and to answer the user's question.


00:35:48.280 --> 00:35:50.240
>> Awesome. Could I do something like,


00:35:50.240 --> 00:35:58.280
I have weather data in this format from the US Weather Service,


00:35:58.280 --> 00:36:01.320
and could you generate me a notebook to plot


00:36:01.320 --> 00:36:04.760
this XYZ and help me answer these questions?


00:36:04.760 --> 00:36:07.120
Could I ask it something like that?


00:36:07.120 --> 00:36:09.120
>> Not at the moment.


00:36:09.120 --> 00:36:13.600
So that would be done best.


00:36:13.600 --> 00:36:15.840
Since the data is already,


00:36:15.840 --> 00:36:21.040
I'm assuming that the data is already available in some format.


00:36:21.040 --> 00:36:27.520
So in a notebook, you could use a chat UI to select that selection,


00:36:27.520 --> 00:36:33.480
and then tell Jupyter AI to generate code to plot that data set.


00:36:33.480 --> 00:36:40.200
So right now, generate only takes a natural language prompt as its only argument.


00:36:40.200 --> 00:36:40.520
>> I see.


00:36:40.520 --> 00:36:43.080
>> So it's stateless in that regard.


00:36:43.080 --> 00:36:43.720
>> Yeah.


00:36:43.720 --> 00:36:45.720
>> So in this case, you can say slash generate


00:36:45.720 --> 00:36:48.800
a demonstration of how to use Matplotlib,


00:36:48.800 --> 00:36:50.240
and then the response is great.


00:36:50.240 --> 00:36:51.720
I'll start working on your notebook.


00:36:51.720 --> 00:36:54.320
It'll take a few minutes, but I'll reply when it's ready.


00:36:54.320 --> 00:36:56.880
In the meantime, let's keep talking.


00:36:56.880 --> 00:37:02.200
So what happens behind the scenes that takes a few minutes here?


00:37:02.200 --> 00:37:05.880
>> Oh, yeah. So this is a bit interesting.


00:37:05.880 --> 00:37:09.280
It does dive deep into the technical details,


00:37:09.280 --> 00:37:12.720
which I'm not sure if you want to just like that.


00:37:12.720 --> 00:37:15.080
>> Yeah, let's do it. Yeah. Let's go tell us how it works.


00:37:15.080 --> 00:37:15.640
>> Yeah.


00:37:15.640 --> 00:37:17.640
>> So it's a good chance to explore that.


00:37:17.640 --> 00:37:19.240
>> Yeah. So slash generate,


00:37:19.240 --> 00:37:29.000
the first thing is that the prompt is first expanded into essentially a table of contents.


00:37:29.000 --> 00:37:31.240
So basically, we tell the language model,


00:37:31.240 --> 00:37:35.920
generate us a table of contents conforming to this JSON schema.


00:37:35.920 --> 00:37:39.560
When you pass a JSON schema included in your prompt,


00:37:39.560 --> 00:37:45.800
the language model will have a much higher likelihood


00:37:45.800 --> 00:37:50.080
of returning exclusively a JSON object


00:37:50.080 --> 00:37:53.800
that matches that JSON schema that you had initially provided.


00:37:53.800 --> 00:37:57.120
So in our case, we generate a table of contents,


00:37:57.120 --> 00:37:58.920
and then we take that table of contents,


00:37:58.920 --> 00:38:01.400
and then we say for each section,


00:38:01.400 --> 00:38:04.040
and we do this in parallel,


00:38:04.040 --> 00:38:07.240
generate us some code cells that are


00:38:07.240 --> 00:38:11.040
appropriate for teaching this section of the document.


00:38:11.040 --> 00:38:13.920
So for example, for matplotlib,


00:38:13.920 --> 00:38:17.960
maybe the first section is generating your first plot,


00:38:17.960 --> 00:38:20.520
plotting 3D functions, and the next one is like


00:38:20.520 --> 00:38:24.280
plotting complex functions with phase or something like that.


00:38:24.280 --> 00:38:27.520
Then with each of these sections,


00:38:27.520 --> 00:38:31.480
we then send another prompt template to


00:38:31.480 --> 00:38:35.200
the language model for each of these sections asking it to generate the code.


00:38:35.200 --> 00:38:37.680
Then at the end, we join it all together,


00:38:37.680 --> 00:38:38.920
and then we save it to disk,


00:38:38.920 --> 00:38:41.240
and then emit that message and say we're done.


00:38:41.240 --> 00:38:48.580
>> Okay. So maybe the English literature equivalent would be,


00:38:48.580 --> 00:38:50.880
instead of just saying, write me a story


00:38:50.880 --> 00:38:54.520
about a person who goes on an adventure and gets lost,


00:38:54.520 --> 00:38:58.080
it's like, give me an outline,


00:38:58.080 --> 00:39:00.680
bullet points of interesting things that would make up


00:39:00.680 --> 00:39:03.280
a story of how somebody goes on an adventure and gets lost.


00:39:03.280 --> 00:39:05.120
>> Right. Give us the-


00:39:05.120 --> 00:39:06.840
>> Yeah. Then for each one of those, you're like,


00:39:06.840 --> 00:39:08.440
now tell this part of the story,


00:39:08.440 --> 00:39:13.280
now tell this part, and somehow that makes it more focused and accurate, right?


00:39:13.280 --> 00:39:18.460
>> Well, the main limitation is that because we're model agnostic,


00:39:18.460 --> 00:39:22.400
language models are limited in how much output they can generate.


00:39:22.400 --> 00:39:23.080
>> Yeah.


00:39:23.080 --> 00:39:27.920
>> So the issue we were running into when we were trying to do the whole thing all at


00:39:27.920 --> 00:39:33.000
once like generate a whole notebook is that some language models just couldn't do it.


00:39:33.000 --> 00:39:36.580
In an effort to stay model agnostic,


00:39:36.580 --> 00:39:42.960
we deliberately broke this process down into smaller sub-tasks,


00:39:42.960 --> 00:39:47.960
each with its own prompt template in order to accommodate these models that may


00:39:47.960 --> 00:39:53.760
lack the same token size windows that other models have.


00:39:53.760 --> 00:39:59.760
>> Yeah. Interesting. I think just even for ones that have a large token spaces,


00:39:59.760 --> 00:40:03.640
I think the more specific you can be,


00:40:03.640 --> 00:40:08.360
the more likely you're going to get a focused result instead of a wandering,


00:40:08.360 --> 00:40:14.840
vague result. Teach me about math or teach me how to factor,


00:40:14.840 --> 00:40:19.280
how to integrate this difference or solve this series of differential equations.


00:40:19.280 --> 00:40:19.880
>> Right.


00:40:19.880 --> 00:40:23.960
>> Physics. You're going to get a really different answer to those two questions.


00:40:23.960 --> 00:40:32.720
>> Yeah. That's also circles back to a topic I did want to call out,


00:40:32.720 --> 00:40:35.360
but I don't think we hit on it,


00:40:35.360 --> 00:40:40.760
is that the chat UI actually does support rendering in both Markdown and LaTeX,


00:40:40.760 --> 00:40:43.440
which is a markup language for math.


00:40:43.440 --> 00:40:48.680
You can ask it both complex engineering and mathematical questions,


00:40:48.680 --> 00:40:50.720
like asking it to explain you.


00:40:50.720 --> 00:40:53.160
Yeah. There might be a demo here.


00:40:53.160 --> 00:40:55.960
I'm not sure if it's on this page though.


00:40:55.960 --> 00:41:03.080
>> If I had a Fast Fourier Transform in LaTeX,


00:41:03.080 --> 00:41:04.760
and I put it in there and say,


00:41:04.760 --> 00:41:08.280
what is this? It'll say it's a Fast Fourier Transform or something like that?


00:41:08.280 --> 00:41:11.720
>> Yes. It also works the other way around.


00:41:11.720 --> 00:41:13.920
You can also use it to say,


00:41:13.920 --> 00:41:17.120
"Hey, explain to me what the 2D Laplace equation is,"


00:41:17.120 --> 00:41:20.680
or "Explain to me what does this do?"


00:41:20.680 --> 00:41:27.400
It will actually generate and format the equation inside the chat UI,


00:41:27.400 --> 00:41:31.040
which is really remarkable. I love that feature.


00:41:31.040 --> 00:41:32.920
>> It's actually really awesome.


00:41:32.920 --> 00:41:38.000
It's also really appropriate for a scientific-oriented thing like Jupyter.


00:41:38.000 --> 00:41:44.480
>> Yeah. The remarkable thing is that because ChatGPT and other such language models,


00:41:44.480 --> 00:41:47.760
like the ones from Anthropic and AI21,


00:41:47.760 --> 00:41:50.440
because they are founded on the premise


00:41:50.440 --> 00:41:54.720
where their functionality comes from having such a large corpus of data,


00:41:54.720 --> 00:41:57.720
they know a remarkable amount of information.


00:41:57.720 --> 00:42:04.800
We've tried some example notebooks of quantum computing and explained those really well.


00:42:04.800 --> 00:42:10.880
I tried one with the Black-Scholes options pricing model used in financial engineering.


00:42:10.880 --> 00:42:18.000
It's really remarkable, the utility that it offers just by being there in the side panel.


00:42:18.000 --> 00:42:26.040
You essentially have a math wizard available to you in JupyterLab at all times.


00:42:26.040 --> 00:42:29.320
>> It's probably better than a lot of math professors.


00:42:29.320 --> 00:42:32.760
Not necessarily in the depth of one area,


00:42:32.760 --> 00:42:39.000
but if you ask somebody who does abstract algebra about real analysis,


00:42:39.000 --> 00:42:40.600
they're like, "I don't really do that part."


00:42:40.600 --> 00:42:42.920
Or if you ask somebody about real analysis, number three.


00:42:42.920 --> 00:42:45.320
You can hit on all the areas,


00:42:45.320 --> 00:42:48.440
at least a generalist professor thing.


00:42:48.440 --> 00:42:52.400
All right. We talked about the /learn command.


00:42:52.400 --> 00:42:56.640
That's pretty excellent already and where that's going.


00:42:56.640 --> 00:42:59.520
So I'm pretty excited about that.


00:42:59.520 --> 00:43:07.440
>> The /learn command actually does have a lot of interesting technical tidbits to it.


00:43:07.440 --> 00:43:10.680
>> Okay. Actually, this is one of the really challenging things with


00:43:10.680 --> 00:43:15.440
these chat bots and things.


00:43:15.440 --> 00:43:17.840
For example, I've tried to ask ChatGPT,


00:43:17.840 --> 00:43:21.720
if I gave just one of the transcripts from the show,


00:43:21.720 --> 00:43:23.400
I want to have a conversation about it.


00:43:23.400 --> 00:43:25.000
He says, "No, that's too much. I can't do it."


00:43:25.000 --> 00:43:26.000
>> Yeah.


00:43:26.000 --> 00:43:29.040
>> It's just one show.


00:43:29.040 --> 00:43:32.560
In your documentation, there might be a lot of files in there, right?


00:43:32.560 --> 00:43:35.360
More than just one transcript levels worth.


00:43:35.360 --> 00:43:40.760
So that alone, I think it's interesting just how to ingest that much data into it.


00:43:40.760 --> 00:43:47.000
>> Yeah. This is a very interesting subject and it actually is a bit complex.


00:43:47.000 --> 00:43:48.360
>> I'm sure it is.


00:43:48.360 --> 00:43:50.080
>> Yeah. I mean, if we have the time.


00:43:50.080 --> 00:43:53.280
I think there are some other features you want to discuss. I'm not sure.


00:43:53.280 --> 00:43:55.960
>> Let's dive into this for just a minute because I think it is interesting.


00:43:55.960 --> 00:43:58.320
Because this makes it yours, right?


00:43:58.320 --> 00:44:00.280
It's one thing to ask Vagant,


00:44:00.280 --> 00:44:03.920
like tell me about the Laplace equation and how does it apply to heat transfer?


00:44:03.920 --> 00:44:06.920
Like, okay, great. I have a specific problem with


00:44:06.920 --> 00:44:11.280
a specific library and I want to solve it and you don't seem to understand enough of it.


00:44:11.280 --> 00:44:16.920
So it really limits the usefulness if it's not a little closer to what you're actually doing.


00:44:16.920 --> 00:44:19.800
I think this brings it closer. So yeah, tell us about it.


00:44:19.800 --> 00:44:22.280
>> Yeah. So I mean, when you think about it,


00:44:22.280 --> 00:44:27.240
language models aren't just governed by their intelligence.


00:44:27.240 --> 00:44:29.280
However you measure that, right?


00:44:29.280 --> 00:44:32.000
They're also governed by how much context they can take.


00:44:32.000 --> 00:44:35.720
So one of the reasons ChatGPT was so remarkable is that it had


00:44:35.720 --> 00:44:40.640
a great way of managing context through conversation history.


00:44:40.640 --> 00:44:48.360
That seemingly small leap and seemingly small feature is what made


00:44:48.360 --> 00:44:53.480
ChatGPT so remarkably disruptive to this industry,


00:44:53.480 --> 00:44:55.960
is because of that additional context.


00:44:55.960 --> 00:44:57.880
We think about extending that idea,


00:44:57.880 --> 00:44:59.800
like how do we give an AI more context,


00:44:59.800 --> 00:45:03.040
make it even more human-like and personal?


00:45:03.040 --> 00:45:05.160
Well, the idea is similar.


00:45:05.160 --> 00:45:08.680
We add more context and that's what learning does.


00:45:08.680 --> 00:45:12.080
So the way learning works is that we're actually using


00:45:12.080 --> 00:45:15.480
another set of models called embedding models.


00:45:15.480 --> 00:45:17.760
Embedding models are very,


00:45:17.760 --> 00:45:23.200
very underrated in the AI modeling space.


00:45:23.200 --> 00:45:27.560
These are really remarkable things and they have this one,


00:45:27.560 --> 00:45:31.640
I'll only cover the most important characteristic of embedding models,


00:45:31.640 --> 00:45:35.240
which is embedding models take


00:45:35.240 --> 00:45:42.440
syntax and map it to a high-dimensional vector space called a semantic space.


00:45:42.440 --> 00:45:44.520
Inside of the semantic space,


00:45:44.520 --> 00:45:49.960
nearby vectors indicate semantic similarity.


00:45:49.960 --> 00:45:51.880
I know that's a lot of words,


00:45:51.880 --> 00:45:53.560
I'm going to break that idea down.


00:45:53.560 --> 00:45:55.600
So like canine and dog,


00:45:55.600 --> 00:45:57.840
let's take these two words as an example.


00:45:57.840 --> 00:46:00.120
These two words are completely different.


00:46:00.120 --> 00:46:05.760
They don't even share a single character in similarity together.


00:46:05.760 --> 00:46:08.480
They don't have a single letter in common with one another.


00:46:08.480 --> 00:46:14.360
Yet we know as humans that these two words mean the same thing.


00:46:14.360 --> 00:46:16.800
They refer to a dog.


00:46:16.800 --> 00:46:22.240
They have different syntax but the same semantics.


00:46:22.240 --> 00:46:24.440
>> So their vectors would be?


00:46:24.440 --> 00:46:26.040
>> Would be mapped close.


00:46:26.040 --> 00:46:28.280
>> Would be very close by whatever metric you're using.


00:46:28.280 --> 00:46:32.280
>> Yeah. So if you extend this idea and you imagine,


00:46:32.280 --> 00:46:35.240
okay, what if you split a document?


00:46:35.240 --> 00:46:39.040
What if you split a file into one to two sentence chunks?


00:46:39.040 --> 00:46:41.160
Then for each of these sentence,


00:46:41.160 --> 00:46:43.240
let's just say sentences for example.


00:46:43.240 --> 00:46:46.640
Let's say we split a document to sentences and then we take each of


00:46:46.640 --> 00:46:51.280
those sentences and then use an embedding model to compute their embedding,


00:46:51.280 --> 00:46:53.960
and then store them inside of a vector store.


00:46:53.960 --> 00:46:57.320
Basically like a local database that just


00:46:57.320 --> 00:47:00.880
stores all of these vectors in a file or something.


00:47:00.880 --> 00:47:05.200
Now imagine what happens if we then take a prompt,


00:47:05.200 --> 00:47:07.160
like a question that we might have,


00:47:07.160 --> 00:47:09.120
encode that as a embedding,


00:47:09.120 --> 00:47:11.680
and then we say to the vector store,


00:47:11.680 --> 00:47:13.880
for this prompt embedding,


00:47:13.880 --> 00:47:18.200
find me all of the other embeddings are close to this.


00:47:18.200 --> 00:47:23.600
Well, what you've just done in this process is called a semantic search.


00:47:23.600 --> 00:47:27.560
So it's like syntax search except instead of searching based off of


00:47:27.560 --> 00:47:31.240
keywords or tokens or other syntactic traits,


00:47:31.240 --> 00:47:37.040
you are searching based off the actual natural language meaning of the word.


00:47:37.040 --> 00:47:42.440
So this is much more applicable when it comes to natural language prompts and


00:47:42.440 --> 00:47:46.480
natural language corpuses of data.


00:47:46.480 --> 00:47:51.000
Because this is the actual information that's being stored.


00:47:51.000 --> 00:47:52.360
We don't care about the characters,


00:47:52.360 --> 00:47:54.920
we care about the information that they represent.


00:47:54.920 --> 00:47:56.360
>> The essence of it. Yeah.


00:47:56.360 --> 00:47:56.800
>> Yeah.


00:47:56.800 --> 00:48:00.720
>> These vectors are computed by the larger language model?


00:48:00.720 --> 00:48:04.720
>> The vectors, the embeddings are computed by an embedding model,


00:48:04.720 --> 00:48:06.760
and they're actually a separate category of


00:48:06.760 --> 00:48:10.120
model that we have our own special APIs for.


00:48:10.120 --> 00:48:12.320
So in our settings panel,


00:48:12.320 --> 00:48:14.360
you can change the language model,


00:48:14.360 --> 00:48:16.280
and I think we already discussed that.


00:48:16.280 --> 00:48:18.760
But what's interesting is that underneath that,


00:48:18.760 --> 00:48:21.560
you'll also see a section that says embedding model.


00:48:21.560 --> 00:48:24.480
You can change the embedding model to,


00:48:24.480 --> 00:48:30.640
we also offer that same principle of model agnosticism there.


00:48:30.640 --> 00:48:31.480
>> Nice.


00:48:31.480 --> 00:48:32.560
>> Yeah.


00:48:32.560 --> 00:48:35.480
>> Yeah, this is very interesting.


00:48:35.480 --> 00:48:38.320
Let's talk really a bit about the format.


00:48:38.320 --> 00:48:42.360
You said obviously that you can do LaTeX,


00:48:42.360 --> 00:48:44.440
which you say in, you say math, right?


00:48:44.440 --> 00:48:46.120
You tell us, give me math.


00:48:46.120 --> 00:48:47.480
>> Yeah, give me math.


00:48:47.480 --> 00:48:48.640
>> Yeah, it's pretty interesting.


00:48:48.640 --> 00:48:50.120
But you can do images,


00:48:50.120 --> 00:48:53.160
markdown code, HTML, JSON, text.


00:48:53.160 --> 00:48:58.240
There's a lot of different formats you can get the answer back in.


00:48:58.240 --> 00:49:04.080
>> Yeah. So when you use the AI magics,


00:49:04.080 --> 00:49:06.960
we can pass them to a renderer


00:49:06.960 --> 00:49:10.600
first before we show it to output to the user.


00:49:10.600 --> 00:49:13.320
>> Yeah, and with the AI magic,


00:49:13.320 --> 00:49:15.320
the percent percent AI in the cell,


00:49:15.320 --> 00:49:17.680
you can also specify,


00:49:17.680 --> 00:49:19.840
that's where you put the format potentially,


00:49:19.840 --> 00:49:23.320
but you can also specify the model and the service,


00:49:23.320 --> 00:49:25.280
I guess, for the provider.


00:49:25.280 --> 00:49:30.240
>> Yeah. So the IPython magics are basically


00:49:30.240 --> 00:49:31.960
stateless in the sense that you always


00:49:31.960 --> 00:49:34.200
have to specify the model explicitly.


00:49:34.200 --> 00:49:39.520
So they don't operate off the premise that you are using JupyterLab.


00:49:39.520 --> 00:49:42.200
They don't run off the premise that you have JupyterLab


00:49:42.200 --> 00:49:45.480
installed or using the lab extension that we offer.


00:49:45.480 --> 00:49:47.280
So because of that,


00:49:47.280 --> 00:49:50.600
the model is stated explicitly every time,


00:49:50.600 --> 00:49:53.040
and that's by design.


00:49:53.920 --> 00:49:56.760
>> When you sat down,


00:49:56.760 --> 00:50:04.040
what is your Jupyter AI provider set to? What's your favorite?


00:50:04.040 --> 00:50:08.160
>> What's my favorite? As a developer,


00:50:08.160 --> 00:50:14.040
I literally pick a random one to give the most test coverage at all times,


00:50:14.040 --> 00:50:16.280
and that's actually a great way of finding bugs.


00:50:16.280 --> 00:50:19.680
So yeah, I don't have a favorite one.


00:50:19.680 --> 00:50:22.400
My favorite one is the one that works,


00:50:22.400 --> 00:50:25.120
and hopefully, that should be all of them.


00:50:25.120 --> 00:50:29.640
>> Yeah, hopefully. Okay. Interesting.


00:50:29.640 --> 00:50:37.320
Yes, you can also tell it to forget what I was talking about.


00:50:37.320 --> 00:50:38.600
We're going to start over.


00:50:38.600 --> 00:50:41.160
That's pretty interesting that you can do that along the way


00:50:41.160 --> 00:50:43.240
because you maybe had a bunch of conversations,


00:50:43.240 --> 00:50:44.920
and we talked about the benefit of it,


00:50:44.920 --> 00:50:47.720
like knowing the history of that conversation,


00:50:47.720 --> 00:50:50.480
but you're like, "All right, no idea."


00:50:50.480 --> 00:50:52.240
Switching topics.


00:50:52.240 --> 00:50:53.760
>> Yeah.


00:50:53.760 --> 00:50:56.280
>> Yeah.


00:50:56.280 --> 00:50:59.360
>> Yeah. Let's see.


00:50:59.360 --> 00:51:01.800
I think the last one I wanted to talk about


00:51:01.800 --> 00:51:05.640
specifically was interpolating prompts.


00:51:05.640 --> 00:51:12.320
You kind of almost like f-strings where you can put in the prompt text,


00:51:12.320 --> 00:51:13.440
you can put a variable,


00:51:13.440 --> 00:51:18.240
and then other parts of your notebook or program can set that value.


00:51:18.240 --> 00:51:20.040
Tell us about this.


00:51:20.040 --> 00:51:27.320
>> Yeah. Basically, you can define a variable in your IPython kernel.


00:51:27.320 --> 00:51:31.240
That's just dumb, but just like how you define any other variable.


00:51:31.240 --> 00:51:33.760
But what's interesting is that IPython is actually


00:51:33.760 --> 00:51:37.320
aware of the variables that you are defining.


00:51:37.320 --> 00:51:43.880
We can programmatically access that when we implement the magic.


00:51:43.880 --> 00:51:48.800
Basically, if you define any variable at the top-level scope,


00:51:48.800 --> 00:51:51.720
like let's say poet equals Walt Whitman.


00:51:51.720 --> 00:51:54.120
We have a named variable called poet.


00:51:54.120 --> 00:51:57.120
Then you can send a prompt over like,


00:51:57.120 --> 00:52:02.960
"Write a poem in the style of curly braces poet and curly braces."


00:52:02.960 --> 00:52:05.200
When that is run,


00:52:05.200 --> 00:52:08.120
the value of that variable is


00:52:08.120 --> 00:52:12.280
interpolated and substitutes around the curly braces.


00:52:12.280 --> 00:52:13.760
The final prompt becomes,


00:52:13.760 --> 00:52:16.680
"Write a poem in the style of Walt Whitman."


00:52:16.680 --> 00:52:18.280
When that prompt is sent,


00:52:18.280 --> 00:52:22.120
well, you can imagine it generates a poem of Walt Whitman.


00:52:22.120 --> 00:52:26.280
The variable interpolation that we offer in IPython


00:52:26.280 --> 00:52:31.400
is very useful for very quick debugging.


00:52:31.400 --> 00:52:36.600
You can actually reference a cell in the notebook directly.


00:52:36.600 --> 00:52:38.520
I think a lot of people don't know this,


00:52:38.520 --> 00:52:40.480
but in a Jupyter Notebook,


00:52:40.480 --> 00:52:46.400
there's the in and out indicators to the left of each cell.


00:52:46.400 --> 00:52:50.000
Say like in.1, out.1.


00:52:50.000 --> 00:52:55.000
Those are actual variables and you can use them here too.


00:52:55.000 --> 00:52:58.800
You can reference like, "Tell me why this code is failing,


00:52:58.800 --> 00:53:01.680
curly braces in.1."


00:53:01.680 --> 00:53:06.160
You just screen there, just scroll there.


00:53:06.160 --> 00:53:08.440
>> Explain what this does,


00:53:08.440 --> 00:53:16.360
the in bracket 11 or what went wrong in out bracket 11?


00:53:16.360 --> 00:53:18.680
>> Yeah, something like this.


00:53:18.680 --> 00:53:23.400
>> Okay. It's fine long as you don't go and rerun that cell.


00:53:23.400 --> 00:53:24.600
>> Yeah.


00:53:24.600 --> 00:53:28.560
>> But like you said, this is not for long.


00:53:28.560 --> 00:53:32.440
>> Yeah. You can make it


00:53:32.440 --> 00:53:36.760
more independent of the order of execution


00:53:36.760 --> 00:53:43.520
just by assigning whatever content that is to a named variable.


00:53:43.520 --> 00:53:46.480
That way, no matter what order you run them in.


00:53:46.480 --> 00:53:49.640
>> Exactly. People might be thinking when you describe


00:53:49.640 --> 00:53:52.480
this interpolation thing just bracket,


00:53:52.480 --> 00:53:54.560
poem, bracket, curly brace,


00:53:54.560 --> 00:53:57.000
curly brace, you're like,


00:53:57.000 --> 00:53:58.480
"We already have that in Python."


00:53:58.480 --> 00:54:00.600
You just put an F in front of the string and so on.


00:54:00.600 --> 00:54:05.200
But this is in the message that goes to the AI cell magic,


00:54:05.200 --> 00:54:06.760
not straight Python.


00:54:06.760 --> 00:54:08.720
So that's the relevance.


00:54:08.720 --> 00:54:10.680
That's why this is interesting, right?


00:54:10.680 --> 00:54:11.720
>> Yes.


00:54:11.720 --> 00:54:12.360
>> Yeah.


00:54:12.360 --> 00:54:15.200
>> This is going straight there.


00:54:15.200 --> 00:54:19.160
>> Okay. I guess it


00:54:19.160 --> 00:54:21.560
really comes down to the different models that you select.


00:54:21.560 --> 00:54:24.320
So you opt into this a little bit or maybe you need to


00:54:24.320 --> 00:54:28.840
understand it that way but talk to us a bit about privacy.


00:54:28.840 --> 00:54:31.360
If I select something and say,


00:54:31.360 --> 00:54:34.520
"What does this do?" What happens?


00:54:34.520 --> 00:54:36.080
>> So I think like, yeah,


00:54:36.080 --> 00:54:39.320
something important to emphasize here is that whenever you


00:54:39.320 --> 00:54:44.000
use a language model that's hosted by a third party,


00:54:44.000 --> 00:54:46.800
so it's running on their servers.


00:54:46.800 --> 00:54:50.280
Regardless of whether this model is free or not,


00:54:50.280 --> 00:54:55.280
the fact that you're sending data to a third party over the Internet,


00:54:55.280 --> 00:54:59.280
that's where the privacy and security concerns happen.


00:54:59.280 --> 00:55:01.680
That happens whenever you're sending


00:55:01.680 --> 00:55:04.320
data across the wire over the Internet.


00:55:04.320 --> 00:55:08.680
But we have some special safeguards in place here,


00:55:08.680 --> 00:55:11.040
specifically to assuage fears of


00:55:11.040 --> 00:55:17.360
concerns over privacy and security that a lot of open-source users have.


00:55:17.360 --> 00:55:20.200
One of the important ideas here is


00:55:20.200 --> 00:55:24.680
that Jupyter AI is both transparent and traceable.


00:55:24.680 --> 00:55:30.440
So when we send a prompt to a language model,


00:55:30.440 --> 00:55:33.480
that's always captured in the server logs by default.


00:55:33.480 --> 00:55:35.000
So that's always being logged,


00:55:35.000 --> 00:55:37.280
that's always being captured.


00:55:37.280 --> 00:55:40.000
Yeah. So it's always going to be traceable.


00:55:40.000 --> 00:55:41.560
>> There's no secret back channel.


00:55:41.560 --> 00:55:43.760
You tell people this is happening.


00:55:43.760 --> 00:55:46.640
>> Yeah. So if an operator needs to audit,


00:55:46.640 --> 00:55:52.920
"Oh, dang, let me check just to make sure nothing scary was sent over to OpenAI."


00:55:52.920 --> 00:55:57.400
Well, the operator can review the server logs and make sure that


00:55:57.400 --> 00:56:03.160
all usage is compliant with whatever privacy policy their company has.


00:56:03.160 --> 00:56:08.600
Jupyter AI is also exclusively user-driven,


00:56:08.600 --> 00:56:12.840
meaning that we will never by default send


00:56:12.840 --> 00:56:17.400
data to a language model even if you selected one.


00:56:17.400 --> 00:56:20.160
We will never send data to that language model


00:56:20.160 --> 00:56:24.080
until explicit action is done by the user.


00:56:24.080 --> 00:56:26.480
So in this case, clicking the "Send" button,


00:56:26.480 --> 00:56:29.440
clicking "Shift + Enter" and running to sell.


00:56:29.440 --> 00:56:34.840
Nothing is sent to language model or embedding model until that happens.


00:56:34.840 --> 00:56:35.520
>> Cool.


00:56:35.520 --> 00:56:36.360
>> Yeah.


00:56:36.360 --> 00:56:38.800
>> Yeah. I think that's really all you can do.


00:56:38.800 --> 00:56:40.720
That sounds great.


00:56:40.720 --> 00:56:44.000
Because you don't control what happens once it hits OpenAI,


00:56:44.000 --> 00:56:45.680
or Dropbake, or whatever, right?


00:56:45.680 --> 00:56:50.440
>> Yeah. That's why the transparency is so important.


00:56:50.440 --> 00:56:52.720
Oh, I forgot to touch on traceability.


00:56:52.720 --> 00:56:56.480
So with these AI-generated cells,


00:56:56.480 --> 00:56:58.000
so like the alpha cells,


00:56:58.000 --> 00:57:00.000
in the metadata we indicate that


00:57:00.000 --> 00:57:03.160
with the model that was used to generate an alpha cell.


00:57:03.160 --> 00:57:03.920
>> Oh, nice.


00:57:03.920 --> 00:57:05.920
>> If it comes from the Jupyter AI magic.


00:57:05.920 --> 00:57:09.080
So that way it's also traceable not just in the logs,


00:57:09.080 --> 00:57:13.120
but in the actual files metadata itself as well.


00:57:13.120 --> 00:57:16.360
>> That's cool because it'd be really easy to say,


00:57:16.360 --> 00:57:21.440
"Have the cell magic," and then you know how notebooks store their last output.


00:57:21.440 --> 00:57:22.800
If you upload them to GitHub,


00:57:22.800 --> 00:57:27.160
they'll keep their last output unless you say clear all cell outputs.


00:57:27.160 --> 00:57:32.120
>> Well, depending on which model you have selected,


00:57:32.120 --> 00:57:34.520
you might not get the same output,


00:57:34.520 --> 00:57:36.000
not even close to the same output.


00:57:36.000 --> 00:57:37.160
So you might want to know, "Well,


00:57:37.160 --> 00:57:38.520
how did you get that picture?


00:57:38.520 --> 00:57:41.440
Oh, I had a drop it selected and not the other."


00:57:41.440 --> 00:57:43.280
Like that is really nice.


00:57:43.280 --> 00:57:47.000
>> Right. I've actually used the server logs myself


00:57:47.000 --> 00:57:50.160
to debug like prompt templates for instance.


00:57:50.160 --> 00:57:55.000
Because what we show in the logs is the full prompt.


00:57:55.000 --> 00:57:58.120
After applying our template, after applying the edits,


00:57:58.120 --> 00:57:59.800
that's what's actually shown.


00:57:59.800 --> 00:58:07.440
So that's also really helpful for developers who need to debug what's happening.


00:58:07.440 --> 00:58:12.120
>> Yeah, of course. Or if you're a scientist and you're looking for reproducibility,


00:58:12.120 --> 00:58:17.480
I doubt there's much guaranteed reproducibility even across the versions of the same model,


00:58:17.480 --> 00:58:19.720
but at least you are in the same ballpark.


00:58:19.720 --> 00:58:21.880
At least I know what model it came from.


00:58:21.880 --> 00:58:24.160
>> You can set the temperature to zero,


00:58:24.160 --> 00:58:26.800
but it won't generate very fun output.


00:58:26.800 --> 00:58:29.840
But that is a workaround if you truly need a reproducibility.


00:58:29.840 --> 00:58:31.120
>> I suppose, yeah.


00:58:31.120 --> 00:58:36.960
The temperature being the ability to tell it how creative do you want,


00:58:36.960 --> 00:58:40.280
to figure out how focused do you want the answer to be, right?


00:58:40.280 --> 00:58:45.640
>> It's a hyperparameter that basically governs the randomness,


00:58:45.640 --> 00:58:49.520
like how far away from the mean it's willing to deviate.


00:58:49.520 --> 00:58:54.720
Like vaguely describable as creativity.


00:58:54.720 --> 00:58:57.400
>> Yeah, I suppose.


00:58:57.400 --> 00:59:01.840
I guess if you're looking for privacy,


00:59:01.840 --> 00:59:04.400
the GPT for all might be a good option.


00:59:04.400 --> 00:59:05.800
>> Oh, yeah, absolutely.


00:59:05.800 --> 00:59:08.440
>> For that, right? Because that's not going anywhere.


00:59:08.440 --> 00:59:13.040
>> Yeah. However, some of them do have license restrictions,


00:59:13.040 --> 00:59:17.720
and that's also why we have also taken it this low when it comes


00:59:17.720 --> 00:59:20.920
to adding more GPT for all support,


00:59:20.920 --> 00:59:23.920
is because different models are licensed differently,


00:59:23.920 --> 00:59:27.000
and that's another consideration we have to take in mind.


00:59:27.000 --> 00:59:30.600
>> Yeah, of course. You're playing in a crazy space with


00:59:30.600 --> 00:59:34.840
many different companies evolving licenses.


00:59:34.840 --> 00:59:40.120
Yeah. Let's close out with one more thing here, maybe two more things.


00:59:40.120 --> 00:59:46.040
One is, there's a lot of interest in these agents,


00:59:46.040 --> 00:59:47.840
and it sounds, for example,


00:59:47.840 --> 00:59:51.120
your Create a Notebook that does this sort of thing,


00:59:51.120 --> 00:59:52.680
like teach me about Matplotlib,


00:59:52.680 --> 00:59:55.080
is a little bit agent-driven.


00:59:55.080 --> 00:59:59.000
Thinking of like Lang chain and stuff like that,


00:59:59.000 --> 01:00:02.080
or even GPT engineer.


01:00:02.080 --> 01:00:03.200
>> Yeah.


01:00:03.200 --> 01:00:05.560
>> What's the story? Do you have any integrations with that,


01:00:05.560 --> 01:00:07.160
any of those types of things?


01:00:07.160 --> 01:00:09.440
>> We're working on one where


01:00:09.440 --> 01:00:13.720
basically you won't need to use slash commands anymore.


01:00:13.720 --> 01:00:16.920
Again, we're just playing around with this,


01:00:16.920 --> 01:00:19.080
seeing how well it behaves.


01:00:19.080 --> 01:00:21.200
But we are trying to use agents to,


01:00:21.200 --> 01:00:23.600
for example, remove the need to use slash commands.


01:00:23.600 --> 01:00:26.400
When you say generate a notebook,


01:00:26.400 --> 01:00:28.120
it will just generate one.


01:00:28.120 --> 01:00:30.840
You don't have to know the slash command for that.


01:00:30.840 --> 01:00:39.080
However, we don't use agents at the present moment now.


01:00:39.080 --> 01:00:41.280
The reason for that is that they


01:00:41.280 --> 01:00:45.280
are not very model agnostic,


01:00:45.280 --> 01:00:48.040
at least the ones from our research.


01:00:48.040 --> 01:00:49.960
They only work well for


01:00:49.960 --> 01:00:52.520
a specific model and a specific prompt template.


01:00:52.520 --> 01:00:54.920
But beyond that, it's hard.


01:00:54.920 --> 01:00:57.720
>> Nice. All right.


01:00:57.720 --> 01:01:00.720
Last question. Running out of time here.


01:01:00.720 --> 01:01:01.160
>> Yeah.


01:01:01.160 --> 01:01:03.440
>> What's your favorite thing you've done with Jupyter AI?


01:01:03.440 --> 01:01:04.760
Not like a feature,


01:01:04.760 --> 01:01:07.320
but what have you made it do to help you?


01:01:07.320 --> 01:01:10.360
>> Teach Jupyter AI about Jupyter AI.


01:01:10.360 --> 01:01:11.840
That's an easy one.


01:01:11.840 --> 01:01:14.200
>> What did it learn?


01:01:14.200 --> 01:01:16.480
>> Well, learn about itself.


01:01:16.480 --> 01:01:18.480
I was at a conference,


01:01:18.480 --> 01:01:20.120
so this was at Pi Data,


01:01:20.120 --> 01:01:23.440
and I actually got enough questions to the point where I just


01:01:23.440 --> 01:01:25.800
downloaded I had the documentation already


01:01:25.800 --> 01:01:29.000
available in my home directory because of some of the previous.


01:01:29.000 --> 01:01:30.080
>> Like Markdown?


01:01:30.080 --> 01:01:32.240
>> Yeah, the Markdown source.


01:01:32.240 --> 01:01:32.520
>> Okay.


01:01:32.520 --> 01:01:35.920
>> The Markdown source. Then I just had slash learn.


01:01:35.920 --> 01:01:37.280
I just learned that doc,


01:01:37.280 --> 01:01:39.760
and then I just had that laptop on the side and


01:01:39.760 --> 01:01:42.640
told people if you have any questions, try answering that.


01:01:42.640 --> 01:01:43.640
>> Just ask it.


01:01:43.640 --> 01:01:47.200
>> Yeah, in case you don't want to wait in line.


01:01:47.200 --> 01:01:51.080
Yeah, it's pretty remarkable.


01:01:51.080 --> 01:01:54.080
The learn feature by far is definitely my favorite,


01:01:54.080 --> 01:01:57.800
and it's the one I want to spend the most time developing and pushing.


01:01:57.800 --> 01:02:01.440
>> I think there's massive possibility there because to deeply


01:02:01.440 --> 01:02:07.240
understand what you're working on in a multifaceted way.


01:02:07.240 --> 01:02:08.320
What are the documentations?


01:02:08.320 --> 01:02:09.000
What is the code?


01:02:09.000 --> 01:02:10.040
What is the data?


01:02:10.040 --> 01:02:12.960
What is the network topology and servers I can work with?


01:02:12.960 --> 01:02:14.360
All of that kind of stuff,


01:02:14.360 --> 01:02:17.720
or pick your specialty.


01:02:17.720 --> 01:02:21.120
>> Yeah, and that's how we make AI more human.


01:02:21.120 --> 01:02:23.400
>> Yeah, right before it takes over,


01:02:23.400 --> 01:02:24.760
so no problem there.


01:02:24.760 --> 01:02:25.280
>> Oh, yeah.


01:02:25.280 --> 01:02:27.080
>> Just kidding.


01:02:27.080 --> 01:02:28.760
>> Yeah.


01:02:28.760 --> 01:02:31.520
>> All right. Let's wrap it up.


01:02:31.520 --> 01:02:32.680
I think we're out of time, David.


01:02:32.680 --> 01:02:35.240
So final question.


01:02:35.240 --> 01:02:37.720
Some notable Pi PI package,


01:02:37.720 --> 01:02:41.240
maybe something awesome you discovered to help you write Jupyter AI.


01:02:41.240 --> 01:02:44.120
Anything you want to give a shout out to?


01:02:44.120 --> 01:02:48.760
>> So we definitely use LangChain a lot,


01:02:48.760 --> 01:02:52.880
and it has some pretty fantastic integrations,


01:02:52.880 --> 01:02:56.640
and we're actually built on top of LangChain really.


01:02:56.640 --> 01:02:58.600
But also Dask.


01:02:58.600 --> 01:03:00.240
Dask is really nice.


01:03:00.240 --> 01:03:04.160
Dask has some great visualization capabilities for when you're doing


01:03:04.160 --> 01:03:08.640
parallel compute as a great dashboard that's also available in JupyterLab.


01:03:08.640 --> 01:03:14.600
>> The dashboard that shows it running and distributing the work in JupyterLab is amazing.


01:03:14.600 --> 01:03:17.560
>> One of the contributors also reached out once.


01:03:17.560 --> 01:03:21.480
He had heard I was integrating Dask into Jupyter.


01:03:21.480 --> 01:03:24.080
He actually reached out to help us and offer


01:03:24.080 --> 01:03:27.280
direct one-on-one guidance with using Dask.


01:03:27.280 --> 01:03:30.920
Yeah, it's just been a fantastic experience using Dask.


01:03:30.920 --> 01:03:32.760
Yeah, I have no complaints.


01:03:32.760 --> 01:03:36.040
It's just pretty awesome that somebody has


01:03:36.040 --> 01:03:39.840
finally made parallel and distributed compute better in Python.


01:03:39.840 --> 01:03:41.000
>> Yeah, for sure.


01:03:41.000 --> 01:03:41.520
>> Yeah.


01:03:41.520 --> 01:03:43.800
>> Yeah, Dask is cool. Dask is very cool.


01:03:43.800 --> 01:03:45.920
All right. Well, final call to action.


01:03:45.920 --> 01:03:48.280
People want to get started with Jupyter AI, what do you tell them?


01:03:48.280 --> 01:03:51.440
>> Have installed Jupyter AI or Conda.


01:03:51.440 --> 01:03:54.600
Conda install works too. It's on both.


01:03:54.600 --> 01:03:57.760
>> Awesome. Well, really good work.


01:03:57.760 --> 01:03:59.320
This is super interesting,


01:03:59.320 --> 01:04:01.200
and I think a lot of people are going to find value in it.


01:04:01.200 --> 01:04:03.000
So I could see some nice comments in


01:04:03.000 --> 01:04:05.480
the audience that people are excited as well.


01:04:05.480 --> 01:04:07.120
So thanks for being here.


01:04:07.120 --> 01:04:07.800
>> Thank you so much.


01:04:07.800 --> 01:04:08.200
>> Yeah.


01:04:08.200 --> 01:04:08.720
>> Thank you so much.


01:04:08.720 --> 01:04:09.520
>> See you later.


01:04:09.520 --> 01:04:10.520
>> You bet. Bye-bye.

