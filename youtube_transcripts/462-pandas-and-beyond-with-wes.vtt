WEBVTT

00:00:00.001 --> 00:00:04.000
Hey Wes, welcome to DocPythonomy.

00:00:04.000 --> 00:00:06.000
Thanks for having me.

00:00:06.000 --> 00:00:11.000
You know, honestly, I feel like it's been a long time coming having you on the show.

00:00:11.000 --> 00:00:15.000
You've had such a big impact in the Python space, especially the data science side of that space.

00:00:15.000 --> 00:00:19.000
It's high time to have you on the show, so welcome. Good to have you.

00:00:19.000 --> 00:00:28.000
Yeah, it's great to be here. I've been heads down a lot the last N years.

00:00:28.000 --> 00:00:33.000
I actually haven't been, because I think a lot of my work has been more like data infrastructure

00:00:33.000 --> 00:00:42.000
and working at even a lower level than Python, so I haven't been engaging as much directly with the Python community.

00:00:42.000 --> 00:00:50.000
But it's been great to kind of get back more involved and start catching up on all the things that people have been building.

00:00:50.000 --> 00:01:01.000
And yeah, so being at Posit gives me the ability to have more exposure to what's going on in people that are using Python in the real world.

00:01:01.000 --> 00:01:06.000
Yeah, there's a ton of stuff going on at Posit that's super interesting, and we'll talk about some of that.

00:01:06.000 --> 00:01:13.000
You know, sometimes it's just really fun to build and work with people building things, and I'm sure you're enjoying that aspect of it.

00:01:13.000 --> 00:01:15.000
For sure.

00:01:15.000 --> 00:01:21.000
Well, before we dive into Pandas and all the things that you've been working on after that,

00:01:21.000 --> 00:01:26.000
let's just hear a quick bit about yourself for folks who don't know you.

00:01:26.000 --> 00:01:34.000
Sure. So yeah, my name is Wes McKinney. I grew up in Akron, Ohio, mostly, and I got involved,

00:01:34.000 --> 00:01:42.000
started getting involved in Python development around 2007, 2008, and built, I was working in QuantFinance at the time.

00:01:42.000 --> 00:01:48.000
I started building a personal data analysis toolkit that turned into the Pandas project,

00:01:48.000 --> 00:01:53.000
and then open sourced that in 2009, started getting involved in the Python community.

00:01:53.000 --> 00:01:58.000
And I spent several years writing my book, Python for Data Analysis,

00:01:58.000 --> 00:02:07.000
and then working with the broader scientific Python data science community to help enable Python to become a mainstream programming language

00:02:07.000 --> 00:02:11.000
for doing data analysis and data science.

00:02:11.000 --> 00:02:14.000
And in the meantime, I've become an entrepreneur.

00:02:14.000 --> 00:02:28.000
I've started some companies and have been working to innovate and improve the computing infrastructure that powers data science tools and libraries like Pandas.

00:02:28.000 --> 00:02:34.000
So that's led to some other projects like Apache Arrow and Ibis and some other things.

00:02:34.000 --> 00:02:46.000
And yeah, and in recent years, I've been, I've worked on a startup, Voltron Data, which is still, you know, very much going strong and,

00:02:46.000 --> 00:02:51.000
and, you know, has a big team and is off to the races.

00:02:51.000 --> 00:03:03.000
And I've had a long relationship with Posit, formerly RStudio, and they were my home for doing Arrow development from 2018 to 2020.

00:03:03.000 --> 00:03:08.000
They helped me incubate the startup that became Voltron Data.

00:03:08.000 --> 00:03:24.000
And so I've gone back to work full time there as a software architect to help them with their Python strategy to make sort of their data science platform a delight to use for the Python user base.

00:03:24.000 --> 00:03:26.000
I'm pretty impressed with what they're doing.

00:03:26.000 --> 00:03:36.000
I don't realize the connection between Voltron and Posit, but I have had Joe Chung on the show before to talk about Shiny for Python.

00:03:36.000 --> 00:03:43.000
And I've seen him demo a few really interesting things, how it integrates to notebooks these days.

00:03:43.000 --> 00:03:46.000
Some of the stuff that you all are doing and yeah, it's just, it's fascinating.

00:03:46.000 --> 00:03:51.000
Maybe give people a quick elevator pitch on that while we're on that subject.

00:03:51.000 --> 00:03:56.000
On Shiny or on Posit in general?

00:03:56.000 --> 00:03:57.000
Whichever you feel like.

00:03:57.000 --> 00:04:04.000
Yeah, so Posit started out 2009 as RStudio.

00:04:04.000 --> 00:04:07.000
And so it didn't start out intending to be a company.

00:04:07.000 --> 00:04:17.000
JJ Allaire and Joe Chang built a new IDE, integrated development environment for R, because what was available at the time wasn't great.

00:04:17.000 --> 00:04:23.000
And so they made that into, I think, probably one of the best data science IDEs that's ever been built.

00:04:23.000 --> 00:04:25.000
It's really an amazing piece of tech.

00:04:25.000 --> 00:04:33.000
So it started becoming a company with customers and revenue in the 2013 timeframe.

00:04:33.000 --> 00:04:41.000
And they've built a whole suite of tools to support enterprise data science teams to make open source data science work in the real world.

00:04:41.000 --> 00:04:47.000
But the company itself, it's a certified B corporation, has no plans to go public or IPO.

00:04:47.000 --> 00:04:52.000
It is dedicated to the mission of open source software for data science and technical communication.

00:04:52.000 --> 00:05:02.000
And it basically building itself to be a hundred year company that has a revenue generating enterprise product side and an open source side.

00:05:02.000 --> 00:05:07.000
The open source feeds the enterprise part of the business.

00:05:07.000 --> 00:05:11.000
The enterprise part of the business generates revenue to support the open source development.

00:05:11.000 --> 00:05:21.000
And the goal is to be able to sustainably support the mission of open source data science for hopefully the rest of our lives.

00:05:21.000 --> 00:05:24.000
And it's an amazing company.

00:05:24.000 --> 00:05:32.000
It's been one of the most successful companies that dedicates a large fraction of its engineering time to open source software development.

00:05:32.000 --> 00:05:40.000
So it's been very impressed with the company and JJ Allaire, its founder.

00:05:40.000 --> 00:05:51.000
And so I'm excited to be helping it grow and become a sustainable long-term fixture in the ecosystem.

00:05:51.000 --> 00:05:54.000
It's definitely doing cool stuff.

00:05:54.000 --> 00:05:57.000
Incentives are aligned well, right?

00:05:57.000 --> 00:06:01.000
It's not private equity or IPO.

00:06:01.000 --> 00:06:03.000
I think it helps.

00:06:03.000 --> 00:06:13.000
Many people know JJ Allaire created ColdFusion, which is like the original dynamic web development framework in the 1990s.

00:06:13.000 --> 00:06:21.000
And so he and his brother, Jeremy, and some others built Allaire Corp to commercialize ColdFusion.

00:06:21.000 --> 00:06:27.000
And they built a successful software business that was acquired by Macromedia, which was eventually acquired by Adobe.

00:06:27.000 --> 00:06:32.000
But they did go public as Allaire Corp during the dot-com bubble.

00:06:32.000 --> 00:06:36.000
And JJ went on to found a couple of other successful startups.

00:06:36.000 --> 00:06:48.000
And so I think he found himself in his late 30s 15 years ago or around age 40, around the age I am now, having been very successful as an entrepreneur,

00:06:48.000 --> 00:06:55.000
no need to make money and looking for a mission to spend the rest of his career on.

00:06:55.000 --> 00:07:08.000
And identifying data science and statistical computing as an open source, in particular making open source for data science work,

00:07:08.000 --> 00:07:15.000
was the mission that he aligned with and something that he had been interested in earlier in his career, but he had gotten busy with other things.

00:07:15.000 --> 00:07:23.000
So I think it's really refreshing to work with people who are really mission focused and focused on making impact in the world,

00:07:23.000 --> 00:07:31.000
creating great software, empowering people, increasing accessibility, and making most of it available for free on the internet,

00:07:31.000 --> 00:07:43.000
and not being so focused on empire building and producing great profits for venture investors and things like that.

00:07:43.000 --> 00:07:55.000
So I think the goal of the company is to provide an amazing home for top tier software developers to work on this software,

00:07:55.000 --> 00:08:04.000
to spend their careers and to build families and to be a happy and healthy culture for working on this type of software.

00:08:04.000 --> 00:08:10.000
That sounds excellent. Wow. Very cool. I didn't realize the history all the way back to ColdFusion.

00:08:10.000 --> 00:08:19.000
Speaking of history, let's jump in. Wes, there's a possibility that people out there listening don't know what Pandas is.

00:08:19.000 --> 00:08:26.000
You would think it's pretty ubiquitous and I certainly would say that it is, especially in the data science space.

00:08:26.000 --> 00:08:33.000
But I've got a bunch of listeners who listen and they say really surprising things.

00:08:33.000 --> 00:08:39.000
They'll say stuff to me like, Michael, I've been listening for six weeks now and I'm starting to understand some of the stuff you all are talking about.

00:08:39.000 --> 00:08:44.000
I'm like, why did you listen for six weeks? You didn't know what I was talking about. That's crazy.

00:08:44.000 --> 00:08:49.000
And a lot of people use it as language immersion to get into the Python space.

00:08:49.000 --> 00:08:54.000
So I'm sure there's plenty of people out there who are immersing themselves but are pretty new.

00:08:54.000 --> 00:08:58.000
So maybe for that crew, we could introduce what Pandas is then.

00:08:58.000 --> 00:09:05.000
Absolutely. It's a data manipulation and analysis toolkit for Python.

00:09:05.000 --> 00:09:10.000
So it's a Python library that you install that enables you to read data files.

00:09:10.000 --> 00:09:20.000
So read many different types of data files off of disk or off of remote storage or read data out of a database or some other remote data storage system.

00:09:20.000 --> 00:09:24.000
This is tabular data. So it's structured data like with columns.

00:09:24.000 --> 00:09:28.000
You can think of it like a spreadsheet or some other tabular data set.

00:09:28.000 --> 00:09:40.000
And then it provides you with this data frame object, which is kind of pandas.dataframe that is the main tabular data object.

00:09:40.000 --> 00:09:57.000
And it has a ton of methods for accessing, slicing, grabbing subsets of the data, applying functions on it that do filtering and subsetting and selection, as well as like more analytical operations like things that you might do with a database system.

00:09:57.000 --> 00:10:09.000
Or SQL. So joins and lookups, as well as analytical functions like summary statistics, grouping by some key and producing summary statistics.

00:10:09.000 --> 00:10:18.000
So it's basically a Swiss Army knife for doing data manipulation, data cleaning and supporting the data analysis workflow.

00:10:18.000 --> 00:10:32.000
But it doesn't actually include very much as far as actual statistics or models or, you know, if you're doing something with LLMs or linear regression or some type of type of machine learning, you have to use another library.

00:10:32.000 --> 00:10:39.000
But pandas is the on ramp for all of the data into into your environment in Python.

00:10:39.000 --> 00:11:05.000
So when people are building some kind of application that touches data in Python, pandas is often like the initial like on ramp for how data gets into Python, where you clean up the data, you regularize it, you get it ready for analysis, and then you feed the clean data into the downstream, you know, statistical library or data analysis library that you're using.

00:11:05.000 --> 00:11:09.000
That whole data wrangling side of things, right?

00:11:09.000 --> 00:11:11.000
Yeah, that's right. That's right.

00:11:11.000 --> 00:11:36.000
And so, you know, it's some history like it. So, so, Python had arrays like matrices and what we call tensors now multi dimensional arrays going back all the way to 1995, which is pretty, pretty early history for for Python like the Python programming language has only been around since like 1990 or 1991.

00:11:36.000 --> 00:12:05.000
If my memory serves, but the NumPy what became NumPy in 2005 2006 started out as numeric in 1995, and it provided numerical computing multi dimensional arrays matrices, the kind of stuff that you might do might do in MATLAB but it was mainly focused on numerical computing, and not with the type of business data sets that you find in database systems which contain a lot of strings or dates or non numeric data.

00:12:05.000 --> 00:12:28.000
And so my initial interest was I found Python to be a really productive programming language I really liked writing code in it writing simple scripts like, you know, doing random things you know for my for my job but, and you had this numerical computing library NumPy which enabled you to work with large numeric arrays and, and large, you know, data sets with a single data type.

00:12:28.000 --> 00:12:56.000
But working with this more tabular type data stuff that you would do in Excel or stuff that you do in a database. It wasn't very easy to do that with with NumPy or it wasn't really designed for that and so that's what led to building this like higher level library that deals with these tabular data sets in the pandas library which was originally focused on, you know, building really with a really close relationship with NumPy so pandas itself was like a thin layer on top of NumPy originally.

00:12:56.000 --> 00:13:08.000
One thing I find interesting about pandas is it's almost its own programming environment, these days, in the sense that, you know, traditional Python.

00:13:08.000 --> 00:13:29.000
We do a lot of loops, we do a lot of attribute dereferencing function calling, and a lot of what happens in pandas is more, more functional it's more apply to us is only like set operations right and a lot of vector operations and so on.

00:13:29.000 --> 00:13:57.000
Yeah, it's, and that and that was inherited. That was behaviors inherited from from NumPy. So NumPy is very array oriented vector oriented, so you rather than write a for loop you would write a an array expression, which would operate on whole batches of data in a single in a single function call, which is a lot faster because you can drop down into into C code and get good performance that way.

00:13:57.000 --> 00:14:08.000
And so pandas adopted the you know the NumPy way of like the NumPy like array, array expression or, you know, vector, vector operations.

00:14:08.000 --> 00:14:33.000
But it's true that that's extended to the types of like non numeric data operations that you can do in pandas like set you know vectorized set lookups where you can say like, you would say like oh like this I have this array of strings and I have this subset of strings and I want to compute a Boolean array which says whether or not each string is contained in this set of strings and so in pandas that's the is in function.

00:14:33.000 --> 00:14:45.000
So you would say like, you know, column A like is in some set of substrings and that produces that single function call produces a whole Boolean array that you can use for subsetting later on.

00:14:45.000 --> 00:14:56.000
Yeah, there's a ton of things that are really interesting in there. One of the challenges maybe you could speak to this a little bit, and I want to come back to your performance comment.

00:14:56.000 --> 00:15:13.000
One of the challenges I think is that some of its some of these operations are not super obvious that they exist or that they're discoverable right like instead of just indexing into say a column, you can index on an expression that might filter out the columns or project them or things like that.

00:15:13.000 --> 00:15:20.000
How do you recommend people like kind of discover a little bigger breadth of what they can do.

00:15:20.000 --> 00:15:34.000
I mean there's there's plenty of, I mean, I think the, the, but there's great books written about pandas so so there's my book Python for data. So I think Matt Harrison has written an excellent book effective pandas.

00:15:34.000 --> 00:16:02.000
The pandas documentation, I think has provides really nitty gritty detail about how all the different things, things work but, you know, when I, when I was writing my when I was writing this book Python for data analysis my goal was to, you know, provide a primer like a tutorial on how to solve data problems with solve data problems with pandas, and so for that you know I had to introduce how some basics of how NumPy works so people

00:16:02.000 --> 00:16:12.000
array oriented computing basics of Python so you know enough Python to be able to understand what things that pandas is doing.

00:16:12.000 --> 00:16:14.000
But the.

00:16:14.000 --> 00:16:29.000
Yeah, kind of it builds incrementally and so like as you go through the book, the content gets, you know, gets more and more advanced it introduces it built you learn you master initial set of techniques and then you can start learning about more advanced techniques.

00:16:29.000 --> 00:16:40.000
So it's definitely a big logical pedagogical resource, and it is now freely as you as you're showing there on the screen it is freely available on the internet.

00:16:40.000 --> 00:16:54.000
So, JJ Lair helped me port the book to use quarto, which is a new technical publishing system for writing books and blogs and and website you know corto.org.

00:16:54.000 --> 00:17:09.000
And there it is. And yes, so that's how I was able to publish my book on the internet, as you know, essentially, you can use corto to write books using notebooks which is cool.

00:17:09.000 --> 00:17:27.000
My book was written, long time ago and O'Reilly's doc XML so not particularly fun to edit but yeah but because a quarter quarter is built on Pandoc, which is a sort of markup language translation system so you can use Pandoc to convert from one.

00:17:27.000 --> 00:17:45.000
You know you can to convert documents from one format to another.

00:17:45.000 --> 00:18:05.000
And I think that's what I was able to do. And I think that's what I was able to do. And I think that's what I was able to do. And I think that's what I was able to do.

00:18:05.000 --> 00:18:30.000
And I think that's what I was able to do. And I think that's what I was able to do. And I think that's what I was able to do. And I think that's what I was able to do.

00:18:30.000 --> 00:18:47.000
And if you go to the search bar in, if you go back to the book and just look at the search bar, you know, look at just search for like group by like no all one word or, you know, yeah it's like it comes up really fast you can go to that section and it's a, it's, it's pretty cool.

00:18:47.000 --> 00:18:56.000
I thought that releasing the book for free online would would affect sales but now people just really like having paper books, it seems, even in 2024.

00:18:56.000 --> 00:19:08.000
Even digital books are nice you got them with you all the time you can, I think it's about taking the notes, where do I put my highlights and how do I remember it and that's right, yeah, yeah, stuff like that.

00:19:08.000 --> 00:19:21.000
This quarter thing looks super interesting. If you look at Pandoc, if people haven't looked at this before, the conversion matrix.

00:19:21.000 --> 00:19:27.000
I don't know how you would, how would you describe this Wes? Busy? Complete? What is this? This is crazy.

00:19:27.000 --> 00:19:42.000
It's very busy, yeah, it's, it can convert from looks like about, you know, 30 or 40 input formats formats to, you know, 50 or 60 output formats maybe, maybe more than that. It's kind of my just my just eyeballing it but yeah it's like a pretty, pretty impressive.

00:19:42.000 --> 00:20:05.000
And then if you took the combinatorial of like how many different ways can you combine the 30 to the 50. It's kind of what it looks like it's, it's truly amazing. So if you've got Markdown you want to turn it into a PDF or you've got a doku wiki and you want to turn it into an EPUB or whatever right or even like reveal JS, probably to PowerPoint I would imagine I don't know.

00:20:05.000 --> 00:20:29.000
Yeah, yeah. Well, as, as history like backstory about about Quarto, so, you know, it helps to keep in mind that that JJ created cold fusion, which was this, you know, essentially publishing system early publishing system for, for the internet, similar to CGI and and PHP and and other dynamic, you know, web, web publishing systems.

00:20:29.160 --> 00:20:58.960
And so early on at RStudio, they created R Markdown, which is a basically a extensions to Markdown that allow you to have code cells written in R, and then eventually they added support for some other languages where it's kind of like a Jupyter notebook in the sense you could have some Markdown and some code and plots and output and you would run the R Markdown renderer and it would, it would, it would, you know, generate all the output and insert it into the, into the rendering.

00:20:58.960 --> 00:21:12.960
into the document. And so you could use that to write blogs and websites and everything. But, but our Markdown was written in R and so that that limited in a sense like it made it harder to install because you would have to install R to use it.

00:21:12.960 --> 00:21:35.960
And also people, you know, it, it, it, an association with R that perhaps was like, like unmerited. And so in the meantime, you know, with all everything that's happened with web technology, there's, it's, it's now very easy to put a complete JavaScript engine in a small install footprint, you know, on a machine with no dependencies.

00:21:35.960 --> 00:22:04.960
And to be able to run a system that is, you know, written in a system that's written in JavaScript and so Quarto is completely language agnostic it's written in TypeScript, and it uses Pandoc as an as an underlying engine, and it's very easy to install and so it addresses some of the portability and, and, you know, extensive that were that were in R Markdown but but as a result, you know, I think are the

00:22:04.960 --> 00:22:31.960
the posit team had a lot of just has more than a, you know, a decade or if you include cold fusion, you know, more than 25 years of experience and in building really developer friendly technical publishing tools and so I think that that it's, it's not data science but it's something that is an important part of the data science workflow which is, how do you present your make your analysis and your work available for consumption and different forms.

00:22:31.960 --> 00:22:41.960
And so, having this system that can, you know, publish outputs in, in many different places is, is, is super valuable.

00:22:41.960 --> 00:22:56.960
So a lot of people start out in Jupyter notebooks but, you know, but there's many different, you know, many different possible input formats. And so to be able to, you know, use the same source to publish to a website or to a confluence page or to a PDF is like super valuable.

00:22:56.960 --> 00:23:13.960
Yeah, it's super interesting. Okay, so then I got to explore some more. Let's go back to pandas for a minute. First, how about some kind words from the audience for you. Ravi says West your work has changed my life. It's very, very nice.

00:23:13.960 --> 00:23:32.960
Happy to hear I'm happy to hear it. but yeah I yeah I'm more than happy to.

00:23:32.960 --> 00:23:52.960
Yeah, let's talk. Let's talk derivatives for a minute. So, growth and speed of adoption and all those things are. When you first started working on this and you first put it down, did you foresee a world where this was so popular and so important.

00:23:52.960 --> 00:24:19.960
Did you think of yeah pretty soon black holes, I'm pretty sure I'll be part of that somehow. I mean it was, it was always, it was always the aspiration, like there were there was the ask, there was the aspiration there of making, making Python, this mainstream, mainstream language for statistical computing and data analysis.

00:24:19.960 --> 00:24:23.960
Like I didn't.

00:24:23.960 --> 00:24:42.960
I didn't occur to me that it would become this popular, or that it would become like the one of the main tools that people use for working with data and a business in a business setting like that, you know, it would have been like if that was the if that was the aspiration or if that was, you know what I needed to achieve to be satisfied.

00:24:42.960 --> 00:24:52.960
That that would have been completely unreasonable. And I also don't think that that, you know, in a certain sense like I don't I don't know that that it's popularity.

00:24:52.960 --> 00:25:07.960
But, you know, it is deserved and it's served like I think there's there's many other worthy efforts that have been created over the years that have been the, you know, really, you know, really great work that that others have have done in this, this domain.

00:25:07.960 --> 00:25:26.960
And so the fact that that pandas caught on and became as popular as it is.

00:25:26.960 --> 00:25:39.960
And then the serendipitous open source developer community that came together on the project to, you know, grow and expand like really rapidly and in the early 2010s.

00:25:39.960 --> 00:25:55.960
And, you know, and I yeah I definitely spent a lot of work like recruiting people to work on the project and encouraging you know others to work on it because sometimes people create open source projects and then it's hard for hard for others to get involved and get a seat at the table, so to speak.

00:25:55.960 --> 00:26:07.960
But I was very keen to bring on others and to give them responsibility and you know ultimately, you know, hand over the reins to the to the project to others.

00:26:07.960 --> 00:26:24.960
And I've spoken a lot about that, you know, over the years how important that is to, you know, for open source project creators to room for others in in growing the project so that they can become, you know, owner owners of it as well.

00:26:24.960 --> 00:26:45.960
It's tough to make space and tough to bring on folks. Have you heard of the Djangonauts? Djangonauts, I think it's django knots dot space they have an awesome domain, but it's basically like kind of like a boot camp but it's for taking people who just like Django and turn them into actually contributors or core contributors.

00:26:45.960 --> 00:26:53.960
Do you guys have, what's your onboarding story for people who do want to participate?

00:26:53.960 --> 00:27:07.960
I'm embarrassed to say that I'm not, I'm not, I don't have a comprehensive view of like all of the, the different, you know, community outreach channels that the pandas project has done to help grow new contributors.

00:27:07.960 --> 00:27:36.960
So one of the core team members, Mark Garcia has done an amazing job organizing documentation sprints and other like contributor sourcing events, essentially creating very, very friendly accessible events where people who are interested in getting involved in pandas can meet each other, and then assist each other in making their first request and it could be something as simple as, you know, making a small improvement.

00:27:36.960 --> 00:27:52.960
To the, to the pandas documentation, because it's such a large project. The, the documentation is like always something that could be that could be made better, you know, either adding more adding more examples or documenting things that aren't documented, or making.

00:27:52.960 --> 00:28:21.960
Yeah, just just making the documentation better. And so it's something that for new contributors is more accessible than working on the internals of like one of the, you know, algorithms or something, and, and, or like we working on some significant performance improvement might be a bit intimidating if you've never worked on the pandas code base and it's a pretty large code base because it's been it's been worked on continuously for, you know, for like going on 20 years so it's, yeah, it can be takes a while to really get to the point where you can actually do that.

00:28:21.960 --> 00:28:31.960
It's a while to really get to a place where you can be productive, and that can be discouraging for new contributors, especially those who don't have a lot of open source experience.

00:28:31.960 --> 00:28:47.960
That's one of the ironies of challenges of these big projects is they're just so finely polished. So many people are using them. Every edge case matters to somebody. Right. And so to become a contributor and make changes to that, it takes a while I'm sure.

00:28:47.960 --> 00:29:11.960
Yeah, yeah. I mean I think it's, it's definitely a big, I think, definitely a big thing that helped is allowing people to get paid to work on pandas or to be able to contribute to pandas as, as a part of their job description, like, as you know, maybe part of their job is maintaining, maintaining pandas so Anaconda.

00:29:11.960 --> 00:29:39.960
It was like, you know, one of the earliest companies who had engineers on staff, you know, like, yeah, like, you know, Brock Mendel, Tom Augsberger, Jeff Reback who part of their job was maintaining and developing, developing pandas and that was, that was huge because prior to that the project was purely based on volunteers like I, I was a volunteer and everyone was working on the project as a, as a passion project in their, in their free time.

00:29:39.960 --> 00:29:46.960
And then, you know, Travis Oliphant, one of the founders, he and Peter Wang founded Anaconda.

00:29:46.960 --> 00:30:07.960
Travis spun out from Anaconda to create Quonsight and has continued to sponsor, sponsor development in and around, in and around pandas and that's enabled people like Mark to do these community building events and, and for it to not be, you know, something that's, you know, totally, totally uncompensated.

00:30:07.960 --> 00:30:10.960
Yeah, that's a lot of, a lot of stuff going on.

00:30:10.960 --> 00:30:13.960
And I think, I mean, it's, yeah, interest is awesome, right?

00:30:13.960 --> 00:30:32.960
I mean, if there's just a different level of problems, I feel like you can, you could take on, you know, you know what, I got this entire week and someone, that's my job is to make this work rather than I've, I've got two hours and can't really take on a huge project and so I'll work on the smaller improvements or whatever.

00:30:32.960 --> 00:30:44.960
Yeah, but I think, and I think many people know, but I haven't been involved day to day in pandas since 2013. So that's, that's getting on, that's, that's a lot of, that's a lot of years.

00:30:44.960 --> 00:30:58.960
You know, I still, you know, I still talk to the pandas contributors. We had a, we had a pandas meetup, core developer meetup here in Nashville, pre-COVID, I think it was in 2019, maybe.

00:30:58.960 --> 00:31:10.960
So, you know, I'm still in active contact with the pandas developers, but it's, it's been a, you know, a different team of people leading the project. It's taken on a life of its own, which is, which is amazing.

00:31:10.960 --> 00:31:29.960
That's exactly, that's, that's as an open, yeah, as a project creator, that's exactly what you want is to not be beholden to the project that you created and forced and, you know, kind of, you know, have to, you know, be responsible for it and take care of it for the rest of the rest of your life.

00:31:29.960 --> 00:31:43.960
But if you look at like a lot of the community, a lot of the most kind of intensive community development has happened since, like, since I moved on to work on, on other projects. And so now the project has, I don't know the exact count, but it's had thousands of contributors.

00:31:43.960 --> 00:32:11.960
And so, you know, to have thousands of different unique individuals contributing to an open source project is, it's, it's a big, it's a big deal. So I think even, I don't know what it says on the bottom of, on the bottom of GitHub, it says, you know, 30, 3200 contributors, but that's maybe not even the full story because sometimes, you know, people, you know, they don't have their email address associated with their GitHub profile and, you know, how GitHub counts contributors.

00:32:11.960 --> 00:32:27.960
I would say probably the true number is closer to 4000. So that's, yeah, but that's, I think that's a testament, you know, to the, to the core team and all the outreach they've done and work making, making the project accessible and easy to contribute to.

00:32:27.960 --> 00:32:41.960
Because if people, if you go and try to make a pull request to a project, and there's many different ways that you can fail. So like, either the project is technically like there's issues with the build system or the developer tooling.

00:32:41.960 --> 00:33:05.960
And so you struggle with the developer tooling. And so if you aren't working on it every day and every night, you can't make heads or tails of how the developer tools work. But then there's also like the level of accessibility of the core development team, like if they don't, if they aren't there to support you in getting involved in the project and learning how it works, and creating documentation, tribute and what's expected of you.

00:33:05.960 --> 00:33:29.960
That can also be, you know, a source of frustration where people churn out of the project, you know, because it's just, it's too hard to find their sea legs. And maybe, and maybe also, you know, sometimes development teams are unfriendly or unhelpful, or, you know, they make, they make others feel like they make others feel like they're annoyed with them or like they're wasting their time or something.

00:33:29.960 --> 00:33:51.960
It's like, I don't want to look at your, you know, this pull request and give you feedback because, you know, I could do it more quickly by myself or something. Like sometimes you see that an open source projects. But they've, they've created a very, like they've, they've created a very welcoming, very welcoming environment. And yeah, I think the contribution numbers speak for themselves.

00:33:51.960 --> 00:34:09.960
They definitely do. Maybe the last thing before we move on to the other stuff you're working on, but the other interesting GitHub statistic here is the used by 1.6 million projects. That's, I don't know if I've ever seen it used by that high. There's probably some that are higher, but not many.

00:34:09.960 --> 00:34:33.960
Yeah, it's, it's, it's a lot of projects. I think it's, it's, it's, it's interesting. I think it's, it's like many projects, it's reached a point where it's, it's a, it's an essential and assumed part of the, of many, many people's toolkit.

00:34:33.960 --> 00:35:02.960
Like they, like the first thing that they write at the top of a file that they're working on is, you know, import, import pandas as PD or import, import NumPy as PD. And, and so it's, yeah, I think, you know, to create, I think in a sense, like, I think one of the reasons why, you know, pandas has gotten so popular is that it is beneficial to, it is beneficial to the community, to the Python community to have, you know,

00:35:02.960 --> 00:35:18.960
fewer solutions, you know, kind of the Zen of Python, there should be one and preferably only one obvious, obvious way to do things. And so if there were 10 different pandas like projects, you know, that creates skill portability problems.

00:35:18.960 --> 00:35:46.960
And, you know, it's just easier if everyone says, Oh, we just pandas is the thing that we use, change jobs, and you can take all your skills like how to use pandas with you. And I think that's also one of the reasons why Python has become so successful in the business world is because you can teach somebody even without a lot of programming experience, how to use Python, how to use pandas and become productive doing, doing, you know, basic work very, very quickly.

00:35:46.960 --> 00:36:13.960
And so, you know, one of the solutions I remember, back in the early 2010s, there were a lot of articles and talks about how to address the data science shortage. And my my belief and I gave a I gave a talk at Web Summit in Dublin in 2000. Gosh, maybe 2017 2008 I have to look exactly.

00:36:13.960 --> 00:36:30.960
Basically, it was the data science, data scientist shortage. And I and my thesis was always it we should make it easier to be a data scientist or like, like lower the bar for like what sort of skills you have to master before you can you can do productive work in a in a business setting.

00:36:30.960 --> 00:36:52.960
And so I think the fact that that there is just pandas and that's like the one thing that people have to have to learn how to use is like their essential like starting point for doing any data has also led to this piling on of like people being motivated to make this one thing better, because it you know you make improvements to pandas and they benefit millions of projects and millions of people around the world.

00:36:52.960 --> 00:36:56.960
And that's, yeah, so it's like a, you know, steady snowballing effect.

00:36:58.960 --> 00:37:06.960
I agree with you. And I think that I think doing data science is getting easier. We've got a lot of interesting frameworks and tools.

00:37:06.960 --> 00:37:14.960
Yeah, for Python, one of them, right, that makes it easier to share and run, run your code, you know?

00:37:15.960 --> 00:37:44.960
Yeah, so yeah, I think the shiny, shiny for Python streamlit, you know, dash like these different interactive data application publishing framework so you can go from, you know, a few few lines of pandas code loading some data and doing some analysis and visualization to changing that as an interactive, interactive website, without having to know how to use any web development frameworks or node j s or anything like that.

00:37:44.960 --> 00:37:54.960
And so to be able to get up and running and build a working, you know, interactive web application that's powered by powered by Python.

00:37:54.960 --> 00:38:01.960
Yeah, it's it's it's a game changer in terms of, you know, shortening end to end development life cycles.

00:38:02.960 --> 00:38:18.960
What do you think about a Jupyter light? And these pioxidide and basically Jupyter, Jupyter in a browser? Yeah, of things. Yeah. WebAssembly and all that.

00:38:18.960 --> 00:38:31.960
Yeah, so I definitely very excited about have been following WebAssembly in general. And so I guess some people listening will know about know about WebAssembly.

00:38:31.960 --> 00:38:53.960
But But basically, it's a portable machine code that can be compiled and executed in within your browser and a sandbox environment. So it protects against security issues and allows prevents like the person who wrote the WebAssembly code from doing something malicious on your on your machine, which is very important.

00:38:54.960 --> 00:39:16.960
And won't necessarily stop them from like, you know, mining cryptocurrency while you have the browser tab open. That's a whole separate problem. But, but yeah, I think it's, it's enabled us to, you know, run the whole scientific Python stack, including Jupyter and NumPy and pandas totally in the browser without, you know, having a client and server and needing to, you know, run a container someplace in the cloud.

00:39:17.960 --> 00:39:42.960
And so I think in terms of creating application deployment, so like being able to deploy an interactive data application, like with shiny, for example, without needing to have a server, it's actually it's actually pretty amazing. And so I think that that, you know, simplifies and opens it opens up new new new use cases like new, you know, application architectures and make that makes things a lot for

00:39:43.960 --> 00:39:54.960
Because setting up and running a server creates brittleness, like it has cost. And so if the if the browser is doubling as your server process, like that's, I think that's really cool.

00:39:55.960 --> 00:40:19.960
And you also have like other projects like duck DB, which is a, you know, high performance, embeddable analytic SQL engine. And so you know, now with duck DB compiled to wasm, you can get a, you know, high performance database running in your browser. And so you can get low latency interactive queries and interactive dashboards and, and so it's, yeah, there's

00:40:20.960 --> 00:40:29.960
It's, it's, it's WebAssembly has opened up this whole kind of new world of possibilities. And it's so it's, it's, it's transformative, I think.

00:40:30.960 --> 00:40:51.960
I think for Python, in particular, you mentioned Pyodide, which is kind of a whole package stack. So it's, it's like a framework for build and build and packaging and basically building an application and managing its dependencies. So you could create a web WebAssembly version of your application to be deployed like this.

00:40:52.960 --> 00:41:13.960
But yeah, so I think, I think the one of the Pyodide, either the Pyodide main creator or maintainer went to Anaconda, they create a PyScript, which made, which is another attempt to make it even easier to use Python to, to make it even easier to use Python to create web applications, interactive web applications.

00:41:14.960 --> 00:41:28.960
And, yeah, so there's, there's so many cool things here, like in the R community, they have WebR, which is, you know, similar to PyScript and Pyodide in some ways, like compiling the whole R stack to WebAssembly. There was just an article I saw on Hacker News where

00:41:29.960 --> 00:41:52.960
somebody worked on, you know, figuring out how to get, how to trick LLVM into compiling Fortran code, like legacy Fortran code to WebAssembly, because when you're talking about all this, you know, scientific computing stack, you need the linear algebra and all of the, you know, 40 years of Fortran code that have been built to support scientific applications, like you need all that to compile to and run in the browser. So

00:41:54.960 --> 00:42:02.960
yeah, that's pretty wild to think of putting that in there, but very useful. I didn't realize that you could use DuckDB as a WebAssembly component. That's pretty cool.

00:42:03.960 --> 00:42:32.960
Yeah, like there's, there's a company, I'm not an investor or plugging them or anything, but it's called evidence.dev. And they, they, it's like a whole beat like business intelligence, open source business intelligence application that's powered by, powered by DuckDB. And so if you have data that that fits in the fits in the browser, you know, to have a whole, like interactive dashboard, or to be able to do business intelligence, like fully, like fully in the browser with no need of a, no need of a server.

00:42:33.960 --> 00:43:01.960
It's, yeah, it's, it's, it's, it's very, very cool. So I'm, yeah, I've been following DuckDB since the early days and, you know, my company, Voltron Data, like, you know, we, we became members of the DuckDB foundation and built, you know, build, actively build a relationship with, with DuckDB Labs, so we could help accelerate progress in this space, because I think the impact, the impact is so,

00:43:02.960 --> 00:43:28.960
is so immense. And we just, we, it's hard to predict, like what, you know, what people are going to build, build with all this stuff. And, and so that was all, you know, with, I guess, going back, you know, 15 years ago to Python, like one of the reasons I became so passionate about building stuff for Python was about, in, I think, the way that Peter Wang puts that, it puts it is, you know, giving people superpowers.

00:43:28.960 --> 00:43:57.960
So we want to enable people to build things with much less code and much less time. And so by making it, things that much more accessible, that much easier to do, like, in the mantra in Pandas was like, how do we make things, you know, one line of code, or like, this must be easy. It's like, one line of code, one line of code, it must be like, like, make this as terse and simple as possible so that you can move on and focus on building the more interesting parts of your application rather than, you know,

00:43:57.960 --> 00:44:10.960
struggling with how to read a CSV file or, you know, how to do, you know, whichever, whichever, you know, data munging technique that you need for your for your data set.

00:44:10.960 --> 00:44:20.960
Maybe an interesting mental model for DuckDB is kind of an equivalent to SQLite, but more analytics database for folks, you know, in process. Yeah, things right. What do you think?

00:44:20.960 --> 00:44:42.960
Yeah, so yeah, DuckDB is like SQLite. And in fact, it can run the whole SQLite test suite, I believe. So it's a full database, but it's a, it's a, it's for analytic processing. So it's optimized for analytic processing. And as compared, you know, with SQLite, which is not, it's not designed for for data processing.

00:44:42.960 --> 00:44:56.960
Yeah, cool. All right. Well, let's talk about some things that you're working on beyond pandas. You talked about Apache Arrow earlier. What's, what are you doing with the arrow? And how's it fit in your world?

00:44:56.960 --> 00:45:13.960
Yeah, so the, the backstory there was, I don't know if you can hear the sirens in, in downtown Nashville, but...

00:45:13.960 --> 00:45:33.960
Yeah, so in, like around the mid, like the mid 2010s, 2015, I started working, I started working at Cloudera, like in the, which is a company that was like one of the pioneers in the big data ecosystem.

00:45:33.960 --> 00:45:45.960
And I had, you know, been spent several years working on five, five years, five, six years working on pandas. And so I had gone through the experience of building pandas from top to bottom.

00:45:45.960 --> 00:45:58.960
And it was this, you know, full stack system that had had its own, you know, mini query engine, all of its own algorithms and data structures and all the stuff that we had to build from, build from scratch.

00:45:58.960 --> 00:46:18.960
And I started thinking about, you know, what if it was possible to build some of the underlying computing technology, like data readers, like file readers, all the algorithms that power the core components of pandas, like group operations, aggregations, filtering, selection, all those things.

00:46:18.960 --> 00:46:42.960
Like, what if it were possible to have a general purpose library that isn't specific to Python, but is really, really fast, really efficient, and has a large community building, building it so that you could, you know, take that code with you and use it to build many different types of libraries, not just data frame libraries, but also database engines and, you know, stream processing engines and all kinds of things.

00:46:42.960 --> 00:47:04.960
And so that was kind of what was in my mind when I started getting interested in what turned into Arrow. And one of the problems we realized we needed to solve, this was like a group of other open source developers and me, was that we needed to create a way to represent data that was not tied to a specific programming language.

00:47:04.960 --> 00:47:19.960
And that could be used for a very efficient interchange between components. And the idea is that you would have this immutable, like this, this kind of constant data structure, which is like it's the same in every programming language.

00:47:19.960 --> 00:47:29.960
And then you can use that as the basis for writing all of your algorithms. So as long as it's Arrow, you have these reusable algorithms that process, process Arrow data.

00:47:29.960 --> 00:47:45.960
So we started with building the Arrow format and standardizing it. And then we've built a whole ecosystem of components, like library components and different programming languages for building applications that use the, use the Arrow format.

00:47:45.960 --> 00:48:04.960
So that includes, you know, not only tools for building and interacting with the data, but also file readers. So you can read CSV files and JSON data and Parquet files, read data out of database systems, you know, wherever the data comes from, we want to have an efficient way to get it into the Arrow format.

00:48:04.960 --> 00:48:21.960
And then we moved on to building data processing engines that are native to the Arrow format, so that Arrow goes in, the data is processed, Arrow goes out. So DuckDB, for example, supports Arrow as a preferred input format.

00:48:21.960 --> 00:48:36.960
And is, DuckDB is more or less Arrow-like in its internals. It has kind of Arrow, Arrow format plus a number of extensions that are DuckDB, DuckDB specific for better performance within the context of, of DuckDB.

00:48:36.960 --> 00:48:48.960
And so, so in numerous communities, so there's the Rust community, which has built Data Fusion, which is an execution engine for Arrow, SQL engine for Arrow.

00:48:48.960 --> 00:49:06.960
And so yeah, we've kind of like looked at the different layers of the stack, like data access, computing, data transport, you know, everything under the sun, and then we've built libraries that are across many different programming languages, so that, that are, you can pick and choose the pieces that you need to build your, build your system.

00:49:06.960 --> 00:49:24.960
And the goal ultimately, was that we, in the future, which is now, we don't want people to have to reinvent the wheel whenever they're building something like Pandas, that they could just pick up these off the shelf components, they can design the developer experience, the user experience that they want to create.

00:49:24.960 --> 00:49:43.960
And, and they can get built, you know, so if you were building Pandas now, you could build a Pandas-like library based on the Arrow components in much less time, and it would be, it would be fast and efficient and interoperable with the whole ecosystem of other projects that use Arrow.

00:49:43.960 --> 00:49:59.960
So it's, it's, it's very cool. It's, I mean, it was really ambitious, in some ways, obvious to people, they would, they would hear about Arrow, and they say, that sounds obvious, like, clearly, we should have a universal way of transporting data between systems and processing it in memory.

00:49:59.960 --> 00:50:13.960
Why hasn't this been done in the past? And it turns out that, as is true, many open source software problems that, that many of these problems are, the social problems are harder than the technical problems.

00:50:13.960 --> 00:50:40.960
And so, if you can solve the kind of people coordination and consensus problems, solving the technical issues is much, much easier by comparison. So I think we were lucky in that we found like the right, you know, the right group of people, the right personalities where we were able to, you know, as soon as I met, I met Jacques Nadeau, who had been at MapR, and we was working on his startup, Dremio.

00:50:40.960 --> 00:50:58.960
Like, I knew instantly when I met Jacques Nadeau, I was like, I can work, he's like, he's like him, like, he's gonna help me make this happen. I met Julien Ledam, who had also co-created Parquet, I was like, yes, like, we are going to make this like, like, we like, I found the right people, like, we are, we are going to make this happen.

00:50:58.960 --> 00:51:24.960
So it's been a, it's been a labor of love and much, much work and stress and everything. But it's, yeah, that's, you know, I've been working on things circling, you know, with Aero as the sun, you know, I've been building kind of satellites and moons and planets, circling the Aero sun over the last, over the last eight years or so. And that's kept me pretty busy.

00:51:24.960 --> 00:51:47.960
Yeah, it's only getting more exciting and interesting. Over here, it says, it uses efficient analytic operations on modern hardware like CPUs and GPUs. One of the big challenges of Python has been the GIL, also one of its big benefits, but one of its challenges when you get to multicore computational stuff has been the GIL. What's the story here?

00:51:47.960 --> 00:52:16.960
Yeah, so in Aero land, when we're talking about analytic efficiency, it, it mainly has to do with the like underlying like how the how modern CPU works, or how, how, how a GPU works. And so when the data is arranged in column oriented format, that enables the, the data to be moved efficiently through the CPU.

00:52:16.960 --> 00:52:38.960
The CPU cache pipelines, so the data is made, made available efficiently to the, to the CPU cores. And so we spent a lot of energy in Aero, making decisions, firstly, to enable very cache of like CPU cache or GPU cache efficient analytics on, on the data.

00:52:38.960 --> 00:52:47.960
And so, so we were kind of always when we were deciding, we would break ties and make decisions based on like what's going to be more efficient for the for the computer chip.

00:52:47.960 --> 00:53:02.960
The other thing is that modern.

00:53:02.960 --> 00:53:09.840
CPUs but in CPUs they've focused on adding what are called single instruction multiple data

00:53:09.840 --> 00:53:16.960
intrinsic. Like built-in operations in the processor where you know now you can process up to

00:53:16.960 --> 00:53:25.440
512 bytes of data in a single CPU instruction. And so that's like you know my brain's doing the math

00:53:25.440 --> 00:53:31.280
right like 16 32-bit floats or you know eight 64-bit integers in a single

00:53:31.280 --> 00:53:36.400
uh cycle. There's like intrinsic operations. Instead of multiply this number by that one

00:53:36.400 --> 00:53:41.120
multiply that number to these eight things all at once something like that. That's right yeah or you

00:53:41.120 --> 00:53:48.800
might say like oh I have a bit mask and I want to select I want to gather like the the one bits

00:53:48.800 --> 00:53:54.640
that are set in this you know this bit mask from this array of of integers. And so there's like a

00:53:54.640 --> 00:54:04.000
gather instruction which allows you to select a subset a sort of a SIMD vector of integers you

00:54:04.000 --> 00:54:08.720
know using a bit mask. And so that you know turns out to be like a pretty you know pretty critical

00:54:08.720 --> 00:54:15.040
operation in certain certain data analytic workloads. So yeah we were really you know we

00:54:15.040 --> 00:54:21.280
wanted to have a data format that was essentially you know future-proofed in the sense that it's

00:54:21.680 --> 00:54:28.640
it's ideal for the coming wave like current generation of CPUs but also given that a lot

00:54:28.640 --> 00:54:36.560
of processing is moving to GPUs and to FPGAs and and to custom silicon. Like we wanted Arrow to be

00:54:36.560 --> 00:54:42.960
usable there as well. And it's Arrow's been successfully you know used as the foundation

00:54:42.960 --> 00:54:49.920
of GPU computing libraries like we you know kind of at Voltron Data built we've built a whole

00:54:50.480 --> 00:54:58.000
accelerator native GPU native you know scalable execution engine that's Arrow-based.

00:54:58.000 --> 00:55:02.560
And so I think the fact that we that was our aspiration and we've been able to to prove that

00:55:02.560 --> 00:55:07.520
out in in real world workloads and show the kinds of efficiency gains that you can get with

00:55:07.520 --> 00:55:13.840
using modern computing hardware correctly or at least as well as it's intended to be used.

00:55:14.800 --> 00:55:20.320
That's that's a big deal in terms of like making applications faster, reducing the carbon footprint

00:55:20.320 --> 00:55:26.160
of large-scale data workloads, things like that. Yeah amazing. All right let's see what else have

00:55:26.160 --> 00:55:34.240
I got on deck here to talk to you about. Let's maybe we could talk about we got a little time

00:55:34.240 --> 00:55:38.960
left we got a yeah we can talk about this. Yeah we could we could probably spend another hour

00:55:38.960 --> 00:55:44.720
talking. Yes easy. I think I think one of the more interesting areas in in recent years has been

00:55:44.720 --> 00:55:51.920
new DataFrame libraries and DataFrame APIs that that transpile or compile to different

00:55:51.920 --> 00:55:58.080
execute on different backends. And so around the time that that I was helping start Arrow

00:55:58.080 --> 00:56:03.760
I created this project called Ibis which is basically a portable DataFrame API that knows

00:56:03.760 --> 00:56:12.080
how to generate SQL queries and compile to pandas and pollers and different DataFrame DataFrame

00:56:12.080 --> 00:56:19.360
backends. And the goal is to provide a really productive DataFrame API that gives you portability

00:56:19.360 --> 00:56:25.280
across different execution backends with the goal of enabling what we call the multi-engine

00:56:25.280 --> 00:56:30.480
data stack. So you aren't stuck with using one particular system because all of the code that

00:56:30.480 --> 00:56:36.480
you've written is specialized to that system. You have this tool which so maybe you could work with

00:56:36.480 --> 00:56:42.880
you know DuckDB on your laptop or pandas or pollers with Ibis on your laptop. But if you have

00:56:42.880 --> 00:56:47.280
if you need to run that workload someplace else maybe with you know Clickhouse or BigQuery

00:56:47.280 --> 00:56:52.720
or maybe it's a large big data workload that's too big to fit on your laptop and you need to use

00:56:52.720 --> 00:57:00.000
Spark SQL or something that you can just ask Ibis say hey I want to do the same thing on this larger

00:57:00.000 --> 00:57:07.440
data set over here and it has all the logic to generate the correct you know query representation

00:57:07.440 --> 00:57:12.880
and run that workload for you. So it's super useful. But there's a whole wave of like you know

00:57:12.880 --> 00:57:19.520
work right now to help enable people to work in a pandas like way but get work with big data or

00:57:19.520 --> 00:57:25.600
you know get better performance than pandas because pandas is a Swiss army knife but is you know

00:57:25.600 --> 00:57:32.880
isn't a chainsaw so it isn't you know if you were rebuilding pandas from scratch it would end up a

00:57:32.880 --> 00:57:38.800
lot you know there's areas of the projects that are you know more bloated or have performance

00:57:38.800 --> 00:57:43.840
overhead that's hard to get rid of and so that's why you have you know Richie Fink started the

00:57:43.840 --> 00:57:52.080
Polars project which is kind of a reimagining of pandas data frames written in Rust and exposed

00:57:52.080 --> 00:58:01.280
in Python and Polars of course is built on Apache Arrow at its core so you know building an Arrow

00:58:01.280 --> 00:58:06.960
native data frame library in Rust and you know all the benefits that come with you know building

00:58:06.960 --> 00:58:11.360
Python extensions in Rust you know you avoid the GIL and you can manage the multi-threading in a

00:58:11.360 --> 00:58:16.800
systems language all that all that fun stuff. Yeah when you're talking about Arrow and supporting

00:58:16.800 --> 00:58:21.520
different ways of using it and things being built on it certainly Polars keep in mind for me.

00:58:22.320 --> 00:58:28.000
You know when you talk about Ibis I think it's interesting that a lot of these data frame

00:58:28.000 --> 00:58:34.960
libraries they try to base their API to be pandas like but not identical potentially you know

00:58:34.960 --> 00:58:45.200
thinking of Dask and yeah others but this Ibis sort of has the ability to configure it and extend

00:58:45.200 --> 00:58:50.320
it make it different kind of like for example Dask which is one of the back ends here. Yeah.

00:58:50.320 --> 00:58:56.320
It does but the API doesn't change right it just it it talks to the different back ends.

00:58:56.320 --> 00:59:01.280
Yeah there's there's different schools of thought on this so there's there's a another project

00:59:01.280 --> 00:59:06.800
called Modin which is similar to Ibis in many ways in the sense of like transpilation and

00:59:06.800 --> 00:59:12.800
sort of dynamically supporting different back ends but sought to closely emulate the you know

00:59:12.800 --> 00:59:20.240
exact details of like you know the API call the function name the function arguments must

00:59:20.240 --> 00:59:25.920
be exactly the same as pandas to with the goal of being a drop-in replacement for people's

00:59:25.920 --> 00:59:31.360
for people's pandas code and that's one approach kind of the pandas emulation route

00:59:31.360 --> 00:59:37.600
and there's a library called koalas for for spark which is like a pi spark emulation layer

00:59:37.600 --> 00:59:43.760
for the pandas API and then there's other projects like Polars and Ibis and Dask DataFrame that

00:59:44.480 --> 00:59:51.040
take like design cues from pandas in the sense of like the general way in which the API works but

00:59:51.040 --> 00:59:57.280
this made meaningful departures in the interest of doing things better in many ways than than

00:59:57.280 --> 01:00:03.440
than pandas did in certain parts of the parts of the API and making things simpler and not being

01:00:03.440 --> 01:00:09.120
beholden like beholden to decisions that were made in pandas you know 15 years ago not to say

01:00:09.120 --> 01:00:14.480
there's anything bad about the pandas API but like with any API it's large like it's it's it's very

01:00:14.480 --> 01:00:22.000
large as evidenced by you know the 2000 page pages of documentation and so I understand the desire

01:00:22.000 --> 01:00:28.080
to make things simpler while also refining certain things making certain types of workloads easier to

01:00:28.080 --> 01:00:34.240
easier to express and so Polars for example is very expression based and so everything is column

01:00:34.240 --> 01:00:41.920
expressions and is lazy and not eagerly computed whereas pandas is eager execution just like numpy

01:00:41.920 --> 01:00:49.120
is which is how pandas you know became eager eagerly executed in the first place and so I

01:00:49.120 --> 01:00:55.920
think the mantra with Polars was we don't want to support the eager execution by default that

01:00:55.920 --> 01:01:00.880
pandas provides we want to be able to build expressions so that we can do query optimization

01:01:00.880 --> 01:01:06.480
and take inefficient code and under the hood rewrite it to you know be more efficient which

01:01:06.480 --> 01:01:11.760
is you know what you can do with a query optimizer and so ultimately like and that

01:01:11.760 --> 01:01:18.080
matters a lot when you're executing code remotely or in like a big data system that you want to have

01:01:18.080 --> 01:01:23.760
the freedom to be able to take like a lazy you know analytic expression and rewrite it you know

01:01:23.760 --> 01:01:29.360
based on it might be like you need to seriously rewrite the expression in the case of like

01:01:29.360 --> 01:01:37.040
dask for example like dask has to do planning across a distributed cluster and so you know

01:01:37.040 --> 01:01:42.640
dask data frame is very pandas like but it also includes some explicit details of being able to

01:01:42.640 --> 01:01:47.760
control how the data is partitioned and being able to have some knobs to turn in terms of like

01:01:47.760 --> 01:01:53.360
having more control over what's happening on a distributed cluster and I think the goal there

01:01:53.360 --> 01:01:57.200
is like to give the developer more control as opposed to like trying to be intelligent you know

01:01:57.200 --> 01:02:02.640
make all of the decisions on behalf of the developer so if you you know if you know about

01:02:02.640 --> 01:02:07.360
how you know know a lot about your data set then you can make more you can make a

01:02:07.360 --> 01:02:13.360
you know decisions about how to how to schedule and execute it of course dask is building query

01:02:13.360 --> 01:02:17.760
you know query optimization to to start making more of those decisions on behalf of the user but

01:02:17.760 --> 01:02:24.880
you know dask has become very popular and impactful and making distributed computing easier in python

01:02:25.840 --> 01:02:30.080
so they've gotten you know I think gotten a long way without turning into a database and I think

01:02:30.080 --> 01:02:36.160
dask never aspired to be a to be a database engine which is a lot of distributed computing is you

01:02:36.160 --> 01:02:40.960
know not database like it could be distributed array computing or distributed model training and

01:02:40.960 --> 01:02:47.120
just being able to easily run distributed you know python functions on a cluster and do distributed

01:02:47.120 --> 01:02:53.440
computing that way it was amazing like how many people were using pi spark in the early days just

01:02:53.440 --> 01:02:59.280
for the convenience of being able to run python functions in parallel on a cluster

01:02:59.280 --> 01:03:05.840
yeah and that's pretty interesting not exactly what it's designed for right desk you know you

01:03:05.840 --> 01:03:12.320
probably come across situations where you do a sequence of operations they're kind of commutative

01:03:12.320 --> 01:03:16.640
in the end in practice but from a computational perspective like how do I distribute this amongst

01:03:16.640 --> 01:03:24.000
different servers maybe one order matters a lot more than the other out performance you know yeah

01:03:24.000 --> 01:03:34.720
yeah interesting all right one final thing sequel glot yeah so so sequel glot project started by

01:03:34.720 --> 01:03:44.000
toby mao so he's a netflix alum and you know really yeah really talented talented developer

01:03:44.000 --> 01:03:50.880
who's created this sequel query transpilation framework library for python and you know kind

01:03:50.880 --> 01:03:55.840
of underlying core library and so the problem that's being solved there is that

01:03:55.840 --> 01:04:02.160
sequel despite being a quote-unquote standard is not at all standardized across different

01:04:02.160 --> 01:04:07.840
database systems and so if you want to take your sequel queries written for one engine and use them

01:04:07.840 --> 01:04:13.600
someplace else without something like sequel glot you would have to manually rewrite and make sure

01:04:13.600 --> 01:04:21.760
you get the typecasting and coalescing rules correct and so sequel glot has understands

01:04:21.760 --> 01:04:28.560
the intricacies and the quirks of every database dialect sequel dialect and knows how to correctly

01:04:28.560 --> 01:04:35.600
translate from one dialect to another and so ibis now uses sequel glot as its underlying engine for

01:04:36.800 --> 01:04:43.600
query transpilation and generating generating sequel generating sequel outputs so originally

01:04:43.600 --> 01:04:50.880
ibis had its own kind of bad version of sequel glot kind of a query transpi like sequel

01:04:50.880 --> 01:04:58.720
transpilation that was uh powered by i think powered by uh sequel alchemy and and some

01:04:58.720 --> 01:05:03.200
and a bunch of custom code and so i think they've been able to delete a lot in ibis by moving to

01:05:03.200 --> 01:05:09.200
sequel glot and i know that you know sequel lot is also you know being used to you know power

01:05:09.200 --> 01:05:14.800
kind of a new yeah being used in people building new products that are python powered

01:05:14.800 --> 01:05:22.320
things like that so and and toby like his uh company tobiko data they uh yeah they're building

01:05:22.320 --> 01:05:28.560
a product called sequel mesh that's uh that's powered by sequel glot so very cool project and

01:05:28.560 --> 01:05:32.320
maybe a bit in the weeds but if you've ever needed to convert a sequel query from one dialect to

01:05:32.320 --> 01:05:38.720
another it's uh yeah sequel lot is here to save the day i would say you know even simple things

01:05:38.720 --> 01:05:45.840
is how do you specify a parameter variable you know for a parameterized query right and

01:05:45.840 --> 01:05:52.080
microsoft sequel server it's like at the very the parameter name and oracle it's like question

01:05:52.080 --> 01:05:56.160
mark or sequel i think it's also quite you know just that even those simple things it's a pain

01:05:56.160 --> 01:06:01.520
and without it you end up with little bobby tables which is also not good so that's true

01:06:01.520 --> 01:06:07.680
that's true nobody wants to talk to him yeah this is really cool sequel a lot like polyglot but

01:06:07.680 --> 01:06:16.160
all the languages of sequel nice and uh you do things like you can say read duck db and

01:06:16.160 --> 01:06:22.160
write to hive or read duck db and then write to spark or whatever it's pretty cool yeah

01:06:22.160 --> 01:06:30.800
all right wes i think we're getting short on time but you know i know everybody appreciated

01:06:30.800 --> 01:06:34.960
hearing from you and hearing what you're up to these days anything you want to add before we

01:06:34.960 --> 01:06:45.840
wrap up i don't think so yeah and i enjoyed the conversation and yeah yeah there's a lot

01:06:45.840 --> 01:06:50.560
of stuff a lot of stuff going on and uh still plenty of things to get get excited about so i

01:06:50.560 --> 01:06:57.360
think often people feel like people feel like uh you know all the all the exciting problems in the

01:06:57.360 --> 01:07:03.200
in the python ecosystem have been solved but there's still still a lot to do and yeah we've

01:07:03.200 --> 01:07:10.080
made a lot of progress in the last uh uh you know 15 plus years but you know in some ways feels

01:07:10.080 --> 01:07:14.240
like we're feels like we're just getting started so we are just excited excited to see where things

01:07:14.240 --> 01:07:18.720
go next yeah every time i think all the problems are solved then you discover all these new things

01:07:18.720 --> 01:07:22.400
that are so creative and you're like oh well that was a big problem i didn't even know it was a

01:07:22.400 --> 01:07:30.480
problem it's great yeah well all right thank you for being here and taking the time and yeah keep

01:07:30.480 --> 01:07:35.760
us updated on what you're up to sounds good all right thanks for joining us yeah bye-bye

