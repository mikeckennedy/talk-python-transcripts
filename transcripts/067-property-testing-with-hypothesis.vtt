WEBVTT

00:00:00.001 --> 00:00:04.740
Let's talk about your unit testing strategy. How do you select the tests you write, or do you even

00:00:04.740 --> 00:00:09.240
write tests? Typically, when you write a test, you have to think about what you are testing and the

00:00:09.240 --> 00:00:13.960
exact set of inputs and outcomes that you're looking for. And there are strategies for this.

00:00:13.960 --> 00:00:18.620
Try to hit the boundary conditions, test the most common use cases, seek out error handling,

00:00:18.620 --> 00:00:23.480
things like that. We all do this to varying degrees of success. But what if we didn't have

00:00:23.480 --> 00:00:27.820
to do this? What if there is some kind of way to express relationships between inputs and outputs,

00:00:27.820 --> 00:00:33.640
but your test could explore the problem space independently on its own? Well, there is a way,

00:00:33.640 --> 00:00:37.600
and it's called property-based testing. This week, you'll learn about Hypothesis,

00:00:37.600 --> 00:00:42.340
the most popular property-based testing system for Python created by David McLeaver.

00:00:42.340 --> 00:00:48.340
This is Talk Python to Me, episode 67, recorded Monday, July 11th, 2016.

00:00:48.340 --> 00:01:18.060
Welcome to Talk Python to Me, a weekly podcast on Python, the language, the libraries, the ecosystem,

00:01:18.260 --> 00:01:22.840
and the personalities. This is your host, Michael Kennedy. Follow me on Twitter, where I'm at,

00:01:22.840 --> 00:01:28.220
mkennedy. Keep up with the show and listen to past episodes at talkpython.fm, and follow the show

00:01:28.220 --> 00:01:34.680
on Twitter via at Talk Python. This episode is brought to you by Hired and SnapCI. Thank them for

00:01:34.680 --> 00:01:42.860
supporting the show on Twitter via at Hired underscore HQ and at Snap underscore CI. David, welcome to the show.

00:01:43.260 --> 00:01:44.680
Thanks, Michael. It's a pleasure to be here.

00:01:44.680 --> 00:01:51.880
I'm really excited to talk about this new angle on testing that I honestly just learned about a few

00:01:51.880 --> 00:01:56.720
weeks ago. I'm really thrilled to talk about property-based testing, where the computer can

00:01:56.720 --> 00:02:00.220
sort of do more of the heavy lifting and more of the automation, honestly, for us, right?

00:02:00.220 --> 00:02:05.640
Yep, that's the idea. Sometimes when I'm feeling particularly salty, describe it as real automated

00:02:05.640 --> 00:02:06.000
testing.

00:02:06.500 --> 00:02:11.340
Yes, yeah. And we'll get into why that is and what that is. But before we do, let's talk about

00:02:11.340 --> 00:02:13.080
your story. How did you get started in programming?

00:02:13.080 --> 00:02:17.260
So I actually got started in programming for very selfish reasons, which was simply for the money.

00:02:17.260 --> 00:02:23.400
I got out of university with a maths degree and didn't really want to stay in academia,

00:02:23.400 --> 00:02:30.080
but was completely lost in anything fewer than 10 dimensions. So I had to figure out what to do

00:02:30.080 --> 00:02:34.340
if that would actually work for me. And the options that came up were banking or software,

00:02:34.340 --> 00:02:38.620
and software seemed the lesser of the two evils.

00:02:38.620 --> 00:02:40.640
I would say you made a good choice.

00:02:40.640 --> 00:02:46.520
And I found a nice little company that was happy to take a chance on me and let me learn to program

00:02:46.520 --> 00:02:53.600
on the job. And it turned out I quite liked this programming thing. And ironically, having gotten to

00:02:53.600 --> 00:02:57.060
it for the money, I've been spending a lot of the last year and a half doing work entirely for free.

00:02:57.240 --> 00:02:58.440
But hey, it's been fun.

00:02:58.440 --> 00:03:05.200
You know, that's a really interesting sort of view of things. And I kind of got into programming,

00:03:05.200 --> 00:03:10.520
not I didn't think of it as for the money, but for the opportunity. I also was working on my PhD in

00:03:10.520 --> 00:03:16.660
math. And, and I looked at the jobs and that how difficult and limited the jobs were people coming

00:03:16.660 --> 00:03:20.980
out of you, like you said, you can basically go work at like a bank or an insurance company,

00:03:20.980 --> 00:03:27.140
or you can be in academia. And it's, it's either super boring, you're working, you know,

00:03:27.180 --> 00:03:33.120
like optimizing a little bit of some kind of insurance policy, or it's, it's extremely competitive.

00:03:33.120 --> 00:03:39.700
And I decided to go into computers as well, sort of after the fact. And I had a similar experience

00:03:39.700 --> 00:03:45.100
that I started working for a company that was willing to take a chance on me. I basically said,

00:03:45.100 --> 00:03:50.240
look, I don't know much programming, but this one thing that you need, this is the one thing I can

00:03:50.240 --> 00:03:53.280
actually do really well. You know, if you're willing to just let me do this one thing,

00:03:53.280 --> 00:03:57.380
and then you give me some support, I can do a great job for you. And they did. And,

00:03:57.380 --> 00:04:02.360
you know, it's no, no looking back. It's great. So yeah, very cool that you got that start that way.

00:04:02.360 --> 00:04:04.400
Where'd Python come along?

00:04:04.400 --> 00:04:10.440
Python originally came along just for another job. I've always been sort of a programming languages

00:04:10.440 --> 00:04:15.720
polyglot and tends to just pick up languages either when they seem interesting or when I need them for

00:04:15.720 --> 00:04:22.840
a particular problem or a particular job. In Python's case, it was a, it was a job. I had

00:04:22.840 --> 00:04:30.660
been working in a company doing Ruby actually for a couple of years. But when I was looking to move on,

00:04:30.660 --> 00:04:35.620
the job that seemed most interesting was one using Python. Python seems like a perfectly nice language.

00:04:35.620 --> 00:04:40.960
I was very happy to learn it. So I did. I learned it in advance of the job rather than actually

00:04:40.960 --> 00:04:46.980
starting from day one with zero Python knowledge. And musingly, a hypothesis was actually my learning

00:04:46.980 --> 00:04:49.260
Python project originally. Wow.

00:04:49.260 --> 00:04:54.000
It's so the version of hypothesis you see today isn't my learning, isn't my learning Python project

00:04:54.000 --> 00:04:59.820
has been essentially completely rewritten since then. I wrote basically a prototype back in 2013,

00:04:59.820 --> 00:05:08.180
when I was first learning Python. And then beginning of 2015, when I left another job and had some time on

00:05:08.180 --> 00:05:13.200
my hands, I got frustrated that no one had improved on this silly little prototype I wrote. So I took

00:05:13.200 --> 00:05:15.900
sort of the core idea and turned it into a real thing.

00:05:15.900 --> 00:05:19.980
That's cool. Yeah. And it, you know, looking on GitHub, it's got over a thousand stars. It's,

00:05:19.980 --> 00:05:26.680
it's definitely going, how long has it been sort of actively used or, you know, you said you sort of

00:05:26.680 --> 00:05:30.760
did it as a learning thing and then you revamped it. Like when was that revamping and when is it kind

00:05:30.760 --> 00:05:31.300
of taken off?

00:05:31.300 --> 00:05:37.280
So people were actively using even the prototype, which is sort of what prompted me to do this because

00:05:37.280 --> 00:05:39.300
Oh my gosh, people don't put another thing.

00:05:39.300 --> 00:05:45.380
Yeah, exactly. It's, it was terrible at that point, but it managed to be less terrible than

00:05:45.380 --> 00:05:49.600
all the alternatives. So I don't think it had a lot of users then, but I think it had maybe

00:05:49.600 --> 00:05:55.700
five or something like that. I know that someone at Twitter once told me a couple of months ago that

00:05:55.700 --> 00:06:03.180
they were just updating from hypothesis not 0.1.4, I think, which I think wasn't the 2013 version.

00:06:03.300 --> 00:06:08.540
I think that was the first patch release I put out beginning of 2015, but there were definitely

00:06:08.540 --> 00:06:16.480
some people using the ridiculous early prototype. In terms of serious usage, basically the first

00:06:16.480 --> 00:06:24.960
three months, I think of 2015 were what it took me to go from the 0.1.x series to the 1.0 series.

00:06:25.620 --> 00:06:44.740
And by that point, there were definitely a couple of significant users. I think Rob Smallshear was one of the early users of the 1.0 series and whose colleague Austin you had on recently, which is why I mentioned him. And there were another handful.

00:06:44.740 --> 00:07:01.740
Yeah, yeah. The mutation testing show. Yeah, exactly. And there were another handful then. But 1.3, which I think happened April 2015 was probably, it's the first one that is really recognizable as sort of modern hypothesis. And I think that's when sort of the traction started

00:07:01.740 --> 00:07:22.900
really, sort of when it really started building momentum, because early hypothesis, even in the 1.0 series was still a bit weird. And 1.3 was the point at which all of the visible weirdness went away. And I think it just became a really good, useful system. And people stopped having to scratch their head whenever they got started.

00:07:22.900 --> 00:07:29.820
Yeah. I mean, people are still scratching their head a bit, but mostly because they're new to property-based testing rather than because hypothesis is weird.

00:07:29.820 --> 00:07:42.840
Yeah, it's a really interesting idea. And I definitely want to talk about it. You know, maybe we should take just a little bit of a step back and talk about testing and property-based testing in general, and then we can dig into hypothesis itself.

00:07:43.160 --> 00:08:03.440
So I have a lot of listeners that have a diverse background. So many programmers, but also scientists and data scientists and so on. So maybe not everyone fully gets the benefits of testing. So could you maybe just touch on three reasons why we care about testing that we'll dig into what is property testing and so on?

00:08:03.940 --> 00:08:28.000
Sure. So I can give you one reason I care about testing, which is mainly it's about confidence. I was having a good discussion with some people recently about the fact that I do something slightly weird in the releasing hypothesis, which is that someone reports a bug. I go, here's a bug patch. I've done a patch release. Okay. New version is out. And that's fine. You can use it and install it.

00:08:28.340 --> 00:08:48.640
And the reason I can do that is because I've got an incredibly comprehensive build and automated test. So if the build passes and if all the tests pass, then I can be basically sure that this code is, if not perfect, it's at least working as well as I've known previous versions to work and I can rely on it.

00:08:49.340 --> 00:09:05.000
And for me, that's sort of the big thing about testing is that it gives me a reproducible idea of how well my software is working. And I'm free to make changes to it. I'm free to push out new releases because I know those releases aren't broken.

00:09:07.340 --> 00:09:16.980
These days, trying to do any sort of software without that level of confidence feels really upsetting and alien to me.

00:09:16.980 --> 00:09:28.300
I mean, I absolutely have some small hack projects which are terribly tested, but that's because they have a single user who is me and I don't really care if they break because I'll just fix them when they break.

00:09:28.300 --> 00:09:33.320
But for anything where I want other people to be able to depend on me, tests are amazing for that.

00:09:33.320 --> 00:09:42.720
Yeah, and if you're doing an open source project, having tests supporting it make it much more likely that people will have confidence in using your library, right?

00:09:42.720 --> 00:09:54.380
If you have an actual product or website that you're shipping, it would be really nice to be able to say, you know, this morning I came in and there was this cool feature somebody wanted.

00:09:54.380 --> 00:09:59.280
It took me about an hour. I added it. It has the test to support it. Let's push it live, right?

00:09:59.280 --> 00:10:08.840
Rather than we're going to queue up all of our work and give it off the QA and in two months we'll get together over the weekend and take the site down and upgrade it, right?

00:10:08.840 --> 00:10:14.740
Like those are super different philosophies and the first one deeply depends on automated testing and automated builds, I think.

00:10:14.740 --> 00:10:25.020
Although ideally you wouldn't do the first one either because the problem with adding features that quickly is that you need to then support the features indefinitely.

00:10:25.020 --> 00:10:25.600
Sure.

00:10:25.880 --> 00:10:36.860
Features are made in haste and regretted at leisure. So I tend to do very rapid releases for bug fixes, but I'm much more cautious about adding new features.

00:10:36.860 --> 00:10:45.420
Yeah, I guess there's two parts to it, right? The part that I was expressing is like the ability to make changes to code with confidence quickly.

00:10:45.900 --> 00:10:51.700
And then there's the part of product design and evolution needs to be, you know, not done in haste, right?

00:10:51.700 --> 00:11:03.420
Like it needs to be carefully thought through. Yeah. All right, cool. So what's the guy, we know what unit testing is and automated testing, but what is this property based testing? What's the idea there?

00:11:03.420 --> 00:11:12.800
So one thing where I slightly differ in my framing of this from other people is that I don't regard property based testing as distinct from unit testing.

00:11:13.340 --> 00:11:25.720
I think property based testing is an extension to your normal testing tools, but a property based test could be a unit test or it could be an integration test or it could be a functional test.

00:11:25.720 --> 00:11:36.280
However, you want to draw those boundaries. It's property based tests versus example based tests are sort of orthogonal to the usual splits that people do for testing.

00:11:36.280 --> 00:11:50.140
Right. It's like a layer that you can lay upon any of your testing layers, right? So if you're doing integration tests, you can add property based testing to that to get better coverage.

00:11:50.140 --> 00:12:00.940
Yeah. Yeah. Yeah. And so I guess we need to have some terminology here that people may or may not be familiar with, like the traditional unit test with the three A's.

00:12:00.940 --> 00:12:10.640
You know, you write out your test and you say, here's the the range. I'm going to set up my data. I'm going to act. I'm going to pass in this number and expect this individual specific value back.

00:12:10.640 --> 00:12:16.640
And then I'm going to, you know, assert that that's true. Yeah. That is referred to as example based testing, right?

00:12:16.760 --> 00:12:25.820
Yeah. That's certainly the term I use. And it's, it seems to be reasonably standard. Like a number of people have independently invented it. So I think that's probably the best terminology to use.

00:12:25.820 --> 00:12:38.260
Yeah. When I first learned about property based testing, I didn't, I didn't know how to refer to it distinctly from, I didn't know how to call the regular or like we're sort of traditional unit based testing and what we're calling example based testing.

00:12:38.380 --> 00:12:49.860
I didn't know like what the terminology for that was. It was hard to compare it. But once that sort of realized that that was how it was done before, it's like, here's one specific example of a test.

00:12:49.860 --> 00:12:57.140
This number equals this value, but property based testing tries to cover a range or a set of constraints, almost like rules, right?

00:12:57.140 --> 00:13:04.240
Yep, exactly. So it looks very like what you were describing, the example based test, but just you replace the steps slightly.

00:13:04.620 --> 00:13:15.140
So the first step where you would set up the data yourself, instead you sort of describe to the testing library hypothesis in this case, how to set up the data that you want.

00:13:15.600 --> 00:13:25.660
So instead of saying, I want this string, you say, give me a string here. Instead of saying, I want this user, you say, give me a user object here, that sort of thing.

00:13:26.920 --> 00:13:43.560
And then at the end, instead of being able to say, you should get this exact value, you, well, I mean, sometimes you can say this, you should get this exact value if the exact value is one of the things you passed in or something you can easily compute to it from it.

00:13:43.560 --> 00:13:51.480
Right. If I get a user that's not registered, not a pain user, I should be able to ask the question, is this a pain user? No. Something like this.

00:13:51.480 --> 00:14:21.460
Yes, exactly. Yeah. Okay.

00:14:21.460 --> 00:14:26.860
Could do an example based test, but it's often not worth doing if you just got a very small range of examples.

00:14:26.860 --> 00:14:29.720
Right. A single example really would cover it.

00:14:29.720 --> 00:14:37.740
So I think it might be worth to discuss, you know, it's hard to talk about code, but just to give people an idea of what this is like.

00:14:38.240 --> 00:14:50.000
So we install hypothesis, the package, and then suppose I've, let's just take an example of, I have some kind of method and it's just calling a function and that function is supposed to return an even number.

00:14:50.240 --> 00:14:53.960
And so I'm, I'm doing something like deriving from test case.

00:14:53.960 --> 00:15:01.720
So self dot assert this, if I pass it five, it returns me like the number four, like the closest even number or something.

00:15:01.720 --> 00:15:07.140
Like how would I change that code to, to become something property based with hypothesis?

00:15:07.640 --> 00:15:21.640
So you would start by adding a given decorator, which is the main entry point hypothesis that describes essentially how to call your test function with a bunch of arguments that are your test data.

00:15:21.980 --> 00:15:31.960
So you'd say given integers, for example, integers is a standard strategy that hypothesis provides for generating integers clues in the name.

00:15:31.960 --> 00:15:39.880
But now you've got your test function, which has an extra argument and is being passed in integers via that argument.

00:15:39.880 --> 00:15:47.680
You would then call your test, your method that you're testing and look at the result.

00:15:47.680 --> 00:15:51.680
Right. And would you like add a parameter or something to that method or something like this?

00:15:51.680 --> 00:16:05.080
Yes, exactly. You, you would add a parameter for the integer you want passed in and you would either pass in, either tell given to pass the parameter by name or positionally as you prefer.

00:16:05.080 --> 00:16:05.700
Right. Okay.

00:16:05.700 --> 00:16:13.420
But yeah, so then you call your function with the integer you've been passed in and you look at the value.

00:16:13.420 --> 00:16:18.480
The first thing you would do is you would assert that the value is always even.

00:16:19.200 --> 00:16:21.880
So value mod two equals equals zero.

00:16:21.880 --> 00:16:36.560
And then if you wanted that it was always the closest even number, then what you could do is say assert that the absolute value of the difference of your argument and your return value is less than or equal to one.

00:16:36.560 --> 00:16:37.520
Okay.

00:16:38.480 --> 00:16:40.560
And then I don't change anything else.

00:16:40.560 --> 00:16:49.000
Right. Like that's one of the beautiful things about hypothesis is it's not like, well, now use instead of using nose, you now use hype test or whatever.

00:16:49.000 --> 00:16:49.240
Right.

00:16:49.240 --> 00:16:52.520
It's just you run the test like unchanged basically.

00:16:52.520 --> 00:16:52.800
Yeah.

00:16:53.240 --> 00:16:59.240
Absolutely. It works with essentially any of the testing frameworks without you having to use its own test runner.

00:16:59.240 --> 00:17:11.420
It's a little annoying in unit test because when it fails, it will print out the falsifying example sort of in the middle of your test runs because unit test doesn't have a good way of hooking into test output sensor plane.

00:17:11.920 --> 00:17:14.540
Okay. Which one would you say is the best to connect it to?

00:17:14.540 --> 00:17:15.140
pytest?

00:17:15.140 --> 00:17:16.460
pytest is probably the best.

00:17:16.460 --> 00:17:21.820
It works about, pytest has a special, has a plugin for it, but that does almost nothing.

00:17:21.820 --> 00:17:26.440
So it works maybe 10% better with pytest than with nose.

00:17:26.440 --> 00:17:32.440
And unit test is, or unit test isn't the least supported.

00:17:32.440 --> 00:17:34.380
Unit test works fine other than the printing issue.

00:17:34.380 --> 00:17:43.580
The least supported is probably Twisted's test runner because the asynchronous support and hypothesis isn't quite there.

00:17:43.580 --> 00:17:45.000
Right. That's the most different.

00:17:45.000 --> 00:17:50.980
You can do it okay with some hooks to basically turn the asynchronous test into a synchronous test.

00:17:51.140 --> 00:17:56.160
So if you want to test asynchronous code from pytest or nose, that will mostly work.

00:17:56.160 --> 00:18:07.960
But the, I'm temporarily blanking on what the Twisted test runner is called, but it's assumptions about how tests work or slightly different from the assumption, from hypothesis assumptions about how tests work.

00:18:07.960 --> 00:18:09.100
Right. Okay. Sure.

00:18:09.100 --> 00:18:15.340
You know, I think I got a pretty good handle on how it works, but a lot of people are hearing this for the first time.

00:18:15.460 --> 00:18:22.360
When you run the test, basically the decorator wraps your test function and then it looks at the description and the parameters.

00:18:22.360 --> 00:18:30.100
And then it comes up with ways in which it tries to break your test and it passes and executes it with many different values, right?

00:18:30.100 --> 00:18:31.480
Yep. That's exactly right.

00:18:31.480 --> 00:18:34.400
It doesn't so much come up with ways to break your test.

00:18:34.400 --> 00:18:37.540
It's just try it with a large range of values, many of which are quite nasty.

00:18:38.180 --> 00:18:48.260
So it doesn't currently sort of look inside your test and figure out what might break things, but it has pretty good heuristics for figuring out values that are like the break things.

00:18:48.520 --> 00:19:04.340
Right. And, you know, I think one of the really important and maybe under, you know, it's hard to make generalizations for people, but under tested parts of code is the error handling and the off by one errors and just like right on the boundary of good and bad.

00:19:04.600 --> 00:19:11.660
And it seems like hypothesis is really good at like looking for that and unusual inputs and values in general, right?

00:19:11.660 --> 00:19:18.900
Yeah, it's definitely, there are a lot of things that hypothesis will try most times, which people sort of forget or a thing.

00:19:18.900 --> 00:19:26.880
I don't know if this is actually true, but I often joke that about a third of the bugs that you'll find when running hypothesis on a new code base is that you forgot that something could be empty.

00:19:27.200 --> 00:19:27.400
Yeah.

00:19:27.400 --> 00:19:30.800
And the third is a made up number, but I've definitely seen this pattern a lot.

00:19:30.800 --> 00:19:37.380
It's good at finding Unicode bugs because people often forget about particularly weird Unicode cases.

00:19:37.380 --> 00:19:41.760
Although I found recently a class of bugs where it needs to be better at finding them.

00:19:41.760 --> 00:19:43.000
Right. Okay.

00:19:43.000 --> 00:19:56.240
Yeah. So for example, if I have like some kind of scientific thing that expects, you know, numbers, you know, maybe it assumes that they're all positive and hypothesis would of course pass negative numbers or zero.

00:19:56.660 --> 00:20:06.640
But it also might pass like not a number, infinity, all sorts of funky inputs that people typically don't test for in example-based testing or if they even have these at all, right?

00:20:06.640 --> 00:20:18.320
One problem I found is that a lot of people who are doing floating point maths don't actually care enough about getting the right answer to make using hypothesis really valuable.

00:20:18.320 --> 00:20:25.260
Like if you do care about it and you're sort of prepared to do a numeric stability analysis on your code, then hypothesis is great.

00:20:26.120 --> 00:20:33.740
For the most part, I think people who are using floating point kind of want to pretend that they're not.

00:20:33.740 --> 00:20:40.320
And the edge cases that hypothesis will tell them about are somewhat unwelcome.

00:20:40.320 --> 00:20:50.480
Right. So what if I have like some kind of scientific thing and I know it only expects, say, positive integers, but not infinity, something like this.

00:20:50.480 --> 00:20:58.520
And is there a way in the given decorator to say, I want to give it integers, but only in say this range or something like that?

00:20:58.880 --> 00:21:04.040
Yeah. So the strategy functions all come with a huge number of mobs to twiddle.

00:21:04.040 --> 00:21:08.620
So the integers function accepts both min value and max value parameters.

00:21:08.620 --> 00:21:13.760
And you can just specify those as you want.

00:21:13.760 --> 00:21:17.340
So you can say min value equals one if you only want to strike the positive integers.

00:21:17.340 --> 00:21:23.200
Similarly, floats have, the float strategy has both min and max value.

00:21:23.260 --> 00:21:27.660
And you can also tell it, and don't give me infinity or don't give me none.

00:21:27.660 --> 00:21:34.960
So you could do something like floats, min value equals zero, allow infinity equals false, allow none equals false.

00:21:34.960 --> 00:21:36.980
Right. Okay. Yeah, that's really cool.

00:21:49.180 --> 00:21:51.820
This portion of Talk Python To Me is brought to you by Hired.

00:21:51.820 --> 00:21:54.860
Hired is the platform for top Python developer jobs.

00:21:54.860 --> 00:21:59.660
Create your profile and instantly get access to 3,500 companies who will work to compete with you.

00:21:59.660 --> 00:22:02.520
Take it from one of Hired's users who recently got a job and said,

00:22:02.520 --> 00:22:07.820
I had my first offer on Thursday after going live on Monday, and I ended up getting eight offers in total.

00:22:07.820 --> 00:22:11.280
I've worked with recruiters in the past, but they've always been pretty hit and miss.

00:22:11.280 --> 00:22:14.120
I tried LinkedIn, but I found Hired to be the best.

00:22:14.120 --> 00:22:16.220
I really liked knowing the salary up front.

00:22:16.220 --> 00:22:18.580
Privacy was also a huge seller for me.

00:22:19.260 --> 00:22:20.260
Sounds awesome, doesn't it?

00:22:20.260 --> 00:22:22.260
Well, wait until you hear about the sign-in bonus.

00:22:22.260 --> 00:22:25.680
Everyone who accepts a job from Hired gets $1,000 sign-in bonus.

00:22:25.680 --> 00:22:28.340
And as Talk Python listeners, it gets way sweeter.

00:22:28.340 --> 00:22:33.580
Use the link Hired.com slash Talk Python To Me, and Hired will double the sign-in bonus to $2,000.

00:22:33.580 --> 00:22:35.380
Opportunity's knocking.

00:22:35.380 --> 00:22:39.140
Visit Hired.com slash Talk Python To Me and answer the door.

00:22:45.580 --> 00:22:50.880
And another thing that I thought was nice on these, I don't know if this works for every strategy you'll have to tell me,

00:22:50.880 --> 00:22:56.920
is if I have a set of whatever, like let's just stick with numbers for a moment, integers,

00:22:56.920 --> 00:23:02.040
and I want to somehow control them in a way that is not just min-max or something like that.

00:23:02.100 --> 00:23:07.640
Like I want only, I don't know, Fibonacci numbers or prime numbers or something like this.

00:23:07.640 --> 00:23:13.940
Like you can do a dot filter and then add a lambda expression that will further, like say, you're going to give me a bunch of stuff,

00:23:13.940 --> 00:23:16.540
but actually these are the only ones I'll allow you to give me.

00:23:16.540 --> 00:23:17.340
Something like this, right?

00:23:18.040 --> 00:23:22.580
So you can do that with every strategy, but for a lot of the examples you just gave, you shouldn't.

00:23:22.580 --> 00:23:26.740
Because the problem with filter is that it's not magic.

00:23:26.740 --> 00:23:36.260
Essentially the way it works is by generating values and then throwing them away and trying again if they don't pass the lambda expression.

00:23:36.820 --> 00:23:45.020
So if you've got something like the Fibonacci numbers and you were trying to filter by a test is a Fibonacci number,

00:23:45.020 --> 00:23:53.840
then you would basically, the only numbers you'd ever really find from there are the really small ones probably because high up they're too sparse.

00:23:53.840 --> 00:23:58.080
But what you can also do is you can, instead of filtering, you can map.

00:23:58.080 --> 00:24:08.560
So you could instead generate a positive integer and then say map that by a give me the nth Fibonacci number function.

00:24:08.560 --> 00:24:13.600
So you would start by generating, say, 10.

00:24:13.600 --> 00:24:16.660
This would get mapped through and you would get the 10th Fibonacci number.

00:24:16.660 --> 00:24:18.180
Yeah, some huge number there.

00:24:18.180 --> 00:24:25.880
Yeah, can you take like a bounded set of like here's 214 things that I think are valid inputs and please start from here.

00:24:25.880 --> 00:24:26.580
Pick one of these.

00:24:26.580 --> 00:24:27.360
Yep, absolutely.

00:24:27.540 --> 00:24:30.380
The strategy is called sampled from in hypothesis.

00:24:30.380 --> 00:24:31.040
Right.

00:24:31.040 --> 00:24:34.220
You just give it an arbitrary collection and it will give you values from that collection.

00:24:34.220 --> 00:24:38.400
And can you compose or combinatorially combine these?

00:24:38.400 --> 00:24:41.780
Like let's say have like three sets.

00:24:41.780 --> 00:24:44.600
One's got five, one's got two, and another's got three.

00:24:44.600 --> 00:24:45.300
Yep.

00:24:45.300 --> 00:24:52.200
Can I say I want to sample from this and this and this and who knows how many combinations that would have to be able to work that out?

00:24:52.200 --> 00:24:56.000
But, you know, like it would figure that out and sort of pick some from here and there and test them together.

00:24:56.280 --> 00:25:08.860
So if you want one from each, then you can use the tuple strategy, which you can basically just pass in a sequence of n strategies and it will give you a tuple of n elements with the sort of ith element drawn from the ith strategy.

00:25:08.860 --> 00:25:09.460
Okay.

00:25:10.000 --> 00:25:13.660
So in that case, you could use something like tuples sampled from, sampled from, sampled from.

00:25:13.660 --> 00:25:14.100
Right.

00:25:14.100 --> 00:25:15.140
Okay, cool.

00:25:15.240 --> 00:25:18.020
And then what are some of the other strategies?

00:25:18.020 --> 00:25:20.780
Like we're talking about numbers a lot, but there's more to it than that, right?

00:25:20.780 --> 00:25:21.160
Yeah.

00:25:21.160 --> 00:25:24.960
So there's strategies for most of the built-in types.

00:25:24.960 --> 00:25:35.240
You've got, there's unicode, there's datetime, although the datetime one's in an extra package because it depends on PYTZ because no one actually works with datetimes without PYTZ.

00:25:35.960 --> 00:25:39.040
You've got all the collections types.

00:25:39.040 --> 00:25:44.020
So you've got the tuples one dimension, but you can also generate dicts of things.

00:25:44.020 --> 00:25:45.860
You can generate sets, frozen sets.

00:25:45.860 --> 00:25:48.620
You've got unicode and byte string generators, of course.

00:25:48.620 --> 00:25:49.680
Permutations, right?

00:25:50.020 --> 00:25:50.360
Oh, yes.

00:25:50.360 --> 00:25:50.580
Yep.

00:25:50.580 --> 00:25:53.880
You can generate permutations of a given list.

00:25:53.880 --> 00:25:57.300
I don't think many people use that one, but I've occasionally found it super useful.

00:25:57.300 --> 00:25:57.780
Right.

00:25:57.780 --> 00:26:00.320
When you need it, I'm sure you're like, this is exactly what I need.

00:26:00.320 --> 00:26:01.060
Yeah, yeah, exactly.

00:26:01.060 --> 00:26:02.460
But not so often, right?

00:26:02.460 --> 00:26:02.840
Yep.

00:26:02.840 --> 00:26:07.500
And then you've got, those are sort of most of the primitive strategies.

00:26:07.500 --> 00:26:13.540
I think there are a few more of them I'm temporary writing on, but essentially if it's in the standard library, you can probably generate it.

00:26:13.540 --> 00:26:14.120
Okay.

00:26:14.260 --> 00:26:24.260
And then on top of that, you've got various strategy composition functions, which is for things like the map and filter I mentioned.

00:26:24.260 --> 00:26:32.520
You've also got the composite decorator, which lets you sort of chain together a bunch of strategy generators and generate whatever data you like.

00:26:32.520 --> 00:26:36.060
And you've got builds, which is a bit of a simpler version of that.

00:26:36.760 --> 00:26:48.560
And there's also, there's special Django support in, again, in an extra package so that it can generate more or less arbitrary Django models without you having to specify anything in particular.

00:26:48.560 --> 00:26:49.240
Right.

00:26:49.240 --> 00:26:53.000
You just say models from my model class and it will do the rest.

00:26:53.000 --> 00:26:57.840
You can override any of the rest it does, but by default it works reasonably well out of the box.

00:26:57.840 --> 00:26:58.320
That's cool.

00:26:58.320 --> 00:27:11.740
So if I had like a users table and I wanted to sort of replace that and not actually talk to the database, but I want to test the code that works with those, I could have some kind of, some kind of thing that would randomly generate users that match my data model.

00:27:11.740 --> 00:27:12.160
Yeah.

00:27:12.160 --> 00:27:13.680
It actually does talk to the database.

00:27:13.680 --> 00:27:18.800
It works with the normal Django testing and the model objects it generates are persisted in the database.

00:27:18.800 --> 00:27:19.420
Okay.

00:27:19.420 --> 00:27:19.900
Interesting.

00:27:19.900 --> 00:27:20.260
Yeah.

00:27:20.260 --> 00:27:21.360
So this is real Django.

00:27:21.360 --> 00:27:22.080
Yeah.

00:27:22.080 --> 00:27:24.420
Rather than any sort of mocking.

00:27:24.420 --> 00:27:25.440
I see.

00:27:25.440 --> 00:27:25.800
Okay.

00:27:25.800 --> 00:27:26.600
Well, very cool.

00:27:26.600 --> 00:27:27.080
Very cool.

00:27:28.260 --> 00:27:32.860
One thing that I thought was neat is, is you can say at given, you give it this strategy.

00:27:32.860 --> 00:27:44.880
You say like, so if you're given an, you know, some set of, you know, something from the set of integers and something from the set of floating points where, and do like a filter, like, you know, some, some property on your floating points.

00:27:44.880 --> 00:27:48.320
It's going to randomly pick a bunch of combinations and try them.

00:27:48.320 --> 00:27:49.160
And that's cool.

00:27:49.160 --> 00:27:58.200
But maybe, you know, maybe you're coming from a particular example-based testing scenario where like this combination of numbers for whatever reason is super important.

00:27:58.200 --> 00:27:59.000
It's important to test.

00:27:59.000 --> 00:28:00.200
And it has to be this.

00:28:00.200 --> 00:28:09.480
And so like, in addition to all the randomness that you're doing to sort of explore the problem space, you can say, you can give an example decorator, right?

00:28:09.480 --> 00:28:16.080
And say, and also include this particular test that I was testing before, say, if you're like upgrading to property-based testing.

00:28:16.320 --> 00:28:16.820
Yep, absolutely.

00:28:16.820 --> 00:28:24.960
The example decorator is sort of one of those little touches that as far as I know is unique to Hypothesis, but was in retrospect, obviously a good idea.

00:28:25.520 --> 00:28:34.840
The original use case was more for people who wanted to include the examples that Hypothesis had found for them in their source code.

00:28:34.840 --> 00:28:35.260
Mm-hmm.

00:28:35.260 --> 00:28:38.260
But it found a whole bunch of other applications.

00:28:38.260 --> 00:28:42.740
Like one is the one you mentioned of making it much more comfortable to transition from example-based testing.

00:28:43.520 --> 00:28:56.840
Another thing that I sometimes do, and I've seen a couple other people doing, is that you use example to ensure that the test always gets the maximum coverage that the test can reach.

00:28:57.600 --> 00:29:14.740
So if there's some line that your test covers 80% of the time, then you just add an at example that touches that line and so means that your coverage isn't varying from test run to test run as a result of Hypothesis if you do that.

00:29:14.740 --> 00:29:15.360
Interesting.

00:29:15.360 --> 00:29:16.600
Yeah, that's really cool.

00:29:16.600 --> 00:29:22.100
Can you use it as a negative, a negation operator to like do all this stuff but don't send it this example?

00:29:22.100 --> 00:29:24.060
Or is there some way to say this?

00:29:24.060 --> 00:29:25.740
I'm not sure that that would make any sense.

00:29:26.080 --> 00:29:27.800
You can't do that with example.

00:29:27.800 --> 00:29:30.260
That's what the assume function is for.

00:29:30.260 --> 00:29:39.100
It basically gives you a way of filtering out some classes of example as not good examples that you don't really care about.

00:29:39.100 --> 00:29:39.760
I see.

00:29:39.760 --> 00:29:42.820
Is that like a precondition sort of concept?

00:29:42.820 --> 00:29:53.500
So I could say like if I was generated an automatic user, I would say and assume the user was generated today or assume that their age is over 18 or something like that.

00:29:53.500 --> 00:29:54.300
Yeah, exactly.

00:29:54.560 --> 00:29:59.100
It is essentially a precondition, although you can put it anywhere in the test.

00:29:59.100 --> 00:30:05.740
So sometimes you would put it in the middle of a test when some previous operations produced a result that you don't care about.

00:30:05.740 --> 00:30:26.520
So you could do a bunch of, say you got a list of users and you did a bunch of calculations and you would then assume that, I don't know, you have at least one of the users passed through the test that you just, in the past through the code you just ran and you haven't sort of unregistered all the users or something.

00:30:26.860 --> 00:30:27.280
Yeah, sure.

00:30:27.280 --> 00:30:28.220
Okay.

00:30:28.220 --> 00:30:31.100
I think that's really useful and helpful.

00:30:31.400 --> 00:30:35.520
Does hypothesis ever look at the code or the code coverage or anything like this?

00:30:35.520 --> 00:30:38.920
Or does it just work black box style?

00:30:38.920 --> 00:30:41.360
I'm going to put stuff in and see if it crashes.

00:30:41.360 --> 00:30:44.420
So it currently just works black box style.

00:30:44.580 --> 00:30:51.820
I've got most of the pieces for making it work in a less black box style.

00:30:51.820 --> 00:31:01.840
In fact, I've got a separate library I wrote called Glassbox, which is designed for adding sort of glass box elements to your testing, which uses some coverage information.

00:31:01.840 --> 00:31:11.780
A lot of hypothesis internals look quite a lot like the internals of a fuzzer called American Fuzzy Wop, which has this really nice coverage-based metric.

00:31:11.780 --> 00:31:21.560
But the reason why the coverage stuff hasn't ever made it into a hypothesis is because coverage metrics work really well when you've got some sort of long-running fuzzing process.

00:31:22.240 --> 00:31:35.500
So if you're prepared to run your tests for a day, then coverage metrics are amazing and will do a really good job at poking inside the code and seeing what is going on.

00:31:35.900 --> 00:31:47.960
But if you are sort of upset, if your tests take as long as 10 seconds to run, then coverage starts to become less useful because it doesn't really have the time to actually figure things out.

00:31:47.960 --> 00:31:58.180
I think I've mostly figured out a way of making use of coverage in a way that works okay within those time constraints, but it hasn't made it into anything production-like yet.

00:31:58.180 --> 00:31:58.640
Okay.

00:31:58.640 --> 00:32:00.360
Well, that sounds like a cool direction.

00:32:00.360 --> 00:32:05.200
However, I have been saying that for about six months, so I wouldn't hold your breath on waiting for those features.

00:32:05.200 --> 00:32:06.560
Yeah, of course, of course.

00:32:06.560 --> 00:32:15.600
So it seems that it would be interesting for Hypothesis to remember scenarios it came up with that failed.

00:32:15.600 --> 00:32:23.440
Like, is there a way to say, try all these examples, and if you ever discover one, you know, save it to something that you're going to know to replay?

00:32:23.440 --> 00:32:25.680
It actually does that out in the box.

00:32:25.680 --> 00:32:26.000
Okay.

00:32:26.000 --> 00:32:29.600
Hypothesis has a database of examples.

00:32:29.600 --> 00:32:30.580
Well, I say database.

00:32:30.580 --> 00:32:35.020
It's basically an exploded directory format that is designed so you can check it into Git if you want.

00:32:35.020 --> 00:32:42.480
But whenever a test fails, then what Hypothesis does is, well, first of all, it shrinks the example.

00:32:42.480 --> 00:32:46.560
So it turns the example into a simpler one.

00:32:47.020 --> 00:32:59.500
And then in the process of doing that, every example it sees fail is saved in this database so that when you rerun the test, it replays those examples, starting from the simplest and working its way upwards.

00:33:00.220 --> 00:33:12.680
So whenever, basically, one of the things that I've made really sure in Hypothesis is that although it's randomness, it's sort of the good kind of randomness because it finds bugs, but it never forgets bugs.

00:33:13.680 --> 00:33:19.000
If the test fails, then when you rerun it, the test will continue failing until you fix the bug.

00:33:19.000 --> 00:33:26.980
Yeah, because if it didn't remember it, I guess, like, suppose the number 13, for some reason, broke your code, and you just said the strategy was integers.

00:33:26.980 --> 00:33:34.020
Like, how would it know to try 13 again if it's, like, really going that, you know, out to that many numbers and so on?

00:33:34.020 --> 00:33:34.760
So, okay.

00:33:34.760 --> 00:33:36.800
Because it did seem to keep failing consistently.

00:33:36.800 --> 00:33:42.420
And I guess I kind of clued in, like, that is a little odd that it seemed just, I'm like, oh, it's really smart.

00:33:42.420 --> 00:33:43.420
Yeah.

00:33:43.420 --> 00:33:46.300
But I didn't put it together how it's smart, of course.

00:33:46.300 --> 00:33:56.220
For a lot of tests, the failure is common enough that even without the database, Hypothesis would be able to do this and would be able to find the test, find a failure consistently each time.

00:33:56.220 --> 00:34:05.120
But one of the annoying things that happens there is that sometimes, if it didn't have the database, it would be finding often different bugs each time.

00:34:05.120 --> 00:34:09.440
Because you don't always shrink to sort of the globally minimal failure.

00:34:09.440 --> 00:34:15.280
And sometimes there are two bugs, and starting from one will shrink one way, and starting from another will shrink the other way.

00:34:15.620 --> 00:34:19.540
So, in many ways, like, that's almost more annoying than it failing unreliably.

00:34:19.540 --> 00:34:25.280
Because you don't know whether you've just changed the bug or whether you've introduced a new one or what.

00:34:25.280 --> 00:34:45.600
Gone are the days of tweaking your server, merging your code, and just hoping it works in your production.

00:34:45.600 --> 00:34:55.960
With SnapCI's cloud-based, hosted, continuous delivery tool, you simply do a git push, and they auto-detect and run all the necessary tests through their multi-stage pipelines.

00:34:55.960 --> 00:34:57.260
Something fails?

00:34:57.260 --> 00:34:59.460
You can even debug it directly in the browser.

00:34:59.460 --> 00:35:07.420
With a one-click deployment that you can do from your desk or from 30,000 feet in the air, Snap offers flexibility and ease of mind.

00:35:07.420 --> 00:35:09.260
Imagine all the time you'll save.

00:35:10.040 --> 00:35:14.960
Thanks, SnapCI, for sponsoring this episode by trying them for free at snap.ci.com.

00:35:14.960 --> 00:35:30.460
One of the things I think is interesting is the scenarios we've been talking about so far.

00:35:30.460 --> 00:35:33.960
They've been, I give you two numbers, something happens.

00:35:33.960 --> 00:35:38.400
The other side, it gives you the right answer, the wrong answer, and the test detects that.

00:35:39.020 --> 00:35:47.460
But I think one of the things that seems very powerful is for sort of stateful testing and these set of steps.

00:35:47.460 --> 00:35:53.960
Like, let's suppose I have a board game, and I've got to move things around the board in a certain way,

00:35:53.960 --> 00:36:00.140
and some property is supposed to always hold like, you know, the number of chips on the board is always the same,

00:36:00.140 --> 00:36:04.200
even if it comes from one player comes off and another has to go back on from the other player, right?

00:36:04.900 --> 00:36:06.480
Who knows? I have no idea what game this is.

00:36:06.480 --> 00:36:13.040
But, like, you could write a test to say, do all these operations, and then keep asserting that this is true.

00:36:13.040 --> 00:36:17.400
And if it fails, it gives you a really nice reproducible output, right?

00:36:17.400 --> 00:36:19.300
A set of steps almost, right?

00:36:19.300 --> 00:36:25.740
The output format is a little idiosyncratic, so it's not, unfortunately, something you can just copy and paste into a test currently.

00:36:26.560 --> 00:36:28.760
But, yeah, the stateful testing is really cool.

00:36:28.760 --> 00:36:36.060
One of the reasons I don't push it quite as hard as I could is that I don't feel like we've got very good workflows for this right now,

00:36:36.060 --> 00:36:42.560
because the stateful testing sort of, it's exploring an incredibly large search space.

00:36:42.560 --> 00:36:49.900
So you do want something that's more like the fuzzing workflows I talked about, where you do want to set it running for 24 hours or whatever.

00:36:50.760 --> 00:36:59.800
You can use it in your CI, and I'm using it in my CI in a few places, and at least one other project I know of is using it, but I wrote those tests.

00:36:59.800 --> 00:37:01.120
That's material.

00:37:02.160 --> 00:37:10.900
But the, I think it's not quite there yet in terms of being as, it's certainly not as usable as the rest of the library.

00:37:10.900 --> 00:37:17.040
And I think it has some more work required before it gets there.

00:37:17.040 --> 00:37:21.020
But I'm really excited about it, and I do want to sort of spend more time developing that.

00:37:21.020 --> 00:37:24.220
Yeah, the possibilities of it are really amazing.

00:37:24.220 --> 00:37:30.460
It's just, like you said, it's such a problem space that how are you going to find it, right?

00:37:30.460 --> 00:37:37.620
In many ways, my crack about hypothesis being true automated testing, it's really only true for the stateful testing.

00:37:37.620 --> 00:37:45.220
Because one of the things I emphasize sometimes is that hypothesis doesn't write your test for you.

00:37:45.220 --> 00:37:46.620
You're still writing your tests.

00:37:46.620 --> 00:37:52.460
It's just the hypothesis is doing some of the heavy lifting in terms of the boring coming up with examples.

00:37:52.460 --> 00:37:56.240
But the stateful testing, it's sort of, that's almost no longer true.

00:37:56.240 --> 00:37:59.120
At that point, hypothesis is almost writing tests for you.

00:37:59.300 --> 00:38:01.920
Right, yeah, that is actually really cool.

00:38:01.920 --> 00:38:07.580
But I think even so, that level of automation that you talked about that it's already really good at is super helpful.

00:38:07.580 --> 00:38:11.180
Because coming up with those examples is hard.

00:38:12.040 --> 00:38:23.260
And like I talked about earlier, I think coming up with examples that are just inside the working realm and just on the edge of the error conditions or these weird inputs, you know, those are hard to come up with.

00:38:23.260 --> 00:38:26.220
And you just, I think there's just fatigue as well.

00:38:26.220 --> 00:38:27.700
Like, okay, I've tested three cases.

00:38:27.700 --> 00:38:28.900
Like, that's probably good enough.

00:38:28.900 --> 00:38:31.040
Let's just move on to building new features, right?

00:38:31.040 --> 00:38:35.800
Whereas property-based testing will sort of explore that space for you automatically, right?

00:38:36.160 --> 00:38:36.880
Yeah, yeah, absolutely.

00:38:36.880 --> 00:38:43.820
The fact that it's, the fact that you're still writing the test isn't intended to take away from what hypothesis is doing.

00:38:43.820 --> 00:38:49.560
The coming up with examples part is both the most time-consuming part and the most boring part.

00:38:49.960 --> 00:38:52.380
And also, it's the bit that people are really bad at.

00:38:52.380 --> 00:38:59.340
So having software which can take this boring, difficult task and just do it for you is amazing.

00:38:59.340 --> 00:39:06.060
And I get really frustrated when I have to write tests that I don't have the capability for now.

00:39:06.060 --> 00:39:07.460
Yeah, I'm sure you do.

00:39:07.460 --> 00:39:13.080
And the fact that the problems that it finds get permanently remembered, that's really cool.

00:39:13.080 --> 00:39:17.800
So do you recommend people check in their .hypothesis folder into Git?

00:39:17.800 --> 00:39:19.860
It's designed so that you can do that.

00:39:19.860 --> 00:39:22.040
But I mostly don't do it myself.

00:39:22.040 --> 00:39:31.240
I generally think that you're probably better off writing at example decorators for any example you specifically want to be remembered.

00:39:31.240 --> 00:39:37.060
The main problem with the example database is that its format is quite opaque.

00:39:37.060 --> 00:39:41.580
You can't really look at a file in there and say, I know what example this corresponds to.

00:39:41.580 --> 00:39:48.180
So even though from a computer's point of view, it will remember what you need and it will do what you want,

00:39:48.380 --> 00:39:52.580
from a human's point of view, you probably want to be more explicit than that.

00:39:52.580 --> 00:40:03.260
One of the things I'd like to work on at some point but haven't sort of found the time or bluntly the customers for doing this work

00:40:03.260 --> 00:40:10.520
is better sort of centralized management of test examples so that you can have your cake and eat it too.

00:40:10.520 --> 00:40:16.280
And rather than checking it into Git, you can have a nice management interface where you can see what hypothesis has run

00:40:16.280 --> 00:40:21.480
and get both the memory and the visibility.

00:40:22.260 --> 00:40:26.460
But that's sort of a, at some point in the future project.

00:40:26.460 --> 00:40:29.200
It's not anything on the short-term roadmap.

00:40:29.200 --> 00:40:30.060
Yeah.

00:40:30.060 --> 00:40:37.380
Well, even something automated that would take the hypothesis database and inject them as ad examples into your code would be cool.

00:40:38.240 --> 00:40:38.540
Yeah.

00:40:38.540 --> 00:40:51.540
Because the major problem is that there's no real way of taking a Python object and going from that to a literal that you can just copy and paste into your code and it will produce that object.

00:40:52.520 --> 00:40:54.540
You can do it for really simple things.

00:40:54.540 --> 00:41:00.060
So a lot of the built-in types, the wrapper will do that.

00:41:00.060 --> 00:41:00.380
Yeah.

00:41:00.380 --> 00:41:02.380
Basically, the wrapper output is parsable.

00:41:02.380 --> 00:41:06.280
It's probably okay, but that's often not the case, especially for custom types.

00:41:06.280 --> 00:41:06.840
Yeah.

00:41:06.920 --> 00:41:09.440
I would say it's almost never the case for custom types.

00:41:09.440 --> 00:41:18.080
One of my idiosyncrasies as programmers, I do try to make sure that all my types have good wrappers and that will evaluate the thing you started with.

00:41:18.080 --> 00:41:21.540
But this is very rarely the case in the wild.

00:41:21.540 --> 00:41:22.700
Yeah.

00:41:22.700 --> 00:41:28.500
Sort of one of the cute little details in hypothesis that I spent far too much time on for what it's worth.

00:41:28.500 --> 00:41:40.420
But almost any of the strategies you get out of the standard hypothesis strategies will give you a very nicely formatted wrapper that will exactly reproduce the strategy.

00:41:40.420 --> 00:41:48.920
And this is true even up to the point of if you filter by a lambda, it will give you the source code of the lambda in the wrapper for the strategy.

00:41:48.920 --> 00:41:50.240
Oh, that's great.

00:41:50.240 --> 00:41:55.060
It's all a bit ridiculous and I really don't recommend emulating it, but every time I see it, it makes me smile.

00:41:55.060 --> 00:41:56.940
Yeah, yeah, I'm sure.

00:41:58.040 --> 00:41:58.600
That's cool.

00:41:58.600 --> 00:42:13.600
So, you know, one thing that I seem to hear a lot about when I was looking into property-based testing was there seemed to be a set of like patterns that people come across that seem to repeat themselves that the property-based testing is well served.

00:42:13.600 --> 00:42:14.440
Yeah.

00:42:14.440 --> 00:42:16.700
Can you talk about some of those?

00:42:16.700 --> 00:42:20.220
Do you mean like sort of standard styles of tests that?

00:42:20.220 --> 00:42:32.240
Well, I'm thinking like one of the things people often say is a really good type of thing to send to turn this type of system onto is serialization and deserialization.

00:42:32.780 --> 00:42:36.340
Or upgrading from like a legacy code to a rewrite.

00:42:36.340 --> 00:42:43.980
You could be able to like pull an old library and always ensure that the same inputs here get the same outputs and the same thing in the new system.

00:42:43.980 --> 00:42:51.080
Or if I have a really simple algorithm, I'm optimizing using the simple slow version to verify the fast version.

00:42:51.220 --> 00:42:51.660
Things like this.

00:42:51.660 --> 00:42:52.160
Yeah, absolutely.

00:42:52.160 --> 00:43:00.540
So sort of the absolute best thing to test with property-based testing in general is these two things should always give the same answer.

00:43:00.600 --> 00:43:11.320
Because it covers such a wide range of behaviors and gives you so many opportunities to get wrong.

00:43:11.320 --> 00:43:19.780
And particularly for the sort of the optimized and naive version of an algorithm is great because often they're very different styles of algorithm.

00:43:19.780 --> 00:43:25.120
So what you're essentially testing for is have I made the same mistake on both of these things?

00:43:25.120 --> 00:43:29.420
And usually you'll make different mistakes.

00:43:29.420 --> 00:43:32.820
And so a test failure is either a bug in your optimized one or your naive one.

00:43:32.820 --> 00:43:33.200
Right.

00:43:33.200 --> 00:43:34.680
It's almost like double-check accounting.

00:43:34.680 --> 00:43:40.020
It doesn't necessarily mean your new one is wrong, but something needs to be looked at and it's who knows.

00:43:40.020 --> 00:43:40.300
Yeah.

00:43:40.300 --> 00:43:41.280
Yep.

00:43:41.280 --> 00:43:51.840
One of the things that I've been trying to do with the new-ish hypothesis website, hypothesis.works, is gather a family of these different properties.

00:43:51.840 --> 00:43:55.860
Because relatively few of them have been written down.

00:43:56.720 --> 00:43:59.420
And there's a blog post scattered across the internet with some of them.

00:43:59.420 --> 00:44:01.640
And there are a few really good prior articles.

00:44:01.640 --> 00:44:11.180
But a lot of them are either folklore, which hasn't been written down, or start with this long diatribe about category theory.

00:44:11.180 --> 00:44:11.540
Yeah.

00:44:12.040 --> 00:44:18.180
And I'm not against category theory, but I don't really use it myself.

00:44:18.180 --> 00:44:20.280
And I think it tends to scare people off.

00:44:20.680 --> 00:44:25.360
So most of the time I'm just starting with, here's a concrete problem.

00:44:25.360 --> 00:44:26.640
Let's solve it in Python.

00:44:26.640 --> 00:44:28.300
Here's how you test it with hypothesis.

00:44:28.300 --> 00:44:38.540
Oh, there is one pattern that I've noticed recently, which is either original to me or is an independent reinvention that no one else has written down before.

00:44:38.780 --> 00:44:45.400
But I really like this as a style of testing, which is rather than these two things should give the same thing.

00:44:46.040 --> 00:44:49.640
It's if I change this data, then this should move in this direction.

00:44:49.640 --> 00:44:50.360
Okay.

00:44:50.460 --> 00:45:01.060
So you generate some data, you run a function on it, and you then change the data in some way, and you run it again.

00:45:01.060 --> 00:45:07.580
And the change in the output should in some way be reflective of the change in the input.

00:45:09.160 --> 00:45:19.980
I originally came up with this for sort of optimization problems, where you run the optimizer and you make some changes, which should make the problem harder.

00:45:19.980 --> 00:45:23.720
And you assert that the score of the output doesn't get better.

00:45:23.720 --> 00:45:27.240
Or you make the problem easier and you assert that the score of the output doesn't get worse.

00:45:27.240 --> 00:45:39.060
But I also had a nice example recently with binary search, which is if you run a binary search, then you insert an extra copy of the value at the point that you search to.

00:45:39.060 --> 00:45:48.540
Then this shouldn't change the output of the binary search, because it's only sort of shifted stuff to the right.

00:45:48.540 --> 00:45:49.420
Right. Exactly.

00:45:49.420 --> 00:46:00.320
And so sort of in general, looking for things where functions should move in predictable ways and end up moving in ways that you didn't expect.

00:46:00.320 --> 00:46:01.020
Okay. Interesting.

00:46:01.020 --> 00:46:05.300
Like I increased the tax rate in my e-commerce system and the price went down.

00:46:05.300 --> 00:46:05.740
What happened?

00:46:05.740 --> 00:46:08.080
Or the price didn't change, for example.

00:46:08.200 --> 00:46:08.620
Oh, whoops.

00:46:08.620 --> 00:46:09.960
We're not actually including the tax.

00:46:09.960 --> 00:46:10.620
Who knows?

00:46:10.620 --> 00:46:11.300
Yep.

00:46:11.300 --> 00:46:11.640
Exactly.

00:46:11.640 --> 00:46:12.720
Yeah.

00:46:12.720 --> 00:46:13.440
Very interesting.

00:46:13.440 --> 00:46:19.440
So I think property-based testing and hypothesis is really exciting.

00:46:19.440 --> 00:46:24.380
I think it, in a couple of ways, I think it means that testing is more effective.

00:46:25.140 --> 00:46:28.200
And I think that it's less work to write those tests.

00:46:28.200 --> 00:46:30.680
So that's like a perfect combination.

00:46:30.680 --> 00:46:42.340
Have you done anything like, just as a proof of concept, like grab some popular open source project that has good test support and like convert its test to hypothesis tests and found new bugs or anything like that?

00:46:42.600 --> 00:46:43.600
I haven't personally.

00:46:43.600 --> 00:46:50.860
One of the problems here is that, as like with any testing, it's very hard to test a project you don't understand.

00:46:50.860 --> 00:46:57.780
So I generally don't go into other people's projects and try and write tests for them.

00:46:58.540 --> 00:47:00.140
I've done it once or twice.

00:47:00.140 --> 00:47:06.640
A customer paid me to do some testing work on Mercurial and add some tests to that, which was interesting.

00:47:06.640 --> 00:47:07.420
Did you find any bugs?

00:47:07.420 --> 00:47:07.680
Yeah.

00:47:07.800 --> 00:47:09.600
So I found a bunch of bugs, actually.

00:47:09.600 --> 00:47:09.880
Wow.

00:47:09.880 --> 00:47:12.200
None of them particularly critical.

00:47:12.200 --> 00:47:16.620
But some of the, so the encoding one came up.

00:47:16.620 --> 00:47:20.540
Mercurial has a bunch of internal encoding representations.

00:47:20.540 --> 00:47:24.640
There is, for some reason, Mercurial has three different JSON encoders.

00:47:24.640 --> 00:47:26.080
And we found bugs in one of them.

00:47:27.460 --> 00:47:37.940
And there is some stuff where Mercurial wants to represent things as UTF-8B, which is a way of taking arbitrary binary data and turning it into valid UTF-8 encoded text and backwards.

00:47:37.940 --> 00:47:40.980
And that sort of hit, that had a bunch of bugs.

00:47:40.980 --> 00:47:44.240
I don't think that's used very widely, but it still had a bunch of bugs.

00:47:44.240 --> 00:47:45.160
Yeah.

00:47:45.160 --> 00:48:00.720
And then we used the stateful testing for sort of validating repository operations and found two interesting bugs in the HDShelv extension, which is, HDShelv is basically GitStash for Mercurial.

00:48:00.720 --> 00:48:04.980
I'm sure someone will be mad at me for saying that, but that's basically what it is.

00:48:05.520 --> 00:48:13.700
And I can't remember except, I think one of them was that the set of valid shelf names was narrower than the set of valid branch names.

00:48:13.700 --> 00:48:20.040
And so you could create a branch which had a name that wasn't a valid shelf.

00:48:20.040 --> 00:48:26.420
And then when you try to shelf stuff, it would default to using the branch name for the shelf name and everything would go wrong.

00:48:27.060 --> 00:48:34.820
And the other was, it was something like if you create a file, then immediately delete it.

00:48:34.820 --> 00:48:39.680
And without committing the delete and then try to shelf stuff.

00:48:39.680 --> 00:48:41.600
No, sorry.

00:48:41.600 --> 00:48:47.500
You delete the file, then you create a new file with the same name, which is untracked.

00:48:47.500 --> 00:48:48.900
And then you try and shelf stuff.

00:48:48.900 --> 00:48:51.920
And the shelf extension gets itself into a complete testing.

00:48:51.920 --> 00:48:52.300
Wow.

00:48:52.300 --> 00:48:53.660
It seems really interesting.

00:48:53.660 --> 00:48:58.760
I'm sure lots of those types of tests were already there, but just those particular cases weren't found.

00:48:58.760 --> 00:49:04.100
There was an example, there was a talk at PyCon this year by Matt Bachman.

00:49:04.100 --> 00:49:10.740
And he talked about property-based testing on his project actually uncovered a bug in DateUtil.

00:49:10.740 --> 00:49:22.560
Because his project was depending on DateUtil and some, you know, your system sent some kind of insane date time, like, you know, before BC or something like this.

00:49:23.300 --> 00:49:24.580
And it freaked it out.

00:49:24.580 --> 00:49:24.880
Yeah.

00:49:24.880 --> 00:49:28.480
So, which they then fixed in DateUtil, which is really cool.

00:49:28.480 --> 00:49:28.880
Mm-hmm.

00:49:28.880 --> 00:49:30.000
Yeah.

00:49:30.000 --> 00:49:37.800
The, I've occasionally thought about making the hypothesis dates a little more restricted by default.

00:49:37.800 --> 00:49:45.540
Because, by and large, no one really cares about the representation of 9000 BC or 9000 AD.

00:49:45.540 --> 00:49:49.500
Because they were, they'll be using a different calendar or they were using a different calendar then anyway.

00:49:49.960 --> 00:49:51.780
But it does come up with these fun bugs.

00:49:51.780 --> 00:49:53.240
So, I sort of, I've left it in for now.

00:49:53.240 --> 00:49:53.400
Okay.

00:49:53.400 --> 00:49:53.700
Yeah.

00:49:53.700 --> 00:49:53.920
Cool.

00:49:53.920 --> 00:49:57.780
I think, I vaguely recall the bug in question.

00:49:57.780 --> 00:50:01.660
I think that one might have actually been slightly more reasonable dates.

00:50:01.660 --> 00:50:04.400
Like, I think it was first century AD that went wrong or something.

00:50:04.640 --> 00:50:11.740
I vaguely recall there being one where if you try and represent the year 99, then it assumed

00:50:11.740 --> 00:50:13.780
that you meant 1999 or something like that.

00:50:13.780 --> 00:50:14.120
Yeah.

00:50:14.120 --> 00:50:14.900
It's cool.

00:50:14.900 --> 00:50:17.140
That's what I kind of asked you about the open source projects.

00:50:17.140 --> 00:50:29.900
because I think, you know, it would be fun to take, fun in quotes, to see the results of somebody taking all the tests for, say, like the top 100 most popular PyPI packages.

00:50:29.900 --> 00:50:36.860
Go and look at their test suite and convert their example-based testing to property-based testing and just see what that spits out.

00:50:37.420 --> 00:50:37.900
Yeah.

00:50:37.900 --> 00:50:48.160
In general, what I would recommend in that space is more, if you're working on a popular Python package and you want to give that a try, pop into IRC or send me an email and I'd be very happy to help you out.

00:50:48.160 --> 00:50:48.460
Okay.

00:50:48.460 --> 00:50:58.820
Because I think, I really do think that what you need for doing that is more experience in the project than it is experience with hypothesis.

00:50:58.820 --> 00:50:59.160
Of course.

00:50:59.160 --> 00:51:02.240
Understanding the domain of what you're trying to actually test, yeah.

00:51:02.780 --> 00:51:03.060
Okay.

00:51:03.060 --> 00:51:03.460
Cool.

00:51:03.460 --> 00:51:08.940
Well, we know quite a bit about property-based testing now, but maybe tell me a little bit about yourself.

00:51:08.940 --> 00:51:13.440
What else do you do in the programming space for work and things like this?

00:51:13.440 --> 00:51:15.340
So this is more or less my day job.

00:51:15.340 --> 00:51:20.680
I've been working too actively on hypothesis in the last couple of months because I've been sort of researching some related things.

00:51:21.000 --> 00:51:33.240
But basically, I do independent R&D and testing tools and I do consulting and training around that, either helping people to use hypothesis or helping people to improve their testing in other ways.

00:51:33.240 --> 00:51:39.100
Historically, I've done more sort of back-end data engineering.

00:51:39.600 --> 00:51:47.260
But once I sort of got into hypothesis properly and found that I really liked working on this sort of thing and there was demand for it, that's mostly what I've been doing.

00:51:47.260 --> 00:51:47.520
Yeah.

00:51:47.520 --> 00:51:53.720
I can see a huge demand of companies that have large projects that have tests, but not this kind of test.

00:51:53.720 --> 00:52:01.820
They're like, you know, it might be really useful to spend two weeks with you just giving another go with a different system, right?

00:52:01.820 --> 00:52:02.360
With hypothesis.

00:52:02.360 --> 00:52:03.000
Yeah.

00:52:03.600 --> 00:52:04.780
Well, I wish there were a huge demand.

00:52:04.780 --> 00:52:06.420
There's huge demand for hypothesis.

00:52:06.420 --> 00:52:15.480
I've currently got the thing that I think most new businesses have in their first year where the best way to get new customers is to have existing customers.

00:52:15.480 --> 00:52:20.980
So it turns out that sales and marketing are hard.

00:52:20.980 --> 00:52:25.260
So right now, I would say that I'm experiencing demand, but not huge.

00:52:25.260 --> 00:52:25.360
Right.

00:52:25.360 --> 00:52:25.680
Okay.

00:52:25.680 --> 00:52:28.540
It may take a few years to get there.

00:52:29.000 --> 00:52:38.640
Yeah, but it's a cool project and I definitely can see it growing in the future because it's solving a real problem and it solves it in a better way than what we're doing today.

00:52:38.640 --> 00:52:41.140
Well, I mean, hypothesis itself is getting plenty of demand.

00:52:41.140 --> 00:52:56.940
I think PyPI stats are broken right now, but certainly when they were last working, it was getting quite a respectable number of downloads compared to projects that I'd really thought of as being much more popular than they were or much more popular than hypothesis.

00:52:56.940 --> 00:52:57.520
That's awesome.

00:52:58.100 --> 00:52:59.400
Well, yeah, congratulations on it.

00:52:59.400 --> 00:52:59.680
That's cool.

00:52:59.680 --> 00:53:00.200
Thank you.

00:53:00.200 --> 00:53:00.780
All right.

00:53:00.780 --> 00:53:01.840
We're getting near the end of the show.

00:53:01.840 --> 00:53:04.260
I have two questions I always ask my guests.

00:53:04.260 --> 00:53:11.160
First of all, if you're going to write some code, what editor, specifically Python, but in general as well, what editor do you open up?

00:53:11.160 --> 00:53:12.100
Basically, Vim.

00:53:12.100 --> 00:53:24.220
I've been experimenting with using Windows as my primary operating system recently, and I was trying PyCharm with Vim mode, but I've ended up mostly just going back to Vim even on Windows.

00:53:24.220 --> 00:53:24.800
Right.

00:53:24.800 --> 00:53:25.380
Okay, cool.

00:53:25.380 --> 00:53:31.820
And of all the PyPI packages out there, you know, what one, maybe there's, you know, there's over 80,000.

00:53:31.820 --> 00:53:38.860
There's a bunch that we all have exposure to that aren't necessarily mainstream, but you're like, wow, this is really a gem that I found that people didn't know about.

00:53:39.320 --> 00:53:45.160
In addition to hypothesis, so in addition to pip install hypothesis, which is cool, what else, what would you recommend?

00:53:45.160 --> 00:53:51.920
Most of my, I don't really have any niche packages I can recommend in that regard.

00:53:52.320 --> 00:53:57.500
I really like Py.test and coverage is an exceptionally good piece of work.

00:53:57.500 --> 00:54:03.460
Both of these are quite well known and unsurprising opinions.

00:54:04.460 --> 00:54:16.680
There's actually a package by Matt Pacman, I think, called diff cover that I keep meaning, I haven't used myself, but it looks really good for putting your CI.

00:54:17.180 --> 00:54:25.280
So when you want, say, 100% coverage, it's very hard to get to 100% coverage in a single leap.

00:54:25.280 --> 00:54:33.280
So what you do is you set ratcheting and just say, I don't care what the coverage currently is, but you can never make it worse.

00:54:33.760 --> 00:54:46.680
And so diff cover is just a nice little tool that is designed to help you configure that ratcheting on coverage or on pepH checks or things like that.

00:54:46.680 --> 00:54:47.200
Nice.

00:54:47.200 --> 00:54:52.600
So you can basically enforce the rule, like coverage always needs to get better as our project grows.

00:54:52.600 --> 00:54:53.600
Or at least never gets worse.

00:54:53.600 --> 00:54:54.740
Right.

00:54:54.740 --> 00:54:55.200
Okay.

00:54:55.200 --> 00:55:01.900
But I don't currently use that because I already have 100% coverage and play gate cloneness on all my main projects.

00:55:01.900 --> 00:55:11.380
But if I were coming into someone's existing project, which was large and had a slightly less good state, I would absolutely recommend checking something like this out.

00:55:11.380 --> 00:55:11.840
Oh, that's excellent.

00:55:11.840 --> 00:55:12.760
Thanks for the recommendation.

00:55:12.760 --> 00:55:13.560
All right.

00:55:13.560 --> 00:55:14.960
Any final call to actions?

00:55:14.960 --> 00:55:16.720
How do people get started with Hypothesis?

00:55:16.720 --> 00:55:27.540
So getting started with Hypothesis, what I would really recommend is just checking out the Hypothesis.Works website and sort of getting a feel for it, trying it out a bit.

00:55:27.880 --> 00:55:38.580
And then if you are a company who wants to improve your testing with Hypothesis, I would very strongly recommend hiring me to come in and either do a consult or run a training workshop.

00:55:39.180 --> 00:55:51.820
One of the workshops I run is basically an exploratory thing where you stick me in a room with 10 devs for a day and we figure out how to write a whole bunch of new tests for your software.

00:55:51.820 --> 00:55:57.020
And we usually find some interesting bugs, particularly in the Unicode handling.

00:55:57.020 --> 00:56:02.860
Well, most of the time we've done it, but also in other weird edge cases that you wouldn't necessarily have thought to test.

00:56:02.860 --> 00:56:03.480
Yeah, that's awesome.

00:56:03.480 --> 00:56:05.180
I'm sure you do find some weird ones.

00:56:05.180 --> 00:56:05.840
That's great.

00:56:06.400 --> 00:56:06.620
All right.

00:56:06.620 --> 00:56:08.020
Well, David, thanks so much for being on the show.

00:56:08.020 --> 00:56:08.820
It's been great to talk to you.

00:56:08.820 --> 00:56:09.620
Thank you very much, Michael.

00:56:09.620 --> 00:56:10.420
It's been a pleasure.

00:56:10.420 --> 00:56:10.860
Yeah, bye.

00:56:10.860 --> 00:56:11.200
Bye.

00:56:11.200 --> 00:56:15.440
This has been another episode of Talk Python to Me.

00:56:15.440 --> 00:56:20.240
Today's guest was David McClaver, and this episode has been sponsored by Hired and SnapCI.

00:56:20.240 --> 00:56:22.140
Thank them both for supporting the show.

00:56:22.140 --> 00:56:24.960
Hired wants to help you find your next big thing.

00:56:24.960 --> 00:56:33.480
Visit Hired.com slash Talk Python to Me to get five or more offers with salary and equity presented right up front and a special listener signing bonus of $2,000.

00:56:34.680 --> 00:56:37.360
SnapCI is modern, continuous integration and delivery.

00:56:37.360 --> 00:56:43.240
Build, test, and deploy your code directly from GitHub, all in your browser with debugging, Docker, and parallels included.

00:56:43.240 --> 00:56:46.300
Try them for free at snap.ci slash Talk Python.

00:56:46.300 --> 00:56:49.000
Are you or a colleague trying to learn Python?

00:56:49.000 --> 00:56:53.680
Have you tried books and videos that just left you bored by covering topics point by point?

00:56:53.760 --> 00:57:02.280
Well, check out my online course, Python Jumpstart, by building 10 apps at talkpython.fm/course to experience a more engaging way to learn Python.

00:57:02.280 --> 00:57:09.600
And if you're looking for something a little more advanced, try my WritePythonic code course at talkpython.fm/Pythonic.

00:57:10.280 --> 00:57:16.000
You can find the links from this episode at talkpython.fm/episodes slash show slash 67.

00:57:16.000 --> 00:57:18.140
Be sure to subscribe to the show.

00:57:18.140 --> 00:57:20.340
Open your favorite podcatcher and search for Python.

00:57:20.340 --> 00:57:21.580
We should be right at the top.

00:57:21.580 --> 00:57:24.920
You can also find the iTunes feed at /itunes.

00:57:24.920 --> 00:57:26.980
Google Play feed at /play.

00:57:27.240 --> 00:57:30.860
And direct RSS feed at /rss on talkpython.fm.

00:57:30.860 --> 00:57:35.800
Our theme music is Developers, Developers, Developers by Corey Smith, who goes by Smix.

00:57:35.800 --> 00:57:42.500
Corey just recently started selling his tracks on iTunes, so I recommend you check it out at talkpython.fm/music.

00:57:42.500 --> 00:57:47.860
You can browse his tracks he has for sale on iTunes and listen to the full-length version of the theme song.

00:57:47.860 --> 00:57:50.360
This is your host, Michael Kennedy.

00:57:50.360 --> 00:57:51.640
Thanks so much for listening.

00:57:51.640 --> 00:57:52.820
I really appreciate it.

00:57:52.820 --> 00:57:54.960
Smix, let's get out of here.

00:57:55.640 --> 00:57:58.740
Stating with my voice, there's no norm that I can feel within.

00:57:58.740 --> 00:58:01.560
Haven't been sleeping, I've been using lots of rest.

00:58:01.560 --> 00:58:04.380
I'll pass the mic back to who rocked it best.

00:58:04.380 --> 00:58:09.700
I'll pass the mic back to who rocked it best.

00:58:09.700 --> 00:58:10.840
I'll pass the mic back to who rocked it best.

00:58:10.840 --> 00:58:12.580
I'll pass the mic back to who rocked it best.

00:58:12.580 --> 00:58:13.740
I'll pass the mic back to who rocked it best.

00:58:13.740 --> 00:58:15.840
I'll pass the mic back to who rocked it best.

00:58:15.840 --> 00:58:16.120
I'll pass the mic back to who rocked it best.

00:58:16.120 --> 00:58:16.720
I'll pass the mic back to who rocked it best.

