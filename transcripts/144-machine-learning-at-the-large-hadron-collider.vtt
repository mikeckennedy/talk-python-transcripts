WEBVTT

00:00:00.001 --> 00:00:04.280
We all know Python is becoming increasingly important in both science and machine learning.

00:00:04.280 --> 00:00:07.780
This week, we journey to the very forefront of physics.

00:00:07.780 --> 00:00:11.640
You'll meet Michaela Paganini, Michael Kagan, and Matthew Feikert.

00:00:11.640 --> 00:00:15.640
They all work at the Large Hadron Collider and are using Python and machine learning

00:00:15.640 --> 00:00:18.320
to help make the next major discovery in physics.

00:00:18.320 --> 00:00:24.780
Join us this week on Talk Python to Me, episode 144, recorded December 14, 2017.

00:00:24.780 --> 00:00:43.600
Welcome to Talk Python to Me, a weekly podcast on Python, the language, the libraries, the

00:00:43.600 --> 00:00:45.120
ecosystem, and the personalities.

00:00:45.120 --> 00:00:47.240
This is your host, Michael Kennedy.

00:00:47.240 --> 00:00:49.240
Follow me on Twitter, where I'm @mkennedy.

00:00:49.240 --> 00:00:53.120
Keep up with the show and listen to past episodes at talkpython.fm.

00:00:53.120 --> 00:00:55.660
And follow the show on Twitter via at Talk Python.

00:00:55.660 --> 00:00:59.720
This episode is brought to you by Linode and Talk Python Training.

00:00:59.720 --> 00:01:03.100
Be sure to check out what the offers are for both of these segments.

00:01:03.100 --> 00:01:04.880
It really helps support the show.

00:01:04.880 --> 00:01:06.020
Hey, everyone.

00:01:06.020 --> 00:01:09.720
Before we get to the interview, I want to share a quick update about our Python courses with

00:01:09.720 --> 00:01:09.880
you.

00:01:09.880 --> 00:01:14.260
Do you work on a software team that needs training and could really use a chance to level up their

00:01:14.260 --> 00:01:14.520
Python?

00:01:14.520 --> 00:01:17.540
Maybe your entire company is looking to become more proficient.

00:01:17.540 --> 00:01:22.800
We have special offers that make our courses here at Talk Python the best option for everyone

00:01:22.800 --> 00:01:23.400
you work with.

00:01:23.400 --> 00:01:27.900
Our courses don't require an ongoing subscription like so many corporate training options do.

00:01:27.900 --> 00:01:31.200
And they're roughly priced about the same as a book.

00:01:31.200 --> 00:01:33.020
We're here to help you succeed.

00:01:33.020 --> 00:01:36.820
Send us a note at sales at talkpython.fm to start a conversation.

00:01:36.820 --> 00:01:38.600
Now, let's get to the interview.

00:01:38.600 --> 00:01:42.400
Michael, Michaela, Matthew, welcome to Talk Python.

00:01:42.400 --> 00:01:43.320
Hi, everyone.

00:01:43.320 --> 00:01:44.740
Thanks for having me.

00:01:44.740 --> 00:01:46.580
Yeah, it's an honor to be on the show.

00:01:46.580 --> 00:01:47.800
Thanks so much for inviting us.

00:01:47.800 --> 00:01:48.480
Yeah, you bet.

00:01:48.480 --> 00:01:50.980
I'm really, it's, you know, the honor is mine, to be honest.

00:01:51.020 --> 00:01:52.520
You guys are doing amazing stuff.

00:01:52.520 --> 00:01:54.160
You are pushing the boundaries of science.

00:01:54.160 --> 00:01:59.160
And I'm just really excited that you're here to share what you're working on and how that

00:01:59.160 --> 00:02:01.640
intersects with the Python space with my listeners.

00:02:01.640 --> 00:02:07.760
So we're going to talk about large Hadron Collider, particle physics, and machine learning and how

00:02:07.760 --> 00:02:09.460
those three amazing things go together.

00:02:09.620 --> 00:02:15.800
But before we get into them, let's just start real quickly with how you guys got into programming

00:02:15.800 --> 00:02:16.220
in Python.

00:02:16.220 --> 00:02:17.380
Michaela, you want to go first?

00:02:17.380 --> 00:02:17.720
Yeah.

00:02:17.720 --> 00:02:21.920
So actually, when I started programming, it certainly wasn't Python for me at first.

00:02:21.920 --> 00:02:26.140
It was IDL and MATLAB and some undergraduate physics and astronomy labs.

00:02:26.140 --> 00:02:31.860
But then when I got to CERN and I started my career as a graduate student in Atlas, that's

00:02:31.860 --> 00:02:34.120
when I first encountered Python and C++.

00:02:34.860 --> 00:02:38.380
And so just like everybody else, I guess I learned on the job.

00:02:38.380 --> 00:02:42.680
And I'm really thankful that I had great mentors that really taught me what good code and bad

00:02:42.680 --> 00:02:45.680
code looked like, such that I could start writing good code instead.

00:02:46.840 --> 00:02:52.060
Yeah, I feel like so much of what people do in programming really is learning on the job.

00:02:52.060 --> 00:02:57.080
Even for people who have degrees in computer science, you study one thing, but then you go

00:02:57.080 --> 00:02:58.560
and you actually build something different.

00:02:58.560 --> 00:03:03.180
So I think, you know, it's probably not that different, actually, than what most people went

00:03:03.180 --> 00:03:03.380
through.

00:03:03.380 --> 00:03:04.100
Yeah, absolutely.

00:03:04.100 --> 00:03:06.680
I think that's something that a lot of people could relate to.

00:03:06.680 --> 00:03:07.260
Yeah, nice.

00:03:07.260 --> 00:03:10.220
And IDL, that's the astrophysics type language.

00:03:10.220 --> 00:03:10.620
Is that right?

00:03:10.620 --> 00:03:11.700
That's correct.

00:03:11.700 --> 00:03:12.340
That's correct.

00:03:12.540 --> 00:03:15.040
Yeah, I started as an astronomer in undergraduate.

00:03:15.040 --> 00:03:18.840
And so that was my first encounter, I would say, with real coding.

00:03:18.840 --> 00:03:19.340
I see.

00:03:19.340 --> 00:03:21.560
So you started out with really tremendously large things.

00:03:21.560 --> 00:03:23.520
You're like, no, no, let's look at the really small stuff instead.

00:03:23.520 --> 00:03:25.260
Yes, it was quite a transition.

00:03:25.260 --> 00:03:26.840
All right.

00:03:26.840 --> 00:03:27.860
Michael Kagan, how about you?

00:03:27.860 --> 00:03:28.340
Yeah.

00:03:28.340 --> 00:03:32.240
So I guess my story is pretty similar to Michaela's.

00:03:32.240 --> 00:03:35.820
But so I took a few classes in undergraduate.

00:03:36.020 --> 00:03:40.360
And actually, the first language I really used on the job was Fortran, because a lot of the

00:03:40.360 --> 00:03:45.200
old physics simulation and code for doing calculations is built in Fortran.

00:03:45.200 --> 00:03:48.660
So that was kind of my first real projects.

00:03:48.660 --> 00:03:54.040
And then once I got to graduate school, everything on the modern high energy physics experiments

00:03:54.040 --> 00:03:54.740
was C++.

00:03:54.740 --> 00:03:56.520
And that's where I kind of learned on the job.

00:03:56.520 --> 00:03:59.100
And then Python is kind of used there a bit.

00:03:59.100 --> 00:04:04.840
And so I was beginning to learn Python through some of the glue scripting that's done there.

00:04:05.100 --> 00:04:09.140
And then once I kind of got into more machine learning stuff and realized how many great

00:04:09.140 --> 00:04:12.200
tools there were, then I dove a lot more into using Python.

00:04:12.200 --> 00:04:12.960
Yeah, that's cool.

00:04:12.960 --> 00:04:18.140
At CERN, a lot of the actual data processing is done in C++.

00:04:18.140 --> 00:04:22.140
But then the consumption of that is done in Python, if I understand it right.

00:04:22.140 --> 00:04:22.860
Yeah, that's right.

00:04:22.860 --> 00:04:28.280
So basically, the enormous kind of database of code that kind of crunches on the data is

00:04:28.280 --> 00:04:28.980
all in C++.

00:04:29.620 --> 00:04:36.880
And then there's a wide range of different Python scripts that help guide the set of code

00:04:36.880 --> 00:04:41.340
that helps crunch on data from one type of experimental apparatus and then from another

00:04:41.340 --> 00:04:44.580
piece of experimental apparatus and then put that information together.

00:04:44.580 --> 00:04:44.920
Nice.

00:04:44.920 --> 00:04:46.220
Matthew, how about you?

00:04:46.340 --> 00:04:46.520
Yeah.

00:04:46.520 --> 00:04:52.520
So similarly, the first time I really did any programming was in a CS 101 course at university

00:04:52.520 --> 00:04:55.920
where I learned a little bit of C and some MATLAB.

00:04:55.920 --> 00:04:58.600
And nothing was really kind of clicking then.

00:04:58.600 --> 00:05:01.240
But then I joined an undergraduate.

00:05:01.240 --> 00:05:05.400
Well, as an undergraduate, I joined the research group at the University of Illinois.

00:05:05.640 --> 00:05:08.360
And my advisor at the time was like, hey, do you want to do this stuff?

00:05:08.360 --> 00:05:09.560
And it seemed really exciting.

00:05:09.560 --> 00:05:12.140
He was telling me about his work at the Large Hadron Collider.

00:05:12.140 --> 00:05:13.400
So I said, sure.

00:05:13.400 --> 00:05:14.060
And he was like, great.

00:05:14.060 --> 00:05:16.720
And he sent me down in front of a Linux terminal.

00:05:16.720 --> 00:05:21.040
And that's when I first got introduced to Bash and C++ and things like that.

00:05:21.040 --> 00:05:25.620
Then actually, I was starting to get to see how I could use programming to actually solve

00:05:25.620 --> 00:05:26.660
problems in analysis.

00:05:26.660 --> 00:05:30.540
That's kind of when I had this aha moment and it started to click.

00:05:30.980 --> 00:05:34.760
But I really didn't start using Python much until I got to grad school.

00:05:34.760 --> 00:05:43.940
And yeah, as Michael said, once I started to get a bit more introduced to how the physics

00:05:43.940 --> 00:05:46.900
community is using machine learning, then Python.

00:05:46.900 --> 00:05:51.320
And it's a really great ecosystem for machine learning tools.

00:05:51.320 --> 00:05:57.040
That kind of became a really obvious choice for me to start to hone my Python skills.

00:05:57.240 --> 00:06:01.200
But for me, pretty much all of my programming was just learned on the job.

00:06:01.200 --> 00:06:01.860
That's really cool.

00:06:01.860 --> 00:06:06.120
I think one of the things that really draws people in once they get started with Python

00:06:06.120 --> 00:06:09.980
is all the extra libraries that you could use.

00:06:09.980 --> 00:06:11.860
Oh, you can just pip install TensorFlow.

00:06:11.860 --> 00:06:13.860
You can just do this, just do that.

00:06:13.860 --> 00:06:14.780
And it's like, wait a minute.

00:06:14.780 --> 00:06:16.260
It's just, it's all right here.

00:06:16.260 --> 00:06:20.820
That's been fantastic, especially with these like very, very quickly growing set of libraries

00:06:20.820 --> 00:06:21.340
and tools.

00:06:21.340 --> 00:06:25.640
And even in the machine learning domain, there's a new package pretty frequently that you want

00:06:25.640 --> 00:06:26.140
to try out.

00:06:26.140 --> 00:06:31.000
And yeah, pip and Kanda and these kinds of these package managers that make this so quick

00:06:31.000 --> 00:06:37.120
is really much faster development time than a lot of the stuff we're doing in C++, where

00:06:37.120 --> 00:06:38.160
a new package comes out.

00:06:38.160 --> 00:06:43.700
Given the code base is enormous, can take a relatively long amount of time just to compile

00:06:43.700 --> 00:06:44.900
any changes you want to make.

00:06:44.900 --> 00:06:46.040
Yeah, that's for sure.

00:06:46.040 --> 00:06:50.440
So it's really, I think it's a really, I think of it as one of Python's superpowers.

00:06:50.440 --> 00:06:51.220
It's really cool.

00:06:52.220 --> 00:06:56.300
Michaela, one of the things I wanted to ask you about is it feels like this open source

00:06:56.300 --> 00:07:01.940
aspect of Python fits really well with the sort of open science.

00:07:01.940 --> 00:07:04.780
You know, this research is for public consumption.

00:07:04.780 --> 00:07:10.880
A lot of stuff you can do here seems like it's much better done with an open source set of

00:07:10.880 --> 00:07:14.940
software than say like MATLAB and proprietary paid add-ons or whatever.

00:07:14.940 --> 00:07:16.600
Is that for Michael or Michaela?

00:07:16.600 --> 00:07:17.200
Michaela.

00:07:17.320 --> 00:07:17.780
Yeah, Michaela.

00:07:17.780 --> 00:07:18.660
Oh, hi.

00:07:18.660 --> 00:07:18.800
Sorry.

00:07:18.800 --> 00:07:19.280
Yes.

00:07:19.280 --> 00:07:25.920
No, I totally agree with you in the sense that it's a lot easier for us being a large

00:07:25.920 --> 00:07:32.200
collaboration to be able to share tools and be able to collaborate across so many different

00:07:32.200 --> 00:07:37.120
domains, even within physics, using these libraries that one can very simply, as you both

00:07:37.120 --> 00:07:42.400
said, install in their environment, as opposed to perhaps using something closed source, which

00:07:42.400 --> 00:07:44.180
then needs to be distributed correctly.

00:07:44.340 --> 00:07:49.920
And so I totally agree that it fits very well with the design that at least I have in

00:07:49.920 --> 00:07:53.240
mind for what our workflows should look like.

00:07:53.240 --> 00:07:56.220
And I think a lot of the other people on this podcast would agree with me.

00:07:56.220 --> 00:07:56.840
Yeah, very cool.

00:07:56.840 --> 00:08:00.260
I was going to say, I would just add, I mean, and this is something like even the experiments

00:08:00.260 --> 00:08:05.280
take quite seriously, even, you know, kind of within any language, even if there's proprietary

00:08:05.280 --> 00:08:09.460
software that could be quite useful, we're very, very hesitant about, you know, kind of engaging

00:08:09.460 --> 00:08:13.740
with it because of our inability to look inside the box and our inability to really know that

00:08:13.740 --> 00:08:14.720
it's doing what we want to do.

00:08:14.720 --> 00:08:19.100
So, which is why even within the experiment, we end up writing a lot of our own code, even

00:08:19.100 --> 00:08:23.240
if there's a proprietary solution, just because it's not really adequate for us to be able

00:08:23.240 --> 00:08:24.460
to do the research we need to do.

00:08:24.460 --> 00:08:25.260
That makes a lot of sense.

00:08:25.260 --> 00:08:25.960
Is that new?

00:08:25.960 --> 00:08:30.540
You know, is that something like 15 years ago would have people said the same thing?

00:08:30.780 --> 00:08:31.460
Yeah, absolutely.

00:08:31.460 --> 00:08:35.020
Because a lot of these decisions that I'm even thinking about are about packages from

00:08:35.020 --> 00:08:39.860
15 years ago at the beginning of the experiment where there's even a nice neural network package

00:08:39.860 --> 00:08:43.960
that from 10 years ago that was decided not to really be used because it was proprietary

00:08:43.960 --> 00:08:44.540
at the time.

00:08:44.540 --> 00:08:44.760
Okay.

00:08:44.760 --> 00:08:46.140
Yeah, very interesting.

00:08:46.140 --> 00:08:51.700
I would say that those decisions were even more important back then than they are right now.

00:08:51.700 --> 00:08:57.680
I think these days, like we see more adoption of standard tools, still open source tools, but

00:08:57.680 --> 00:09:02.060
a lot of the standard tools from industry, whereas I would say that back in the days, maybe

00:09:02.060 --> 00:09:06.860
like decades ago at CERN, we would tend to really customize every single piece of code

00:09:06.860 --> 00:09:09.140
and write it ourselves.

00:09:09.140 --> 00:09:10.440
I guess you're probably right.

00:09:10.440 --> 00:09:15.140
That's an interesting point because today we're swimming in open source software and these ideas

00:09:15.140 --> 00:09:16.720
and it kind of seems more accepted.

00:09:16.720 --> 00:09:21.920
So yeah, it's even more important to have that sort of early when it wasn't so obvious.

00:09:21.920 --> 00:09:22.560
Nice.

00:09:22.560 --> 00:09:25.400
So let's talk about what you guys each do day today.

00:09:25.400 --> 00:09:27.380
You all are doing such amazing stuff.

00:09:27.380 --> 00:09:31.020
You're all involved in Atlas to some degree.

00:09:31.020 --> 00:09:34.520
And we'll talk about the Large Hadron Collider a little bit, but maybe you could just touch

00:09:34.520 --> 00:09:35.300
on what that is.

00:09:35.300 --> 00:09:36.820
So maybe Matthew, let's start with you.

00:09:36.820 --> 00:09:37.420
Yeah, sure.

00:09:37.420 --> 00:09:44.100
So I'm a graduate student at Southern Methodist University in the United States, but I'm stationed

00:09:44.100 --> 00:09:44.880
over at CERN.

00:09:44.880 --> 00:09:46.980
And so like you said, I work on Atlas.

00:09:46.980 --> 00:09:51.880
So as a graduate student, in some sense, you could say like one of my main responsibilities

00:09:51.880 --> 00:09:53.260
is to make plots.

00:09:53.260 --> 00:09:58.760
But I mean, what I mean by that is that I'm both like learning how to do analyses and then

00:09:58.760 --> 00:10:02.100
I'm actually one of the people who's kind of going in and writing the code.

00:10:02.800 --> 00:10:09.940
And so I do work on a specific analysis that's looking at trying to measure specific properties

00:10:09.940 --> 00:10:16.060
of the Higgs boson given a specific decay channel that it might have or that it does have.

00:10:16.280 --> 00:10:22.280
And then the way that I'm kind of doing this right now is actually by using Jupyter notebooks.

00:10:22.280 --> 00:10:30.100
So I can actually go in and use some of the great Python tools like Keres to be able to

00:10:30.100 --> 00:10:36.200
interact with the data and actually write some neural networks and actually try and do some

00:10:36.200 --> 00:10:37.480
exploratory data analysis.

00:10:38.020 --> 00:10:42.040
So in addition to that, I also do some operations work on Atlas.

00:10:42.040 --> 00:10:44.480
So I work on something that's called our trigger system.

00:10:44.480 --> 00:10:47.500
We can talk a bit more about that if there's time.

00:10:47.500 --> 00:10:51.320
Yeah, the trigger system is really amazing and critical as well, right?

00:10:51.320 --> 00:10:51.780
Yeah.

00:10:51.780 --> 00:10:52.460
Yeah.

00:10:52.460 --> 00:10:53.960
We'll definitely talk about that.

00:10:53.960 --> 00:10:54.220
Yeah.

00:10:54.220 --> 00:11:01.300
So there I'm basically just writing a combination of C++ and Python to try and do optimization studies

00:11:01.300 --> 00:11:03.500
and provide support for the trigger system.

00:11:03.500 --> 00:11:04.140
Okay, cool.

00:11:04.140 --> 00:11:05.260
How about you, Michaela?

00:11:05.260 --> 00:11:06.380
What do you do day to day?

00:11:06.500 --> 00:11:08.180
What are you doing on all these experiments?

00:11:08.180 --> 00:11:08.920
Yeah, okay.

00:11:08.920 --> 00:11:14.320
So, well, these days I always joke about this, but what I spend most of my time on is preparing

00:11:14.320 --> 00:11:18.080
talks and posters for conferences and workshops and interviews and all of that.

00:11:18.080 --> 00:11:23.440
But I guess my average day as a PhD student in Atlas consists primarily of three things,

00:11:23.440 --> 00:11:23.880
I would say.

00:11:23.880 --> 00:11:25.860
First of all, training neural networks.

00:11:25.860 --> 00:11:31.360
And then while those are training, I contribute to the experiments or my analysis code base.

00:11:31.540 --> 00:11:36.760
I read lots of papers on the archive, but I would say certainly roughly 90% of my time

00:11:36.760 --> 00:11:39.300
is spent on coding and documenting the code.

00:11:39.360 --> 00:11:43.800
And I'm also involved in various reconstruction and analysis groups.

00:11:43.800 --> 00:11:50.080
And my goal personally is to bring better algorithmic thinking to the table to help identify bottom

00:11:50.080 --> 00:11:55.800
quarks or pairs of Higgs bosons in my case or any other particle that we might be interested

00:11:55.800 --> 00:11:57.120
in at the Large Hadron Collider.

00:11:57.360 --> 00:12:02.640
I can go into more detail if you want about some of the work that I'm doing with neural networks.

00:12:02.640 --> 00:12:08.240
Specifically, I'm working on speeding up a part of our simulation that is very competentially

00:12:08.240 --> 00:12:08.860
intensive.

00:12:08.860 --> 00:12:16.060
And my idea is to use generative adversarial networks to have a higher accuracy, but at the

00:12:16.060 --> 00:12:18.940
same time, a faster simulator that is powered by deep learning.

00:12:19.140 --> 00:12:20.800
That is really awesome.

00:12:20.800 --> 00:12:22.440
And I do want to ask you more about that.

00:12:22.440 --> 00:12:26.820
But one question that came to mind while you're describing that is, you say you spend 90% of

00:12:26.820 --> 00:12:28.700
your time writing code.

00:12:28.700 --> 00:12:33.180
When you got into physics, did you think, and you thought about, what am I going to do as

00:12:33.180 --> 00:12:35.160
a physicist once I'm done with the books?

00:12:35.160 --> 00:12:37.600
Like, is that what you actually saw yourself doing?

00:12:37.600 --> 00:12:38.660
Absolutely not.

00:12:38.800 --> 00:12:41.600
I was not ready for that at first.

00:12:41.600 --> 00:12:42.840
It came as a surprise.

00:12:42.840 --> 00:12:44.380
It was a pleasant surprise, I have to say.

00:12:44.380 --> 00:12:49.860
It turned out that I love coding a lot more, perhaps, than I like more traditional ideas

00:12:49.860 --> 00:12:50.460
of physics.

00:12:50.460 --> 00:12:58.360
But it was a transition, let's say, because not much of our coursework prepares us for what

00:12:58.360 --> 00:13:02.520
the reality of the work of a graduate student in an experiment like Atlas is really like.

00:13:02.520 --> 00:13:04.260
And as I said, the majority of it is coding.

00:13:04.260 --> 00:13:04.920
Yeah, for sure.

00:13:04.920 --> 00:13:08.940
I didn't take any graduate classes in physics, but I took a number of high-level ones.

00:13:08.940 --> 00:13:11.800
And I don't remember doing hardly any coding for them.

00:13:11.800 --> 00:13:15.480
It was all pen, paper, you know, prove this, prove that.

00:13:15.480 --> 00:13:18.760
A lot of equations, not much actual software.

00:13:18.760 --> 00:13:24.020
I would certainly advocate that some of the curriculums should be probably updated to reflect

00:13:24.020 --> 00:13:28.740
what the real life of a graduate student in experiments in high-energy physics looks like.

00:13:28.740 --> 00:13:31.120
Yeah, that makes a lot of sense.

00:13:31.120 --> 00:13:32.140
All right, Michael, how about you?

00:13:32.140 --> 00:13:38.500
I'm a research scientist at Slack without the K. So that's the Stanford Linear Accelerator Center.

00:13:38.500 --> 00:13:41.580
Not the chat thing, but the really fast thing at Stanford, right?

00:13:41.580 --> 00:13:42.460
Exactly. Yeah.

00:13:42.460 --> 00:13:44.740
So it's a linear accelerator center.

00:13:44.740 --> 00:13:49.040
It's a kind of one of the DOE, Department of Energy National Laboratories, that's run by

00:13:49.040 --> 00:13:49.380
Stanford.

00:13:49.380 --> 00:13:52.480
And actually, most of what they...

00:13:52.480 --> 00:13:54.940
This used to be a high-energy physics lab, primarily.

00:13:55.280 --> 00:13:59.720
And that's kind of been turned into basically a big X-ray laser, a free electronic laser.

00:13:59.720 --> 00:14:01.840
But anyways, I don't really work on that stuff.

00:14:01.840 --> 00:14:03.820
I'm part of the team that works on Atlas.

00:14:03.820 --> 00:14:10.760
And so kind of as a research scientist, you know, we work in Atlas, which is this 3,000-person

00:14:10.760 --> 00:14:11.260
collaboration.

00:14:11.260 --> 00:14:14.380
And, you know, each of us kind of work for our own institutions.

00:14:14.380 --> 00:14:20.180
And so one aspect of that is with 3,000 people trying to, you know, run, improve and run a

00:14:20.180 --> 00:14:25.260
large piece of equipment and then do all this data analysis is we have to organize.

00:14:25.620 --> 00:14:31.560
So I found myself now in kind of a phase of my career where, you know, I'm more and more

00:14:31.560 --> 00:14:32.800
a part of that organization.

00:14:32.800 --> 00:14:38.100
So I'm helping to run one of the groups which looks at a certain kind of particle, which we

00:14:38.100 --> 00:14:40.960
might find in our detectors, which is called a bottom quark.

00:14:40.960 --> 00:14:46.400
And we basically develop algorithms to find those particles in the detector.

00:14:46.400 --> 00:14:52.280
And so once we find them, we can then give those algorithms out, supply them to any other

00:14:52.280 --> 00:14:56.020
analyzer on the experiment who wants to, you know, look at some data and find out, you

00:14:56.020 --> 00:14:59.760
know, in a given collision, how many bottom quarks were there in that collision.

00:14:59.760 --> 00:15:04.320
So that's, I spend a lot of my time kind of running that group, which is kind of maybe 50

00:15:04.320 --> 00:15:09.100
or 60 people kind of organized together, working together to get that moving forward and working.

00:15:09.100 --> 00:15:15.260
And then, you know, with the free time that I have left, you know, working with kind of

00:15:15.260 --> 00:15:17.480
postdocs and grad students on data analysis.

00:15:17.480 --> 00:15:21.820
I mean, I've worked both with Michaela and Matthew on various data analysis projects.

00:15:21.820 --> 00:15:27.440
And then also on exploring how we might take some new ideas in machine learning or even

00:15:27.440 --> 00:15:30.840
develop some when needed to solve some of our specific tasks.

00:15:30.840 --> 00:15:32.900
Yeah, that sounds really, really interesting.

00:15:32.900 --> 00:15:39.960
And you're using some machine learning and those types of algorithms to create these techniques

00:15:39.960 --> 00:15:41.400
for discovering these bottom quarks?

00:15:41.400 --> 00:15:42.140
Yeah, absolutely.

00:15:42.140 --> 00:15:46.940
So kind of machine learning, you know, machine learning is, you know, has been around for a

00:15:46.940 --> 00:15:47.140
while.

00:15:47.140 --> 00:15:51.140
And especially, you know, so we've had algorithms that that kind of work.

00:15:51.140 --> 00:15:55.200
And I guess this is probably a pretty, you know, still a very common and powerful paradigm,

00:15:55.200 --> 00:15:57.140
which is we look at our data.

00:15:57.140 --> 00:16:01.940
And, you know, based on our domain knowledge, we can we can develop all sorts of features

00:16:01.940 --> 00:16:07.240
or all sorts of use all sorts of algorithms to say, OK, well, this looks like the properties

00:16:07.240 --> 00:16:08.040
of a bottom quark.

00:16:08.040 --> 00:16:11.920
And we can compute all these features and then run them, train machine learning algorithms to,

00:16:11.920 --> 00:16:16.080
you know, for instance, classify whether this set of data really was a bottom quark or not.

00:16:16.220 --> 00:16:18.040
And so that's been around for a while.

00:16:18.040 --> 00:16:21.860
And one of the things we're also working on then is saying, OK, well, let's if we can take

00:16:21.860 --> 00:16:26.660
a step back and look at this data and see if, you know, if we can think about it in different

00:16:26.660 --> 00:16:31.920
ways, sometimes that maps on to problems like problems in vision or even natural language

00:16:31.920 --> 00:16:36.320
processing, where we can then start to use some of these super modern techniques, you know,

00:16:36.320 --> 00:16:40.800
coming from things like deep learning so that, you know, we can even improve our classification

00:16:40.800 --> 00:16:45.900
or in some cases we're doing regression or even generative type problems.

00:16:45.900 --> 00:16:47.720
So there's a lot of work.

00:16:47.720 --> 00:16:49.000
Yeah, you basically are working.

00:16:49.000 --> 00:16:51.900
Yeah, you're working with a huge camera, basically.

00:16:51.900 --> 00:16:52.540
Exactly.

00:16:52.540 --> 00:16:53.460
With Atlas.

00:16:53.460 --> 00:16:59.480
Yeah, it's it's it is a keep thinking of it in meters, but it's like a 75, 75 foot tall and

00:16:59.480 --> 00:17:04.340
100 and 120 foot long detector that sits 100 meters underground.

00:17:04.820 --> 00:17:07.980
And it's built of like many different kinds of cameras.

00:17:07.980 --> 00:17:11.780
And so we have to kind of take that and these different kinds of cameras detect different

00:17:11.780 --> 00:17:13.040
kinds of particles.

00:17:13.040 --> 00:17:16.660
And we take all that information together to build a picture of what happened every time

00:17:16.660 --> 00:17:17.140
we collide.

00:17:17.140 --> 00:17:17.320
Yeah.

00:17:17.320 --> 00:17:23.260
So maybe that's a great place to segue into just like a really quick summary of this particle

00:17:23.260 --> 00:17:24.120
physics stuff.

00:17:24.320 --> 00:17:27.240
So Matthew, maybe we'll start let you start this off.

00:17:27.240 --> 00:17:34.140
You know, we were told I was told in, I don't know, fifth grade or something that atoms, that's

00:17:34.140 --> 00:17:36.640
the smallest stuff that everything is made of.

00:17:36.640 --> 00:17:36.860
Right.

00:17:36.860 --> 00:17:38.220
But, you know, not so much.

00:17:38.220 --> 00:17:38.440
Right.

00:17:38.440 --> 00:17:39.180
Tell us about it.

00:17:39.260 --> 00:17:39.420
Yeah.

00:17:39.420 --> 00:17:44.940
So we I think everyone kind of has this idea going through like schooling that, you know,

00:17:44.940 --> 00:17:49.000
you have the periodic table where you have like your atoms that are made of protons and

00:17:49.000 --> 00:17:50.120
neutrons and electrons.

00:17:50.120 --> 00:17:51.120
And that's it.

00:17:51.120 --> 00:17:51.340
Right.

00:17:51.340 --> 00:17:52.580
Well, yeah.

00:17:52.580 --> 00:17:54.720
So it turns out that's not really the whole story.

00:17:54.720 --> 00:18:00.900
And that while electrons, as it turns out, do seem to be fundamental particles, protons

00:18:00.900 --> 00:18:06.060
and neutrons, these are these are actually composite particles that are made of even more

00:18:06.060 --> 00:18:08.360
fundamental particles that we call quarks.

00:18:08.700 --> 00:18:13.020
And so and there's also gluons in there, which are other subatomic particles.

00:18:13.020 --> 00:18:17.420
So it turns out that there's kind of this whole particle zoo, if you will.

00:18:17.420 --> 00:18:23.200
But kind of the amazing thing is that when we go and kind of explore the world and and

00:18:23.200 --> 00:18:29.160
actually look, it turns out that there's 12 matter type particles in the world that there's

00:18:29.160 --> 00:18:32.780
six of these quark particles that make up the protons and neutrons.

00:18:32.780 --> 00:18:36.640
We call these things protons and neutrons more generally hadrons.

00:18:36.640 --> 00:18:42.360
And then there's things like the electron that are called we call leptons.

00:18:42.360 --> 00:18:48.020
And there's both electrically charged things, electrically charged leptons like the like the

00:18:48.020 --> 00:18:48.440
electron.

00:18:48.440 --> 00:18:54.420
And then there's also kind of their ghostly neutral cousins that hardly even interact with

00:18:54.420 --> 00:18:55.060
matter at all.

00:18:55.420 --> 00:18:59.480
So I think maybe Michaela and Michael can talk about the fundamental forces and other

00:18:59.480 --> 00:18:59.860
things.

00:18:59.860 --> 00:19:00.420
Yeah, sure.

00:19:00.420 --> 00:19:01.460
Michaela, take it away.

00:19:01.460 --> 00:19:02.940
Yeah, of course.

00:19:02.940 --> 00:19:07.960
But on top of all of these matter particles that Matthew just described, we also have what

00:19:07.960 --> 00:19:11.980
we call the force carriers or gauge bosons in a way.

00:19:12.520 --> 00:19:17.180
And these are particles that can be thought of as being exchanged among other fundamental

00:19:17.180 --> 00:19:22.560
particles to mediate some of the forces and attractions that connect these that are more

00:19:22.560 --> 00:19:23.580
fundamental particles.

00:19:23.580 --> 00:19:29.740
And so we can think of the photon, for example, as being the force carrier for electromagnetism.

00:19:29.740 --> 00:19:35.340
And then I think to complete the puzzle, we have the most recently discovered particle

00:19:35.340 --> 00:19:38.660
for the standard model of particle physics, which is the Higgs boson.

00:19:38.660 --> 00:19:43.800
And by the way, the standard model of particle physics is just this great theory that we've

00:19:43.800 --> 00:19:49.720
come up with over the years that kind of puts all of our fundamental particles in this periodic

00:19:49.720 --> 00:19:52.820
table that Matthew and I have just tried to describe to you.

00:19:52.880 --> 00:19:56.880
So there is a little bit of an analogy maybe to chemistry, but at even more fundamental

00:19:56.880 --> 00:19:58.760
level than the atom itself.

00:19:58.760 --> 00:19:59.120
Yeah.

00:19:59.120 --> 00:20:02.820
And it's really amazing that this was created somewhat theoretically.

00:20:02.820 --> 00:20:07.020
And then the machine to go find things like the Higgs boson was built.

00:20:07.020 --> 00:20:08.620
And then it really was there.

00:20:08.620 --> 00:20:09.200
That's true.

00:20:09.200 --> 00:20:10.600
It's very fascinating.

00:20:10.600 --> 00:20:15.000
I think at a certain point in history, experiment was ahead of theory.

00:20:15.000 --> 00:20:18.680
And then theory surpassed experiment once again.

00:20:18.680 --> 00:20:22.340
And so it's a very fascinating field to be in.

00:20:22.460 --> 00:20:26.660
And I think at different historical moments, things were very different from what they look

00:20:26.660 --> 00:20:27.180
like today.

00:20:27.180 --> 00:20:27.480
Yeah.

00:20:27.480 --> 00:20:33.680
How much do you think that comes from people just getting better at theory versus computational

00:20:33.680 --> 00:20:36.260
techniques assisting theory?

00:20:36.260 --> 00:20:43.840
I think it's probably part of what you said, but as well as the energy regimes that we are

00:20:43.840 --> 00:20:44.860
trying to probe now.

00:20:44.860 --> 00:20:50.880
So I think most particle physicists would agree that in terms of the energies that we were able

00:20:50.880 --> 00:20:56.620
to probe right now, we think we have a good understanding of all of the particles that could exist there.

00:20:56.620 --> 00:21:01.900
But what we're really searching for right now is something that is an even higher energy.

00:21:01.900 --> 00:21:03.880
And that's the main issue.

00:21:03.880 --> 00:21:09.300
At that point, the complication is not so much whether the theory is there or not, but it's

00:21:09.300 --> 00:21:12.080
being able to produce machines.

00:21:12.080 --> 00:21:16.260
So it's the hardware technology even to go search for these particles.

00:21:16.260 --> 00:21:20.880
And I think certainly software will help us get the most out of the hardware that we currently

00:21:20.880 --> 00:21:24.080
have and the next hardware that we will build in the future generations.

00:21:24.080 --> 00:21:27.280
But it's both hardware and software, in my opinion.

00:21:27.280 --> 00:21:27.720
Yeah.

00:21:27.720 --> 00:21:28.200
Okay.

00:21:28.200 --> 00:21:28.860
Really cool.

00:21:28.860 --> 00:21:29.800
All right.

00:21:29.800 --> 00:21:32.060
Maybe the last one on this physics intro stuff.

00:21:33.060 --> 00:21:38.160
Michael, what was the big deal about finding the Higgs boson and what did people learn from

00:21:38.160 --> 00:21:38.340
it?

00:21:38.340 --> 00:21:39.260
Yeah, absolutely.

00:21:39.260 --> 00:21:43.740
The Higgs boson is probably the hardest of the particles to describe in some ways.

00:21:43.740 --> 00:21:49.540
So it sounds enormous, but the job of the Higgs boson is many things.

00:21:49.540 --> 00:21:54.140
And it's probably the easiest way to explain is it gives mass to all the other particles.

00:21:54.200 --> 00:22:00.700
And so the way you can think about that is as particles move around, they bump into Higgs

00:22:00.700 --> 00:22:01.200
bosons.

00:22:01.200 --> 00:22:06.160
And the Higgs bosons kind of slow them down and effectively give them mass.

00:22:06.160 --> 00:22:10.020
Kind of like if you're trying to walk through water, it takes a lot more force or a lot more

00:22:10.020 --> 00:22:11.800
effort to kind of move your body.

00:22:11.800 --> 00:22:14.460
And that's the same way that's kind of happening with the particles.

00:22:14.460 --> 00:22:15.980
This is an imperfect analogy.

00:22:15.980 --> 00:22:19.040
So if any theoretical physicists are listening, I apologize.

00:22:19.040 --> 00:22:20.940
That's roughly the idea.

00:22:21.260 --> 00:22:26.440
But it turns out the Higgs boson really played a fundamental role in making this theory that

00:22:26.440 --> 00:22:30.860
Michaela and Matthew explain, which is incredibly predictive, maybe the most predictive theory

00:22:30.860 --> 00:22:32.500
ever, kind of makes sense.

00:22:32.500 --> 00:22:37.700
Without the Higgs boson, the theory effectively predicts things that have probabilities larger

00:22:37.700 --> 00:22:39.580
than one, which we knew didn't make any sense.

00:22:39.580 --> 00:22:44.040
And that's kind of a little bit how you were saying that that knowledge of the theory breaking

00:22:44.040 --> 00:22:50.640
down really helped drive what became a 40 or 50 year long experimental search.

00:22:50.640 --> 00:22:54.580
Yeah, it's amazing how people get excited when they're wrong.

00:22:54.580 --> 00:22:56.940
It's like, oh, the theory might be wrong.

00:22:56.940 --> 00:22:57.960
It would be so exciting.

00:22:57.960 --> 00:22:59.260
Yeah, absolutely.

00:22:59.260 --> 00:23:03.300
I think, you know, that's every time something doesn't make sense, the theoretical physicists

00:23:03.300 --> 00:23:06.480
get incredibly excited that they're going to have to come up with a completely new theory.

00:23:06.480 --> 00:23:06.920
Yeah.

00:23:06.920 --> 00:23:09.960
And I don't think we're done with that at all.

00:23:09.960 --> 00:23:10.720
That's cool.

00:23:10.720 --> 00:23:15.080
So if people are out there listening and they really want to get a sense for what's going

00:23:15.080 --> 00:23:20.980
on LHC and particle physics, I definitely want to recommend the documentary Particle Fever,

00:23:20.980 --> 00:23:22.520
which I think is available on Netflix.

00:23:22.520 --> 00:23:24.660
But I'll link to at least the trailer.

00:23:24.660 --> 00:23:28.940
And there's a book called Present at Creation, Discovering the Higgs boson, which is great.

00:23:28.940 --> 00:23:32.240
And then there's another one, We Have No Idea, A Guide to the Unknown Universe.

00:23:32.240 --> 00:23:33.140
Who recommended that one?

00:23:33.380 --> 00:23:34.300
Yeah, so I recommended that.

00:23:34.300 --> 00:23:41.220
That's actually co-written by our Atlas colleague, Daniel Whiteson, and then also the famous PhD

00:23:41.220 --> 00:23:43.040
comics cartoonist, Jorge Sham.

00:23:43.040 --> 00:23:48.980
I really like that book because I think it's both like a celebration of how much we still

00:23:48.980 --> 00:23:54.200
don't know about the universe and how now is really a great time to get into science because

00:23:54.200 --> 00:23:56.500
we're truly in an age of discovery.

00:23:56.500 --> 00:24:00.260
But it also talks about just how much we do know as well.

00:24:00.360 --> 00:24:03.180
So I think it's both, it's just a great celebration of science.

00:24:03.180 --> 00:24:08.560
And it also, given that it's co-written by a physicist, it really does convey ideas really

00:24:08.560 --> 00:24:08.860
well.

00:24:08.860 --> 00:24:09.480
Yeah, excellent.

00:24:09.480 --> 00:24:11.360
Yeah, so people can check all three of those things out.

00:24:11.360 --> 00:24:13.420
They're really good background information.

00:24:13.420 --> 00:24:18.860
This portion of Talk Python to me is brought to you by Linode.

00:24:18.860 --> 00:24:22.900
Are you looking for bulletproof hosting that's fast, simple, and incredibly affordable?

00:24:22.900 --> 00:24:29.060
Look past that bookstore and check out Linode at talkpython.fm/Linode, L-I-N-O-D-E.

00:24:29.540 --> 00:24:33.380
Plans start at just $5 a month for a dedicated server with a gig of RAM.

00:24:33.380 --> 00:24:35.600
They have 10 data centers across the globe.

00:24:35.600 --> 00:24:38.320
So no matter where you are, there's a data center near you.

00:24:38.320 --> 00:24:42.400
Whether you want to run your Python web app, host a private Git server, or a file server,

00:24:42.400 --> 00:24:48.620
you'll get native SSDs on all machines, a newly upgraded 200 gigabit network, and 24-7 friendly

00:24:48.620 --> 00:24:51.640
support, even on holidays, and a seven-day money-back guarantee.

00:24:51.640 --> 00:24:55.140
Want a dedicated server for free for the next four months?

00:24:55.400 --> 00:24:59.480
Use the coupon code python17 at talkpython.fm/Linode.

00:25:00.520 --> 00:25:03.820
So let's catch up on the LHC just a little bit.

00:25:03.820 --> 00:25:07.360
Michaela, maybe could you just give people a sense of the scale?

00:25:07.360 --> 00:25:12.060
Like Michael said, there are 3,000 people working on Atlas, and Atlas is just part of one of the

00:25:12.060 --> 00:25:12.640
experiments.

00:25:12.640 --> 00:25:14.780
Give us a sense of what this place is like.

00:25:14.780 --> 00:25:15.540
Yeah, absolutely.

00:25:15.540 --> 00:25:16.700
I really love CERN.

00:25:16.700 --> 00:25:20.220
It's a group of fantastic people, some of the brightest minds in the world.

00:25:20.360 --> 00:25:25.800
And it brings together researchers and engineers and computer scientists from all around the

00:25:25.800 --> 00:25:28.180
world, probably hundreds of different nationalities.

00:25:28.180 --> 00:25:32.800
And one of my favorite thing about it is that I can absolutely say that every single time I

00:25:32.800 --> 00:25:37.240
am there, I'm never hanging out with more than one or two people from the same nationality.

00:25:37.240 --> 00:25:39.440
So to me, that's the best thing about it.

00:25:39.760 --> 00:25:45.080
Again, there's probably tens of thousands of people across the various different experiments.

00:25:45.080 --> 00:25:51.860
We've been talking a lot about the LHC, a large hadron collider that hosts four experiments.

00:25:51.860 --> 00:25:56.120
So not only Atlas, but also LHC-B, ELISE, and CMS.

00:25:56.120 --> 00:26:02.280
But then again, the large hadron collider is only one small part of the entirety of CERN.

00:26:02.400 --> 00:26:04.760
There are a lot of other experiments going on.

00:26:04.760 --> 00:26:11.000
For example, some antimatter experiments at the antiproton decelerator, as well as even

00:26:11.000 --> 00:26:13.280
astronomy experiments, as far as I know.

00:26:13.280 --> 00:26:19.680
So it's a large laboratory that spans across two different countries at the intersection,

00:26:19.680 --> 00:26:22.340
at the border, basically between France and Switzerland.

00:26:22.340 --> 00:26:24.220
So fantastic place to work at.

00:26:24.220 --> 00:26:25.840
Yeah, it sounds really, really cool.

00:26:25.840 --> 00:26:27.160
And people can go tour it, right?

00:26:27.160 --> 00:26:27.960
They can set up a tour?

00:26:27.960 --> 00:26:29.100
Absolutely, yes.

00:26:29.300 --> 00:26:36.300
Anybody can just show up and do the quick tour of, for example, Point One, which is where

00:26:36.300 --> 00:26:37.760
Atlas is located.

00:26:37.760 --> 00:26:42.480
So you can visit our control room and learn more about our experiment.

00:26:42.480 --> 00:26:47.520
And if you're lucky enough to be able to visit during a shutdown period, so oftentimes in the

00:26:47.520 --> 00:26:49.960
winter, that's when we have quick shutdowns.

00:26:49.960 --> 00:26:54.600
You might even be able to go underground and visit the actual experiment, which is absolutely

00:26:54.600 --> 00:26:55.140
breathtaking.

00:26:55.140 --> 00:26:56.500
Yeah, I'm sure that it is.

00:26:56.540 --> 00:26:59.380
Just the scale from the pictures, it looks like, is just incredible.

00:26:59.380 --> 00:26:59.700
Yeah.

00:26:59.700 --> 00:27:05.040
I just want to jump in and just make an additional comment about what Michaela said, in the sense

00:27:05.040 --> 00:27:07.540
that CERN really is an open laboratory.

00:27:07.540 --> 00:27:12.740
And it's a big part of CERN's mission to make sure that the scientific discoveries that are

00:27:12.740 --> 00:27:14.480
made there are made for all of humanity.

00:27:14.480 --> 00:27:21.320
So CERN really welcomes the public getting involved and being curious and coming and asking

00:27:21.320 --> 00:27:21.780
questions.

00:27:22.040 --> 00:27:27.220
So yeah, if you're ever traveling through nearby Geneva, Switzerland, sign up for a tour.

00:27:27.220 --> 00:27:27.900
Come visit.

00:27:27.900 --> 00:27:29.120
Yeah, that sounds great.

00:27:29.120 --> 00:27:36.840
Michael, the data that flows through these experiments out to the collectors and then into the trigger

00:27:36.840 --> 00:27:42.300
that Matthew mentioned, and then on to the larger computing structures that are there is

00:27:42.300 --> 00:27:43.080
pretty insane.

00:27:43.080 --> 00:27:45.980
Do you want to give us like a kind of an overview of the scale of data?

00:27:46.140 --> 00:27:46.760
Yeah, absolutely.

00:27:46.760 --> 00:27:51.760
The way that kind of the LHC works is, well, I'll give you a little bit of the physics

00:27:51.760 --> 00:27:54.220
background about why we have to design the systems this way.

00:27:54.220 --> 00:27:57.580
So a lot of the things we're searching for are very rare.

00:27:57.580 --> 00:28:00.080
And the physics that we deal with is probabilistic.

00:28:00.260 --> 00:28:04.840
So we might be looking for something that's interesting that only happens in one out of a trillion

00:28:04.840 --> 00:28:06.600
collisions or maybe even less frequently.

00:28:06.600 --> 00:28:10.440
And so we have to collide protons as many times as possible.

00:28:10.440 --> 00:28:13.420
So we collide protons 40 million times a second.

00:28:13.420 --> 00:28:20.820
And those collisions fly out into the massive detectors that Michaela was describing or that

00:28:20.820 --> 00:28:22.260
we've been discussing.

00:28:22.480 --> 00:28:25.240
So the thing is, we can't record all that data.

00:28:25.240 --> 00:28:29.480
So we can only record a fraction of that data because it would simply be too much.

00:28:29.480 --> 00:28:34.840
So we have a set of systems called the trigger, which allows us to go from 40 million collisions,

00:28:34.840 --> 00:28:40.260
of which many are not super interesting, down to about kilohertz, so about 1,000 a second.

00:28:40.260 --> 00:28:45.580
And each of those collisions, the data that comes out of the detector, I think it's about a megabyte

00:28:45.580 --> 00:28:46.180
per collision.

00:28:46.180 --> 00:28:49.320
So that means we're taking about a gigabyte of data per second.

00:28:49.320 --> 00:28:53.960
That's the data that made it through the trigger that was not discarded by the hardware, right?

00:28:53.960 --> 00:28:54.660
Exactly.

00:28:54.660 --> 00:28:56.700
That's just the data that made it through the detector.

00:28:56.700 --> 00:29:03.600
And the actual processing there is a combination of custom-built hardware and FPGAs, which are

00:29:03.600 --> 00:29:07.980
fast enough to deal with looking at the data really quickly at 40 million times a second and

00:29:07.980 --> 00:29:15.900
helping us pipeline it down through various both hardware and software systems down to this

00:29:15.900 --> 00:29:16.980
kind of kilohertz rate.

00:29:16.980 --> 00:29:18.820
So we don't run the detector all the time.

00:29:18.820 --> 00:29:19.780
We run it a lot.

00:29:19.780 --> 00:29:20.780
There's shutdown periods.

00:29:20.780 --> 00:29:22.940
There's times when you have to kind of refill the beam.

00:29:22.940 --> 00:29:28.820
And I think we accumulate something like, what is it, like three or four petabytes of data

00:29:28.820 --> 00:29:29.220
a year.

00:29:29.220 --> 00:29:30.280
Yeah, that's just crazy.

00:29:30.280 --> 00:29:33.480
And then it's not just there in Geneva.

00:29:33.480 --> 00:29:35.520
It's also broadcast out, right?

00:29:35.520 --> 00:29:36.000
Right.

00:29:36.000 --> 00:29:41.360
So I think I was, yeah, I think there's something like 170 institutions around the world that make

00:29:41.360 --> 00:29:44.700
up the kind of worldwide computing grid or the LHC worldwide computing grid.

00:29:44.700 --> 00:29:50.000
And there's something like 300,000 or maybe more at this point computing cores, which then

00:29:50.000 --> 00:29:53.400
can, well, we distribute the data around the world.

00:29:53.400 --> 00:29:56.840
And then we often need to process and reprocess and analyze that data.

00:29:56.840 --> 00:29:59.060
And that's done on this enormous computing grid.

00:29:59.060 --> 00:30:04.060
And so that's kind of once it's stored and distributed, that's how we go and analyze it is by basically

00:30:04.060 --> 00:30:09.200
sending jobs to this grid, which you can kind of think of as a precursor to what the cloud

00:30:09.200 --> 00:30:09.640
is now.

00:30:09.640 --> 00:30:09.980
Right.

00:30:09.980 --> 00:30:14.360
And so there's so much data probably that I suspect putting it on your laptop doesn't

00:30:14.360 --> 00:30:15.440
make a lot of sense, right?

00:30:15.440 --> 00:30:21.520
You need to send your computation to the data rather than bringing the data to you, right?

00:30:21.520 --> 00:30:22.140
Exactly.

00:30:22.140 --> 00:30:23.180
I know.

00:30:23.180 --> 00:30:24.300
I just said exactly.

00:30:24.300 --> 00:30:25.980
That's exactly how it works.

00:30:25.980 --> 00:30:29.720
You certainly wouldn't want to be downloading petabytes of data onto your computer.

00:30:29.720 --> 00:30:36.960
So thankfully, we have this grid of tier zero, tier one, tier two, tier three locations spread

00:30:36.960 --> 00:30:41.360
all around the world where you can send your scripts and they'll be run and then you'll

00:30:41.360 --> 00:30:42.360
get the results back.

00:30:42.360 --> 00:30:42.640
Yeah.

00:30:42.640 --> 00:30:44.120
So give me a sense of what that's like.

00:30:44.120 --> 00:30:45.840
You have a question you want to ask about the data.

00:30:45.840 --> 00:30:47.120
You could write some C++.

00:30:47.120 --> 00:30:48.560
You could write some Python.

00:30:48.560 --> 00:30:50.260
You could write maybe even Fortran.

00:30:50.260 --> 00:30:52.340
What is the mechanism from?

00:30:52.340 --> 00:30:55.420
I have this thing, this Python, let's say Python here.

00:30:55.620 --> 00:30:59.200
And I would like to make it run there and analyze the data.

00:30:59.200 --> 00:31:00.680
What's the steps look like there?

00:31:00.680 --> 00:31:05.280
From the user's perspective, which is the one that I get, it's very simple because of the

00:31:05.280 --> 00:31:08.180
work of hundreds of people who've made it simple for us.

00:31:08.180 --> 00:31:15.020
So we simply have an interface with our computing grid where we can specify specific locations

00:31:15.020 --> 00:31:16.780
if we want to do so.

00:31:16.780 --> 00:31:21.500
We can specify the length of the job, the number of cores that we're requiring, the number of

00:31:21.500 --> 00:31:22.220
nodes, et cetera.

00:31:22.640 --> 00:31:28.320
And then we can simply submit our script as long as it's in a format that is compliant

00:31:28.320 --> 00:31:31.300
with what our systems are able to handle.

00:31:31.300 --> 00:31:36.980
And then you specify what data to operate on, whether it's true data that has been collected

00:31:36.980 --> 00:31:41.600
from the LHC or if it's simulated data, you can also operate on that.

00:31:41.600 --> 00:31:46.680
And then you're able to monitor all of your jobs and eventually get the results back and

00:31:46.680 --> 00:31:52.180
download the histograms or whatever format your results will come to you as.

00:31:52.340 --> 00:31:53.140
That sounds really cool.

00:31:53.140 --> 00:31:56.560
And Michael, you said 300,000 computing cores?

00:31:56.560 --> 00:31:58.780
It's something I think it might even be larger at this point.

00:31:58.780 --> 00:32:01.520
As of 2010, it was more than 200,000.

00:32:01.520 --> 00:32:06.720
It's ever growing with the amount of data we have and the amount of computing we need.

00:32:06.720 --> 00:32:11.860
Just as Michaela was saying, we kind of send our jobs out and it's kind of built on top of

00:32:11.860 --> 00:32:18.020
a kind of virtual machine file system where we actually, all these sites are kind of working

00:32:18.020 --> 00:32:23.280
in coherently with having the same distribution of the Atlas software located all these sites.

00:32:23.280 --> 00:32:28.040
So you can send your job with a known even version of the software and it's already available

00:32:28.040 --> 00:32:28.920
locally to you.

00:32:28.920 --> 00:32:29.220
Wow.

00:32:29.340 --> 00:32:29.600
Okay.

00:32:29.600 --> 00:32:30.760
That sounds really, really fun.

00:32:30.760 --> 00:32:39.820
I guess one thing that I was wondering looking at the LHC is we have Atlas, we have LHCB and

00:32:39.820 --> 00:32:40.720
Alice and CMS.

00:32:40.720 --> 00:32:43.360
What is the purpose of Atlas?

00:32:43.460 --> 00:32:47.360
Like what is Atlas trying to do relative to the larger goal of LHC?

00:32:47.360 --> 00:32:48.220
Maybe Matthew take that.

00:32:48.220 --> 00:32:52.340
So Atlas and CMS, these are two of our general purpose detectors.

00:32:52.340 --> 00:32:59.600
And so the idea there is these detectors were explicitly designed and then built to be sensitive

00:32:59.600 --> 00:33:02.280
to a wide range of interesting physics.

00:33:02.280 --> 00:33:07.800
Whereas for example, backtracking a little bit like Atlas and CMS, they kind of have, if

00:33:07.800 --> 00:33:11.800
you will, like they're sometimes referred to as like cylindrical onions in the sense that

00:33:11.800 --> 00:33:17.480
the architecture is you kind of have your beam pipe and then you have successive layers of

00:33:17.480 --> 00:33:21.460
very detailed and detectors going out around them.

00:33:21.460 --> 00:33:23.700
Is that so you can basically take a 3D picture?

00:33:23.700 --> 00:33:24.600
Yeah, exactly.

00:33:24.600 --> 00:33:30.340
Because when we have these collisions, when you have the really hard collisions, then in

00:33:30.340 --> 00:33:35.360
some sense you have like just the result of the collision is that you have sprays of

00:33:35.360 --> 00:33:37.240
particles kind of coming out in all directions.

00:33:37.240 --> 00:33:40.740
And so you want to have as much coverage as possible.

00:33:41.020 --> 00:33:46.120
And the idea is if you have both like calorimetry systems and tracking systems, then you're

00:33:46.120 --> 00:33:49.420
able to get a much more detailed picture of what actually happened.

00:33:49.420 --> 00:33:55.520
Because we're trying to reconstruct essentially point-like interactions that are happening at

00:33:55.520 --> 00:33:56.580
the subatomic level.

00:33:56.580 --> 00:34:01.180
But we're doing that by seeing what kind of is just coming splattering through our detector.

00:34:01.180 --> 00:34:06.860
So it's kind of like trying to reconstruct what might have happened if you took like two,

00:34:06.860 --> 00:34:09.600
if there is some sort of like car crash.

00:34:09.600 --> 00:34:14.260
And the only way you could investigate what happened is if you were able to look at what

00:34:14.260 --> 00:34:16.180
the walls of a tunnel or something nearby.

00:34:16.180 --> 00:34:19.700
But so Atlas and CMS, they're general purpose detectors.

00:34:19.700 --> 00:34:24.300
And then Elise and LHCB, their geometries are a little bit different.

00:34:24.300 --> 00:34:29.240
And so they're more specialized detectors that are looking at specific types of physics.

00:34:29.460 --> 00:34:29.660
All right.

00:34:29.660 --> 00:34:29.960
Okay.

00:34:29.960 --> 00:34:30.360
Yeah.

00:34:30.360 --> 00:34:30.560
Yeah.

00:34:30.560 --> 00:34:30.920
Very cool.

00:34:30.920 --> 00:34:36.700
So one of the things I want to dig into with each one of you is what you're doing day to

00:34:36.700 --> 00:34:41.020
day and sort of how Python and machine learning fit into that.

00:34:41.020 --> 00:34:42.380
So Michaela, let's start with you.

00:34:42.380 --> 00:34:47.400
You're doing some, you already mentioned your generative adversarial networks and some really

00:34:47.400 --> 00:34:47.960
amazing stuff.

00:34:47.960 --> 00:34:52.720
You said that you were able to speed up some of these simulations like 100,000 times.

00:34:52.720 --> 00:34:53.600
That's correct.

00:34:53.600 --> 00:34:53.760
Cool.

00:34:53.760 --> 00:34:54.240
Techniques.

00:34:54.240 --> 00:34:54.900
That's incredible.

00:34:54.900 --> 00:34:56.440
Can you talk about how you're doing that?

00:34:56.440 --> 00:34:56.740
Yeah.

00:34:56.740 --> 00:35:01.680
So these are obviously preliminary results and there's a lot more R&D that is now just being

00:35:01.680 --> 00:35:03.400
started within our collaboration.

00:35:03.400 --> 00:35:08.000
But the point is that we built this great prototype we call the Calogan.

00:35:08.000 --> 00:35:10.080
Calo stands for calorimeter.

00:35:10.080 --> 00:35:17.000
That is one of the detector layers inside of this big onion-like structure that we just described

00:35:17.000 --> 00:35:18.120
our detector to look like.

00:35:18.120 --> 00:35:24.160
And the calorimeter measures the energy deposited by certain particles as they travel through,

00:35:24.160 --> 00:35:25.400
as Matthew was just describing.

00:35:25.400 --> 00:35:32.020
The issue is that because some of these physical processes that the particles undergo are so

00:35:32.020 --> 00:35:40.080
complicated, simulating these traversals of the particles through the calorimeter is really,

00:35:40.080 --> 00:35:41.300
really computationally intensive.

00:35:41.300 --> 00:35:47.080
And that's actually taking more than half of the computing grid power that we were just

00:35:47.080 --> 00:35:47.480
describing.

00:35:47.660 --> 00:35:49.900
So it's billions of CPU hours per year.

00:35:49.900 --> 00:35:54.480
So what I'm working on is this new technique to speed up that part of the simulation, which

00:35:54.480 --> 00:35:57.560
currently occupies the majority of our computing resources worldwide.

00:35:57.560 --> 00:36:00.940
And what I'm using is generative adversarial networks.

00:36:00.940 --> 00:36:05.200
I think some of your audience maybe will recognize this word.

00:36:05.200 --> 00:36:14.580
So we use GANs to provide a function approximator to our simulator while retaining, hopefully, the majority

00:36:14.580 --> 00:36:19.040
of the accuracy that the slower physics-driven simulator possesses.

00:36:19.040 --> 00:36:23.880
And again, as I said, multiple preliminary results have been put out so far.

00:36:23.880 --> 00:36:27.200
And we are achieving speed up times of over 100,000 times.

00:36:27.200 --> 00:36:33.980
But now the complicated part will really be to learn how to calibrate all of these, this machinery and

00:36:33.980 --> 00:36:39.000
bring it, port it into the real simulation within the experiment.

00:36:39.000 --> 00:36:44.660
But speaking of Python, I think the cool thing for everybody to know is that this is very

00:36:44.660 --> 00:36:46.840
easily built using Keras and TensorFlow.

00:36:46.840 --> 00:36:53.140
So very standard machine learning tools from Python, as well as other standard tools from the Python

00:36:53.140 --> 00:36:58.480
ecosystem, such as NumPy, scikit-learn, H5Py, and Matplotlib all make it into my project.

00:36:58.700 --> 00:36:59.460
Yeah, that's really cool.

00:36:59.460 --> 00:37:01.560
I think people are probably familiar with most of those.

00:37:01.560 --> 00:37:03.700
But H5Py, what is that?

00:37:03.700 --> 00:37:08.300
Oh, it's just the interface for HDF5 in Python.

00:37:08.300 --> 00:37:13.340
HDF5 being like a very standard data format that can be ported across various different languages,

00:37:13.340 --> 00:37:20.000
C++, Python, and H5Py certainly saved my life in terms of being able to open these files.

00:37:20.000 --> 00:37:20.420
Yes.

00:37:20.420 --> 00:37:21.500
Of course.

00:37:21.500 --> 00:37:22.180
That's really cool.

00:37:22.180 --> 00:37:26.940
Michael, you want to talk a bit about how you're using machine learning for what you're up

00:37:26.940 --> 00:37:27.100
to?

00:37:27.320 --> 00:37:28.040
Yeah, absolutely.

00:37:28.040 --> 00:37:33.980
So kind of in the past, I've been working a lot on, again, these kind of ideas of taking

00:37:33.980 --> 00:37:38.120
detector measurements and turning them into classifying whether this data was from a given

00:37:38.120 --> 00:37:39.060
kind of particle or not.

00:37:39.060 --> 00:37:44.120
And so I've been working on connecting kind of the data that we have with ideas of machine

00:37:44.120 --> 00:37:44.340
learning.

00:37:44.340 --> 00:37:49.400
So we have, so when we were talking about quarks, and when you produce quarks, it turns out

00:37:49.400 --> 00:37:54.400
they produce kind of collimated streams of particles that smash into these calorimeters and

00:37:54.400 --> 00:37:58.000
kind of leave a bunch of energy with kind of distributed in space.

00:37:58.000 --> 00:38:02.760
It turns out we can connect those distribution of energy in space with basically imaging type

00:38:02.760 --> 00:38:03.140
approaches.

00:38:03.140 --> 00:38:07.380
And then we can, we've been running a lot of computer vision type techniques to study those

00:38:07.380 --> 00:38:08.820
jets, those quarks.

00:38:09.200 --> 00:38:14.540
And so that's, you know, really jumped into connecting with things like convolutional neural

00:38:14.540 --> 00:38:16.620
networks and modern computer vision.

00:38:16.620 --> 00:38:21.660
And so, yeah, I've been working on that just like Michaela with tools like Keras and TensorFlow

00:38:21.660 --> 00:38:26.620
and, you know, built on top of core packages like SciPy and NumPy.

00:38:26.760 --> 00:38:30.480
Well, one of the things I was going to ask you is it seems like the things you're actually

00:38:30.480 --> 00:38:32.100
looking for are quite rare.

00:38:32.100 --> 00:38:38.060
Like there was only a few collisions that produced the Higgs boson, for example, back in 2013,

00:38:38.060 --> 00:38:38.580
14.

00:38:38.580 --> 00:38:39.780
Right.

00:38:39.780 --> 00:38:44.460
And my sense, I haven't done a lot of machine learning, but my sense is you have to give a

00:38:44.460 --> 00:38:47.060
lot of examples to machine learning.

00:38:47.060 --> 00:38:51.200
And then the machine learning can find more of those, even if they're subsequently rare.

00:38:51.200 --> 00:38:53.420
But how do you like bootstrap this?

00:38:53.420 --> 00:38:55.360
How do you get it started and where there's enough?

00:38:55.360 --> 00:38:56.540
How do you teach them?

00:38:56.540 --> 00:38:57.620
I guess is what I'm asking.

00:38:57.620 --> 00:39:01.480
And so that they can then go find these things, especially when those occurrences are rare.

00:39:01.480 --> 00:39:04.980
In some sense, it is kind of a bootstrap, but it's based on this idea that those super

00:39:04.980 --> 00:39:07.840
rare particles like Higgs bosons, we don't observe them directly.

00:39:07.840 --> 00:39:11.140
They decay into other things that we know about, like electrons.

00:39:11.140 --> 00:39:11.520
I see.

00:39:11.520 --> 00:39:12.960
So they're easy to find.

00:39:12.960 --> 00:39:15.040
Decay result is easy to find.

00:39:15.040 --> 00:39:16.400
And you can teach it to find those.

00:39:16.480 --> 00:39:20.580
And when you find them in certain configurations, you're like, oh, this may have originated

00:39:20.580 --> 00:39:22.260
from what we're looking for.

00:39:22.260 --> 00:39:22.780
Exactly.

00:39:22.780 --> 00:39:27.840
So I work a lot on making sure we can find not electrons, but other types of particles that

00:39:27.840 --> 00:39:29.140
are produced copiously.

00:39:29.140 --> 00:39:33.540
And then we can say, okay, in this case, okay, I know how to find electrons.

00:39:33.540 --> 00:39:37.360
I can train an algorithm using our simulation, which is very precise.

00:39:37.360 --> 00:39:42.080
I can go look for those in our real data to make sure that the simulation makes sense in

00:39:42.080 --> 00:39:42.900
a different configuration.

00:39:43.780 --> 00:39:49.140
And then I can go hunting with my well calibrated and well tuned algorithm for finding electrons.

00:39:49.140 --> 00:39:52.020
I can go hunt for configurations with four or five electrons.

00:39:52.020 --> 00:39:54.720
And that might be a really rare thing you want to look for.

00:39:54.720 --> 00:39:55.000
Okay.

00:39:55.000 --> 00:39:55.420
Yeah.

00:39:55.420 --> 00:39:55.900
Very interesting.

00:39:55.900 --> 00:39:56.580
I see how that works.

00:39:56.580 --> 00:39:58.560
Because I was thinking about this.

00:39:58.560 --> 00:40:02.260
You know, how do you get started trying to find very rare things?

00:40:02.260 --> 00:40:03.480
But I can see that now.

00:40:03.480 --> 00:40:03.860
Okay.

00:40:04.600 --> 00:40:06.180
I'll throw this question out to everyone.

00:40:06.180 --> 00:40:08.780
Are any of you using special hardware?

00:40:08.780 --> 00:40:13.800
Or are you just leveraging the many, many cores out there on the computing environment?

00:40:13.800 --> 00:40:19.360
Like are you using the tensor CPU type things or GPUs or those sorts of things?

00:40:19.460 --> 00:40:25.220
I think the majority of us are using GPUs to train some of our machine learning models these

00:40:25.220 --> 00:40:29.180
days on top of obviously the worldwide grid that we just described.

00:40:29.180 --> 00:40:36.520
But that's more reserved for simulating samples or doing, you know, more standard types of analysis.

00:40:36.520 --> 00:40:39.680
But I think most of us rely on GPUs these days.

00:40:39.680 --> 00:40:44.460
I would add, I think one of the interesting potential future directions on top of all the

00:40:44.460 --> 00:40:49.260
work with GPUs and CPUs is if we ever want some of these algorithms to work in our trigger

00:40:49.260 --> 00:40:54.600
in that super fast system that needs to operate at 40 megahertz, this may be a place where,

00:40:54.600 --> 00:40:57.480
you know, there's some people already in the community beginning to look at, you know,

00:40:57.480 --> 00:41:02.280
putting neural networks onto FPGAs so that we can actually run them at super high speed.

00:41:02.280 --> 00:41:04.220
So that may be a future direction in the field move.

00:41:04.220 --> 00:41:04.480
Okay.

00:41:04.480 --> 00:41:04.740
Yeah.

00:41:04.740 --> 00:41:06.000
That would be really amazing.

00:41:06.420 --> 00:41:09.800
Maybe Matthew, this is a good time to talk about that trigger thing.

00:41:09.800 --> 00:41:16.260
So you have to take 40 million observations across this thing and get it down to 1000.

00:41:16.260 --> 00:41:20.400
And you've got to do that extremely, like every second.

00:41:20.400 --> 00:41:20.780
Yeah.

00:41:20.780 --> 00:41:22.520
So how do you do that?

00:41:22.520 --> 00:41:22.920
Yeah.

00:41:22.920 --> 00:41:27.600
So Michael's already given a very nice intro summary there, but the trigger is an immensely

00:41:27.600 --> 00:41:28.660
complex system.

00:41:28.660 --> 00:41:31.700
And so, I mean, I definitely don't understand how all of it works.

00:41:31.700 --> 00:41:36.380
I actually just work on a subsystem of the trigger subsystem.

00:41:36.380 --> 00:41:47.040
So if you want to actually do an analysis, then you need to have, let's say like you're looking for a Higgs decay that goes to two B quarks.

00:41:47.040 --> 00:42:00.440
Well, if you want to go searching for that, then you want to have some confidence that there ever could have been a recording of an event in the Atlas detector that had two B quarks in it.

00:42:00.440 --> 00:42:05.380
So you want to make sure that some of these interesting collisions aren't getting thrown out.

00:42:05.380 --> 00:42:21.360
And so we have, that's one of the reasons we have the trigger system is that we kind of have what we call a trigger menu, which is a list of basically logical sequences in that we are looking for in the different subsystems of the detector to say that, oh, okay.

00:42:21.360 --> 00:42:26.020
So I think that's a lot of the way.

00:42:26.020 --> 00:42:30.680
So I think that's a lot of the way.

00:42:30.680 --> 00:42:34.680
So I think that's a lot of the way that we're looking for a lot of the things that we're looking for.

00:42:34.680 --> 00:42:36.680
So I think that's a lot of the way that we're looking for a lot of the things that we're looking for.

00:42:36.680 --> 00:42:43.680
We're looking for a lot of the things that we're looking for, but we're looking for a lot of the things that we're looking for, but we're looking for a lot of the things that we're looking for.

00:42:43.680 --> 00:42:56.440
But we want to make sure that as we go up to higher energies and as we go up to even more collisions, so basically what we call luminosity is the number of collisions that we have per crossing increases.

00:42:56.720 --> 00:43:07.100
We want to make sure that our trigger system can still deal with this because in a single crossing of the beam, we don't just get one kind of collision point.

00:43:07.100 --> 00:43:19.840
We might get right now somewhere between like 30 and 50, but as we go to higher energies and to higher luminosities, we're looking at getting something like 200 collisions that are happening every beam crossing.

00:43:19.840 --> 00:43:42.940
So if you're trying to say, oh, I have like an electron or I have a B-jet that looks interesting over here, I wonder if it might also have a partner that can tell me if I had a collision, you have a really difficult problem because you're trying to now pick out what might these other energy deposits or tracks be from 200 other collisions that are happening at the same time.

00:43:42.940 --> 00:43:46.140
Yeah, it's one of these combinatorial types of things.

00:43:46.220 --> 00:43:50.040
How many different relationships between, you know, 30 or 200, right?

00:43:50.040 --> 00:43:52.600
That turns out to be astronomically bigger, right?

00:43:52.600 --> 00:43:53.960
Yeah, it gets pretty crazy.

00:43:53.960 --> 00:43:56.560
I'm sure it does.

00:43:56.560 --> 00:43:57.340
I'm sure it does.

00:43:57.340 --> 00:44:01.600
So that's mostly like embedded C++.

00:44:01.600 --> 00:44:02.440
Is that right?

00:44:02.440 --> 00:44:04.500
But what other stuff are you doing there?

00:44:04.500 --> 00:44:05.980
Is there any Python at that layer?

00:44:05.980 --> 00:44:08.480
So actually like in the trigger system, no.

00:44:08.480 --> 00:44:15.800
But so as a student, what I'm doing a lot more is performance studies of how our trigger is doing.

00:44:16.120 --> 00:44:20.300
And so there I do use kind of both a hybrid of Python and C++.

00:44:20.300 --> 00:44:29.300
We have colleagues at the University of Chicago that have written a really nice analysis framework that a lot of people in the collaboration use.

00:44:29.300 --> 00:44:32.200
And that's it's all implemented in C++.

00:44:32.200 --> 00:44:37.360
But then there is also a way for us to interact with it from Python.

00:44:37.360 --> 00:44:47.500
So that's really nice because then that allows us to, for example, I was able to write some tools, some command line interface tools using arg parse and things like that.

00:44:47.500 --> 00:44:58.660
So I can give, if I want to do a quick performance study, I can, without really having to ever write a line of C++, I can kind of spin up a small analysis to say like, oh, okay.

00:44:58.660 --> 00:45:03.400
How is the trigger performing under these scenarios and things like that?

00:45:03.400 --> 00:45:07.280
And using things like Jupyter Notebooks and stuff to sort of do that exploration?

00:45:07.620 --> 00:45:09.720
For the trigger, I don't use Jupyter Notebooks so much.

00:45:09.720 --> 00:45:11.920
That's more regular.

00:45:11.920 --> 00:45:18.720
I use Jupyter more for the exploratory data analysis that I'm using for, if you will, like my actual like PhD research.

00:45:18.720 --> 00:45:28.480
But for the operations work that I do on Atlas, that's more just kind of going into your favorite editor and then writing some command line tools in Python.

00:45:28.480 --> 00:45:34.540
And then also making sure that it interfaces well with these C++ frameworks that we've developed.

00:45:34.540 --> 00:45:34.920
Okay.

00:45:34.920 --> 00:45:35.680
Very interesting.

00:45:35.680 --> 00:45:40.960
So, Michaela, maybe we could talk a little bit about where you guys are going in the future.

00:45:40.960 --> 00:45:44.300
LHC has been a really long running project.

00:45:44.300 --> 00:45:50.180
It started producing major results in 2013, 14, but it was started way before then, right?

00:45:50.180 --> 00:45:54.380
So where's the future going now with what you're up to?

00:45:54.380 --> 00:45:54.860
Of course.

00:45:54.860 --> 00:45:59.760
So there are still so many open questions, I think, about the standard model.

00:45:59.760 --> 00:46:04.480
We know, as Michael said, that the standard model has been validated over and over again.

00:46:04.480 --> 00:46:10.360
And it's probably one of the most predictive theories in the history of all theories.

00:46:10.360 --> 00:46:12.640
But there are some missing pieces.

00:46:12.640 --> 00:46:17.360
So, for example, we don't have a complete quantum theory of gravity, for example.

00:46:17.360 --> 00:46:22.160
So there is this hypothetical particle called the graviton that some are searching for.

00:46:22.660 --> 00:46:34.760
So I think there are very many open questions that could potentially be answered through some of the theories that have already been proposed, whether it's supersymmetry or others.

00:46:34.760 --> 00:46:47.320
And so I think right now the goal is to continue looking for those while we also continue making very high precision measurements of all of the particles that we know and love.

00:46:47.320 --> 00:47:01.180
So I think some ideas for what else could be out there doesn't necessarily come from directly searching for these new particles, but it could come from measuring properties of particles that are already a part of the standard model.

00:47:01.180 --> 00:47:03.320
So I think that's a good thing.

00:47:03.320 --> 00:47:03.320
So I think that's a good thing.

00:47:03.320 --> 00:47:04.320
I think that's a good thing.

00:47:04.320 --> 00:47:05.320
So I think that's a good thing.

00:47:05.320 --> 00:47:06.320
So I think that's a good thing.

00:47:06.320 --> 00:47:07.320
So I think that's a good thing.

00:47:07.320 --> 00:47:08.320
So I think that's a good thing.

00:47:08.320 --> 00:47:08.320
So I think that's a good thing.

00:47:08.320 --> 00:47:09.320
So I think that's a good thing.

00:47:09.320 --> 00:47:10.320
So I think that's a good thing.

00:47:10.320 --> 00:47:11.320
So I think that's a good thing.

00:47:11.320 --> 00:47:12.320
So I think that's a good thing.

00:47:12.320 --> 00:47:13.320
So I think that's a good thing.

00:47:13.320 --> 00:47:15.320
So I think that's a good thing.

00:47:15.320 --> 00:47:16.320
So I think that's a good thing.

00:47:16.320 --> 00:47:17.320
So I think that's a good thing.

00:47:17.320 --> 00:47:19.320
So I think that's a good thing.

00:47:19.320 --> 00:47:20.320
So I think that's a good thing.

00:47:20.320 --> 00:47:21.320
So I think that's a good thing.

00:47:21.320 --> 00:47:23.320
So I think that's a good thing.

00:47:23.320 --> 00:47:25.020
It could look entirely right.

00:47:25.020 --> 00:47:27.460
But there could be these subtle, subtle deviations.

00:47:27.460 --> 00:47:32.940
Like just look at Newton and gravity and then Einstein and gravity, right?

00:47:32.940 --> 00:47:33.980
Newton looked right.

00:47:33.980 --> 00:47:34.760
Yes, of course.

00:47:34.760 --> 00:47:36.940
I mean, to first degree, it was.

00:47:36.940 --> 00:47:43.540
But obviously, there are then in certain regimes, some deviations from the more simplistic theory.

00:47:43.540 --> 00:47:49.040
And so we think that perhaps there could be other corrections to what we're measuring that

00:47:49.040 --> 00:47:52.780
could come from more complicated theories that we're still to validate.

00:47:52.780 --> 00:47:53.160
Cool.

00:47:53.160 --> 00:47:59.280
So, Michael, one thing I wanted to ask you is, how do you feel that machine learning and

00:47:59.280 --> 00:48:04.240
doing that with Python and TensorFlow and all these things have changed physics and the

00:48:04.240 --> 00:48:05.120
physics exploration?

00:48:05.120 --> 00:48:10.180
Like how much would have it just been more work or stuff actually being discovered that

00:48:10.180 --> 00:48:11.200
wouldn't have been discovered?

00:48:11.200 --> 00:48:12.120
What do you think?

00:48:12.120 --> 00:48:17.980
Yeah, I think a lot of algorithms may not have been discovered or maybe, or I should say,

00:48:17.980 --> 00:48:20.400
maybe applied or implemented in a reasonable timescale.

00:48:20.540 --> 00:48:25.520
I mean, so, you know, we're not, since we're not professional machine learning, you know,

00:48:25.520 --> 00:48:28.840
machine learning engineers or machine learning researchers, a lot of the times what, you know,

00:48:28.840 --> 00:48:32.460
the tools we're using, we either built from scratch or use what's available.

00:48:32.880 --> 00:48:43.520
And so the kind of this availability of things like TensorFlow make it really easy for us to implement and just try things out on our data and then try to make these connections between the machine learning world.

00:48:43.520 --> 00:48:56.880
So things like these generative models, you know, they might not, we may not have access to these kind of really, potentially really fast speed ups or new ways to look at the data, like, you know, with vision or natural language processing approaches.

00:48:57.080 --> 00:48:59.420
You know, these algorithms may not even be implemented anywhere.

00:48:59.420 --> 00:49:04.500
Nowadays, you can just kind of download the model if you want and use that as a place to start.

00:49:04.700 --> 00:49:18.840
And I think that one of the things that's been really helpful is the core kind of C++ libraries that we use at CERN are built with this, with kind of internal dictionaries with the kind of have this idea of reflection so that they can really easy be bound to Python.

00:49:18.840 --> 00:49:30.400
And we can just take our data that's in one format and really quickly switch into Python and just start pounding or, you know, running through all these different kind of algorithms and see what comes out.

00:49:30.400 --> 00:49:30.920
That's really cool.

00:49:30.920 --> 00:49:34.380
So it's a little bit like what NumPy did for numerical analysis.

00:49:34.380 --> 00:49:40.100
You kind of have something similar where it's got this sort of native layer really close to Python.

00:49:40.320 --> 00:49:40.800
Exactly.

00:49:40.800 --> 00:49:42.180
And so, yeah.

00:49:42.180 --> 00:49:50.520
And in fact, then we can also interface directly with NumPy and pull our data right into formats that are kind of standardized in the machine learning and data science community.

00:49:50.520 --> 00:49:51.320
Yeah, very cool.

00:49:51.320 --> 00:49:58.660
Michaela, maybe I could ask you the same question because your work has made computational so much faster.

00:49:59.160 --> 00:50:02.900
And it seems like the machine learning played an important role there as well.

00:50:02.900 --> 00:50:10.340
If you could really make these simulations 100,000 times faster, that's almost like going from regular computers to quantum computers.

00:50:10.340 --> 00:50:11.500
It's such a jump.

00:50:11.500 --> 00:50:12.220
Yes, of course.

00:50:12.220 --> 00:50:21.400
I mean, obviously, we would like to empower more physicists to be able to do the analysis that they want to do at the precision levels that they require.

00:50:21.900 --> 00:50:32.140
And right now, obviously, that simulation bottleneck means that some analysis cannot necessarily be performed with the statistical uncertainties that we would like to have.

00:50:32.140 --> 00:50:38.820
Or some of them cannot be performed at all in terms of the required accuracy that they need.

00:50:38.820 --> 00:50:45.540
And so hopefully this type of projects will enable more physics to be done and more analysis in the future of the LHC.

00:50:45.540 --> 00:50:46.520
Yeah, very cool.

00:50:47.100 --> 00:50:58.900
So, Matthew, do you think there's room for normal developers, people who want to just go contribute to open source, maybe some kind of open science they want to play around and just like, hey, they're really good at programming, but they're not physicists.

00:50:58.900 --> 00:51:01.740
Is there a place for them to kind of jump in and be part of this?

00:51:01.740 --> 00:51:02.940
Yeah, I think so.

00:51:02.940 --> 00:51:08.120
I mean, so we have, as Michael mentioned earlier, it's really important that our code is open source.

00:51:08.120 --> 00:51:12.000
And so if people actually want to go take a look at it, it's out there.

00:51:12.300 --> 00:51:26.020
And then there's also, I think there's some efforts to get something like this started on Atlas, but then on one of our other, one of the, at LHCB, one of the other experiments, they have what's called the LHCB Starter Kit.

00:51:26.240 --> 00:51:35.640
And that was started by two PhD students, Kevin Duns and Tim Head, who are now, have now left the field to go work at Google and be a data science consultant.

00:51:35.640 --> 00:51:43.980
But they thought like, hey, you know, a lot of our students are coming in and we don't have, and we're all physicists, but we're not all necessarily software experts.

00:51:44.340 --> 00:52:02.320
So they created this thing called the LHCB Starter Kit, and they've done some partnering with softwarecarpentry.org to actually go ahead and hold training seminars where people from software carpentry come in and help them learn how to actually get started to do data analysis and programming and physics.

00:52:03.440 --> 00:52:08.500
So I think software carpentry and data carpentry, those are really great organizations.

00:52:08.500 --> 00:52:11.940
So if people want to get involved there, they definitely can as well.

00:52:11.940 --> 00:52:12.640
Yeah, that's really cool.

00:52:12.640 --> 00:52:15.840
And I had the software carpentry guys on the show a couple of months back.

00:52:15.840 --> 00:52:17.040
Yeah, that was a good episode.

00:52:17.040 --> 00:52:17.980
Yeah, thanks.

00:52:17.980 --> 00:52:18.760
Yeah, it was really nice.

00:52:18.760 --> 00:52:21.380
So I think that's, I think it's great.

00:52:21.380 --> 00:52:25.760
I think, you know, there's more and more of these tools are becoming accessible to everyone.

00:52:25.760 --> 00:52:31.880
And I suspect uploading to the whole computing system and running stuff there is probably restricted to you guys.

00:52:31.880 --> 00:52:34.260
But still, people can work on these algorithms.

00:52:34.260 --> 00:52:35.840
And in a sense, they're contributing, right?

00:52:35.840 --> 00:52:40.820
People who make TensorFlow or Keras better, they're in a sense making it better for you guys as well, right?

00:52:40.820 --> 00:52:41.160
Yeah.

00:52:41.160 --> 00:52:41.540
Absolutely.

00:52:41.540 --> 00:52:44.080
Yeah, there's a big push right now.

00:52:44.080 --> 00:52:55.740
I think Michaela is a great example of this to really kind of go out into the communities that are going and actually building these great tools and interact with them directly and contribute.

00:52:55.740 --> 00:53:01.420
So, I mean, we want to try and use really powerful tools to do the best analysis we can.

00:53:01.420 --> 00:53:09.760
So the people that are making the Python tools that we use better, they're really directly impacting science.

00:53:09.760 --> 00:53:11.180
So we appreciate it.

00:53:11.180 --> 00:53:12.240
Yeah, that's amazing.

00:53:12.240 --> 00:53:12.720
All right.

00:53:13.040 --> 00:53:18.220
Well, I have so many more questions to ask you all, but we're pretty much out of time.

00:53:18.220 --> 00:53:20.120
So I want to be respectful of that as well.

00:53:20.120 --> 00:53:23.000
Before we go, I'm going to ask you all the two questions.

00:53:23.000 --> 00:53:25.700
I'll start with Michael, favorite editor.

00:53:25.700 --> 00:53:26.220
Emacs.

00:53:26.220 --> 00:53:26.600
Michaela.

00:53:26.600 --> 00:53:27.420
Sublime.

00:53:27.420 --> 00:53:28.020
Matthew.

00:53:28.020 --> 00:53:29.820
Adam with Vim key bindings.

00:53:29.820 --> 00:53:30.440
All right.

00:53:30.440 --> 00:53:30.860
Right on.

00:53:30.860 --> 00:53:33.460
And Michael, notable PyPI package?

00:53:33.460 --> 00:53:35.880
PyTorch and Scikit Optimize.

00:53:35.880 --> 00:53:38.060
I don't think I've talked about PyTorch on the show before.

00:53:38.060 --> 00:53:41.340
Maybe just really briefly talk about what that is.

00:53:41.440 --> 00:53:41.600
Yeah.

00:53:41.600 --> 00:53:51.720
So PyTorch is a deep learning library that's built on a different way of building the graphical representation of your deep neural network for all the downstream computations.

00:53:51.900 --> 00:53:59.900
But actually, the API is just really, it's very, I find it very smooth and easy to really quickly spin up a neural network and have it running.

00:53:59.900 --> 00:54:00.300
Excellent.

00:54:00.300 --> 00:54:01.500
Michaela, how about you?

00:54:01.500 --> 00:54:02.940
I'm a huge fan of Keras.

00:54:03.280 --> 00:54:05.940
I've been using it since it was first started as a project.

00:54:05.940 --> 00:54:11.400
And it's been enabling me to go from idea to experimentation very quickly.

00:54:11.400 --> 00:54:14.720
And then huge shout out, I think, to Matplotlib.

00:54:14.720 --> 00:54:18.080
It allowed me to make all the plots that I've ever made during my PhD.

00:54:18.080 --> 00:54:19.820
So those two for sure.

00:54:19.820 --> 00:54:20.260
All right.

00:54:20.260 --> 00:54:20.600
Excellent.

00:54:20.600 --> 00:54:21.420
Matthew?

00:54:21.420 --> 00:54:22.280
Definitely Keras.

00:54:22.600 --> 00:54:26.500
I mean, like Michaela said, like, that's kind of a bread and butter thing for us.

00:54:26.500 --> 00:54:29.280
And then also Scikit-HEP, which is a...

00:54:29.280 --> 00:54:30.580
HEP is high energy physics?

00:54:30.580 --> 00:54:30.980
Yeah.

00:54:30.980 --> 00:54:55.180
So similar to Scikit-Learn, how that was supposed to be a toolbox for machine learning, Scikit-HEP is a collection of Python tools that have been developed inside the high energy physics community, but are meant to try and help us interface with things like with NumPy and Pandas data frames and make our lives a lot easier when we're actually trying to go from our root data file format to things like NumPy.

00:54:55.180 --> 00:54:56.060
Yeah, very cool.

00:54:56.060 --> 00:54:59.620
I'll give you guys a chance for a final call to action.

00:54:59.620 --> 00:55:01.480
And people are excited about what they heard.

00:55:01.480 --> 00:55:02.340
They want to get involved.

00:55:02.340 --> 00:55:03.760
Michael, you can go first.

00:55:03.760 --> 00:55:04.080
Sure.

00:55:04.080 --> 00:55:04.340
Yeah.

00:55:04.340 --> 00:55:10.900
I think there's a lot of ways in which you can get involved, especially through the CERN, you know, open data portal and CERN Open Science.

00:55:10.900 --> 00:55:13.060
You can download some of our data.

00:55:13.060 --> 00:55:24.740
You can play around with it and also just help spread knowledge about, you know, science and scientific reasoning and, you know, how that can benefit society, both from advancing science and advancing the way we think.

00:55:24.740 --> 00:55:25.300
All right.

00:55:25.300 --> 00:55:25.580
Excellent.

00:55:25.580 --> 00:55:26.000
Michaela?

00:55:26.000 --> 00:55:39.860
Well, certainly if you're interested in working at the intersection of science and machine learning or data science, CERN, I think, is quickly becoming one of the best places in the world to do that because the scale and the fascinating problems that we're working on are completely unparalleled.

00:55:39.860 --> 00:55:45.600
And we're always looking for new skilled software engineers that are curious about mysteries of the universe.

00:55:45.600 --> 00:55:49.360
And as I said before, the community of CERN is truly fantastic.

00:55:49.360 --> 00:55:52.600
So we can always use more Python experts.

00:55:52.600 --> 00:55:56.640
And you absolutely don't have to be a physicist to make a huge impact at CERN.

00:55:56.640 --> 00:55:57.200
Oh, excellent.

00:55:57.200 --> 00:55:58.120
Sounds really fun.

00:55:58.120 --> 00:55:59.000
I'd love to work there.

00:55:59.000 --> 00:56:00.480
All right, Matthew, you got the final word.

00:56:00.480 --> 00:56:00.820
Yeah.

00:56:00.960 --> 00:56:06.480
So physicists, we love to collaborate and we like to think at least that we have really cool and really hard problems.

00:56:06.480 --> 00:56:09.040
So we were always looking for it.

00:56:09.040 --> 00:56:11.000
And you consider those to be the same thing, right?

00:56:11.000 --> 00:56:12.060
Cool equals hard.

00:56:12.060 --> 00:56:12.920
Yeah, probably.

00:56:12.920 --> 00:56:21.840
So and we've we've started in the last couple of years to have some really fruitful collaborations with our CS colleagues.

00:56:21.840 --> 00:56:28.940
So, you know, if you if you know a friendly neighborhood particle physicist and you want to talk with them, please do.

00:56:28.940 --> 00:56:37.380
We we're happy to talk and we really want to try and have our field grow with other fields and try and do the best science we can.

00:56:37.380 --> 00:56:37.780
All right.

00:56:37.780 --> 00:56:40.160
Thank you all so much for being on the show.

00:56:40.160 --> 00:56:43.400
It's been really interesting and I've learned a lot.

00:56:43.400 --> 00:56:45.420
Thanks for sharing what you're doing and keep up the good work.

00:56:45.420 --> 00:56:46.000
Thank you.

00:56:46.000 --> 00:56:46.500
Yeah.

00:56:46.500 --> 00:56:47.540
Thanks for having us on, Michael.

00:56:47.540 --> 00:56:48.280
Yeah, thanks.

00:56:48.280 --> 00:56:50.500
And thanks for for hosting this great show.

00:56:50.500 --> 00:56:51.080
You're welcome.

00:56:51.080 --> 00:56:51.640
Bye, everyone.

00:56:51.760 --> 00:56:52.000
Bye bye.

00:56:52.000 --> 00:56:52.240
Bye.

00:56:52.240 --> 00:56:55.940
This has been another episode of Talk Python to Me.

00:56:55.940 --> 00:57:00.500
This week's guests have been Michaela Paganini, Michael Kagan and Matthew Feigert.

00:57:00.500 --> 00:57:04.540
And this episode has been brought to you by Linode and Talk Python Training.

00:57:04.540 --> 00:57:09.200
Linode is bulletproof hosting for whatever you're building with Python.

00:57:09.200 --> 00:57:13.560
Get your four months free at talkpython.fm/Linode.

00:57:13.560 --> 00:57:16.060
Just use the code Python 17.

00:57:16.060 --> 00:57:19.120
Are you or a colleague trying to learn Python?

00:57:19.120 --> 00:57:21.740
Have you tried books and videos that just left you?

00:57:21.740 --> 00:57:23.800
bored by covering topics point by point?

00:57:23.800 --> 00:57:29.820
Well, check out my online course Python Jumpstart by building 10 apps at talkpython.fm/course

00:57:29.820 --> 00:57:32.420
to experience a more engaging way to learn Python.

00:57:32.980 --> 00:57:39.760
And if you're looking for something a little more advanced, try my write pythonic code course at talkpython.fm/pythonic.

00:57:39.760 --> 00:57:42.480
Be sure to subscribe to the show.

00:57:42.480 --> 00:57:44.680
Open your favorite podcatcher and search for Python.

00:57:44.680 --> 00:57:45.920
We should be right at the top.

00:57:46.300 --> 00:57:55.240
You can also find the iTunes feed at /itunes, Google Play feed at /play, and direct RSS feed at /rss on talkpython.fm.

00:57:55.780 --> 00:57:57.120
This is your host, Michael Kennedy.

00:57:57.120 --> 00:57:58.480
Thanks so much for listening.

00:57:58.480 --> 00:57:59.540
I really appreciate it.

00:57:59.540 --> 00:58:01.500
Now get out there and write some Python code.

00:58:01.500 --> 00:58:02.000
Thank you.

00:58:02.000 --> 00:58:02.340
Thank you.

00:58:02.340 --> 00:58:32.320
I'm out.

