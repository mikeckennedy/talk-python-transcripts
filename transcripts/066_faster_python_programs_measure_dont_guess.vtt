WEBVTT

00:00:00.001 --> 00:00:05.100
Python is a wonderful programming language that is often underestimated because it's so clear and

00:00:05.100 --> 00:00:11.640
simple. People mistake the simplicity for being too simple for real programs. After all, you didn't

00:00:11.640 --> 00:00:16.860
even struggle to get your program to link against an incompatible static library or battle a DLL

00:00:16.860 --> 00:00:22.180
version mismatch in your Python app at all today, did you? Usually we find the simple and clear

00:00:22.180 --> 00:00:27.360
programming language to be powerful and fast. But what happens when it's not fast enough? Do you have

00:00:27.360 --> 00:00:32.740
to stop and rewrite your code in C, C# or Java? Well, before you do something drastic,

00:00:32.740 --> 00:00:37.660
Mike Mueller is here to teach us the techniques and steps to determine why our Python programs

00:00:37.660 --> 00:00:43.260
might be slow and give us some tips to make them faster. This is Talk Python To Me, episode 66,

00:00:43.260 --> 00:00:46.180
recorded Monday, July 4th.

00:00:46.180 --> 00:00:49.120
Developers, developers, developers, developers, developers.

00:00:49.120 --> 00:00:55.100
I'm a developer in many senses of the word because I make these applications, but I also use these

00:00:55.100 --> 00:01:00.620
verbs to make this music. I construct it line by line, just like when I'm coding another software

00:01:00.620 --> 00:01:06.760
design. In both cases, it's about design patterns. Anyone can get the job done. It's the execution

00:01:06.760 --> 00:01:12.960
that matters. I have many interests. Welcome to Talk Python To Me, a weekly podcast on Python,

00:01:12.960 --> 00:01:18.140
the language, the libraries, the ecosystem, and the personalities. This is your host, Michael Kennedy.

00:01:18.140 --> 00:01:22.520
Follow me on Twitter where I'm @mkennedy. Keep up with the show and listen to past episodes

00:01:22.520 --> 00:01:28.920
at talkpython.fm and follow the show on Twitter via at Talk Python. This episode has been brought

00:01:28.920 --> 00:01:34.580
to you by Rollbar and SnapCI. Hey everyone, it's great to be with you today. I have a couple of

00:01:34.580 --> 00:01:39.700
news items for you this week. First up, Stitcher. If you've been listening to Talk Python To Me on

00:01:39.700 --> 00:01:44.160
Stitcher, I have some news for you. Unfortunately, due to some business practices that I believe are

00:01:44.160 --> 00:01:48.680
harmful to the podcasting space in general, I've asked them to delete my listing and remove the show

00:01:48.680 --> 00:01:53.780
from Stitcher. If you're unfamiliar with the service, it's basically a podcasting client. But instead of

00:01:53.780 --> 00:02:00.020
just serving up my content from my servers like most players do, unmodified, Stitcher downloads my MP3

00:02:00.020 --> 00:02:06.040
files, re-encodes them into a low bandwidth format, and then slices it apart and inserts their own audio

00:02:06.040 --> 00:02:10.980
ads into my show without any form of revenue share whatsoever. I don't get a penny from them for this.

00:02:11.480 --> 00:02:17.440
To me, this is unacceptable. I did a big write-up about it and you can find the link to the write-up

00:02:17.440 --> 00:02:22.200
in the show notes. The article was entitled Stitcher and Talk Python Podcast, a farewell letter.

00:02:22.200 --> 00:02:28.640
Next up, on a positive note, a review. I want to say thank you to Jamal Moyer who did an excellent

00:02:28.640 --> 00:02:33.980
review of my WritePython at Code course called The Course Everyone New to Python Desperately Needs to

00:02:33.980 --> 00:02:39.280
Take. It's an interesting view on what's good and what's not so good about my course. I gave Jamal access

00:02:39.280 --> 00:02:43.340
because I was interested in his opinion. I didn't know he'd write a review, but I'm really glad he did.

00:02:43.340 --> 00:02:47.780
Thank you, Jamal. You can find the link to his review in the show notes. Jamal actually has a

00:02:47.780 --> 00:02:52.960
bunch of great Python content, so I encourage you to go check out his site. Now, let's hear from Mike

00:02:52.960 --> 00:02:59.260
about making Python programs faster. Mike, welcome to the show. Hello. It's great to have you finally

00:02:59.260 --> 00:03:03.920
on the show. I've seen a lot of your presentations and I've always thought that they were really great.

00:03:04.640 --> 00:03:10.020
Today, we're going to take one of your PyCon 2016 presentations and sort of have a conversation

00:03:10.020 --> 00:03:15.200
around it about making Python programs faster. Yeah. Very nice to have me on the show. I listen

00:03:15.200 --> 00:03:19.000
to most of your podcasts. It's very, very interesting. It's an honor to be here.

00:03:19.000 --> 00:03:23.220
Thanks so much. Thanks so much. Well, let's get started by sharing your story. How did you get

00:03:23.220 --> 00:03:26.460
into programming in Python? I know you've been doing Python for a long time, right?

00:03:26.460 --> 00:03:30.520
Yeah. Actually, I pretty much know. I started Python in 1999.

00:03:30.920 --> 00:03:36.540
So I worked on my PhD thesis and the task was to couple numerical models. We had existing

00:03:36.540 --> 00:03:41.620
numerical models. One was written in Fortran, one was written in C and I looked for a language

00:03:41.620 --> 00:03:44.940
to couple. I knew I'm not going to write it in Fortran or C. I just want to have something

00:03:44.940 --> 00:03:50.080
else. And I looked at different languages, Java and other things. And then I somehow hit Python

00:03:50.080 --> 00:03:56.280
and I asked on a mailing list, would be a good language to couple things. And then I got a call

00:03:56.280 --> 00:04:02.840
from Martin van Leeuwen. He's a German, he's a Python core developer now for quite a while.

00:04:02.840 --> 00:04:08.500
And he just talked me into Python. And that's how I got started. So I used it for my PhD thesis

00:04:08.500 --> 00:04:12.380
and coupled models. And I got into work, which was not really clear if this would work because

00:04:12.380 --> 00:04:17.140
numerical models from different fields. And I put them together and made them work as one

00:04:17.140 --> 00:04:20.260
model, as one program. And it worked out pretty well.

00:04:20.260 --> 00:04:22.940
Oh, that's really cool. What was the subject of those models?

00:04:22.940 --> 00:04:28.940
So actually, my background is hydrology. So I coupled a lake model, a groundwater model,

00:04:28.940 --> 00:04:35.320
and a hydrogeochemical model. So you have this, after mining, you have, you do surface mining.

00:04:35.320 --> 00:04:41.180
And then you take something out of the subsurface, which is the purpose of mining. And then you have

00:04:41.180 --> 00:04:46.260
a void left. And this void typically fills with water, with groundwater and forms a lake. Now you have

00:04:46.260 --> 00:04:50.720
this, what's called the pit lake. And the pit lake is pretty different than an actual lake in terms of

00:04:50.720 --> 00:04:55.080
water quality. And that was the purpose to find out the water quality of this lake.

00:04:55.080 --> 00:05:00.460
And there were hydrocarbular, this lake model, which kind of takes care of the lake itself.

00:05:00.460 --> 00:05:04.100
And the groundwater model, which takes care of the groundwater part and the hydrochemistry,

00:05:04.100 --> 00:05:10.380
which is pretty different from a lake model. Typic lake models are very different systems determined

00:05:10.380 --> 00:05:15.180
by algae growth and stuff like this. And now this lake is different because you have these chemicals

00:05:15.180 --> 00:05:19.040
going in there. Actually, you have a very acidic lake most of the time that depends on the

00:05:19.040 --> 00:05:23.240
nature of this thing. So if you draw down the groundwater table, there are some kind of directions going on.

00:05:23.240 --> 00:05:28.640
And they take things out of the subsurface and then the groundwater moves over the stuff into the lake.

00:05:28.640 --> 00:05:32.840
And you have a very acidic system, which is very different than the natural system.

00:05:32.840 --> 00:05:35.840
That's what I was working on. And for this one, I need a new model.

00:05:35.840 --> 00:05:41.120
And I cuddled it and I did it with Python. And Python worked out very well for this.

00:05:41.120 --> 00:05:46.520
Was it controversial or a big risk that you took to try to do this with Python?

00:05:46.520 --> 00:05:53.400
No, I was pretty free. So in academia, just the end result counts. Nobody really was into programming.

00:05:53.400 --> 00:05:58.040
All these people are experts in groundwater and lakes and stuff. But the programming part,

00:05:58.040 --> 00:06:02.200
I was pretty much on my own and I could decide whatever I want. And nobody asked how I do it.

00:06:02.200 --> 00:06:03.720
Just as long as it works, it works.

00:06:03.720 --> 00:06:08.020
Yeah, that's cool. Yeah, I suppose probably the major alternatives were things like MATLAB or

00:06:08.020 --> 00:06:10.100
not other programming languages necessarily.

00:06:10.100 --> 00:06:15.740
Yeah, yes. I looked at different systems and first you have to get into this, how you're going to do

00:06:15.740 --> 00:06:20.920
this. And then you say you can extend Python with C and then you can connect C to Fortran. And that's

00:06:20.920 --> 00:06:25.360
how it did it actually. I wrote at this time, we didn't have tools yet. At least I wasn't aware of

00:06:25.360 --> 00:06:30.200
this F2Py tool, which I use now regularly. It didn't exist yet. So I wrapped every single

00:06:30.200 --> 00:06:35.920
subroutine in Fortran with C and then every single C function with Python by hand, which is a

00:06:35.920 --> 00:06:37.640
very tedious undertaking.

00:06:37.640 --> 00:06:41.580
I can't imagine, but you probably learned a lot doing it, right?

00:06:41.580 --> 00:06:46.500
I learned a lot. I learned a lot. I learned a lot about the C API and I tried to keep it very simple,

00:06:46.500 --> 00:06:49.840
which was still complicated enough because for every function, I had to study this,

00:06:49.840 --> 00:06:54.180
how to do it and looked at some examples. And this time we didn't have stack overflow or anything

00:06:54.180 --> 00:06:59.000
like this. You know, everything was just, if you email us, if best, and everything was much,

00:06:59.000 --> 00:07:02.400
much slower than nowadays in terms of getting help on the internet.

00:07:02.400 --> 00:07:07.660
Yeah, absolutely. Learning to program or learning the right techniques was completely different back

00:07:07.660 --> 00:07:08.320
then, wasn't it?

00:07:08.320 --> 00:07:08.900
Yeah.

00:07:08.900 --> 00:07:14.440
Yeah, cool. So we're going to talk about making Python programs faster.

00:07:14.440 --> 00:07:15.180
Yes.

00:07:15.180 --> 00:07:21.920
Yeah. And you did a really great presentation at PyCon and we'll kind of go through the details

00:07:21.920 --> 00:07:28.240
there. But in general, you know, how much does performance matter? Like on one hand, it might be

00:07:28.240 --> 00:07:33.480
great to have faster code, but on the other, maybe we just get more VMs or pay a little more for our

00:07:33.480 --> 00:07:34.640
cloud bill or something like this.

00:07:34.640 --> 00:07:39.100
The answer is clear. It depends. It's very, very depends what you're going to do. For a lot of people,

00:07:39.100 --> 00:07:43.640
Python is plenty fast. So I think for most cases, actually, Python is pretty fast. For most web

00:07:43.640 --> 00:07:49.160
developers, I talk to them and they're pretty happy with it because most of the time the database or the

00:07:49.160 --> 00:07:53.720
network or something, it's what causes the problem. Most of the time, Python is fast enough for this.

00:07:54.120 --> 00:08:01.140
But I also teach a lot of scientists and engineers and depends what they're doing. If they just move data from A to B,

00:08:01.140 --> 00:08:06.400
then Python might be fast enough. But if they do real simulations, then Python is way too slow for a lot of things.

00:08:06.400 --> 00:08:13.040
So it depends very much what you're doing. And if they have the right case, then you need to do something to make Python

00:08:13.040 --> 00:08:18.400
faster. That's one thing. But having a faster is always a good thing. Typically, faster is better than slower.

00:08:18.400 --> 00:08:22.680
And sometimes it doesn't take a lot of effort to make things faster. That's one case.

00:08:22.680 --> 00:08:27.040
So sometimes it's just making things a bit nicer and as a side effect, making it faster.

00:08:27.040 --> 00:08:33.140
And the other thing is you put a lot of effort in to make it faster. So you make it actually more difficult to

00:08:33.140 --> 00:08:38.720
understand, but you make it faster because you need it. I think these are two areas here.

00:08:38.720 --> 00:08:45.160
Yeah, I think you're right. Certainly the computational stuff is really, really important. And I suppose with the

00:08:45.160 --> 00:08:51.100
websites, it depends a little bit if you're a web developer on what is fast enough and how fast you have to be.

00:08:51.240 --> 00:08:58.740
There was a really interesting study. I think it was by Amazon. It was something around like the price of latency.

00:08:58.740 --> 00:09:01.380
And it said something like for

00:09:01.380 --> 00:09:07.860
for every hundred milliseconds that our site is slower, we lose, you know, some percentage, like

00:09:07.860 --> 00:09:14.300
several percent of business, maybe one, like something like one percent or something of business. And

00:09:14.300 --> 00:09:17.400
you know, if you have a lot of traffic that also really matters, right?

00:09:17.400 --> 00:09:22.220
Yeah. So performance matters. Just a question if Python is the one that that is a bottleneck for the

00:09:22.220 --> 00:09:27.340
performance, it's not totally clear. There can be many other things that cause your site to be slow.

00:09:27.340 --> 00:09:31.320
It doesn't have necessarily the programming language. There can be other considerations.

00:09:31.320 --> 00:09:36.340
Absolutely. For example, if you've got a million records and you're doing a non-indexed query on your

00:09:36.340 --> 00:09:38.700
database, it probably doesn't matter what language you're using.

00:09:38.700 --> 00:09:39.680
Yeah.

00:09:39.680 --> 00:09:39.740
Yeah.

00:09:39.740 --> 00:09:44.060
All right. Cool. So let's talk about your tutorial. What was it called?

00:09:44.260 --> 00:09:50.220
Yeah. It was faster Python programs, measure, don't guess. So you should to stress actually on the

00:09:50.220 --> 00:09:58.340
measuring part. I started out actually in 2007. It was my first two tutorials at PyCon US in Dallas,

00:09:58.340 --> 00:10:02.960
Texas, my first PyCon. And I gave two tutorials right away, which was amazing. And actually it was

00:10:02.960 --> 00:10:08.420
a two part one. The first one was more or less what I have here, the measuring. And the second part was

00:10:08.420 --> 00:10:14.120
actually extending Python with other languages, like C extensions and using other tools there.

00:10:14.180 --> 00:10:19.120
And now it's just the first part. And this is stress actually in measuring, just getting a grip on your

00:10:19.120 --> 00:10:24.180
system to find out what's going on and where are potential points to improve. I think it's very

00:10:24.180 --> 00:10:29.840
important to have something to quantify what's going on and see where the problems are and then

00:10:29.840 --> 00:10:35.620
maybe come up as a solution. And that's very important to measure things and to make it quantified.

00:10:35.620 --> 00:10:38.980
You just have a gut feeling it's slow enough to do something that's not really going to work.

00:10:38.980 --> 00:10:43.860
Yeah, I totally agree. When I started out doing professional programming, I did,

00:10:44.100 --> 00:10:51.320
sort of scientific visualization tools. I found that lots of people, including my intuition,

00:10:51.320 --> 00:10:57.440
was really quite wrong often about what was fast and what was slow. Maybe have like some really

00:10:57.440 --> 00:11:03.060
complicated like wavelet algorithm. You think, oh, this has got to be just super slow and some other

00:11:03.060 --> 00:11:07.560
place where you're just working with a basic data structure. And it turns out like the majority of

00:11:07.560 --> 00:11:11.400
the time you're fiddling with the data structure or something like this, right? So this measurement

00:11:11.400 --> 00:11:12.780
idea is really important.

00:11:12.780 --> 00:11:17.680
Yeah, I can confirm that. I do it for quite a while and most of the time I have a kind of feeling

00:11:17.680 --> 00:11:22.300
what's going on. But very often actually I'm wrong. So for some reason there's something else I didn't

00:11:22.300 --> 00:11:26.960
think about it, which is very clear afterwards. You say, of course, that's the reason, but you didn't

00:11:26.960 --> 00:11:27.920
think about it beforehand.

00:11:27.920 --> 00:11:32.620
Yeah, and that's coming from somebody who spent a lot of time doing this optimization, right?

00:11:33.120 --> 00:11:37.180
So even with the experience, sometimes it still can be hard.

00:11:37.180 --> 00:11:42.440
Experience helps. It helps. It helps. But it's no guarantee that you get it right. So you have to

00:11:42.440 --> 00:11:42.640
measure.

00:11:42.640 --> 00:11:46.780
Right. And of course, you can look at code. You can say, well, I'm sure that this version of that code

00:11:46.780 --> 00:11:52.520
is faster and this version of the code is slower. But if it's 10 microseconds versus 20 microseconds,

00:11:52.520 --> 00:11:56.060
who cares? It really matters how you're using the code as well.

00:11:56.860 --> 00:12:01.900
So one thing I thought was interesting when you talked about your PyCon tutorial was that

00:12:01.900 --> 00:12:07.540
you said, we're going to use Python 3. And I've been on a big push to evangelize Python 3

00:12:07.540 --> 00:12:11.600
and so on lately as well. And what was the reason you said it?

00:12:11.600 --> 00:12:16.960
Yeah. So of course, I think you have Python 3 and you have legacy Python. So I would like

00:12:16.960 --> 00:12:22.760
to use the current version of Python 3. I always, because I teach Python for a living, and I always

00:12:22.760 --> 00:12:26.840
use Python 3 whenever possible. Because when you're outside in the real world, there's

00:12:26.840 --> 00:12:31.480
still a lot of systems running 2.7. But still, I like to use Python 3 for teaching. And so

00:12:31.480 --> 00:12:35.960
that's a nice way to do it. And if you want to do Python 2, then it's not that difficult

00:12:35.960 --> 00:12:39.900
to write programs with the same source code that run with Python 2 and Python 3 if it just

00:12:39.900 --> 00:12:44.900
takes a few steps. So it's all this deprecated stuff in Python 2. And you're mainly there.

00:12:44.900 --> 00:12:50.540
It pulls from future prints, something like this. And then you are there. So most of the things

00:12:50.540 --> 00:12:54.980
you can write. And that's why you are ready to run in Python 3. And you still have to

00:12:54.980 --> 00:12:59.320
support Python 2 for some reason. And you still can do this. And that's very important, because

00:12:59.320 --> 00:13:04.620
otherwise, you might redo a lot of work later on when you now kind of focus on Python 2.7,

00:13:04.620 --> 00:13:07.580
even though all your colleagues are still on 2.7.

00:13:07.580 --> 00:13:12.740
Yeah, absolutely. I agree with you. I think that's great. You know, from futures import, was it Python?

00:13:12.740 --> 00:13:17.600
Sorry, print function. And then maybe, maybe range equals x range.

00:13:18.100 --> 00:13:19.020
Those kind of things.

00:13:19.020 --> 00:13:24.760
Those kinds of things. And then you can write your Python 3 code in a way that is compatible

00:13:24.760 --> 00:13:28.680
with Python 2, long as you, the other way, you can get yourself more into trouble, I think.

00:13:28.680 --> 00:13:29.260
Yeah.

00:13:29.260 --> 00:13:30.440
Yeah, very interesting.

00:13:30.440 --> 00:13:34.900
The PyData balloon, I just gave it a troll about writing code that runs with Python 2 and

00:13:34.900 --> 00:13:39.180
3 at the same time. And actually, I think the best thing is use the Python future org library.

00:13:39.180 --> 00:13:43.580
Maybe you're familiar with this. This gives you a lot of whatever you said, but on a much higher

00:13:43.580 --> 00:13:48.840
level, there are a thousand tests on it. And instead of making your own kind of compatibility,

00:13:48.840 --> 00:13:54.320
they just use this library and they take care of a lot of details, including rearranging the standard

00:13:54.320 --> 00:13:59.680
library imports and stuff like this, which can be really useful. So you pretty much write Python 3 code

00:13:59.680 --> 00:14:06.540
that also runs on Python 2. So in Python 3, nothing changes, but on Python 2, you get all this

00:14:06.540 --> 00:14:11.460
compatibility stuff in there automatically, which is very nice. And I think it's a good solution.

00:14:11.460 --> 00:14:17.900
Yeah, that's fantastic. And in 2020, coming up, there's going to be no more Python 2 support.

00:14:17.900 --> 00:14:18.440
Yeah, yeah.

00:14:18.440 --> 00:14:23.140
And that seems like that's way in the future. It seems like there could be a science fiction

00:14:23.140 --> 00:14:26.080
show set in 2020, but that's like three and a half years away.

00:14:26.080 --> 00:14:31.240
Yeah, I still remember 10 years ago, like yesterday, so it's not a long time.

00:14:31.240 --> 00:14:36.120
Yeah, that's right. That's right. Let's dig into some of the topics of your tutorial.

00:14:36.120 --> 00:14:41.360
You started out by saying there are some general guidelines for measuring and understanding

00:14:41.360 --> 00:14:42.240
performance.

00:14:42.240 --> 00:14:46.260
The first thing is, it's the execution. How fast is fast enough? So the customer says,

00:14:46.260 --> 00:14:51.660
do I have a case in the first case? So do I need to do something? And that's a lot of things.

00:14:51.660 --> 00:14:56.540
Just look at your system and say, do I need the performance? What am I going to do with it?

00:14:56.540 --> 00:15:00.920
It's a one-way thing, and I'm going to throw away. It doesn't matter if it runs half an hour or one

00:15:00.920 --> 00:15:05.880
hour if I throw it away. So if I spend another day improving it, just to be half an hour faster and

00:15:05.880 --> 00:15:10.600
never use it. Like scientists write a lot of one-way just moving data from here to there or something.

00:15:10.600 --> 00:15:15.200
That could be the case. Or the other, you have to have some realistic use cases. Is this really

00:15:15.200 --> 00:15:20.320
too slow? What am I going to do with it? Those kind of questions. It seems very common sense,

00:15:20.320 --> 00:15:24.240
but actually most of the things are common sense. You just need to think through. Do I have a case?

00:15:24.240 --> 00:15:28.080
Do I need to improve it? And look through. And then again, as I said, if you're in a web

00:15:28.080 --> 00:15:33.220
application, maybe it's a database. Check how fast the database is. The network connection is really

00:15:33.220 --> 00:15:37.460
Python. Can you gain anything out of it? And how much can you potentially gain?

00:15:37.460 --> 00:15:42.840
Sometimes you can say, okay, I can gain not more than 20% anyway. Is it worth the effort?

00:15:42.840 --> 00:15:48.320
If you have to pay for the CPUs and you use a lot of CPU, maybe 20% is worth the effort.

00:15:48.920 --> 00:15:53.780
If you just wait overnight and if it takes six hours or seven hours to calculate,

00:15:53.780 --> 00:15:58.100
so scientific calculation can easily take as long. It doesn't really matter because it runs

00:15:58.100 --> 00:16:02.360
overnight. It doesn't matter if it's finished in the morning. It's fine. Those kinds of things.

00:16:02.360 --> 00:16:06.100
Those are very common things. So those things you need to maybe reiterate after a while,

00:16:06.100 --> 00:16:08.320
then you do something if you have a case. That's the first thing.

00:16:08.320 --> 00:16:13.660
Right. And then the next one you said was sort of the whole story around premature optimization.

00:16:13.660 --> 00:16:17.680
Like don't optimize as you go. Write your code in the most understandable way

00:16:17.680 --> 00:16:21.280
so that it works. And then think about performance, right?

00:16:21.280 --> 00:16:24.780
Yeah. First make it right, then make it fast. That's very important.

00:16:24.780 --> 00:16:28.700
Yeah. Yeah. And the make it fast part is all about the measurement. Yeah?

00:16:28.700 --> 00:16:33.420
Yeah. The first thing that I've ever focused here on the measurement, how to measure.

00:16:33.960 --> 00:16:39.180
There are several tools out here. I use most of the time C-profile, which comes with Python,

00:16:39.180 --> 00:16:43.620
which is a good tool. The other tools now actually are the Intel is doing a big thing. I just have this

00:16:43.620 --> 00:16:50.780
beta stage kind of profiler, which is probably quite a bit better because when you measure something,

00:16:50.780 --> 00:16:56.320
you always influence your system. There's no way out of it. So the problem, the question is just how much

00:16:56.320 --> 00:17:02.700
you influence the system. If it's 1%, it seems to be okay. If it's 100%, it doesn't sound very right.

00:17:02.700 --> 00:17:07.760
Yeah? So the question is you don't know how much you influence the system most of the time.

00:17:07.760 --> 00:17:12.900
And then you want to have some kind of profiler that doesn't interfere too much with your things,

00:17:12.980 --> 00:17:19.660
especially if you measure time and things are very short, then the timing might be too coarse to give

00:17:19.660 --> 00:17:21.360
you some reliable results.

00:17:21.360 --> 00:17:26.540
Right. Or the overhead to time, very, very small function calls might be 10 times the cost of the

00:17:26.540 --> 00:17:27.460
function calls themselves.

00:17:27.460 --> 00:17:32.020
Yeah. And that's kind of tricky how to do this, how to measure this actually.

00:17:32.020 --> 00:17:37.680
And you can't avoid this, right? This is sort of the Heisenberg uncertainty principle kind of like

00:17:37.680 --> 00:17:41.340
it behaves in one way until you observe it, then it behaves in a different way.

00:17:41.500 --> 00:17:46.400
So it seems really important to me to just look at the difference, to measure one version,

00:17:46.400 --> 00:17:51.100
change your algorithm and measure it again, rather than try to just, you know,

00:17:51.100 --> 00:17:54.220
poke at it and say, well, I ran this test and now this is slow, right?

00:18:08.220 --> 00:18:13.260
This portion of Talk Python To Me has been brought to you by Rollbar. One of the frustrating things

00:18:13.260 --> 00:18:18.480
about being a developer is dealing with errors, relying on users to report errors, digging through

00:18:18.480 --> 00:18:23.280
log files, trying to debug issues, or a million alerts just flooding your inbox and ruining your day.

00:18:23.280 --> 00:18:28.780
With Rollbar's full stack error monitoring, you'll get the context insights and control that you need

00:18:28.780 --> 00:18:34.740
to find and fix bugs faster. It's easy to install. You can start tracking production errors and deployments

00:18:34.740 --> 00:18:39.600
in eight minutes or even less. Rollbar works with all the major languages and frameworks,

00:18:39.600 --> 00:18:44.480
including the Python one such as Django, Flask, Pyramid, as well as Ruby, JavaScript, Node,

00:18:44.480 --> 00:18:50.320
iOS, and Android. You can integrate Rollbar into your existing workflow, send error alerts to Slack or

00:18:50.320 --> 00:18:55.020
HipChat, or even automatically create issues in Jira, Pivotal Tracker, and a whole bunch more.

00:18:55.560 --> 00:18:59.740
Rollbar has put together a special offer for Talk Python To Me listeners. Visit rollbar.com

00:18:59.740 --> 00:19:05.760
slash talkpython to me, sign up, and get the bootstrap plan free for 90 days. That's 300,000

00:19:05.760 --> 00:19:10.380
errors tracked all for free. But hey, just between you and me, I really hope you don't encounter that

00:19:10.380 --> 00:19:15.580
many errors. Loved by developers at awesome companies like Heroku, Twilio, Kayak, Instacart,

00:19:15.580 --> 00:19:21.080
Zendesk, Twitch, and more. Give Rollbar a try today. Go to rollbar.com slash talkpython to me.

00:19:21.080 --> 00:19:33.760
Yeah, that's very, very important. To compare to something, okay, that's this version. This

00:19:33.760 --> 00:19:38.480
version, the difference is very small, but this version is twice as long. Why do we spend so much

00:19:38.480 --> 00:19:43.700
effort to make it just a little bit faster? Maybe it's not, it's not, it's much easier to understand

00:19:43.700 --> 00:19:49.480
shorter version than a longer version. And again, 10% and just say how often is this called and what

00:19:49.480 --> 00:19:52.080
does 10% mean for my whole application?

00:19:52.080 --> 00:19:57.420
Right. Absolutely. You've gone through and you've measured it. Now you've decided, okay,

00:19:57.420 --> 00:20:01.180
it's time to make it faster. And you had some really good advice that you probably shouldn't

00:20:01.180 --> 00:20:04.260
start trying to make it faster before you have good test coverage, right?

00:20:04.260 --> 00:20:09.500
Yeah. That's a big problem, especially with scientists. So even though science is supposed

00:20:09.500 --> 00:20:15.900
to be reproducible, test coverage is not something that a lot of scientists use. You cannot generalize

00:20:15.900 --> 00:20:21.620
this, but very often a lot of scientists, depends how much they program, they might not even use

00:20:21.620 --> 00:20:26.720
version control in any way or something because they're not aware of it. Some people do for sure,

00:20:26.720 --> 00:20:31.720
but some don't. And also they don't know what measuring means just run it and look at their

00:20:31.720 --> 00:20:37.400
results and say, that's right. So when I say tests, you want to have automated tests or something

00:20:37.400 --> 00:20:41.720
that you can reproduce later on. And that's a big problem in scientific community. Depends

00:20:41.720 --> 00:20:47.820
where else some scientists do this, but some don't do it all. And then if you try to increase the

00:20:47.820 --> 00:20:52.840
performance, you change your code. And when you change your code, you might change behavior and you

00:20:52.840 --> 00:20:56.700
don't want to change the behavior. You want to have the same result at the end, but just a bit faster.

00:20:56.700 --> 00:21:00.500
Yeah, absolutely. You don't want it to be really fast and wrong. That would be terrible.

00:21:00.500 --> 00:21:07.220
Yeah. But I think testing scientific results can be super tricky, right? You change the algorithm and

00:21:07.220 --> 00:21:14.240
you've got floating point cutoffs and different heuristics and it might still model the thing

00:21:14.240 --> 00:21:19.320
correctly, but in a slightly different way. And so you can't just do like a old value, equal,

00:21:19.320 --> 00:21:21.560
equal, new value sort of analysis, right?

00:21:21.560 --> 00:21:28.120
You need some help. So you need to do something like NumPy provides a few helpers. NumPy is a package

00:21:28.120 --> 00:21:32.440
used in scientific computing quite a bit. And they have, they have offers some helpers to compare

00:21:32.440 --> 00:21:38.440
arrays with some tolerance. So you can say, I want to compare the tolerance can be X. So it doesn't have

00:21:38.440 --> 00:21:43.720
to be exactly the same. There can be small variations, which is totally normal if you do numerical calculations

00:21:43.720 --> 00:21:49.080
and you do a lot of, a lot of them, then there will be some small differences, those kinds of things.

00:21:49.080 --> 00:21:51.960
There are some tools there that make the work easier.

00:21:51.960 --> 00:21:56.760
Yeah. I think there's a couple of really interesting speed ups that you talked about.

00:21:56.760 --> 00:22:02.520
And one of them, I'm sure it's obvious once you know about it, but depending on your background,

00:22:02.520 --> 00:22:05.880
where you're coming from, maybe you just have no awareness or it's not on your mind in terms

00:22:05.880 --> 00:22:11.160
of thinking about performance, but alternate interpreters or runtimes, right? Like under certain

00:22:11.160 --> 00:22:15.960
circumstances, you might be able to switch your code from say CPython to PyPy and get pretty

00:22:15.960 --> 00:22:18.840
serious performance improvements, right?

00:22:18.840 --> 00:22:23.560
Yeah, that's true. So PyPy can be very nice, especially for numerical competitions for

00:22:23.560 --> 00:22:29.320
loops. And when you write a for loop in pure Python and use only integer or floats in there,

00:22:29.320 --> 00:22:34.920
typically PyPy can find this out and just make it faster. So it compiles it to machine code in the

00:22:34.920 --> 00:22:41.480
background. And very often you get close to C speed. So somewhere the range of C speed, then writing the

00:22:41.480 --> 00:22:46.440
same loop in C, which can be easily factor of 10 faster or even more than pure Python.

00:22:46.440 --> 00:22:52.360
Yeah. So if you need a 20% speed up and you can get your code right on PyPy, maybe you're already done,

00:22:52.360 --> 00:22:52.520
right?

00:22:52.520 --> 00:22:58.440
Yeah. Yeah. Very likely you're done. PyPy, of course, they have this benchmark and I think it's a very

00:22:58.440 --> 00:23:05.640
big benchmark and they factor 6.5 times faster. So that's way more than 20% for average benchmark.

00:23:05.640 --> 00:23:11.240
I think they don't have a single one where they are slower, actually. The worst thing is we get the

00:23:11.240 --> 00:23:17.320
same thing. So as soon as you have a problem, it's not going to work for something like Mercurial,

00:23:17.320 --> 00:23:24.680
where the time is in starting the interactive program. So of course, PyPy, I think it's slow

00:23:24.680 --> 00:23:29.640
on startup. It works for things that are computationally expensive and we repeat it many times. If you do

00:23:29.640 --> 00:23:35.640
something only twice here and there, PyPy doesn't have any way to do it. It has to be done like 10,000

00:23:35.640 --> 00:23:39.960
times and then PyPy says, okay, it's doing almost the same thing. I can compile this with the machine

00:23:39.960 --> 00:23:45.080
code and then you can get a speed up, which is pretty close to C. Actually, they had this one contrived

00:23:45.080 --> 00:23:52.680
example where PyPy was faster than C because PyPy managed to inline some function calls where C just

00:23:52.680 --> 00:23:59.400
was steadily compiled and called some library functions. And PyPy was bit faster than C, which

00:23:59.400 --> 00:24:02.760
is of course, it's pretty contrived, but still you can get close to C.

00:24:02.760 --> 00:24:09.080
Yeah, it's really cool to see those examples though, that that's a possibility, right? And if you use it

00:24:09.080 --> 00:24:14.440
in the right circumstances, it's very cool. Let's suppose we've decided that we need to make our code

00:24:14.440 --> 00:24:20.120
faster. And just to keep the conversation a little bit simpler, let's say we'll stick to CPython for

00:24:20.120 --> 00:24:25.640
for whatever reason. So you talked about a couple of tools that we can use to understand our code

00:24:25.640 --> 00:24:30.280
and a variety of them. And the first thing you talked about was this thing called PyStone. What's that?

00:24:30.280 --> 00:24:33.800
Yeah, PyStone is just a benchmark, a simple benchmark. And benchmarks are always

00:24:33.800 --> 00:24:38.920
wrong in one case, in one way, because benchmarks are designed to measure some kind of performance.

00:24:38.920 --> 00:24:43.640
But maybe they're better than us. PyStone comes with Python. So it comes with the Python standard

00:24:43.640 --> 00:24:48.840
installation. And you can run PyStones and this gives you a feeling of, because it's pure Python,

00:24:48.840 --> 00:24:54.360
Python. So it runs with CPython, with PyPy, with Jison, with IronPython, whatever you want to test it

00:24:54.360 --> 00:24:58.840
with. And you can see how fast the Python installation is. And of course, it's also connected to your

00:24:58.840 --> 00:25:04.440
hardware. But if you run it the same machine, then you get a PyStone. You can also have this

00:25:04.440 --> 00:25:09.160
kilo PyStone. So something can, of course, get a big number, but also gives you a time.

00:25:09.160 --> 00:25:14.760
The bigger the PyStone, the better or the shorter the time, of course. And you can see if you run this

00:25:14.760 --> 00:25:20.280
over this PyStone, you can have a very rough comparison of different Python interpreters.

00:25:20.280 --> 00:25:24.600
And of course, if you run on different hardfares, the hardware will be included in this thing.

00:25:24.600 --> 00:25:29.320
Right. For example, if you're working on your dev machine, but then you have some big virtual machine

00:25:29.320 --> 00:25:34.120
in the cloud that you're actually going to run your code on, you could say, well, if it's taking this long

00:25:34.120 --> 00:25:39.480
here and I can compare the PyStones from my dev machine versus production, then you get like a

00:25:39.480 --> 00:25:42.600
little bit of a sense for the scaling factor up or down, right?

00:25:42.600 --> 00:25:47.720
Yeah. So it's very, so it's pretty controversial if PyStone is a good benchmark. So it's one of the

00:25:47.720 --> 00:25:51.640
benchmarks. If you want to, you can take other benchmarks. You could go for the PyPy benchmark

00:25:51.640 --> 00:25:56.760
and see how this works or other benchmarks around to run it. But it just gives, the thing is simple

00:25:56.760 --> 00:26:01.080
because it's there. You don't have to go and make it work because it's supposed to work

00:26:01.080 --> 00:26:05.240
with this part of the standard library. And as far as I know, it works with giant

00:26:05.240 --> 00:26:10.600
sin. It works with iron Python. It works with PyPy. It works with CPython from the old to

00:26:10.600 --> 00:26:14.840
Python 2 and Python 3 everything. So you can get a feeling how fast your Python is.

00:26:14.840 --> 00:26:19.720
And then the next thing you talked about is using C profile to measure your code.

00:26:19.720 --> 00:26:23.800
A bunch of profilers around. One of them is profile that's written in Python, which has a lot of

00:26:23.800 --> 00:26:28.920
overhead. So it's not really recommended. And C profile is, as the name suggests, written in C.

00:26:28.920 --> 00:26:30.200
So the overhead is much less.

00:26:30.200 --> 00:26:33.640
But it only works on CPython, right? It won't work on something like PyPy, will it?

00:26:33.640 --> 00:26:39.800
It works only on CPython. So I focus this on CPython because it's more about the principles

00:26:39.800 --> 00:26:45.400
than about the tools. In particular, I would say just get people kind of a workflow how to do this.

00:26:45.400 --> 00:26:49.320
And if you have a different setup, you might need to use a different system. Like if you work on

00:26:49.320 --> 00:26:55.080
Json, you might use some profilers that job-up gives you or something. I'm not an expert in this field,

00:26:55.080 --> 00:26:59.640
but the procedure is the same. So you want to measure. And my caution is always,

00:26:59.640 --> 00:27:06.840
those tools can be wrong. Sometimes you get strange results and you always have to question it. But you

00:27:06.840 --> 00:27:13.320
might find a second or third way to measure pretty much the same thing and see, look at the results

00:27:13.320 --> 00:27:17.240
correspond somehow. If you get totally different results with different tools for the same

00:27:18.040 --> 00:27:22.920
kind of measurement, then there must be something wrong with the tool or how you apply the tool.

00:27:22.920 --> 00:27:25.400
Maybe you just use it in the wrong way.

00:27:25.400 --> 00:27:30.360
Right. Or even there's some really crazy overhead like you're calling a function a million times,

00:27:30.360 --> 00:27:34.680
but the measurement of it is so much larger that it's just completely out of whack, right?

00:27:34.680 --> 00:27:35.080
Yeah.

00:27:35.080 --> 00:27:35.160
Yeah.

00:27:35.160 --> 00:27:35.800
Yeah.

00:27:35.800 --> 00:27:42.280
Okay. And so if I had like some method and I wanted to profile this, what's the steps? Like,

00:27:42.280 --> 00:27:44.840
what do I do with C profile to get this rolling?

00:27:44.840 --> 00:27:51.240
So first I shows this slow way to do it just with the standard library. So it's just import C profile. You

00:27:51.240 --> 00:27:57.560
have to make instance of a class profile, and then you have several abilities or possibilities

00:27:57.560 --> 00:28:02.440
how to call the method. You just can put the method in these arguments. You can also use a string,

00:28:02.440 --> 00:28:09.720
let's evaluate a string, and then as Python code and get the results. And then you get an object that

00:28:09.720 --> 00:28:14.840
it represents the results and it calls different kinds of print or other evaluation methods on it.

00:28:14.840 --> 00:28:20.120
And it shows you the results. The three profile only goes by function. So the finest resolution,

00:28:20.120 --> 00:28:25.720
you see how much time it takes to run one function call. It shows you how many times a function being

00:28:25.720 --> 00:28:34.120
called, how long it took per call, and how long it took in total. You have a lot of ways to sort this

00:28:34.120 --> 00:28:38.920
by the most calls, the longest times. And you can also sort who called whom. So you can have a

00:28:38.920 --> 00:28:43.640
relationship, this function called this function. So you get some kind of a graph there.

00:28:43.640 --> 00:28:48.760
Yeah. Okay. Yeah. That's really nice. And then you also use Jupyter Notebooks to do some cool stuff

00:28:48.760 --> 00:28:55.080
there. Yeah. So if you're mind it easier. So Jupyter Notebook is actually the killer application of

00:28:55.080 --> 00:29:01.400
Python, I would say. Because of Jupyter Notebooks, they have what's called %PRUN, which is kind of a wrapper

00:29:01.400 --> 00:29:07.480
around C profile. There's no new functionality in this case, but it makes it much more convenient. So instead of doing

00:29:07.480 --> 00:29:12.200
doing three steps, importing, making instance, you just say P run. And P run takes some like

00:29:12.200 --> 00:29:16.360
command line arguments. So you can specify how many times you want to repeat or whatever you want to do.

00:29:16.360 --> 00:29:22.360
And then it runs your function call and gives you all the results in the same way, which is very nice.

00:29:22.360 --> 00:29:29.080
So that's, if you use the Jupyter Notebook, used to be called IPyce Notebook, but nowadays it's Jupyter Notebook.

00:29:29.080 --> 00:29:35.800
You should use it anyways. It comes from the scientific field and it's used a lot. So if you go to your

00:29:35.800 --> 00:29:42.520
PsyPy or PsyPy conference, pretty much everybody's using Notebook. But it can be useful for everybody who's

00:29:42.520 --> 00:29:48.760
programming a person just to try something out. And this profiling means to try something out, try something

00:29:48.760 --> 00:29:54.440
again and again. So the Notebook is great for this. And this gives you pretty much everything you can do with

00:29:54.440 --> 00:29:59.880
the C profile, just much shorter and much more convenient. And if things are shorter, then you're

00:29:59.880 --> 00:30:05.240
more inclined to do something. So if it makes it more convenient to do something, then you will do it more

00:30:05.240 --> 00:30:07.160
often and you measure more often, I would say.

00:30:07.160 --> 00:30:11.080
Yeah, the easier you can make this and the more automated you can make it, the more people will

00:30:11.080 --> 00:30:15.080
do it, right? And with Jupyter Notebooks, you can go back and you see the cells that you ran before

00:30:15.080 --> 00:30:18.920
and you just kind of like rerun them again if you want to retry it. And yeah, it's really nice.

00:30:18.920 --> 00:30:25.400
Yeah. And then of course you get a table output, but then you say, you know, a picture saves more

00:30:25.400 --> 00:30:30.600
than a thousand words. There are some nice tools to actually visualize it. And I use this one called

00:30:30.600 --> 00:30:37.000
SnakeWiz. So you just run SnakeWiz on the output. So the C profile, you can store all this profiling data

00:30:37.000 --> 00:30:43.400
in a file. I think just Marshall, just put there. And then you read it back with SnakeWiz, for instance,

00:30:43.400 --> 00:30:48.600
and it gives you a nice picture. So you can look in the browser and it's an interactive thing. So it's

00:30:48.600 --> 00:30:53.320
running a, this file is interactive. So you can zoom in and look at different things. It gives you everything

00:30:53.320 --> 00:30:59.480
in a nice colorful picture, which is really helpful because you can see in a part of a second what's

00:30:59.480 --> 00:31:04.760
going on, what function takes most time. So you can look at it and then you can click it and you see,

00:31:04.760 --> 00:31:09.320
have all the information. It's nothing different than the table. But if you have a big table with

00:31:09.320 --> 00:31:14.360
a lot of numbers, it takes you a lot of time to make sense out of it. And this diagram you get there,

00:31:14.360 --> 00:31:16.360
it's much more convenient to use.

00:31:16.360 --> 00:31:21.080
Yeah. Certainly when you have tons of data, almost anything like having a really fantastic

00:31:21.080 --> 00:31:27.240
way to slice and dice and visualize it makes all the difference. And this SnakeWiz is really cool. So

00:31:27.240 --> 00:31:32.600
the way it works is basically you run the profiler and instead of printing out the stats to the terminal

00:31:32.600 --> 00:31:39.320
or something, you save them to a binary file. And then this SnakeWiz can like suck that up and it runs a

00:31:39.320 --> 00:31:42.840
little local web server that then opens your browser and you cruise around in there, right?

00:31:42.840 --> 00:31:47.560
Yeah. Yeah. Yeah. It's very nice. And you have, you have the table and you can also sort the tables,

00:31:47.560 --> 00:31:51.880
tables and there also, so you have the raw numbers if you want to, but you also have this picture and

00:31:51.880 --> 00:31:57.160
they have two different ones. They have one is a circle, like the outermost function is innermost

00:31:57.160 --> 00:32:01.320
circle and that's going outwards. And you see the circles getting smaller and smaller, like fractions of

00:32:01.320 --> 00:32:08.120
a circle. It's very nice visualization. The other one is called icicles. So instead of a circle, you have squares

00:32:08.120 --> 00:32:15.400
of areas that are, or not rectangles actually, stacked on top of each other, which also kind of symbolize this

00:32:15.400 --> 00:32:18.040
function is calling this function. You see where the time goes.

00:32:18.040 --> 00:32:23.000
Yeah. That's really cool. And I think, you know, if you have a program that you care about performance,

00:32:23.000 --> 00:32:29.320
run this through it really quick and just look at the picture and you'll probably learn something right away.

00:32:30.040 --> 00:32:34.760
The case is if you see one big function that uses all the time, then you know where to go.

00:32:34.760 --> 00:32:39.480
If it's very evenly distributed among a lot of functions, then it's much harder because we're

00:32:39.480 --> 00:32:43.480
going to start. You have to look at all these functions at the end. So that's, that's the first

00:32:43.480 --> 00:32:48.760
thing. You see, you have a case. If you all the time is in this function, then it's very worthwhile to

00:32:48.760 --> 00:32:54.840
look at its functions. If you have 20 functions, so they all take about 5% of the time. Yeah. We're

00:32:54.840 --> 00:32:58.440
going to start. So it's, it will be much more works to work through all these functions.

00:32:58.760 --> 00:33:03.640
Right. It's, it's not just low hanging fruit, but it's, it's something, it's something different.

00:33:03.640 --> 00:33:09.800
Right. So that's CPU profiling. And one of the ways your code can be slow is it's just

00:33:09.800 --> 00:33:15.160
computationally expensive, or you could also discover, I guess, external things that are slow.

00:33:15.160 --> 00:33:21.320
It could be, you know, the cumulative time for like a disk IO or a database call or a web service call

00:33:21.320 --> 00:33:24.440
could be really large. And that would tell you that that thing is slow. Right.

00:33:24.440 --> 00:33:29.480
Yes. Yeah. Actually you can do it as a C profile. C profile typically measures wall clock time.

00:33:29.480 --> 00:33:35.480
So the time that actually elapses from the beginning to the start, but you can also also provide your own

00:33:35.480 --> 00:33:41.080
timing function and you could do something like CPU time. So CPU time is where the CPU spends doing

00:33:41.080 --> 00:33:46.040
something. So if you have, if you use something like time sleep in your code, say time sleep,

00:33:46.040 --> 00:33:51.880
two seconds, and the wall clock time will be two seconds, but the CPU time will be nearly zero because

00:33:51.880 --> 00:33:56.200
the processor is not doing nothing actually just waiting for you to do this time sleep.

00:33:56.200 --> 00:34:01.560
So this is very important to see. So if you do input output, you wouldn't see the wait for something to

00:34:01.560 --> 00:34:06.840
that, that it happens in time sleep. Just let the processor rest in, in terms of your process,

00:34:06.840 --> 00:34:10.280
of course, the processor is still working, but not for your process, not what you're measuring.

00:34:10.280 --> 00:34:14.360
And that's an important thing. So that's what I try to get across that most of the time you measure

00:34:14.360 --> 00:34:19.800
wall clock time. Unfortunately, that difference is between windows and Unix systems. What if you,

00:34:19.800 --> 00:34:26.440
especially in Python 2, if you just use a time.time, which gives you a timestamp on Unix,

00:34:26.440 --> 00:34:31.960
which is nice on windows, it's way too close to do anything useful. Therefore you should use time.it

00:34:31.960 --> 00:34:37.880
default timer. So something I was like to get across because we very often, you see a lot of online

00:34:37.880 --> 00:34:42.840
tutorials that use time time, which is totally fine on Unix. But if somebody, somebody is trying this in

00:34:42.840 --> 00:34:49.480
Windows, you might not get useful results because it's pretty coarse. And also there's a time.clock, which is much faster

00:34:49.480 --> 00:34:55.560
on Windows means measuring time on Windows, but measuring CPU times on Unix. So it's pretty, pretty messy

00:34:55.560 --> 00:35:01.480
here. So you might mix things up. You measure, might measure the wrong thing depending on your platform, which is

00:35:01.480 --> 00:35:07.640
not, it's not good. And therefore I give you like a small helper function that gives you also the CPU time on, on

00:35:07.640 --> 00:35:13.240
Windows. That's completely not obvious that the time is evaluated difference and sometimes even means

00:35:13.240 --> 00:35:16.840
something different on the different platforms. Yeah. Actually, I think it has a reason because it's

00:35:16.840 --> 00:35:21.400
just a thin wrap around C library. The C library is different on the platform. In Python 3, it's better.

00:35:21.400 --> 00:35:26.200
There's a time perth time, I think, which kind of attracts this away a little bit. So it's getting better.

00:35:26.200 --> 00:35:34.600
There's another reason I use Python 3. So you have a better abstraction of this, platform abstraction of this measurement.

00:35:48.600 --> 00:35:53.800
Continuous delivery isn't just a buzzword. It's a shift in productivity that will help your whole team become

00:35:53.800 --> 00:36:00.600
more efficient. With SnapCI's continuous delivery tool, you can test, debug and deploy your code quickly and reliably.

00:36:00.600 --> 00:36:06.680
Get your product in the hands of your users faster and deploy from just about anywhere at any time.

00:36:06.680 --> 00:36:11.640
And did you know that ThoughtWorks literally wrote the book on continuous integration and continuous delivery?

00:36:11.640 --> 00:36:16.840
Connect Snap to your GitHub repo and they'll build and run your first pipeline automagically.

00:36:17.400 --> 00:36:23.400
Thanks SnapCI for sponsoring this episode by trying them for free at snap.ci/talkpython.

00:36:23.400 --> 00:36:36.600
So that lets us measure things like web service calls and databases as well, which is really interesting.

00:36:36.600 --> 00:36:52.600
But sometimes it's more of a memory pressure or a memory issue. Like maybe the reason our code is slow is because actually we're running out of memory and it's like going to page on memory on disk for the virtual swap files and all that kind of stuff. Right? So can we profile memory as well?

00:36:52.600 --> 00:37:07.400
Profiling memory is not the simplest thing, but there are some tools out there. And I used to use Hippie, which is part of this GUPP project. But as far as I know, it's only Python 2 so far, if I'm not mistaken.

00:37:07.400 --> 00:37:13.400
Profiling memory is not the same. But there's another project called Pimpler, which is also a merger of three different projects in the past, and this supports Python 3.

00:37:13.400 --> 00:37:20.200
They do pretty much the same thing. And if you have the chance, actually, you can use both of them to compare the results with the Re, which is always good.

00:37:20.200 --> 00:37:22.200
Profiling memory is also a Pimpler, which is always good.

00:37:22.200 --> 00:37:24.200
Profiling memory is also a Pimpler, which is always good.

00:37:24.200 --> 00:37:26.200
Profiling memory is also a Pimpler, which is always good.

00:37:26.200 --> 00:37:28.200
Profiling memory is also a Pimpler, which is always good.

00:37:28.200 --> 00:37:30.200
Profiling memory is also a Pimpler, which is always good.

00:37:30.200 --> 00:37:32.200
Profiling memory is also a Pimpler, which is always good.

00:37:32.200 --> 00:37:34.200
Profiling memory is also a Pimpler, which is always good.

00:37:34.200 --> 00:37:36.200
Profiling memory is also a Pimpler, which is always good.

00:37:36.200 --> 00:37:38.200
Profiling memory is also a Pimpler, which is always good.

00:37:38.200 --> 00:37:40.200
Profiling memory is also a Pimpler, which is always good.

00:37:40.200 --> 00:37:44.200
Profiling memory is also a Pimpler, which is always good.

00:37:44.200 --> 00:37:46.200
Profiling memory is also a Pimpler, which is always good.

00:37:46.200 --> 00:37:48.200
Profiling memory is also a Pimpler, which is always good.

00:37:48.200 --> 00:37:50.200
Profiling memory is also a Pimpler, which is always good.

00:37:50.200 --> 00:37:52.200
Profiling memory is also a Pimpler, which is always good.

00:37:52.200 --> 00:37:54.200
Profiling memory is also a Pimpler, which is always good.

00:37:54.200 --> 00:37:56.200
Profiling memory is also a Pimpler, which is always good.

00:37:56.200 --> 00:37:58.200
Profiling memory is also a Pimpler, which is always good.

00:37:58.200 --> 00:38:00.200
Profiling memory is also a Pimpler, which is always good.

00:38:00.200 --> 00:38:02.200
Profiling memory is also a Pimpler, which is always good.

00:38:02.200 --> 00:38:04.200
Profiling memory is also a Pimpler, which is always good.

00:38:04.200 --> 00:38:06.200
Profiling memory is also a Pimpler, which is always good.

00:38:06.200 --> 00:38:08.200
Profiling memory is also a Pimpler, which is always good.

00:38:08.200 --> 00:38:10.200
Profiling memory is also a Pimpler, which is always good.

00:38:10.200 --> 00:38:12.200
Profiling memory is also a Pimpler, which is always good.

00:38:12.200 --> 00:38:14.200
Profiling memory is also a Pimpler, which is always good.

00:38:14.200 --> 00:38:16.200
Profiling memory is also a Pimpler, which is always good.

00:38:16.200 --> 00:38:18.200
Profiling memory is also a Pimpler, which is always good.

00:38:18.200 --> 00:38:20.200
Profiling memory is also a Pimpler, which is always good.

00:38:20.200 --> 00:38:22.200
Profiling memory is also a Pimpler, which is always good.

00:38:22.200 --> 00:38:24.200
Profiling memory is also a Pimpler, which is always good.

00:38:24.200 --> 00:38:26.200
Profiling memory is also a Pimpler, which is always good.

00:38:26.200 --> 00:38:28.200
Profiling memory is also a Pimpler, which is always good.

00:38:28.200 --> 00:38:30.200
Profiling memory is also a Pimpler, which is always good.

00:38:30.200 --> 00:38:34.200
So you can just use a Pimpler, which is always good for the Pimpler, which is always good for the Pimpler, which is always good.

00:38:34.200 --> 00:38:40.200
Profiling memory is also a Pimpler, which is always good for the Pimpler, which is always good for the Pimpler, which is always good for the Pimpler, which is always good for the Pimpler.

00:38:40.200 --> 00:38:48.200
And if you have a Pimpler, which is always good for the Pimpler, which is always good for the Pimpler, which is always good for the Pimpler.

00:38:48.200 --> 00:38:55.200
So you can write your own tooling that is totally tuned to what you want to do.

00:38:55.200 --> 00:39:03.200
So you have this basic tools and then Python makes it so easy to write some nice special tools for your use cases.

00:39:03.200 --> 00:39:04.200
Yeah, that was really cool.

00:39:04.200 --> 00:39:08.200
And I really liked the way that you're using decorators in your tutorial.

00:39:08.200 --> 00:39:11.200
Like here you have one called like, you know, measure memory or something like that.

00:39:11.200 --> 00:39:14.200
So you want to measure some function and all the functions it calls.

00:39:14.200 --> 00:39:17.200
Of course, you just say at measurement memory in front of it.

00:39:17.200 --> 00:39:21.200
Right. And then you get this little little summary, which is really cool.

00:39:21.200 --> 00:39:29.200
Yeah. So I guess if you want to answer the question of how many bytes is this, you know, five or 10 byte thing I've created, like this wouldn't be the way to do it.

00:39:29.200 --> 00:39:36.200
This is a way to get like large scale pictures of, oh, I have 10 million integers and I didn't expect that.

00:39:36.200 --> 00:39:37.200
What's the problem, right?

00:39:37.200 --> 00:39:38.200
Yeah.

00:39:38.200 --> 00:39:43.200
But there's also ways to like actual get object size from this library, isn't there?

00:39:43.200 --> 00:39:44.200
Like, yeah.

00:39:44.200 --> 00:39:44.200
Yeah.

00:39:44.200 --> 00:39:45.200
Yeah.

00:39:45.200 --> 00:39:49.200
Difficult to know the full like closure of an object graph size, right?

00:39:49.200 --> 00:39:50.200
Yeah.

00:39:50.200 --> 00:39:59.200
You can use this, get a size of your, the system module and the sound library has a size of, which gives you the memory size of one object, but it's only the object itself.

00:39:59.200 --> 00:40:05.200
Like you have a list, you get the size of the list, but you don't get the size of all the objects stored inside the list.

00:40:05.200 --> 00:40:08.200
And the objects that they store and the objects that those objects store, right?

00:40:08.200 --> 00:40:09.200
And so on.

00:40:09.200 --> 00:40:10.200
Yeah.

00:40:10.200 --> 00:40:12.200
Pimplot is giving you this so you can actually measure.

00:40:12.200 --> 00:40:14.200
And actually I have a very nice example.

00:40:14.200 --> 00:40:16.200
They're very interesting.

00:40:16.200 --> 00:40:24.200
So I start with an empty list and then I keep appending to this list, one integer after the other, up to a million or 10 million actually.

00:40:24.200 --> 00:40:29.200
And then every time I append, I measure the size of this object and see how it changes.

00:40:29.200 --> 00:40:30.200
And use two different ways.

00:40:30.200 --> 00:40:35.200
I use this kind of built-in, sys get size of, which gives you just the size of the list.

00:40:35.200 --> 00:40:42.200
And then the other one from Pimplot that gives you the whole memory, the list and everything, or the integers in my case use up.

00:40:42.200 --> 00:40:47.200
And you can see a very nice step function because that's how Python actually works.

00:40:47.200 --> 00:40:51.200
So you have a list and Python always allocates a bit more, roughly 50.

00:40:51.200 --> 00:40:54.200
There's a formula, but roughly 50% or so more memory.

00:40:54.200 --> 00:40:55.200
It fills this memory.

00:40:55.200 --> 00:40:59.200
And then when it hits the limit, it allocates more and more.

00:40:59.200 --> 00:41:07.200
So if you have 10 million appends, I get like 104 or so allocations, which is, because allocations are pretty expensive in terms of time.

00:41:07.200 --> 00:41:10.200
So this makes lists so efficient if you do append.

00:41:10.200 --> 00:41:16.200
If you use some other structure that would need to be rebuilt, or if you use, instead of appending to a list, you insert zero.

00:41:16.200 --> 00:41:18.200
You insert at the beginning of a list.

00:41:18.200 --> 00:41:25.200
That means every time you do this, you have to rebuild the whole list and you have to allocate all the memory again, which is a big anti-pattern actually.

00:41:25.200 --> 00:41:26.200
And it's very nice.

00:41:26.200 --> 00:41:27.200
You can visualize this.

00:41:27.200 --> 00:41:28.200
Why is it like this?

00:41:28.200 --> 00:41:30.200
And you can see a picture.

00:41:30.200 --> 00:41:34.200
And I think it always, if somebody tells you that's the case, it's one thing.

00:41:34.200 --> 00:41:38.200
If you do it yourself and you measure yourself, it's a very big difference in terms of experience.

00:41:38.200 --> 00:41:42.200
You see, okay, I measured myself and I can play around with it and see how things change.

00:41:42.200 --> 00:41:43.200
Yeah.

00:41:43.200 --> 00:41:47.200
I thought the graph was really cool because it made it very clear what the algorithm was.

00:41:47.200 --> 00:41:51.200
You can see that it's basically trading a little bit of memory for a lot of performance.

00:41:51.200 --> 00:41:52.200
Yeah.

00:41:52.200 --> 00:41:53.200
In the general case, right?

00:41:53.200 --> 00:41:54.200
In the general case.

00:41:54.200 --> 00:41:55.200
Yeah.

00:41:55.200 --> 00:41:56.200
And obviously you can buy memory.

00:41:56.200 --> 00:42:02.200
So buy a bit more memory just cost you a few bucks, but make making a CPU a thousand times faster is pretty hard.

00:42:02.200 --> 00:42:03.200
I would say.

00:42:03.200 --> 00:42:04.200
Yeah, absolutely.

00:42:04.200 --> 00:42:12.200
So the other thing that you showed in your tutorial that I thought was really cool that I don't think we've touched on yet is line by line measurements.

00:42:12.200 --> 00:42:18.200
So you could do line by line CPU profiling as well as allocation management, right?

00:42:18.200 --> 00:42:19.200
Yes.

00:42:19.200 --> 00:42:20.200
You can do this.

00:42:20.200 --> 00:42:23.200
So there's, there's a line profiler from Robert Cron.

00:42:23.200 --> 00:42:28.200
So, as an author and actually he ported it to Python 3 now, because I asked him several times to do this.

00:42:28.200 --> 00:42:32.200
And now the line profiler works with Python 3, which is, it gives you a lot of overhead.

00:42:32.200 --> 00:42:41.200
So it makes your program much, much slower, but gives you a line by line breakdown because the C profile is just the smallest increment you get as a function.

00:42:41.200 --> 00:42:48.200
And you can do a line by line profiling, which is especially important if you write a function and you have like calls to NumPy.

00:42:48.200 --> 00:42:50.200
So in a function, you might have three, four, five calls to NumPy.

00:42:50.200 --> 00:42:54.200
Then you can see which call takes the most time, for instance.

00:42:54.200 --> 00:42:55.200
Right.

00:42:55.200 --> 00:42:57.200
And this is what the line by line profiling is doing.

00:42:57.200 --> 00:42:58.200
Yeah.

00:42:58.200 --> 00:43:07.200
So maybe you, you see profile and, and those sorts of techniques to go and figure out, all right, well, I know it's really this function that is a problem, but it's, it's 10 or, you know, hopefully not a thousand.

00:43:07.200 --> 00:43:08.200
It's 10 lines long.

00:43:08.200 --> 00:43:10.200
What's actually slow about that, right?

00:43:10.200 --> 00:43:12.200
Then you can turn on this line profiler business.

00:43:12.200 --> 00:43:13.200
Yeah.

00:43:13.200 --> 00:43:14.200
Exactly.

00:43:14.200 --> 00:43:16.200
Because you only want to have one or two functions.

00:43:16.200 --> 00:43:24.200
You use a decorator, add profile, you know, you put a decorator called profile there, and then you run it through your profile with some options.

00:43:24.200 --> 00:43:27.200
It is this line profiler, and then it makes it very, very slow.

00:43:27.200 --> 00:43:30.200
So either maybe factor of a hundred slow or something like this.

00:43:30.200 --> 00:43:38.200
It feels very slow because it just has to go line by line and check everything what's, what's doing and gives you a nice breakdown.

00:43:38.200 --> 00:43:46.200
You see, okay, allocating this list or this number is array takes so much time and then looping over it takes so much time and doing this takes so much time.

00:43:46.200 --> 00:43:51.200
So you could see pretty much where the time goes, which is, can be a way to be useful educational.

00:43:51.200 --> 00:43:58.200
So you can get a feeling what these functions are doing, but also useful to make your program faster.

00:43:58.200 --> 00:44:03.200
You also talked about some anti patterns, things that you should try to avoid or, you know.

00:44:03.200 --> 00:44:04.200
Yeah.

00:44:04.200 --> 00:44:05.200
Can you go through a couple of those?

00:44:05.200 --> 00:44:06.200
Yeah.

00:44:06.200 --> 00:44:10.200
One is I just said is about maybe the most important one is about the list.

00:44:10.200 --> 00:44:13.200
So if you have a list, then you want to append at the end.

00:44:13.200 --> 00:44:16.200
And maybe when you, when you're done, you just reverse the list.

00:44:16.200 --> 00:44:21.200
If you want the other way around, instead of inserting at the position zero, which is what you want.

00:44:21.200 --> 00:44:23.200
And this would be big anti pattern.

00:44:23.200 --> 00:44:27.200
And this can be many orders of magnitude difference in terms of performance.

00:44:27.200 --> 00:44:30.200
If you do this one, and this is one of the biggest anti patterns.

00:44:30.200 --> 00:44:37.200
And now the anti patterns is like string concatenation, which is an old one, which it's now kind of optimized in CPython.

00:44:37.200 --> 00:44:46.200
So instead of saying string plus equals new string, string plus equals new string in a loop, you just use a list and append to the list.

00:44:46.200 --> 00:44:49.200
Take advantage of this list behavior.

00:44:49.200 --> 00:44:51.200
And then at the end, you just join the list.

00:44:51.200 --> 00:44:52.200
Yeah.

00:44:52.200 --> 00:44:54.200
Because the list do this pre allocation thing where strings don't.

00:44:54.200 --> 00:44:55.200
Yeah.

00:44:55.200 --> 00:44:57.200
So this is actually, it's not really true anymore.

00:44:57.200 --> 00:44:58.200
It used to be the beginning of Python.

00:44:58.200 --> 00:45:00.200
So I started with Python 1.5 too.

00:45:00.200 --> 00:45:02.200
This was my first Python version.

00:45:02.200 --> 00:45:03.200
And it was pretty true, I think.

00:45:03.200 --> 00:45:06.200
And then pretty soon they changed, they optimized it away.

00:45:06.200 --> 00:45:08.200
And it works with CPython.

00:45:08.200 --> 00:45:13.200
But if you use PyPy, which is supposed to be much faster, then this anti pattern just kicks you.

00:45:13.200 --> 00:45:20.200
Then if you do this for a string of like say 100,000 characters, it just kills the performance.

00:45:20.200 --> 00:45:23.200
And you have to wait an hour or so instead of a second.

00:45:23.200 --> 00:45:24.200
Wow.

00:45:24.200 --> 00:45:25.200
Yeah.

00:45:25.200 --> 00:45:29.200
I don't really, I remember, I just read it somewhere.

00:45:29.200 --> 00:45:32.200
There's a reason that it didn't put it in this optimization.

00:45:32.200 --> 00:45:35.200
So you never know if CPython always can optimize it.

00:45:35.200 --> 00:45:39.200
Because for simple cases that I use string literals and stuff like this, it should be easy.

00:45:39.200 --> 00:45:43.200
If it maybe is a code a bit more complex, who knows if it's still up to an optimization.

00:45:43.200 --> 00:45:46.200
So it's better not to rely on this.

00:45:46.200 --> 00:45:49.200
As long as the string is short, it's fine.

00:45:49.200 --> 00:45:50.200
But it gets a bit longer than...

00:45:50.200 --> 00:45:51.200
Yeah.

00:45:51.200 --> 00:45:52.200
Yeah, it's a problem.

00:45:52.200 --> 00:45:55.200
Maybe you're streaming stuff out of a CSV file and trying to build up a thing.

00:45:55.200 --> 00:45:57.200
And maybe it can no longer optimize that.

00:45:57.200 --> 00:45:58.200
Who knows, right?

00:45:58.200 --> 00:45:59.200
Yeah.

00:45:59.200 --> 00:46:00.200
Who knows?

00:46:00.200 --> 00:46:07.200
Another interesting example you had was looking, sort of optimizing variable and function

00:46:07.200 --> 00:46:08.200
lookups.

00:46:08.200 --> 00:46:14.200
Like caching global things like square root versus math.square root, for example, in a function.

00:46:14.200 --> 00:46:15.200
Those kind of things.

00:46:15.200 --> 00:46:16.200
So it depends what you're doing.

00:46:16.200 --> 00:46:19.200
It can be, it can give you a little bit speed up.

00:46:19.200 --> 00:46:23.200
So because that's just the basic thing how Python looks up names.

00:46:23.200 --> 00:46:30.200
So first looks in local namespace in your function, then goes to the global namespace, which is your module, and then goes to the built-in namespace.

00:46:30.200 --> 00:46:33.200
And every lookup is kind of a dictionary lookup.

00:46:33.200 --> 00:46:41.200
And instead of going like this global, even the built-in namespace, like if you use sum, the built-in sum, then every time you use sum,

00:46:41.200 --> 00:46:44.200
Python has to go and hunt for sum until it finds it.

00:46:44.200 --> 00:46:52.200
And so if you say, okay, my sum equals sum in a function, for instance, then you make it a local variable, and you avoid like two lookups for every time you use sum.

00:46:52.200 --> 00:46:53.200
This can be useful.

00:46:53.200 --> 00:46:56.200
Yeah, if that's in a tight loop, then maybe that becomes a problem, right?

00:46:56.200 --> 00:46:57.200
Or a loop within a loop.

00:46:57.200 --> 00:47:00.200
So if it's a tight loop, you don't do much else, that might be something.

00:47:00.200 --> 00:47:10.200
If you do a lot of other heavy computations, or the sum itself takes a long time to sum something big, then you still get the speed up from the lookup avoidance.

00:47:10.200 --> 00:47:14.200
But percentage-wise, it might be just half a percent faster.

00:47:14.200 --> 00:47:16.200
It might not make a lot of difference.

00:47:16.200 --> 00:47:21.200
If you don't do much else, you can get several 10% faster behavior or something like this.

00:47:21.200 --> 00:47:26.200
So it depends very much on your use case, but at least you can try, and this would be one thing.

00:47:26.200 --> 00:47:34.200
So if you call a function or some built-in or global variable in the inner loop again and again and again, making it local might speed up things a bit.

00:47:34.200 --> 00:47:35.200
Sure.

00:47:35.200 --> 00:47:47.200
You had a cool example where one of the points you were making was saying, look, sometimes it's an algorithm, and you just need to look, you know, not optimize a function a little bit, but you need to rethink how you're doing something.

00:47:47.200 --> 00:47:54.200
And you had this created, it was a contrived example that was computational, but you were basically computing pi using the Monte Carlo method.

00:47:54.200 --> 00:48:01.200
And you said the naive way to do this would be to sort of generate a couple of random numbers, do some math, save some values, and do this in a loop.

00:48:01.200 --> 00:48:08.200
And you said, well, look, if we do this with a different algorithm like using NumPy, we get dramatically different performance, right?

00:48:08.200 --> 00:48:09.200
Yeah.

00:48:09.200 --> 00:48:13.200
So NumPy, the algorithm with the Monte Carlo algorithm is maybe the worst algorithm to calculate pi.

00:48:13.200 --> 00:48:18.200
There are much better algorithms out there than give you pi with 100 digits with 100 iterations or so.

00:48:18.200 --> 00:48:23.200
So in Monte Carlo, 100 iterations would take the age of the universe to calculate, I guess.

00:48:23.200 --> 00:48:34.200
So it's computationally, but it makes a good example because it's very slow and you can use a lot of techniques to improve it because also it's embarrassing parallel.

00:48:34.200 --> 00:48:40.200
So I use it later on, actually, to do some parallel calculations with multiprocessing and other means.

00:48:40.200 --> 00:48:47.200
So it's a very good example because I use the same example with many different kind of approaches, which is useful.

00:48:47.200 --> 00:48:49.200
So it is simple enough to understand.

00:48:49.200 --> 00:48:53.200
So I use this and then you just run it in normal Python.

00:48:53.200 --> 00:49:02.200
And as I said, one of the things where Python is slow is when you use numerical computations and you use 4x in range, 4y in range loops, something like this.

00:49:02.200 --> 00:49:04.200
This is pretty slow.

00:49:04.200 --> 00:49:13.200
And if you just use NumPy and NumPy is doing this vectorize, so instead of writing a loop, then you vector, you just call the function in NumPy.

00:49:13.200 --> 00:49:17.200
The fact that when you program NumPy, you should pretty much never write a loop.

00:49:17.200 --> 00:49:20.200
You should always vectorize things if you want to make it fast.

00:49:20.200 --> 00:49:27.200
And then NumPy just runs us with a function which is running in C, when you get near C speed for a lot of things.

00:49:27.200 --> 00:49:28.200
So it makes it much faster.

00:49:28.200 --> 00:49:29.200
Yeah, yeah.

00:49:29.200 --> 00:49:30.200
That was really cool.

00:49:30.200 --> 00:49:31.200
I like that example.

00:49:31.200 --> 00:49:34.200
What if people missed this, right?

00:49:34.200 --> 00:49:37.200
This was actually done in June.

00:49:37.200 --> 00:49:39.200
So if they weren't there, they've already missed it.

00:49:39.200 --> 00:49:43.200
There's a couple of opportunities to still check out your presentation though, right?

00:49:43.200 --> 00:49:45.200
Yeah, the presentation does, you can go to YouTube.

00:49:45.200 --> 00:49:48.200
If you go to PyVideo, it's on YouTube.

00:49:48.200 --> 00:49:50.200
If you search for it, you will find it.

00:49:50.200 --> 00:49:54.200
So if you go to the PyCon US site, then it should be there.

00:49:54.200 --> 00:49:56.200
So if you look for faster bicycle.

00:49:56.200 --> 00:49:57.200
Yeah, and I'll link to it in the show notes as well.

00:49:57.200 --> 00:49:59.200
Yeah, you can link to it.

00:49:59.200 --> 00:50:05.200
And also I will give this tutorial again at Europython in just in a few weeks in Bilbao,

00:50:05.200 --> 00:50:06.200
in Spain.

00:50:06.200 --> 00:50:09.200
So then there will be this tutorial again.

00:50:09.200 --> 00:50:10.200
So I will talk about it.

00:50:10.200 --> 00:50:13.200
Yeah, Europython is going to be very exciting.

00:50:13.200 --> 00:50:15.200
That'll be a good conference.

00:50:15.200 --> 00:50:16.200
Yeah, it's a nice conference.

00:50:16.200 --> 00:50:17.200
I gave it last year there.

00:50:17.200 --> 00:50:22.200
So it seems like because there's always demand for it and people like it.

00:50:22.200 --> 00:50:27.200
So it's just because it's, I try to always make it hands on so you can follow everything.

00:50:27.200 --> 00:50:28.200
So there's nothing.

00:50:28.200 --> 00:50:31.200
It's too sophisticated, which is on purpose.

00:50:31.200 --> 00:50:34.200
Because if you show too sophisticated code, people won't understand.

00:50:34.200 --> 00:50:37.200
So everything is pretty simple and you can use it right away.

00:50:37.200 --> 00:50:40.200
And hopefully when you understand, you can apply it to your own use case.

00:50:40.200 --> 00:50:42.200
That's the whole purpose of the thing.

00:50:42.200 --> 00:50:43.200
Yeah, that's great.

00:50:43.200 --> 00:50:45.200
So we just have a few minutes left in the show.

00:50:45.200 --> 00:50:50.200
Before we call it quits, let's talk a little bit about what else you have going on in the Python space.

00:50:50.200 --> 00:50:54.200
I've been in the Python space for quite a while, since 1999.

00:50:54.200 --> 00:50:58.200
Actually, I spent my most, actually my professional life in the Python space.

00:50:58.200 --> 00:50:59.200
I teach Python.

00:50:59.200 --> 00:51:03.200
So I'm the founder of Python Academy, which is now more than 10 years old, actually.

00:51:03.200 --> 00:51:05.200
And that's where I teach Python.

00:51:05.200 --> 00:51:08.200
I gave my first Python course in 2004.

00:51:08.200 --> 00:51:09.200
It's quite a while ago.

00:51:09.200 --> 00:51:15.200
So I taught university level courses in different fields, but in scientific fields.

00:51:15.200 --> 00:51:20.200
So I had some experience in teaching and gave a lot of talks at conferences.

00:51:20.200 --> 00:51:23.200
And it was just a small step to Python courses.

00:51:23.200 --> 00:51:24.200
So just how it developed.

00:51:24.200 --> 00:51:26.200
I really like teaching.

00:51:26.200 --> 00:51:27.200
It seems like it fits.

00:51:27.200 --> 00:51:30.200
So people understand most of the time what I'm saying.

00:51:30.200 --> 00:51:33.200
And so I started teaching Python.

00:51:33.200 --> 00:51:34.200
And meanwhile, Python Academy grew.

00:51:34.200 --> 00:51:37.200
Now we have about 11 teachers.

00:51:37.200 --> 00:51:42.200
So not everybody's teaching all the time, but I have a lot of different teachers doing web development,

00:51:42.200 --> 00:51:48.200
testing, Sison, database programming, and have also teachers that have different native languages.

00:51:48.200 --> 00:51:52.200
So I myself can teach in German, which is my native, and in English.

00:51:52.200 --> 00:51:57.200
But I also have people teaching in Italian and Polish and maybe in other language later on.

00:51:57.200 --> 00:52:02.200
So some people might be taught in their own native language, which might help to understand.

00:52:02.200 --> 00:52:03.200
Yeah, absolutely.

00:52:03.200 --> 00:52:05.200
And congratulations on the success there.

00:52:05.200 --> 00:52:06.200
That's awesome.

00:52:06.200 --> 00:52:11.200
I've been deeply involved in training and training companies for the last 10 years myself.

00:52:11.200 --> 00:52:14.200
And I know what it takes to put those together and keep them running.

00:52:14.200 --> 00:52:15.200
Yeah.

00:52:15.200 --> 00:52:16.200
It's a lot of effort.

00:52:16.200 --> 00:52:19.200
So I myself, Python Academy, we focus on everything Python.

00:52:19.200 --> 00:52:23.200
So very wide range in terms of Python, but deep in there.

00:52:23.200 --> 00:52:28.200
So I teach beginners, people that barely programmed before, or other programmers,

00:52:28.200 --> 00:52:32.200
so professional programmers that wanted to switch to Python, but also advanced Python.

00:52:32.200 --> 00:52:35.200
I have advanced Python course, but that's a Python track.

00:52:35.200 --> 00:52:38.200
I also teach scientific tools, which is a very wide field.

00:52:38.200 --> 00:52:47.200
So I teach how to use an IPython on Jupyter Notebook and NumPy and some SciPy tools and Matplotlib and Pandas,

00:52:47.200 --> 00:52:53.200
those kind of things, which is in very high demand in terms, because scientists need these tools all day long.

00:52:53.200 --> 00:52:54.200
Yeah, absolutely.

00:52:54.200 --> 00:52:55.200
What's your favorite one to teach?

00:52:55.200 --> 00:52:56.200
It depends.

00:52:56.200 --> 00:52:57.200
So I like to have variations.

00:52:57.200 --> 00:53:00.200
I like the scientific thing.

00:53:00.200 --> 00:53:08.200
And actually, but I typically like in a course, I like to include examples from the people and actually do a little bit of live programming in the course.

00:53:08.200 --> 00:53:17.200
So they have a problem and say, okay, I need to, scientists are often to have to read in this file A and have to convert it into file B, which is a very common thing.

00:53:17.200 --> 00:53:18.200
And Python really shines in this.

00:53:18.200 --> 00:53:23.200
So you can actually in the course ad hoc, develop a smaller routine and read in the file.

00:53:23.200 --> 00:53:24.200
There's different tools.

00:53:24.200 --> 00:53:25.200
So you can do it.

00:53:25.200 --> 00:53:26.200
It's just standard library.

00:53:26.200 --> 00:53:30.200
You can use Pandas, which is great for CSV files and something like this.

00:53:30.200 --> 00:53:35.200
And then you say, okay, now I have this file and I've assigned the data and then I write it back somewhere.

00:53:35.200 --> 00:53:40.200
And this is very often very useful because then I can use a lot of things I taught in the course.

00:53:40.200 --> 00:53:47.200
So how to do this, how to write a function, how to write a doc string in a function and how to make a function, refactor the function because it's getting too big.

00:53:47.200 --> 00:53:50.200
Everything, but on a small scale, so understandable for people.

00:53:50.200 --> 00:53:52.200
And I see it's really useful if you do this.

00:53:52.200 --> 00:53:55.200
So you can reuse this function again and again, do something.

00:53:55.200 --> 00:54:02.200
That's mainly for scientists that very often scientists use programming as a tool, but they're not so deep into programming very often.

00:54:02.200 --> 00:54:05.200
As compared to professional programmers, for them, there's nothing new.

00:54:05.200 --> 00:54:10.200
They pretty much understand when I say something, they understand the concept and that's pretty easy.

00:54:10.200 --> 00:54:11.200
You can go on.

00:54:11.200 --> 00:54:23.200
For scientists, you have to give an example because just explaining it in this very kind of generic examples might not be kind of really good enough to understand what is useful, can be useful for.

00:54:23.200 --> 00:54:28.200
Right, because they spend most of their time working on physics or biology or something real, right?

00:54:28.200 --> 00:54:29.200
Yeah, something real.

00:54:29.200 --> 00:54:30.200
So it depends.

00:54:30.200 --> 00:54:32.200
Some of them tend to become programmers.

00:54:32.200 --> 00:54:34.200
Very often scientists turn programmers.

00:54:34.200 --> 00:54:38.200
But many of them just use Python from time to time as a tool, as an important tool.

00:54:38.200 --> 00:54:42.200
And as soon as they get their result, they just forget about it and go on.

00:54:42.200 --> 00:54:48.200
Then I said, wait a minute, you can, now, the script works, but if you spend another half an hour, you can make it reusable.

00:54:48.200 --> 00:54:53.200
And next time you can save a lot of time because you put this in a function and you put a dong string.

00:54:53.200 --> 00:54:54.200
Maybe you put a test in there.

00:54:54.200 --> 00:54:55.200
So make it work.

00:54:55.200 --> 00:54:57.200
And it makes it much more useful.

00:54:57.200 --> 00:54:58.200
Even your colleague might be useful.

00:54:58.200 --> 00:55:04.200
Now, if you write this small function or several functions that can read this file format, then your colleague can use it.

00:55:04.200 --> 00:55:11.200
If you just have a script that's doing everything in one piece, then your colleague is probably not really able to use it without a lot of efforts.

00:55:11.200 --> 00:55:12.200
Right.

00:55:12.200 --> 00:55:14.200
You end up starting from scratch every single time.

00:55:14.200 --> 00:55:15.200
Yeah.

00:55:15.200 --> 00:55:16.200
Yeah.

00:55:16.200 --> 00:55:17.200
So it's kind of, it depends.

00:55:17.200 --> 00:55:18.200
Yeah.

00:55:18.200 --> 00:55:19.200
So that's what I like.

00:55:19.200 --> 00:55:20.200
I like the variety.

00:55:20.200 --> 00:55:25.200
So I like introductory courses, even though I taught it many times, but also like the advanced course.

00:55:25.200 --> 00:55:29.200
And lately I had a course where people really used meta classes.

00:55:29.200 --> 00:55:32.200
And I had to really dig deep into meta classes.

00:55:32.200 --> 00:55:34.200
There were some very hard meta class questions.

00:55:34.200 --> 00:55:42.200
I had to kind of think really hard because meta classes, but as a topic, most of the time, if you don't know what they're doing, you don't need them.

00:55:42.200 --> 00:55:44.200
And then you could stop the course.

00:55:44.200 --> 00:55:47.200
But sometimes they can be useful for certain types of things.

00:55:47.200 --> 00:55:51.200
And these people will use them to a high extent, which is interesting.

00:55:51.200 --> 00:55:52.200
Pretty challenging course though.

00:55:52.200 --> 00:55:53.200
Yeah.

00:55:53.200 --> 00:55:54.200
Cool.

00:55:54.200 --> 00:55:55.200
Yeah.

00:55:55.200 --> 00:55:56.200
It's great to have that spectrum.

00:55:56.200 --> 00:55:57.200
Do you also do consulting?

00:55:57.200 --> 00:55:59.200
So actually I do some scientific consulting.

00:55:59.200 --> 00:56:01.200
I just help some develop some lake model.

00:56:01.200 --> 00:56:04.200
So we do some consulting in scientific area.

00:56:04.200 --> 00:56:07.200
Of course not only me, so we have a team of a few people.

00:56:07.200 --> 00:56:10.200
So we also do consulting and program something.

00:56:10.200 --> 00:56:13.200
And sometimes you also connect to consulting actually pretty frequently.

00:56:13.200 --> 00:56:18.200
So develop a solution, but instead of handing over a black box solution, we do a workshop with it.

00:56:18.200 --> 00:56:23.200
So we explain how things work and how people can extend what we did.

00:56:23.200 --> 00:56:30.200
So you get a really solid solution, which is people probably like scientists would not be able to write themselves, but they can build on it.

00:56:30.200 --> 00:56:31.200
So they have this basics.

00:56:31.200 --> 00:56:32.200
This is a good foundation.

00:56:32.200 --> 00:56:38.200
You can build it and you can use it and increase it and make it more useful for their needs.

00:56:38.200 --> 00:56:39.200
Yeah, that's great.

00:56:39.200 --> 00:56:40.200
That's really helpful.

00:56:40.200 --> 00:56:42.200
And then finally, you've done a lot with conferences.

00:56:42.200 --> 00:56:47.200
You were involved with EuroSciPy, EuroPython, and so on, right?

00:56:47.200 --> 00:56:51.200
So it seems like I'm pretty much into conferences.

00:56:51.200 --> 00:56:56.200
So actually, I started pretty much, I was the guy behind pushing EuroSciPy.

00:56:56.200 --> 00:56:59.200
So I started here in Leipzig where I'm located.

00:56:59.200 --> 00:57:04.200
And 2008 and 2009, the first two EuroSciPys happened here and I was a chair.

00:57:04.200 --> 00:57:06.200
And now EuroSciPy has a great success.

00:57:06.200 --> 00:57:10.200
Then we had two conferences in Paris, two conferences in Brussels, two in Cambridge.

00:57:10.200 --> 00:57:16.200
And now we're back to Germany in Erlang and it will be in August, so just a bit more than a month from now.

00:57:16.200 --> 00:57:18.200
That will be the number nine EuroSciPy.

00:57:18.200 --> 00:57:19.200
So it seems like a success.

00:57:19.200 --> 00:57:22.200
People come to EuroSciPy and it's continuing.

00:57:22.200 --> 00:57:24.200
So I've been involved in EuroSciPy.

00:57:24.200 --> 00:57:27.200
And also I'm involved in PyConDE.

00:57:27.200 --> 00:57:29.200
So 2011, 2012, PyConDE.

00:57:29.200 --> 00:57:33.200
The German PyCon was here in Leipzig and I was a chair.

00:57:33.200 --> 00:57:41.200
And also, of course, PyConDE is run by the Python Software Verband, which is a kind of a German Python Software Association.

00:57:41.200 --> 00:57:43.200
And I happened to be the chair there also.

00:57:43.200 --> 00:57:45.200
So I'm a bit involved in community work.

00:57:45.200 --> 00:57:53.200
And then we also did EuroPython 2014 in Berlin and I was also the chair, which was a big conference, more than 1,200 people.

00:57:53.200 --> 00:57:55.200
Yeah, that must have been a lot of work to put that together.

00:57:55.200 --> 00:57:57.200
Yeah, so we had a very great team.

00:57:57.200 --> 00:57:58.200
So, of course, you have the chair.

00:57:58.200 --> 00:58:07.200
You just kind of oversee everything and try to delegate as much as possible, which is necessary because it's just impossible to do main or bother work yourself.

00:58:07.200 --> 00:58:09.200
You have to have a great team.

00:58:09.200 --> 00:58:14.200
And we had a lot of people that are very enthusiastic about it and put a lot of effort in there.

00:58:14.200 --> 00:58:16.200
And it was a great conference.

00:58:16.200 --> 00:58:19.200
I had a lot of good feedback from people about the conference.

00:58:19.200 --> 00:58:20.200
Yeah, cool.

00:58:20.200 --> 00:58:22.200
Are you involved in any upcoming ones?

00:58:22.200 --> 00:58:24.200
Yes, I'm at the EuroSciPy.

00:58:24.200 --> 00:58:28.200
So the Python Software Verband is also taking over the legal part of the EuroSciPy.

00:58:28.200 --> 00:58:33.200
So we do the – somebody has to move the money and sign the contracts and stuff like this.

00:58:33.200 --> 00:58:41.200
So I'm involved also and we use our software that we use for PyCon.de and EuroPython for EuroSciPy now, the web conferencing software.

00:58:41.200 --> 00:58:44.200
We are involved in – I'm involved there.

00:58:44.200 --> 00:58:46.200
But I'm also a little bit involved.

00:58:46.200 --> 00:58:49.200
We have what's called the Python Unconference in September.

00:58:49.200 --> 00:58:53.200
And we are – our software fund is a sponsor and we help and support a little bit.

00:58:53.200 --> 00:58:55.200
So we are not the main organizer, but we help.

00:58:55.200 --> 00:59:02.200
There will be likely – so we are very close actually – there will be PyCon.de in October in Munich.

00:59:02.200 --> 00:59:08.200
And we are involved there and there will be next year PyCon.de and EuroSciPy next year also.

00:59:08.200 --> 00:59:12.200
So – and I'm always involved to some degree in these conferences.

00:59:12.200 --> 00:59:18.200
I'm not doing the main work for sure, but I'm on the mailing list and I'm somehow involved in organizing.

00:59:18.200 --> 00:59:19.200
Yeah, that's cool.

00:59:19.200 --> 00:59:22.200
And it sounds like there's a lot of conferences coming up in Germany around Python.

00:59:22.200 --> 00:59:23.200
That's cool.

00:59:23.200 --> 00:59:24.200
Yeah, there's several of them.

00:59:24.200 --> 00:59:27.200
They have this bar camp, which is interesting in Cologne.

00:59:27.200 --> 00:59:29.200
I myself, I'm just a participant.

00:59:29.200 --> 00:59:31.200
Other people do this, which is pretty relaxing.

00:59:31.200 --> 00:59:33.200
So it's a kind of a – also like an unconference.

00:59:33.200 --> 00:59:35.200
So we have this unconference in Hamburg.

00:59:35.200 --> 00:59:38.200
So there's several events we have.

00:59:38.200 --> 00:59:45.200
So now actually we have not this one big one, but multiple smaller ones, which are more original, which can also be nice.

00:59:45.200 --> 00:59:48.200
So people don't have to say, if I miss one, then that's it.

00:59:48.200 --> 00:59:50.200
I have still two or three more alternatives.

00:59:50.200 --> 00:59:51.200
Right.

00:59:51.200 --> 00:59:53.200
A little bit of each is really great.

00:59:53.200 --> 00:59:56.200
Final two questions, Mike, before I let you go.

00:59:56.200 --> 00:59:59.200
When you write some Python code, what's your standard editor?

00:59:59.200 --> 01:00:01.200
What do you open up to code?

01:00:01.200 --> 01:00:04.200
Typically, I go with sublime nowadays.

01:00:04.200 --> 01:00:11.200
Sometimes I use Wing IDE if I have some projects, which of course has a nice debugging and refactorization things.

01:00:11.200 --> 01:00:17.200
For courses, I use Spider, which is kind of a scientifically inclined thing.

01:00:17.200 --> 01:00:18.200
So I use it for my courses.

01:00:18.200 --> 01:00:20.200
That's the one that comes with the Anaconda distribution, right?

01:00:20.200 --> 01:00:21.200
Yes.

01:00:21.200 --> 01:00:23.200
So Anaconda has it out of the box.

01:00:23.200 --> 01:00:26.200
If you don't, if you just install Anaconda, you have it.

01:00:26.200 --> 01:00:28.200
So Spider, it's not perfect.

01:00:28.200 --> 01:00:30.200
There's sometimes crushes, but it's okay.

01:00:30.200 --> 01:00:35.200
So of course, then I think it's, you don't have to tell people to install a different editor just to have one.

01:00:35.200 --> 01:00:37.200
And they have a debugger and they have an object.

01:00:37.200 --> 01:00:43.200
You can look at the objects, the Python objects for debugging and stuff like this, an interactive console, if you like.

01:00:43.200 --> 01:00:46.200
But most of the time, of course, actually I spend a notebook anyway.

01:00:46.200 --> 01:00:47.200
So, right.

01:00:47.200 --> 01:00:48.200
Yeah.

01:00:48.200 --> 01:00:49.200
The Jupyter notebooks is also really nice.

01:00:49.200 --> 01:00:50.200
That's cool.

01:00:50.200 --> 01:00:51.200
All right.

01:00:51.200 --> 01:00:56.200
And the 80,000 plus PyPI packages, what's your one to recommend that people maybe don't know about?

01:00:56.200 --> 01:00:59.200
The one that we talked about already is that Jupyter notebook is the killer app.

01:00:59.200 --> 01:01:04.200
So if you don't know Jupyter notebook, you're missing something out in the Python community.

01:01:04.200 --> 01:01:05.200
That's something you should go.

01:01:05.200 --> 01:01:08.200
So that's very interesting to work with Jupyter notebooks.

01:01:08.200 --> 01:01:09.200
Jupyter notebooks.

01:01:09.200 --> 01:01:11.200
I always have a notebook open to just try something out.

01:01:11.200 --> 01:01:13.200
If you have an idea, you can use it.

01:01:13.200 --> 01:01:17.200
It's a very nice mixture between an editor, a full editor and the interactive prompt.

01:01:17.200 --> 01:01:19.200
And that makes it.

01:01:19.200 --> 01:01:21.200
Maybe that's not the real secret.

01:01:21.200 --> 01:01:23.200
The one I'm looking right now is sconch.

01:01:23.200 --> 01:01:24.200
You know sconch?

01:01:24.200 --> 01:01:25.200
It's this interactive shell.

01:01:25.200 --> 01:01:26.200
Yeah.

01:01:26.200 --> 01:01:27.200
With an X.

01:01:27.200 --> 01:01:28.200
Yeah.

01:01:28.200 --> 01:01:30.200
There's an X, but it's pronounced sconch.

01:01:30.200 --> 01:01:33.200
And I know Anthony Scopeparts pretty well.

01:01:33.200 --> 01:01:41.200
We're hanging out at PyCon lately and he's a funny person and very knowledgeable on top of this.

01:01:41.200 --> 01:01:45.200
And he wrote a very interesting tool which gives you a kind of a shell that incorporates Python.

01:01:45.200 --> 01:01:46.200
So in a nutshell.

01:01:46.200 --> 01:01:47.200
It's very interesting.

01:01:47.200 --> 01:01:49.200
So you get a shell that incorporates Python.

01:01:49.200 --> 01:01:51.200
And then I think it works on Windows.

01:01:51.200 --> 01:01:56.200
So you get a much more powerful shell on Windows because the shell on Windows is typically not that great.

01:01:56.200 --> 01:02:03.200
So if you need to work on Windows sometime, as I do for my courses, then this would be a good alternative to the workers.

01:02:03.200 --> 01:02:04.200
Right.

01:02:04.200 --> 01:02:05.200
Yeah.

01:02:05.200 --> 01:02:06.200
That's cool.

01:02:06.200 --> 01:02:07.200
Okay.

01:02:07.200 --> 01:02:08.200
Great recommendation.

01:02:08.200 --> 01:02:09.200
Yeah.

01:02:09.200 --> 01:02:10.200
And you know, the story on Windows is getting better, right?

01:02:10.200 --> 01:02:11.200
Like Steve Dower redid the installer.

01:02:11.200 --> 01:02:15.200
So now the installer is not a complete challenge to like get Python on there.

01:02:15.200 --> 01:02:17.200
The Windows 10 shell is a lot nicer.

01:02:17.200 --> 01:02:23.200
They're bringing the Ubuntu binaries to Windows 10 starting in a month or two.

01:02:23.200 --> 01:02:24.200
So it's getting better.

01:02:24.200 --> 01:02:27.200
But yeah, it still is sometimes painful to work on Windows.

01:02:27.200 --> 01:02:28.200
Yeah.

01:02:28.200 --> 01:02:29.200
Yeah.

01:02:29.200 --> 01:02:33.200
But in the compiling C extensions still not the smoothest experience.

01:02:33.200 --> 01:02:34.200
No.

01:02:34.200 --> 01:02:35.200
No.

01:02:35.200 --> 01:02:38.200
Let me just say VCVAR's bat was not found.

01:02:38.200 --> 01:02:39.200
Right.

01:02:39.200 --> 01:02:40.200
Yeah.

01:02:40.200 --> 01:02:41.200
I have this message about a hundred times.

01:02:41.200 --> 01:02:42.200
Gosh.

01:02:42.200 --> 01:02:43.200
Yes.

01:02:43.200 --> 01:02:44.200
All right.

01:02:44.200 --> 01:02:45.200
Well, Mike, it's been really fun to have you on the show.

01:02:45.200 --> 01:02:48.200
Thanks for sharing your optimization experience.

01:02:48.200 --> 01:02:49.200
Thanks for having me.

01:02:49.200 --> 01:02:50.200
Thank you very much.

01:02:50.200 --> 01:02:51.200
Yeah.

01:02:51.200 --> 01:02:52.200
Talk to you later.

01:02:52.200 --> 01:02:55.200
This has been another episode of Talk Python To Me.

01:02:55.200 --> 01:02:56.200
Today's guest was Mike Mueller.

01:02:56.200 --> 01:02:59.200
And this episode has been sponsored by Rollbar and SnapCI.

01:02:59.200 --> 01:03:02.200
Thank you both for supporting the show.

01:03:02.200 --> 01:03:04.200
Rollbar takes the pain out of errors.

01:03:04.200 --> 01:03:09.200
They give you the context and insight you need to quickly locate and fix errors that might

01:03:09.200 --> 01:03:12.200
have gone unnoticed until your users complain, of course.

01:03:12.200 --> 01:03:19.200
As Talk Python To Me listeners track a ridiculous number of errors for free at rollbar.com/talkpythontome.

01:03:19.200 --> 01:03:23.200
SnapCI is modern continuous integration and delivery.

01:03:23.200 --> 01:03:26.200
Build, test, and deploy your code directly from GitHub.

01:03:26.200 --> 01:03:29.200
All in your browser with debugging, Docker, and parallels included.

01:03:29.200 --> 01:03:32.200
Try them for free at snap.ci/talkpython.

01:03:32.200 --> 01:03:35.200
Are you or a colleague trying to learn Python?

01:03:35.200 --> 01:03:39.200
Have you tried books and videos that just left you bored by covering topics point by point?

01:03:39.200 --> 01:03:45.200
Well, check out my online course Python Jumpstart by building 10 apps at talkpython.fm/course

01:03:45.200 --> 01:03:48.200
to experience a more engaging way to learn Python.

01:03:48.200 --> 01:03:56.200
And if you're looking for something a little more advanced, try my WritePythonic code course at talkpython.fm/pythonic.

01:03:56.200 --> 01:04:03.200
You can find the links from this episode at talkpython.fm/episodes/show/66.

01:04:03.200 --> 01:04:04.200
Be sure to subscribe to the show.

01:04:04.200 --> 01:04:06.200
Open your favorite podcatcher and search for Python.

01:04:06.200 --> 01:04:08.200
We should be right at the top.

01:04:08.200 --> 01:04:14.200
You can also find the iTunes feed at /itunes, Google Play feed at /play, and direct

01:04:14.200 --> 01:04:17.200
RSS feed at /rss on talkpython.fm.

01:04:17.200 --> 01:04:22.200
Our theme music is Developers, Developers, Developers by Corey Smith, who goes by Smix.

01:04:22.200 --> 01:04:26.200
You can hear the entire song at talkpython.fm/music.

01:04:26.200 --> 01:04:28.200
This is your host, Michael Kennedy.

01:04:28.200 --> 01:04:29.200
Thanks so much for listening.

01:04:29.200 --> 01:04:30.200
I really appreciate it.

01:04:30.200 --> 01:04:32.200
Smix, let's get out of here.

01:04:32.200 --> 01:04:54.200
*outro music*

