WEBVTT

00:00:00.001 --> 00:00:03.780
Python's place in climate research is an important one. In this episode, you'll meet

00:00:03.780 --> 00:00:09.720
Joe Heyman and Ryan Abernathy, two researchers using powerful cloud computing systems and Python

00:00:09.720 --> 00:00:14.520
to understand how the world around us is changing. They are both involved in the Pangeo project,

00:00:14.520 --> 00:00:19.520
which brings a great set of tools for scaling complex compute in the cloud with Python.

00:00:19.520 --> 00:00:25.680
This is Talk Python to Me, episode 361, recorded April 1st, 2022.

00:00:25.680 --> 00:00:43.820
Welcome to Talk Python to Me, a weekly podcast on Python. This is your host, Michael Kennedy.

00:00:43.820 --> 00:00:48.460
Follow me on Twitter where I'm @mkennedy and keep up with the show and listen to past episodes

00:00:48.460 --> 00:00:54.860
at talkpython.fm and follow the show on Twitter via at talkpython. We've started streaming most

00:00:54.860 --> 00:00:59.760
of our episodes live on YouTube. Subscribe to our YouTube channel over at talkpython.fm

00:00:59.760 --> 00:01:06.020
slash YouTube to get notified about upcoming shows and be part of that episode. This episode is

00:01:06.020 --> 00:01:12.180
sponsored by SignalWire and Sentry. Transcripts for this and all of our episodes are brought to you by

00:01:12.180 --> 00:01:17.820
Assembly AI. Do you need a great automatic speech-to-text API? Get human-level accuracy in just a few lines

00:01:17.820 --> 00:01:24.620
of code. Visit talkpython.fm/assemblyAI. Joe, Ryan, welcome to Talk Python to Me.

00:01:24.620 --> 00:01:28.980
So much for having us. Hey, it's great to be here. It's fantastic to have you here. I'm really excited

00:01:28.980 --> 00:01:36.620
to talk about earth science and all the cool large-scale computing stuff and cloud computing and

00:01:36.620 --> 00:01:41.000
things like that with you. It'll be a lot of fun. So really looking forward to getting into that. Now,

00:01:41.000 --> 00:01:46.340
before we dive into the topics, let's just start with your story. Joe, I guess you go first. How'd you

00:01:46.340 --> 00:01:51.740
get into programming in Python? My path came through grad school. I was studying civil engineering and

00:01:51.740 --> 00:01:56.560
climate modeling as a graduate student at the University of Washington. I was in a computational

00:01:56.560 --> 00:02:01.800
hydrology group. So we were doing lots of computer things. And my PhD advisor at the time was like,

00:02:01.800 --> 00:02:06.340
we want to do Python stuff. I don't know anything about it, or I don't know much about it. You should

00:02:06.340 --> 00:02:12.180
be the kind of the guinea pig student to bring our group into the modern era. So it kind of threw me to

00:02:12.180 --> 00:02:17.920
the wolves. And I ended up really kind of taking on a role that not just learned it, but then started

00:02:17.920 --> 00:02:22.820
teaching other people and I ended up contributing to open source packages and the rest is history.

00:02:22.820 --> 00:02:27.660
Oh, that's fantastic. What were you using before? You said you brought them into the modern era. Where were you

00:02:27.660 --> 00:02:29.280
coming from? What was the dark ages?

00:02:29.280 --> 00:02:37.380
Some terrible mix of like Perl and Seashell and C and Fortran and a bunch of other shell scripting

00:02:37.380 --> 00:02:39.900
languages. So it was a total spaghetti land.

00:02:39.900 --> 00:02:45.280
Wow. That is a spaghetti land. I would say Python and Jupyter and the PyStack probably sounds a little

00:02:45.280 --> 00:02:48.240
simpler. Yeah. The connections are a little more natural for sure.

00:02:48.240 --> 00:02:49.920
Indeed. How did people receive it?

00:02:49.920 --> 00:02:55.120
It's been great. I think for me personally, it was like quite revolution in what was possible,

00:02:55.120 --> 00:03:00.000
but then passing it around the lab and then around kind of the research community since then,

00:03:00.000 --> 00:03:05.160
it's been overwhelmingly positively received and we're doing totally different things than we could

00:03:05.160 --> 00:03:09.920
have done without it. And I think that's the big change. It's not just that it's like a little easier

00:03:09.920 --> 00:03:13.540
to program, but that you can do things that you couldn't have done before.

00:03:13.540 --> 00:03:18.420
Interesting. Yeah. Were people worried that, you know, coming from C and Fortran that Python wasn't

00:03:18.420 --> 00:03:21.620
fast enough? They're like, we can't use this. This is one of these slow scripting languages.

00:03:21.620 --> 00:03:24.160
Or was it, you'd already proven and it was fine.

00:03:24.160 --> 00:03:28.300
No, I think, I mean, you hear that on occasion, but I think the developer velocity

00:03:28.300 --> 00:03:31.180
outweighs that in 99% of the cases.

00:03:31.180 --> 00:03:31.480
Yeah, I agree.

00:03:31.480 --> 00:03:34.420
You can always optimize the 1% case further.

00:03:34.420 --> 00:03:37.100
For sure. Ryan, how about you? How'd you get into programming in Python?

00:03:37.100 --> 00:03:40.600
Kind of been a lifelong programmer. So I was actually just thinking back when you asked that,

00:03:40.600 --> 00:03:45.800
I think I wrote my first basic code at age seven. My dad worked for IBM. And so I'm like,

00:03:45.800 --> 00:03:51.960
really been like a kind of lifelong computer nerd. Drifted away from that a little bit in college,

00:03:51.960 --> 00:03:56.920
where I majored in physics. But then, you know, in graduate school, I did a PhD in climate physics

00:03:56.920 --> 00:04:03.800
and chemistry at MIT. And, you know, I had a huge need for scientific computing in that. So all of a sudden,

00:04:03.800 --> 00:04:09.480
sort of my computer, you know, stuff started really coming back full and center of my world.

00:04:09.480 --> 00:04:15.280
It was a MATLAB shop, you know, MATLAB Fortran was the stack, you know, there.

00:04:15.280 --> 00:04:22.200
After I sort of did my first project around 2006, 2007 in MATLAB, I decided, you know,

00:04:22.200 --> 00:04:27.000
I just, I'd been doing open source hackery for in other languages for many years. And I was like,

00:04:27.000 --> 00:04:33.800
I got it. I need an open source scientific computing solution. So I tried Python, got into Python around

00:04:33.800 --> 00:04:38.200
2008. So that was when like, I've been at it long enough to have like compiled NumPy.

00:04:38.200 --> 00:04:42.600
Yeah, that's like really when NumPy was basically coming out is right around that time.

00:04:42.600 --> 00:04:43.000
Yeah.

00:04:43.000 --> 00:04:48.200
Or Anaconda and the other one, you know, then just rolled with it ever since. And I,

00:04:48.200 --> 00:04:52.760
so I was like the early adopter Python guy around there. I like helped get a lot of other people

00:04:52.760 --> 00:04:58.840
into it, but it was still always just in my own projects, right? Like there was no community.

00:04:58.840 --> 00:05:04.920
I really got into open source community development probably around 2014, 2015,

00:05:04.920 --> 00:05:09.640
when I discovered X-Array. That is like the project that really like turned me from just

00:05:09.640 --> 00:05:11.160
a user to a contributor.

00:05:11.160 --> 00:05:17.880
Fantastic. Yeah. That's like multi-dimensional NumPy goodness, right? Yeah. We'll dig into it.

00:05:17.880 --> 00:05:24.520
Yeah. Was that other distribution to Anaconda? Was that Canopy? I see out in the audience. Thank you,

00:05:24.520 --> 00:05:28.760
Erie. Cool. The other one I was thinking of was ActiveState. So yeah, there's these,

00:05:28.760 --> 00:05:32.760
these different distributions people can get for optimizing for different stuff. It's great.

00:05:32.760 --> 00:05:37.080
Ryan, what are you doing day to day? It sounds like you both are still doing research university-like

00:05:37.080 --> 00:05:41.320
things. Absolutely. So I'm a professor here at Columbia University in Lamont-Darody Earth

00:05:41.320 --> 00:05:49.240
Observatory. And I sort of manage a medium-sized scientific research lab and teach at the university.

00:05:49.240 --> 00:05:53.560
But then I like wear this total other hat as like open source developer and contributor.

00:05:53.560 --> 00:05:58.040
That can be sometimes a little exhausting to try and wear both those hats at the same time.

00:05:58.040 --> 00:06:03.960
But I really enjoy it. Our work in our lab is focused on computational oceanography,

00:06:03.960 --> 00:06:09.720
trying to understand the role the ocean plays in the climate system, particularly the role that small

00:06:09.720 --> 00:06:18.040
scale ocean processes, eddies, fronts, instabilities that are occurring at the say 10 to 100 kilometer

00:06:18.040 --> 00:06:25.000
scale, how that sort of turbulence and variability influences a large scale ocean and the role it plays

00:06:25.000 --> 00:06:26.840
in our changing climate. Right.

00:06:26.840 --> 00:06:32.440
So the way we do that research is by working with large scale satellite data from NASA. So I'm involved

00:06:32.440 --> 00:06:38.040
in the NASA surface water and ocean topography science team, a new satellite mission that's launching

00:06:38.040 --> 00:06:44.360
this year. I do a ton of numerical modeling, simulating the ocean with computers. At the end of the day,

00:06:44.360 --> 00:06:49.480
we're doing data analysis. Some of which was just sort of traditional statistics and visualization.

00:06:49.480 --> 00:06:55.720
increasingly machine learning, AI, ML are part of our toolkit that we use to try and understand the ocean.

00:06:55.720 --> 00:07:00.840
But the bottom line is, yeah, just working with a lot of data every day, diverse projects,

00:07:00.840 --> 00:07:07.800
that has really forced me to center the rule of these tools in our work and recognize them as sort of

00:07:07.800 --> 00:07:12.840
that is our, like a lot of my colleagues, you know, in other buildings at our lab, like they'll have

00:07:12.840 --> 00:07:18.280
like a million dollar mass spectrometer. You know, they have like an instrument they use to do science.

00:07:18.280 --> 00:07:22.760
Some crazy laser or something. Right. And they like turn the crate and like, we just,

00:07:22.760 --> 00:07:28.200
we're like a data driven lab. And so I think a Pangino in a way is our instrument that we all

00:07:28.200 --> 00:07:34.280
contribute to and maintain that then helps us to do all of the research projects that we want to pursue.

00:07:34.280 --> 00:07:40.440
Yeah. How fascinating. You talked about running a research lab and then also doing this open source

00:07:40.440 --> 00:07:45.480
thing, this dual hat thing. The world may has changed, but when I took a couple of computer science

00:07:45.480 --> 00:07:50.200
classes at university, when I was my undergraduate degree, I didn't feel like a lot of the,

00:07:50.200 --> 00:07:57.320
the instructors or professors there really had much real world experience in programming and stuff.

00:07:57.320 --> 00:08:02.360
And I think things like this, like contributing to X-Array, it must give you this really grounded

00:08:02.360 --> 00:08:07.560
sense of that. Not only these are the tools that you can use, but like, here's how it works. These are

00:08:07.560 --> 00:08:10.760
the people doing it. Like you're in the trenches. Like, do you think that makes a big difference?

00:08:10.760 --> 00:08:16.760
I think we have a major education problem around computational science. And I say this as someone,

00:08:16.760 --> 00:08:22.360
like a university professor, like we have no curriculum to teach someone how to be like an

00:08:22.360 --> 00:08:28.360
effective computational science. And particularly in the context of like open source community driven

00:08:28.360 --> 00:08:32.920
software development, we have computer science classes that will teach you a lot of great things

00:08:32.920 --> 00:08:37.560
about algorithms and data structures and even machine learning, but they won't teach you how to like

00:08:37.560 --> 00:08:42.360
write effective software. And as you said, you have currently, we assume you have to learn that in

00:08:42.360 --> 00:08:48.760
the trenches, just getting into a project. Maybe you work at a company for me, definitely. I upped my

00:08:48.760 --> 00:08:53.000
software engineering team so much after I got involved in community open source, because there were people

00:08:53.000 --> 00:08:58.040
like Stefan Hoyer, you know, you know, Google staff engineer who were like reviewing my PR.

00:08:58.040 --> 00:09:03.800
Right. So that was huge. But I wish actually the university could teach this skill set because I

00:09:03.800 --> 00:09:09.240
think it would help the world a lot. Yeah, I think it would as well. I think we can absolutely find space

00:09:09.240 --> 00:09:16.040
in the curriculum for it. I think math might need to give up a little bit to allow for computational

00:09:16.040 --> 00:09:21.400
math rather than symbolic math. If I could pick, I don't want to do realist too much. And I would do

00:09:21.400 --> 00:09:26.920
want to let Joe get a chance to talk as well. But I think geometry and the going through proofs

00:09:26.920 --> 00:09:32.040
and the thinking about how do I take axioms and using proofs to solve problems is exactly the same

00:09:32.040 --> 00:09:37.640
mindset and the way of thinking as solving a computer program or solving a problem with a program.

00:09:37.640 --> 00:09:41.640
And I haven't applied geometry that much, but I've sure applied a lot of computers. So anyway,

00:09:41.640 --> 00:09:44.680
I just put that out there for people. You might be a little biased.

00:09:44.680 --> 00:09:48.280
I may be a little biased, although I do have two math degrees for what it's worth,

00:09:48.280 --> 00:09:53.000
but I'm still willing to put geometry out there. Anyway, Joe, how about you? What are you doing

00:09:53.000 --> 00:09:57.480
these days? Yeah, so I wear a couple of hats as well. My main hat is I'm the technology director

00:09:57.480 --> 00:10:04.280
at CarbonPlan, a nonprofit that's working on the climate problem. And our focus is really on

00:10:04.280 --> 00:10:09.240
improving the transparency and the scientific integrity and quality of climate solutions.

00:10:09.240 --> 00:10:16.360
We do that by building open source tools and open data and doing research into the various climate

00:10:16.360 --> 00:10:20.840
solutions that are out there. So we use a lot of open source software to do that. And we build a lot

00:10:20.840 --> 00:10:26.360
of open source tooling, including software to help tell stories about different climate solutions.

00:10:26.360 --> 00:10:29.960
The other hat I wear, I'm a scientist at the National Center of Bramispheric Research,

00:10:29.960 --> 00:10:35.320
where that part of my role looks a lot like Ryan's day to day. Yeah, we study different things, but

00:10:35.320 --> 00:10:38.840
I work with big climate model data and do data analysis and all of that.

00:10:38.840 --> 00:10:45.320
Yeah, sounds really fascinating. At CarbonPlan, what kind of data do you all use? Do you hook into electric grids?

00:10:45.320 --> 00:10:51.560
Do you analyze the mixture of electric grids or transportation or what kind of problems and things

00:10:51.560 --> 00:10:56.520
are you solving there? It's a bit of everything. We do big data, little data and everything in between.

00:10:56.520 --> 00:11:01.480
But, you know, so like one of the areas we spend a lot of time working is in the area of forest offsets

00:11:01.480 --> 00:11:07.080
and trying to ask questions about the quality and the potential of using forests as a climate solution.

00:11:07.080 --> 00:11:13.400
And so there we're using everything from in situ observations where people go out and measure trees

00:11:13.400 --> 00:11:17.720
with a tape measure. And they do that every five years. And that's like actually a fairly small data

00:11:17.720 --> 00:11:22.120
set, even though there's a lot of trees out there. It's just there's only so many measurements you can make.

00:11:22.120 --> 00:11:28.360
And then we also work with climate model data. We study climate risk to forests. And so there we're

00:11:28.360 --> 00:11:35.640
building models of future forest fire risk and trying to understand how that's going to change in the

00:11:35.640 --> 00:11:41.080
future. So that's like big data stuff. We're doing lots and lots of model building and machine learning

00:11:41.080 --> 00:11:44.600
and that sort of thing on top of climate model data. Sounds fascinating. You and I are both in the

00:11:44.600 --> 00:11:51.880
Pacific Northwest and last year was not too terrible for fire season, but recently we've had some pretty bad

00:11:51.880 --> 00:11:57.800
forest fires up here and it's definitely concerning. So yeah, it definitely feels like I'm studying the

00:11:57.800 --> 00:12:03.240
world around me more than ever when we're working on these fire problems and then also experiencing the

00:12:03.240 --> 00:12:07.000
smoky weather that we've been having in the summers in the West Coast last few years.

00:12:07.000 --> 00:12:12.760
Yeah, it's been pretty crazy. And I mean, I think that's a theme in climate science right now. It's gone

00:12:12.760 --> 00:12:20.520
very quickly from this academic problem to something that so much of society and our economy is engaging with.

00:12:20.520 --> 00:12:27.800
companies, you know, are just getting to work on adapting to climate change because they're feeling

00:12:27.800 --> 00:12:31.480
it in their bottom line. And it's different than things were 10 years ago.

00:12:31.480 --> 00:12:37.000
Well, that's good to hear. I'm both pessimistic and optimistic about how things could go. You know,

00:12:37.000 --> 00:12:43.880
I, there are so many full discoveries. Folks like you are using, you know, Python and computation

00:12:43.880 --> 00:12:48.280
to really understand exactly what's happening and keep your finger on the pulse of where they're going.

00:12:48.440 --> 00:12:56.040
And then also I see, you know, a mom and her one small kid in a Chevy Suburban next to me,

00:12:56.040 --> 00:13:02.680
you know, idling in traffic. And it's like, I don't know. People do have to internalize it,

00:13:02.680 --> 00:13:07.720
I think a little bit more, but it's, I wonder if it is maybe in business, they're starting to see,

00:13:07.720 --> 00:13:14.360
starting to react a little bit sooner. Right. I think companies feel pressure of economics sooner

00:13:14.360 --> 00:13:17.880
than people do a lot of the times. I know. I think that's true. I think there's also

00:13:17.880 --> 00:13:23.800
social pressures that are pushing companies to act soon. And so I, you know, it's not just it,

00:13:23.800 --> 00:13:29.480
sometimes it's altruistic, but also I think there's marketing involved in a range of other.

00:13:29.480 --> 00:13:33.160
Right. Just don't want to look like the bad company. Yeah.

00:13:33.160 --> 00:13:37.000
If they can put on a good image and it's, it's worth it, but whatever gets them to do it,

00:13:37.000 --> 00:13:37.880
I don't care if it's.

00:13:37.880 --> 00:13:41.560
Yeah. That's what I was just going to say. We don't have to shame that. I think that

00:13:41.560 --> 00:13:45.800
there's a lot of action that needs to happen in a lot of sectors right now. And so to the extent

00:13:45.800 --> 00:13:49.560
we can motivate that through one mechanism or another, that sounds like a good idea.

00:13:49.560 --> 00:13:49.560
Yeah.

00:13:49.560 --> 00:13:56.680
This portion of Talk Python to Me is brought to you by SignalWire. Let's kick this off with a question.

00:13:56.680 --> 00:14:01.560
Do you need to add multi-party video calls to your website or app? I'm talking about live video

00:14:01.560 --> 00:14:06.680
conference rooms that host 500 active participants, run in the browser and work within your existing stack,

00:14:06.680 --> 00:14:13.480
and even support 1080p without devouring the bandwidth and CPU on your users devices. SignalWire

00:14:13.480 --> 00:14:18.760
offers the APIs, the SDKs and edge networks around the world for building the realest of real time

00:14:18.760 --> 00:14:24.440
voice and video communication apps with less than 50 milliseconds of latency. Their core products use

00:14:24.440 --> 00:14:31.320
WebSockets to deliver 300% lower latency than APIs built on rest, making them ideal for apps where every

00:14:31.320 --> 00:14:36.520
millisecond of responsiveness makes a difference. Now you may wonder how they get 500 active participants in

00:14:36.520 --> 00:14:42.040
a browser-based app. Most current approaches use a limited but more economical approach called SFU or

00:14:42.040 --> 00:14:46.920
Selective Forwarding Units, which leaves the work of mixing and decoding all those video and audio

00:14:46.920 --> 00:14:53.400
streams of every participant to each user's device. Browser-based apps built on SFU struggle to support

00:14:53.400 --> 00:14:59.320
more than 20 interactive participants. So SignalWire mixes all the video and audio feeds on the server and

00:14:59.320 --> 00:15:04.360
distributes a single unified stream back to every participant. So you can build things like live

00:15:04.360 --> 00:15:08.840
streaming fitness studios where instructors demonstrate every move from multiple angles,

00:15:08.840 --> 00:15:14.040
or even live shopping apps that highlight the charisma of the presenter and the charisma of the products

00:15:14.040 --> 00:15:19.080
they're pitching at the same time. SignalWire comes from the team behind Free Switch, the open source

00:15:19.080 --> 00:15:25.160
telecom infrastructure toolkit used by Amazon, Zoom, and tens of thousands of more to build mass-scale

00:15:25.160 --> 00:15:31.800
telecom products. So sign up for your free account at talkpython.fm/signalwire and be sure to mention

00:15:31.800 --> 00:15:37.880
Talk Python To Me to receive an extra 5,000 video minutes. That's talkpython.fm/signalwire and mention

00:15:37.880 --> 00:15:39.560
Talk Python To Me for all those credits.

00:15:39.560 --> 00:15:45.560
We're getting like maybe down this climate rabbit hole, but this is the probably the most important

00:15:45.560 --> 00:15:50.840
issue of our time. So let's go down it. I mean, you got to distinguish between the terms we use are

00:15:50.840 --> 00:15:57.800
mitigation versus adaptation, right? So mitigating climate is doing things like burning less fossil

00:15:57.800 --> 00:16:04.680
fuels that are going to reduce the potential impacts of climate change. Adaptation is just accepting that

00:16:04.680 --> 00:16:09.960
climate change is happening and it's going to happen and changing our behavior like infrastructure.

00:16:09.960 --> 00:16:14.520
And so when I say to see a lot of companies taking action, I see especially a lot of companies taking

00:16:14.520 --> 00:16:20.040
action from where I sit on adaptation using earth system data, using our projections from our climate

00:16:20.040 --> 00:16:26.040
models to make business decisions under this changing climate. Mitigation is what we've been calling

00:16:26.040 --> 00:16:32.760
for for decades. And that's where like the Chevy Suburban comes in. I guess I really push back against

00:16:32.760 --> 00:16:38.200
the idea that like personal choices are like an important part of mitigation. This is, I think,

00:16:38.200 --> 00:16:42.280
this narrative has actually been counterproductive. Like, don't think we need to rely on personal,

00:16:42.280 --> 00:16:47.160
like ethical choices about like which type of like bags to bring to the grocery store. I mean,

00:16:47.160 --> 00:16:50.440
like it's important, but like this is a very, very large scale.

00:16:50.440 --> 00:16:54.200
It's solving the problem. I think very, very much on the edges when you're just going like,

00:16:54.200 --> 00:16:56.280
but there's this huge middle part. What do we need?

00:16:56.280 --> 00:17:02.440
Global scale regulation around carbon emissions in order to mitigate climate. And that's a political

00:17:02.440 --> 00:17:02.760
problem.

00:17:02.760 --> 00:17:08.120
It is. Well, the renewable energy story seems to be coming on faster than people thought recently. So

00:17:08.120 --> 00:17:13.000
there is a lot of hope in that space. Now let's talk, you know, you both mentioned a little bit of

00:17:13.000 --> 00:17:20.520
this sort of blend of like open source side of things, and then the science side of things. Let's just

00:17:20.520 --> 00:17:27.320
talk for a moment about just some general best practices with open source and science and stuff

00:17:27.320 --> 00:17:34.120
like that. One of the things I guess is Ryan, you talked about having people like these high-end

00:17:34.120 --> 00:17:38.680
software engineers reviewing your code and stuff. And I suspect there's a lot of lessons you've learned

00:17:38.680 --> 00:17:43.000
that you can kind of bring back to the science world from that open source experience.

00:17:43.000 --> 00:17:49.400
I think there's this whole spectrum of open science, right? And open source activities,

00:17:49.400 --> 00:17:54.520
right? So right now it's pretty common in scientific fields to encourage projects,

00:17:54.520 --> 00:18:01.400
research projects to publish their research code under an open license or put it on GitHub or

00:18:01.400 --> 00:18:06.200
something like that. And I see that it's just like a very, very first step towards a much more

00:18:06.200 --> 00:18:13.720
transformative way we do science as a community. As you well know, like just putting a repo up on

00:18:13.720 --> 00:18:23.320
GitHub has essentially no impact, right? Like if no one uses it, like if someone pushes their commit in a

00:18:23.320 --> 00:18:30.200
forest and no one hears it land, like, okay, great. Like you checked a box. The real goal of open science

00:18:30.200 --> 00:18:37.880
is to encourage more reuse, more collaboration, and accelerate the velocity of scientific discovery.

00:18:37.880 --> 00:18:41.960
And that takes more than just putting your code out there. And of course, putting your code out there

00:18:41.960 --> 00:18:46.840
is the first step, but it actually takes making sure people can run it. Like they have the environment for

00:18:46.840 --> 00:18:51.720
it. Like they can access the data that it needs to run, that they understand what it can do, that it's

00:18:51.720 --> 00:18:57.240
coded in a way that is extensible and modular. And all of those things are a lot more than a license.

00:18:57.240 --> 00:19:03.720
They're about actually writing good scientific code. And so I do think that just the process of getting

00:19:03.720 --> 00:19:11.080
involved in open source is a huge form of education for scientists about how collaboration can work,

00:19:11.080 --> 00:19:16.840
not just even in code, but in general, the way the collaboration process works in a

00:19:16.840 --> 00:19:20.840
well-functioning open source project is kind of miraculous.

00:19:20.840 --> 00:19:25.720
Yeah, it absolutely is. And you know, there's a lot of barometers people use when they go and

00:19:25.720 --> 00:19:31.240
look at an open source project to decide, can I trust this thing? The obvious ones are like,

00:19:31.240 --> 00:19:35.320
how many stars and forks? Like, is it a popular thing that people seem to care about? But others are,

00:19:35.320 --> 00:19:41.000
does it have tests or, you know, does it seem to be operated in a way that is going to lead to

00:19:41.000 --> 00:19:46.600
contributors being able to contribute and, you know, the software evolving over time in a way that

00:19:46.600 --> 00:19:48.120
that they could depend upon? Right?

00:19:48.120 --> 00:19:52.760
Yeah. I think another thing here that's really important to think about is what the incentive

00:19:52.760 --> 00:19:58.040
structures are for a researcher working on a scientific programming problem. And for, you know,

00:19:58.040 --> 00:20:02.840
for most graduate students or researchers at institutions that Ryan and I work at, the goal

00:20:02.840 --> 00:20:08.200
is to write a paper or to produce a dataset. And the software has been kind of thought of as like

00:20:08.200 --> 00:20:13.080
a tool you use to get there, but it's not necessarily a tool that you fix up or improve along the way.

00:20:13.080 --> 00:20:17.960
And I think one of the things that we've been trying to do is kind of break that pattern

00:20:17.960 --> 00:20:23.080
a little bit and think of that, like the whole ecosystem of tools that we're working with is

00:20:23.080 --> 00:20:28.280
improvable so that we don't have to reinvent the wheel. And individual researchers have,

00:20:28.280 --> 00:20:33.320
I actually, and I just, maybe just speaking for myself, being able to say, okay, I'm going to take a,

00:20:33.320 --> 00:20:37.560
rather than take the like shortcut path to get to my, the end of the paper, I'm going to like,

00:20:37.560 --> 00:20:43.880
improve the ecosystem so that later on I can reuse this improvement, but also so Ryan and others in

00:20:43.880 --> 00:20:47.560
the community can reuse it. And that's like a fundamentally different way of thinking about

00:20:47.560 --> 00:20:48.440
the tools you're using.

00:20:48.440 --> 00:20:54.600
That's interesting. It sure is. I've only spent a couple of years in that space, but my experience was

00:20:54.600 --> 00:21:01.080
so much of the code, at least traditionally had been written just to solve a very focused problem and

00:21:01.080 --> 00:21:06.600
not in a way that could be adapted to future problems, right? It'd be like, well, we're changing

00:21:06.600 --> 00:21:10.200
the algorithm or we're, it's slightly different data. So we'll make a copy of this script and we'll

00:21:10.200 --> 00:21:13.560
copy it over there. And we'll just like, you know, maybe there's not a single function in the whole

00:21:13.560 --> 00:21:19.560
thing. It's just top to bottom. And I suspect adopting some of these techniques to sort of produce

00:21:19.560 --> 00:21:25.320
more of a library out of it. Even if you put it on GitHub and nobody comes, it still would benefit

00:21:25.320 --> 00:21:27.480
you and your research over time, I would imagine.

00:21:27.480 --> 00:21:34.440
And I would say that that is a big part of Python and why Python is a good tool for science, because

00:21:34.440 --> 00:21:40.920
it is easy to build higher level abstraction. Like coming from the MATLAB world, you basically got

00:21:40.920 --> 00:21:46.120
MATLAB and its toolboxes, and then you got like your scripts. And like, there's like this hard divide

00:21:46.120 --> 00:21:51.400
between like the platform and the tool and like your own work. You're used to thinking, well, these are

00:21:51.400 --> 00:21:57.320
like the primitives provided by like the tool and here's what I have to do. But Python allows you to

00:21:57.320 --> 00:22:02.760
build very flexibly and has this great ecosystem. I mean, I think this segues naturally into X-Ray.

00:22:02.760 --> 00:22:03.080
Yeah.

00:22:03.080 --> 00:22:10.520
Like many of us in say in 2014 had our own sort of private version of a code that did what X-Ray did,

00:22:10.520 --> 00:22:16.600
thinking like that's code that needs to live in users. That's not a package provided by the ecosystem.

00:22:16.600 --> 00:22:21.480
But then once sort of X-Ray started to catch on and we realized how powerful and how cool it was and how

00:22:21.480 --> 00:22:26.520
well it was, what a solid foundation it had, many of us immediately stopped working on our own sort of

00:22:26.520 --> 00:22:33.000
private X-Ray like thing and started contributing to X-Ray. And we've seen over say the past, you know,

00:22:33.000 --> 00:22:38.920
five or six years, this really steady growth in like the capabilities, both in terms of features and

00:22:38.920 --> 00:22:44.840
robustness like of X-Ray that we never would have done if there hadn't been that coalescence around

00:22:44.840 --> 00:22:47.400
like, okay, we're all going to work together on this.

00:22:47.400 --> 00:22:53.800
Yeah. And then you have these knock on effects, right? There's now other libraries and other systems

00:22:53.800 --> 00:22:58.520
that use X-Ray. And so if you're programming against it, it's super easy to plug into it, kind of like

00:22:58.520 --> 00:23:03.880
what Pandas and Dask are doing. Like if you program against Pandas, you kind of automatically get like

00:23:03.880 --> 00:23:07.160
the scale up version because Dask is just Pandas, but more.

00:23:07.160 --> 00:23:10.600
Right. And you don't have to write code to like read CSVs.

00:23:10.600 --> 00:23:17.560
Yes, exactly. So I gave the elevator pitch for X-Ray, but let's go ahead and dive into that. And some of,

00:23:17.560 --> 00:23:23.800
maybe, maybe give us the story for Pangeo where X-Ray is one of the sort of umbrella,

00:23:23.800 --> 00:23:26.280
is covered under that umbrella. Yeah. So whoever wants to take it.

00:23:26.280 --> 00:23:31.160
Yeah. I'll start, but I think, you know, just to take maybe a slight step back and say what X-Ray is

00:23:31.720 --> 00:23:36.440
one more time, I think. So X-Ray is a package, a Python package for working with multidimensional

00:23:36.440 --> 00:23:42.280
labeled arrays and datasets. And it integrates with NumPy and Pandas. And in many ways, you can think

00:23:42.280 --> 00:23:44.520
of it as a multidimensional Pandas. Yeah.

00:23:44.520 --> 00:23:50.520
And it's used really widely in the climate science community and the geosciences, but it's also used

00:23:50.520 --> 00:23:55.480
in fields outside of the geosciences. Right. It could be finance or all sorts of things.

00:23:55.480 --> 00:24:00.360
Yeah. So finance and biomedical and bioimaging, et cetera.

00:24:00.360 --> 00:24:03.880
Give us a sense of like the data that you might load up off of some oceanography.

00:24:03.880 --> 00:24:08.600
Something I think that our go-to data set for oceanography is like ocean sea surface temperature.

00:24:08.600 --> 00:24:15.640
Satellites observe the ocean from space, infrared or microwave observations can tell how warm the water

00:24:15.640 --> 00:24:22.120
is. That gets processed by NASA. They distribute essentially a bunch of net CDF files that are up

00:24:22.120 --> 00:24:30.360
on like basically an HTTP or FTP server, one file per day for the past 30 years, quarter degree resolution.

00:24:30.360 --> 00:24:36.440
Each file is a couple of, you know, 10, 10, 20 megabytes or something like that. We want to do an analysis on that data.

00:24:36.440 --> 00:24:44.680
And so X-Ray can open that individual file, but it can also open that collection of thousands of files as one coherent data set object.

00:24:44.680 --> 00:24:52.920
Interesting. So do you give it something like a directory and a file pattern and it just somehow does a sort and then loads them up?

00:24:52.920 --> 00:24:54.120
Yeah.

00:24:54.120 --> 00:24:59.560
It can do globbing, you know, it can X-Piles pass it a list. That is one of the killer features of X-Ray that I think brought a lot of people into it.

00:24:59.560 --> 00:25:05.000
Because like, you know, we were all kind of used to like writing code around files. Like, okay, I've gone to this analysis.

00:25:05.000 --> 00:25:18.680
Here's like a hundred files for like each file in my list of files, you know, do this. Right. And instead, the workflow changes with X-Ray. It's like, okay, open multi-file data set. Mean. Done. Right. Like, right.

00:25:18.680 --> 00:25:22.040
And so it's just like this cognitive load that's lifted.

00:25:22.040 --> 00:25:31.960
That's cool. Yeah. There's, especially in the data science space, I see a lot of these things and it's almost about learning about the packages and the way that you can use them.

00:25:32.140 --> 00:25:45.480
You know, like an example that really quickly comes to mind for me is like, if I wanted to get a table out of an HTML page, out of a website, and then pull that in and like process it, I could do, go get the page with requests.

00:25:45.480 --> 00:25:51.860
I could do some beautiful soup thing to find the table. And then I could, I don't know, try to parse it or something and then convert the elements.

00:25:51.860 --> 00:25:57.480
If they were really supposed to be numbers, you got to parse them as numbers and then get that into some data structure.

00:25:57.820 --> 00:26:15.200
Or you go to pandas and you say, read HTML tables bracket two or something like that. Like those kinds of things seem to appear so much in these data science libraries. Like, oh, you could do this big, long computer science thing. You could call this function over here and you got the same outcome. And it's really about knowing about that those exist. Right.

00:26:15.200 --> 00:26:45.180
Yeah, totally.

00:26:45.180 --> 00:26:52.240
So, you know, the workshop was Pi AOS atmosphere, ocean sciences or something like that. It was, it was a name that got dropped pretty quickly.

00:26:52.240 --> 00:27:19.240
But the idea was that we were like 20, it was probably 20 of us that worked on X-ray and Dask mostly. And it was kind of a mix of software developers and scientists. We got together and like just kind of shared out the use cases that we were wrestling with and the problems. And out of that grew the Pangea project and a few ideas. So the, like the mission that you read on the website today is like what we wrote that weekend, which is, you know, to try to tackle a few key problems that we're facing our community.

00:27:19.240 --> 00:27:43.620
We're facing our community, mostly big data, reproducibility and really aiming at supporting the software ecosystem that connected all those dots. And since then, the Pangea project is, has grown into a wider community project that has a lot of software packages involved, not just X-ray and Dask. That's the origin story. It really started with X-ray as the, like the beginnings. And then from there.

00:27:43.620 --> 00:27:55.920
Okay. Interesting. Yeah. Yeah. Very cool. Maybe we could talk a little bit about the other packages, but X-ray and then there's a list of packages on the website. Iris, I know y'all don't do too much with Iris, but maybe just tell us really quick with that.

00:27:55.920 --> 00:28:16.320
So it's at the same level of the stack as X-ray. So you're probably using either X-ray or Iris. Iris is, I would say like, it's maybe a little bit more opinionated than X-ray in the sense that it's scoped to like geo data or like data, you know, or that has things built into it that are like more specific to that domain.

00:28:16.320 --> 00:28:22.220
Like some of the specific file formats, which I'm not familiar with, but like GRIB and those kinds of things.

00:28:22.220 --> 00:28:24.680
No, actually X-ray gets all of those file formats.

00:28:24.680 --> 00:28:25.100
Okay.

00:28:25.100 --> 00:28:38.780
I think it's more about the API, like, you know, an understanding that like what latitude and longitude actually mean and like supporting things like re-gridding directly rather than say through third party packages, like we would use with X-ray.

00:28:38.920 --> 00:28:44.740
It's a great project and it is in many ways very complimentary to X-ray and highly interoperable as well.

00:28:44.740 --> 00:28:50.180
You can like X-ray to Iris dataset, you know, Iris dataset to X-ray.

00:28:50.180 --> 00:28:55.960
You can think of them all as wrappers, higher level data structures around arrays, right?

00:28:55.960 --> 00:29:07.020
So many of us have probably coded, written, if you work with NumPy at any level, you probably had a dictionary with NumPy arrays in it, like multiple different arrays you want to keep together.

00:29:07.340 --> 00:29:12.000
And at that point, I would say like, just use X-ray whenever you're starting programming that pattern.

00:29:12.000 --> 00:29:14.700
Yeah, because that's basically what X-ray is, right?

00:29:14.700 --> 00:29:16.540
As how do you label it?

00:29:16.540 --> 00:29:17.620
That's the keys.

00:29:17.620 --> 00:29:21.040
And then multidimensional is multiple arrays, right?

00:29:21.040 --> 00:29:21.260
Yes.

00:29:21.260 --> 00:29:26.160
And then understanding relationships between these and then metadata, another huge part of this, right?

00:29:26.160 --> 00:29:32.460
Like so X-ray, both X-ray and Iris and anything in the space is going to really understand metadata that comes with those things.

00:29:32.460 --> 00:29:38.460
So things like units or, you know, conventions that tell you how the variables are related to each other.

00:29:38.460 --> 00:29:50.840
And then it can do things with that metadata computationally, not just like drag it around, you know, for posterity, but actually leverage it to make certain syntax or certain computation.

00:29:51.080 --> 00:29:54.800
You filter by all the ones that are tagged by state or whatever.

00:29:54.800 --> 00:29:55.280
Interesting.

00:29:55.280 --> 00:29:56.200
Okay.

00:29:56.200 --> 00:30:01.140
Then the next one in the overall banner of Pangeo is Dask.

00:30:01.140 --> 00:30:01.460
Yep.

00:30:01.460 --> 00:30:05.080
I've had Matthew Rocklin on the show before to talk about Dask, but it's been a while.

00:30:05.080 --> 00:30:07.100
So maybe tell folks about Dask.

00:30:07.160 --> 00:30:07.300
Yeah.

00:30:07.300 --> 00:30:14.660
So Dask is a, it's a library for doing parallel computing in Python and it, and it has a bunch of different containers.

00:30:14.660 --> 00:30:24.060
And so there's Dask array, which is what X-ray uses, but there's also a Dask data frame, which does kind of parallel chunked operations on pandas data frames.

00:30:24.260 --> 00:30:30.800
And then there's the like catch all the Dask bag, which does graph style parallel computing.

00:30:30.800 --> 00:30:43.760
So where this comes in for X-ray is that, I mean, actually for Iris as well, since we were just talking about Iris, but the arrays in an X-ray data set can be backed by a Dask array instead of a NumPy array.

00:30:43.980 --> 00:30:48.080
And by just kind of swapping that out, it's almost a behind the scenes swap out.

00:30:48.080 --> 00:30:58.240
You do a dot chunk on your X-ray data set, and then your operations are going to be handled by Dask, which means they're going to be streamed through the Dask scheduler.

00:30:58.240 --> 00:31:10.880
You'll be able to scale out to a cluster of workers and do, instead of say gigabyte style operations, you can do terabyte scale or even petabyte scales at someday data analysis.

00:31:11.380 --> 00:31:15.420
So Dask is the thing that gives X-ray its horizontal scalability.

00:31:15.420 --> 00:31:16.380
Yeah, very cool.

00:31:16.380 --> 00:31:18.440
So scaling across machines.

00:31:18.440 --> 00:31:30.840
Now, when I learned about Dask, I saw it as it's like the local pandas or the other types of things that models, but you set up a cluster and it runs there.

00:31:30.840 --> 00:31:37.660
And then when I spoke to Matthew about it, I realized he pointed out that it's useful even on a single machine.

00:31:37.660 --> 00:31:38.820
Some of the times, right?

00:31:38.820 --> 00:31:46.740
If you've got a ton of data, but not enough RAM to hold it, or you even have like a pretty simple computer here as eight cores.

00:31:46.740 --> 00:31:51.120
If I run something on pandas, I get one core worth of processing power, right?

00:31:51.120 --> 00:31:52.980
So maybe, Joe, you're shaking your head.

00:31:52.980 --> 00:31:55.160
Like, tell people about that use case.

00:31:55.280 --> 00:31:57.160
Yeah, so Dask has a bunch of schedulers.

00:31:57.160 --> 00:32:00.220
And some of those are local schedulers that run on a single machine.

00:32:00.220 --> 00:32:06.060
And they can either use Python's multi-processing module or threading multiple threads to do computation.

00:32:06.060 --> 00:32:11.600
It also has distributed schedulers that might live on Kubernetes or on an HPC machine.

00:32:12.080 --> 00:32:19.840
There's now companies like Matt Rocklin has gone on to start Coiled, which has managed Dask clusters for you.

00:32:19.840 --> 00:32:26.180
But the idea is that like at a small scale, when you're using the threaded scheduler, it's going to stream computation.

00:32:26.380 --> 00:32:36.940
So when you say taking the average of a terabyte size array, it's going to like use its chunks and process those chunks one at a time and then aggregate those process chunks to the final result.

00:32:36.940 --> 00:32:37.220
Yeah.

00:32:37.300 --> 00:32:46.300
A lot of times, if it's the simple path of you go find some tutorial or example code or something on Stack Overflow, it's just like, well, first you just load this up.

00:32:46.300 --> 00:32:50.780
You read the CSV or you load the JSON file and then you go over it like this.

00:32:50.780 --> 00:32:55.300
And you're like, well, I have a terabyte of data and, you know, 16 gigs of RAM.

00:32:55.300 --> 00:32:59.160
And so you need this sort of iterative streaming style to get there.

00:32:59.160 --> 00:33:09.260
The brilliance of Dask, the game changing flavor of Dask is that for many cases, the user doesn't really have to rewrite their code at all to scale out.

00:33:09.260 --> 00:33:21.020
And so typically with X-Ray, like we, when we teach it and we really want to like get people the sense of the power, we like start by downloading like a 10 megabyte file and opening it with X-Ray and doing some analysis.

00:33:21.320 --> 00:33:34.160
And then they like learn the API and, you know, they use it and then, and then, and then we point it, then we sort of point people to like a massive, you know, a hundred gigabyte data set in the cloud and a Dask cluster.

00:33:34.160 --> 00:33:45.000
And we say like, write the same code and it like just works and it's pretty fast and it is able to scale out without much, really any expertise on the user's side about distributed computing.

00:33:45.240 --> 00:33:46.320
I love that feature.

00:33:46.320 --> 00:33:53.980
On the other hand, I've also come around to the feeling that it can sometimes it's a double-edged sword because some things actually just fail.

00:33:53.980 --> 00:33:58.060
If you don't think hard about the parallelization strategy, it's not magic.

00:33:58.060 --> 00:34:00.640
It depends on the operation that you want to do.

00:34:00.640 --> 00:34:14.620
And so the flip side of that ease of parallelization is that sometimes users will think it is Dask is smarter or more capability than it really could ever be and expect it to just automatically parallelize anything.

00:34:14.620 --> 00:34:20.760
Even say just IO patterns that are just not parallelizable, not scalable, right?

00:34:20.760 --> 00:34:23.500
Or other operations that are just can't be accelerated.

00:34:23.980 --> 00:34:27.360
Yeah, it seems like it just automatically fixes the problem.

00:34:27.360 --> 00:34:30.080
It just makes it faster with magic programming dust.

00:34:30.080 --> 00:34:33.360
And then like, obviously some points that comes undone, right?

00:34:35.380 --> 00:34:38.480
This portion of Talk Python To Me is brought to you by Sentry.

00:34:38.480 --> 00:34:41.360
How would you like to remove a little stress from your life?

00:34:41.360 --> 00:34:47.340
Do you worry that users may be encountering errors, slowdowns, or crashes with your app right now?

00:34:47.340 --> 00:34:50.380
Would you even know it until they sent you that support email?

00:34:50.380 --> 00:34:55.160
How much better would it be to have the error or performance details immediately sent to you,

00:34:55.160 --> 00:35:00.800
including the call stack and values of local variables and the active user recorded in the report?

00:35:00.800 --> 00:35:04.220
With Sentry, this is not only possible, it's simple.

00:35:04.220 --> 00:35:07.780
In fact, we use Sentry on all the Talk Python web properties.

00:35:07.780 --> 00:35:14.320
We've actually fixed a bug triggered by a user and had the upgrade ready to roll out as we got the support email.

00:35:14.320 --> 00:35:16.320
That was a great email to write back.

00:35:16.320 --> 00:35:19.700
Hey, we already saw your error and have already rolled out the fix.

00:35:19.700 --> 00:35:21.120
Imagine their surprise.

00:35:21.120 --> 00:35:23.340
Surprise and delight your users.

00:35:23.340 --> 00:35:27.380
Create your Sentry account at talkpython.fm/sentry.

00:35:27.600 --> 00:35:34.420
And if you sign up with the code talkpython, all one word, it's good for two free months of Sentry's business plan,

00:35:34.420 --> 00:35:39.080
which will give you up to 20 times as many monthly events as well as other features.

00:35:39.080 --> 00:35:43.480
Create better software, delight your users, and support the podcast.

00:35:43.960 --> 00:35:48.460
Visit talkpython.fm/sentry and use the coupon code talkpython.

00:35:50.940 --> 00:35:55.500
Hoiled, Matthew Rocklin's company that he started with some other folks, I believe.

00:35:55.500 --> 00:35:58.980
And this is a really interesting story.

00:35:58.980 --> 00:36:07.220
Like we talked about how you've got your X-array like code or you got your pandas like code that you just wrote for yourself.

00:36:07.220 --> 00:36:14.560
And then by sort of adopting one of these libraries and maybe even contributing it and like building it up, you get these knock-on effects, right?

00:36:14.560 --> 00:36:17.780
So I gave the pandas to Dask example.

00:36:17.780 --> 00:36:19.980
And like, here's the next step in that chain, right?

00:36:19.980 --> 00:36:30.900
Like now you have, oh, I can just spin up a cluster on the cloud automatically through coiled with like one or two lines of code because I built on Dask because I built on pandas.

00:36:30.900 --> 00:36:35.980
You know, like that chain just keeps going of like how it all sort of this synergy between all of them.

00:36:35.980 --> 00:36:36.340
Absolutely.

00:36:36.600 --> 00:36:40.280
And I mean, I would say like it's not just like coincidental.

00:36:40.280 --> 00:36:46.180
Like Matt Rocklin was at that Pangeo meeting at Columbia like in 2016 or whatever.

00:36:46.180 --> 00:36:48.980
And actually we've watched this evolution.

00:36:48.980 --> 00:36:53.980
So a big part of what we have been doing in Pangeo is experimenting with cloud computing.

00:36:53.980 --> 00:37:00.980
I think a little bit earlier and more sort of in a different way than a lot of the other scientific community was.

00:37:00.980 --> 00:37:05.980
I think we can thank Matt for that to some degree because the story with Pangeo and cloud

00:37:05.980 --> 00:37:17.400
is that we after that workshop, we brought our first proposal to the NSF and we got a grant to like develop some of these stuff and develop Pangeo and support scientific use cases.

00:37:17.660 --> 00:37:23.500
And what we had put into that original grant were like we had a bunch of servers that we wanted to buy to like host data and run.

00:37:23.500 --> 00:37:24.240
That's what you do.

00:37:24.240 --> 00:37:30.340
Like when you write a scientific grant and NSF asked us to trim our budget and we decided, OK, we will cut out these servers.

00:37:30.340 --> 00:37:32.760
But why don't you give us some credits on the cloud?

00:37:32.760 --> 00:37:39.720
Because at the time NSF was running this pilot program called Big Data where they were granting those like a partnership with Google and Amazon stuff.

00:37:39.720 --> 00:37:42.760
And so we got like $100,000 worth of Google cloud credits.

00:37:43.480 --> 00:37:48.920
And we just started playing around to see how well we could make this stack work in the cloud.

00:37:48.920 --> 00:37:51.880
And Matt was instrumental actually at that time.

00:37:51.880 --> 00:37:56.640
He was really involved and was helping us figure out like how to deploy stuff on the cloud.

00:37:56.640 --> 00:38:00.540
And we learned all about Kubernetes and object storage and like all of this stuff.

00:38:00.540 --> 00:38:11.620
And it was incredibly fortunate for us to have that because I think we really figured out a lot of stuff early on about how science, scientific research can interact with cloud computing.

00:38:11.620 --> 00:38:15.020
And that's where a lot of our focus and energy is today.

00:38:15.020 --> 00:38:15.480
That's cool.

00:38:15.480 --> 00:38:18.860
Yeah, there's a lot of interesting things about large data sets, right?

00:38:18.860 --> 00:38:27.360
You can put them, as you said, in object storage and then people can come into that cloud and like use the data without trying to download it or move it around.

00:38:27.360 --> 00:38:29.580
Or like some of these data sets are terabytes, right?

00:38:30.200 --> 00:38:31.860
What are you going to do to get those shared, right?

00:38:31.860 --> 00:38:34.080
Yeah, I know some of them are petabytes at this point.

00:38:34.080 --> 00:38:35.820
Which that's terabytes you can do.

00:38:35.820 --> 00:38:40.400
Petabytes, that might be like a little bit beyond what your ISP is going to let you do to download.

00:38:40.400 --> 00:38:41.120
That's right.

00:38:41.120 --> 00:38:41.420
Yeah.

00:38:41.420 --> 00:38:42.340
Where are you going to put it?

00:38:42.340 --> 00:38:43.560
Or you can get a hard drive for it, right?

00:38:43.560 --> 00:38:44.160
Where are you going to put it?

00:38:44.160 --> 00:38:44.760
Yeah, exactly.

00:38:44.760 --> 00:39:00.020
The thing that our work on cloud computing really unlocked for us was this idea that we could federate access, not only to compute, like everyone kind of knew, yeah, you can spin up a VM, but federating access to the data in a way that was infinitely.

00:39:00.020 --> 00:39:05.800
That's a totally scalable, both in terms of access, but also in storage is a totally, like is a total game changer.

00:39:05.800 --> 00:39:22.260
And so, you know, as we've gone down this rabbit hole and we've gone fairly deep at this point, the idea of putting data in object store and letting anyone in the research community access that has kind of revolutionized the way we think about what scientific computing platforms should look like going forward.

00:39:22.260 --> 00:39:22.580
Yeah.

00:39:22.920 --> 00:39:23.420
Yeah, absolutely.

00:39:23.420 --> 00:39:25.360
It solves like half of the problem.

00:39:25.360 --> 00:39:30.660
One is the computational time and power, but the other is just the storage and the data and the memory and all those kinds of things, right?

00:39:30.760 --> 00:39:31.120
Absolutely.

00:39:31.120 --> 00:39:37.700
And, you know, I think data providers are really reckoning now with what the cloud means, right?

00:39:37.740 --> 00:39:44.740
Because what we had seen in geosciences and climate sciences is there were a lot of data portals out there.

00:39:44.740 --> 00:39:48.520
An agency or a group would decide, we have a data set we want to share.

00:39:48.520 --> 00:39:49.600
We have this data we want to share.

00:39:49.600 --> 00:39:54.360
Let's make a portal, which was almost always sort of a highly customized website with a browser.

00:39:54.620 --> 00:40:00.520
And maybe you had to click through to do some JavaScript and like do it like you had to interact with the browser to get data files.

00:40:00.520 --> 00:40:02.660
And then you would like get some data files.

00:40:02.660 --> 00:40:04.740
And maybe it seemed like a good idea.

00:40:04.740 --> 00:40:06.640
There was reasons why they wanted to have a portal.

00:40:06.640 --> 00:40:13.420
But like from a user point of view, especially like an expert user point of view, they're just incredibly frustrating to interact with data that way.

00:40:13.420 --> 00:40:24.000
With cloud object storage, I linked in the chat this blog post I wrote about this, my fantasy about how if this facetious post about how to create a big data portal, there's like one step.

00:40:24.000 --> 00:40:26.640
It's like upload your data to S3.

00:40:26.640 --> 00:40:27.780
Right.

00:40:27.780 --> 00:40:29.200
Exactly.

00:40:29.200 --> 00:40:30.400
I think it's provocative.

00:40:30.400 --> 00:40:40.780
But like I think the fact is there's a lot of vested energy expertise within the scientific community about how to build and maintain these really bespoke data access solutions.

00:40:40.780 --> 00:40:51.220
When I think really we should be moving to a very, we should just really be using object storage and the scalability of cloud style computing to distribute scientific data.

00:40:51.440 --> 00:40:54.540
It doesn't mean we need to just like go all in on AWS.

00:40:54.540 --> 00:40:57.340
Although actually that's exactly what NASA has done.

00:40:57.340 --> 00:41:05.560
You know, there's a lot of cloud storage like things that provide a really scalable base layer of storage for internet enabled computing.

00:41:05.560 --> 00:41:08.960
There's like Wasabi alternative to Amazon.

00:41:08.960 --> 00:41:14.860
There's like Cloudflare now is launching a data storage service, which I'm super excited about.

00:41:15.100 --> 00:41:15.400
Interesting.

00:41:15.400 --> 00:41:16.000
Yeah.

00:41:16.000 --> 00:41:18.360
And you might end up with a copy of that data.

00:41:18.360 --> 00:41:24.560
Maybe you have a copy in AWS, a copy in Azure, maybe even like Linode, DigitalOcean, places like that.

00:41:24.560 --> 00:41:25.420
Right.

00:41:25.740 --> 00:41:27.880
But that's only four or five copies.

00:41:27.880 --> 00:41:31.560
Not every researcher trying to figure out how they're going to deal with it.

00:41:31.560 --> 00:41:31.740
Right.

00:41:31.740 --> 00:41:32.180
Exactly.

00:41:32.180 --> 00:41:38.500
Or you outsource that sort of mirroring to, you know, a service that knows how to scale that sort of thing.

00:41:38.500 --> 00:41:38.780
Yeah.

00:41:38.920 --> 00:41:44.980
Another really interesting player in this space is IPFS and the, you know, distributed web.

00:41:44.980 --> 00:41:51.620
And, you know, where we're at now is like people are very excited about in science or like about S3 and, you know, cloud computing.

00:41:51.740 --> 00:41:56.440
But on the other hand, the scientific community is wary of being too dependent on the big tech giants.

00:41:56.440 --> 00:42:00.540
And it always has sort of a do-it-yourself sort of distributed approach to infrastructure.

00:42:00.540 --> 00:42:12.320
So I'm really excited about IPFS Interplanetary File System, which is a distributed yet highly performant way of sharing petabyte scale data on the internet.

00:42:12.320 --> 00:42:14.300
Oh, that sounds really interesting.

00:42:14.300 --> 00:42:20.820
Really quick, James out in the audience is asking, are there any Pangeo specific resources to help with that transition,

00:42:20.920 --> 00:42:22.740
that jump from the workstation to cloud computing?

00:42:22.740 --> 00:42:23.040
Yeah.

00:42:23.040 --> 00:42:29.740
I think this is a good opportunity to show off and talk about the list of tutorials and examples that we have.

00:42:29.740 --> 00:42:36.560
So I think, you know, Pangeo has a collection of Jupyter notebooks that show how to use.

00:42:36.560 --> 00:42:36.780
Yeah.

00:42:36.780 --> 00:42:39.200
So this is gallery.pangeo.io.

00:42:39.200 --> 00:42:49.160
There's a list of Jupyter notebooks here that walk through kind of real world examples of working with large geoscientific data in the cloud.

00:42:49.420 --> 00:42:53.000
And so I'd encourage people that are listening to pull this site up and take a look.

00:42:53.000 --> 00:42:55.680
And I'll put a link in the show notes if you can find it as well.

00:42:55.680 --> 00:42:55.960
Yeah.

00:42:55.960 --> 00:42:56.260
Yeah.

00:42:56.260 --> 00:43:01.080
So I think this is a good example of like one of the best resources we can point folks to.

00:43:01.080 --> 00:43:01.320
Okay.

00:43:01.320 --> 00:43:07.780
But I mean, to get a little bit more specific, the idea is actually you don't have to change very much at all about your workflow when moving to the cloud.

00:43:07.780 --> 00:43:08.040
Right.

00:43:08.080 --> 00:43:10.100
That's what we're aiming for with Pangeo.

00:43:10.100 --> 00:43:14.500
And a key part of this and sort of that another pillar of Pangeo we haven't discussed yet is Jupyter.

00:43:14.500 --> 00:43:16.600
So Jupyter is a key part of our effort.

00:43:16.600 --> 00:43:22.520
We work very closely with the Jupyter developers and also the team at 2i2c, who I know you've talked to recently.

00:43:22.740 --> 00:43:33.900
And Jupyter has been, I think, an amazing sort of Trojan horse for like cloud computing because the way when you use Jupyter for the first time, it launches up a notebook in your browser.

00:43:34.640 --> 00:43:40.060
And honestly, the hardest way to use Jupyter is to try to use it locally because you've got to install it and configure it and run it and get.

00:43:40.060 --> 00:43:40.540
Exactly.

00:43:40.540 --> 00:43:42.100
The cloud story is just even easier.

00:43:42.100 --> 00:43:42.540
Exactly.

00:43:42.540 --> 00:43:53.840
So we now have the scientific community who like scientists, you know, huge number of scientists in many different languages are all just used to the idea that we're going to do our science in our browser through this type of, you know, IDE, essentially.

00:43:53.840 --> 00:44:02.820
And JupyterLab especially has so many more features than classic notebook that make for a really rich, interactive scientific computing environment.

00:44:03.180 --> 00:44:05.980
So then moving the work to the cloud is trivial.

00:44:05.980 --> 00:44:11.060
I mean, for the user, you know, and so in Pangeo, we operate some cloud-based Jupyter hubs.

00:44:11.060 --> 00:44:13.520
And also we have an operating binder.

00:44:13.520 --> 00:44:22.580
And those Jupyter hubs, basically, you just log on like and anyone out there can actually sign up for the Pangeo hubs or apply to be to get access to these hubs.

00:44:22.580 --> 00:44:25.500
And then you just have a notebook environment in the cloud.

00:44:25.500 --> 00:44:29.180
Of course, just having your notebook in the cloud is not that cool.

00:44:29.180 --> 00:44:33.260
But what we can augment it with are some capabilities to run DAS.

00:44:33.260 --> 00:44:37.300
So we use DAS Gateway in those hubs as a DAS deployment solution.

00:44:37.300 --> 00:44:40.180
But, you know, Coiled is another example of a DAS deployment solution.

00:44:40.180 --> 00:44:46.640
And then the key, though, what is going to bring scientists to the cloud is that data, right?

00:44:46.740 --> 00:44:51.020
So that is what makes this appealing and game-changing.

00:44:51.020 --> 00:44:52.920
The fact that now you log into this hub.

00:44:52.920 --> 00:44:55.760
Okay, you're in Google Cloud US Central 1.

00:44:55.760 --> 00:45:03.560
We got a petabyte of data from the World Climate Research Program coupled model enter comparison program project sitting there.

00:45:03.560 --> 00:45:06.920
Organized, analysis ready that you can start doing science with.

00:45:06.920 --> 00:45:20.840
So when before you know that, if a grad student, say, would decide, okay, I'm going to work with these cloud models and do this research project, they would literally spend months downloading, organizing, and sorting that data on a computer before they can get started.

00:45:20.840 --> 00:45:24.220
Now you can start it in five minutes and be processing data.

00:45:24.220 --> 00:45:25.700
And that's amazing.

00:45:25.700 --> 00:45:31.520
That's what gets me excited and motivates me to devote energy to this project.

00:45:31.520 --> 00:45:37.780
Maybe that also involved writing a grant so that you could get enough compute to locally hold and work on that.

00:45:37.780 --> 00:45:38.440
And you got to wait.

00:45:38.440 --> 00:45:45.300
Okay, I'm going to order our Silicon Graphics or a Cray or whatever it is you're getting, you know, whatever the supercomputers are these days.

00:45:45.300 --> 00:45:47.180
And they'll wait for some big thing to show up.

00:45:47.180 --> 00:45:50.280
Or you fire up a notebook in AWS.

00:45:50.280 --> 00:45:51.740
Take your pick, right?

00:45:52.340 --> 00:45:57.900
I suspect this is also democratized computing somewhat as well, right?

00:45:57.900 --> 00:46:04.000
You don't need as much compute to set up something like this and access that existing cloud database.

00:46:04.000 --> 00:46:04.760
That's true.

00:46:04.760 --> 00:46:10.140
And I think part of that is we've separated in the cloud model of separation of concerns.

00:46:10.140 --> 00:46:13.300
The storage is not necessarily directly attached to compute.

00:46:13.300 --> 00:46:17.140
Whereas like in a supercomputing center, it's all one warehouse, right?

00:46:17.140 --> 00:46:17.680
Yeah.

00:46:17.680 --> 00:46:17.860
Yeah.

00:46:17.860 --> 00:46:22.320
And so, yeah, like if you're just, if you're doing a small problem, you get a small VM or you get a small cluster.

00:46:22.320 --> 00:46:23.840
A small DAS cluster.

00:46:23.840 --> 00:46:25.860
If you're doing a big problem, you scale out.

00:46:25.860 --> 00:46:26.360
Right.

00:46:26.360 --> 00:46:33.200
And so you can sort your, or you can arrange your compute infrastructure appropriate to the problem you're working on.

00:46:33.200 --> 00:46:40.220
And the other thing is the way we're storing the data allows for partial queries of these large array.

00:46:40.420 --> 00:46:49.180
You know, so the data that Ryan was talking about, the CMIP 6 data is a petabyte in scale, but you don't have to open the whole thing and you don't have to load the whole thing.

00:46:49.180 --> 00:46:54.940
We're able to slice into that and grab out just the parts that are interesting for the research project that's being done.

00:46:54.940 --> 00:46:55.160
Right.

00:46:55.160 --> 00:46:55.600
Exactly.

00:46:55.600 --> 00:46:56.480
That's, that's awesome.

00:46:56.480 --> 00:47:12.380
One thing I did want to ask you to about real quick, or we're getting short on time here, but do projects like Jupyter Lite, which is Jupyter, but running on WebAssembly in the browser or the Python stack a little bit running on the browser or PyIoDied.

00:47:12.520 --> 00:47:16.360
Do these projects offer any benefits to you all or are you tracking them?

00:47:16.360 --> 00:47:16.860
Are you interested?

00:47:16.860 --> 00:47:17.880
It's super cool.

00:47:17.880 --> 00:47:24.920
Um, Jupyter Lite and PyIoDied, it really lowers the bar to providing just like getting something up and running.

00:47:24.920 --> 00:47:30.880
The motivations for getting into Pangeo, for starting Pangeo, a big part of it was big data.

00:47:30.880 --> 00:47:32.660
We have large data sets, right?

00:47:32.660 --> 00:47:34.600
And so we want to do data approximate computing.

00:47:34.600 --> 00:47:38.560
So by putting a hub in the cloud, we're putting our compute next to the data.

00:47:38.560 --> 00:47:40.780
PyIoDied actually takes it back to the laptop.

00:47:40.780 --> 00:47:41.280
Yeah, yeah.

00:47:41.480 --> 00:47:42.980
This doesn't solve the data problem.

00:47:42.980 --> 00:47:48.980
This kind of wrecks that, but it does give computational capabilities without.

00:47:48.980 --> 00:47:57.120
And if you couple that with something like Coiled or any Dask solution where you can actually then call out to a data processing layer that isn't.

00:47:57.120 --> 00:47:57.640
Interesting.

00:47:57.640 --> 00:48:03.440
It's, it will, it would make the need for us to operate those Jupyter hubs potentially go away or reduce or go away.

00:48:03.440 --> 00:48:04.340
Oh, that's interesting.

00:48:04.340 --> 00:48:08.760
So you've got the, the Dask cluster or whatever next to the data in the cloud.

00:48:08.760 --> 00:48:11.740
And this is just handling the results of all that.

00:48:11.740 --> 00:48:12.100
Okay.

00:48:12.100 --> 00:48:14.000
I hadn't thought about combining those in that way.

00:48:14.000 --> 00:48:14.760
Very cool.

00:48:14.760 --> 00:48:18.720
If you want to process like many terabytes of data, you're not going to do it in your browser.

00:48:18.720 --> 00:48:20.060
You're not going to do it in your laptop at all.

00:48:20.060 --> 00:48:22.000
You do need a big computer, right?

00:48:22.000 --> 00:48:22.280
Yeah.

00:48:22.460 --> 00:48:27.760
But still, like it, it certainly expands what's possible larger than like maybe the first impressions.

00:48:27.760 --> 00:48:29.240
Oh, I was going to say a quick shout out.

00:48:29.240 --> 00:48:36.240
Recently added a Jupyter Lite example tutorial to the X-Ray homepage at xray.dev.

00:48:36.340 --> 00:48:44.360
So if folks want to go try out xray really quick without having to start up a Jupyter lab server or something, it's there and you can run through.

00:48:44.360 --> 00:48:45.780
Oh, that's fantastic.

00:48:45.780 --> 00:48:46.480
Yeah.

00:48:46.480 --> 00:48:54.020
We've got binder, which sort of creates a cloud instance to run all these examples, but a lot of them could actually just run like this, which is great.

00:48:54.020 --> 00:48:54.220
Yeah.

00:48:54.220 --> 00:48:54.920
It makes it a lot simpler.

00:48:54.920 --> 00:48:55.240
Yeah.

00:48:55.240 --> 00:48:55.800
All right.

00:48:55.920 --> 00:49:01.020
We could talk for much longer, but we also know that we don't have too much time left.

00:49:01.020 --> 00:49:04.340
So let me ask you both real quickly the final two questions.

00:49:04.340 --> 00:49:06.060
Joe, you can go first.

00:49:06.060 --> 00:49:06.840
You're going to write some code.

00:49:06.840 --> 00:49:08.560
What Python code, what editor do you use?

00:49:08.560 --> 00:49:09.220
VS Code.

00:49:09.220 --> 00:49:09.760
VS Code.

00:49:09.760 --> 00:49:10.060
All right.

00:49:10.060 --> 00:49:11.220
Or Vim if I can't.

00:49:11.220 --> 00:49:12.400
Okay.

00:49:12.400 --> 00:49:16.300
I use Atom and I feel like I'm behind the times, but like that's what I use.

00:49:16.300 --> 00:49:18.500
That's like OG VS Code.

00:49:18.500 --> 00:49:21.740
It gets what I need to do done, but I feel like I'm missing out on things.

00:49:21.740 --> 00:49:23.080
So I probably need to upgrade.

00:49:23.480 --> 00:49:27.420
I do feel like my development environment is increasingly owned by Microsoft.

00:49:27.420 --> 00:49:32.500
And so I have like some mild reluctance to switch to VS Code.

00:49:32.500 --> 00:49:32.920
Sure.

00:49:32.920 --> 00:49:33.620
I hear you.

00:49:33.620 --> 00:49:33.980
Awesome.

00:49:33.980 --> 00:49:38.720
And then notable PyPI package, some library you've come across lately that you thought was awesome.

00:49:38.720 --> 00:49:41.240
I mean, obviously, shout out to Pangio and all of it.

00:49:41.240 --> 00:49:42.720
But it's not a package.

00:49:42.720 --> 00:49:43.660
You can't pick.

00:49:43.660 --> 00:49:44.240
No, I know.

00:49:44.240 --> 00:49:44.940
Yeah, I know.

00:49:44.940 --> 00:49:50.260
I'm going to go with one of my favorites, which is not something I came across recently, but I think everybody should know about it.

00:49:50.260 --> 00:49:51.100
Yeah, that works as well.

00:49:51.100 --> 00:49:52.400
It's FSSpec.

00:49:52.760 --> 00:49:57.280
And it's a library for accessing data across a bunch of different file storage systems.

00:49:57.280 --> 00:50:00.380
And it is a game changer for working with remote data.

00:50:00.380 --> 00:50:01.760
Everyone should know about it.

00:50:01.760 --> 00:50:02.600
Yeah, absolutely.

00:50:02.600 --> 00:50:04.100
That's a super interesting one.

00:50:04.100 --> 00:50:08.600
Basically, you can connect it to all these different backends and stuff, right?

00:50:08.600 --> 00:50:09.860
I don't remember where to find them.

00:50:09.860 --> 00:50:13.780
But yeah, like you could connect S3 like it was a file and stuff like that, right?

00:50:13.780 --> 00:50:14.560
Yep, exactly.

00:50:14.560 --> 00:50:15.480
Yeah, fantastic.

00:50:15.740 --> 00:50:16.360
All right, Ryan.

00:50:16.360 --> 00:50:27.600
Well, mine is a shameless self-promotion, but it's for a new project we have called Pangio Forge, which is basically an ETL tool for this X-ray scientific data space.

00:50:27.740 --> 00:50:28.160
Right.

00:50:28.160 --> 00:50:41.600
So what we found is that a lot of the ETL tools that exist for business style data analytics don't necessarily play well with our multidimensional data model that we use in geosciences and sciences more broadly.

00:50:41.860 --> 00:50:57.080
And so we're building this open source Python package called Pangio Forge recipes that is designed to help us with all this data movement, data transformation that needs to happen as we're migrating so many sort of legacy data sets into this cloud native format.

00:50:57.080 --> 00:50:58.320
That looks fantastic.

00:50:58.320 --> 00:50:58.760
All right.

00:50:58.760 --> 00:50:58.920
Yeah.

00:50:58.920 --> 00:51:00.280
Good shout out.

00:51:00.280 --> 00:51:02.480
And we'll put a link to that so people can check it out.

00:51:02.480 --> 00:51:03.060
All right.

00:51:03.060 --> 00:51:04.500
Joe, Ryan, it's been a lot of fun.

00:51:04.500 --> 00:51:06.260
A quick final call to action.

00:51:06.260 --> 00:51:08.280
People want to get started with Pangio.

00:51:08.280 --> 00:51:09.360
What do you say?

00:51:09.360 --> 00:51:10.500
Come to our forum.

00:51:10.500 --> 00:51:15.780
Yeah, you mentioned before we hit record, you mentioned that the discuss page is like really where the action is at right now.

00:51:15.780 --> 00:51:16.700
The discuss forum, right?

00:51:16.700 --> 00:51:17.220
Discourse.

00:51:17.220 --> 00:51:18.580
Discourse, sorry, not discuss.

00:51:18.580 --> 00:51:19.440
Yeah, discourse.

00:51:19.440 --> 00:51:22.760
The tools and the stuff is self-documenting on the website.

00:51:22.760 --> 00:51:26.720
But what we really emphasize about Pangio is the community aspect of it.

00:51:26.720 --> 00:51:28.980
We are not just trying to build a tool and like put it out there.

00:51:28.980 --> 00:51:38.040
You know, we're really trying to build a community where scientists are talking to software developers, are talking to infrastructure maintainers, are talking to data providers,

00:51:38.200 --> 00:51:46.340
and are collectively sort of trying to keep this flywheel spinning of innovation and development, ultimately with the goal of empowering more scientific discoveries.

00:51:46.840 --> 00:51:55.560
And so if you have questions, if you have, if you stop breaks, if you don't know if it's the right tool for you, this forum is where we can welcome you.

00:51:55.560 --> 00:51:56.780
We don't have a Slack.

00:51:56.780 --> 00:52:00.580
We try to be more open than as a community than a Slack.

00:52:00.580 --> 00:52:02.220
So this is where I should have.

00:52:03.320 --> 00:52:04.400
I can second that.

00:52:04.400 --> 00:52:05.000
It's a good idea.

00:52:05.000 --> 00:52:05.320
Awesome.

00:52:05.320 --> 00:52:09.080
And then, Joe, you also wanted to throw out xray.dev, right?

00:52:09.080 --> 00:52:09.440
Yeah.

00:52:09.440 --> 00:52:10.700
So there's, yeah, exactly.

00:52:10.700 --> 00:52:14.220
This is a relatively new site that sits on top of our documentation site.

00:52:14.220 --> 00:52:17.760
But there's the Jupyter Lite interface is down below the fold.

00:52:17.760 --> 00:52:19.840
And it's a great starting point.

00:52:19.840 --> 00:52:21.080
Thank you both for being here.

00:52:21.080 --> 00:52:22.100
This has been really interesting.

00:52:22.100 --> 00:52:23.440
And thanks for all the hard work.

00:52:23.440 --> 00:52:24.100
Thanks for having us.

00:52:24.100 --> 00:52:25.140
Thank you for creating this.

00:52:25.140 --> 00:52:25.460
Yep.

00:52:25.460 --> 00:52:25.760
You bet.

00:52:25.840 --> 00:52:25.960
Bye.

00:52:25.960 --> 00:52:29.860
This has been another episode of Talk Python to Me.

00:52:29.860 --> 00:52:31.620
Thank you to our sponsors.

00:52:31.620 --> 00:52:33.280
Be sure to check out what they're offering.

00:52:33.280 --> 00:52:34.700
It really helps support the show.

00:52:34.700 --> 00:52:39.580
Add high-performance, multi-party video calls to any app or website with SignalWire.

00:52:39.580 --> 00:52:46.840
Visit talkpython.fm/SignalWire and mention that you came from Talk Python to Me to get started and grab those free credits.

00:52:46.840 --> 00:52:49.760
Take some stress out of your life.

00:52:49.760 --> 00:52:55.540
Get notified immediately about errors and performance issues in your web or mobile applications with Sentry.

00:52:55.540 --> 00:53:00.540
Just visit talkpython.fm/sentry and get started for free.

00:53:00.540 --> 00:53:04.140
And be sure to use the promo code talkpython, all one word.

00:53:04.140 --> 00:53:06.240
Want to level up your Python?

00:53:06.240 --> 00:53:10.300
We have one of the largest catalogs of Python video courses over at Talk Python.

00:53:10.300 --> 00:53:15.460
Our content ranges from true beginners to deeply advanced topics like memory and async.

00:53:15.460 --> 00:53:18.140
And best of all, there's not a subscription in sight.

00:53:18.140 --> 00:53:21.040
Check it out for yourself at training.talkpython.fm.

00:53:21.040 --> 00:53:22.940
Be sure to subscribe to the show.

00:53:22.940 --> 00:53:25.800
Open your favorite podcast app and search for Python.

00:53:25.800 --> 00:53:27.040
We should be right at the top.

00:53:27.040 --> 00:53:36.380
You can also find the iTunes feed at /itunes, the Google Play feed at /play, and the direct RSS feed at /rss on talkpython.fm.

00:53:36.380 --> 00:53:39.820
We're live streaming most of our recordings these days.

00:53:39.820 --> 00:53:47.600
If you want to be part of the show and have your comments featured on the air, be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

00:53:48.100 --> 00:53:49.520
This is your host, Michael Kennedy.

00:53:49.520 --> 00:53:50.800
Thanks so much for listening.

00:53:50.800 --> 00:53:51.960
I really appreciate it.

00:53:51.960 --> 00:53:53.860
Now get out there and write some Python code.

00:53:53.860 --> 00:54:16.680
We'll talk to you soon.

