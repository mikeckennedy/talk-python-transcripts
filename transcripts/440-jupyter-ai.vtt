WEBVTT

00:00:00.520 --> 00:00:05.960
We all know that LLMs and generative AI have been working their way into many products.


00:00:05.960 --> 00:00:08.760
Well, it's Jupyter's turn to get a really awesome integration.


00:00:08.760 --> 00:00:12.560
We have David Kui here to tell us about Jupyter AI.


00:00:12.560 --> 00:00:18.680
Jupyter AI provides a user-friendly and powerful way to apply generative AI to your notebooks.


00:00:18.680 --> 00:00:22.600
It lets you choose from many different LLM providers and models to get just the help


00:00:22.600 --> 00:00:24.120
that you're looking for.


00:00:24.120 --> 00:00:28.360
And it does way more than just add a chat pane in the UI.


00:00:28.360 --> 00:00:30.000
Listen in to find out.


00:00:30.000 --> 00:00:49.400
This is Talk Python to Me, episode 440, recorded October 30th, 2023.


00:00:49.400 --> 00:00:52.800
Welcome to Talk Python to Me, a weekly podcast on Python.


00:00:52.800 --> 00:00:54.640
This is your host, Michael Kennedy.


00:00:54.640 --> 00:00:59.560
Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython,


00:00:59.560 --> 00:01:02.160
both on fosstodon.org.


00:01:02.160 --> 00:01:07.400
Keep up with the show and listen to over seven years of past episodes at talkpython.fm.


00:01:07.400 --> 00:01:11.160
We've started streaming most of our episodes live on YouTube.


00:01:11.160 --> 00:01:17.000
Subscribe to our YouTube channel over at talkpython.fm/youtube to get notified about upcoming shows and be


00:01:17.000 --> 00:01:19.720
part of that episode.


00:01:19.720 --> 00:01:24.080
This episode is sponsored by Posit Connect from the makers of Shiny.


00:01:24.080 --> 00:01:28.640
Just share and deploy all of your data projects that you're creating using Python.


00:01:28.640 --> 00:01:35.080
Streamlet, Dash, Shiny, Bokeh, FastAPI, Flask, Quattro, Reports, Dashboards, and APIs.


00:01:35.080 --> 00:01:37.000
Posit Connect supports all of them.


00:01:37.000 --> 00:01:43.480
Try Posit Connect for free by going to talkpython.fm/posit, P-O-S-I-T.


00:01:43.480 --> 00:01:47.240
And it's also brought to you by us over at Talk Python Training.


00:01:47.240 --> 00:01:51.480
Did you know that we have over 250 hours of Python courses?


00:01:51.480 --> 00:01:52.760
Yeah, that's right.


00:01:52.760 --> 00:01:56.400
Check them out at talkpython.fm/courses.


00:01:56.400 --> 00:01:59.520
David, welcome to Talk Python to Me.


00:01:59.520 --> 00:02:00.520
Awesome to have you.


00:02:00.520 --> 00:02:01.520
Yeah, thank you, Michael.


00:02:01.520 --> 00:02:04.520
I'm really excited to see what the AIs have to say today.


00:02:04.520 --> 00:02:07.280
The AIs, yeah, language models, sure.


00:02:07.280 --> 00:02:08.280
Yes, exactly.


00:02:08.280 --> 00:02:09.280
Exactly.


00:02:09.280 --> 00:02:14.720
Now, you've built a really cool extension for Jupyter that plugs in large language models


00:02:14.720 --> 00:02:17.400
for people and it's looking super interesting.


00:02:17.400 --> 00:02:19.400
So I'm excited to talk to you about it.


00:02:19.400 --> 00:02:22.120
Yeah, I'm excited to talk about Jupyter AI too.


00:02:22.120 --> 00:02:26.520
I've actually presented this twice at one set, actually three times.


00:02:26.520 --> 00:02:30.720
I did a short demo at like this like tech meetup thing in Seattle.


00:02:30.720 --> 00:02:34.040
That was actually the first time Jupyter AI was shown to the public.


00:02:34.040 --> 00:02:39.120
And then I presented at PyData Seattle at Microsoft's Redmond campus.


00:02:39.120 --> 00:02:43.720
And then I got to present again at JupyterCon in Paris this May.


00:02:43.720 --> 00:02:45.520
It was a really wonderful experience.


00:02:45.520 --> 00:02:46.520
But yeah.


00:02:46.520 --> 00:02:47.520
Wow.


00:02:47.520 --> 00:02:48.520
Yeah, you're making the rounds.


00:02:48.520 --> 00:02:49.520
Yeah.


00:02:49.520 --> 00:02:50.840
I love to talk about Jupyter AI.


00:02:50.840 --> 00:02:53.640
It happens to get me some plane tickets.


00:02:53.640 --> 00:02:56.000
Just joking.


00:02:56.000 --> 00:03:00.160
Honestly that's like half the bonus of conferences is like awesome places you get to go.


00:03:00.160 --> 00:03:02.120
The other half probably is the people you meet.


00:03:02.120 --> 00:03:03.120
You know, it's really cool.


00:03:03.120 --> 00:03:05.400
Oh, for me, it's like almost like all the people.


00:03:05.400 --> 00:03:08.280
Like the people are just so great, especially JupyterCon.


00:03:08.280 --> 00:03:09.280
Yeah.


00:03:09.280 --> 00:03:12.880
And the JupyterCon videos are now out for JupyterCon 2023.


00:03:12.880 --> 00:03:14.920
And there's a ton of good looking talks there.


00:03:14.920 --> 00:03:17.040
So yeah, lots of really smart people.


00:03:17.040 --> 00:03:22.360
I mean, like I was chatting to a few folks there and that's like the only place where


00:03:22.360 --> 00:03:27.360
you're going to find like people who work at these hedge funds and trading firms just


00:03:27.360 --> 00:03:29.800
lounging so idly and casually.


00:03:29.800 --> 00:03:30.800
Right.


00:03:30.800 --> 00:03:31.800
Like, yeah.


00:03:31.800 --> 00:03:33.680
The market's opening and they're chilling.


00:03:33.680 --> 00:03:34.680
It's all fine.


00:03:34.680 --> 00:03:35.680
Yeah.


00:03:36.680 --> 00:03:37.680
There's a lot of smart people there.


00:03:37.680 --> 00:03:38.680
Yeah.


00:03:38.680 --> 00:03:43.520
Jupyter more than a lot of programming technologies bring people from all sorts of different places


00:03:43.520 --> 00:03:45.000
together, different backgrounds.


00:03:45.000 --> 00:03:46.000
There's like a huge.


00:03:46.000 --> 00:03:49.040
So yeah, there's like a lot of reasons behind that.


00:03:49.040 --> 00:03:51.120
But long story short, Jupyter is pretty awesome.


00:03:51.120 --> 00:03:53.600
And that's kind of why I work to contribute to it.


00:03:53.600 --> 00:03:54.600
Awesome.


00:03:54.600 --> 00:03:57.880
Well, let's start this whole conversation with a bit of background about yourself.


00:03:57.880 --> 00:04:02.120
And for people who didn't see your talk and don't know you yet, tell them a bit about


00:04:02.120 --> 00:04:03.120
you.


00:04:03.120 --> 00:04:05.880
I didn't really give much of an intro there either, but sure.


00:04:05.880 --> 00:04:06.880
Yeah.


00:04:06.880 --> 00:04:13.000
So I've worked for AWS as a software engineer and I've been with AWS, specifically the AI


00:04:13.000 --> 00:04:15.560
ML organization at AWS.


00:04:15.560 --> 00:04:18.320
I've been with them for almost two years now.


00:04:18.320 --> 00:04:23.920
Right now, my manager is actually Brian Granger, who's the co-founder of Project Jupyter.


00:04:23.920 --> 00:04:25.360
He also works for AWS.


00:04:25.360 --> 00:04:26.360
Yeah.


00:04:26.360 --> 00:04:32.480
So he's been offering some technical and product guidance for the things that we're building.


00:04:32.480 --> 00:04:35.240
And he's a fantastic gentleman to work with.


00:04:35.240 --> 00:04:40.720
Oh, that's a really neat, need to have him available as a resource, you know, as a colleague.


00:04:40.720 --> 00:04:41.720
Yeah.


00:04:41.720 --> 00:04:42.720
You know, it's funny.


00:04:42.720 --> 00:04:45.000
Like I actually, yeah, I met him internally.


00:04:45.000 --> 00:04:49.640
So when I first joined, I wasn't working for him, but at tech companies, you can do this


00:04:49.640 --> 00:04:51.480
internal transfer thing.


00:04:51.480 --> 00:04:55.960
And basically my old team was kind of, the team I just joined right after I joined, it


00:04:55.960 --> 00:05:01.160
sort of started to dissolve a little because they just launched a product at reInvent,


00:05:01.160 --> 00:05:03.720
which happens in like December.


00:05:03.720 --> 00:05:08.680
And then, so I joined in December and it's like, oh, hi.


00:05:08.680 --> 00:05:09.680
So then I, yeah.


00:05:09.680 --> 00:05:14.400
And then I joined, I messaged, I saw just, saw Brian Granger's name somehow.


00:05:14.400 --> 00:05:18.920
I messaged him and I didn't even know that he was the co-founder of Project Jupiter.


00:05:18.920 --> 00:05:21.760
I just wanted to work for him because I used it before.


00:05:21.760 --> 00:05:24.040
And yeah, it's a pretty funny story.


00:05:24.040 --> 00:05:25.040
Indeed.


00:05:25.040 --> 00:05:29.160
I imagine that project has, has really, you know, this Jupiter AI is a great example,


00:05:29.160 --> 00:05:34.840
but I just, thinking of being say a founder of Jupiter or, or something like that, these


00:05:34.840 --> 00:05:36.760
things take on a life of their own.


00:05:36.760 --> 00:05:41.600
And he's probably in awe of all the stuff happening and all the things going on.


00:05:41.600 --> 00:05:43.360
And there's probably a lot of stuff in Jupiter.


00:05:43.360 --> 00:05:44.840
He doesn't even know about, right.


00:05:44.840 --> 00:05:45.840
It's like that's happening.


00:05:45.840 --> 00:05:46.840
Yeah.


00:05:46.840 --> 00:05:47.840
It's huge.


00:05:47.840 --> 00:05:48.840
And yeah.


00:05:48.840 --> 00:05:50.640
And like the leadership structure has changed to accommodate that.


00:05:50.640 --> 00:05:54.880
So Brian is no longer the benevolent dictator for life.


00:05:54.880 --> 00:05:59.880
Jupiter Project Jupiter is now governed by a committee, decentralized and democratized


00:05:59.880 --> 00:06:00.880
just to allow it to scale.


00:06:00.880 --> 00:06:02.280
Yeah, of course.


00:06:02.280 --> 00:06:07.960
Let's start by talking about a bit of the role of AI in data science.


00:06:07.960 --> 00:06:09.520
I don't know how you feel about it.


00:06:09.520 --> 00:06:14.240
You must be somewhat of an advocate putting this much time and energy into bringing it


00:06:14.240 --> 00:06:15.240
to Jupiter.


00:06:15.240 --> 00:06:16.240
Wow.


00:06:16.240 --> 00:06:22.440
However, personally, when I, I want to know something, I don't think there's a great specific


00:06:22.440 --> 00:06:27.440
search result for it straight to ChatGPT or friends.


00:06:27.440 --> 00:06:31.960
I think there's, there's such a, such a wealth of information there from, I need to take


00:06:31.960 --> 00:06:36.280
this paragraph and clean it up and make it sound better to I have this program.


00:06:36.280 --> 00:06:41.560
I want to convert to another language or I have this data in this website.


00:06:41.560 --> 00:06:42.560
How do I get it?


00:06:42.560 --> 00:06:47.740
You know, like just, you can ask so many open-ended questions and really get great answers.


00:06:47.740 --> 00:06:52.680
So it seems to me, especially coming from, like I mentioned before, those diverse backgrounds,


00:06:52.680 --> 00:06:56.840
people not necessarily being like super deep in programming, maybe they're deep in finance,


00:06:56.840 --> 00:07:02.520
but they do programming that having this AI capability to ask like, Hey, I know I can,


00:07:02.520 --> 00:07:05.840
but how, what do you think for data science in particular?


00:07:05.840 --> 00:07:08.160
This is an interesting topic, right?


00:07:08.160 --> 00:07:14.000
Because I think the whole power of language models stems from their ubiquity and versatility


00:07:14.000 --> 00:07:16.880
and how they can sort of be very generally applicable.


00:07:16.880 --> 00:07:21.800
So like the thing about language models is that they're basically statistical models


00:07:21.800 --> 00:07:26.000
that have been trained on a very, very, very large corpus of data.


00:07:26.000 --> 00:07:28.320
And that's really to the computer.


00:07:28.320 --> 00:07:31.280
The computer doesn't really understand English.


00:07:31.280 --> 00:07:33.160
It doesn't really understand natural language.


00:07:33.160 --> 00:07:39.520
And it has, it's basically making, it has like, when it's trained, it has like knowledge


00:07:39.520 --> 00:07:45.880
of the distribution of information and how information sort of interacts with other information.


00:07:45.880 --> 00:07:49.960
Because of that, it's a very, it has a very general applicability, right?


00:07:49.960 --> 00:07:53.480
And I don't think that's utilities limited to data science.


00:07:53.480 --> 00:07:58.560
Now if we're talking about the field of data science specifically, I think language models


00:07:58.560 --> 00:08:04.840
have extraordinary utility and explanatory natural language tasks, which I think everybody


00:08:04.840 --> 00:08:09.240
is aware of now, now that a ChatGPT has been out for almost a year.


00:08:09.240 --> 00:08:14.400
But I think in the field of data science and other like deep technical fields, they're


00:08:14.400 --> 00:08:19.080
especially applicable because of how complicated some of the work is.


00:08:19.080 --> 00:08:24.160
Chat AI can also help analyze and debug code, which JupyterLab also allows you to do.


00:08:24.160 --> 00:08:29.120
I know it's statistics, but when you look at it, it seems like it understands, right?


00:08:29.120 --> 00:08:32.200
It seems like it understands my question.


00:08:32.200 --> 00:08:38.020
And I think one of the really interesting parts is the fact that these LLMs have a context.


00:08:38.020 --> 00:08:40.040
I would like you to write a program in Python.


00:08:40.040 --> 00:08:41.320
Okay, great.


00:08:41.320 --> 00:08:42.320
Tell it to me.


00:08:42.320 --> 00:08:46.640
I want a program that does X, Y, Z, and then it writes it in Python, which sounds so simple.


00:08:46.640 --> 00:08:53.360
But up until then, things like Siri and all the other voice assistants, they seemed so


00:08:53.360 --> 00:08:56.120
disjointed and so not understanding.


00:08:56.120 --> 00:08:57.800
I just asked you about the weather.


00:08:57.800 --> 00:09:03.760
And when I ask you how hot it is, how do you not understand that that applies to the weather?


00:09:03.760 --> 00:09:09.160
The fact that you converse with them over a series of interactions is pretty special.


00:09:09.160 --> 00:09:14.240
The context, yeah, it's basically implemented just by passing the history, the whole history


00:09:14.240 --> 00:09:16.240
appended to your prompt.


00:09:16.240 --> 00:09:22.900
So yeah, it's not like any super staple magic or whatever, but it's still very interesting


00:09:22.900 --> 00:09:25.080
like how far you can take it.


00:09:25.080 --> 00:09:31.960
And yeah, definitely like the context allows you to interact with the AI a lot more conversationally


00:09:31.960 --> 00:09:32.960
and humanly.


00:09:32.960 --> 00:09:36.600
Like you don't have to pretend like you're talking to an AI.


00:09:36.600 --> 00:09:40.840
You can actually just kind of treat it like a human and it still answers questions very


00:09:40.840 --> 00:09:41.840
well.


00:09:41.840 --> 00:09:45.920
Yeah, it was even at Google, there was that engineer who said they thought it had become


00:09:45.920 --> 00:09:49.120
sentient and there was that whole drama around that, right?


00:09:49.120 --> 00:09:54.380
This is such a crazy coincidence, but my roommate is actually friends with that gentleman.


00:09:54.380 --> 00:09:55.380
Oh really?


00:09:55.380 --> 00:09:56.380
I know.


00:09:56.380 --> 00:09:57.380
Wow.


00:09:57.380 --> 00:09:58.380
Absolutely crazy coincidence.


00:09:58.380 --> 00:10:02.420
Like, I just thought it was really funny you bringing it up.


00:10:02.420 --> 00:10:05.900
It was a Cajun gentleman, a senior engineer at Google, right?


00:10:05.900 --> 00:10:06.900
Yeah.


00:10:07.900 --> 00:10:08.900
Very funny, mate.


00:10:08.900 --> 00:10:09.900
It's been a little while.


00:10:09.900 --> 00:10:12.540
I don't remember all the details, but yeah, I mean, it's pretty wild and pretty powerful.


00:10:12.540 --> 00:10:16.700
I think I've recently read, I'm trying to quick look it up, but I didn't find it.


00:10:16.700 --> 00:10:21.700
I think they just used LLMs to discover like a new protein folding.


00:10:21.700 --> 00:10:27.220
And it's that kind of stuff that makes me think like, okay, how interesting that knowledge


00:10:27.220 --> 00:10:29.700
wasn't out there in the world necessarily.


00:10:29.700 --> 00:10:31.740
I have a lot to say on that subject.


00:10:31.740 --> 00:10:36.140
So personally, I don't believe that language models are actually intelligent.


00:10:36.140 --> 00:10:41.380
I think that people are conflating, well, they're certainly not conscious, right?


00:10:41.380 --> 00:10:42.380
Absolutely.


00:10:42.380 --> 00:10:43.380
Yeah.


00:10:43.380 --> 00:10:44.380
As to whether they're intelligent, I don't think they are.


00:10:44.380 --> 00:10:50.180
I think that intelligence has a, like intelligence as we know it, as humans know it, has some


00:10:50.180 --> 00:10:53.940
different characteristics that language models don't really exhibit.


00:10:53.940 --> 00:10:58.300
They're best thought of as like really, really, really good statistical models.


00:10:58.300 --> 00:11:00.780
Like are you familiar with the mirror test?


00:11:00.780 --> 00:11:01.780
Maybe but I don't think so.


00:11:01.780 --> 00:11:02.780
Yeah.


00:11:02.780 --> 00:11:07.020
So it's like this idea on animal psychology, but like if a cat sees a mirror, it thinks


00:11:07.020 --> 00:11:10.380
it's another cat because it doesn't recognize its own inflection.


00:11:10.380 --> 00:11:11.380
Right.


00:11:11.380 --> 00:11:14.580
So it's like, oh, they all get like big and like, we're trying to like act big and tough


00:11:14.580 --> 00:11:15.580
to chase it off.


00:11:15.580 --> 00:11:16.580
And it's just them.


00:11:16.580 --> 00:11:17.580
Yeah.


00:11:17.580 --> 00:11:19.660
Language models are kind of like that for, but for humans, right?


00:11:19.660 --> 00:11:24.660
Like if something mimics human like quality closely enough, very tempting to think of


00:11:24.660 --> 00:11:25.660
it as human.


00:11:25.660 --> 00:11:26.660
Yeah.


00:11:26.660 --> 00:11:29.780
We see faces on Mars when it's really just erosion, stuff like that.


00:11:29.780 --> 00:11:30.780
Exactly.


00:11:30.780 --> 00:11:34.980
I could talk about this like for a full hour, but yeah, we should totally move on to another


00:11:34.980 --> 00:11:36.940
topic before I go on a tirade.


00:11:36.940 --> 00:11:41.300
Well, when you're saying it's not that intelligent, I'm wondering if maybe you've mis-misnamed


00:11:41.300 --> 00:11:42.860
you this project.


00:11:42.860 --> 00:11:45.060
Maybe it should just be Jupiter A, like the eye.


00:11:45.060 --> 00:11:46.340
Do you got to drop the eye?


00:11:46.340 --> 00:11:47.340
I don't know.


00:11:47.340 --> 00:11:48.820
We're just following convention, right?


00:11:48.820 --> 00:11:51.380
So I still use the term AI.


00:11:51.380 --> 00:11:52.780
Of course, you got to talk to people.


00:11:52.780 --> 00:11:53.780
Yeah.


00:11:53.780 --> 00:11:55.020
Emphasize the artificial, huh?


00:11:55.020 --> 00:11:57.540
Before we get to Jupiter AI, what came before?


00:11:57.540 --> 00:12:02.420
How did people work with things like ChatGPT and other LLMs in Jupiter before stuff


00:12:02.420 --> 00:12:03.940
like Jupiter AI came along?


00:12:03.940 --> 00:12:07.060
I think initially it was a combination.


00:12:07.060 --> 00:12:12.100
So the initial motivation for this project came through a combination of sort of a demo


00:12:12.100 --> 00:12:16.300
put together by Fernando Perez, who is another, I believe.


00:12:16.300 --> 00:12:18.020
I think he's another co-founder.


00:12:18.020 --> 00:12:20.300
Another co-founder of Project Jupiter.


00:12:20.300 --> 00:12:26.100
And he put together this demo called JupiT, which is, it's spelled like Jupiter, except


00:12:26.100 --> 00:12:30.620
the last letter is an E. And it's a pun of like ChatGPT, right?


00:12:30.620 --> 00:12:31.620
Ju-pi-ti.


00:12:31.620 --> 00:12:39.260
So it was a combination of that demo project set by Fernando and some motivation from my


00:12:39.260 --> 00:12:47.460
manager, Brian, who also was, you know, as a leader in the AWS AI organization, you know,


00:12:47.460 --> 00:12:51.820
he's always trying to think of fancy schmancy new ideas, right?


00:12:51.820 --> 00:12:54.820
And this is a pretty fun idea to work out.


00:12:54.820 --> 00:12:59.620
So I put together, I think this was sometime in early January, I put together the first


00:12:59.620 --> 00:13:04.780
demo, it was private, and I showed it off to the team and they were like, wow, this


00:13:04.780 --> 00:13:05.780
has a lot of potential.


00:13:05.780 --> 00:13:08.020
Let's see if we can grow it a bit more.


00:13:08.020 --> 00:13:13.540
And then as we worked on it for the next few months, it became clear like, oh, wow, this


00:13:13.540 --> 00:13:15.740
is actually, it's actually really significant.


00:13:15.740 --> 00:13:17.260
Let's keep working on this.


00:13:17.260 --> 00:13:23.820
So it's definitely been a collaborative effort to bring a Jupiter AI to where it is today.


00:13:23.820 --> 00:13:24.820
Sounds like it.


00:13:24.820 --> 00:13:27.380
Definitely a lot of contributors over on the GitHub listing.


00:13:27.380 --> 00:13:28.380
Let's get into it.


00:13:28.380 --> 00:13:29.380
What is Jupiter AI?


00:13:29.380 --> 00:13:33.940
I mean, people can guess, but it's also different in ways than maybe just plug it in a chat


00:13:33.940 --> 00:13:34.940
window.


00:13:34.940 --> 00:13:39.740
Jupiter AI is actually, right now it's two packages, but it's best thought of as just


00:13:39.740 --> 00:13:44.500
a set of packages that bring generative AI to Project Jupiter as a whole.


00:13:44.500 --> 00:13:48.820
So not just JupyterLab, but also Jupyter Notebook and IPython.


00:13:48.820 --> 00:13:53.900
Even the shell, how do you, I guess you invoke it by doing like the magic there as well.


00:13:53.900 --> 00:13:59.340
It's the IPython shell, which is not the same as like a bash shell for instance, in a terminal.


00:13:59.340 --> 00:14:03.580
You can dive into a little bit more detail on what these two packages are.


00:14:03.580 --> 00:14:09.300
So we have the base Jupyter AI package, which is spelled exactly as you might imagine it.


00:14:09.300 --> 00:14:10.300
It's Jupyter-AI.


00:14:10.300 --> 00:14:18.380
That is a JupyterLab extension that brings a UI to JupyterLab, which is the screenshot


00:14:18.380 --> 00:14:23.420
that you're showing on your screen, but for viewers without a screen, it basically is


00:14:23.420 --> 00:14:28.700
the package that gives, adds that chat panel to the left-hand side and allows you to speak


00:14:28.700 --> 00:14:30.860
conversationally with an AI.


00:14:30.860 --> 00:14:36.220
And then the second package is Jupyter-AI-Magix, which is spelled the same, except at the end,


00:14:36.220 --> 00:14:38.820
it's spelled hyphen-magix.


00:14:38.820 --> 00:14:44.860
And that is actually the base library that implements some of the AI providers we use


00:14:44.860 --> 00:14:48.820
and brings things called magic commands to the IPython shell.


00:14:48.820 --> 00:14:54.980
And magic commands basically let you invoke the library code, aka like calling, using


00:14:54.980 --> 00:14:57.540
it to like call language models, for instance.


00:14:57.540 --> 00:15:00.940
And that allows you to do it inside an IPython context.


00:15:00.940 --> 00:15:05.540
So what's crazy is that if you run IPython in your terminal shell, you can actually run


00:15:05.540 --> 00:15:09.580
Jupyter-AI from your terminal, which is pretty cool.


00:15:09.580 --> 00:15:11.020
Yeah, I didn't realize that.


00:15:11.020 --> 00:15:14.300
I mean, it makes sense, of course, but I hadn't really thought about it.


00:15:14.300 --> 00:15:18.460
Yeah, I thought more of this like kind of a GUI type of thing that was alongside what


00:15:18.460 --> 00:15:19.460
you were doing.


00:15:19.460 --> 00:15:20.460
Yeah.


00:15:20.460 --> 00:15:23.820
We try to make it flexible and there's reasons for the magic commands, which I can, I can


00:15:23.820 --> 00:15:26.180
talk about that later though.


00:15:26.180 --> 00:15:27.180
Sure.


00:15:27.180 --> 00:15:31.580
This portion of Talk Python to Me is brought to you by Posit, the makers of Shiny, formerly


00:15:31.580 --> 00:15:35.700
RStudio, and especially Shiny for Python.


00:15:35.700 --> 00:15:36.700
Let me ask you a question.


00:15:36.700 --> 00:15:38.700
Are you building awesome things?


00:15:38.700 --> 00:15:39.700
Of course you are.


00:15:39.700 --> 00:15:41.180
You're a developer or data scientist.


00:15:41.180 --> 00:15:42.220
That's what we do.


00:15:42.220 --> 00:15:44.540
And you should check out Posit Connect.


00:15:44.540 --> 00:15:49.300
Posit Connect is a way for you to publish, share, and deploy all the data products that


00:15:49.300 --> 00:15:51.940
you're building using Python.


00:15:51.940 --> 00:15:54.140
People ask me the same question all the time.


00:15:54.140 --> 00:15:57.820
Michael, I have some cool data science project or notebook that I built.


00:15:57.820 --> 00:16:00.500
How do I share it with my users, stakeholders, teammates?


00:16:00.500 --> 00:16:06.100
Do I need to learn FastAPI or Flask or maybe Vue or React.js?


00:16:06.100 --> 00:16:07.100
Hold on now.


00:16:07.100 --> 00:16:10.520
Those are cool technologies and I'm sure you'd benefit from them, but maybe stay focused


00:16:10.520 --> 00:16:11.980
on the data project?


00:16:11.980 --> 00:16:14.420
Let Posit Connect handle that side of things?


00:16:14.420 --> 00:16:18.900
With Posit Connect, you can rapidly and securely deploy the things you build in Python.


00:16:18.900 --> 00:16:25.540
Streamlet, Dash, Shiny, Bokeh, FastAPI, Flask, Quadro, Reports, Dashboards, and APIs.


00:16:25.540 --> 00:16:27.820
Posit Connect supports all of them.


00:16:27.820 --> 00:16:32.620
And Posit Connect comes with all the bells and whistles to satisfy IT and other enterprise


00:16:32.620 --> 00:16:33.620
requirements.


00:16:33.620 --> 00:16:37.860
Make deployment the easiest step in your workflow with Posit Connect.


00:16:37.860 --> 00:16:44.260
For a limited time, you can try Posit Connect for free for three months by going to talkpython.fm/posit.


00:16:44.260 --> 00:16:47.540
That's talkpython.fm/POSIT.


00:16:47.540 --> 00:16:49.980
The link is in your podcast player show notes.


00:16:49.980 --> 00:16:55.020
Thank you to the team at Posit for supporting Talk Python.


00:16:55.020 --> 00:17:00.420
Now one thing, kinda, you said this, but I want to emphasize it a little bit in that


00:17:00.420 --> 00:17:05.700
this will run anywhere IPython kernel runs, which is JupyterLab notebook, but also Google


00:17:05.700 --> 00:17:09.140
Colab, VS Code, other places as well.


00:17:09.140 --> 00:17:13.300
So pretty much it comes to you wherever your Jupyter type of stuff is.


00:17:13.300 --> 00:17:14.300
Yeah.


00:17:14.300 --> 00:17:15.660
And the same goes for your lab extension.


00:17:15.660 --> 00:17:20.500
So the great thing about lab extensions is that they work anywhere where the product


00:17:20.500 --> 00:17:23.940
is just sort of built on top of JupyterLab, right?


00:17:23.940 --> 00:17:30.300
So Google Colab is essentially what Google has is a, well, I can't, I obviously can't


00:17:30.300 --> 00:17:34.340
attest to what they're actually doing, but most likely it's like a set of extensions


00:17:34.340 --> 00:17:39.780
or CSS themes that are built on top of JupyterLab, but like the underlying code is still, is


00:17:39.780 --> 00:17:40.780
still JupyterLab.


00:17:40.780 --> 00:17:41.780
It's still mostly JupyterLab.


00:17:41.780 --> 00:17:46.260
So you can actually just install extensions and they work just fine, which is another


00:17:46.260 --> 00:17:49.700
reason why JupyterLab is just a pretty awesome.


00:17:49.700 --> 00:17:51.380
Yeah, it sure is.


00:17:51.380 --> 00:17:52.380
Yeah.


00:17:52.380 --> 00:17:57.300
JupyterLab itself is basically a preselected, pre-configured set of extensions, right?


00:17:57.300 --> 00:17:58.300
That's pretty cool.


00:17:58.300 --> 00:17:59.300
That is true.


00:17:59.300 --> 00:18:00.300
Yeah.


00:18:00.300 --> 00:18:04.780
Giving preference to, or my showing just what I play with mostly, which is ChatGPT.


00:18:04.780 --> 00:18:08.140
There's actually a lot of language models that you can work with, right?


00:18:08.140 --> 00:18:12.700
One of the big things, and this is something I'll circle back to later, is that JupyterAI


00:18:12.700 --> 00:18:17.620
is meant to be model agnostic, meaning that we don't discriminate against the choice of


00:18:17.620 --> 00:18:22.340
model or model provider, because as an open source project, it's very imperative that


00:18:22.340 --> 00:18:24.340
we maintain the trust of our users, right?


00:18:24.340 --> 00:18:29.060
Like users have to be sure that this isn't just some product that exists to force them


00:18:29.060 --> 00:18:36.540
and force or pigeonhole them into using a certain model provider like OpenAI or Anthropic.


00:18:36.540 --> 00:18:38.680
This, the product makes no opinions.


00:18:38.680 --> 00:18:42.180
We simply try to support everything as best as we can.


00:18:42.180 --> 00:18:49.180
And we've written a lot of code to make sure that all of these models and just play nicely


00:18:49.180 --> 00:18:50.460
together essentially.


00:18:50.460 --> 00:18:57.420
Like every model provider, like let's say from Anthropic or AI21 or Cohere, every one


00:18:57.420 --> 00:19:00.260
of these APIs kind of has its own quirks.


00:19:00.260 --> 00:19:03.460
Every one of its Python SDKs has its own quirks.


00:19:03.460 --> 00:19:08.780
And we work very hard to basically iron out the surface and make everything have the same


00:19:08.780 --> 00:19:09.780
interface.


00:19:09.780 --> 00:19:11.220
We can talk about that later though.


00:19:11.220 --> 00:19:14.060
- Sure, and it also makes it pretty easy to try them out, right?


00:19:14.060 --> 00:19:19.460
If you switch from one to the other, you're like, I wonder how Hugging Face versus OpenAI


00:19:19.460 --> 00:19:21.700
would do to solve this problem, right?


00:19:21.700 --> 00:19:22.700
- Absolutely.


00:19:22.700 --> 00:19:27.500
Like that's kind of one of the ideas is like, while certain model providers, they might


00:19:27.500 --> 00:19:34.220
offer a UI if they're very well funded by their investors, for example, OpenAI to have


00:19:34.220 --> 00:19:41.980
a UI, but that UI only allows you to compare between different models from OpenAI, which


00:19:41.980 --> 00:19:48.300
as an independent third party looking to use an AI service, that information is obviously


00:19:48.300 --> 00:19:49.580
a little biased, right?


00:19:49.580 --> 00:19:52.740
You want to see what other providers have to offer.


00:19:52.740 --> 00:19:54.100
What does AI21 have?


00:19:54.100 --> 00:19:55.740
What does Anthropic have?


00:19:55.740 --> 00:20:05.180
And right now, there really is no cross model provider UI or interface in general.


00:20:05.180 --> 00:20:09.660
But that's kind of one of the use cases that Jupyter AI was intended to fit.


00:20:09.660 --> 00:20:13.500
- Yeah, provides a standard way to interact with all these and sort of compare them.


00:20:13.500 --> 00:20:16.540
And it's also a UI for them if you're using JupyterLab, yeah.


00:20:16.540 --> 00:20:20.820
I'm not familiar with all of these different models and companies.


00:20:20.820 --> 00:20:26.380
Do any of those run locally, like things like GPT4ALL, where it's a local model versus some


00:20:26.380 --> 00:20:27.540
kind of cloud?


00:20:27.540 --> 00:20:28.540
Where's your key?


00:20:28.540 --> 00:20:30.460
Where's your billing details and all that?


00:20:30.460 --> 00:20:36.460
- We actually recently just merged a PR that adds a GPT4ALL support.


00:20:36.460 --> 00:20:38.260
That's included in the release.


00:20:38.260 --> 00:20:43.620
However, back when we first implemented this a few months ago, I had a few issues with


00:20:43.620 --> 00:20:45.340
the platform compatibility.


00:20:45.340 --> 00:20:51.780
So like some of the binaries that we downloaded from GPT4ALL didn't seem to work well on my


00:20:51.780 --> 00:20:53.860
M1 Mac, for instance.


00:20:53.860 --> 00:20:58.500
I'd say, yes, we do have local model support, but it's a bit experimental right now.


00:20:58.500 --> 00:21:03.540
We're still like ironing out the edges and testing, like seeing how we can make the experience


00:21:03.540 --> 00:21:04.540
better.


00:21:04.540 --> 00:21:09.180
Like does it sometimes only get bad output because we forgot to install the shared library?


00:21:09.180 --> 00:21:12.540
Those are the type of questions that our team is wrangling with right now.


00:21:12.540 --> 00:21:13.540
- I see.


00:21:13.540 --> 00:21:17.860
So maybe, I just jumped right into it, but maybe tell people what GPT4ALL is just real


00:21:17.860 --> 00:21:18.860
quickly.


00:21:18.860 --> 00:21:21.380
- GPT4ALL offers a few local models.


00:21:21.380 --> 00:21:23.860
Actually they offer several, I believe, not just a few.


00:21:23.860 --> 00:21:26.580
- I think they offer maybe like 10 to 15.


00:21:26.580 --> 00:21:28.300
It's the numbers getting quite large.


00:21:28.300 --> 00:21:29.300
- Yeah.


00:21:29.300 --> 00:21:30.980
- To the point where I don't know what the right choice is.


00:21:30.980 --> 00:21:32.980
I'm like, which one do I download?


00:21:32.980 --> 00:21:33.980
They say they're all good.


00:21:33.980 --> 00:21:34.980
Of course they're going to say they're good.


00:21:34.980 --> 00:21:35.980
- I'll be frank.


00:21:35.980 --> 00:21:39.240
I don't actually have that much experience with GPT4ALL.


00:21:39.240 --> 00:21:44.920
We mainly use them as sort of a provider for like these free and open source language models.


00:21:44.920 --> 00:21:48.480
I think they offer a UI as well for multiple platforms.


00:21:48.480 --> 00:21:52.200
- I've only played with them a little bit, just started checking it out, but it's basically


00:21:52.200 --> 00:21:56.680
a local, you download the model, you run it locally, you don't pay anything because it's


00:21:56.680 --> 00:21:58.880
just running on your machine, right?


00:21:58.880 --> 00:22:04.760
As opposed to say OpenAI and others where you've at least got rate limiting and certain


00:22:04.760 --> 00:22:09.200
amount of queries before you have to pay and maybe potentially access to better models


00:22:09.200 --> 00:22:12.520
like ChatGPT4 versus 3.5 and so on.


00:22:12.520 --> 00:22:18.840
- That's also taking the characteristics of these service providers for granted, right?


00:22:18.840 --> 00:22:26.640
So yes, definitely, while it does hurt the wallet to pay for usage credits, right?


00:22:26.640 --> 00:22:31.800
It's also pretty remarkable how small the latency has gotten with some of these APIs.


00:22:31.800 --> 00:22:36.240
I've gotten like sub 500 millisecond latency on some of these APIs.


00:22:36.240 --> 00:22:41.560
That's really incredible because when I was using GPT4ALL, the latency was a little bit


00:22:41.560 --> 00:22:42.560
high, right?


00:22:42.560 --> 00:22:45.640
When running locally with limited computer resources.


00:22:45.640 --> 00:22:48.480
It's really remarkable like how fast these APIs are.


00:22:48.480 --> 00:22:50.420
- It is pretty insane.


00:22:50.420 --> 00:22:51.760
Sometimes it drives me crazy.


00:22:51.760 --> 00:22:55.240
I'm just only again, referring to ChatGPT because I don't have the experience with the


00:22:55.240 --> 00:23:00.880
others to the degree, but it drives me crazy how it artificially limits the response based


00:23:00.880 --> 00:23:02.480
on the speed of the response.


00:23:02.480 --> 00:23:04.280
So it looks like it's chatting with you.


00:23:04.280 --> 00:23:09.520
I'm like, no, I have four pages of stuff because you just get it out.


00:23:09.520 --> 00:23:14.840
It'll say something like, if you can ask, I gave you a five page program.


00:23:14.840 --> 00:23:15.840
Let's call it X.


00:23:15.840 --> 00:23:17.400
If you say, what is X?


00:23:17.400 --> 00:23:19.960
It'll just start printing it slowly line by line.


00:23:19.960 --> 00:23:22.120
You know, you just are echoing it back.


00:23:22.120 --> 00:23:23.120
Just get it out.


00:23:23.120 --> 00:23:24.520
I want to ask you the next question.


00:23:24.520 --> 00:23:25.680
You know what I mean?


00:23:25.680 --> 00:23:29.480
- In that case, that's actually a feature request that we've gotten because it doesn't


00:23:29.480 --> 00:23:30.880
actually slow down.


00:23:30.880 --> 00:23:33.320
Like, it's not just like a pointless animation.


00:23:33.320 --> 00:23:34.320
Yeah.


00:23:34.320 --> 00:23:37.480
The servers are streaming essentially token by token, right?


00:23:37.480 --> 00:23:39.600
As the language model generates output.


00:23:39.600 --> 00:23:44.240
So it's kind of more like a progress indicator than a superfluous animation.


00:23:44.240 --> 00:23:45.240
Yeah.


00:23:45.240 --> 00:23:46.240
- Yeah, of course.


00:23:46.240 --> 00:23:50.280
But if you've got something large, large blocks of text you're working with, it can be, it


00:23:50.280 --> 00:23:51.280
can be a drag.


00:23:51.280 --> 00:23:52.280
All right.


00:23:52.280 --> 00:23:55.640
I wanted to kind of touch on some of the different features that I pulled out that I thought


00:23:55.640 --> 00:23:56.640
were cool.


00:23:56.640 --> 00:24:01.800
I mean, obviously it goes without saying that Jupyter AI is on GitHub.


00:24:01.800 --> 00:24:04.000
I mean, because it's software.


00:24:04.000 --> 00:24:09.120
So it's open source, which I don't know if we said that, but obviously free open source


00:24:09.120 --> 00:24:12.080
on GitHub, BSD3 license.


00:24:12.080 --> 00:24:17.240
But it's also noteworthy that it's officially under the JupyterLab organization, not under


00:24:17.240 --> 00:24:19.920
the, the David, or David account.


00:24:19.920 --> 00:24:21.120
You know what I mean?


00:24:21.120 --> 00:24:23.600
- It's officially part of the JupyterLab sub project.


00:24:23.600 --> 00:24:28.000
And yep, as you pointed out, we're under the JupyterLab GitHub org as well.


00:24:28.000 --> 00:24:29.360
- Yeah, that's awesome.


00:24:29.360 --> 00:24:32.520
Let's talk about some of the different things you can do with it.


00:24:32.520 --> 00:24:36.480
Some of them will be straightforward, like just like, how do I write a function, Jupyter


00:24:36.480 --> 00:24:37.480
AI?


00:24:37.480 --> 00:24:40.160
And others I think are going to be a little more interesting.


00:24:40.160 --> 00:24:43.520
So let's start with asking something about your notebook.


00:24:43.520 --> 00:24:46.060
Tell us what people can, can do here.


00:24:46.060 --> 00:24:50.360
- Asking about your notebook basically means like you can actually teach Jupyter AI about


00:24:50.360 --> 00:24:52.120
certain files, right?


00:24:52.120 --> 00:24:57.880
So the way you do this is via a Slack command in the chat UI that you type slash learn and


00:24:57.880 --> 00:24:59.360
then file path.


00:24:59.360 --> 00:25:04.080
And you essentially teaches the Jupyter AI about that file.


00:25:04.080 --> 00:25:08.240
Now it works best with files are written in natural language, right?


00:25:08.240 --> 00:25:11.800
So like text files or markup or markdown rather.


00:25:11.800 --> 00:25:12.800
Yeah.


00:25:12.800 --> 00:25:17.120
So like those, like, especially like developer documentation as well, right?


00:25:17.120 --> 00:25:21.280
It works really well with those, it works best with those kinds of files.


00:25:21.280 --> 00:25:27.840
And after Jupyter AI learns about these files, you can then ask questions about the files.


00:25:27.840 --> 00:25:32.800
It's learned by prefixing your question with the slash ask.


00:25:32.800 --> 00:25:33.800
- That is so cool.


00:25:33.800 --> 00:25:34.800
- It is pretty cool, I know.


00:25:34.800 --> 00:25:39.600
- It's so cool because what I've done a lot of times, if I want ChatGPT to help me, it's


00:25:39.600 --> 00:25:42.040
like, I'm like, all right, well, let me copy some code.


00:25:42.040 --> 00:25:43.040
- Right.


00:25:43.040 --> 00:25:44.680
- Then I'm going to have a conversation about it.


00:25:44.680 --> 00:25:48.560
But a lot of the context of, well, it's actually referencing this other function and what does


00:25:48.560 --> 00:25:54.880
that do or just a broader understanding of what am I actually working on is missing,


00:25:54.880 --> 00:25:55.880
right?


00:25:55.880 --> 00:25:57.120
Because I've only copied it.


00:25:57.120 --> 00:26:01.240
You can't paste, you know, 20 files into ChatGPT and start talking about it.


00:26:01.240 --> 00:26:02.680
But with this, you can, right?


00:26:02.680 --> 00:26:06.320
You can say, learn about, you can say, learn about different things, right?


00:26:06.320 --> 00:26:10.200
You can say, learn about your notebook, but you can also probably tell it like, learn


00:26:10.200 --> 00:26:13.760
about my documentation or learn about my dataset.


00:26:13.760 --> 00:26:15.240
And now let me talk to you about it.


00:26:15.240 --> 00:26:20.280
What's interesting is that the right now, while it works best for natural language documents,


00:26:20.280 --> 00:26:23.360
we are working on improving the experience for code.


00:26:23.360 --> 00:26:29.880
From our testing, like the code is mostly the capabilities of Jupyter AI after learning


00:26:29.880 --> 00:26:35.760
code is right now mostly limited to explaining what code does, but sort of like explains


00:26:35.760 --> 00:26:37.400
it from the doc stream.


00:26:37.400 --> 00:26:42.320
So we're working on a way to format the code in a manner that is more interpretable to


00:26:42.320 --> 00:26:44.480
a language model.


00:26:44.480 --> 00:26:49.040
We're working on ways to improve the experience for code, but yeah, definitely the long-term


00:26:49.040 --> 00:26:54.680
vision is to have Jupyter AI literally be able to learn from a whole directory of files


00:26:54.680 --> 00:27:01.640
or possibly even a URL, like a remote URL to like a documentation page.


00:27:01.640 --> 00:27:02.840
We have some big ideas there.


00:27:02.840 --> 00:27:03.960
We're still working on them.


00:27:03.960 --> 00:27:06.480
I want to work with a new package XYZ.


00:27:06.480 --> 00:27:08.320
Like I don't know what XYZ is.


00:27:08.320 --> 00:27:09.320
You know what?


00:27:09.320 --> 00:27:10.580
Here's where you can learn about it.


00:27:10.580 --> 00:27:11.800
Go and figure it out.


00:27:11.800 --> 00:27:17.840
Like query the PyPI API, get the docs page from the metadata, and then go to that URL,


00:27:17.840 --> 00:27:18.840
scrape it.


00:27:18.840 --> 00:27:20.440
Like lots of things we're exploring.


00:27:20.440 --> 00:27:22.120
It's still kind of early days for this, right?


00:27:22.120 --> 00:27:23.920
You've been at it about a year or so?


00:27:23.920 --> 00:27:25.760
It's been out for a while.


00:27:25.760 --> 00:27:30.200
Recently I've had to work on a few other things as well, like Jupyter AI.


00:27:30.200 --> 00:27:33.840
Unfortunately I cannot give my entire life to Jupyter AI.


00:27:33.840 --> 00:27:38.080
So I've been working on a few other things these past few months, but yeah, there are


00:27:38.080 --> 00:27:41.560
a lot of things that I envision for Jupyter AI.


00:27:41.560 --> 00:27:46.640
I have a much bigger vision for what I want this project to be in, what it can be capable


00:27:46.640 --> 00:27:47.640
of.


00:27:47.640 --> 00:27:48.640
Exciting.


00:27:48.640 --> 00:27:51.800
So what did this screenshot that you got here in the section that I'll link to in the show


00:27:51.800 --> 00:27:57.120
notes is cool because you can select a piece of a portion, not even a whole cell, but a


00:27:57.120 --> 00:27:59.540
portion of code in a cell.


00:27:59.540 --> 00:28:01.840
And then you can ask, what does this code do?


00:28:01.840 --> 00:28:06.200
We have an integration with the JupyterLab editor APIs.


00:28:06.200 --> 00:28:10.560
So you can select a block of code and then include that in your prompt.


00:28:10.560 --> 00:28:13.640
And it will be appended to your prompt below, right?


00:28:13.640 --> 00:28:15.240
It's appended to prompt, sorry.


00:28:15.240 --> 00:28:18.280
You can basically ask, so you can select a block of code.


00:28:18.280 --> 00:28:22.920
So in this screenshot right here, there's this block of code that computes the least


00:28:22.920 --> 00:28:26.160
common multiple of two integers, right?


00:28:26.160 --> 00:28:31.000
And you can select that and then click include selection and then ask Jupyter AI, what does


00:28:31.000 --> 00:28:32.000
this do?


00:28:32.000 --> 00:28:33.340
Which is pretty awesome.


00:28:33.340 --> 00:28:35.240
Another checkbox is replace selection.


00:28:35.240 --> 00:28:39.520
I'm guessing that is like, help me rewrite this code to be more efficient.


00:28:39.520 --> 00:28:41.240
Or if there's any bugs, fix it.


00:28:41.240 --> 00:28:44.240
So the replace selection checkbox is totally independent.


00:28:44.240 --> 00:28:47.800
So both, so you can actually use both at the same time.


00:28:47.800 --> 00:28:50.340
And one of the use cases for this is refactoring.


00:28:50.340 --> 00:28:55.440
And I've actually applied this in practice a few times, where you can basically select


00:28:55.440 --> 00:29:00.440
a block of code, and then click both include and replace selection.


00:29:00.440 --> 00:29:04.920
And then you can pull out your prompt to say, refactor this block of code, do not include


00:29:04.920 --> 00:29:06.920
any additional help or text.


00:29:06.920 --> 00:29:11.640
And when you send that prompt over, it will actually refactor the code for you in your


00:29:11.640 --> 00:29:15.960
notebook, which is, yeah, like is, is pretty great.


00:29:15.960 --> 00:29:16.960
That's pretty awesome.


00:29:16.960 --> 00:29:20.860
You know, you could do things like refactor this to use guarding clauses.


00:29:20.860 --> 00:29:24.960
So it's less nested, less, less arrow code or whatever, right?


00:29:24.960 --> 00:29:25.960
Yeah.


00:29:25.960 --> 00:29:27.840
Or like add a docstring, right?


00:29:27.840 --> 00:29:32.360
Summarize this purpose of this function, and then enclose that in a docstring and add it


00:29:32.360 --> 00:29:33.360
to the function.


00:29:33.360 --> 00:29:34.360
Right.


00:29:34.360 --> 00:29:36.640
Or this code is pandas code, but I'd like to use pollers.


00:29:36.640 --> 00:29:40.000
Please rewrite it for pollers, which is not a super compatible API.


00:29:40.000 --> 00:29:42.440
It's not like DAST to pandas, where it's basically the same.


00:29:42.440 --> 00:29:43.440
Yeah.


00:29:43.440 --> 00:29:46.040
And this kind of circles back to that question that you had asked earlier.


00:29:46.040 --> 00:29:50.320
I think I went on a tangent there and didn't fully answer, but like, what is like the utility


00:29:50.320 --> 00:29:53.240
of Jupyter AI to like data practitioners, right?


00:29:53.240 --> 00:29:57.280
So we're talking data scientists, machine learning engineers, like this, the include


00:29:57.280 --> 00:30:02.880
selection features, we've heard great feedback about how helpful it is to like actually explain


00:30:02.880 --> 00:30:03.960
a data set.


00:30:03.960 --> 00:30:09.080
So sometimes like you're working with a test set and it's not immediately clear what the


00:30:09.080 --> 00:30:13.120
features of this test set are, or like what this even does, because sometimes it's like


00:30:13.120 --> 00:30:17.960
high dimensional data and they can literally select it and then click include selection


00:30:17.960 --> 00:30:21.560
and say, and tell Jupyter AI, explain to me what this does.


00:30:21.560 --> 00:30:24.040
Just like, what, what is this like data frame stuff?


00:30:24.040 --> 00:30:26.120
Like, whoa, we got data frames in data frames.


00:30:26.120 --> 00:30:27.480
Like what's going on here?


00:30:27.480 --> 00:30:28.880
Like what even is structure?


00:30:28.880 --> 00:30:29.880
That's awesome.


00:30:29.880 --> 00:30:30.880
And I think it's super valuable.


00:30:30.880 --> 00:30:34.480
And this is like a little bit I was getting to before one of the features that I think


00:30:34.480 --> 00:30:35.480
is cool.


00:30:35.480 --> 00:30:39.560
Whereas if you just go with straight ChatGPT, you copy your code, you paste it into


00:30:39.560 --> 00:30:40.560
the chat.


00:30:40.560 --> 00:30:43.720
Hopefully it doesn't say it's too much text and then you can talk about it.


00:30:43.720 --> 00:30:47.220
But then when you get an answer, you've got to grab it, move it back over.


00:30:47.220 --> 00:30:50.520
And this just, this fluid back and forth is really nice.


00:30:50.520 --> 00:30:51.520
Yeah.


00:30:51.520 --> 00:30:56.200
And that's actually one of the design principles that we worked out when first starting this


00:30:56.200 --> 00:30:57.760
project officially.


00:30:57.760 --> 00:31:03.440
Was the idea that Jupyter AI should be human centered as in you shouldn't be expected to


00:31:03.440 --> 00:31:06.120
be a developer to know how to use this tool.


00:31:06.120 --> 00:31:11.080
Like this tool is for humans, not, not for any specific persona, just for humans in general.


00:31:11.080 --> 00:31:12.080
That's awesome.


00:31:12.080 --> 00:31:13.080
Yeah.


00:31:13.080 --> 00:31:16.680
So in this case, you select the function that does the lowest common denominator bit and


00:31:16.680 --> 00:31:17.840
you ask it what it does.


00:31:17.840 --> 00:31:23.000
It says the code will print out the least common multiple of two numbers passed to it.


00:31:23.000 --> 00:31:24.920
Super simple, very concise.


00:31:24.920 --> 00:31:26.160
Okay, great.


00:31:26.160 --> 00:31:28.080
Now we can go on to the next thing, right?


00:31:28.080 --> 00:31:29.080
Yeah.


00:31:29.080 --> 00:31:32.600
There's a, this LCD function that, that we're kind of talking about here.


00:31:32.600 --> 00:31:39.960
This example is recursive, which I think recursion is, is pretty insane, right?


00:31:39.960 --> 00:31:42.440
As a, just a concept for people to get their head around.


00:31:42.440 --> 00:31:43.640
This is the iterative version.


00:31:43.640 --> 00:31:46.120
So this is after they, yeah, this is the iterative.


00:31:46.120 --> 00:31:47.120
Oh, this is after.


00:31:47.120 --> 00:31:48.120
Yeah.


00:31:48.120 --> 00:31:51.720
So if we go back up, Oh, one of the things you ask it is things like, and the example


00:31:51.720 --> 00:31:54.440
is rewrite this function to be iterative, not recursive.


00:31:54.440 --> 00:31:55.440
Right.


00:31:55.440 --> 00:31:56.440
That's really, really awesome.


00:31:56.440 --> 00:31:57.440
Right.


00:31:57.440 --> 00:31:59.120
You're like, this is breaking my brain.


00:31:59.120 --> 00:32:04.080
Let's, let's see if we can not do that anymore.


00:32:04.080 --> 00:32:07.540
This portion of Talk Python to me is brought to you by us.


00:32:07.540 --> 00:32:11.520
Have you heard that Python is not good for concurrent programming problems?


00:32:11.520 --> 00:32:16.160
Whoever told you that is living in the past because it's prime time for Python's asynchronous


00:32:16.160 --> 00:32:21.840
features with the widespread adoption of async methods and the async and await keywords.


00:32:21.840 --> 00:32:26.800
Python's ecosystem has a ton of new and exciting frameworks based on async and await.


00:32:26.800 --> 00:32:32.280
That's why we created a course for anyone who wants to learn all of Python's async capabilities,


00:32:32.280 --> 00:32:34.540
async techniques and examples in Python.


00:32:34.540 --> 00:32:40.160
Just visit talkpython.fm/async and watch the intro video to see if this course is for you.


00:32:40.160 --> 00:32:42.620
It's only $49 and you own it forever.


00:32:42.620 --> 00:32:43.620
No subscriptions.


00:32:43.620 --> 00:32:48.980
And there are discounts for teams as well.


00:32:48.980 --> 00:32:53.780
Another thing I wanted to talk about, and you talked a fair amount about this in your


00:32:53.780 --> 00:32:54.960
presentations that you did.


00:32:54.960 --> 00:32:59.360
I can't remember if it was the JupyterCon or the PyData one that I saw, but one of those


00:32:59.360 --> 00:33:06.040
two, you talked about generating new notebooks and how it's, it's actually quite a tricky


00:33:06.040 --> 00:33:07.040
process.


00:33:07.040 --> 00:33:08.040
You've got to break it down into little steps.


00:33:08.040 --> 00:33:12.400
Cause if you ask too much from the AI, it kind of doesn't give you a lot of great answers.


00:33:12.400 --> 00:33:13.520
Tell us about making new notebooks.


00:33:13.520 --> 00:33:17.840
Like, why would I even use, like I can go to JupyterLab and say, file new, it'll make


00:33:17.840 --> 00:33:18.840
that for me.


00:33:18.840 --> 00:33:19.840
What's this about?


00:33:19.840 --> 00:33:25.200
The generate capability is great because it generates a file that is essentially a tutorial


00:33:25.200 --> 00:33:28.200
that can be used as a tutorial to teach you about new subjects.


00:33:28.200 --> 00:33:29.200
Right?


00:33:29.200 --> 00:33:34.640
So like you could, for example, submit a prompt, like slash generate a notebook about async


00:33:34.640 --> 00:33:38.600
IO or a demonstration of how to use matplotlib.


00:33:38.600 --> 00:33:43.640
And after, so this will take a bit of time, but eventually Jupyter AI is done thinking


00:33:43.640 --> 00:33:49.360
and generates a file and it names a file and generates a notebook and the notebook has


00:33:49.360 --> 00:33:54.840
a name, it has a title, it has like sections, table of contents, and each of the cells within


00:33:54.840 --> 00:34:01.480
it like is tied to some like topic that is determined to be helpful and to answer the


00:34:01.480 --> 00:34:02.480
user's question.


00:34:02.480 --> 00:34:03.480
Awesome.


00:34:03.480 --> 00:34:11.120
Could I do something like I have weather data in this format from the US weather service.


00:34:11.120 --> 00:34:15.920
Could you generate me a notebook to plot this XYZ and help me answer these questions?


00:34:15.920 --> 00:34:17.920
Or like, could I ask it something like that?


00:34:17.920 --> 00:34:19.080
Not at the moment.


00:34:19.080 --> 00:34:21.120
So like that would best be done.


00:34:21.120 --> 00:34:25.760
That would be done best if since the data is already like, I'm assuming that the data


00:34:25.760 --> 00:34:27.720
is already available, right.


00:34:27.720 --> 00:34:29.440
And some kind of like format.


00:34:29.440 --> 00:34:34.320
So like in a notebook, you could use the chat UI to like select that entire, select that


00:34:34.320 --> 00:34:40.160
selection and then tell it, tell Jupyter AI to generate code to plot that data set.


00:34:40.160 --> 00:34:46.320
So right now generate only takes a natural language prompt as its only argument.


00:34:46.320 --> 00:34:48.320
So it's kind of like stateless in that regard.


00:34:48.320 --> 00:34:53.560
So in this case, you can say slash generate a demonstration on of how to use Matplotlib.


00:34:53.560 --> 00:34:54.760
And then the response is great.


00:34:54.760 --> 00:34:56.360
I'll start working on your notebook.


00:34:56.360 --> 00:34:58.680
It'll take a few minutes, but I'll reply when it's ready.


00:34:58.680 --> 00:35:01.520
In the meantime, let's keep talking.


00:35:01.520 --> 00:35:05.400
So what happens behind the scenes that takes a few minutes here?


00:35:05.400 --> 00:35:06.720
This is a bit interesting.


00:35:06.720 --> 00:35:12.280
It does kind of dive deep into like the technical details, which I'm not sure is, which do you


00:35:12.280 --> 00:35:13.280
want to just like dive?


00:35:13.280 --> 00:35:14.920
Yeah, let's go tell us how it works.


00:35:14.920 --> 00:35:16.640
Probably a good chance to explore that.


00:35:16.640 --> 00:35:17.640
Yeah.


00:35:18.640 --> 00:35:19.640
So slash generate.


00:35:19.640 --> 00:35:24.280
The first thing is that the prompt is first expanded into essentially a table of contents.


00:35:24.280 --> 00:35:29.120
So basically we tell the language model generate us a table of contents conforming to this


00:35:29.120 --> 00:35:30.640
JSON schema.


00:35:30.640 --> 00:35:36.200
And when you pass a JSON schema included in your prompt, the language model will be much


00:35:36.200 --> 00:35:42.440
more, will have a much higher likelihood of returning exclusively an object, a JSON object


00:35:42.440 --> 00:35:46.360
that matches that JSON schema that you had initially provided.


00:35:46.360 --> 00:35:51.360
So in our case, we generate a table of contents and then we take that table of contents.


00:35:51.360 --> 00:35:56.840
And then we say for each section, we do this in parallel, like generate us some code cells


00:35:56.840 --> 00:36:00.680
that are appropriate for teaching this section of the document.


00:36:00.680 --> 00:36:03.520
So for example, for Matplotlib, right?


00:36:03.520 --> 00:36:07.720
Like maybe the first section is your first, like generating your first plot, plotting


00:36:07.720 --> 00:36:08.720
3D functions.


00:36:08.720 --> 00:36:13.400
And the next one is like plotting complex functions with face or something like that.


00:36:13.400 --> 00:36:18.400
And then with each of these sections, we then send another prompt template to the language


00:36:18.400 --> 00:36:21.960
model for each of these sections, asking it to generate the code.


00:36:21.960 --> 00:36:25.960
And then at the end, we join it all together and then we save it to disk and then emit


00:36:25.960 --> 00:36:27.360
that message and say, we're done.


00:36:27.360 --> 00:36:32.800
Maybe the English literature equivalent would be, instead of just saying, write me a story


00:36:32.800 --> 00:36:36.040
about a person who goes on an adventure and gets lost.


00:36:36.040 --> 00:36:41.280
It's like, I want, give me an outline, bullet points of interesting things that would make


00:36:41.280 --> 00:36:44.160
up a story of how somebody goes on an adventure and gets lost.


00:36:44.160 --> 00:36:47.440
And then for each one of those, you're like, now tell this part of the story, now tell


00:36:47.440 --> 00:36:48.440
this part.


00:36:48.440 --> 00:36:51.720
And somehow that makes it more focused and accurate, right?


00:36:51.720 --> 00:36:57.120
The main limitation is that because we're model agnostic, language models are limited


00:36:57.120 --> 00:36:59.640
in how much output they can generate, right?


00:36:59.640 --> 00:37:03.560
The issue we were running into when we were trying to do the whole thing all at once,


00:37:03.560 --> 00:37:07.560
like generate, meet a whole notebook, is that some language models just couldn't do it.


00:37:07.560 --> 00:37:12.400
In an effort to, you know, sort of stay model agnostic, we deliberately implemented, we


00:37:12.400 --> 00:37:17.160
deliberately broke this process down into like smaller sub tasks, each with its own


00:37:17.160 --> 00:37:23.120
like prompt template in order to accommodate these models that may lack the same token


00:37:23.120 --> 00:37:26.040
size windows that other models have.


00:37:26.040 --> 00:37:32.320
I think just even for ones that have a large token spaces, I think they still, the more


00:37:32.320 --> 00:37:36.720
specific you can be, the more likely you're going to get a focused result instead of a


00:37:36.720 --> 00:37:38.720
wandering vague result.


00:37:38.720 --> 00:37:44.440
Teach me about math or teach me how to factor, you know, how to integrate this differential,


00:37:44.440 --> 00:37:47.440
solve this series of differential equations or physics.


00:37:47.440 --> 00:37:50.360
Like you're going to get a really different answer to those two questions.


00:37:50.360 --> 00:37:51.360
Yeah.


00:37:51.360 --> 00:37:55.400
That's also, so it goes back to like a topic I didn't, I did want to call out, but I don't,


00:37:55.400 --> 00:38:00.240
I don't think we hit on it is that the chat UI actually does support rendering in both


00:38:00.240 --> 00:38:04.520
Markdown and LaTeX, which is a markup language for math.


00:38:04.520 --> 00:38:10.240
So you can ask it both complex engineering and mathematical questions, like asking it


00:38:10.240 --> 00:38:13.760
to explain you like, yeah, so like there might be a demo here.


00:38:13.760 --> 00:38:16.280
I'm not sure if it's on this page though.


00:38:16.280 --> 00:38:22.120
So if I had a Fourier, fast Fourier transform in LaTeX and I put it in there and say, what


00:38:22.120 --> 00:38:25.520
is this, it'll say it's a fast Fourier transform or something like that.


00:38:25.520 --> 00:38:26.520
Yes.


00:38:26.520 --> 00:38:27.880
And it also works the other way around.


00:38:27.880 --> 00:38:34.000
You can also use it to say like, Hey, explain to me what the 2D Laplace equation is, or


00:38:34.000 --> 00:38:35.840
explain to me like what, what does this do?


00:38:35.840 --> 00:38:36.840
Right.


00:38:36.840 --> 00:38:42.640
And it will actually generate and format the equation inside the chat UI, which is really


00:38:42.640 --> 00:38:43.640
remarkable.


00:38:43.640 --> 00:38:44.640
I love that feature.


00:38:44.640 --> 00:38:45.720
It's actually really awesome.


00:38:45.720 --> 00:38:50.880
And it's also really appropriate for a scientific oriented thing like Jupiter, right?


00:38:50.880 --> 00:38:56.160
The remarkable thing is that because chat UBT and like other such language models, like


00:38:56.160 --> 00:39:01.600
the ones from Anthropic and AI21, because like they are founded on the premise of where


00:39:01.600 --> 00:39:06.560
like their functionality comes from having such a large corpus of data, they know a remarkable


00:39:06.560 --> 00:39:08.080
amount of information.


00:39:08.080 --> 00:39:13.440
So like we've tried like some example notebooks of quantum computing and explained those really


00:39:13.440 --> 00:39:14.440
well.


00:39:14.440 --> 00:39:19.320
We try, I tried one of like the Black-Scholes options pricing model, we use them in financial


00:39:19.320 --> 00:39:20.320
engineering.


00:39:20.320 --> 00:39:25.040
And it's really remarkable, like the utility that it offers just by being there in the


00:39:25.040 --> 00:39:26.040
side panel.


00:39:26.040 --> 00:39:30.960
Like you essentially have like a math wizard available to you in JupyterLab all the time.


00:39:30.960 --> 00:39:35.680
It's probably better than a lot of math professors in terms of not necessarily in the depth of


00:39:35.680 --> 00:39:41.560
one area, but you know, if you ask somebody who does like abstract algebra about real


00:39:41.560 --> 00:39:43.880
analysis, they're like, I don't really do that part.


00:39:43.880 --> 00:39:46.680
Or if you ask somebody about real analysis, about number theory, like I don't really,


00:39:46.680 --> 00:39:50.980
you can hit on all the areas, at least a generalist professor sort of thing.


00:39:50.980 --> 00:39:53.700
We talked about the slash learn command.


00:39:53.700 --> 00:39:56.680
That's pretty excellent already and where that's going.


00:39:56.680 --> 00:39:58.480
So I'm pretty excited about that.


00:39:58.480 --> 00:40:04.080
Yeah, it actually does have a lot of interesting technical tidbits to it, like the implementation.


00:40:04.080 --> 00:40:05.080
Okay.


00:40:05.080 --> 00:40:09.240
Yeah, actually, this is one of the really challenging things with these chat bots and


00:40:09.240 --> 00:40:10.240
things.


00:40:10.240 --> 00:40:14.320
For example, I've tried to ask ChatGPT, if I gave it one, just one of the transcripts


00:40:14.320 --> 00:40:17.120
from the show, I want to have a conversation about it.


00:40:17.120 --> 00:40:18.120
It's too much.


00:40:18.120 --> 00:40:19.120
I can't do it.


00:40:19.120 --> 00:40:24.440
It's just one, one show and in doc, like in your documentation, there might be a lot of


00:40:24.440 --> 00:40:25.800
files in there, right?


00:40:25.800 --> 00:40:28.400
More than just one transcript levels worth.


00:40:28.400 --> 00:40:32.920
So that alone, I think is kind of interesting just how to ingest that much data into it.


00:40:32.920 --> 00:40:33.920
Yeah.


00:40:33.920 --> 00:40:38.360
You know, this is a very interesting subject and it actually is a bit complex.


00:40:38.360 --> 00:40:39.360
I'm sure it is.


00:40:39.360 --> 00:40:41.760
I think there are some other features you want to discuss.


00:40:41.760 --> 00:40:43.040
Let's dive into this for just a minute.


00:40:43.040 --> 00:40:44.240
Because I think it is interesting.


00:40:44.240 --> 00:40:46.720
How do you make, because this makes it yours, right?


00:40:46.720 --> 00:40:50.580
It's one thing to ask vague, like, tell me about the Laplace equation and how does it


00:40:50.580 --> 00:40:51.880
apply to heat transfer?


00:40:51.880 --> 00:40:52.880
Like, okay, great.


00:40:52.880 --> 00:40:56.440
I have a specific problem with a specific library and I want to solve it.


00:40:56.440 --> 00:40:58.880
And you don't seem to understand about enough of it.


00:40:58.880 --> 00:41:03.000
So it really limits the usefulness if it doesn't, if it's not a little closer to what you're


00:41:03.000 --> 00:41:04.000
actually doing.


00:41:04.000 --> 00:41:05.000
And I think this brings it closer.


00:41:05.000 --> 00:41:06.440
So yeah, tell us about it.


00:41:06.440 --> 00:41:11.760
Language models aren't just governed by like their intelligence, however you measure that,


00:41:11.760 --> 00:41:12.760
right?


00:41:12.760 --> 00:41:14.920
They're governed by how much context they can take.


00:41:14.920 --> 00:41:19.720
So one of the reasons ChatGPT was so remarkable is that it had a great way of managing context


00:41:19.720 --> 00:41:21.560
through conversation history.


00:41:21.560 --> 00:41:28.080
And that like seemingly small, like leap and like seemingly small feature, like is what


00:41:28.080 --> 00:41:34.360
made ChatGPT so remarkably disruptive to this industry is because of that additional


00:41:34.360 --> 00:41:35.360
context.


00:41:35.360 --> 00:41:39.160
And we think about like extending that idea, like how do we give an AI more context, make


00:41:39.160 --> 00:41:42.480
it like even more human-like and personal?


00:41:42.480 --> 00:41:44.320
Well, the idea is similar.


00:41:44.320 --> 00:41:47.680
We add more context and that's what learning does, right?


00:41:47.680 --> 00:41:52.560
And so the way learning works is that we're actually using another set of models called


00:41:52.560 --> 00:41:54.200
embedding models.


00:41:54.200 --> 00:42:01.200
And embedding models are very, very underrated and the AI like modeling space, right?


00:42:01.200 --> 00:42:02.880
These are really remarkable things.


00:42:02.880 --> 00:42:07.480
And they have this one, I'll only cover like the most important characteristic of embedding


00:42:07.480 --> 00:42:15.440
models, which is embedding models take syntax and map it to a high dimensional vector space


00:42:15.440 --> 00:42:17.400
called a semantic space.


00:42:17.400 --> 00:42:23.360
And inside of the semantic space, nearby vectors indicate semantic similarity.


00:42:23.360 --> 00:42:26.600
I know that's like a lot of words, I'm going to break that idea down, right?


00:42:26.600 --> 00:42:30.680
So like canine and dog, let's take these two words as an example, right?


00:42:30.680 --> 00:42:32.440
These two words are completely different.


00:42:32.440 --> 00:42:36.760
They don't even share a single character in similarity together, right?


00:42:36.760 --> 00:42:39.520
They don't have a single letter in common with one another.


00:42:39.520 --> 00:42:45.080
And yet we know as humans that these two dogs, these two words mean the same thing.


00:42:45.080 --> 00:42:46.840
They refer to a dog.


00:42:46.840 --> 00:42:51.680
So like they have different syntax, but the same semantics, the same semantic meaning.


00:42:51.680 --> 00:42:52.840
So their vectors would be-


00:42:52.840 --> 00:42:53.840
Would be mapped close.


00:42:53.840 --> 00:42:56.440
Would be very close by whatever metric you're using, yeah.


00:42:56.440 --> 00:43:01.760
If you extend this idea and like you imagine, okay, what if you split a document?


00:43:01.760 --> 00:43:05.600
What if you split a file into like one to two sentence chunks?


00:43:05.600 --> 00:43:09.440
And then for each of these like sentence, let's just say sentences, for example.


00:43:09.440 --> 00:43:13.480
Let's say we split a document to sentences and then we take each of those sentences and


00:43:13.480 --> 00:43:18.040
then map, use an embedding model to compute their embedding and then store them inside


00:43:18.040 --> 00:43:19.400
of a vector store.


00:43:19.400 --> 00:43:24.360
Like basically like a local database that has, that just stores all of these vectors


00:43:24.360 --> 00:43:26.400
in like a file or something, right?


00:43:26.400 --> 00:43:32.360
Now imagine what happens if we then take a prompt, like a question that we might have,


00:43:32.360 --> 00:43:33.960
so that is an embedding.


00:43:33.960 --> 00:43:39.240
And then we say to the vector store, okay, for this prompt embedding, find me all of


00:43:39.240 --> 00:43:42.280
the other embeddings that are close to this.


00:43:42.280 --> 00:43:46.700
Well, what you've just done in this process is called a semantic search.


00:43:46.700 --> 00:43:50.920
So it's kind of like syntax search, except instead of searching based off of keywords


00:43:50.920 --> 00:43:57.020
or tokens or other syntactic traits, you are searching based off the actual natural language


00:43:57.020 --> 00:43:59.640
meaning of the word.


00:43:59.640 --> 00:44:04.200
This is much more applicable when it comes to like natural language prompts and natural


00:44:04.200 --> 00:44:09.640
language, like corpuses of data, because this is like the actual information that's being


00:44:09.640 --> 00:44:10.640
stored.


00:44:10.640 --> 00:44:11.640
Now we don't care about the characters.


00:44:11.640 --> 00:44:14.280
We care about the information that they represent, right?


00:44:14.280 --> 00:44:15.280
The essence of it.


00:44:15.280 --> 00:44:16.280
Yeah.


00:44:16.280 --> 00:44:18.840
And these vectors are computed by the larger language model?


00:44:18.840 --> 00:44:23.120
The vectors, the embeddings are computed by an embedding model and they're actually a


00:44:23.120 --> 00:44:27.920
separate category of model that we have our own special APIs for.


00:44:27.920 --> 00:44:31.520
So in our settings panel, you can change the language model.


00:44:31.520 --> 00:44:33.440
And I think we already discussed that, right?


00:44:33.440 --> 00:44:34.440
Yeah.


00:44:34.440 --> 00:44:37.400
But what's interesting is that underneath that, you'll also see a section that says


00:44:37.400 --> 00:44:44.240
embedding model and you can change the embedding model to like, we also offer that same principle


00:44:44.240 --> 00:44:46.000
of model agnosticism there.


00:44:46.000 --> 00:44:47.000
Yeah.


00:44:47.000 --> 00:44:48.000
This is very interesting.


00:44:48.000 --> 00:44:49.000
Very interesting.


00:44:49.000 --> 00:44:50.000
Let's talk a little bit about the format.


00:44:50.000 --> 00:44:54.960
You said obviously that you can do LaTeX, which you say in, you say math, right?


00:44:54.960 --> 00:44:55.960
You tell us, give me math.


00:44:55.960 --> 00:44:56.960
Yeah.


00:44:56.960 --> 00:45:01.840
Which is, yeah, it's pretty interesting, but you can do images, markdown code, HTML,


00:45:01.840 --> 00:45:03.040
JSON, text.


00:45:03.040 --> 00:45:05.880
There's a lot of, a lot of different formats you can get the answer back in.


00:45:05.880 --> 00:45:12.280
When you use the AI magics, we can pass them to like a renderer first before we show it


00:45:12.280 --> 00:45:13.480
to output to the user.


00:45:13.480 --> 00:45:14.480
Yeah.


00:45:14.480 --> 00:45:19.720
And with the AI magic, the percent percent AI in the cell, you can also specify, that's


00:45:19.720 --> 00:45:24.520
where you put the format potentially, but you can also specify the model and the service,


00:45:24.520 --> 00:45:25.920
I guess, for the provider.


00:45:25.920 --> 00:45:31.520
The IPython magics are basically stateless in the sense that you always have to specify


00:45:31.520 --> 00:45:33.080
the model explicitly.


00:45:33.080 --> 00:45:36.960
They don't operate off the premise that you are using JupyterLab.


00:45:36.960 --> 00:45:40.840
They don't run off the premise that you have JupyterLab installed or are using the lab


00:45:40.840 --> 00:45:42.680
extension that we offer.


00:45:42.680 --> 00:45:46.880
Because of that, like the model is stated explicitly every time.


00:45:46.880 --> 00:45:47.880
That's by design.


00:45:47.880 --> 00:45:54.080
When you sat down, what is your Jupyter AI provider set to?


00:45:54.080 --> 00:45:55.160
What's your favorite?


00:45:55.160 --> 00:46:00.320
As a developer, I like literally pick a random one to give the most test coverage at all


00:46:00.320 --> 00:46:01.360
times.


00:46:01.360 --> 00:46:03.520
And that's actually a great way of finding bugs.


00:46:03.520 --> 00:46:05.520
So yeah, I don't have a favorite one.


00:46:05.520 --> 00:46:11.520
My favorite one is the one that works and hopefully that should be all of them.


00:46:11.520 --> 00:46:14.480
You can also tell it to forget what I was talking about.


00:46:14.480 --> 00:46:16.040
We're going to start over.


00:46:16.040 --> 00:46:19.080
That's pretty interesting that you can do that along the way because you maybe had a


00:46:19.080 --> 00:46:23.560
bunch of conversations and we talked about the benefit of it, like knowing the history


00:46:23.560 --> 00:46:26.280
of that conversation, but you're like, all right, new idea.


00:46:26.280 --> 00:46:27.280
Switching topics.


00:46:27.280 --> 00:46:28.280
Chat.


00:46:28.280 --> 00:46:32.560
I think the last one I wanted to talk about specifically was interpolating prompts.


00:46:32.560 --> 00:46:39.160
Kind of almost like f-strings where you can put in the prompt text, you can put a variable


00:46:39.160 --> 00:46:43.520
and then other parts of your notebook or program can set that value.


00:46:43.520 --> 00:46:44.520
Yeah.


00:46:44.520 --> 00:46:45.520
Tell us about this.


00:46:45.520 --> 00:46:48.520
You can define a variable in your IPython kernel, right?


00:46:48.520 --> 00:46:52.840
So like, and that's just dumb, but just like how you define any other variable.


00:46:52.840 --> 00:46:57.320
But what's interesting is that IPython is actually aware of the variables that you are


00:46:57.320 --> 00:46:58.320
defining.


00:46:58.320 --> 00:47:03.520
So we can programmatically access that when we implement the magic, right?


00:47:03.520 --> 00:47:08.160
Basically if you define any variable at a top level scope, like let's say a poet equals


00:47:08.160 --> 00:47:09.160
Walt Whitman, right?


00:47:09.160 --> 00:47:11.600
So we have a name variable called poet.


00:47:11.600 --> 00:47:18.080
And then you can send a prompt over like write a poem in the style of curly braces, poet


00:47:18.080 --> 00:47:19.440
and curly braces.


00:47:19.440 --> 00:47:26.160
And when that is run, the variable, the value of that variable is interpolated and substitutes


00:47:26.160 --> 00:47:27.400
around the curly braces.


00:47:27.400 --> 00:47:31.840
So the final prompt becomes write a poem in the style of Walt Whitman.


00:47:31.840 --> 00:47:36.000
And when that prompt is sent, well, you can imagine it generates a poem of Walt Whitman.


00:47:36.000 --> 00:47:37.000
Yeah.


00:47:37.000 --> 00:47:41.840
The variable interpolation that we offer in IPython is very useful for like very quick


00:47:41.840 --> 00:47:42.840
like debugging.


00:47:42.840 --> 00:47:47.560
So you can actually reference a cell in the notebook directly.


00:47:47.560 --> 00:47:51.760
I think a lot of people don't know this, but like in a Jupyter notebook, there's like the


00:47:51.760 --> 00:47:55.160
in and out indicators to the left of each cell.


00:47:55.160 --> 00:47:58.360
So it'll say like in.1, out.1, right?


00:47:58.360 --> 00:48:01.880
So those are actual variables and you can use them here too.


00:48:01.880 --> 00:48:08.040
So you can reference like debug, tell me why this code is failing, curly braces in.1.


00:48:08.040 --> 00:48:11.520
That's you just on the screen there, just scroll there.


00:48:11.520 --> 00:48:18.480
Imagine what this does, the in bracket 11 or what went wrong in out bracket 11 or yeah,


00:48:18.480 --> 00:48:19.480
something like this.


00:48:19.480 --> 00:48:20.480
Right.


00:48:20.480 --> 00:48:22.280
It's fine as long as you don't go and rerun that cell.


00:48:22.280 --> 00:48:23.280
Yeah.


00:48:23.280 --> 00:48:25.680
But like you said, this is not for long-term.


00:48:25.680 --> 00:48:30.840
You can make it independent of the order of execution just by like assigning whatever


00:48:30.840 --> 00:48:35.680
variable that is to whatever like content that is to a named variable.


00:48:35.680 --> 00:48:38.400
That way, no matter what order you run them in.


00:48:38.400 --> 00:48:43.120
People might be thinking when you describe this interpolation thing, just bracket, bracket,


00:48:43.120 --> 00:48:44.880
curly brace, curly brace.


00:48:44.880 --> 00:48:46.640
They're like, we already have that in Python.


00:48:46.640 --> 00:48:49.040
You just put an F in front of the string and so on.


00:48:49.040 --> 00:48:55.460
But this is in the message that goes to the AI cell magic, not straight Python, right?


00:48:55.460 --> 00:48:56.460
That's the relevance.


00:48:56.460 --> 00:48:57.800
That's why this is interesting, right?


00:48:57.800 --> 00:48:58.800
Yes.


00:48:58.800 --> 00:49:00.960
I guess it really comes down to the different models that you select.


00:49:00.960 --> 00:49:05.700
So you opt into this a little bit, or maybe you need to understand it that way, but talk


00:49:05.700 --> 00:49:07.240
to us a bit about privacy.


00:49:07.240 --> 00:49:11.080
If I select something and say, what does this do?


00:49:11.080 --> 00:49:12.080
What happens?


00:49:12.080 --> 00:49:17.520
Something important to emphasize here is that whenever you use a language model that's hosted


00:49:17.520 --> 00:49:21.640
by a third party, so like it's running on their servers, right?


00:49:21.640 --> 00:49:27.080
Regardless of whether this model is free or not, like the fact that you're sending data


00:49:27.080 --> 00:49:32.480
to a third party over the internet, like that's where the privacy and security concerns happen,


00:49:32.480 --> 00:49:33.480
right?


00:49:33.480 --> 00:49:37.760
So that happens whenever you're sending data across the wire over the internet.


00:49:37.760 --> 00:49:44.560
But we have some special safeguards in place here specifically to assuage fears of concerns


00:49:44.560 --> 00:49:49.400
over privacy and security that a lot of open source users have.


00:49:49.400 --> 00:49:55.080
And one of the important ideas here is that Jupyter AI is both transparent and traceable.


00:49:55.080 --> 00:50:01.440
So when we send a prompt to a language model, that's always captured in the server logs


00:50:01.440 --> 00:50:02.440
by default.


00:50:02.440 --> 00:50:03.560
So that's always being logged.


00:50:03.560 --> 00:50:05.840
That's always being captured.


00:50:05.840 --> 00:50:07.880
So it's always going to be traceable.


00:50:07.880 --> 00:50:10.000
There's no secret back channel.


00:50:10.000 --> 00:50:11.840
You tell people this is happening.


00:50:11.840 --> 00:50:12.840
Okay.


00:50:12.840 --> 00:50:13.840
Yeah.


00:50:13.840 --> 00:50:18.600
So if an operator needs to audit, like, oh, dang, let me check just to make sure nothing


00:50:18.600 --> 00:50:21.080
scary was sent over to OpenAI.


00:50:21.080 --> 00:50:27.280
Well, the operator can review the server logs and make sure that all usage is compliant


00:50:27.280 --> 00:50:31.120
with whatever privacy policy their company has.


00:50:31.120 --> 00:50:38.680
And Jupyter AI is also exclusively user driven, meaning that we will never by default send


00:50:38.680 --> 00:50:43.080
data to a language model, even if you selected one, right?


00:50:43.080 --> 00:50:48.920
Like, we will never send data to that language model until explicit action is done by the


00:50:48.920 --> 00:50:49.920
user.


00:50:49.920 --> 00:50:54.320
So in this case, like clicking the send button, clicking shift, enter and running to sell.


00:50:54.320 --> 00:50:57.560
Nothing is sent to language model or embedding model until that happens.


00:50:57.560 --> 00:50:58.920
That's really all you can do.


00:50:58.920 --> 00:50:59.920
That's sounds great.


00:50:59.920 --> 00:51:04.360
Because you don't control what happens once it hits OpenAI or Anthropic or whatever.


00:51:04.360 --> 00:51:06.480
That's why the transparency is so important.


00:51:06.480 --> 00:51:07.480
Right.


00:51:07.480 --> 00:51:09.400
And oh, I forgot to touch on traceability.


00:51:09.400 --> 00:51:12.960
So like with these AI generated cells, right?


00:51:12.960 --> 00:51:17.440
So like the output cells in the metadata, we indicate that with like the model that


00:51:17.440 --> 00:51:21.880
was used to generate an output cell, if it was if it comes from the Jupyter AI magic.


00:51:21.880 --> 00:51:26.880
So that way it's also like traceable, not just in the logs, but like in the actual files,


00:51:26.880 --> 00:51:28.680
metadata and stuff as well.


00:51:28.680 --> 00:51:29.680
That's cool.


00:51:29.680 --> 00:51:34.920
So it'd be real easy to say, have the cell magic and then you know, notebooks store their


00:51:34.920 --> 00:51:35.920
last output.


00:51:35.920 --> 00:51:39.040
Like if you upload them to GitHub, they'll, they'll keep their last output.


00:51:39.040 --> 00:51:43.880
Unless you say clear all cell outputs, depending on which model you have selected, you might


00:51:43.880 --> 00:51:46.480
not get the same output, not even close to the same output.


00:51:46.480 --> 00:51:49.280
So you might want to know like, well, how did you get that picture?


00:51:49.280 --> 00:51:52.160
Oh, I had Anthropic selected and not the other, right?


00:51:52.160 --> 00:51:53.560
Like that is really nice.


00:51:53.560 --> 00:51:58.320
I've actually used the server logs myself to debug like prompt templates, for instance.


00:51:58.320 --> 00:51:59.320
Right.


00:51:59.320 --> 00:52:04.720
Because what we show in the logs is the full prompt, like after applying our template,


00:52:04.720 --> 00:52:08.040
after applying like the edits, like that's what's actually shown.


00:52:08.040 --> 00:52:12.200
So that's really also really helpful for developers who need to debug what's happening.


00:52:12.200 --> 00:52:13.200
Yeah, of course.


00:52:13.200 --> 00:52:17.060
Or if you're a scientist and you're looking for reproducibility, I doubt there's much


00:52:17.060 --> 00:52:21.760
guaranteed reproducibility, even across the versions of the same model, but at least you


00:52:21.760 --> 00:52:25.280
are in the same ballpark, you know, at least I know what model it came from.


00:52:25.280 --> 00:52:29.840
You can set the temperature to zero, but it won't generate a very fun output.


00:52:29.840 --> 00:52:33.600
That is a workaround if you truly need a reproducibility.


00:52:33.600 --> 00:52:34.600
I suppose.


00:52:34.600 --> 00:52:35.600
Yeah.


00:52:35.600 --> 00:52:40.520
The temperature being the ability to tell it how creative do you want to be or how focused


00:52:40.520 --> 00:52:42.000
do you want the answer to be, right?


00:52:42.000 --> 00:52:47.680
It's a hyperparameter that basically governs the randomness, like how far away from the


00:52:47.680 --> 00:52:49.880
mean it's willing to deviate.


00:52:49.880 --> 00:52:52.480
Kind of, yeah, vaguely describable as creativity.


00:52:52.480 --> 00:52:53.480
Yeah.


00:52:53.480 --> 00:52:54.480
Yeah, I suppose.


00:52:54.480 --> 00:52:58.240
And if you're looking for privacy, the GPT for all might be a good option.


00:52:58.240 --> 00:52:59.240
Oh yeah, absolutely.


00:52:59.240 --> 00:53:00.240
For that, right?


00:53:00.240 --> 00:53:01.640
Because that's not going anywhere.


00:53:01.640 --> 00:53:02.640
Yeah.


00:53:02.640 --> 00:53:07.600
However, some of them do have a license restrictions and that's also why we have also taken it


00:53:07.600 --> 00:53:13.000
this slow when it comes to adding more GPT for all support is because different models


00:53:13.000 --> 00:53:17.280
are licensed differently and that's another consideration we have to take in mind.


00:53:17.280 --> 00:53:18.280
Yeah.


00:53:18.280 --> 00:53:19.280
Yeah, of course.


00:53:19.280 --> 00:53:23.160
You're playing in a crazy space with many different companies evolving licenses.


00:53:23.160 --> 00:53:24.160
Yeah.


00:53:24.160 --> 00:53:26.200
Let's close it out with one more thing here.


00:53:26.200 --> 00:53:27.200
Maybe two more things.


00:53:27.200 --> 00:53:33.000
One is there's a lot of interest in these agents and it sounds, for example, your create


00:53:33.000 --> 00:53:37.600
a notebook that does this sort of thing, like teach me about Matplotlib is a little bit


00:53:37.600 --> 00:53:44.480
agent driven thinking of like Lang chain and stuff like that, or even GPT engineer.


00:53:44.480 --> 00:53:45.480
What's the story?


00:53:45.480 --> 00:53:46.720
Do you have any integrations with that?


00:53:46.720 --> 00:53:47.960
Any of those types of things?


00:53:47.960 --> 00:53:52.760
We're working on one where basically you won't need to use slash commands anymore.


00:53:52.760 --> 00:53:57.320
So this is like, again, like we're just kind of playing around with this, seeing how well


00:53:57.320 --> 00:53:58.320
it behaves.


00:53:58.320 --> 00:54:02.960
But we are trying to use agents to, for example, remove the need to use slash commands.


00:54:02.960 --> 00:54:07.360
So when you say like generate a notebook, like it will just generate one.


00:54:07.360 --> 00:54:09.200
You don't have to know to slash command for that.


00:54:09.200 --> 00:54:10.760
Like it will just go.


00:54:10.760 --> 00:54:11.760
So like, yeah.


00:54:11.760 --> 00:54:16.520
However, we don't have any, we don't use agents at the present moment now.


00:54:16.520 --> 00:54:21.120
And the reason for that is that they have very, they're not very model agnostic, at


00:54:21.120 --> 00:54:26.640
least the ones from our research, like they only work well for a specific model and a


00:54:26.640 --> 00:54:29.520
specific prompt template, but beyond that, it's hard.


00:54:29.520 --> 00:54:30.520
All right.


00:54:30.520 --> 00:54:31.920
Last question running out of time here.


00:54:31.920 --> 00:54:34.600
What's your favorite thing you've done with Jupiter AI?


00:54:34.600 --> 00:54:38.720
Not like a feature, but what have you made it do to help you that you really like?


00:54:38.720 --> 00:54:41.360
Teach Jupiter AI about Jupiter AI.


00:54:41.360 --> 00:54:42.360
That's an easy one.


00:54:42.360 --> 00:54:44.200
So we have the documentation.


00:54:44.200 --> 00:54:45.200
What did it learn?


00:54:45.200 --> 00:54:47.400
Well, it learned about itself.


00:54:47.400 --> 00:54:52.400
So like I was at a conference and I, so this was at PI data and I actually got enough questions


00:54:52.400 --> 00:54:57.320
to the point where I did, I just downloaded, I had the documentation already available


00:54:57.320 --> 00:55:02.160
in my home directory because of some other previous or restructure tax them.


00:55:02.160 --> 00:55:03.160
Yeah.


00:55:03.160 --> 00:55:04.960
And the markdown source, the markdown source.


00:55:04.960 --> 00:55:05.960
Right.


00:55:05.960 --> 00:55:07.200
And then I just had slash learn.


00:55:07.200 --> 00:55:11.920
I just learned that doc and then I just had that laptop on the side and told people, like


00:55:11.920 --> 00:55:14.040
if you have any questions, try it.


00:55:14.040 --> 00:55:15.040
Just ask it.


00:55:15.040 --> 00:55:16.040
Yeah.


00:55:16.040 --> 00:55:17.800
In case, in case you don't want to wait in line.


00:55:17.800 --> 00:55:21.640
So yeah, that was yeah, it's, it's pretty remarkable.


00:55:21.640 --> 00:55:22.640
Yeah.


00:55:22.640 --> 00:55:26.360
The learn feature by far is definitely my favorite and it's the one I want to spend


00:55:26.360 --> 00:55:28.720
the most time developing and pushing.


00:55:28.720 --> 00:55:34.020
I think there's massive possibility there because I deeply understand what you're working


00:55:34.020 --> 00:55:36.360
on in a multifaceted way.


00:55:36.360 --> 00:55:37.440
What are the documentations?


00:55:37.440 --> 00:55:38.440
What is the code?


00:55:38.440 --> 00:55:39.440
What is the data?


00:55:39.440 --> 00:55:41.800
What is the network topology and servers I can work with?


00:55:41.800 --> 00:55:44.640
Like all of that kind of stuff or pick your specialty.


00:55:44.640 --> 00:55:45.640
Yup.


00:55:45.640 --> 00:55:48.360
And that's how we make AI more human right before it takes over.


00:55:48.360 --> 00:55:49.360
So no problem there.


00:55:49.360 --> 00:55:50.360
Oh yeah.


00:55:50.360 --> 00:55:51.360
Just kidding.


00:55:52.360 --> 00:55:53.360
Let's wrap it up.


00:55:53.360 --> 00:55:54.360
I think we're out of time, David.


00:55:54.360 --> 00:55:59.480
So final question, some notable PyPI package, maybe something awesome you discovered to


00:55:59.480 --> 00:56:01.500
help you write Jupyter AI.


00:56:01.500 --> 00:56:02.960
Anything you want to give a shout out to?


00:56:02.960 --> 00:56:07.080
We definitely use LangChain a lot and that it's a pretty, it has some pretty fantastic


00:56:07.080 --> 00:56:11.240
integrations and we're actually built on top of LangChain really.


00:56:11.240 --> 00:56:12.240
But also Dask.


00:56:12.240 --> 00:56:13.240
Dask is really nice.


00:56:13.240 --> 00:56:14.240
Yeah.


00:56:14.240 --> 00:56:18.480
It has some great visualization capabilities for when you're doing parallel compute.


00:56:18.480 --> 00:56:22.360
Like it has a great dashboard that's also available in JupyterLab.


00:56:22.360 --> 00:56:23.360
Yeah.


00:56:23.360 --> 00:56:26.840
That shows it running and distributing the work in JupyterLab is amazing.


00:56:26.840 --> 00:56:32.720
And one of the contributors also reached out once he had heard I was integrating Dask into


00:56:32.720 --> 00:56:33.720
Jupyter AI.


00:56:33.720 --> 00:56:38.360
He actually reached out to help us and offer like direct one-on-one guidance with using


00:56:38.360 --> 00:56:39.360
Dask.


00:56:39.360 --> 00:56:42.400
And yeah, it's just been a fantastic experience using Dask.


00:56:42.400 --> 00:56:43.400
Yeah.


00:56:43.400 --> 00:56:44.400
I have no complaints.


00:56:44.400 --> 00:56:49.720
It's, it's just pretty awesome that somebody has finally made parallel and distributed


00:56:49.720 --> 00:56:51.760
compute better in Python.


00:56:51.760 --> 00:56:52.760
For sure.


00:56:52.760 --> 00:56:53.760
Yeah.


00:56:54.760 --> 00:56:55.760
Dask is cool.


00:56:55.760 --> 00:56:56.760
Dask is very cool.


00:56:56.760 --> 00:56:57.760
All right.


00:56:57.760 --> 00:56:58.760
Well, final call to action.


00:56:58.760 --> 00:56:59.760
People want to get started with Jupyter AI.


00:56:59.760 --> 00:57:00.760
What do you tell them?


00:57:00.760 --> 00:57:01.760
Install Jupyter AI or Conda.


00:57:01.760 --> 00:57:02.760
Conda install works too.


00:57:02.760 --> 00:57:03.760
It's on both.


00:57:03.760 --> 00:57:04.760
Awesome.


00:57:04.760 --> 00:57:05.760
Well, really good work.


00:57:05.760 --> 00:57:07.200
This is super interesting and I think a lot of people are going to find value in it.


00:57:07.200 --> 00:57:11.480
So I can see some nice comments in the audience that people are excited as well.


00:57:11.480 --> 00:57:12.480
So thanks for being here.


00:57:12.480 --> 00:57:13.480
Yeah.


00:57:13.480 --> 00:57:14.480
Thank you so much.


00:57:14.480 --> 00:57:15.480
Yeah.


00:57:15.480 --> 00:57:16.480
See you later.


00:57:16.480 --> 00:57:17.480
Bye bye.


00:57:17.480 --> 00:57:18.480
Bye.


00:57:18.480 --> 00:57:19.480
This has been another episode of Talk Python to Me.


00:57:19.480 --> 00:57:20.480
Thank you to our sponsors.


00:57:20.480 --> 00:57:22.040
Be sure to check out what they're offering.


00:57:22.040 --> 00:57:25.480
It really helps support the show.


00:57:25.480 --> 00:57:29.840
This episode is sponsored by Posit Connect from the makers of Shiny.


00:57:29.840 --> 00:57:34.120
Publish, share and deploy all of your data projects that you're creating using Python.


00:57:34.120 --> 00:57:40.840
Streamlet, Dash, Shiny, Bokeh, FastAPI, Flask, Quattro, Reports, Dashboards and APIs.


00:57:40.840 --> 00:57:42.760
Posit Connect supports all of them.


00:57:42.760 --> 00:57:46.760
Try Posit Connect for free by going to talkpython.fm/posit.


00:57:46.760 --> 00:57:47.760
B-O-S-I-T.


00:57:47.760 --> 00:57:51.520
Want to level up your Python?


00:57:51.520 --> 00:57:55.600
We have one of the largest catalogs of Python video courses over at Talk Python.


00:57:55.600 --> 00:58:00.680
Our content ranges from true beginners to deeply advanced topics like memory and async.


00:58:00.680 --> 00:58:03.380
And best of all, there's not a subscription in sight.


00:58:03.380 --> 00:58:06.500
Check it out for yourself at training.talkpython.fm.


00:58:06.500 --> 00:58:08.280
Be sure to subscribe to the show.


00:58:08.280 --> 00:58:11.120
Open your favorite podcast app and search for Python.


00:58:11.120 --> 00:58:12.480
We should be right at the top.


00:58:12.480 --> 00:58:18.040
You can also find the iTunes feed at /iTunes, the Google Play feed at /play, and the Direct


00:58:18.040 --> 00:58:22.080
RSS feed at /rss on talkpython.fm.


00:58:22.080 --> 00:58:24.640
We're live streaming most of our recordings these days.


00:58:24.640 --> 00:58:28.200
If you want to be part of the show and have your comments featured on the air, be sure


00:58:28.200 --> 00:58:33.160
to subscribe to our YouTube channel at talkpython.fm/youtube.


00:58:33.160 --> 00:58:34.440
This is your host, Michael Kennedy.


00:58:34.440 --> 00:58:35.600
Thanks so much for listening.


00:58:35.600 --> 00:58:36.840
I really appreciate it.


00:58:36.840 --> 00:58:38.560
Now get out there and write some Python code.


00:58:39.520 --> 00:58:42.560
[MUSIC PLAYING]


00:58:42.560 --> 00:58:46.560
[MUSIC PLAYING]


00:58:55.560 --> 00:58:58.140
(upbeat music)


00:58:58.140 --> 00:59:07.320
[BLANK_AUDIO]

