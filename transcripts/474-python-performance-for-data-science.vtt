WEBVTT

00:00:00.001 --> 00:00:05.200
Python performance has come a long way in recent times, and it's often the data scientists,

00:00:05.200 --> 00:00:10.480
with their computational algorithms and large quantities of data, who care the most about

00:00:10.480 --> 00:00:11.760
this form of performance.

00:00:11.760 --> 00:00:16.580
It's great to have Stan Siebert back on the show to talk about Python's performance for

00:00:16.580 --> 00:00:17.300
data scientists.

00:00:17.300 --> 00:00:22.400
We cover a wide range of tools and techniques that will be valuable for many Python developers

00:00:22.400 --> 00:00:23.380
and data scientists.

00:00:23.380 --> 00:00:29.320
This is Talk Python To Me, episode 474, recorded July 18th, 2024.

00:00:30.320 --> 00:00:31.820
Are you ready for your host?

00:00:31.820 --> 00:00:32.660
Here he is.

00:00:32.660 --> 00:00:36.100
You're listening to Michael Kennedy on Talk Python To Me.

00:00:36.100 --> 00:00:39.780
Live from Portland, Oregon, and this segment was made with Python.

00:00:39.780 --> 00:00:45.860
Welcome to Talk Python To Me, a weekly podcast on Python.

00:00:45.860 --> 00:00:48.080
This is your host, Michael Kennedy.

00:00:48.080 --> 00:00:53.440
Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython,

00:00:53.440 --> 00:00:56.440
both accounts over at fosstodon.org.

00:00:56.780 --> 00:01:01.360
And keep up with the show and listen to over nine years of episodes at talkpython.fm.

00:01:01.360 --> 00:01:05.940
If you want to be part of our live episodes, you can find the live streams over on YouTube.

00:01:05.940 --> 00:01:11.680
Subscribe to our YouTube channel over at talkpython.fm/youtube and get notified about upcoming

00:01:11.680 --> 00:01:12.160
shows.

00:01:12.160 --> 00:01:16.320
This episode is sponsored by Posit Connect from the makers of Shiny.

00:01:16.320 --> 00:01:20.820
Publish, share, and deploy all of your data projects that you're creating using Python.

00:01:20.820 --> 00:01:27.300
Streamlit, Dash, Shiny, Bokeh, FastAPI, Flask, Quattro, Reports, Dashboards, and APIs.

00:01:27.300 --> 00:01:29.700
Posit Connect supports all of them.

00:01:29.700 --> 00:01:35.360
Try Posit Connect for free by going to talkpython.fm/posit, P-O-S-I-T.

00:01:35.820 --> 00:01:39.760
And it's also brought to you by us over at Talk Python Training.

00:01:39.760 --> 00:01:44.380
Did you know that we have over 250 hours of Python courses?

00:01:44.380 --> 00:01:45.580
Yeah, that's right.

00:01:45.580 --> 00:01:48.160
Check them out at talkpython.fm/courses.

00:01:48.160 --> 00:01:49.200
Hey, Stan.

00:01:49.200 --> 00:01:49.680
Hello.

00:01:49.680 --> 00:01:50.360
Hello, hello.

00:01:50.360 --> 00:01:51.760
Welcome back to Talk Python To Me.

00:01:51.760 --> 00:01:52.760
I'm glad to be here.

00:01:52.760 --> 00:01:53.900
Glad to talk performance.

00:01:53.900 --> 00:01:54.480
I know.

00:01:54.480 --> 00:01:55.700
I'm excited to talk performance.

00:01:56.060 --> 00:02:00.460
It's one of those things I just never get tired of thinking about and focusing on.

00:02:00.460 --> 00:02:02.680
It's just so multifaceted.

00:02:02.680 --> 00:02:08.300
And as we will see, even for a language like Python that is not primarily performance focused,

00:02:08.300 --> 00:02:09.360
there's a lot to talk about.

00:02:09.360 --> 00:02:11.740
Yeah, there's endless bag of tricks.

00:02:11.740 --> 00:02:12.200
Yeah.

00:02:12.200 --> 00:02:16.060
And I would say, you know, sort of undercut my own comment there.

00:02:16.060 --> 00:02:21.600
Python is increasingly focusing on performance since like 3.10 or so, right?

00:02:21.600 --> 00:02:26.680
And the secret is that it's because Python integrates so well with other languages.

00:02:26.680 --> 00:02:29.460
It's sort of always cared about performance in some way.

00:02:29.460 --> 00:02:31.280
It's just sometimes you had to leave Python to do it.

00:02:31.280 --> 00:02:33.920
But you've still got to keep the Python interface.

00:02:33.920 --> 00:02:39.060
There's been such an easy, high-performance escape hatch that making Python itself faster

00:02:39.060 --> 00:02:42.600
is obviously not unimportant, but maybe not the primary focus, right?

00:02:42.600 --> 00:02:45.140
Like usability, standard library, et cetera, et cetera.

00:02:45.140 --> 00:02:45.740
All right.

00:02:45.740 --> 00:02:51.560
For people who have not heard your previous episode, let's maybe just do a quick introduction.

00:02:51.960 --> 00:02:52.340
Who's Stan?

00:02:52.340 --> 00:02:52.680
Yeah.

00:02:52.680 --> 00:02:54.060
So I am Stan Siebert.

00:02:54.060 --> 00:02:59.720
I am a manager at Anaconda, well-known purveyor of Python packages and such.

00:02:59.720 --> 00:03:04.640
My day-to-day job is actually managing most of our open source developers at Anaconda.

00:03:04.640 --> 00:03:05.980
So that includes the Numba team.

00:03:05.980 --> 00:03:09.060
And so we'll be talking about Numba today, but other things like we have people working

00:03:09.060 --> 00:03:13.220
on Jupyter and Beware for Mobile Python and other projects like that.

00:03:13.220 --> 00:03:17.420
And so that's what I do mostly is focus on how do we have an impact on the open source

00:03:17.420 --> 00:03:22.080
community and what does Python need to sort of stay relevant and keep evolving?

00:03:22.080 --> 00:03:22.600
I love it.

00:03:22.600 --> 00:03:24.920
What a cool job as well, position in that company.

00:03:24.920 --> 00:03:25.840
Yeah, I'm really grateful.

00:03:25.840 --> 00:03:27.360
It's a rare position.

00:03:27.360 --> 00:03:29.160
So I'm really glad I've been able to do it for so long.

00:03:29.160 --> 00:03:29.440
Yeah.

00:03:29.440 --> 00:03:34.080
And I would also throw out in the list of things that you've given a shout out to, I would

00:03:34.080 --> 00:03:36.360
point out PyScript as well.

00:03:36.360 --> 00:03:37.220
Oh, yes, of course.

00:03:37.220 --> 00:03:37.380
Yeah.

00:03:37.420 --> 00:03:40.140
I just started managing the PyScript team again, actually.

00:03:40.140 --> 00:03:41.960
And so I forgot about that one too.

00:03:41.960 --> 00:03:42.580
Yes, PyScript.

00:03:42.580 --> 00:03:47.060
So Python in your web browser, Python everywhere, on your phone, in your browser, all the places.

00:03:47.060 --> 00:03:47.400
Yeah.

00:03:47.400 --> 00:03:50.480
I mean, it's a little bit out of left field compared to the other things that you all are

00:03:50.480 --> 00:03:54.340
working on, but it's also a super important piece, I think.

00:03:54.340 --> 00:03:56.320
So yeah, really cool.

00:03:56.320 --> 00:03:57.040
Really cool there.

00:03:57.040 --> 00:04:04.020
So I think we set the stage a bit, but maybe let's start with Numba.

00:04:04.020 --> 00:04:05.820
That one's been around for a while.

00:04:05.820 --> 00:04:07.300
Some people know about it.

00:04:07.460 --> 00:04:08.120
Others don't.

00:04:08.120 --> 00:04:11.680
You know, what is Numba and how do we make Python code faster with Numba?

00:04:11.680 --> 00:04:12.000
Yeah.

00:04:12.000 --> 00:04:15.560
So there have been a lot of Python compilation projects over the years.

00:04:15.560 --> 00:04:18.580
Again, Numba's very fortunate that it's now 12 years old.

00:04:18.580 --> 00:04:22.380
We've been doing it a long time, and I've been involved with it probably almost 10 of those

00:04:22.380 --> 00:04:22.840
years now.

00:04:22.840 --> 00:04:27.380
And Python, I think one of Numba's success points is trying to stay focused on an area where

00:04:27.380 --> 00:04:30.940
we can have a big impact, and that is trying to speed up numerical code.

00:04:30.940 --> 00:04:36.260
So there's a lot of, again, in data science and other sciences, there's a lot of need to

00:04:36.260 --> 00:04:38.200
write custom algorithms that do math.

00:04:38.200 --> 00:04:41.880
And Numba's sweet spot is really helping you to speed those up.

00:04:41.880 --> 00:04:46.240
So we see Numba used in a lot of places where maybe the algorithm you're looking for isn't

00:04:46.240 --> 00:04:48.840
already in NumPy or already in JAX or something like that.

00:04:48.840 --> 00:04:50.360
You need to do something new.

00:04:50.360 --> 00:04:54.180
Projects like UMAP, which do really novel sort of clustering algorithms.

00:04:54.640 --> 00:04:59.680
Or I just at SciPy, I learned more about a project called Stumpy, which is for time series analysis.

00:04:59.680 --> 00:05:04.720
Those authors were able to use Numba to take the numerical core of that project that was

00:05:04.720 --> 00:05:08.160
the sort of the time bottleneck and speed it up without having to leave Python.

00:05:08.160 --> 00:05:12.660
And so that is, I think, really where Numba's most effective.

00:05:12.660 --> 00:05:13.060
Sure.

00:05:13.060 --> 00:05:19.500
If you look at a lot of programs, there might be 5,000 lines of code or more, but even just

00:05:19.500 --> 00:05:22.000
something only as big as 5,000 lines.

00:05:22.280 --> 00:05:26.060
There's a lot of code, but only a little bit of it really actually matters, right?

00:05:26.460 --> 00:05:30.840
Yeah, that's what we find a lot is when you sit down and measure your code, you'll spot

00:05:30.840 --> 00:05:36.300
some hotspots where 60 or 70 or 80% of your time is spent in just like three functions or

00:05:36.300 --> 00:05:36.600
something.

00:05:36.600 --> 00:05:38.060
And that's great.

00:05:38.060 --> 00:05:42.320
And if that's the case, that's great because you can just zero in on that section for speeding

00:05:42.320 --> 00:05:45.680
things up and not ruin the readability of the rest of your program.

00:05:45.680 --> 00:05:49.120
Sometimes optimization can make it harder to read the result.

00:05:49.120 --> 00:05:52.400
And so there's always a balance of you have to keep maintaining this project.

00:05:52.500 --> 00:05:55.800
You don't want to make it unreadable just to get 5% more speed.

00:05:55.800 --> 00:05:56.700
Yeah, absolutely.

00:05:56.700 --> 00:06:02.740
Not just the readability, but the ability to evolve it over time, right?

00:06:02.740 --> 00:06:09.260
So maybe it's like, oh, we're going to compile this section here using Numba or Cython or something

00:06:09.260 --> 00:06:09.620
like that.

00:06:09.620 --> 00:06:15.280
Well, maybe I was going to use this cool new IPI package I found, but I can't just jam it

00:06:15.280 --> 00:06:16.320
in there where it's compiled.

00:06:16.480 --> 00:06:19.020
That's unlikely to work well, right?

00:06:19.020 --> 00:06:20.060
And things like that.

00:06:20.060 --> 00:06:24.420
And so, yeah, a lot of times there's these big sections that look complicated.

00:06:24.420 --> 00:06:25.200
They look slow.

00:06:25.200 --> 00:06:26.400
They're not actually.

00:06:26.400 --> 00:06:26.700
Yeah.

00:06:26.700 --> 00:06:31.360
And one thing I also often emphasize for people is that when you think about the time your

00:06:31.360 --> 00:06:35.020
program takes, think about the time you spent working on it as well as the time you spent

00:06:35.020 --> 00:06:35.600
running it.

00:06:35.600 --> 00:06:40.640
And so because we've heard from a lot of projects who said they were able to get major speed

00:06:40.640 --> 00:06:45.500
ups, not because necessarily because Numba compiled their code to be incredibly fast,

00:06:45.500 --> 00:06:49.300
but it compiled it to be fast enough that they could try new ideas quicker.

00:06:49.300 --> 00:06:54.600
And so they got to the real win, which was a better way to solve their problem because

00:06:54.600 --> 00:06:58.540
they weren't kind of mired in just kind of boilerplate coding for so long.

00:06:58.540 --> 00:06:59.280
Right, right, right.

00:06:59.280 --> 00:07:02.600
It turns out I learned I should use a dictionary and not a list.

00:07:02.600 --> 00:07:04.360
And now it's a hundred times faster.

00:07:04.360 --> 00:07:07.000
And that wasn't actually a compiling thing.

00:07:07.000 --> 00:07:09.180
That was a visibility thing or something, right?

00:07:09.180 --> 00:07:09.440
Yeah.

00:07:09.440 --> 00:07:11.560
Try more things is always helpful.

00:07:11.560 --> 00:07:14.480
And so something that a tool that lets you do that is really valuable.

00:07:14.480 --> 00:07:15.300
A hundred percent.

00:07:15.300 --> 00:07:18.080
So what tools do you recommend for knowing?

00:07:18.080 --> 00:07:23.320
Because our human intuition sometimes is good, but sometimes is really off the mark in terms

00:07:23.320 --> 00:07:25.500
of thinking about what parts are slow, what parts are fast.

00:07:25.500 --> 00:07:26.500
That's something I definitely want.

00:07:26.500 --> 00:07:27.220
I've talked to people.

00:07:27.220 --> 00:07:31.380
Everyone thinks they know where the heart, the slow part is, but sometimes they're surprised.

00:07:31.380 --> 00:07:34.360
And so you definitely before you do anything, it does.

00:07:34.360 --> 00:07:35.580
This is not just number advice.

00:07:35.580 --> 00:07:38.720
This is any time before you're going to speed up your program, measure something.

00:07:38.920 --> 00:07:43.480
So what you want is you want a representative benchmark, something that's not going to run

00:07:43.480 --> 00:07:48.600
too fast because often, you know, like unit tests run too quickly to really tell you to

00:07:48.600 --> 00:07:50.260
exercise the program in a realistic way.

00:07:50.260 --> 00:07:53.720
So you want a benchmark that doesn't run too long, but maybe like five minutes or something.

00:07:53.720 --> 00:07:57.460
And then you're going to want to run that through a profiling tool.

00:07:57.460 --> 00:07:58.860
And there are several options.

00:07:58.860 --> 00:08:00.760
I just usually tell people to use C profile.

00:08:00.760 --> 00:08:02.940
It's built into the standard library in Python.

00:08:03.400 --> 00:08:04.080
It's a great tool.

00:08:04.080 --> 00:08:05.420
It does the job for most stuff.

00:08:05.420 --> 00:08:09.100
And so sometimes you may, there may be other tools, things like snake vis and other things

00:08:09.100 --> 00:08:10.800
to help you interpret the result of the profile.

00:08:10.800 --> 00:08:13.480
But often you'll use C profile to collect the data.

00:08:13.480 --> 00:08:19.380
And what this does is it samples, it sort of records as the program is running, what are

00:08:19.380 --> 00:08:23.380
all the functions that are being called and how much time are they taking?

00:08:23.380 --> 00:08:27.420
And there are different strategies for how to do this, but fundamentally what you get out

00:08:27.420 --> 00:08:32.960
is a essentially a data set that says, you know, 2% of the time in your, in your, in

00:08:32.960 --> 00:08:35.900
your program, this function was running and 3% of this function was running.

00:08:35.900 --> 00:08:40.840
And you can just sort that in descending order and look and see where, what pops out at the

00:08:40.840 --> 00:08:41.100
top.

00:08:41.100 --> 00:08:42.540
And sometimes you're surprised.

00:08:42.540 --> 00:08:45.420
Sometimes you find out it's actually, it wasn't my numerical code.

00:08:45.420 --> 00:08:49.360
It's that I spent, you know, 80% of my time doing some string operation that I didn't

00:08:49.360 --> 00:08:51.080
realize it needed to do over and over again.

00:08:51.080 --> 00:08:52.200
Right, right.

00:08:52.260 --> 00:08:52.620
Exactly.

00:08:52.620 --> 00:08:58.020
Some weird plus equals with a string was just creating a thousand strings to get to

00:08:58.020 --> 00:08:59.420
the end point or something like that.

00:08:59.420 --> 00:08:59.580
Yeah.

00:08:59.580 --> 00:08:59.920
Yeah.

00:08:59.920 --> 00:09:01.440
And it could have just done that once upfront.

00:09:01.440 --> 00:09:06.360
It's good to do the profiling just to make sure there isn't an obvious problem before you

00:09:06.360 --> 00:09:08.180
get into the more detailed optimization.

00:09:08.180 --> 00:09:08.740
Yeah.

00:09:08.740 --> 00:09:13.380
Before you start changing your code completely, it's execution method or whatever.

00:09:13.380 --> 00:09:13.780
Yep.

00:09:13.780 --> 00:09:14.180
Yeah.

00:09:14.180 --> 00:09:14.460
Yeah.

00:09:14.460 --> 00:09:16.360
And, you know, shout out to the PyCharm folks.

00:09:16.360 --> 00:09:20.060
They've got to push the button to profile and they've got to visualize it and they just run

00:09:20.060 --> 00:09:21.200
C profile right in there.

00:09:21.200 --> 00:09:24.000
So that's like C profile on easy mode.

00:09:24.000 --> 00:09:25.840
You know, you get a spreadsheet and you get a graph.

00:09:25.840 --> 00:09:29.840
What about other ones like Bill, FIL or anything else?

00:09:29.840 --> 00:09:30.960
Like any other recommendations?

00:09:30.960 --> 00:09:31.500
Yeah.

00:09:31.500 --> 00:09:35.860
So that's an interesting point is C profile is for compute time profiling.

00:09:35.860 --> 00:09:40.940
An interesting problem you run into is this tool does, which is data is memory profiling,

00:09:40.940 --> 00:09:43.940
which is often a problem when you're scaling up.

00:09:43.940 --> 00:09:46.920
And that's actually one of the other good things to keep in mind when you're optimizing is what

00:09:46.920 --> 00:09:47.500
am I trying to do?

00:09:47.500 --> 00:09:48.900
Am I trying to get done faster?

00:09:48.900 --> 00:09:51.240
Am I trying to save on compute costs?

00:09:51.240 --> 00:09:52.500
Am I trying to go bigger?

00:09:52.500 --> 00:09:55.340
And so I have to speed things up so that I have room to put more data in.

00:09:55.340 --> 00:09:57.820
If that's where you're going, you might want to be out of memory.

00:09:57.820 --> 00:09:58.640
Right.

00:09:58.640 --> 00:09:59.520
Can I just not do?

00:09:59.520 --> 00:09:59.920
Yeah.

00:09:59.920 --> 00:10:01.960
Or is that am I already stuck?

00:10:01.960 --> 00:10:07.420
And so there it is very easy in Python to not recognize when you have temporary arrays and

00:10:07.420 --> 00:10:07.760
things.

00:10:07.840 --> 00:10:11.040
Because again, it's also very compact and you're not seeing what's getting allocated.

00:10:11.040 --> 00:10:13.740
You can accidentally blow up your memory quite a lot.

00:10:13.740 --> 00:10:17.740
And so this kind of a profiler is a great option for it.

00:10:17.740 --> 00:10:20.800
What it can often show you is they'll kind of a line by line.

00:10:20.800 --> 00:10:25.320
This is, you know, how much memory was allocated in each line of your program.

00:10:25.320 --> 00:10:27.120
So you can see, oh, that one line of pandas.

00:10:27.120 --> 00:10:27.500
Oops.

00:10:27.500 --> 00:10:29.640
That that did it.

00:10:29.760 --> 00:10:31.440
Yeah, I can't remember all the details.

00:10:31.440 --> 00:10:36.020
I talked to it more about this one, but I feel like it also keeps track of the memory

00:10:36.020 --> 00:10:41.240
used even down into like NumPy and below, right?

00:10:41.240 --> 00:10:46.260
Not just Python memory where it says now there's some opaque blob of data science stuff.

00:10:46.260 --> 00:10:46.600
Yeah.

00:10:46.600 --> 00:10:49.060
And actually, even on the compute product, there's sort of two approaches.

00:10:49.060 --> 00:10:52.740
So C profile is focused on counting function time.

00:10:52.740 --> 00:10:54.460
But sometimes you have a long function.

00:10:54.460 --> 00:10:59.100
And if you're making a bunch of NumPy calls, you might actually care line by line how much time

00:10:59.100 --> 00:10:59.720
is being taken.

00:11:00.020 --> 00:11:01.560
And that can be a better way to think about it.

00:11:01.560 --> 00:11:03.500
And so I think the tool is called LineProf.

00:11:03.500 --> 00:11:03.980
Yeah.

00:11:03.980 --> 00:11:10.900
I forget the exact URL, but it's an excellent tool in Python for there's one in R and there's

00:11:10.900 --> 00:11:11.740
an equivalent one.

00:11:11.740 --> 00:11:12.460
Yes.

00:11:12.460 --> 00:11:14.580
Robert, Robert, LineProfiler.

00:11:14.580 --> 00:11:15.060
There you go.

00:11:15.060 --> 00:11:16.020
LineProfiler.

00:11:16.020 --> 00:11:18.700
Oh, it's archived, but still can be used.

00:11:18.700 --> 00:11:19.300
Yeah.

00:11:19.300 --> 00:11:20.760
I have to find another tool now.

00:11:20.760 --> 00:11:22.300
This is my go-to for so long.

00:11:22.300 --> 00:11:24.760
I didn't realize it had already been archived.

00:11:24.760 --> 00:11:25.560
Oh, there's a copy.

00:11:25.560 --> 00:11:26.200
Hey, but it still works.

00:11:26.200 --> 00:11:26.740
It's all good.

00:11:26.740 --> 00:11:27.480
It's all good.

00:11:27.480 --> 00:11:28.900
It's been transferred to a new location.

00:11:29.060 --> 00:11:29.980
So that's where it lives now.

00:11:29.980 --> 00:11:32.260
But yeah, line profiling is another.

00:11:32.260 --> 00:11:34.400
I often use them complimentary sort of tools.

00:11:34.400 --> 00:11:39.200
As I zero in on one function that's with C profile, and then I'll go line profile that

00:11:39.200 --> 00:11:39.640
function.

00:11:39.640 --> 00:11:40.320
Oh, interesting.

00:11:40.320 --> 00:11:40.660
Yeah.

00:11:40.660 --> 00:11:40.900
Okay.

00:11:40.900 --> 00:11:41.480
Drilling further.

00:11:41.780 --> 00:11:41.920
Yeah.

00:11:41.920 --> 00:11:42.200
Okay.

00:11:42.200 --> 00:11:42.480
Okay.

00:11:42.480 --> 00:11:43.660
This is the general area.

00:11:43.660 --> 00:11:45.020
Now let's really focus on it.

00:11:45.020 --> 00:11:46.340
Memory is another one.

00:11:46.340 --> 00:11:48.540
I talked to the folks from Bloomberg about that.

00:11:48.540 --> 00:11:49.440
Oh, okay.

00:11:49.440 --> 00:11:50.600
I have not used this one.

00:11:50.600 --> 00:11:50.920
Yeah.

00:11:50.920 --> 00:11:51.880
This is a pretty new one.

00:11:51.880 --> 00:11:53.900
And it's quite neat the way it works.

00:11:54.260 --> 00:11:54.640
Yeah.

00:11:54.640 --> 00:12:01.340
This one actually tracks C and C++ and other aspects of allocations as well.

00:12:01.340 --> 00:12:06.740
So one of the problems you can run into with profiling is especially memory profiling, I think.

00:12:06.740 --> 00:12:10.940
Although if you just want to know about memory, but the more you monitor it, the more it becomes

00:12:10.940 --> 00:12:14.120
kind of a Heisenberg quantum mechanics type thing.

00:12:14.120 --> 00:12:15.800
Once you observe it, you change it.

00:12:16.240 --> 00:12:19.580
And so the answers you get by observing it are not actually what are happening.

00:12:19.580 --> 00:12:23.380
So you got to keep a little bit of an open mind towards that as well, right?

00:12:23.380 --> 00:12:23.720
Yeah.

00:12:23.720 --> 00:12:28.600
And that's even a risk with, you know, other the compute side of the profiling is some you're

00:12:28.600 --> 00:12:32.760
using some compute time to actually observe the program, which means that it can.

00:12:32.760 --> 00:12:37.360
And these tools try to subtract out that bias, but it does impact things.

00:12:37.360 --> 00:12:43.020
And so you may want to have kind of a benchmark that you can run as your kind of real source of

00:12:43.020 --> 00:12:47.840
truth that you run without the profiler turned on just to see a final runtime run with the

00:12:47.840 --> 00:12:48.820
profiler to break it down.

00:12:48.820 --> 00:12:51.800
And then when you're all done, you're going to want to run that program again with the profiler

00:12:51.800 --> 00:12:55.800
off to see if you've actually improved it while clock time wise.

00:12:55.800 --> 00:12:56.200
Yeah.

00:12:56.200 --> 00:12:56.880
Yeah, absolutely.

00:12:56.880 --> 00:12:57.660
That's a really good point.

00:12:57.660 --> 00:13:01.420
It's just maybe do a percent time it type of thing or something along those lines.

00:13:01.420 --> 00:13:02.600
Okay.

00:13:02.600 --> 00:13:07.860
That was a little bit of a side deep dive into profiling because you, before you apply

00:13:07.860 --> 00:13:12.740
some of these techniques like Numba and others, you certainly want to know where to

00:13:12.740 --> 00:13:13.320
apply it.

00:13:13.320 --> 00:13:19.360
And part of that is you might need to rewrite your code a little bit to make it more optimizable

00:13:19.360 --> 00:13:21.300
by Numba or these things.

00:13:21.300 --> 00:13:24.500
So first of all, like what do you do to use Numba, right?

00:13:24.500 --> 00:13:27.220
It's just, you just put a decorator on there and have you go.

00:13:27.220 --> 00:13:30.920
At the very simplest level, Numba's interface is supposed to be just one decorator.

00:13:30.920 --> 00:13:34.580
Now there's some nuance, obviously, and other things you can do, but we tried to get it down

00:13:34.580 --> 00:13:36.920
to for most people, it's just that.

00:13:37.360 --> 00:13:42.260
And the N in NGIT means no Python, meaning we get, this code is not calling the Python

00:13:42.260 --> 00:13:43.620
interpreter anymore at all.

00:13:43.620 --> 00:13:46.640
It is purely machine code, no interpreter access.

00:13:46.640 --> 00:13:47.160
Interesting.

00:13:47.160 --> 00:13:47.540
Okay.

00:13:47.540 --> 00:13:53.600
So some of these like compile, do a thing and compile your Python code to machine instructions.

00:13:53.600 --> 00:13:59.260
I feel like they still interact with like Py object pointers and they still kind of work

00:13:59.260 --> 00:14:02.500
with the API of the Python data types.

00:14:02.500 --> 00:14:03.020
Yeah.

00:14:03.080 --> 00:14:08.300
Which is nice, but it's, it's, it's, it's a whole, whole lot slower of an optimization

00:14:08.300 --> 00:14:13.940
than now it's in 32 and it's, you know, float 32 and these are on registers, you know?

00:14:13.940 --> 00:14:14.240
Yeah.

00:14:14.240 --> 00:14:19.340
And this is part of the reason why Numba focuses on numerical code is that NumPy arrays and actually

00:14:19.340 --> 00:14:22.860
other arrays and PyTorch and other things that support the buffer protocol.

00:14:22.860 --> 00:14:23.680
The API.

00:14:23.680 --> 00:14:26.960
So, so really when Numba compiles this, it compiles sort of two functions.

00:14:26.960 --> 00:14:32.000
One is a wrapper that handles the transition from the interpreter into no Python land, as we call

00:14:32.000 --> 00:14:35.200
it, and then there's the core function that is kind of like you could have written in C

00:14:35.200 --> 00:14:36.100
or Fortran or something.

00:14:36.100 --> 00:14:39.320
And that wrapper is actually doing all the Py object stuff.

00:14:39.320 --> 00:14:43.120
It's reaching in and saying, ah, this, this integer, I'm going to pull out the actual number

00:14:43.120 --> 00:14:47.940
and oh, this NumPy array, I'm going to reach in and grab the data pointer and pass those down

00:14:47.940 --> 00:14:50.560
into the core where the actual, all the math happens.

00:14:50.560 --> 00:14:54.220
So the only time you interact with the interpreter is really at the edge.

00:14:54.220 --> 00:14:57.240
And then once you get in there, you try not to touch it ever again.

00:14:57.240 --> 00:15:01.860
Now, Numba does have a feature that we added some years ago called an object mode block.

00:15:01.860 --> 00:15:05.960
Which lets you in the middle of your no Python code, go back and actually start talking to

00:15:05.960 --> 00:15:06.780
the interpreter again.

00:15:06.780 --> 00:15:07.080
Right.

00:15:07.080 --> 00:15:09.240
Maybe use a standard library feature or something.

00:15:09.240 --> 00:15:09.720
Yeah.

00:15:09.720 --> 00:15:13.180
The most common use we've seen is you like you want a progress bar to update or something

00:15:13.180 --> 00:15:15.320
that's not in your, you know, hot loop.

00:15:15.320 --> 00:15:18.280
You don't want, you don't want to be going back to the interpreter in something that's

00:15:18.280 --> 00:15:19.220
really performance critical.

00:15:19.220 --> 00:15:23.000
But inside of a function, you might have parts that are more or less, you know, one out

00:15:23.000 --> 00:15:23.840
of a million iterations.

00:15:23.840 --> 00:15:26.800
I want to go update the progress bar or something that's totally valid.

00:15:26.800 --> 00:15:28.340
And you can do that with Numba.

00:15:28.340 --> 00:15:30.900
There's a way to get back to the interpreter if you really need to.

00:15:31.020 --> 00:15:31.140
Okay.

00:15:31.140 --> 00:15:31.700
Yeah.

00:15:31.700 --> 00:15:37.160
And it says it takes and translates Python functions to optimize machine code at runtime,

00:15:37.160 --> 00:15:38.140
which is cool.

00:15:38.140 --> 00:15:39.880
So that makes deploying it super easy.

00:15:39.880 --> 00:15:42.760
And you don't have to have like compiled wheels for it and stuff.

00:15:42.760 --> 00:15:45.620
Using industry standard LLVM compilers.

00:15:45.620 --> 00:15:49.680
And then similar speeds to Cs in Fortran.

00:15:49.680 --> 00:15:50.040
Yeah.

00:15:50.040 --> 00:15:55.720
Which is awesome, but also has implications if I can speak.

00:15:55.720 --> 00:16:02.760
For example, when I came to Python, I was blown away that I could just have integers as big

00:16:02.760 --> 00:16:03.200
as I want.

00:16:03.200 --> 00:16:05.380
If I keep adding to them, they just get bigger and bigger.

00:16:05.380 --> 00:16:10.200
Like billions, bazillions of, you know, bits of accuracy.

00:16:10.200 --> 00:16:17.640
And I came from C++ and C# and where you explicitly said it's an N32, it's an N64,

00:16:17.640 --> 00:16:18.820
it's a double.

00:16:18.820 --> 00:16:21.380
And these all had ranges of valid numbers.

00:16:21.380 --> 00:16:23.420
And then you've got weird like wraparounds.

00:16:23.640 --> 00:16:26.180
Maybe you create an unsigned one so you can get a little bit bigger.

00:16:26.180 --> 00:16:34.700
I suspect that you may fall victim or be subjected to these types of limitations without realizing

00:16:34.700 --> 00:16:38.600
them in Python if you at NGIT it because you're back in that land, right?

00:16:38.600 --> 00:16:42.460
Or does it do you guys magic to allow us to have big?

00:16:42.460 --> 00:16:46.540
We do not handle the big integer, which is what you're describing as sort of that integer

00:16:46.540 --> 00:16:47.580
that can grow without bound.

00:16:47.580 --> 00:16:50.920
Because our target audience is very familiar with NumPy.

00:16:50.920 --> 00:16:55.820
NumPy looks at numbers sort of the way you're described from C++ and other languages.

00:16:55.820 --> 00:16:57.260
The D type and all that stuff, right?

00:16:57.260 --> 00:16:57.620
Yeah.

00:16:57.620 --> 00:17:02.260
NumPy arrays always have a fixed size integer and you get to pick what that is, but it has

00:17:02.260 --> 00:17:04.080
to be 8, 16, 32, 64.

00:17:04.080 --> 00:17:07.080
Some machines can handle bigger, but that it is fixed.

00:17:07.080 --> 00:17:12.360
And so once you've locked that in, you can't over, if you go too big, you'll just wrap around

00:17:12.360 --> 00:17:12.820
and overflow.

00:17:12.820 --> 00:17:13.380
Yeah.

00:17:13.380 --> 00:17:17.720
So that limitation is definitely present again in Numba, but fortunately NumPy users are already

00:17:17.720 --> 00:17:18.920
familiar with thinking that way.

00:17:19.100 --> 00:17:22.340
So it isn't an additional constraint on them too much.

00:17:24.300 --> 00:17:28.280
This portion of Talk Python To Me is brought to you by Posit, the makers of Shiny, formerly

00:17:28.280 --> 00:17:31.660
RStudio, and especially Shiny for Python.

00:17:31.660 --> 00:17:33.580
Let me ask you a question.

00:17:33.580 --> 00:17:35.280
Are you building awesome things?

00:17:35.280 --> 00:17:36.340
Of course you are.

00:17:36.340 --> 00:17:37.900
You're a developer or a data scientist.

00:17:37.900 --> 00:17:38.820
That's what we do.

00:17:38.820 --> 00:17:40.940
And you should check out Posit Connect.

00:17:41.500 --> 00:17:46.560
Posit Connect is a way for you to publish, share, and deploy all the data products that you're

00:17:46.560 --> 00:17:47.840
building using Python.

00:17:47.840 --> 00:17:51.020
People ask me the same question all the time.

00:17:51.020 --> 00:17:54.180
Michael, I have some cool data science project or notebook that I built.

00:17:54.180 --> 00:17:57.480
How do I share it with my users, stakeholders, teammates?

00:17:57.480 --> 00:18:02.280
Do I need to learn FastAPI or Flask or maybe Vue or React.js?

00:18:02.700 --> 00:18:03.520
Hold on now.

00:18:03.520 --> 00:18:07.520
Those are cool technologies, and I'm sure you'd benefit from them, but maybe stay focused

00:18:07.520 --> 00:18:08.280
on the data project.

00:18:08.280 --> 00:18:10.780
Let Posit Connect handle that side of things.

00:18:10.780 --> 00:18:15.500
With Posit Connect, you can rapidly and securely deploy the things you build in Python.

00:18:15.500 --> 00:18:21.940
Streamlit, Dash, Shiny, Bokeh, FastAPI, Flask, Quarto, ports, dashboards, and APIs.

00:18:21.940 --> 00:18:24.240
Posit Connect supports all of them.

00:18:24.640 --> 00:18:29.480
And Posit Connect comes with all the bells and whistles to satisfy IT and other enterprise

00:18:29.480 --> 00:18:30.080
requirements.

00:18:30.080 --> 00:18:34.360
Make deployment the easiest step in your workflow with Posit Connect.

00:18:34.360 --> 00:18:39.600
For a limited time, you can try Posit Connect for free for three months by going to talkpython.fm

00:18:39.600 --> 00:18:40.580
slash posit.

00:18:40.580 --> 00:18:44.220
That's talkpython.fm/P-O-S-I-T.

00:18:44.220 --> 00:18:46.100
The link is in your podcast player show notes.

00:18:46.100 --> 00:18:49.360
Thank you to the team at Posit for supporting Talk Python.

00:18:50.860 --> 00:18:55.340
And then one thing you said was that you should focus on using arrays.

00:18:55.340 --> 00:18:56.100
Yes.

00:18:56.100 --> 00:19:02.040
And that kind of data structures before you apply Numba JIT compilation to it.

00:19:02.040 --> 00:19:07.500
Does that mean list as in bracket bracket or these NumPy type vector things?

00:19:07.500 --> 00:19:09.960
We all have different definitions.

00:19:09.960 --> 00:19:10.920
Yes, that's true.

00:19:10.920 --> 00:19:11.260
What are an array?

00:19:11.260 --> 00:19:15.220
Generally, yeah, usually the go-to I talk to about is a NumPy array.

00:19:15.220 --> 00:19:17.040
So it has a shape.

00:19:17.040 --> 00:19:19.300
The nice thing, you know, NumPy arrays can be multidimensional.

00:19:19.500 --> 00:19:22.080
So you can represent a lot of complex data that way.

00:19:22.080 --> 00:19:25.260
But within an array, there's a fixed element size.

00:19:25.260 --> 00:19:27.200
That element could be a record.

00:19:27.200 --> 00:19:31.360
So if you want for every cell in your array to store maybe a set of numbers or a pair of

00:19:31.360 --> 00:19:34.160
numbers, you can do that with custom D types and things.

00:19:34.160 --> 00:19:35.700
And Numba will understand that.

00:19:35.700 --> 00:19:37.380
That's the ideal data structure.

00:19:37.380 --> 00:19:42.180
Numba does have, we added support a couple years ago for other data structures because

00:19:42.180 --> 00:19:45.380
the downside to a NumPy array is that it's fixed size.

00:19:45.380 --> 00:19:49.280
Once you make it, you can't append to it like you can a Python list.

00:19:49.740 --> 00:19:54.280
So Numba does have support for both what we call typed lists and typed dictionaries.

00:19:54.280 --> 00:20:00.200
So these are sort of special cases of lists and dictionaries in Python where the, in the

00:20:00.200 --> 00:20:02.580
case of a list, every element in the list has the same type.

00:20:02.580 --> 00:20:06.280
Or in the case of a dictionary, the keys are all the same type and the values are all the

00:20:06.280 --> 00:20:06.680
same type.

00:20:06.680 --> 00:20:07.000
Right.

00:20:07.080 --> 00:20:12.100
And those cover a lot of the cases where, you know, when users want to make things where

00:20:12.100 --> 00:20:15.080
they don't know how long it's going to be, you're going to append in the algorithm.

00:20:15.080 --> 00:20:18.960
A list is a much more natural thing than an umpire array where you might like over allocate

00:20:18.960 --> 00:20:19.820
or something that seems.

00:20:19.820 --> 00:20:25.480
And dictionaries, our dictionary implementation is basically taken straight from CPython's dictionary

00:20:25.480 --> 00:20:25.880
implementation.

00:20:26.080 --> 00:20:29.560
So it's very tuned and very fast in the same way CPython's is.

00:20:29.560 --> 00:20:33.920
We just had to modify it a little bit to add this type information, but it's really good

00:20:33.920 --> 00:20:36.560
for kind of look up random items kind of stuff.

00:20:36.560 --> 00:20:40.340
So those are available as additional data structures in addition to the array.

00:20:40.580 --> 00:20:45.080
To use those, I would say from number import and something like this.

00:20:45.080 --> 00:20:49.460
There are new type in the, in the docs, I'll show you, you can sort of import a typed list

00:20:49.460 --> 00:20:51.860
as a special class that you can create.

00:20:51.860 --> 00:20:56.440
The downside, by the way, is that, and the reason we have those and we don't just take

00:20:56.440 --> 00:21:01.440
historically number used to try and let you pass in a Python list is that wrapper function

00:21:01.440 --> 00:21:05.320
would have to go recursively through the list of list of lists of whatever you might have

00:21:05.320 --> 00:21:11.880
and pop out all of the elements into some format that wasn't all Py objects so that the no Python

00:21:11.880 --> 00:21:13.280
code could manipulate them quickly.

00:21:13.280 --> 00:21:17.720
And then how do you put it all back if you modify that sort of shadow data structure?

00:21:17.720 --> 00:21:21.800
And so what we realized is that was confusing people and actually added a lot of overhead

00:21:21.800 --> 00:21:23.340
and calling functions took too long.

00:21:23.340 --> 00:21:27.320
So we instead went up a level and said, okay, we're going to make a new kind of list that

00:21:27.320 --> 00:21:29.880
you at the interpreter level can opt into for your algorithm.

00:21:29.880 --> 00:21:35.020
And so accessing that list from Python is slower than a Python list, but accessing it from

00:21:35.020 --> 00:21:37.100
Numba is like a hundred times faster.

00:21:37.100 --> 00:21:43.700
So you kind of have to decide for the, while I'm in this mode, I'm optimizing for numbers

00:21:43.700 --> 00:21:46.360
performance, not for the Python interpreter performance.

00:21:46.360 --> 00:21:50.020
Which is reasonable often, I'd imagine, because this is the part you found to be slow.

00:21:50.020 --> 00:21:52.280
Yeah, that's the trade-off you make.

00:21:52.280 --> 00:21:52.920
And so, yeah.

00:21:52.920 --> 00:21:57.060
So we would not suggest people use type lists just in random places in their program.

00:21:57.060 --> 00:21:58.820
It's really intended to be used.

00:21:58.820 --> 00:21:59.480
Yeah.

00:21:59.480 --> 00:22:00.700
I heard this is fast.

00:22:00.700 --> 00:22:03.580
So we're just going to replace all like new rule.

00:22:03.580 --> 00:22:05.240
Bracket, bracket is disallowed.

00:22:05.240 --> 00:22:06.320
We're now using this one, right?

00:22:06.320 --> 00:22:06.560
Yeah.

00:22:06.560 --> 00:22:09.920
When you're working with Python objects, Python's data structures can't be beat.

00:22:09.920 --> 00:22:15.820
They are so well-tuned that it's very, very hard to imagine something that could be faster

00:22:15.820 --> 00:22:16.200
than them.

00:22:16.200 --> 00:22:16.560
All right.

00:22:16.580 --> 00:22:22.280
So maybe one more thing on Numba we could talk about is, so far, I imagine people have

00:22:22.280 --> 00:22:28.980
been in their mind thinking of, I have at least, running on CPUs, be that on Apple Silicon or

00:22:28.980 --> 00:22:31.320
Intel chips or AMD or whatever.

00:22:31.320 --> 00:22:34.320
But there's also support for graphics cards, right?

00:22:34.460 --> 00:22:34.780
Yes.

00:22:34.780 --> 00:22:35.220
Yeah.

00:22:35.220 --> 00:22:39.040
So for a very long, I mean, we've had this again for 10 plus years.

00:22:39.040 --> 00:22:44.680
We were very early adopters of CUDA, which is the programming interface for NVIDIA GPUs.

00:22:44.680 --> 00:22:49.920
CUDA is supported by every NVIDIA GPU, whether it's a low-end gamer card or a super high-end

00:22:49.920 --> 00:22:50.560
data center card.

00:22:50.560 --> 00:22:51.680
They all support CUDA.

00:22:51.680 --> 00:22:54.940
So that was really nice for people who were trying to get into GPU programming.

00:22:54.940 --> 00:22:57.160
You could use inexpensive hardware to learn.

00:22:57.160 --> 00:23:02.320
And so on both Windows and Linux, Macs don't have NVIDIA GPUs for a long, long time now,

00:23:02.320 --> 00:23:08.180
but on Windows and Linux, you can basically write what they call a CUDA kernel in pure Python.

00:23:08.180 --> 00:23:13.260
And it just, you know, and you can pass up, you know, arrays, either NumPy arrays, which

00:23:13.260 --> 00:23:17.320
then have to be sent to the card or special GPU arrays that are already on the card.

00:23:17.320 --> 00:23:21.740
That is a great way for people to learn a bit more about GPU programming.

00:23:21.740 --> 00:23:26.220
I will say Numba might not be the best place to start with GPU programming in Python because

00:23:26.220 --> 00:23:34.080
there's a great project called Coupy, C-U-P-Y, that is literally a copy of NumPy, but does

00:23:34.080 --> 00:23:35.620
all of the computation on the GPU.

00:23:35.620 --> 00:23:37.300
And Coupy works great with Numba.

00:23:37.300 --> 00:23:42.860
So I often tell people, if you're curious, start with Coupy, use some of those NumPy functions

00:23:42.860 --> 00:23:47.520
to get a sense of, you know, when is an array big enough to matter on the GPU, that sort of

00:23:47.520 --> 00:23:47.720
thing.

00:23:47.720 --> 00:23:52.980
And then when you start wanting to do more custom algorithms, Numba is where you kind of turn

00:23:52.980 --> 00:23:54.240
to for that second level.

00:23:54.240 --> 00:23:54.500
Yeah.

00:23:54.840 --> 00:23:59.620
So I feel like I'm referencing a lot of Etamar's work over here.

00:23:59.620 --> 00:24:04.280
But what if we didn't have a NVIDIA GPU?

00:24:04.280 --> 00:24:05.180
Is there anything we could do?

00:24:05.180 --> 00:24:05.560
Yeah.

00:24:05.560 --> 00:24:06.740
So there are other projects.

00:24:06.740 --> 00:24:11.500
So things like, as I mentioned here, like PyTorch and things are, have been ported to a

00:24:11.500 --> 00:24:12.480
number of different backends.

00:24:12.480 --> 00:24:16.580
Well, this is one thing the Numba team, we are frequently talking about is how do we add

00:24:16.580 --> 00:24:18.980
non-GPU or non-NVIDIA GPU support?

00:24:19.320 --> 00:24:21.580
But it's, I don't have an ETA on that.

00:24:21.580 --> 00:24:24.240
That's something that we just still are kind of thinking about.

00:24:24.240 --> 00:24:25.520
But PyTorch, definitely.

00:24:25.520 --> 00:24:28.080
And you can use PyTorch as an array library.

00:24:28.080 --> 00:24:29.980
You don't have to be doing machine learning necessarily.

00:24:29.980 --> 00:24:32.540
You can use it just for fast arrays.

00:24:32.540 --> 00:24:37.000
It's just most popular for, because it supports, I mean, JAX is a very similar thing, because

00:24:37.000 --> 00:24:39.440
it adds the extra features you want for those machine learning models.

00:24:39.640 --> 00:24:42.340
But at the core of every machine learning model, it's just array math.

00:24:42.340 --> 00:24:45.160
And so you could choose to just do that if that's what you want.

00:24:45.160 --> 00:24:48.340
And then you could even still pass those arrays off to Numba at some point in the future.

00:24:48.340 --> 00:24:49.000
I'm interested.

00:24:49.000 --> 00:24:50.460
Yeah, I didn't realize there was integration.

00:24:50.460 --> 00:24:53.720
Yeah, I didn't realize there was integration with that as well.

00:24:53.720 --> 00:24:54.680
Yeah.

00:24:54.680 --> 00:24:59.000
A while back, we kind of worked with a number of projects to define a GPU array interface

00:24:59.000 --> 00:25:02.160
that's used by a number of them so that we can see each other's arrays without having

00:25:02.160 --> 00:25:03.920
to copy the data, which is very helpful.

00:25:03.920 --> 00:25:04.220
Yeah.

00:25:04.220 --> 00:25:04.680
Yeah.

00:25:04.680 --> 00:25:07.160
We have a lot more topics than Numba, but I'm still fascinated with it.

00:25:07.160 --> 00:25:07.840
Oh, yeah.

00:25:08.040 --> 00:25:13.780
So, you know, one of the big, all the rages, all the rage now is vector databases, obviously,

00:25:13.780 --> 00:25:17.380
because I want to query my way through LLM outputs.

00:25:17.380 --> 00:25:18.100
Right?

00:25:18.100 --> 00:25:22.580
Like, where in 100,000 dimensional space does this question live?

00:25:22.580 --> 00:25:22.880
Yep.

00:25:22.880 --> 00:25:23.480
Or whatever.

00:25:23.480 --> 00:25:27.560
Is there any integration with that kind of stuff back into Numba?

00:25:27.560 --> 00:25:28.860
Numba, not directly.

00:25:28.860 --> 00:25:33.220
Although Numba does have interfaces, an easy way to call out to C functions.

00:25:33.220 --> 00:25:37.980
So a lot of these vector databases are implemented in, you know, C or C++ or something.

00:25:38.380 --> 00:25:41.820
And so if you did have a use case where you needed to call out to one of them, if there

00:25:41.820 --> 00:25:46.000
was a C function call to make directly to sort of the underlying library that bypassed the

00:25:46.000 --> 00:25:47.720
interpreter, you can do that from Numba.

00:25:47.720 --> 00:25:52.840
And so I haven't seen anyone do that yet, but it's a generic sort of C interface.

00:25:52.840 --> 00:25:53.140
Yeah.

00:25:53.140 --> 00:25:57.580
Maybe there's a database driver written in C, in which case, I don't know all the different

00:25:57.580 --> 00:25:57.980
databases.

00:25:58.140 --> 00:26:00.160
I know there are some that are specifically built for it.

00:26:00.160 --> 00:26:06.260
Maybe DuckDB has got something going on here, but also MongoDB has added vector stuff to

00:26:06.260 --> 00:26:06.360
it.

00:26:06.360 --> 00:26:07.920
And I know they have a C library as well.

00:26:07.920 --> 00:26:08.160
Yeah.

00:26:08.160 --> 00:26:12.120
I've looked at LanceDB is one I've seen mentioned by used by a couple of projects.

00:26:12.120 --> 00:26:14.140
That's just for vector stuff.

00:26:14.140 --> 00:26:15.000
It doesn't do anything else.

00:26:15.000 --> 00:26:16.040
LanceDB.

00:26:16.040 --> 00:26:17.000
LanceDB.

00:26:17.000 --> 00:26:17.700
Okay.

00:26:17.820 --> 00:26:21.960
I heard about it in the context of another Python LLM project.

00:26:21.960 --> 00:26:22.180
Cool.

00:26:22.180 --> 00:26:26.540
Well, that's news to me, but it is a developer-friendly open source database for AI.

00:26:26.540 --> 00:26:27.240
Okay.

00:26:27.240 --> 00:26:27.800
Brilliant.

00:26:27.800 --> 00:26:28.580
All right.

00:26:28.580 --> 00:26:31.200
Well, like I said, we have more things to talk about.

00:26:31.200 --> 00:26:32.540
So many things.

00:26:32.540 --> 00:26:34.080
So many things.

00:26:34.080 --> 00:26:34.840
But this is super.

00:26:34.840 --> 00:26:35.460
Okay.

00:26:35.460 --> 00:26:37.600
One more thing I want to ask you about here before we go on.

00:26:37.600 --> 00:26:40.280
This has a Cython feel to it.

00:26:40.280 --> 00:26:42.900
Can you compare and compress Numba to Cython?

00:26:43.220 --> 00:26:48.000
So Cython uses sort of requires you to put in some type information in order to be able

00:26:48.000 --> 00:26:50.240
to generate C code that is more efficient.

00:26:50.240 --> 00:26:53.380
Numba is mainly focused on type inference.

00:26:53.380 --> 00:26:59.020
So we try to figure out all the types in your function based on the types of the inputs.

00:26:59.020 --> 00:27:03.980
And so in general, we, although Numba has places where you can put type annotations, we

00:27:03.980 --> 00:27:09.360
generally discourage people from doing it because we find that it adds work and is error prone

00:27:09.360 --> 00:27:11.080
and doesn't really help the performance in the end.

00:27:11.400 --> 00:27:13.700
Numba will figure out all the types directly.

00:27:13.700 --> 00:27:17.780
And so when it compiles it, if it comes in, if you call it twice with different types,

00:27:17.780 --> 00:27:22.080
does it just say, well, now we're going to need a, this version of the function underscore

00:27:22.080 --> 00:27:24.660
strings list rather than integers list or something.

00:27:24.660 --> 00:27:25.000
Yeah.

00:27:25.000 --> 00:27:25.260
Yeah.

00:27:25.260 --> 00:27:29.280
Every Numba compiled function actually contains a dispatcher that will look at the argument

00:27:29.280 --> 00:27:30.500
types and pick the right one.

00:27:30.500 --> 00:27:32.420
And it's at pretty high granularity.

00:27:32.420 --> 00:27:36.680
You know, for example, people who are familiar with multidimensional arrays and like Fortran and

00:27:36.680 --> 00:27:41.380
C know that they lay out the rows and columns in a different order, which has impact

00:27:41.380 --> 00:27:45.260
on how you do loops and stuff for, for kind of to maximize locality.

00:27:45.260 --> 00:27:49.660
Numba can tell the difference between those two cases and will generate different code for

00:27:49.660 --> 00:27:50.300
those two cases.

00:27:50.300 --> 00:27:50.560
Wow.

00:27:50.560 --> 00:27:53.820
So this is stuff that you as the user don't want to even know.

00:27:53.820 --> 00:27:55.540
No, you don't want to worry about that.

00:27:55.540 --> 00:27:56.560
That's a whole nother level.

00:27:56.560 --> 00:28:02.280
So you were like, okay, well, if it's laid out in this order, it's probably this, it appears

00:28:02.280 --> 00:28:05.940
on the L, you know, the local cache for the CPU in this way.

00:28:06.020 --> 00:28:09.640
And so if we loop in that direction, we'll like iterate through the cache instead of blow

00:28:09.640 --> 00:28:11.360
through it every loop or something like that.

00:28:11.360 --> 00:28:15.400
Basically, we want to make sure that LLVM knows when the step size is one.

00:28:15.400 --> 00:28:18.660
And that's either on the row or the column access, depending on that.

00:28:18.660 --> 00:28:21.120
And because, I mean, compilers in general are magic.

00:28:21.120 --> 00:28:26.620
Like we are grateful that LLVM exists because they can do so many tricks at that level.

00:28:26.620 --> 00:28:29.340
I mean, because, I mean, this is the same thing that powers clang and other stuff.

00:28:29.340 --> 00:28:33.020
So, you know, all of, all of macOS compilers are built on LLVM.

00:28:33.140 --> 00:28:37.860
And so we can leverage all of the tricks they've figured out in decades of development.

00:28:37.860 --> 00:28:38.580
Yeah, that's cool.

00:28:38.580 --> 00:28:42.460
And Python itself is compiled with that, at least on macOS.

00:28:42.460 --> 00:28:46.520
I just saw it last night, you know, that I have clang some version, whatever.

00:28:46.520 --> 00:28:49.620
When I was just looking at the version for my, my Python, I was compiled with that.

00:28:49.620 --> 00:28:50.000
Cool.

00:28:50.380 --> 00:28:50.620
Okay.

00:28:50.620 --> 00:28:53.760
So we've been on the Numba JIT.

00:28:53.760 --> 00:28:57.980
Anthony Shaw wrote an article, Python 3.13 gets a JIT.

00:28:57.980 --> 00:29:03.660
And this is a pretty comprehensive and interesting article on what's going on here.

00:29:03.660 --> 00:29:07.220
What's your experience with this JIT coming to Python 3.13?

00:29:07.220 --> 00:29:12.900
This is, and they've definitely tried to set expectations here, that this first release is really planting a flag.

00:29:12.900 --> 00:29:17.040
It's to say, we're going to start building on top of this base.

00:29:17.240 --> 00:29:23.440
And so as far as I've seen, the benchmarks for 3.13 are not going to be like the world on fire kind of stuff.

00:29:23.440 --> 00:29:27.460
So throwing away RC code and rewriting operating systems and JIT in Python.

00:29:27.460 --> 00:29:29.080
But you have to take a first step.

00:29:29.080 --> 00:29:37.780
And this is honestly pretty, it impressed me because as a library, we can take a lot of risks and do things that I know we can depend on LLVM.

00:29:37.780 --> 00:29:43.020
We can do all sorts of stuff that may be not work for everyone, because if Numba doesn't solve your problem, you just don't use it.

00:29:43.020 --> 00:29:44.780
You can just leave it out of your environment.

00:29:44.780 --> 00:29:45.880
You don't have to install it.

00:29:46.180 --> 00:29:53.520
And so it's easy to kind of us to zero in on just the problems that we're good at and say, if this doesn't solve your problem, just leave us out.

00:29:53.520 --> 00:29:57.740
When you actually are putting a JIT into the core interpreter, everyone gets it.

00:29:57.740 --> 00:30:06.980
And you have to consider, and Python is so broad that you could grab two Python experts, and they may actually have nothing in common with each other.

00:30:08.480 --> 00:30:14.980
But are both equal claim to being experts at using Python, but they might use them in different domains and have different libraries they care about and all of that.

00:30:14.980 --> 00:30:17.640
I feel that way when I talk to Candace people.

00:30:17.640 --> 00:30:21.580
And I think about me doing like web development APIs and stuff.

00:30:21.680 --> 00:30:26.320
I'm like, I think I'm really good at Python, generally, you know, good enough to write good apps.

00:30:26.320 --> 00:30:28.780
But then I look at this, I'm like, I don't even really this.

00:30:29.220 --> 00:30:33.820
Some of these like expressions that go into the filter brackets.

00:30:33.820 --> 00:30:43.920
I'm like, I didn't even know that that was possible or really how to apply, you know, like it's just, it's weird to both feel really competent and understand it, but also have it kind of no idea.

00:30:43.920 --> 00:30:50.080
Yeah, and what I think what you're getting at is those are two really different use cases, and they're getting the same JIT and has to work for both of them.

00:30:50.080 --> 00:30:53.320
But, you know, combinatorially explode that problem, right?

00:30:53.320 --> 00:30:55.440
Yeah, and, you know, all the different hardware.

00:30:55.440 --> 00:30:59.060
I mean, Numbus supports a lot of different computers, but not everyone that Python supports.

00:30:59.060 --> 00:31:00.760
Like Micro Python?

00:31:00.760 --> 00:31:05.360
Yeah, or we don't work on, you know, HP UX or anything like that, necessarily.

00:31:05.360 --> 00:31:09.840
Python has an enormous support, range of supported platforms, an enormous set of use cases.

00:31:09.840 --> 00:31:12.560
And anything you do is going to affect everyone.

00:31:13.000 --> 00:31:21.980
So this approach, which I would say this copy and patch JIT approach is really clever, because one of the, you know, again, Numba has to bring, we build a, you know, custom version of LVM.

00:31:21.980 --> 00:31:23.960
It's a little stripped down, but it's mostly still there.

00:31:23.960 --> 00:31:25.520
So we have to bring that along for the ride.

00:31:25.520 --> 00:31:29.060
That's a heavy, heavy dependency to put on the core interpreter for everyone.

00:31:29.060 --> 00:31:35.820
So the clever bit here is they figured out how to have a JIT, but still do all the compiler stuff at build time.

00:31:35.820 --> 00:31:40.680
So when you build this copy and patch JIT, you actually need LVM, but only at build time.

00:31:40.680 --> 00:31:41.940
And then it can go away.

00:31:42.080 --> 00:31:44.760
And so the person who receives the interpreter doesn't need LVM anymore.

00:31:44.760 --> 00:31:49.320
And so they basically built for themselves a bunch of little template fragments.

00:31:49.320 --> 00:31:56.440
This is the patching part is basically you're saying, I've got a bunch of fragments that implement different opcodes in the bytecode different ways.

00:31:56.560 --> 00:31:59.240
And I'm going to string them together.

00:31:59.240 --> 00:32:07.140
And then go in and there's a bunch of fill in the blank spots that I can go in and swap in the okay, you get your value from here and then you put it over here and all that.

00:32:07.240 --> 00:32:12.920
But the actual machine code was generated by LVM by the person who built Python in the first place.

00:32:12.920 --> 00:32:13.420
I see.

00:32:13.420 --> 00:32:15.120
It just amazes me this works.

00:32:15.120 --> 00:32:21.680
And I'm excited to see where they go with it because it was a clever way to avoid adding a huge heavy dependency to Python.

00:32:21.680 --> 00:32:23.980
Let's start to get some of that JIT benefit.

00:32:24.320 --> 00:32:27.700
So it looks at some of the common patterns.

00:32:27.700 --> 00:32:28.360
I see.

00:32:28.360 --> 00:32:35.720
Okay, we're looping over a list of loads or something and replaces that with more native code or something along those lines.

00:32:35.720 --> 00:32:36.100
Yeah.

00:32:36.100 --> 00:32:36.360
Yeah.

00:32:36.360 --> 00:32:46.200
You essentially have a compiler from a bunch of little recipes that are if I do this pattern, sub in this machine code, fill in these blanks, and you just have a table of them.

00:32:46.200 --> 00:32:55.200
So it's the challenge there is that there is a combinatorial explosion again of how many different, you know, a full blown compiler like LVM has a bunch of rules.

00:32:55.200 --> 00:32:56.180
It's rule based.

00:32:56.420 --> 00:33:00.740
And so it's saying, if I see this pattern, I do this replacement and it keeps doing all of this.

00:33:00.740 --> 00:33:04.840
And then at the end, it says, okay, now I'm going to generate my machine code from that, those transformations.

00:33:04.840 --> 00:33:11.100
If I don't have LVM at runtime, I have to kind of figure out what are the best templates up front, put them in this table.

00:33:11.100 --> 00:33:22.200
And then, you know, and so this is where honestly looking at different usage patterns will probably be a huge help is in practice, you can have any sequence of byte codes, but in reality, you're probably going to have certain ones a lot.

00:33:22.200 --> 00:33:24.040
And those are the ones you want to focus on.

00:33:24.480 --> 00:33:31.040
So I think once we, I don't know, you know, once we start getting this out in the community and start getting feedback on it, I'm curious to see how rapidly it can evolve.

00:33:31.040 --> 00:33:32.100
That'll be really interesting.

00:33:32.300 --> 00:33:32.740
Yeah.

00:33:32.740 --> 00:33:38.900
And this whole copy and patch it is we often hear people say, I'm a computer, I have a computer science degree.

00:33:38.900 --> 00:33:44.620
And I think what that really means is I have a software engineering degree in, or I am a software engineering person.

00:33:44.800 --> 00:33:50.940
They are not often creating new science, computer science of theories.

00:33:50.940 --> 00:33:54.880
They're more like, I really understand how operating system works and programmers and compilers.

00:33:54.880 --> 00:33:58.400
And I write JSON API, sorry, talk to databases.

00:33:58.400 --> 00:34:02.980
This is like true new research out of legitimate computer science, right?

00:34:02.980 --> 00:34:04.220
This copy and patch shit.

00:34:04.360 --> 00:34:05.360
Yeah.

00:34:05.360 --> 00:34:14.560
They mentioned, I mean, they cite a paper from 2021 and in computer science, going from paper to implementation in one of the most popular languages on earth in three years seems pretty fast.

00:34:14.560 --> 00:34:16.480
It does seem pretty fast, right?

00:34:16.480 --> 00:34:17.900
It definitely seems pretty fast.

00:34:17.900 --> 00:34:29.280
And the reason I bring this up is I imagine that dropping it into one of the most popular programming languages with a super diverse set of architectures and use cases will probably push that science forward.

00:34:29.280 --> 00:34:29.620
Yeah.

00:34:29.620 --> 00:34:34.320
This will be tested in one of the most intense environments you could imagine.

00:34:34.320 --> 00:34:44.640
You know, I mean, whatever they did for their, their research paper or their dissertation or whatever, this is another level of putting it into a test and experimentation to put it into Python.

00:34:44.640 --> 00:34:45.040
Yep.

00:34:45.040 --> 00:34:45.320
Yeah.

00:34:45.320 --> 00:34:45.640
Wild.

00:34:46.000 --> 00:34:46.240
Okay.

00:34:46.240 --> 00:34:47.880
Hopefully this speeds things up.

00:34:47.880 --> 00:34:50.420
This is going to be interesting because it just happens, right?

00:34:50.420 --> 00:34:51.140
It just happens.

00:34:51.140 --> 00:34:55.720
If you have Python 3.13, it's going to be looking at its patterns and possibly swapping out.

00:34:55.720 --> 00:35:02.840
This is complimentary to all of the techniques the faster CPython folks have been doing all along for many releases now.

00:35:02.840 --> 00:35:03.080
Yeah.

00:35:03.080 --> 00:35:11.140
Is they've, they've been looking at other ways to speed up the interpreter without going all the way to a full blown compiler, which this is kind of giving you the final step.

00:35:11.140 --> 00:35:15.680
So that's again, another interesting place is how does this compliment those?

00:35:15.820 --> 00:35:20.600
You know, I don't know those, those details, but it's another tool in the toolbox to sort of go back to the beginning.

00:35:20.600 --> 00:35:29.660
It's speed is about having a bunch of tools and you kind of pick up 5% here and 5% there and you pile up enough 5% and pretty soon you have something substantial.

00:35:29.660 --> 00:35:31.040
Yeah, I absolutely.

00:35:31.040 --> 00:35:31.900
That was the plan, right?

00:35:31.940 --> 00:35:41.820
The faster CPython was to make it multiples of times faster by adding 20% improvements release over release over release and, you know, compounding percentages basically.

00:35:41.820 --> 00:35:49.720
The one thing I don't know about this is we've had the specializing adaptive interpreter that was one of those faster CPython things that came along.

00:35:49.720 --> 00:35:54.220
You know, is this just the next level of that or is this a replacement for that?

00:35:54.220 --> 00:35:54.860
I don't know.

00:35:54.860 --> 00:35:55.720
I'm sure people can.

00:35:55.720 --> 00:35:56.400
Yeah, I don't know.

00:35:56.400 --> 00:36:00.380
I don't know what the, what their roadmap is for that because I think part of this is, this is so new.

00:36:00.380 --> 00:36:04.460
I think they got to see how it works in practice before they start figuring out.

00:36:04.460 --> 00:36:05.000
I agree.

00:36:05.000 --> 00:36:09.760
It feels like an alternative to that specialized and adaptive interpreter, but I don't know.

00:36:09.760 --> 00:36:14.080
Maybe some of the stuff they've learned from one made it possible or even as just an extension of it.

00:36:14.080 --> 00:36:14.600
Okay.

00:36:14.600 --> 00:36:16.520
What do we want to talk about next here?

00:36:16.520 --> 00:36:16.940
I think.

00:36:16.940 --> 00:36:17.800
You want to talk about threads?

00:36:17.800 --> 00:36:19.280
No, let's talk.

00:36:19.280 --> 00:36:22.300
I want to talk about, I want to talk about Rust really quick before we talk about it.

00:36:22.300 --> 00:36:23.560
Then, because that'll be quick.

00:36:23.560 --> 00:36:27.580
And then I want to talk about threads because threads will not be as quick and it's super interesting.

00:36:27.580 --> 00:36:34.600
It's been a problem that people have been chipping at for years and years and years, the threads thing.

00:36:34.600 --> 00:36:37.820
But what do you think about all this Rust mania?

00:36:38.200 --> 00:36:43.140
I mean, it's shown some real positive results, things like rough and pydantic and others,

00:36:43.140 --> 00:36:48.520
but it's actually a little bizarrely controversial or maybe not bizarre, non-obviously controversial.

00:36:48.520 --> 00:36:48.960
Yeah.

00:36:48.960 --> 00:36:53.020
I mean, my take on the Rust stuff is I view it in the same light as when we use C and Fortran

00:36:53.020 --> 00:36:55.880
historically, it's just Rust is a nicer language in many ways.

00:36:55.880 --> 00:37:01.960
And so being a nicer language means it's certainly, you know, you could have taken any of these things

00:37:01.960 --> 00:37:04.660
and rewritten them in C a long time ago and they would have been faster.

00:37:04.660 --> 00:37:06.380
You just didn't want to write that C code.

00:37:07.500 --> 00:37:07.940
Exactly.

00:37:07.940 --> 00:37:08.660
You know what?

00:37:08.660 --> 00:37:11.720
We could do this in assembler and you would fly, guys.

00:37:11.720 --> 00:37:12.620
Yeah.

00:37:12.620 --> 00:37:20.960
So Rust is moving things, Rust lowering the bar to saying, okay, maybe I'll implement the core of my algorithm outside of Python entirely.

00:37:21.160 --> 00:37:22.000
It's interesting.

00:37:22.000 --> 00:37:28.640
And honestly, I would happily see Rust completely replace C as the dominant extension language in Python.

00:37:28.640 --> 00:37:34.620
The trade-off here, and this is one of those things that's sometimes easy to forget, again, because the Python community is so diverse,

00:37:34.620 --> 00:37:41.780
is when you do switch something to Rust, you do reduce the audience who can contribute effectively in some cases.

00:37:41.780 --> 00:37:51.520
That using Python to implement things has a benefit for the maintainers if it lets them get more contributions, more easily onboard new people.

00:37:51.520 --> 00:37:59.320
I hear this a lot actually from academic software where you have this constant rotating students and postdocs and things.

00:37:59.820 --> 00:38:06.220
And so how quickly you can onboard someone who isn't a professional software developer into a project to contribute to it is relevant.

00:38:06.220 --> 00:38:09.880
And so, but I think it's different for every project.

00:38:09.880 --> 00:38:16.360
There are some things like, you know, again, Rust in cryptography makes total sense to me because that's also a very security conscious thing.

00:38:16.660 --> 00:38:20.300
You really don't want to be dealing with C buffer overflows in that kind of code.

00:38:20.300 --> 00:38:23.920
And so the guarantees Rust offers are valuable also.

00:38:23.920 --> 00:38:29.420
Well, and I think that that's also makes sense even outside of just security directly.

00:38:29.420 --> 00:38:30.900
You're going to build a web server.

00:38:30.900 --> 00:38:35.820
It's a nerve wracking thing to run other people's code on an open port on the Internet.

00:38:35.820 --> 00:38:38.700
And so this is better.

00:38:38.700 --> 00:38:46.260
One of the things I switched to is I recently switched to Granian for a lot of my websites, which is a Rust ACP server.

00:38:46.260 --> 00:38:55.680
It's comparable in performance, slightly faster than other things, but it's way more, it's deviation from its average is way, way better.

00:38:55.680 --> 00:38:57.380
So it's just more consistent.

00:38:57.380 --> 00:39:03.280
More consistent, but also, you know, like the average, for example, the average, where's the versus third party server?

00:39:03.280 --> 00:39:03.940
That's the one I want.

00:39:03.940 --> 00:39:10.440
So for against micro WSGI, for example, right, it's six milliseconds versus 17 milliseconds.

00:39:10.440 --> 00:39:11.440
Like, whatever.

00:39:11.440 --> 00:39:14.960
But then you look at the max latency of 60 versus three seconds.

00:39:14.960 --> 00:39:16.040
It's like, oh, wow.

00:39:16.040 --> 00:39:16.640
Hold on.

00:39:16.640 --> 00:39:16.840
Right.

00:39:16.840 --> 00:39:22.620
But the fact it's written in Rust, I think feels it's a little bit of extra safety, all other things being equal.

00:39:22.620 --> 00:39:23.180
Right.

00:39:23.180 --> 00:39:25.780
And that, I mean, obviously a lot of caveats there.

00:39:25.780 --> 00:39:26.080
Yeah.

00:39:26.080 --> 00:39:29.040
The actually the interesting point about, and this is not unique to Rust.

00:39:29.040 --> 00:39:32.100
This is, again, the same problem with C and other things is that it's a little bit interesting.

00:39:32.100 --> 00:39:35.860
On one hand, we're pushing the Python interpreter and JITs and all this other stuff.

00:39:35.940 --> 00:39:39.460
At the same time as you're thinking about whether to pull code entirely out of Python.

00:39:39.460 --> 00:39:43.860
And it creates a barrier where the JIT can't see what's in the Rust code.

00:39:43.860 --> 00:39:49.940
And so if there was an optimization that could have crossed that boundary, it's no longer available to the compilers.

00:39:49.940 --> 00:39:50.380
Yeah.

00:39:50.580 --> 00:40:00.580
This is a problem the number team has been thinking about a lot because our number one request, aside from, you know, other GPUs, is can number be an ahead of time compiler instead of a just in time compiler?

00:40:01.140 --> 00:40:04.660
And we were like, superficially, yes, that's straightforward.

00:40:04.660 --> 00:40:07.600
But then we started thinking about the user experience and the developer experience.

00:40:07.600 --> 00:40:12.500
And there are some things that you lose when you go ahead of time that you have with the JIT.

00:40:12.500 --> 00:40:13.800
And how do you bridge that gap?

00:40:13.800 --> 00:40:14.720
Yeah, it gets tricky.

00:40:14.720 --> 00:40:17.920
We've been trying to figure out some tooling to try and bridge that.

00:40:17.920 --> 00:40:27.260
So we at SciPy, we did a talk on a project we just started called Pixie, which is a sub project of Numba that is trying to, which doesn't have Rust support yet, but that's been one of the requests.

00:40:27.820 --> 00:40:32.720
So if you go to github.com/Numba slash Pixie, see if they've indexed it.

00:40:32.720 --> 00:40:33.300
Oh, they're perfect.

00:40:33.300 --> 00:40:33.660
Okay.

00:40:33.660 --> 00:40:34.460
Search engines.

00:40:34.460 --> 00:40:36.240
Search engines are pure magic.

00:40:36.240 --> 00:40:36.920
They really are.

00:40:36.920 --> 00:40:37.300
But yeah.

00:40:37.300 --> 00:40:41.020
But Pixie, we gave a talk about it at SciPy.

00:40:41.020 --> 00:40:42.460
It's very early stages.

00:40:42.460 --> 00:40:57.460
But what we're trying to do is figure out how to, in the ahead of time compilation, whether that's C or Rust or even Numba eventually, capturing enough info that we can feed that back into a future JIT so that the JIT can still see what's going on in the compiled code as kind of a future-proofing.

00:40:57.460 --> 00:40:59.520
Yeah, that's cool.

00:40:59.520 --> 00:41:04.540
I know some compilers have profiling-based optimization type things.

00:41:04.540 --> 00:41:10.200
Like you can compile it with some instrumentation, run it, and then take that output and feed it back into it.

00:41:10.200 --> 00:41:16.380
And I don't know if I've ever practically done anything with that, but I'm like, oh, that's kind of a neat idea to like try it.

00:41:16.380 --> 00:41:18.020
Let it see what it does and then feed it back.

00:41:18.020 --> 00:41:19.300
Is this sort of like that?

00:41:19.300 --> 00:41:19.780
Or what do you think?

00:41:19.880 --> 00:41:21.060
This is different.

00:41:21.060 --> 00:41:24.080
This is sort of, this is basically capturing in the library file.

00:41:24.080 --> 00:41:32.960
So you compiled ahead of time to a library, capturing the LLVM bitcode so that you could pull it out and embed it into your JIT, which might have other LLVM bitcode.

00:41:32.960 --> 00:41:34.600
So then you can optimize.

00:41:34.600 --> 00:41:39.900
You can have a function you wrote in Python that calls a function in C, and you could actually optimize them together.

00:41:39.900 --> 00:41:45.280
Even though they were compiled at different times, implemented in different languages, you could actually cross that boundary.

00:41:45.740 --> 00:41:48.180
One's like a ahead of time compilation, just standard compilation.

00:41:48.180 --> 00:41:52.040
And one is like a JIT thing, but it's like, oh, we're going to click it together in the right ways.

00:41:52.040 --> 00:41:52.280
Yeah.

00:41:52.280 --> 00:41:52.560
Wow.

00:41:52.560 --> 00:41:52.900
Yeah.

00:41:52.900 --> 00:41:57.700
Because JITs are nice in that they can see everything that's going on, but then they have to compile everything that's going on.

00:41:57.700 --> 00:41:59.560
And that adds time and latency and things.

00:41:59.560 --> 00:42:03.180
And so can you have it both ways is that's really what we're trying to do.

00:42:03.180 --> 00:42:05.380
It's nice when you can't have your cake and eat it too, right?

00:42:05.380 --> 00:42:06.380
Yes.

00:42:06.380 --> 00:42:09.240
My cake before my vegetables and it'd be fine.

00:42:09.240 --> 00:42:12.720
I said that this Rust thing was a little bit controversial.

00:42:12.720 --> 00:42:20.100
I think there's some just, hey, you're stepping on my square of Python space with a new tool.

00:42:20.100 --> 00:42:22.760
I don't think that has anything to do with Rust per se.

00:42:22.760 --> 00:42:28.500
It's just somebody came along and made a tool that is now doing something maybe in better ways or I don't know.

00:42:28.500 --> 00:42:30.460
I don't want to start up a whole debate about that.

00:42:30.460 --> 00:42:38.480
But I think the other one is what you touched on is if we go and write a significant chunk of this stuff in this new language, regardless what language it is,

00:42:38.480 --> 00:42:49.240
Rust is a relatively not popular language compared to others, then people who contribute to that, either from the Python side, we're like, well, there's this big chunk of Rust code now that I don't understand anything about.

00:42:49.240 --> 00:42:50.880
So I can't contribute to that part of it.

00:42:50.880 --> 00:42:57.260
And you might even say, well, what about the pro developers or the experienced core developers and stuff?

00:42:57.440 --> 00:43:01.920
They're experienced and grow at C in Python, which is also not Rust, right?

00:43:01.920 --> 00:43:08.760
Like it's this new area that is more opaque to most of the community, which I think that's part of the challenge.

00:43:08.760 --> 00:43:09.160
Yeah.

00:43:09.160 --> 00:43:12.120
Some people like learning new programming languages and some don't.

00:43:12.120 --> 00:43:17.920
So on some hand, you know, Rust can be this is a new intellectual challenge and it fixes practically some problems you have with C.

00:43:18.200 --> 00:43:22.780
In other cases, it's the I wanted to worry about what this project does and not another programming language.

00:43:22.780 --> 00:43:23.500
Right, right, right.

00:43:23.500 --> 00:43:26.760
Kind of have to look at your communities and decide what's the right tradeoff for you.

00:43:26.760 --> 00:43:31.000
Maybe in 10 years, CPython will be our Python and it'll be written in Rust.

00:43:31.000 --> 00:43:42.000
You know, I mean, if we move to WebAssembly and like PyScript, Pyodid land a lot, like having that right in there's there's there's a non zero probability, but it's not a high number, I suppose.

00:43:42.200 --> 00:43:49.040
Speaking of something I also thought was going to have a very near zero probability, PEP 703 is accepted.

00:43:49.040 --> 00:43:49.700
Oh, my goodness.

00:43:49.700 --> 00:43:50.260
What is this?

00:43:50.260 --> 00:43:50.760
Yeah.

00:43:50.760 --> 00:43:55.300
So this was, you know, again, a couple of years ago now, or I guess a year ago, it was finally accepted.

00:43:55.300 --> 00:44:04.800
So for since very long time, the Python interpreter has, you know, again, threads are an operating system feature that let you do something in a program concurrently.

00:44:04.800 --> 00:44:14.040
And now that all of our computers have, you know, four, eight, even more cores, depending on, you know, what kind of machine you have, even your cell phone has more than one core.

00:44:14.040 --> 00:44:20.260
The using those those cores requires you have some kind of parallel computing in your program.

00:44:20.260 --> 00:44:26.160
And so the problem is that you don't want, you know, once you start doing things in parallel, you have the potential for race conditions.

00:44:26.240 --> 00:44:35.080
You have the two threads might do the same thing at the same time or touch the same data, get it inconsistent, and then your whole program starts to crash and other bad things happen.

00:44:35.080 --> 00:44:41.880
So historically, the global interpreter lock has been sort of sledgehammer protection of the CPython interpreter.

00:44:41.880 --> 00:44:48.080
But the net result was that threads that were running pure Python code basically got no performance benefits.

00:44:48.080 --> 00:44:52.240
You might get other benefits, like you could have one block on IO while the other one does stuff.

00:44:52.240 --> 00:44:54.640
And so it was easier to manage that kind of concurrency.

00:44:54.640 --> 00:45:05.420
But if you were trying to do compute on two cores at the same time in pure Python code, it was just not going to happen because every operation touches a Python object has to lock the interpreter while you make that modification.

00:45:05.420 --> 00:45:10.980
Yeah, you could write all the multi-threaded code with locks and stuff you want, but it's really just going to run one at a time anyway.

00:45:10.980 --> 00:45:11.340
Yeah.

00:45:11.340 --> 00:45:15.440
A little bit like preemptive multi-threading on a single core CPU, right?

00:45:15.440 --> 00:45:16.000
It's weird.

00:45:16.000 --> 00:45:18.740
Like I've added all this complexity, but I haven't got much out of it.

00:45:18.740 --> 00:45:27.820
The secret, of course, is that if your Python program contained not Python, like C or Cython or Fortran, as long as you weren't touching Python objects directly, you could release the GIL.

00:45:27.820 --> 00:45:34.780
And so Python, so especially in the scientific and computing and data science space, multi-threaded code has been around for a long time.

00:45:34.780 --> 00:45:36.140
And we've been using it and it's fine.

00:45:36.140 --> 00:45:39.760
Dask, you can use workers with threads or processes or both.

00:45:39.760 --> 00:45:42.620
And so I frequently will use Dask with four threads.

00:45:42.620 --> 00:45:47.180
And that's totally fine because most of the codes in NumPy and Pandas that release the GIL.

00:45:47.180 --> 00:45:48.840
But that's only a few use cases.

00:45:48.840 --> 00:45:53.220
And so if you want to expand that to the whole Python interpreter, you have to get rid of the GIL.

00:45:53.220 --> 00:45:56.120
You have to have a more fine-grained approach to concurrency.

00:45:56.120 --> 00:46:07.100
And so this proposal from Sam Gross at Meta was basically one of many historical attempts to kind of make that, get rid of that global interpreter lock.

00:46:07.100 --> 00:46:09.240
Many have been proposed and failed historically.

00:46:09.240 --> 00:46:13.800
So getting this all the way through the approval process is a real triumph.

00:46:14.800 --> 00:46:23.480
At the point where it was really being hotly contested, my maybe slightly cynical take is we have between zero and one more chance to get this right in Python.

00:46:23.480 --> 00:46:27.280
Either it's already too late or this is it.

00:46:27.280 --> 00:46:29.460
I don't know which it is.

00:46:29.460 --> 00:46:33.040
I think there were two main complaints against this change.

00:46:33.040 --> 00:46:39.940
Complaint number one was, okay, you theoretically have opened up a parallel compute thing.

00:46:39.940 --> 00:46:44.240
So, for example, on my Apple M2 Pro, I have 10 cores.

00:46:44.640 --> 00:46:49.880
So I could leverage all of those cores, maybe get a five times improvement.

00:46:49.880 --> 00:46:54.880
But single core regular programming is now 50% slower.

00:46:54.880 --> 00:46:57.280
And that's what most people do and we don't accept it.

00:46:57.280 --> 00:46:57.700
All right.

00:46:57.700 --> 00:47:02.500
That's the one of the size, you know, the golectomy and all that was kind of in that realm, I believe.

00:47:02.500 --> 00:47:08.200
The other is yet to be determined, I think, is much like the Python 2 to 3 shift.

00:47:08.200 --> 00:47:12.180
The problem with Python 2 to 3 wasn't that the code of Python changed.

00:47:12.480 --> 00:47:16.360
It was that all the libraries I like and need don't work here.

00:47:16.360 --> 00:47:17.400
Right.

00:47:17.400 --> 00:47:26.220
And so what is going to happen when we take half a million libraries that were written in a world that didn't know or care about threading and are now subjected to it?

00:47:26.220 --> 00:47:26.580
Yeah.

00:47:26.740 --> 00:47:28.700
And there's sort of two levels of problem there.

00:47:28.700 --> 00:47:31.580
There's one that there's work that has to be done to libraries.

00:47:31.580 --> 00:47:38.640
It's usually with C extensions that, you know, didn't assume that, you know, they assumed a global interpreter lock and they'll have to do some changes to change that.

00:47:38.900 --> 00:47:46.540
But the other one is a much more kind of cultural thing where the existence of the guild just meant that Python developers just wrote less threaded code.

00:47:46.540 --> 00:47:46.840
Yeah.

00:47:46.840 --> 00:47:48.040
They don't think about locks.

00:47:48.040 --> 00:47:49.000
They don't worry about locks.

00:47:49.000 --> 00:47:50.560
They just assume it's all going to be fine.

00:47:50.560 --> 00:47:54.560
Because, again, the race condition doesn't protect threaded code from from the guild.

00:47:54.560 --> 00:47:58.860
The guild doesn't protect thread code from race conditions, but it just protects the interpreter from race conditions.

00:47:59.400 --> 00:48:03.120
So you and your application logic are free to make all the thread mistakes you want.

00:48:03.120 --> 00:48:06.180
But if no one ever ran your code in multiple threads, you would never know.

00:48:06.180 --> 00:48:08.580
And so we're going to have to face that now.

00:48:08.580 --> 00:48:13.780
I think that's a super interesting thing that it's a huge cultural issue that people don't think about it.

00:48:13.780 --> 00:48:16.360
Like I said, I used to do a lot of C++ and C#.

00:48:16.360 --> 00:48:18.980
And over there, you're always thinking about threading.

00:48:18.980 --> 00:48:21.240
You're always thinking about, well, what about these three steps?

00:48:21.240 --> 00:48:23.240
Does it go into a temporarily invalid state?

00:48:23.240 --> 00:48:24.300
Do I need to lock this?

00:48:24.300 --> 00:48:24.820
Right.

00:48:24.820 --> 00:48:29.120
And like C# even had literally a keyword lock, which is like a context manager.

00:48:29.220 --> 00:48:35.120
You just say lock curly brace and everything in there is like into a lock and out of like, because it's just so part of that culture.

00:48:35.120 --> 00:48:38.340
And then in Python, you kind of just forget about it and don't worry about it.

00:48:38.340 --> 00:48:43.060
But that doesn't mean that you aren't taking multiple, like five lines of Python code.

00:48:43.060 --> 00:48:53.940
Each one can run all on its own, but taken as a block, they may still get into these like weird states where if another thread after three lines observes the data, it's still busted.

00:48:53.940 --> 00:48:54.520
Right.

00:48:54.520 --> 00:48:57.340
It's just the culture doesn't talk about it very much.

00:48:57.340 --> 00:48:57.580
Yeah.

00:48:57.700 --> 00:49:01.300
If no one ever runs your code in multiple threads, all of those bugs are theoretical.

00:49:01.300 --> 00:49:09.860
And so it's now what's going to shift is, you know, all of those C extensions will get fixed and everything will be, you know, they'll fix those problems.

00:49:09.860 --> 00:49:17.420
And then we're going to have a second wave of everyone seeing their libraries used in threaded programs and starting to discover what are the more subtle bugs.

00:49:17.420 --> 00:49:20.520
Do I have global state that I'm not being careful with?

00:49:20.520 --> 00:49:26.460
And it's going to be painful, but I think it's necessary for Python to stay relevant into the future.

00:49:26.460 --> 00:49:27.420
I'm a little worried.

00:49:27.520 --> 00:49:31.800
I mean, one of the common questions we hear is sort of why is this what multiprocessing is fine.

00:49:31.800 --> 00:49:32.520
Why don't we do that?

00:49:32.920 --> 00:49:38.860
And definitely multiprocessing is big challenge is processes don't get to share data directly.

00:49:39.400 --> 00:49:48.460
So either, you know, if even if I have like read only data, I might if I have to load two gigabytes of data in every process and I want to store start 32 of them because I have a nice big computer.

00:49:48.620 --> 00:49:56.080
I've just 32 X my data, my memory usage, just so that I can have multiple concurrent computations.

00:49:56.080 --> 00:50:03.520
Now, there are tricks you can play on things like Linux, where you load the data once and rely on forking to preserve pages of memory.

00:50:03.520 --> 00:50:09.240
Linux does cool copy on write stuff when you fork, but that's like fragile and not necessarily going to work.

00:50:09.240 --> 00:50:12.160
And then the second thing, of course, is if any of those have to talk to each other.

00:50:12.160 --> 00:50:16.260
Now you're talking about pickling objects and putting them through a socket and handing them off.

00:50:16.520 --> 00:50:20.320
And that is, again, for certain kinds of applications, just a non-starter.

00:50:20.320 --> 00:50:24.440
Yeah, but then people just start going, well, we're just going to rewrite this in a language that lets us share pointers.

00:50:24.440 --> 00:50:24.860
Yeah.

00:50:24.860 --> 00:50:26.560
Or at least memory in process.

00:50:26.560 --> 00:50:26.820
Yeah.

00:50:26.820 --> 00:50:27.160
Yeah.

00:50:27.160 --> 00:50:32.600
There's, again, there are a lot of Python users where this, they don't need this, they don't care about this, this will never impact them.

00:50:32.600 --> 00:50:38.760
And then there's a whole class of Python users who are desperate for this and really, really want it.

00:50:38.760 --> 00:50:39.140
Sure.

00:50:39.140 --> 00:50:42.560
You know, my, I think there's a couple of interesting things here.

00:50:43.180 --> 00:50:49.960
One, I think that this, I think this is important for stemming people leaving.

00:50:49.960 --> 00:50:59.800
I thought, I actually don't hear this that much anymore, but I used to hear a lot of we've left for go because we need better parallelism or we've left for this performance reason.

00:50:59.800 --> 00:51:07.280
And I don't know, that's just a success story of the faster CPython initiative or all the people who had been around and decided they needed to leave.

00:51:07.280 --> 00:51:07.980
They're gone.

00:51:07.980 --> 00:51:10.420
And they, we just don't hear them anymore because they left.

00:51:10.420 --> 00:51:14.260
It's like, you know, I used to hear them say this at the party, but then they said they're going to leave.

00:51:14.260 --> 00:51:15.500
And I don't hear anyone say they're leaving.

00:51:15.620 --> 00:51:16.880
Well, it's because everyone's still here.

00:51:16.880 --> 00:51:17.780
Didn't say that.

00:51:17.780 --> 00:51:18.560
I don't know.

00:51:18.560 --> 00:51:30.160
But I do think having this as a capability will be important for people to be able to maybe adopt Python where Python was rejected at the proposal stage.

00:51:30.160 --> 00:51:32.960
You know, like, should we use Python for this project or something else?

00:51:32.960 --> 00:51:34.160
Oh, we need threading.

00:51:34.160 --> 00:51:35.340
We need computational threading.

00:51:35.340 --> 00:51:37.300
We've got, you know, 128 core.

00:51:37.440 --> 00:51:38.280
It's out, right?

00:51:38.280 --> 00:51:42.020
And then no one comes and complains about it because they never even started that process, right?

00:51:42.020 --> 00:51:48.700
So it'll either allow more people to come into Python or prevent them for leaving for that same argument on some projects.

00:51:48.700 --> 00:51:50.860
I think that's a pretty positive thing here.

00:51:50.860 --> 00:51:58.340
Yeah, there's, yeah, we don't get to count all of the projects that didn't come into existence because of the global interpreter lock.

00:51:58.340 --> 00:52:05.260
It's easy when you're in it to sort of adjust your thinking to not see the limitation anymore because you're so used to routing around it.

00:52:05.320 --> 00:52:08.000
You don't even stop and think, oh, man, I got to worry about threads.

00:52:08.000 --> 00:52:09.000
You just don't think threads.

00:52:09.000 --> 00:52:09.700
I totally agree.

00:52:09.700 --> 00:52:14.820
And I'll give people two other examples that maybe resonate more if this doesn't resonate with them.

00:52:14.820 --> 00:52:22.240
It's the, what if I said, oh, it's a little bit challenging to write this type of mobile phone application in Python.

00:52:22.240 --> 00:52:26.600
Like, well, it's nearly impossible to write a mobile phone application in Python.

00:52:26.600 --> 00:52:34.840
So we're not even focusing on that as an issue because no one is, I know, Beware and a few other things, there's a little bit of work.

00:52:34.900 --> 00:52:37.940
So I don't want to just, I don't want to like, I'm not trying to talk bad about them.

00:52:37.940 --> 00:52:45.520
But as a community, there's not like, it's not a React native sort of thing or a Flutter where there's a huge community of people who are just like, and we could do this.

00:52:45.520 --> 00:52:48.180
And then how do we, like, there's just not a lot of talk about it.

00:52:48.180 --> 00:52:53.440
And that doesn't mean that people wouldn't just love to write mobile apps in Python.

00:52:53.700 --> 00:53:01.460
It's just, it's so far out of reach that it's, it's just a little whisper in the corner for people trying to explore that rather than a big din.

00:53:01.460 --> 00:53:03.740
And I think, you know, same thing about desktop apps.

00:53:03.740 --> 00:53:11.120
Wouldn't it be awesome if we could not have Electron, but like some really cool, super nice UI thing that's almost pure Python?

00:53:11.120 --> 00:53:11.880
It would.

00:53:12.360 --> 00:53:18.280
But people were not focused on it because no one's trying to do it, but no one's trying to do it because there weren't good options to do it with.

00:53:18.280 --> 00:53:18.520
Right.

00:53:18.520 --> 00:53:22.380
And I think the same story is going to happen around performance and stuff with this.

00:53:22.660 --> 00:53:36.380
Just to jump in, you know, since I have to talk about the Beware folks, I mean, you've described exactly the reason why we fund the Beware development is because, yeah, if we don't work on that now before people sort of, there's a lot of work that has to do before you reach that point where it's easy.

00:53:36.380 --> 00:53:43.660
And so recently the team was able to get sort of tier three support for iOS and Android into CPython 313.

00:53:43.660 --> 00:53:48.540
So now we're at the first rung of the ladder of iOS and Android support in CPython.

00:53:48.540 --> 00:53:49.060
That's awesome.

00:53:49.180 --> 00:53:52.920
Poga and Briefcase, the two components of Beware are really focused again on that.

00:53:52.920 --> 00:53:53.220
Yeah.

00:53:53.220 --> 00:53:54.020
How do I make apps?

00:53:54.020 --> 00:53:55.640
How do I make for desktop and mobile?

00:53:55.640 --> 00:54:01.020
And so, but it's, yeah, we ran into people is that they just didn't even realize you could even think about doing that.

00:54:01.020 --> 00:54:06.240
And so they just, they never stopped to say, oh, I wish I could do this in Python because they just assumed you couldn't.

00:54:06.240 --> 00:54:12.120
And all the people who really needed to like were required to leave the ecosystem and make another choice.

00:54:12.120 --> 00:54:15.600
And it will take the same amount of, I was going to say, it takes the same amount of time with this.

00:54:15.700 --> 00:54:19.400
Even once threads are possible in Python, it'll take years to shift the perception.

00:54:19.400 --> 00:54:19.920
Yeah.

00:54:19.920 --> 00:54:21.720
And probably some of the important libraries.

00:54:21.720 --> 00:54:22.160
Yeah.

00:54:22.160 --> 00:54:22.520
Yeah.

00:54:22.520 --> 00:54:23.120
All right.

00:54:23.120 --> 00:54:25.220
So I'm pretty excited about this.

00:54:25.220 --> 00:54:28.360
I was hoping something like this would come and I didn't know what form it would be.

00:54:28.360 --> 00:54:33.700
I said there were the two limitations, the libraries and the culture, which you called out very awesomely.

00:54:33.760 --> 00:54:40.240
And then also the performance in the, this one is either neutral or a little bit better in terms of performance.

00:54:40.240 --> 00:54:45.160
So it doesn't have that disqualifying killing of the single thread of performance.

00:54:45.160 --> 00:54:51.220
The personal taking here, I will say again, because you have to be fairly conservative with CPython because so many people use it.

00:54:51.380 --> 00:54:56.480
Is that this will be an experimental option that by default, Python won't turn this on.

00:54:56.480 --> 00:55:00.880
You will have Python 3.13 when you get it, we'll still have the global interpreter lock.

00:55:00.880 --> 00:55:08.880
But if you build Python 3.13 yourself, or you get another kind of experimental build of it, there's a flag now at the build stage to turn off the GIL.

00:55:08.880 --> 00:55:13.020
So this is, in this mode, they decided to make, you know, not have to make double negatives.

00:55:13.020 --> 00:55:15.080
This is Python in free threading mode.

00:55:15.080 --> 00:55:22.940
And that will be an experimental thing for the community to test, to try out, to benchmark and do all these things for a number of years.

00:55:22.940 --> 00:55:29.460
They've taken a very measured approach and they're saying, we're not going to force the whole community to switch to this until it's proven itself out.

00:55:29.760 --> 00:55:39.280
Everyone's had time to port the major libraries, to try it out, to see that it really does meet the promise of not penalizing single threaded stuff too much.

00:55:39.280 --> 00:55:39.560
Yeah.

00:55:39.560 --> 00:55:41.800
Or breaking single threaded code too much.

00:55:41.800 --> 00:55:41.960
Yeah.

00:55:41.960 --> 00:55:42.260
Yeah.

00:55:42.260 --> 00:55:52.260
The steering council is reserving the right to decide when this becomes, or if this becomes the official way for Python, you know, I don't know, 3.17 or something.

00:55:52.260 --> 00:55:53.740
I mean, it could be, it could be several years.

00:55:53.740 --> 00:55:56.360
And so I just want everyone not to panic.

00:55:56.360 --> 00:55:57.860
Yeah, exactly.

00:55:57.860 --> 00:55:59.380
This doesn't get turned on in October.

00:55:59.380 --> 00:55:59.860
No.

00:55:59.860 --> 00:56:01.760
And this is super interesting.

00:56:01.760 --> 00:56:03.220
It's accepted.

00:56:03.220 --> 00:56:08.520
It only appears in your Python runtime if you build it with this.

00:56:08.520 --> 00:56:14.460
So I imagine, you know, some people will build it themselves, but someone will also just create a Docker container with Python built with this.

00:56:14.460 --> 00:56:17.900
And you can get the free threaded Docker version or whatever, right?

00:56:17.900 --> 00:56:19.660
We've already put out conda packages as well.

00:56:19.660 --> 00:56:24.280
So if you want to build a conda environment, you actually, if you jump over to the PyFreeThread page.

00:56:24.280 --> 00:56:24.660
Yeah.

00:56:24.660 --> 00:56:25.360
Tell people about this.

00:56:25.360 --> 00:56:25.540
Yeah.

00:56:25.540 --> 00:56:26.480
We didn't make this.

00:56:26.480 --> 00:56:28.000
This is the community made this.

00:56:28.000 --> 00:56:30.440
The scientific Python community put this together.

00:56:30.440 --> 00:56:39.560
And this is a really great resource, again, focused on, you know, that community, which really wants threading because we have a lot of, you know, heavy numerical computation.

00:56:40.120 --> 00:56:43.200
And so this is a good resource for things like how do you install it?

00:56:43.200 --> 00:56:46.760
So there's a link there on what are your options for installing the free threaded CPython.

00:56:46.760 --> 00:56:49.820
You can get it from Ubuntu or PyEnv or conda.

00:56:49.820 --> 00:56:53.100
If you go look at the, you know, and you can build it from source.

00:56:53.380 --> 00:56:53.980
Or get a container.

00:56:53.980 --> 00:56:54.260
Yeah.

00:56:54.260 --> 00:56:58.560
So these are, again, this is very focused on the kind of things the scientific Python community cares about.

00:56:58.560 --> 00:57:01.100
But, but these are things like, you know, have we ported Cython?

00:57:01.100 --> 00:57:02.300
Have we ported NumPy?

00:57:02.300 --> 00:57:03.980
Is it being automatically tested?

00:57:03.980 --> 00:57:05.020
Which release has it?

00:57:05.020 --> 00:57:13.280
And the nice thing actually is pip as of 24.1, I believe, can tell the difference between wheels for regular Python and free threaded Python.

00:57:13.500 --> 00:57:15.540
Oh, you can tell by the, there's different wheels as well.

00:57:15.540 --> 00:57:15.840
Yeah.

00:57:15.840 --> 00:57:21.960
So there's a, you know, Python has always had this thing called an ABI tag, which is just a letter that you stick after the version number.

00:57:21.960 --> 00:57:24.540
And T is the one for free threading.

00:57:24.540 --> 00:57:31.780
And so now you, a project can choose to upload wheels for both versions and make it easier for people to test out stuff.

00:57:31.780 --> 00:57:35.820
So for example, I mean, Cython, it looks like there are nightly wheels already being built.

00:57:35.820 --> 00:57:39.180
And so this is, they're moving fast and, you know, definitely.

00:57:39.180 --> 00:57:42.060
And our conda, we're also very interested in getting into this as well.

00:57:42.460 --> 00:57:44.700
So that's why we built the conda package for free threading.

00:57:44.700 --> 00:57:49.080
And we're going to start looking at building more conda packages for these things in order to be able to facilitate testing.

00:57:49.080 --> 00:57:56.380
Because I think the biggest thing we want to make sure is if you want to know if your code works, you want the quickest way to get an environment to have some place to test.

00:57:56.380 --> 00:58:00.680
And so making this more accessible to folks is a really high priority.

00:58:00.680 --> 00:58:01.260
This is cool.

00:58:01.260 --> 00:58:04.100
There was something like this for Python 2 to 3, I remember.

00:58:04.100 --> 00:58:07.720
It showed like the top 1,000 packages on PyPI.

00:58:08.040 --> 00:58:14.580
And how many of them were compatible with Python 3, basically by expressing their language tag or something like that.

00:58:14.580 --> 00:58:14.840
Yep.

00:58:14.840 --> 00:58:15.140
Yep.

00:58:15.140 --> 00:58:15.920
So this is kind of like that.

00:58:15.920 --> 00:58:17.560
It's also like the can I use.

00:58:17.560 --> 00:58:18.740
I don't know if you're familiar with that.

00:58:18.740 --> 00:58:22.400
Can I use from the web dev world?

00:58:22.580 --> 00:58:23.200
Oh, yeah.

00:58:23.200 --> 00:58:24.200
Oh, awesome.

00:58:24.200 --> 00:58:24.520
Yeah, yeah.

00:58:24.520 --> 00:58:24.980
I've seen this.

00:58:24.980 --> 00:58:26.600
You're going to say, I want to use this.

00:58:26.600 --> 00:58:27.680
I want to use this feature.

00:58:27.680 --> 00:58:34.260
And it'll, or, you know, if I want to say web workers or something like that, and then it'll, you can, it'll show you all the browsers and all the versions.

00:58:34.260 --> 00:58:35.360
And when were they supported?

00:58:35.360 --> 00:58:38.720
And this sounds a little bit like that, but for free threaded Python.

00:58:39.080 --> 00:58:41.660
Which, by the way, free threaded Python is the terminology, right?

00:58:41.660 --> 00:58:42.940
Not no gil, but free threaded.

00:58:42.940 --> 00:58:43.900
That is what they've decided.

00:58:43.900 --> 00:58:47.400
I think they're worried about people trying to talk about no, no gil or, I mean, I don't know.

00:58:47.400 --> 00:58:49.740
Gilful.

00:58:49.740 --> 00:58:51.220
Gilful Python.

00:58:51.220 --> 00:58:52.420
Are you running on a gilful?

00:58:52.420 --> 00:58:53.140
Yeah.

00:58:53.140 --> 00:58:54.260
Oh, my gosh.

00:58:54.260 --> 00:58:54.940
Okay.

00:58:54.940 --> 00:58:55.560
Interesting.

00:58:55.700 --> 00:58:59.080
Now, we have a few other things to talk about, but we don't have really much time to talk about them.

00:58:59.080 --> 00:59:04.260
But there was one thing that we were maybe going to talk about a bit with compiling.

00:59:04.260 --> 00:59:10.540
You said, you mentioned some talk or something where people were talking about, well, what if we had a static language Python and we compiled it?

00:59:10.540 --> 00:59:20.100
And related to that, kind of Mr. Magnetic says, could a Python program be compiled into a binary like a JAR or a, you know, Go app or whatever?

00:59:20.100 --> 00:59:24.020
There are other tools that look at that as a, yeah, a standalone executable.

00:59:24.420 --> 00:59:31.680
So, yeah, one of the things I just wanted to, you know, shout out a colleague of mine at Anaconda, Antonio Cuny, who is a well-known PyPy developer from long ago.

00:59:31.680 --> 00:59:32.940
He's worked on PyPy for 20 years.

00:59:32.940 --> 00:59:33.780
He's been working.

00:59:33.780 --> 00:59:36.840
And not the package installing thing, but the JIG compiler.

00:59:36.840 --> 00:59:37.260
PY, PY.

00:59:37.260 --> 00:59:38.420
PY, PY, PY.

00:59:38.420 --> 00:59:38.700
Yes.

00:59:38.700 --> 00:59:39.020
Yeah.

00:59:39.020 --> 00:59:41.860
Sometimes phonetically, like over audio, it's hard to tell.

00:59:41.860 --> 00:59:42.260
Yes.

00:59:42.260 --> 00:59:42.740
Yeah, yeah.

00:59:42.740 --> 00:59:44.840
So he's been thinking about this stuff for a very long time.

00:59:44.840 --> 00:59:51.600
His sort of key insight, at least clicked in my head, was that Python is hard to compile because it is so dynamic.

00:59:52.000 --> 00:59:58.640
I can, in principle, modify the attributes, like even the functions of a class at any point in the execution of the program.

00:59:58.640 --> 01:00:00.220
I can monkey patch anything.

01:00:00.220 --> 01:00:08.460
I can do this dynamicness is really great for making kind of magical metaprogramming libraries that do amazing things with very little typing.

01:00:08.660 --> 01:00:14.580
But it makes compiling them really hard because you don't get to ever say, okay, this can't ever change.

01:00:14.580 --> 01:00:24.220
And so what he's been trying to do with a project called Spy, which he gave a talk on at PyCon 2024, but I think the recordings aren't up yet for that.

01:00:24.220 --> 01:00:28.220
And so there isn't a, I don't think there's a public page on it, but he does have a talk on it.

01:00:28.220 --> 01:00:38.940
And because I think they've got like the key notes up, the key kind of insight for me for Spy was to recognize that in a typical Python program, all the dynamic metaprogramming happens at the beginning.

01:00:38.940 --> 01:00:42.920
You're doing things like data classes, generating stuff and all kinds of things like that.

01:00:43.180 --> 01:00:45.580
And then there's a phase where that stops.

01:00:45.580 --> 01:01:03.300
And so if we could define a sort of variant of Python where those two phases were really clear, then you would get all of the dynamic expressiveness, almost all the dynamic expressiveness of Python, but still have the ability to then feed that into a compiler tool chain and get a binary.

01:01:03.300 --> 01:01:12.300
This is super early R&D experimental work, but I think that's a really great way to approach it because often there's always been this tension of, well, if I make Python standardize.

01:01:12.300 --> 01:01:17.140
I make Python statically compilable, is it just, you know, C with, you know, different keywords?

01:01:17.140 --> 01:01:22.140
Do I lose the thing I loved about Python, which was how quickly I could express my idea?

01:01:22.140 --> 01:01:25.120
And so this is again to our, you know, having your cake and eating it too.

01:01:25.120 --> 01:01:31.000
This is trying to find a way to split that difference in a way that lets us get most of the benefits of both sides.

01:01:31.000 --> 01:01:31.860
That's pretty interesting.

01:01:31.860 --> 01:01:33.500
And hopefully that talks up soon.

01:01:33.500 --> 01:01:34.540
That'd be really neat.

01:01:34.540 --> 01:01:42.020
Maybe by the time this episode's out, I know the Python videos are starting to roll, like not out on YouTube, but out on, out on the podcast channels.

01:01:42.220 --> 01:01:45.820
It would be fantastic to have, here's my binary of Python.

01:01:45.820 --> 01:01:47.620
Take my data science app and run it.

01:01:47.620 --> 01:01:49.300
Take my desktop app and run it.

01:01:49.300 --> 01:01:51.200
I don't care what you have installed on your computer.

01:01:51.200 --> 01:01:57.620
I don't need you to set up Python 3.10 or higher on your machine and set up a virtual environment.

01:01:57.620 --> 01:01:59.360
Just here's my binary.

01:01:59.360 --> 01:02:00.960
Do it as you will.

01:02:00.960 --> 01:02:08.160
That's another, I throw that in with the mobile apps and the front end or the desktop apps or the front end Python.

01:02:08.500 --> 01:02:12.240
You know, that's another one of those things that it's nobody's pushing towards it.

01:02:12.240 --> 01:02:21.040
Not that many people are pushing towards it because there's not that many use cases for it that people are using it for because it was so challenging that people stopped trying to do that.

01:02:21.040 --> 01:02:21.320
You know?

01:02:21.480 --> 01:02:31.180
Yeah, there's one thing I also, you know, people probably hear me say this too many times, but the most people use apps when they use a computer, not packages or environments.

01:02:31.760 --> 01:02:44.380
And so in the Python space, we are constantly grappling with how hard packages and environments are to work with, talk with, you know, decide again, what languages are in, what, you know, do I care about everything or just Python or whatever?

01:02:44.380 --> 01:02:45.540
That's all very hard.

01:02:45.540 --> 01:02:48.200
But that's actually not how most people interact with the computer at all.

01:02:48.600 --> 01:02:51.120
And so it really is one of those things.

01:02:51.120 --> 01:02:55.840
Again, this is one of the reasons I'm so interested in Beware is Briefcase is like the app packager.

01:02:55.840 --> 01:02:59.200
And the more they can push on that, the more we have a story.

01:02:59.200 --> 01:03:02.440
And again, there are other tools that have been around for a long time, but that's just what I think about a lot.

01:03:02.440 --> 01:03:08.380
We need to focus on tools for making apps because that's how we're going to share our work with 99% of the earth.

01:03:09.480 --> 01:03:10.620
Yeah, 100%.

01:03:10.620 --> 01:03:11.220
I totally agree.

01:03:11.220 --> 01:03:23.520
And lots of props to Keith Russell McGee and the folks over at Beware for doing that and for you guys supporting that work because it's one of those things where there's not a ton of people trying to do it.

01:03:23.520 --> 01:03:26.560
It's not like, well, we're using Django, but is there another way we could do it?

01:03:26.560 --> 01:03:27.980
It's basically the same thing, right?

01:03:27.980 --> 01:03:34.380
It's creating a space for Python where it kind of, I know there's PyInstaller and PyTor app, but it's pretty limited, right?

01:03:34.380 --> 01:03:36.220
Yeah, there's not a lot of effort there.

01:03:36.220 --> 01:03:41.260
And so there are a few people who have been doing it for a long time and others are getting more into it.

01:03:41.260 --> 01:03:47.900
And yeah, so I just, yeah, I wish that we could get more focus on it because there are tools that just don't get a lot of attention.

01:03:47.900 --> 01:03:51.880
Yeah, and they're not very polished and there's so many edge cases and scenarios.

01:03:51.880 --> 01:03:52.940
All right, let's close it out.

01:03:52.940 --> 01:03:56.380
Let's just find a thought on this little topic and then let you wrap this up for us.

01:03:56.380 --> 01:03:59.160
Do you think that's maybe a core developer thing?

01:03:59.160 --> 01:04:06.280
I mean, I know it's awesome that Py2App and PyInstaller and PyFreeze are doing their things, that Toga are doing their things to try to make this happen.

01:04:06.280 --> 01:04:13.740
But I feel like they're kind of looking in at Python and go like, how can we grab what we need out of Python and jam it into an executable and make it work?

01:04:13.740 --> 01:04:20.260
Should we be encouraging the core developers to just go like a Python MyScript --Windows?

01:04:21.200 --> 01:04:23.060
And they're out, you get in .exe or something.

01:04:23.060 --> 01:04:24.880
I don't know, actually, that would be a great question.

01:04:24.880 --> 01:04:26.500
Actually, I would ask Russell that question.

01:04:26.500 --> 01:04:29.580
He would have probably better perspective than I would.

01:04:29.580 --> 01:04:34.180
At some level, it is a tool that is dealing with a lot of problems that aren't core to the Python language.

01:04:34.180 --> 01:04:41.660
And so maybe having it outside is helpful, but maybe there are other things that the core could do to support it.

01:04:41.660 --> 01:04:47.800
I mean, again, a lot of it has to do with the realities of when you drop an application onto a system, you need it to be self-contained.

01:04:47.800 --> 01:04:53.660
You need, sometimes you have to, you know, do you have to trick the import library to know where to find things and all of that?

01:04:53.660 --> 01:04:55.260
That's exactly what I was thinking is, right?

01:04:55.260 --> 01:05:07.420
If Python itself didn't require like operating system level fakes to make it think it is, if it could go like, here is a thing in memory where you just import, this is the import space.

01:05:07.420 --> 01:05:16.500
It's this memory address for these things, and we just run from the EXE rather than dump a bunch of stuff temporarily on disk, import it, throw it, you know, like that kind of weirdness that happens sometimes.

01:05:16.500 --> 01:05:20.920
There is probably definitely improvements that could be made to the import mechanism to support applications.

01:05:20.920 --> 01:05:21.680
Yeah, exactly.

01:05:21.680 --> 01:05:23.660
Well, we've planted that seed.

01:05:23.660 --> 01:05:24.780
Maybe it will grow.

01:05:24.780 --> 01:05:25.200
We'll see.

01:05:25.200 --> 01:05:27.480
All right, Stan, this has been an awesome conversation.

01:05:27.480 --> 01:05:36.640
You know, give us a wrap up on all this stuff, just like sort of find a call to action and summary of what you guys are doing in Anaconda, because there's a bunch of different stuff we talked about that are in this space.

01:05:36.640 --> 01:05:42.640
Yeah, I mean, mainly, I would say I would encourage people that if you want to speed up your Python program, you don't necessarily have to leave Python.

01:05:42.640 --> 01:05:44.360
Go take a look at some of these tools.

01:05:44.360 --> 01:05:47.500
Go, you know, measure what your program's doing.

01:05:47.500 --> 01:05:51.240
Look at tools like Numba, but there are other ones out there, you know, PyTorch and Jaxx and all sorts of things.

01:05:51.240 --> 01:05:54.920
There are lots of choices now for speed, and so Python doesn't have to be slow.

01:05:54.920 --> 01:05:59.000
You just have to sort of figure out what you're trying to achieve and find the best tool for that.

01:05:59.000 --> 01:06:01.220
Oh, one other thing I do want to shout out.

01:06:01.220 --> 01:06:09.580
I'm teaching a tutorial in a month over at the Anaconda sort of live tutorial system, which will be how to use Numba.

01:06:09.580 --> 01:06:15.720
So if something you saw here you want to go deep on, there will be a tutorial I hopefully linked in the show notes or something.

01:06:15.720 --> 01:06:17.220
Yeah, I can link that in the show notes.

01:06:17.220 --> 01:06:17.660
No problem.

01:06:17.660 --> 01:06:18.260
Absolutely.

01:06:18.740 --> 01:06:20.180
So I'll be going in.

01:06:20.180 --> 01:06:22.820
Is that the high performance Python with Numba?

01:06:22.820 --> 01:06:23.460
Yes.

01:06:23.460 --> 01:06:24.100
Yes.

01:06:24.100 --> 01:06:28.280
So, yeah, we'll be doing worked examples and you'll get to ask questions and all that stuff.

01:06:28.280 --> 01:06:28.520
Cool.

01:06:28.520 --> 01:06:30.720
I'll make sure to put that in the show notes so people can check it out.

01:06:30.720 --> 01:06:31.020
Mm-hmm.

01:06:31.020 --> 01:06:31.440
Cool.

01:06:31.440 --> 01:06:31.780
All right.

01:06:31.780 --> 01:06:37.800
Well, thanks for sharing all the projects that you guys are working on and just the broader performance stuff that you're tracking.

01:06:37.800 --> 01:06:38.320
Yeah, awesome.

01:06:38.320 --> 01:06:39.480
Glad to chat.

01:06:39.480 --> 01:06:39.840
You bet.

01:06:39.840 --> 01:06:40.460
See you later.

01:06:40.460 --> 01:06:43.800
This has been another episode of Talk Python To Me.

01:06:43.800 --> 01:06:45.600
Thank you to our sponsors.

01:06:45.600 --> 01:06:47.220
Be sure to check out what they're offering.

01:06:47.220 --> 01:06:48.640
It really helps support the show.

01:06:49.320 --> 01:06:53.060
This episode is sponsored by Posit Connect from the makers of Shiny.

01:06:53.060 --> 01:06:57.560
Publish, share, and deploy all of your data projects that you're creating using Python.

01:06:57.560 --> 01:07:04.140
Streamlit, Dash, Shiny, Bokeh, FastAPI, Flask, Quarto, Reports, Dashboards, and APIs.

01:07:04.140 --> 01:07:06.540
Posit Connect supports all of them.

01:07:06.540 --> 01:07:12.220
Try Posit Connect for free by going to talkpython.fm/posit, P-O-S-I-T.

01:07:12.220 --> 01:07:14.080
Want to level up your Python?

01:07:14.080 --> 01:07:18.140
We have one of the largest catalogs of Python video courses over at Talk Python.

01:07:18.580 --> 01:07:23.300
Our content ranges from true beginners to deeply advanced topics like memory and async.

01:07:23.300 --> 01:07:25.980
And best of all, there's not a subscription in sight.

01:07:25.980 --> 01:07:28.860
Check it out for yourself at training.talkpython.fm.

01:07:28.860 --> 01:07:30.980
Be sure to subscribe to the show.

01:07:30.980 --> 01:07:33.760
Open your favorite podcast app and search for Python.

01:07:33.760 --> 01:07:35.060
We should be right at the top.

01:07:35.060 --> 01:07:40.880
You can also find the iTunes feed at /itunes, the Google Play feed at /play, and the

01:07:40.880 --> 01:07:44.420
direct RSS feed at /rss on talkpython.fm.

01:07:44.840 --> 01:07:47.400
We're live streaming most of our recordings these days.

01:07:47.400 --> 01:07:51.520
If you want to be part of the show and have your comments featured on the air, be sure to

01:07:51.520 --> 01:07:55.240
subscribe to our YouTube channel at talkpython.fm/youtube.

01:07:55.240 --> 01:07:57.300
This is your host, Michael Kennedy.

01:07:57.300 --> 01:07:58.580
Thanks so much for listening.

01:07:58.580 --> 01:07:59.740
I really appreciate it.

01:08:00.000 --> 01:08:01.660
Now get out there and write some Python code.

01:08:01.660 --> 01:08:01.660
Now get out there and write some Python code.

01:08:01.660 --> 01:08:22.720
And I'll see you next time.

