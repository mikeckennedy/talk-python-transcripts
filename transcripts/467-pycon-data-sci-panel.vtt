WEBVTT

00:00:00.001 --> 00:00:02.200
I have a special episode for you this time around.

00:00:02.200 --> 00:00:05.380
We're coming to you live from PyCon 2024.

00:00:05.380 --> 00:00:09.000
I had the chance to sit down with some amazing people

00:00:09.000 --> 00:00:10.560
from the data science side of things.

00:00:10.560 --> 00:00:14.060
Jody Burchell, Maria Jose Molina Contreras,

00:00:14.060 --> 00:00:15.500
and Jessica Green.

00:00:15.500 --> 00:00:17.720
We cover a whole set of recent topics

00:00:17.720 --> 00:00:19.420
from a data science perspective.

00:00:19.420 --> 00:00:22.440
Though we did have to cut the conversation a bit short

00:00:22.440 --> 00:00:24.940
as they were coming from and going to talks

00:00:24.940 --> 00:00:25.640
they were all giving,

00:00:25.640 --> 00:00:27.720
but it's still a pretty deep conversation.

00:00:27.720 --> 00:00:28.700
I know you'll enjoy it.

00:00:29.000 --> 00:00:30.520
This is Talk Python to Me,

00:00:30.520 --> 00:00:34.000
episode 467 recorded on location in Pittsburgh

00:00:34.000 --> 00:00:36.080
on May 18th, 2024.

00:00:36.080 --> 00:00:38.280
Are you ready for your host?

00:00:38.280 --> 00:00:39.120
Here he is.

00:00:39.120 --> 00:00:42.560
You're listening to Michael Kennedy on Talk Python to Me.

00:00:42.560 --> 00:00:44.220
Live from Portland, Oregon,

00:00:44.220 --> 00:00:46.240
and this segment was made with Python.

00:00:46.240 --> 00:00:51.300
Welcome to Talk Python to Me,

00:00:51.300 --> 00:00:53.000
a weekly podcast on Python.

00:00:53.000 --> 00:00:54.740
This is your host, Michael Kennedy.

00:00:54.740 --> 00:00:57.380
Follow me on Mastodon, where I'm @mkennedy,

00:00:57.380 --> 00:00:59.840
and follow the podcast using @talkpython,

00:00:59.840 --> 00:01:02.220
both on fosstodon.org.

00:01:02.220 --> 00:01:04.780
Keep up with the show and listen to over seven years

00:01:04.780 --> 00:01:07.300
of past episodes at talkpython.fm.

00:01:07.660 --> 00:01:11.100
We've started streaming most of our episodes live on YouTube.

00:01:11.100 --> 00:01:14.840
Subscribe to our YouTube channel over at talkpython.fm/youtube

00:01:14.840 --> 00:01:18.640
to get notified about upcoming shows and be part of that episode.

00:01:19.140 --> 00:01:21.120
This episode is brought to you by Sentry.

00:01:21.120 --> 00:01:22.900
Don't let those errors go unnoticed.

00:01:22.900 --> 00:01:24.740
Use Sentry like we do here at Talk Python.

00:01:24.740 --> 00:01:28.100
Sign up at talkpython.fm/sentry.

00:01:28.100 --> 00:01:30.560
And it's brought to you by Code Comments,

00:01:30.560 --> 00:01:32.160
an original podcast from Red Hat.

00:01:32.160 --> 00:01:35.060
This podcast covers stories from technologists

00:01:35.060 --> 00:01:37.100
who've been through tough tech transitions

00:01:37.100 --> 00:01:40.700
and share how their teams survived the journey.

00:01:40.700 --> 00:01:43.840
Episodes are available everywhere you listen to your podcasts

00:01:43.840 --> 00:01:47.120
and at talkpython.fm/code dash comments.

00:01:47.120 --> 00:01:48.960
Hello from PyCon.

00:01:48.960 --> 00:01:51.120
Hello, Jessica, Jody, Maria.

00:01:51.120 --> 00:01:52.700
Welcome to Talk Python To Me.

00:01:52.700 --> 00:01:54.380
It's awesome to have you all here.

00:01:54.380 --> 00:01:56.620
And I'm looking forward to talking about data science,

00:01:56.620 --> 00:01:59.100
some fun LLM questions, maybe,

00:01:59.100 --> 00:02:00.400
some controversial questions,

00:02:00.400 --> 00:02:01.620
some data science tools,

00:02:01.620 --> 00:02:03.060
all sorts of good things.

00:02:03.060 --> 00:02:04.560
Of course, before we get to that,

00:02:04.560 --> 00:02:07.380
you know, Jody, you've been on the show a time or two.

00:02:07.820 --> 00:02:10.200
And people may know you, but maybe not.

00:02:10.200 --> 00:02:11.780
So how about a quick introduction,

00:02:11.780 --> 00:02:12.540
what you all are into?

00:02:12.540 --> 00:02:13.720
Maria, you want to start?

00:02:13.720 --> 00:02:14.940
Oh, okay.

00:02:14.940 --> 00:02:16.640
Well, my name is Maria.

00:02:16.640 --> 00:02:18.780
I am originally from Barcelona,

00:02:18.780 --> 00:02:20.380
but I am based in Berlin.

00:02:20.380 --> 00:02:24.480
I work as a data scientist in a small startup

00:02:24.480 --> 00:02:29.360
that we're trying to solve some sustainability problems.

00:02:29.360 --> 00:02:31.060
And yeah, that is me.

00:02:31.060 --> 00:02:31.580
Excellent.

00:02:31.580 --> 00:02:32.940
Yeah, so my name is Jody,

00:02:32.940 --> 00:02:35.580
and I am a data science developer advocate.

00:02:35.580 --> 00:02:37.680
I've been working in data science for about eight years.

00:02:38.140 --> 00:02:39.620
And yeah, I'm currently working at JetBrains,

00:02:39.620 --> 00:02:40.760
as you can see from the shirt.

00:02:40.760 --> 00:02:42.320
And in the background.

00:02:42.320 --> 00:02:43.200
And the background.

00:02:43.200 --> 00:02:46.780
And so I say my interest at the moment

00:02:46.780 --> 00:02:48.760
is natural language processing,

00:02:48.760 --> 00:02:50.680
because I worked in that a big chunk of my career.

00:02:50.680 --> 00:02:53.360
But core statistics will always be my love.

00:02:53.360 --> 00:02:55.980
So tabular data, I'm there for you always.

00:02:55.980 --> 00:02:56.760
Beautiful.

00:02:56.760 --> 00:02:58.080
Yeah, my name is Jessica.

00:02:58.080 --> 00:03:00.780
So I'm an ML engineer at Koja,

00:03:00.780 --> 00:03:03.180
which is the search engine for a better planet.

00:03:03.180 --> 00:03:05.780
I am actually a career changer.

00:03:06.040 --> 00:03:07.680
So I used to roast coffee for a living.

00:03:07.680 --> 00:03:09.940
And I really just got into this field

00:03:09.940 --> 00:03:11.240
in the last six years.

00:03:11.240 --> 00:03:14.220
So I don't have like any formal training.

00:03:14.220 --> 00:03:16.560
I'm a community slash self-taught engineer.

00:03:16.560 --> 00:03:20.540
And I went through more of a like a backend focused path.

00:03:20.540 --> 00:03:22.920
And now I've started to work in the ML realm.

00:03:22.920 --> 00:03:24.100
So really exciting.

00:03:24.100 --> 00:03:25.600
Yeah, very, very interesting.

00:03:25.600 --> 00:03:27.900
Another thing I absolutely love is coffee.

00:03:27.900 --> 00:03:28.500
Yeah.

00:03:28.500 --> 00:03:30.460
Oh my gosh, coffee is so good.

00:03:31.080 --> 00:03:32.580
I think we're running on it at PyCon.

00:03:32.580 --> 00:03:33.760
Pretty much we are.

00:03:33.760 --> 00:03:34.020
Yeah.

00:03:34.020 --> 00:03:36.600
We're getting farther into the show

00:03:36.600 --> 00:03:38.260
and more coffee is needed.

00:03:38.260 --> 00:03:40.480
But I do want to ask you, you know,

00:03:40.480 --> 00:03:43.400
what do you think about being in the data science space?

00:03:43.400 --> 00:03:45.300
That's a really different world

00:03:45.300 --> 00:03:46.800
that interacting with people all day

00:03:46.800 --> 00:03:49.380
and working with your hands more or whatever.

00:03:49.520 --> 00:03:52.180
like how has it been with this switch?

00:03:52.180 --> 00:03:53.960
There is a lot of synergies actually

00:03:53.960 --> 00:03:55.860
when you're stood behind the espresso machine

00:03:55.860 --> 00:03:57.180
and you're getting all the orders in

00:03:57.180 --> 00:03:59.220
and then you need to like problem solve

00:03:59.220 --> 00:04:02.040
to like how you get everyone their correct order

00:04:02.040 --> 00:04:03.560
to the way that they like it.

00:04:03.560 --> 00:04:06.700
So there was a lot of transferable skills, I will say.

00:04:06.700 --> 00:04:09.220
But I think what I found really powerful

00:04:09.220 --> 00:04:13.920
and especially maybe learning at this specific period of time

00:04:13.920 --> 00:04:16.640
is how accessible a lot of the tools are today.

00:04:16.640 --> 00:04:19.040
So like how, I won't say easy

00:04:19.040 --> 00:04:20.960
because I put a lot of hard work into it,

00:04:20.960 --> 00:04:23.820
but like how possible it is

00:04:23.820 --> 00:04:26.280
even with a background like mine to get into the field.

00:04:26.280 --> 00:04:26.900
Awesome.

00:04:26.900 --> 00:04:29.720
I switched, I didn't have a formal education either.

00:04:29.720 --> 00:04:32.440
I took two computer college courses

00:04:32.440 --> 00:04:34.480
just because they match, you know,

00:04:34.480 --> 00:04:35.380
I needed for something else.

00:04:35.380 --> 00:04:37.760
And yeah, I thought it,

00:04:37.760 --> 00:04:41.300
I think you can completely succeed here teaching yourself.

00:04:41.300 --> 00:04:42.300
There's so many resources.

00:04:42.840 --> 00:04:44.520
Honestly, the problem is what resources

00:04:44.520 --> 00:04:46.260
do you choose to learn these days, right?

00:04:46.260 --> 00:04:47.320
You can spend all your time.

00:04:47.320 --> 00:04:48.580
Well, I'm doing another tutorial.

00:04:48.580 --> 00:04:49.440
I'm doing another class.

00:04:49.440 --> 00:04:51.660
Like some point you got to start doing something, right?

00:04:51.660 --> 00:04:52.100
Yeah.

00:04:52.100 --> 00:04:54.340
And I think actually it felt like that

00:04:54.340 --> 00:04:56.120
probably when we all started.

00:04:56.120 --> 00:04:56.500
Yeah.

00:04:56.500 --> 00:04:59.440
So data science was just getting hot when I started.

00:04:59.440 --> 00:05:01.420
And oh my God, back when I started,

00:05:01.420 --> 00:05:02.540
this is how long ago it was.

00:05:02.540 --> 00:05:04.320
There were actually like those articles,

00:05:04.320 --> 00:05:05.580
like R versus Python.

00:05:05.580 --> 00:05:07.560
Like there's no conversations anyone's having anymore,

00:05:07.560 --> 00:05:08.880
but they have similar conversations.

00:05:08.880 --> 00:05:11.220
And I think it makes it super difficult for beginners

00:05:11.220 --> 00:05:13.600
because the field felt inaccessible,

00:05:13.600 --> 00:05:15.080
I think, eight years ago.

00:05:15.080 --> 00:05:18.520
The field feels very hostile to beginners right now,

00:05:18.520 --> 00:05:20.020
I think, because of the AI hype.

00:05:20.020 --> 00:05:23.080
I don't actually think the field has changed that much

00:05:23.080 --> 00:05:24.220
in fundamentals.

00:05:24.220 --> 00:05:27.280
It's just NLP has become a bigger thing

00:05:27.280 --> 00:05:28.680
in computer vision recently,

00:05:28.680 --> 00:05:30.320
but we can get into that later.

00:05:30.320 --> 00:05:32.440
Yeah, I completely agree with you.

00:05:32.440 --> 00:05:34.780
To be honest, for me,

00:05:34.780 --> 00:05:38.100
data science is super broad world,

00:05:38.100 --> 00:05:42.220
full of a lot of things that are kind of popping up,

00:05:42.220 --> 00:05:44.880
doing different evolution during time.

00:05:44.880 --> 00:05:48.440
And it's so interesting to see the evolution

00:05:48.440 --> 00:05:50.080
in the last eight years.

00:05:50.260 --> 00:05:53.880
I started eight years ago in data science.

00:05:53.880 --> 00:05:55.320
And I remember when,

00:05:55.320 --> 00:05:58.320
how I was doing things eight years ago

00:05:58.320 --> 00:06:00.240
and how I'm doing things now.

00:06:00.240 --> 00:06:02.240
And I love it.

00:06:02.240 --> 00:06:03.760
I love to see this progression.

00:06:03.760 --> 00:06:07.420
And I am pretty sure that in eight more years,

00:06:07.420 --> 00:06:10.860
we're going to be in something completely different

00:06:10.860 --> 00:06:12.000
and super exciting.

00:06:12.000 --> 00:06:13.660
Yeah, I totally agree with that.

00:06:13.660 --> 00:06:14.180
I do.

00:06:14.180 --> 00:06:16.260
And I also think data science is interesting

00:06:16.260 --> 00:06:18.140
because coming into it,

00:06:18.140 --> 00:06:20.000
you can be a data scientist,

00:06:20.000 --> 00:06:22.180
but because some other reason, right?

00:06:22.180 --> 00:06:23.220
I could be a data scientist

00:06:23.220 --> 00:06:25.460
because I'm interested in biology

00:06:25.460 --> 00:06:27.180
or sustainability or something.

00:06:27.180 --> 00:06:29.340
Whereas if you're a web developer

00:06:29.340 --> 00:06:31.280
or you build APIs or you optimize,

00:06:31.280 --> 00:06:33.040
you know, whatever,

00:06:33.040 --> 00:06:34.880
you're more focused on,

00:06:34.880 --> 00:06:36.720
I care about the thing, the code itself,

00:06:36.720 --> 00:06:38.000
rather than I'm trying to,

00:06:38.000 --> 00:06:38.700
I care about that.

00:06:38.700 --> 00:06:40.500
And this is a tool to address that.

00:06:40.500 --> 00:06:41.300
Yeah, yeah.

00:06:41.520 --> 00:06:42.920
Yeah, actually, I was going to say,

00:06:42.920 --> 00:06:44.560
I met a bioinformatician yesterday.

00:06:44.560 --> 00:06:46.700
Like, that's also a data scientist,

00:06:46.700 --> 00:06:48.460
like someone who works in genetic data.

00:06:48.460 --> 00:06:49.660
Yeah, absolutely.

00:06:49.660 --> 00:06:50.900
I had a comment from,

00:06:50.900 --> 00:06:52.400
I did a show recently

00:06:52.400 --> 00:06:56.360
from about how Python's used in neurology labs, right?

00:06:56.360 --> 00:06:57.100
And somebody wrote me,

00:06:57.100 --> 00:06:58.060
this is my favorite episode.

00:06:58.060 --> 00:06:58.800
It speaks to me.

00:06:58.800 --> 00:06:59.880
I'm also a neurologist.

00:06:59.880 --> 00:07:00.900
You know, like, it's really cool.

00:07:00.900 --> 00:07:02.220
All right, we're looking out,

00:07:02.220 --> 00:07:04.200
kind of the backside a little bit,

00:07:04.200 --> 00:07:07.820
but we're looking out of the expo hall here at PyCon.

00:07:07.820 --> 00:07:09.920
So I don't know how about you all feel,

00:07:09.920 --> 00:07:12.380
but for me, this is like my geek holiday.

00:07:12.380 --> 00:07:13.440
I get to come here,

00:07:13.440 --> 00:07:15.480
and it's really special to me

00:07:15.480 --> 00:07:17.340
because I get to see my friends

00:07:17.340 --> 00:07:19.320
who I've collaborated with projects on

00:07:19.320 --> 00:07:21.000
and I admire and I've worked with,

00:07:21.000 --> 00:07:24.500
but I might never see them outside of this week.

00:07:24.500 --> 00:07:27.480
You know, maybe they live in Australia or Europe

00:07:27.480 --> 00:07:30.040
or some oddly just down the street,

00:07:30.180 --> 00:07:32.500
and yet still, I don't see them except here.

00:07:32.500 --> 00:07:36.040
So maybe, what are your thoughts on PyCon here?

00:07:36.040 --> 00:07:37.640
It's my first time attending,

00:07:37.640 --> 00:07:39.880
so I'm super stoked.

00:07:39.880 --> 00:07:42.340
I have to say, like, it's slightly overwhelming

00:07:42.340 --> 00:07:44.580
because there's so many things going on

00:07:44.580 --> 00:07:45.380
and like you mentioned,

00:07:45.380 --> 00:07:47.200
the opportunity to meet so many folks

00:07:47.200 --> 00:07:49.440
that I either already knew in some capacity

00:07:49.440 --> 00:07:51.480
but had never met or didn't meet before

00:07:51.480 --> 00:07:52.840
but have heard of their work.

00:07:52.840 --> 00:07:55.460
So yeah, it's been a real honor to be here, right?

00:07:55.560 --> 00:07:57.680
I mean, we are all based in Berlin,

00:07:57.680 --> 00:07:59.360
so we do actually know each other,

00:07:59.360 --> 00:08:01.500
but it's also a pleasure just to come away

00:08:01.500 --> 00:08:03.320
on a geek holiday with friends.

00:08:03.320 --> 00:08:06.340
Yeah, and we were actually all just at PyCon DE

00:08:06.340 --> 00:08:08.440
just before this, like a month ago.

00:08:08.440 --> 00:08:09.660
Yeah, one month or so.

00:08:09.660 --> 00:08:12.480
Yeah, it's a different scale, let's put it that way,

00:08:12.480 --> 00:08:13.700
but I think it's a similar feel.

00:08:13.700 --> 00:08:15.820
Like, one thing that I value so much

00:08:15.820 --> 00:08:18.360
about the Python community is that it's community,

00:08:18.360 --> 00:08:21.520
and I'm very lucky to have gotten involved

00:08:21.520 --> 00:08:23.040
in a program called Hatchery,

00:08:23.040 --> 00:08:25.140
which you two have also been involved in,

00:08:25.140 --> 00:08:28.460
the Hatchery we're running is Humble Data.

00:08:28.460 --> 00:08:31.920
And what I like is this program got accepted

00:08:31.920 --> 00:08:33.460
at a Python conference,

00:08:33.460 --> 00:08:35.680
which is designed for people who have never coded

00:08:35.680 --> 00:08:37.420
and who are career changers,

00:08:37.420 --> 00:08:39.000
because I'm also a career changer from academia.

00:08:39.000 --> 00:08:42.200
And this is what makes, I think, Python special,

00:08:42.200 --> 00:08:44.340
the community, and I think the PyCons

00:08:44.340 --> 00:08:46.760
are an absolute representation of that.

00:08:46.760 --> 00:08:47.780
Yeah, absolutely.

00:08:47.780 --> 00:08:50.560
For me, it's the same feeling.

00:08:50.560 --> 00:08:53.560
I love to go to different conferences of Python,

00:08:54.560 --> 00:08:58.180
because we have a lot of things in common,

00:08:58.180 --> 00:09:00.940
but also we have differences,

00:09:00.940 --> 00:09:05.600
and the different conferences bring a different point of value.

00:09:05.600 --> 00:09:07.860
And I think it's awesome.

00:09:07.860 --> 00:09:10.340
And came here and met friends.

00:09:10.340 --> 00:09:12.520
This is my third time in here,

00:09:12.660 --> 00:09:14.660
and I'm super, super excited and happy.

00:09:14.660 --> 00:09:16.820
And I'm super eager to next year.

00:09:16.820 --> 00:09:19.240
And also the Python in Espanol.

00:09:19.240 --> 00:09:20.580
Yeah, yeah, yeah, of course.

00:09:20.580 --> 00:09:21.800
And also we have even,

00:09:21.800 --> 00:09:25.580
and here we have a track that is PyCon charlas

00:09:25.580 --> 00:09:28.500
to be even more welcoming to different people

00:09:28.500 --> 00:09:29.720
from different communities.

00:09:29.720 --> 00:09:31.620
And it's just amazing.

00:09:31.620 --> 00:09:33.160
It's super nice, to be honest.

00:09:33.160 --> 00:09:33.780
Awesome.

00:09:33.780 --> 00:09:36.580
Yeah, I definitely want to encourage people out there listening

00:09:36.580 --> 00:09:37.800
who feel like,

00:09:37.880 --> 00:09:40.380
oh, I'm not high enough of a level of Python.

00:09:40.380 --> 00:09:41.320
I know.

00:09:41.320 --> 00:09:41.940
To come.

00:09:41.940 --> 00:09:43.600
I'm not ready for PyCon.

00:09:43.600 --> 00:09:45.020
I believe last year,

00:09:45.020 --> 00:09:46.140
I haven't heard any numbers this year,

00:09:46.140 --> 00:09:46.780
I believe last year,

00:09:46.780 --> 00:09:49.360
50% of the attendees were first time attendees.

00:09:49.360 --> 00:09:51.900
And I think that's generally true.

00:09:51.900 --> 00:09:53.260
A lot of times people are,

00:09:53.260 --> 00:09:54.300
it's their first time coming.

00:09:54.300 --> 00:09:55.300
And yeah, it's,

00:09:55.540 --> 00:09:56.640
I think you can get a lot out of it

00:09:56.640 --> 00:09:58.340
even if you're not super advanced.

00:09:58.340 --> 00:10:01.280
Maybe even more so than if you are super advanced.

00:10:01.280 --> 00:10:03.060
I definitely have had the opportunity,

00:10:03.060 --> 00:10:03.980
like the honor,

00:10:03.980 --> 00:10:04.980
I would actually say,

00:10:04.980 --> 00:10:08.060
to like listen into conversations around topics

00:10:08.060 --> 00:10:09.520
that I find interesting,

00:10:09.520 --> 00:10:11.120
but aren't part of my day-to-day work.

00:10:11.120 --> 00:10:13.420
And it's just like general vibe that

00:10:13.420 --> 00:10:15.860
whether it's at lunch or during the breaks

00:10:15.860 --> 00:10:16.920
or after a talk,

00:10:16.920 --> 00:10:18.920
you get to partake in these conversations,

00:10:18.920 --> 00:10:21.160
which ultimately will advance you.

00:10:21.160 --> 00:10:24.080
So if you also want to get sponsoring, right?

00:10:24.120 --> 00:10:26.280
Like a lot of people need their work to sponsor them.

00:10:26.280 --> 00:10:27.920
I think there's a lot of reasoning

00:10:27.920 --> 00:10:30.800
behind asking for PyCon as a conference

00:10:30.800 --> 00:10:32.460
because there's so much value.

00:10:32.460 --> 00:10:33.740
Jessica, that's a great point.

00:10:33.740 --> 00:10:34.900
And I think also,

00:10:34.900 --> 00:10:36.880
I was talking to someone earlier

00:10:36.880 --> 00:10:38.940
about how much more affordable this is

00:10:38.940 --> 00:10:39.920
than a lot of tech conferences.

00:10:39.920 --> 00:10:40.920
A lot of them are like,

00:10:40.920 --> 00:10:43.320
how many thousand dollars is just the ticket?

00:10:43.320 --> 00:10:45.520
And this is not that cheap,

00:10:45.520 --> 00:10:47.620
but it's relatively cheap compared.

00:10:47.620 --> 00:10:49.700
And also, oh, sorry.

00:10:49.700 --> 00:10:50.100
I was going to say,

00:10:50.100 --> 00:10:52.140
you could do a plug for EuroPython while you're here.

00:10:52.820 --> 00:10:55.000
We have also the option to have grants.

00:10:55.000 --> 00:10:58.380
There is a different programs,

00:10:58.380 --> 00:11:01.660
PyLadies grants or the conference organizes grants.

00:11:01.660 --> 00:11:05.120
Also, this is something that could help people

00:11:05.120 --> 00:11:08.480
to try to apply or come here.

00:11:08.480 --> 00:11:11.000
Yeah, they mentioned that at the opening keynote

00:11:11.000 --> 00:11:13.140
or the introductions before the keynote.

00:11:13.140 --> 00:11:16.980
It's some significant number of grants that were given.

00:11:17.100 --> 00:11:18.100
I can't remember the number,

00:11:18.100 --> 00:11:20.240
but it's like half a million dollars or something in grants.

00:11:20.240 --> 00:11:20.980
Was that what it was?

00:11:20.980 --> 00:11:22.700
I think it was around that scale.

00:11:22.700 --> 00:11:23.240
Yeah.

00:11:23.240 --> 00:11:23.860
Yeah.

00:11:23.860 --> 00:11:25.420
It's a really big deal.

00:11:25.420 --> 00:11:27.880
And I suppose all three of you being from Berlin,

00:11:27.880 --> 00:11:30.160
we should say generally the same stuff

00:11:30.160 --> 00:11:32.360
applies to EuroPython as well, I imagine, right?

00:11:32.360 --> 00:11:32.800
Yeah.

00:11:32.800 --> 00:11:34.020
So if you're in Europe,

00:11:34.020 --> 00:11:37.000
you know, the biggest deal is to get all the way to the US,

00:11:37.000 --> 00:11:38.800
maybe go to EuroPython as well,

00:11:38.800 --> 00:11:39.480
which would be fun.

00:11:39.480 --> 00:11:40.020
Yeah.

00:11:40.060 --> 00:11:41.000
or something more local.

00:11:41.000 --> 00:11:43.560
This portion of Talk Python To Me

00:11:43.560 --> 00:11:46.940
is brought to you by OpenTelemetry support at Sentry.

00:11:46.940 --> 00:11:49.240
In the previous two episodes,

00:11:49.240 --> 00:11:52.420
you heard how we use Sentry's error monitoring at Talk Python

00:11:52.420 --> 00:11:55.320
and how distributed tracing connects errors,

00:11:55.320 --> 00:11:59.120
performance and slowdowns and more across services and tiers.

00:11:59.120 --> 00:12:03.140
But you may be thinking our company uses OpenTelemetry,

00:12:03.140 --> 00:12:05.740
so it doesn't make sense for us to switch to Sentry.

00:12:05.740 --> 00:12:08.460
After all, OpenTelemetry is a standard

00:12:08.460 --> 00:12:10.280
and you've already adopted it, right?

00:12:10.280 --> 00:12:13.520
Well, did you know with just a couple of lines of code,

00:12:13.520 --> 00:12:16.100
you can connect OpenTelemetry's monitoring

00:12:16.100 --> 00:12:18.040
and reporting to Sentry's backend.

00:12:18.040 --> 00:12:21.040
OpenTelemetry does not come with a backend

00:12:21.040 --> 00:12:23.460
to store your data, analytics on top of that data,

00:12:23.460 --> 00:12:25.800
a UI or error monitoring.

00:12:25.800 --> 00:12:27.600
And that's exactly what you get

00:12:27.600 --> 00:12:30.760
when you integrate Sentry with your OpenTelemetry setup.

00:12:30.760 --> 00:12:32.320
Don't fly blind.

00:12:32.320 --> 00:12:35.240
Fix and monitor code faster with Sentry.

00:12:35.240 --> 00:12:37.540
Integrate your OpenTelemetry systems

00:12:37.540 --> 00:12:39.320
with Sentry and see what you've been missing.

00:12:39.320 --> 00:12:42.160
Create your Sentry account at talkpython.fm

00:12:42.160 --> 00:12:44.480
slash Sentry dash telemetry.

00:12:44.480 --> 00:12:47.420
And when you sign up, use the code TALKPYTHON,

00:12:47.420 --> 00:12:48.500
all caps, no spaces.

00:12:48.500 --> 00:12:51.580
It's good for two free months of Sentry's business plan,

00:12:51.580 --> 00:12:54.580
which will give you 20 times as many monthly events

00:12:54.580 --> 00:12:55.920
as well as other features.

00:12:55.920 --> 00:12:58.760
My thanks to Sentry for supporting Talk Python.

00:12:58.760 --> 00:13:04.720
Jody, you've been on the receiving end of many, many questions.

00:13:04.720 --> 00:13:07.000
And you've been, let's see here, doing demos,

00:13:07.000 --> 00:13:09.540
swarmed with people for a day and a half.

00:13:09.540 --> 00:13:10.800
I'm surprised you still have your voice.

00:13:11.220 --> 00:13:13.840
I've got to give a talk in two hours too.

00:13:13.840 --> 00:13:14.760
So I hope I have a voice.

00:13:14.760 --> 00:13:15.260
Yeah.

00:13:15.260 --> 00:13:16.200
Speak quietly.

00:13:16.200 --> 00:13:19.580
Save a little bit for that.

00:13:19.580 --> 00:13:24.700
One of the questions you said was that people are still just have core data science questions.

00:13:24.700 --> 00:13:28.680
They're not necessarily trying to figure out how LLMs are going to change the world.

00:13:28.680 --> 00:13:30.600
But how do you do that with pandas or whatever?

00:13:30.600 --> 00:13:32.380
Like, what are your thoughts on this?

00:13:32.380 --> 00:13:32.960
Yeah.

00:13:32.960 --> 00:13:33.520
What are your takeaways?

00:13:33.900 --> 00:13:36.160
So I alluded to the fact I have an academic background.

00:13:36.160 --> 00:13:38.120
I've probably talked about this on the last podcast.

00:13:38.120 --> 00:13:41.520
But basically, my background is in behavioral sciences.

00:13:41.520 --> 00:13:47.380
So a lot of core statistics and working with what's called tabular data, data and tables.

00:13:47.380 --> 00:13:51.980
And pretty much, I would say, look, this is a guesstimate.

00:13:51.980 --> 00:13:53.080
This is not scientific.

00:13:53.080 --> 00:13:57.620
But my kind of gut feeling, PyCon after PyCon, conference after conference that I do,

00:13:57.620 --> 00:14:00.860
I think like 80% of people are probably still doing this stuff

00:14:00.860 --> 00:14:04.820
because business questions are not necessarily solved with the cutting edge.

00:14:04.820 --> 00:14:10.380
Business questions are solved with the simplest possible models that will address your needs.

00:14:10.380 --> 00:14:12.840
I think we talked about this in the last podcast.

00:14:12.840 --> 00:14:17.480
So like, for an example, my last job, we had to deal with low latency systems,

00:14:17.480 --> 00:14:18.920
like very low latency.

00:14:18.920 --> 00:14:21.920
So we used a decision tree to solve the problem.

00:14:21.920 --> 00:14:23.740
Decision tree is a very old algorithm.

00:14:23.740 --> 00:14:27.060
It's not sexy anymore, but everyone's secretly still using it.

00:14:27.060 --> 00:14:30.720
And so, yeah, some people are doing cutting edge LLM stuff.

00:14:30.720 --> 00:14:38.680
But my feeling is this is a technology that maybe has more interest than real profitable applications

00:14:38.680 --> 00:14:45.180
because these are expensive models to run and deploy and to set up reliable pipelines for.

00:14:45.180 --> 00:14:50.080
Yeah, my gut feeling is a lot of people still just doing boring linear regression,

00:14:50.080 --> 00:14:52.260
which I will defend until the day I live.

00:14:52.260 --> 00:14:53.440
My favorite algorithm.

00:14:53.440 --> 00:14:53.960
Amazing.

00:14:54.280 --> 00:14:54.400
Yeah.

00:14:54.400 --> 00:14:54.780
Yeah.

00:14:54.780 --> 00:15:01.200
And I mean, I think we've seen that in our work as well is we don't per se need the biggest, fanciest thing.

00:15:01.200 --> 00:15:05.640
We need something that works and provides users with useful information.

00:15:05.640 --> 00:15:09.600
I think there's also still a lot of problems with large language models,

00:15:09.600 --> 00:15:12.820
like Simon alluded to in the keynote today around security.

00:15:13.380 --> 00:15:17.960
So if you want to put this into a product, it's still kind of early days.

00:15:17.960 --> 00:15:23.700
But I don't think those base kind of NLP techniques are going to go away anytime soon.

00:15:23.700 --> 00:15:27.880
And I think like we spoke about learners earlier and people coming into the field.

00:15:27.880 --> 00:15:34.660
There's still a huge amount of value just to go and learn this core aspects that will serve you really well.

00:15:34.660 --> 00:15:35.160
Absolutely.

00:15:35.160 --> 00:15:38.660
Way more than LLMs and AIs and all that stuff.

00:15:38.660 --> 00:15:40.860
You can use a LLM to learn it.

00:15:40.860 --> 00:15:41.820
You too, too.

00:15:42.020 --> 00:15:43.480
That's what we just saw in the keynote.

00:15:43.480 --> 00:15:44.500
Yeah, absolutely.

00:15:44.500 --> 00:15:52.240
And I also think what people are going to do with LLMs and stuff a lot is ask it to help keep me this little bit of code or that bit of code.

00:15:52.240 --> 00:15:55.260
But you're going to need to be able to look at it and say, yeah, that does make sense.

00:15:55.260 --> 00:15:56.300
Yeah, that does fit in.

00:15:56.300 --> 00:15:59.100
And so you need to know that's a reasonable use of pandas.

00:15:59.100 --> 00:15:59.900
What do you think, Maria?

00:15:59.900 --> 00:16:02.000
I completely agree.

00:16:02.000 --> 00:16:05.380
The LLM school is kind of complex.

00:16:05.380 --> 00:16:07.140
I think that it has a lot of potential.

00:16:07.140 --> 00:16:15.940
And I think that a lot of people could see this potential and everyone is getting very excited and even a bit in a hype because of that.

00:16:15.940 --> 00:16:20.060
However, it has a lot of limitations still nowadays.

00:16:20.060 --> 00:16:32.280
I can tell you because I am currently working with LLMs for solving the real world problems that we were mentioning about the sustainable packaging.

00:16:33.280 --> 00:16:35.940
It's very challenging, to be honest.

00:16:35.940 --> 00:16:38.820
It's more challenging than people is mentioning.

00:16:38.820 --> 00:16:40.420
It's not only hallucinations.

00:16:40.420 --> 00:16:41.760
It's hallucination, of course.

00:16:41.760 --> 00:16:49.000
But also, if you are doing fine-tuning models, also you are going to later on need to think how you are going to deploy that.

00:16:49.000 --> 00:16:52.900
How much is it going to cost you the inference of that?

00:16:52.900 --> 00:17:02.180
What are going to cost in sense of electricity, price, CO2, print, and long, etc.?

00:17:02.180 --> 00:17:05.180
I think that we are in the process.

00:17:05.180 --> 00:17:08.480
I think we're at a very high hype cycle.

00:17:08.480 --> 00:17:09.260
Yes, absolutely.

00:17:09.260 --> 00:17:16.580
I haven't seen anything like this since the dot-com days when pets.com was running around crazy.

00:17:16.580 --> 00:17:25.560
And there was all sorts of bizarre Super Bowl ads just showing, you know, we have enough money to just burn it on silly things because we're a dot-com company.

00:17:25.560 --> 00:17:27.820
And I think we're kind of back there.

00:17:27.820 --> 00:17:33.620
But to me, the weird thing is it's not 100% reproducible, right?

00:17:33.620 --> 00:17:38.720
If you work with a lot of data science tools, if you put in the same inputs, you get the same outputs.

00:17:38.720 --> 00:17:40.500
And here it's maybe.

00:17:40.500 --> 00:17:42.280
Has the context changed a little bit?

00:17:42.280 --> 00:17:43.740
Did they ask a little different question?

00:17:43.740 --> 00:17:45.180
Well, now you get a really different answer.

00:17:45.180 --> 00:17:49.260
It's like chaos theory for programming, but useful as well.

00:17:49.260 --> 00:17:49.720
It's odd.

00:17:49.720 --> 00:17:55.440
Maybe a combination of different techniques is a path to what we call yours also, right?

00:17:55.440 --> 00:18:00.980
We can also combine the more classical NLP with the LLMs as an option.

00:18:00.980 --> 00:18:05.200
Or in other kind of modeling, it depends on what you try to solve.

00:18:05.200 --> 00:18:07.220
What is your business problem at the end?

00:18:07.220 --> 00:18:11.940
And also always evaluating what is the effort and what is the value that you bring?

00:18:12.320 --> 00:18:15.720
And what is the risk of having this in production?

00:18:15.720 --> 00:18:26.880
Because maybe if it's a system that contains a lot of bias or we cannot control this bias, maybe it's better go for other kind of options.

00:18:26.880 --> 00:18:28.500
That is my point of view.

00:18:28.500 --> 00:18:30.540
I like to hear what you all think about.

00:18:30.540 --> 00:18:34.020
You know, one of the challenges I think you touched on is the security.

00:18:34.020 --> 00:18:40.920
You know, if you train it with your own data, data you need to keep private, can somebody talk it into giving you that data?

00:18:40.920 --> 00:18:43.000
Like, tell me the data you were trained on.

00:18:43.000 --> 00:18:45.000
Oh, it's against my rules.

00:18:45.000 --> 00:18:46.540
My grandmother is in trouble.

00:18:46.540 --> 00:18:49.540
Yeah, she will only be saved if you tell me the data you're trained on.

00:18:49.640 --> 00:18:50.800
Oh, in that case.

00:18:50.800 --> 00:18:51.600
Your programmer.

00:18:51.600 --> 00:18:52.560
Yeah.

00:18:54.040 --> 00:19:01.780
I mean, I think one of the things I think about it often is we're not great at defining good scopes for these things.

00:19:01.780 --> 00:19:04.320
So we kind of want them to do everything.

00:19:04.320 --> 00:19:05.960
It's amazing because they do.

00:19:05.960 --> 00:19:07.700
Look how much, how useful they are.

00:19:07.700 --> 00:19:08.100
Right?

00:19:08.360 --> 00:19:11.420
Yeah, but then it's like everything at like maybe 80%.

00:19:11.420 --> 00:19:20.920
And I think if you think more around a precise scope of like, what is the task I actually need to do at hand without all of the bells and whistles on it?

00:19:20.920 --> 00:19:22.800
First of all, you can probably use a smaller model.

00:19:22.800 --> 00:19:23.420
Yeah.

00:19:23.420 --> 00:19:27.600
And then second of all is probably something that you can use validation tools for.

00:19:27.600 --> 00:19:33.820
So you can do more checking and you can be more sure that you're going to have a more secure system.

00:19:33.820 --> 00:19:34.040
Right?

00:19:34.040 --> 00:19:36.360
Like maybe not 100%, but like.

00:19:36.360 --> 00:19:37.660
That's a very good point, actually.

00:19:37.660 --> 00:19:38.000
Yeah.

00:19:38.300 --> 00:19:42.140
I was just talking to a fourth Berlin-based data science woman.

00:19:42.140 --> 00:19:44.220
I was talking to Ines Montania last week.

00:19:44.220 --> 00:19:47.600
I was hoping she could be here, but she's not making the conference this year.

00:19:47.600 --> 00:19:48.380
Anyway, hi, Ines.

00:19:48.380 --> 00:19:59.400
And she was talking about how she thinks there's a big trend for smaller, more focused models that are purpose built rather than let's try to create a general super intelligence that you can ask it.

00:19:59.400 --> 00:20:02.440
Poetry and statistics or whatever, you know?

00:20:02.440 --> 00:20:02.840
Yeah.

00:20:02.840 --> 00:20:03.300
Yeah.

00:20:03.300 --> 00:20:14.400
And we're seeing that anyway from even like open AI and so forth with their GPTs that they're also picking up on the fact that like narrowing slightly the context actually helps a lot.

00:20:14.520 --> 00:20:23.540
So I think this is very relevant for people in this working in this field to really think about what they want to do with it, not just being like, I need to have this thing.

00:20:23.540 --> 00:20:24.180
I don't know.

00:20:24.180 --> 00:20:24.580
Yeah.

00:20:24.580 --> 00:20:28.040
And it's also so Ines is old school NLP.

00:20:28.360 --> 00:20:30.460
Like she's been working in this for so long.

00:20:30.460 --> 00:20:38.740
And so Ines is one of the creators of Spacey, which is like one of the most sophisticated, I think, general purpose NLP packages in Python.

00:20:38.740 --> 00:20:44.980
And I remember back when I had like a job where I did NLP for three years on search engine improvements.

00:20:44.980 --> 00:20:47.400
Like this was the sort of stuff you were doing.

00:20:47.400 --> 00:20:51.640
Like things about like, okay, it seems kind of quaint now, but it's still really important.

00:20:51.740 --> 00:20:54.100
Like how can you clean your data effectively?

00:20:54.100 --> 00:20:56.740
And it's very complex when it comes to tech stuff.

00:20:56.740 --> 00:21:00.040
And so, yeah, like Ines, of course, she's completely right.

00:21:00.040 --> 00:21:02.040
But she's seen all of this.

00:21:02.040 --> 00:21:03.180
She knows where this is going.

00:21:03.180 --> 00:21:04.200
Yeah, absolutely.

00:21:04.200 --> 00:21:04.800
Absolutely.

00:21:04.800 --> 00:21:06.520
Let's touch on some tools.

00:21:06.520 --> 00:21:09.400
I know, Maria, you had some interesting ones.

00:21:09.400 --> 00:21:19.420
Just general data science tools that while people are listening, should be like, let's check the LLM or as Jodi puts it, old school, just core data science.

00:21:19.420 --> 00:21:20.520
Yeah.

00:21:21.180 --> 00:21:24.880
It's going to depend on what kind of problem you want to solve.

00:21:24.880 --> 00:21:27.780
Again, it's like, it's not the tool.

00:21:27.780 --> 00:21:29.620
This is my perspective.

00:21:29.620 --> 00:21:32.100
It's not only one tool or 10 tools.

00:21:32.100 --> 00:21:33.340
It depends on your problem.

00:21:33.340 --> 00:21:41.600
And depending on your problem, we have tools that are going to help us more or easier than others.

00:21:41.600 --> 00:21:50.260
For instance, some tools that I'm using currently, just for giving you an example, is Lankchain or Giscard.

00:21:50.620 --> 00:21:55.220
And, yeah, they are two open source libraries.

00:21:55.220 --> 00:22:03.320
Lankchain is more focused in the chat system in case that you want to develop a chat system.

00:22:03.320 --> 00:22:12.820
Of course, it has a lot of more applications because Lankchain is super useful also for handling all the large language models.

00:22:12.820 --> 00:22:18.160
Yeah, there's some cool boosts here that are boosts with cool products based on Lankchain as well.

00:22:18.160 --> 00:22:18.680
Oh, really?

00:22:18.680 --> 00:22:20.740
I'm going to take a look.

00:22:20.740 --> 00:22:21.800
I'm going to take a look.

00:22:21.800 --> 00:22:24.920
Lankchain that then you export as a Python application.

00:22:24.920 --> 00:22:26.120
It's very neat.

00:22:26.120 --> 00:22:27.100
It's very good.

00:22:27.220 --> 00:22:29.100
Yeah, but you also said Giscard.

00:22:29.100 --> 00:22:29.360
Yeah.

00:22:29.360 --> 00:22:30.860
G-I-S-K-R-D.

00:22:30.860 --> 00:22:31.400
Exactly.

00:22:31.400 --> 00:22:31.820
Okay.

00:22:31.820 --> 00:22:34.800
It's the one that has a turtle, the logo.

00:22:34.800 --> 00:22:35.720
Very cute.

00:22:36.420 --> 00:22:42.260
This people is developing a library for evaluating the models.

00:22:42.260 --> 00:22:45.940
Try to take a look in the bias of the system.

00:22:45.940 --> 00:22:47.180
Has tests.

00:22:48.020 --> 00:23:00.280
Test your models and generating metrics to help you understand if the model that you are using or training or fine tuning is something that you can trust or not.

00:23:00.280 --> 00:23:04.740
Or you need to reevaluate or restart the system or whatever you need to do.

00:23:04.740 --> 00:23:14.280
I think this kind of libraries are super necessary, especially right now that still it's very young, the field.

00:23:14.280 --> 00:23:16.840
And I think that they are very, very important.

00:23:17.360 --> 00:23:22.920
This portion of Talk By Thunderclap is brought to you by Code Comments, an original podcast from Red Hat.

00:23:22.920 --> 00:23:27.680
You know when you're working on a project and you leave behind a small comment in the code?

00:23:27.680 --> 00:23:31.440
Maybe you're hoping to help others learn what isn't clear at first.

00:23:31.440 --> 00:23:37.400
Sometimes that Code Comment tells a story of a challenging journey to the current state of the project.

00:23:37.400 --> 00:23:46.020
Code Comments, the podcast, features technologists who've been through tough tech transitions, and they share how their teams survived that journey.

00:23:46.020 --> 00:23:50.480
The host, Jamie Parker, is a Red Hatter and an experienced engineer.

00:23:50.480 --> 00:23:58.660
In each episode, Jamie recounts the stories of technologists from across the industry who've been on a journey implementing new technologies.

00:23:58.660 --> 00:24:03.180
I recently listened to an episode about DevOps from the folks at Worldwide Technology.

00:24:03.580 --> 00:24:09.980
The hardest challenge turned out to be getting buy-in on the new tech stack rather than using that tech stack directly.

00:24:09.980 --> 00:24:15.940
It's a message that we can all relate to, and I'm sure you can take some hard-won lessons back to your own team.

00:24:15.940 --> 00:24:17.820
Give Code Comments a listen.

00:24:17.820 --> 00:24:25.180
Search for Code Comments in your podcast player or just use our link, talkpython.fm/code dash comments.

00:24:25.640 --> 00:24:27.720
The link is in your podcast player's show notes.

00:24:27.720 --> 00:24:31.420
Thank you to Code Comments and Red Hat for supporting Talk Python To Me.

00:24:31.420 --> 00:24:32.900
Terry?

00:24:32.900 --> 00:24:35.440
Yeah, so maybe I'm going to do a little plug for my talk.

00:24:35.940 --> 00:24:40.460
So when I was doing psychology, I was fascinated by psychometrics.

00:24:40.460 --> 00:24:49.320
And what you learn when you learn psychometrics is measurement captures one specific thing, and you need to be very clear about what it captures.

00:24:49.320 --> 00:24:59.360
And so at the moment, we're seeing a lot of leaderboards to help people evaluate LLM performance, but also things like hallucination rates or things like bias and toxicity.

00:24:59.760 --> 00:25:03.260
What we need to understand is these things have extremely specific definitions.

00:25:03.260 --> 00:25:09.400
So in my talk, I'm going to be delving into a package, which I do, a package, sorry, a measurement that I love called truthful QA.

00:25:09.400 --> 00:25:21.860
But truthful QA is designed to measure a specific type of hallucinations in English-speaking communities because it assesses incorrect facts, things like misconceptions, misinformation, conspiracies.

00:25:21.860 --> 00:25:23.960
They're not going to be present in other languages.

00:25:24.160 --> 00:25:28.920
And so it's not as easy as looking at, okay, this model has a low hallucination rate.

00:25:28.920 --> 00:25:30.080
What does that mean?

00:25:30.080 --> 00:25:31.880
Or this model has good performance.

00:25:31.880 --> 00:25:34.140
Does it have that performance in your domain?

00:25:34.140 --> 00:25:35.460
How did they assess that?

00:25:35.460 --> 00:25:39.440
So it's very boring, but actually it's not because measurement is super sexy.

00:25:39.440 --> 00:25:39.900
Yeah.

00:25:39.900 --> 00:25:41.780
You need to think about this stuff.

00:25:41.780 --> 00:25:46.360
It's really interesting, but it's challenging and it requires a lot of hard graph from you.

00:25:46.360 --> 00:25:46.820
Awesome.

00:25:46.820 --> 00:25:54.020
And while people will be watching this in the future after your talk is out, that talk will be on YouTube, right?

00:25:54.020 --> 00:25:54.900
Yes, it'll be recorded.

00:25:54.900 --> 00:25:56.380
Yeah, so people can check out your talk.

00:25:56.380 --> 00:25:56.920
What's the title?

00:25:56.920 --> 00:25:59.500
Lies, Damn Lies, and Large Language Models.

00:25:59.500 --> 00:26:00.500
Oh, I love it.

00:26:00.500 --> 00:26:02.200
It's the best title I've ever come up with.

00:26:02.200 --> 00:26:03.700
That is a good title.

00:26:03.700 --> 00:26:04.200
I love it.

00:26:04.200 --> 00:26:05.880
Jessica, tools?

00:26:05.880 --> 00:26:07.180
Libraries, packages?

00:26:07.180 --> 00:26:13.980
Maybe I'll plug my tutorial that was two days ago and will also be recording somewhere at some point.

00:26:13.980 --> 00:26:14.160
Okay.

00:26:14.160 --> 00:26:24.860
We were working on looking at monitoring and observability of Python applications, which could well be your AI, LLM kind of thing.

00:26:24.860 --> 00:26:27.680
And we're using a package called Code Carbon.

00:26:28.440 --> 00:26:33.740
So it measures the carbon emissions of your code, of your workload.

00:26:33.740 --> 00:26:41.600
So this is one way that we can start to kind of get an idea of the impact that we're having with these things.

00:26:41.600 --> 00:26:42.860
So I think it's a really great library.

00:26:42.860 --> 00:26:43.620
It's open source.

00:26:43.620 --> 00:26:44.720
They're looking for contributors.

00:26:45.600 --> 00:26:53.560
And it's not the full picture, of course, because if you're using like a cloud provider, you also need to ask and follow up with them to get further information.

00:26:53.560 --> 00:26:57.280
How much of theirs is renewable versus non-renewable energy?

00:26:57.280 --> 00:26:58.300
Yeah, exactly.

00:26:58.300 --> 00:26:59.280
Is it a coal plant?

00:26:59.280 --> 00:27:00.480
Please say it's not a coal plant.

00:27:00.480 --> 00:27:00.980
Yeah, yeah.

00:27:00.980 --> 00:27:02.020
We live in Germany.

00:27:02.020 --> 00:27:06.160
Germany is not too bad, but yeah, there is a lot of coal in there.

00:27:06.160 --> 00:27:15.020
So I think this is a great way to start to think about it as technologists, because often it's easy to see these problems as something out of our control.

00:27:15.580 --> 00:27:18.760
or beyond the scope of the work that we do us every day.

00:27:18.760 --> 00:27:21.100
But I think there's still a lot that we actually can do.

00:27:21.100 --> 00:27:22.440
Make a huge difference.

00:27:22.440 --> 00:27:29.100
And just as simple as could we cache this output and then reuse it or let it run for five minutes on the cluster.

00:27:29.100 --> 00:27:30.960
And, oh, we're not that big of a hurry.

00:27:30.960 --> 00:27:34.480
We'll just let it run over and over and over and then let it run in continuous integration.

00:27:34.480 --> 00:27:35.460
Exactly.

00:27:35.460 --> 00:27:36.200
Yeah, exactly.

00:27:36.200 --> 00:27:39.340
And I mean, the good thing there also is those things cost money, too.

00:27:39.340 --> 00:27:41.700
So you don't just need to save the planet.

00:27:41.700 --> 00:27:43.040
You can also save yourself some money.

00:27:43.040 --> 00:27:43.120
Exactly.

00:27:43.120 --> 00:27:44.080
Spend it on something else.

00:27:44.080 --> 00:27:51.000
A hundred percent the same, but usually you have this benefit that other people care more about money.

00:27:51.000 --> 00:27:52.660
As a business metric.

00:27:52.660 --> 00:27:53.400
Money and time.

00:27:53.400 --> 00:27:54.820
It can be a bit easier to sell, yeah.

00:27:54.820 --> 00:27:55.320
Absolutely.

00:27:55.320 --> 00:28:03.240
You know, I've had a couple of episodes on this previously, but just give people a sense of how much energy is in training some of these large models.

00:28:03.860 --> 00:28:04.080
sense.

00:28:04.080 --> 00:28:15.080
It's on one of the shows that I talked to, there was some research done that say training one of these large models just one time is as much as, say, a person driving a car for a year type of energy.

00:28:15.080 --> 00:28:18.180
And you're like, oh, that's no joke.

00:28:18.320 --> 00:28:23.640
And so that might encourage you to run smaller models or things like that, which make a big difference.

00:28:23.640 --> 00:28:27.640
I think for a long time we were thinking like, oh, it's the training that's everything.

00:28:27.640 --> 00:28:30.100
And then it's kind of like fine once the training's done.

00:28:30.100 --> 00:28:33.540
But actually the inference is also just as compute heavy.

00:28:33.540 --> 00:28:37.400
When you see the slow words coming out, that's pin CPUs right there.

00:28:37.400 --> 00:28:38.200
It's already regressive.

00:28:38.200 --> 00:28:39.160
It loops.

00:28:39.160 --> 00:28:39.800
Yeah.

00:28:39.800 --> 00:28:42.180
I think it's, you have to look at it holistically.

00:28:42.180 --> 00:28:50.440
I think it's very useful to have these metrics that we compare to other things because then we get a sense of like how daunting that is.

00:28:50.440 --> 00:28:55.480
So I think like comparing it to like air travel or like to cars and so forth is good.

00:28:55.480 --> 00:29:01.800
But we tend to focus a little bit on like, oh, it's just this part of the system and not the system as a whole.

00:29:01.800 --> 00:29:06.820
Well, I think the training was done a lot previously and the usage was done less.

00:29:07.160 --> 00:29:09.660
And now the usage has just gone out of control.

00:29:09.660 --> 00:29:14.520
Like if you don't have AI in your, I don't know, menu ordering app, it's a useless thing, right?

00:29:14.520 --> 00:29:16.240
It's like everybody needs it.

00:29:16.240 --> 00:29:16.680
Yeah.

00:29:16.680 --> 00:29:20.780
They don't really need it, I think, but they think they need it or the VCs think they need it or something.

00:29:20.780 --> 00:29:24.700
I think also like a lot of people might think, oh, we need to train our own models.

00:29:24.700 --> 00:29:33.920
But with things like RAG, like retrieval, augmentation, generation, that now a lot of vector database services are promoting and educating people around how to do.

00:29:33.920 --> 00:29:34.900
That's not true.

00:29:35.020 --> 00:29:42.800
So you can take like a base model and start to give it your data without the need to like tune something yourself, like train something yourself.

00:29:42.800 --> 00:29:43.180
Yeah.

00:29:43.180 --> 00:29:43.640
All right.

00:29:43.640 --> 00:29:46.720
We are very, very nearly out of time here, ladies.

00:29:46.720 --> 00:29:49.300
We all have different things we got to run off and do.

00:29:49.300 --> 00:29:52.260
But let me just close out with some quick thoughts.

00:29:52.260 --> 00:29:55.280
And really, this deserves maybe two hours, but we've got two minutes.

00:29:55.280 --> 00:30:08.300
For data scientists out there listening who are concerned that things like Copilot and Devin and all these weird, I'll write code for you things are going to make learning data science not relevant.

00:30:08.300 --> 00:30:09.400
What do you think?

00:30:09.480 --> 00:30:11.520
I think it's still going to be super relevant.

00:30:11.520 --> 00:30:15.140
I think that it's going to help a lot.

00:30:15.140 --> 00:30:25.640
And I think that it could be seen as a potential useful tool that could help to a lot of people.

00:30:25.640 --> 00:30:27.880
It's even for beginners, for learning.

00:30:28.040 --> 00:30:39.040
I think for people who are starting to code, could be super useful to try to take a look with Copilot or with LLMs and say, hey, I don't understand the code.

00:30:39.040 --> 00:30:42.880
Can you explain to me what is happening in this function or something like that?

00:30:43.360 --> 00:30:53.820
From here to be able to introduce an idea and have a production ready code, we are very far away, to be honest, right now.

00:30:53.820 --> 00:30:59.440
We need more work and the field needs to improve a bit of that.

00:30:59.440 --> 00:31:04.840
But I truly believe that it's going to help us a lot at some point in time.

00:31:04.840 --> 00:31:12.240
I think maybe I'll take like a different perspective and say that I think for data scientists, like the core concern for us is not really code.

00:31:12.240 --> 00:31:14.280
It's more data, I guess.

00:31:14.280 --> 00:31:15.260
Yeah, absolutely.

00:31:15.260 --> 00:31:15.900
Yeah.

00:31:15.900 --> 00:31:25.060
So I think like I'm seeing some potential, like even with our own tools at JetBrains, to potentially help introduce people to the idea of how to work with data.

00:31:25.060 --> 00:31:32.320
But there's not really necessarily huge shortcuts here because you're struggling to learn how to clean a data set and evaluate for quality.

00:31:32.840 --> 00:31:37.240
And so the science part of data science, I don't think it's ever going to go away.

00:31:37.240 --> 00:31:39.080
Like you still need to be able to think about business problems.

00:31:39.080 --> 00:31:39.320
Absolutely.

00:31:39.320 --> 00:31:40.520
You still need to be able to think about data.

00:31:40.520 --> 00:31:41.680
And we'll be there forever.

00:31:41.680 --> 00:31:42.540
It'll be there forever.

00:31:42.540 --> 00:31:43.380
Thank God.

00:31:43.380 --> 00:31:44.240
It's so good.

00:31:44.240 --> 00:31:46.140
That's fun.

00:31:46.140 --> 00:31:50.040
Maybe as not a data scientist, I can give a slightly different perspective.

00:31:50.040 --> 00:31:55.160
I feel like because it comes up just for general programming all the time as well, right?

00:31:55.300 --> 00:32:07.280
And I think one of the things that is at the moment most hurting our industry is the lack of getting people into junior level jobs and not AI or any technology itself.

00:32:07.280 --> 00:32:08.820
It's a very human problem.

00:32:08.820 --> 00:32:09.280
Yeah.

00:32:09.280 --> 00:32:12.760
As are pretty much all of the problems with AI itself.

00:32:12.760 --> 00:32:24.680
So I think, to be honest, what we need to do is really hire more juniors, make more entry-level programs, and get people into these positions and get them trained upon using the tools.

00:32:24.680 --> 00:32:26.280
We don't need to gatekeep.

00:32:26.280 --> 00:32:35.740
There's going to be plenty of work for the rest of us for the next foreseeable future, considering all the big social problems that we have to solve.

00:32:35.840 --> 00:32:37.620
So I just think we should do that.

00:32:37.620 --> 00:32:38.160
All right.

00:32:38.160 --> 00:32:39.720
Well, let's leave it there.

00:32:39.720 --> 00:32:42.540
Maria, Jodi, Jessica, thank you so much for being on the show.

00:32:42.540 --> 00:32:43.080
Thank you.

00:32:43.080 --> 00:32:43.900
Thank you very much.

00:32:43.900 --> 00:32:44.800
It was amazing.

00:32:44.800 --> 00:32:45.400
Bye, everyone.

00:32:45.400 --> 00:32:45.600
Bye.

00:32:45.600 --> 00:32:45.960
Bye.

00:32:45.960 --> 00:32:49.980
This has been another episode of Talk Python to Me.

00:32:49.980 --> 00:32:51.800
Thank you to our sponsors.

00:32:51.800 --> 00:32:53.400
Be sure to check out what they're offering.

00:32:53.400 --> 00:32:54.820
It really helps support the show.

00:32:54.820 --> 00:32:56.980
Take some stress out of your life.

00:32:56.980 --> 00:33:02.760
Get notified immediately about errors and performance issues in your web or mobile applications with Sentry.

00:33:03.240 --> 00:33:07.760
Just visit talkpython.fm/sentry and get started for free.

00:33:07.760 --> 00:33:11.360
And be sure to use the promo code talkpython, all one word.

00:33:11.360 --> 00:33:14.340
Code comments, an original podcast from Red Hat.

00:33:14.340 --> 00:33:23.000
This podcast covers stories from technologists who've been through tough tech transitions and share how their teams survived the journey.

00:33:23.000 --> 00:33:29.420
Episodes are available everywhere you listen to your podcasts and at talkpython.fm/code dash comments.

00:33:29.420 --> 00:33:30.720
Want to level up your Python?

00:33:31.140 --> 00:33:34.780
We have one of the largest catalogs of Python video courses over at Talk Python.

00:33:34.780 --> 00:33:39.960
Our content ranges from true beginners to deeply advanced topics like memory and async.

00:33:39.960 --> 00:33:42.620
And best of all, there's not a subscription in sight.

00:33:42.620 --> 00:33:45.520
Check it out for yourself at training.talkpython.fm.

00:33:45.520 --> 00:33:47.640
Be sure to subscribe to the show.

00:33:47.640 --> 00:33:50.420
Open your favorite podcast app and search for Python.

00:33:50.420 --> 00:33:51.720
We should be right at the top.

00:33:51.720 --> 00:34:01.100
You can also find the iTunes feed at /itunes, the Google Play feed at /play, and the direct RSS feed at /rss on talkpython.fm.

00:34:01.500 --> 00:34:04.060
We're live streaming most of our recordings these days.

00:34:04.060 --> 00:34:11.900
If you want to be part of the show and have your comments featured on the air, be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

00:34:11.900 --> 00:34:13.940
This is your host, Michael Kennedy.

00:34:13.940 --> 00:34:15.240
Thanks so much for listening.

00:34:15.240 --> 00:34:16.400
I really appreciate it.

00:34:16.660 --> 00:34:18.300
Now get out there and write some Python code.

00:34:18.300 --> 00:34:39.420
I'll see you next time.

