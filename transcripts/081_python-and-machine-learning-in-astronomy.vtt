WEBVTT

00:00:00.001 --> 00:00:06.740
The advances in astronomy over the past century are both evidence of and confirmation of the highest heights of human ingenuity.

00:00:06.740 --> 00:00:11.100
We have learned by studying the frequency of light that the universe is expanding.

00:00:11.100 --> 00:00:16.660
By observing the orbit of Mercury, that Einstein's theory of general relativity is correct.

00:00:16.660 --> 00:00:22.360
It probably won't surprise you to learn that Python and data science play a central role in modern-day astronomy.

00:00:22.360 --> 00:00:28.080
This week, you'll meet Jake Vanderplass, an astrophysicist and data scientist from the University of Washington.

00:00:28.280 --> 00:00:31.820
Join Jake and me while we discuss the state of Python in astronomy.

00:00:31.820 --> 00:00:37.300
This is Talk Python To Me, episode 81, recorded October 21st, 2016.

00:00:56.560 --> 00:01:09.920
Welcome to Talk Python To Me, a weekly podcast on Python, the language, the libraries, the ecosystem, and the personalities.

00:01:09.920 --> 00:01:12.040
This is your host, Michael Kennedy.

00:01:12.040 --> 00:01:14.020
Follow me on Twitter where I'm @mkennedy.

00:01:14.020 --> 00:01:17.900
Keep up with the show and listen to past episodes at talkpython.fm.

00:01:17.900 --> 00:01:20.420
And follow the show on Twitter via at Talk Python.

00:01:20.940 --> 00:01:25.980
I'm very excited to announce this episode is sponsored by not one, but two new sponsors.

00:01:25.980 --> 00:01:28.980
And they both have excellent offerings for Python developers.

00:01:28.980 --> 00:01:32.920
Welcome GoCD by ThoughtWorks and DataSchool to the show.

00:01:32.920 --> 00:01:34.940
Thank you both for supporting the show.

00:01:34.940 --> 00:01:37.500
Jake, welcome to Talk Python.

00:01:37.500 --> 00:01:38.440
Thanks.

00:01:38.440 --> 00:01:39.080
Good to be here.

00:01:39.080 --> 00:01:40.600
Yeah, it's great to have you.

00:01:40.600 --> 00:01:43.880
I'm a huge fan of astronomy and science.

00:01:44.140 --> 00:01:49.860
And I'd love to talk to you about how Python and astronomy interact and all the problems you're solving.

00:01:49.860 --> 00:01:52.080
But before we get to those, let's start with your story.

00:01:52.080 --> 00:01:54.000
How did you get into programming in Python in the first place?

00:01:54.000 --> 00:01:57.620
Well, you know, I came to programming relatively late.

00:01:57.620 --> 00:02:05.520
I had a little bit of early experience in like sixth grade with Hyperscript, HyperCard, but didn't do much.

00:02:05.520 --> 00:02:20.020
I took a small programming class in high school, and I didn't really do much programming aside from evaluating physics stuff and Mathematica, simple things, until I was in grad school, actually.

00:02:20.020 --> 00:02:29.520
So I arrived at grad school and started working with a research scientist who later became a faculty.

00:02:29.520 --> 00:02:36.220
And I asked him, hey, this was around 2006, I asked him, hey, what programming language should I use?

00:02:36.220 --> 00:02:41.380
Most of the people around were using IDL, this interactive data language.

00:02:41.380 --> 00:02:47.600
It's a proprietary scripting language that's similar to MATLAB or Python in some ways.

00:02:47.600 --> 00:02:53.720
And he was one of the only people using Python at the time, and he said, well, you should use Python.

00:02:53.720 --> 00:02:54.420
That's the future.

00:02:54.420 --> 00:02:56.060
Everyone's going to be doing that soon.

00:02:56.380 --> 00:03:00.680
And so I decided to do it, and I learned Python.

00:03:00.680 --> 00:03:04.280
I taught myself Python over winter break.

00:03:04.280 --> 00:03:12.060
Sudoku was big at the time, so I wrote a Sudoku solver, and that was my way of learning how to do control flow and everything in Python.

00:03:12.060 --> 00:03:13.680
Yeah, that's great.

00:03:13.680 --> 00:03:22.100
I really think writing those little games like that are a great way to learn a language, or at least get started with it, right?

00:03:22.140 --> 00:03:24.940
Because the problems are not so complicated.

00:03:24.940 --> 00:03:26.480
There's not a lot of interaction.

00:03:26.480 --> 00:03:27.980
It's not like, well, how do I talk to a database?

00:03:27.980 --> 00:03:29.000
How do I do a UI?

00:03:29.000 --> 00:03:30.160
How do I call the web?

00:03:30.160 --> 00:03:31.160
Yeah.

00:03:31.160 --> 00:03:33.040
It was super fun.

00:03:33.040 --> 00:03:41.640
And being someone who didn't really have a background, any formal background in algorithms, it was a nice way to wrap my head around what sort of problems you can do in programming.

00:03:42.480 --> 00:03:46.020
So pretty much Python from then on, you've been doing a lot of stuff.

00:03:46.020 --> 00:03:51.020
You've been contributing to some machine learning libraries, to the whole Scikit area.

00:03:51.020 --> 00:03:52.400
Yeah.

00:03:52.400 --> 00:03:56.960
So where that came in is basically I started doing all this work in Python.

00:03:56.960 --> 00:04:04.160
I was writing horrible little one-off scripts like most scientists do who don't have formal training.

00:04:04.880 --> 00:04:09.020
And a couple years into my PhD program, I wrote my first paper.

00:04:09.020 --> 00:04:12.060
And the first paper was pretty interesting.

00:04:12.060 --> 00:04:19.920
It was using this relatively new at the time algorithm, a version of manifold learning called locally linear embedding.

00:04:19.920 --> 00:04:23.720
It was using that to explore some astronomical spectra.

00:04:24.100 --> 00:04:25.580
This algorithm was implemented out there.

00:04:25.580 --> 00:04:30.940
The paper introducing it had a link to a little tarball of MATLAB code.

00:04:30.940 --> 00:04:43.140
But I found pretty quickly that the code didn't scale to the size of the problem we had, which was hundreds of thousands or millions of spectra in several thousand dimensions.

00:04:43.960 --> 00:04:56.680
And so I spent a summer basically looking at this science and nature paper, looking at this MATLAB software, and trying to figure out how to write a more scalable version of the algorithm.

00:04:56.680 --> 00:05:01.160
And what came out of that was this C++ package.

00:05:01.160 --> 00:05:08.440
And I published the paper and did this standard thing of putting the C++ package in a tarball on my website.

00:05:08.440 --> 00:05:11.940
And I thought to myself, this is ridiculous.

00:05:12.280 --> 00:05:22.500
And the next person who tries to use this in astronomy is going to have to spend, you know, they're going to have to hire another grad student to spend a summer and figure out how to implement this.

00:05:22.500 --> 00:05:30.400
So I started asking people about how to make sure that your code can be used by other people.

00:05:30.400 --> 00:05:36.080
And then I found out there's this whole catchphrase called reproducibility, open science and things like that.

00:05:36.080 --> 00:05:39.920
So that was my foray into reproducibility and open science.

00:05:40.140 --> 00:05:49.460
And as I was asking around, someone mentioned that there was this brand new package that might be interested in that algorithm called scikit-learn.

00:05:49.460 --> 00:05:59.620
And so I got in touch with Gael Varaku, who was getting scikit-learn off the ground, and they thought it would be a good contribution.

00:05:59.620 --> 00:06:06.480
So I started, I think that was 2010 or somewhere around there, I started contributing to scikit-learn when I was really young.

00:06:06.480 --> 00:06:09.040
And, you know, I haven't looked back.

00:06:09.040 --> 00:06:21.300
I've really been turned on by this idea of open and reproducible science by making sure your software products that come out of your research are actually well-documented and reusable.

00:06:21.300 --> 00:06:29.120
And this thing that was sort of a side project in the beginning has turned into most of what I do during my day-to-day work.

00:06:29.120 --> 00:06:32.540
Isn't it funny how life takes those kinds of turns?

00:06:32.540 --> 00:06:39.360
Like you plan to do one thing and you discover another and it really becomes something you're passionate about.

00:06:39.360 --> 00:06:39.700
That's cool.

00:06:39.700 --> 00:06:49.040
Yeah, and it turns out now I'm way more excited by general software tools than I am about the astronomy research that drew me into grad school.

00:06:49.040 --> 00:06:51.900
Yeah, it sounds a lot like my story.

00:06:51.900 --> 00:06:52.380
That's awesome.

00:06:52.380 --> 00:06:57.280
Let's talk just really high level for just a moment about machine learning.

00:06:57.280 --> 00:07:03.240
I know a lot of people out there are into data science and they know machine learning, but there's all sorts of listeners.

00:07:03.240 --> 00:07:09.140
So, I mean, like how people used to solve problems and they would use like statistics or other types of things.

00:07:09.140 --> 00:07:13.400
But then this whole machine learning seemed to formalize it, bring some algorithms together.

00:07:13.400 --> 00:07:16.600
Like kind of give us an overview of what the whole story there is.

00:07:16.600 --> 00:07:25.360
Yeah, so whenever I introduce machine learning, I always emphasize the fact that it's just a set of models to data.

00:07:25.360 --> 00:07:29.540
You know, when you fit a line to data, you're doing machine learning.

00:07:29.540 --> 00:07:37.700
When you take two clumps on a two-dimensional plot and draw a line between them to say this side is one type and that side is the other type,

00:07:37.860 --> 00:07:40.520
that's a form of machine learning.

00:07:40.520 --> 00:07:47.320
And where machine learning gets powerful is these algorithms that you can do by eye or by hand in two dimensions,

00:07:47.320 --> 00:07:49.460
like drawing a line on a piece of paper.

00:07:49.460 --> 00:07:55.520
Once you formalize those algorithms, you can scale them up to a large number of points and large number of dimensions.

00:07:55.520 --> 00:07:58.080
You said you have a thousand dimensions in your previous problem.

00:07:58.080 --> 00:07:59.520
Like you're not doing that by eye, right?

00:07:59.520 --> 00:08:00.180
Yeah, yeah.

00:08:00.180 --> 00:08:07.940
So, you know, a thousand dimensions, the equivalent is fitting a 999-dimensional hyperplane to split things into two groups.

00:08:07.940 --> 00:08:09.420
And you can't really do that by eye.

00:08:09.960 --> 00:08:19.860
But the key is that machine learning is nothing more complicated than fitting these models to data in a way that scales to large data sets and to high-dimensional data sets.

00:08:19.860 --> 00:08:25.260
And, of course, it grew out of artificial intelligence and statistics in some sense.

00:08:25.260 --> 00:08:32.800
But I think the core distinction between the machine learning way of doing things and the statistics way of doing things,

00:08:32.800 --> 00:08:42.520
this distinction is put together, is described really well in this paper by Leo Briman called Statistical Modeling, the Two Cultures.

00:08:42.520 --> 00:08:50.420
The overview summary of that is that, you know, in classic statistics, you're building models where you care about the model parameters.

00:08:50.420 --> 00:08:56.580
Like you fit a line to the data and the slope is telling you something fundamental about the world.

00:08:56.580 --> 00:08:59.900
Whereas in machine learning, you fit a line to data.

00:08:59.900 --> 00:09:02.180
And you're not so much interested in the slope.

00:09:02.180 --> 00:09:09.180
You're just interested in what that line can tell you about new data that you, you know, you want to predict something about.

00:09:09.180 --> 00:09:10.380
I see.

00:09:10.380 --> 00:09:13.900
So a lot of machine learning is about prediction in the future.

00:09:13.900 --> 00:09:16.320
Like you create a model and then you want to ask it questions.

00:09:16.320 --> 00:09:17.360
Yeah, absolutely.

00:09:17.360 --> 00:09:20.020
And you're learning something about unknown data.

00:09:20.020 --> 00:09:27.600
I mean, the distinction is not completely black and white there, but I think it's a useful way to think about machine learning versus statistics.

00:09:27.600 --> 00:09:29.780
Yeah, that is an interesting way to put it.

00:09:29.780 --> 00:09:32.680
What are some of the major tools in Python that people use?

00:09:32.680 --> 00:09:36.060
Yeah, so scikit-learn is one of them.

00:09:36.060 --> 00:09:43.780
This is the Python package that's built on NumPy and SciPy and kind of uses the classic tools.

00:09:43.780 --> 00:09:51.380
It's really nice for doing sort of small to medium scale machine learning and modeling problems.

00:09:52.100 --> 00:09:56.220
It doesn't have a particularly good scalability story.

00:09:56.220 --> 00:10:01.920
There are some ways to kind of parallelize certain operations within scikit-learn.

00:10:01.920 --> 00:10:07.360
But if you want to go to out-of-core data and things like that, there are other ways to do this.

00:10:07.620 --> 00:10:15.380
So scikit-learn, to be honest, is for the bulk of what I end up doing in my work, I can use scikit-learn.

00:10:15.380 --> 00:10:25.200
And for scaling to large data sets, often in the work I'm doing, I'm doing kind of massively parallel stuff where I can split the data into chunks

00:10:25.200 --> 00:10:33.700
and run a small scikit-learn algorithm on one of those chunks and loop through them that way or parallelize them that way.

00:10:33.700 --> 00:10:34.460
Yeah, that makes sense.

00:10:34.460 --> 00:10:38.660
So maybe you're looking at some large part of the sky and you could break it into little grids or something.

00:10:38.660 --> 00:10:39.360
Yeah, exactly.

00:10:39.360 --> 00:10:44.200
I'm often looking at things object by object rather than trying to do things all at once.

00:10:44.200 --> 00:10:54.040
If you need to do larger models that are doing things all at once, there are these interesting libraries recently built around things like Spark and TensorFlow.

00:10:54.040 --> 00:11:00.160
TensorFlow. I'm not as experienced with those, but the TensorFlow stuff is interesting.

00:11:00.160 --> 00:11:10.100
And in particular, there's this SKFlow package that I've been rather intrigued by that kind of builds a scikit-learn API around a TensorFlow backend.

00:11:10.100 --> 00:11:12.700
Oh, that sounds like that's worth looking into. That sounds cool.

00:11:12.700 --> 00:11:16.120
And there's also PySpark, which is interesting.

00:11:16.120 --> 00:11:19.840
So where I'm in the process right now, it's been kind of fun.

00:11:19.840 --> 00:11:35.560
I'm working with some computer scientists and some neuroimaging people and some database specialists to put together comparison between a number of Python-oriented approaches to doing scalable computation in a scientific setting.

00:11:36.940 --> 00:11:40.240
And so hopefully that paper will be coming out in the next several months.

00:11:40.240 --> 00:11:41.920
Oh, yeah. That sounds really interesting.

00:11:41.920 --> 00:11:49.060
Give us some examples of how this whole machine learning story applies to astronomy.

00:11:49.060 --> 00:11:53.540
So, like, what types of problems or things are people doing with this?

00:11:53.540 --> 00:12:00.640
Yeah, so astronomy, we have a lot of areas where we want to predict certain aspects of things.

00:12:00.640 --> 00:12:08.780
So one example, just to be concrete, is let's say we're looking for the distances to distant galaxies.

00:12:08.780 --> 00:12:19.160
And the distances or red shifts of galaxies are important in constraining things about our understanding about the cosmology of the universe, the structure of the universe.

00:12:19.160 --> 00:12:25.240
But getting an accurate distance to a galaxy is an expensive observation.

00:12:25.240 --> 00:12:39.760
You have to do a spectral observation, which basically, you know, you look at an individual object and you split the light from that object using, like, a diffraction grading into its whole spectrum.

00:12:39.760 --> 00:12:43.520
You know, red on one end, blue on the other end, and a thousand bins in between.

00:12:43.520 --> 00:12:52.080
And given something like that, you can isolate certain emission lines or absorption lines and calculate its red shift, which is similar to its distance.

00:12:52.080 --> 00:12:53.660
And that's really, really accurate.

00:12:53.660 --> 00:13:01.420
But the problem is it's incredibly expensive because you have to look at individual objects and line up these diffraction gratings one by one.

00:13:01.420 --> 00:13:08.560
And when we're looking at, when we're just taking pictures of the sky, we're getting thousands or millions of galaxies a night.

00:13:08.560 --> 00:13:13.280
And it's, and we don't have the resources to take a spectrum of all of those.

00:13:13.720 --> 00:13:31.800
So the question then is, can you, can you take a small set of objects where you have these very detailed spectral observations and learn something about them so that you can predict what the red shift might be from a more coarse photometric kind of, kind of picture observation of them.

00:13:32.460 --> 00:13:35.900
And this maps pretty well onto a machine learning model, right?

00:13:35.900 --> 00:13:43.040
You have, you, you take a picture of the whole sky, and so you get data about each object that way at a coarse level.

00:13:43.040 --> 00:13:51.120
And then you take spectra of a certain collection of objects, and that gives you finer detail, more information about a subset of them.

00:13:51.380 --> 00:13:58.180
And then you want to build a model that can predict that, you know, that more information, the red shift and the distance for, for all the rest of them.

00:13:58.180 --> 00:14:04.820
So at first glance, machine learning seems to map pretty well onto astronomy data.

00:14:04.820 --> 00:14:17.440
The thing that's difficult about it in practice is most machine learning models assume some sort of statistical similarity between your training set and your unknown set.

00:14:18.240 --> 00:14:24.400
And in astronomy, unless you specifically design it that way, it's difficult to get that statistical similarity.

00:14:24.400 --> 00:14:31.060
So for example, we tend to have spectra of nearby bright objects because they're easier to take spectra for.

00:14:31.060 --> 00:14:38.620
If you're looking at distant, faint objects, the noise characteristics are different, the statistical distributions are different.

00:14:38.620 --> 00:14:46.700
So a straightforward machine learning approach to that will miss some things, and you might not even know you're missing things.

00:14:47.020 --> 00:14:48.460
Yeah, I can imagine.

00:14:48.460 --> 00:15:01.660
You know, one of the things that just blows my mind is that we can see things so far away and so small and effectively so far in the past and still make intelligent statements about them.

00:15:01.660 --> 00:15:02.420
Yeah, yeah.

00:15:02.420 --> 00:15:03.360
It's pretty amazing.

00:15:03.360 --> 00:15:06.300
It's unbelievable some of the things you guys are doing.

00:15:06.300 --> 00:15:23.540
And the thing that blows me away, actually, about astronomy and astrophysics in general is the fact that these laws that we discovered in the laboratory here over the course of the centuries actually apply to what we see out there 10 billion light years away, right?

00:15:24.060 --> 00:15:30.180
And it's not just that we're assuming they apply, it's that we can actually test and confirm that they apply.

00:15:30.180 --> 00:15:39.640
One example is, you know, there's all these scientists in the 18th and 19th centuries studied the behavior of gases, right?

00:15:39.640 --> 00:15:43.420
What happens if you blow up a balloon and how fast does the air come out?

00:15:43.420 --> 00:15:50.040
And all that led to this formalized field of thermodynamics and statistical mechanics.

00:15:51.320 --> 00:16:01.840
And as you go into more detail, like what happens if you're looking at ionized gases and things like that, we learned all this stuff in the lab.

00:16:01.840 --> 00:16:15.560
And then in the mid-20th century, figured out that the cosmic microwave background, this light echo of the Big Bang, actually comes from a plasma in the early universe.

00:16:15.560 --> 00:16:19.460
And we can understand the properties of the plasma there by the same laws.

00:16:19.960 --> 00:16:30.060
And the reason that we know that the universe is 13 point, you know, I can't remember the decimals, but 13 point something billion years old with a very good accuracy.

00:16:30.060 --> 00:16:42.840
One of the reasons we know that is because we understand the thermodynamics and statistical mechanics of the plasma in the early universe and can compute what that says about the cosmic microwave background.

00:16:42.840 --> 00:16:45.780
And that story right there is just fascinating to me.

00:16:45.780 --> 00:16:47.180
Yeah, it's totally fascinating.

00:16:47.500 --> 00:16:52.640
And what I think is also fascinating is the guys who discovered it, was that Bell Labs in New Jersey?

00:16:52.640 --> 00:16:53.060
I think.

00:16:53.060 --> 00:16:53.800
Yeah, yeah.

00:16:53.800 --> 00:16:59.420
Yeah, the guys who discovered that whole cosmic background radiation weren't looking for it.

00:16:59.420 --> 00:17:01.900
They found it on accident and it was in their way, right?

00:17:01.900 --> 00:17:02.680
Yeah.

00:17:02.680 --> 00:17:08.120
And their first hypothesis, I guess, was that it was pigeon droppings on the detector.

00:17:08.280 --> 00:17:13.240
And once they cleaned off all the pigeon droppings, they had to figure out it was something else.

00:17:13.240 --> 00:17:19.840
Yeah, so they got the Nobel Prize for finding static in their instrument and realizing the static was significant.

00:17:19.840 --> 00:17:22.420
Yeah, normally that would be a problem, right?

00:17:22.420 --> 00:17:22.740
Yeah.

00:17:22.740 --> 00:17:23.940
You want to get rid of it.

00:17:23.940 --> 00:17:24.760
Very cool.

00:17:24.840 --> 00:17:28.580
And so you gave a really interesting talk at PyData, I think 2015.

00:17:28.580 --> 00:17:31.260
I'll be sure to link to the – it's up on YouTube.

00:17:31.260 --> 00:17:32.140
I'll link to the video.

00:17:32.140 --> 00:17:32.600
Mm-hmm.

00:17:32.600 --> 00:17:35.620
And you talked about how distance is super important in astronomy.

00:17:35.620 --> 00:17:42.000
And it relates to many of these big ideas that we hear about if we're sort of paying attention, I guess.

00:17:42.500 --> 00:17:43.180
Yeah, absolutely.

00:17:43.180 --> 00:17:46.060
Distance is fundamental to a lot of what we do.

00:17:46.060 --> 00:17:48.800
And it's also really, really hard to figure out.

00:17:48.800 --> 00:17:57.080
I mean, if you think about just looking at a dot of light in the sky, how do you tell how far away that is?

00:17:57.080 --> 00:18:07.320
And so a big part of the story of astronomy over the past couple centuries has been people figuring out how to determine how far away things are.

00:18:07.320 --> 00:18:12.140
So the first step that people figured out is we can do it geometrically.

00:18:12.340 --> 00:18:21.260
You know, the same way as if you put your finger in front of your eye and close one eye in the other, your finger seems to jump around in front of the background.

00:18:21.260 --> 00:18:23.720
That's called parallax.

00:18:23.720 --> 00:18:33.760
And we can use a similar type of trick to find the distance to nearby stars because the Earth is on one side of the sun in June and on the other side of the sun in December.

00:18:33.760 --> 00:18:41.500
And if you compare what the nearby stars look like compared to the background stars in June and December, you see them jump back and forth.

00:18:41.500 --> 00:18:45.020
And you can use the geometry of that to figure out the distance to those stars.

00:18:45.020 --> 00:18:46.020
I see.

00:18:46.020 --> 00:18:51.920
So you measure the sky and you basically see which ones kind of move more and which ones are more or less fixed.

00:18:51.920 --> 00:18:56.480
And then based on the parallax, you can say, well, these ones that move, they're five light years away or something.

00:18:56.480 --> 00:18:56.840
Yeah.

00:18:56.840 --> 00:19:01.100
And you can calculate that based on the angle and what we know about the Earth's orbit about the sun.

00:19:01.100 --> 00:19:09.900
But that only works to within, well, up until a couple months ago, it was within maybe a few thousand light years.

00:19:10.520 --> 00:19:11.940
There's this Gaia mission.

00:19:11.940 --> 00:19:16.780
The data was just released in the last couple weeks.

00:19:16.780 --> 00:19:28.720
And that one of the things that Gaia can do is it's going to give us really accurate parallax distances out to previously unheard of distances.

00:19:28.720 --> 00:19:34.640
So we're going to really be able to figure out the three-dimensional structure of the stars in our galaxy.

00:19:35.440 --> 00:19:40.620
But that parallax is not going to work when you go out to more distant galaxies.

00:19:40.620 --> 00:19:43.680
So you have to come up with other ideas.

00:19:43.680 --> 00:19:48.420
And one of the ideas that's been really fruitful is this idea of standard candles.

00:19:49.140 --> 00:19:56.680
If I stick you on a street in the dark and I turn on a 100-watt light bulb and I put it right next to your eye, it's really bright.

00:19:56.680 --> 00:19:59.520
But if I put it two blocks away down the street, it's really dim.

00:19:59.520 --> 00:20:11.380
And that brightness and dimness, you can compute that because the apparent brightness is attenuated by a factor of one over the distance squared.

00:20:12.880 --> 00:20:20.080
So if you know that, if you look at a light two blocks away and you know that it's a 100-watt light bulb and you have a very accurate photometer,

00:20:20.080 --> 00:20:24.680
you can compute exactly how far away that light bulb is.

00:20:24.680 --> 00:20:26.240
And this works with stars, too.

00:20:26.240 --> 00:20:36.520
If we know the exact brightness of a star, the exact intrinsic brightness of a star, and we look at its apparent brightness, we can compute the distance very easily.

00:20:36.520 --> 00:20:42.400
Now, the trick there is you need to know the intrinsic brightness of the star.

00:20:43.060 --> 00:20:48.720
Yeah, that's the kind of stuff that amazes me because you look out at these things super far away.

00:20:48.720 --> 00:20:50.540
And how do you know their intrinsic brightness, right?

00:20:50.540 --> 00:20:52.480
Yeah, and it's really difficult.

00:20:52.480 --> 00:21:01.460
One thing you can do is build off these things we learned from parallax, and you can look for certain classes of stars that are always around the same brightness.

00:21:01.460 --> 00:21:04.780
And you know their brightness when you know the parallax distance.

00:21:05.720 --> 00:21:13.000
And then you look for the same class of stars that are further out, and you can sort of infer their distance that way.

00:21:13.000 --> 00:21:16.580
So this is why it's kind of called, in astronomy, it's known as the distance ladder.

00:21:16.580 --> 00:21:23.280
You know, we have these direct methods that lead to more indirect methods of distances as we go further and further out.

00:21:23.280 --> 00:21:32.900
And one of the coolest stories of this distance ladder is back in the early 20th century, there was this woman named Henrietta Leavitt.

00:21:33.440 --> 00:21:36.500
And she was looking at variable stars.

00:21:36.500 --> 00:21:39.940
So there are stars out there that get brighter and fainter with time.

00:21:39.940 --> 00:21:44.800
And she was looking at particularly at this class of stars called Cepheid variables.

00:21:44.800 --> 00:21:49.960
It was named after the fourth brightest star in the constellation Cepheus.

00:21:52.100 --> 00:21:58.080
And she found something curious when she was looking at the variation of these.

00:21:58.080 --> 00:22:04.920
They would get brighter and fainter with a period of, you know, somewhere between a day and a couple days, something like that.

00:22:04.920 --> 00:22:14.060
And she found that the period of how fast they got brighter and dimmer was related to their intrinsic brightness.

00:22:15.340 --> 00:22:18.220
And so there's this nice plot where she shows that.

00:22:18.220 --> 00:22:22.220
And it's this roughly linear trend between period and intrinsic brightness.

00:22:22.220 --> 00:22:28.260
And that's really nice because then you can, a period is something that you can find out in the sky.

00:22:28.260 --> 00:22:34.040
So she looked out and, you know, found all these stars and confirmed that the period and the intrinsic brightness were related.

00:22:34.040 --> 00:22:35.600
So then Hubble came along.

00:22:35.600 --> 00:22:37.960
And you've probably heard of Hubble from the Hubble Space Telescope.

00:22:37.960 --> 00:22:44.200
And what he did is he used the telescopes available to him and found more and more of these stars.

00:22:44.860 --> 00:22:50.440
And based on this period brightness relation, was able to estimate the distances to all these stars.

00:22:50.440 --> 00:23:02.000
And the thing that really completely blew open our understanding of the universe was when Hubble pointed his telescope at one of the, what they called then the spiral nebula.

00:23:02.000 --> 00:23:11.580
So there were these spiral shaped clouds out in the sky that for a long time people thought were just clouds of dust in our galaxy.

00:23:11.860 --> 00:23:20.160
But Hubble found individual Cepheid variables in the Andromeda spiral nebula and found that it wasn't in our galaxy.

00:23:20.160 --> 00:23:26.520
It was about two and a half million light years away, farther away than anything we ever would have imagined existed.

00:23:26.740 --> 00:23:37.840
So in one fell swoop, the study of variable stars led to us understanding that the universe is orders of magnitude bigger than we ever imagined.

00:23:37.840 --> 00:23:41.500
That's really amazing how that ladders up there, right?

00:23:41.500 --> 00:23:51.720
And beyond that, we also learned that the universe is not contracting or sort of static, but it's sort of going away from itself and accelerating, right?

00:23:51.800 --> 00:23:52.040
Yeah.

00:23:52.040 --> 00:24:00.280
Yeah, so at the same time, he found that not only were these galaxies really far away, but if he looked at all these galaxies,

00:24:00.280 --> 00:24:08.440
they were the spiral nebula, and now we know them as galaxies because we know they're separate groups of stars.

00:24:08.440 --> 00:24:16.180
He looked at all these and he found that there was a relationship between how far away they are and how fast they're receding from us.

00:24:16.680 --> 00:24:20.320
We can measure their recession velocity by looking at the redshift of the light.

00:24:20.320 --> 00:24:30.160
And it's kind of like the Doppler shift, when a siren goes by you, you hear it high at first and it passes and it goes low.

00:24:30.160 --> 00:24:31.200
You know, this...

00:24:31.200 --> 00:24:32.600
Yeah.

00:24:32.600 --> 00:24:38.560
And that same Doppler shift, we could see that the light was shifting to a lower frequency,

00:24:38.560 --> 00:24:43.200
just like the sound shifts to a lower frequency when a car goes away from you.

00:24:43.700 --> 00:24:46.600
And you can measure the velocity.

00:24:46.600 --> 00:24:50.360
And he found this relationship between the distance and the velocity,

00:24:50.360 --> 00:24:55.040
which basically describes a uniformly expanding universe.

00:24:55.040 --> 00:25:03.560
And, you know, right around the same time, Einstein's predictions, general relativity,

00:25:04.040 --> 00:25:09.140
people were realizing that the general relativity equations that describe gravity

00:25:09.140 --> 00:25:12.640
and explained the orbit of Mercury, among other things,

00:25:12.640 --> 00:25:17.760
you could solve those in a way that led to an expanding universe.

00:25:17.760 --> 00:25:21.220
So it was another confirmation of general relativity.

00:25:21.220 --> 00:25:25.500
And this was all based on finding distances to galaxies.

00:25:25.500 --> 00:25:45.240
This portion of Talk Python To Me is brought to you by GoCD from ThoughtWorks.

00:25:45.660 --> 00:25:49.920
GoCD is the on-premise, open-source, continuous delivery server.

00:25:49.920 --> 00:25:52.760
With GoCD's comprehensive pipeline and model,

00:25:52.760 --> 00:25:56.720
you can model complex workflows for multiple teams with ease.

00:25:56.720 --> 00:26:02.540
And GoCD's value stream map lets you track changes from commit to deployment at a glance.

00:26:02.540 --> 00:26:07.540
GoCD's real power is in the visibility it provides over your end-to-end workflow.

00:26:07.940 --> 00:26:12.480
You get complete control of and visibility into your deployments across multiple teams.

00:26:12.480 --> 00:26:17.000
Say goodbye to release day panic and hello to consistent, predictable deliveries.

00:26:17.000 --> 00:26:21.880
Commercial support and enterprise add-ons, including disaster recovery, are available.

00:26:21.880 --> 00:26:27.860
To learn more about GoCD, visit talkpython.fm/gocd for a free download.

00:26:27.860 --> 00:26:31.160
That's talkpython.fm/gocd.

00:26:31.160 --> 00:26:33.260
Check them out. It helps support the show.

00:26:33.260 --> 00:26:45.600
I think one of the things that's super interesting about this is, you know,

00:26:45.600 --> 00:26:51.980
this concept of variable stars and the work that woman did was very manual, right?

00:26:51.980 --> 00:26:55.340
Like she would look at pictures and so on.

00:26:55.340 --> 00:26:59.880
Yeah, she was measuring the way they measured brightness of stars back before CCDs

00:26:59.880 --> 00:27:07.680
is you're looking at photographic plates and the brighter something is, the more it's saturated.

00:27:07.680 --> 00:27:13.680
So you'd have to do a detailed measurement of the size of the dot on your photographic plate

00:27:13.680 --> 00:27:16.800
and use that to compute the brightness of the star.

00:27:16.800 --> 00:27:22.080
It's just, it's amazing to me that any of that work got done given how easy we have it now, you know?

00:27:22.080 --> 00:27:24.680
Yeah, exactly. Exactly. It's such a different world.

00:27:24.880 --> 00:27:30.460
But at the same time, we've kind of answered those questions for the simple small ones we focused on.

00:27:30.460 --> 00:27:38.640
And now the amount of data that you guys are getting is so much larger that you have to start applying these machine learning algorithms

00:27:38.640 --> 00:27:40.280
just to deal with it, right?

00:27:40.280 --> 00:27:41.140
Yeah, absolutely.

00:27:41.140 --> 00:27:45.680
So the project I've been involved in that's just starting to get off the ground,

00:27:45.680 --> 00:27:47.220
first light is going to be in a couple of years,

00:27:47.220 --> 00:27:50.020
is this project called the Large Synoptic Survey Telescope.

00:27:50.560 --> 00:27:57.560
And you can think of this as an overview as it's a 10-year movie of the entire southern sky.

00:27:57.560 --> 00:28:03.320
So it's a very wide-field camera that's going to be on a mountaintop in Chile in the Atacama Desert,

00:28:03.320 --> 00:28:08.040
one of the driest places on the Earth, so we don't get, don't run into much weather.

00:28:09.400 --> 00:28:14.560
And it'll be able to scan the entire night sky every three nights or so.

00:28:14.560 --> 00:28:21.000
So get about 100 full sky frames in this movie per year and then do that for a decade.

00:28:21.000 --> 00:28:26.700
And the big thing this is going to open up is more of the time domain.

00:28:26.700 --> 00:28:32.480
You know, typically astronomers tend to treat the sky as this fixed thing.

00:28:32.480 --> 00:28:40.000
There are individual times where we look at specific regions of the sky and see what has changed.

00:28:40.000 --> 00:28:45.640
But we don't really have a global survey yet of the time domain of the sky.

00:28:45.640 --> 00:28:48.580
And LSST is going to do this on a huge scale.

00:28:48.580 --> 00:28:56.960
We're going to have 10 years of data with something like 30-ish terabytes per night of data coming through.

00:28:58.020 --> 00:29:02.360
So the full survey size is going to be in the hundreds of petabytes by the end.

00:29:02.360 --> 00:29:07.260
So it's really, it's bigger than anything that's been done before.

00:29:07.260 --> 00:29:15.660
And it's really forcing astronomers to confront these old tool chains that they've had that don't really scale anymore.

00:29:15.660 --> 00:29:21.240
You know, the stuff that you could do, you could sit down, 10 years ago you could sit down on a computer

00:29:21.240 --> 00:29:28.120
and download, you know, all of the Sloan Digital Sky Survey and do some sort of local analysis.

00:29:28.120 --> 00:29:37.180
I don't know if, even in 10 years, I don't know if we're going to have hundreds of petabyte size hard drives in our laptop, right?

00:29:37.180 --> 00:29:39.680
We're going to have to do it a little bit differently.

00:29:39.680 --> 00:29:41.260
Yeah, that's really a lot of data.

00:29:41.740 --> 00:29:46.700
And the other thing you had said that was interesting is this data is being collected for everyone,

00:29:46.700 --> 00:29:54.360
which means that it's not specifically focused on some type of answering some type of questions.

00:29:54.360 --> 00:30:00.220
So the techniques and the tools and like the machine learning stuff you have to apply has a greater challenge.

00:30:01.680 --> 00:30:07.740
Yeah, it has to be really, really general because this data, like you said, it's collected for everyone.

00:30:07.740 --> 00:30:12.180
There's not really specific areas that it's addressing.

00:30:12.180 --> 00:30:16.360
It's one of these discovery class missions, similar to the Hubble Space Telescope.

00:30:16.360 --> 00:30:22.440
You know, you put it out there and you hope that the things you find are things that you're not going to be able to predict at the moment.

00:30:22.440 --> 00:30:33.660
And what that means is that for any particular science case, you're not necessarily going to have the best data.

00:30:33.660 --> 00:30:42.660
You know, if you were designing LSST to do one thing, like look for variable stars, you would do it very differently than if you're doing it in general

00:30:42.660 --> 00:30:48.580
because you have to balance all these different concerns and different areas of research.

00:30:48.760 --> 00:30:58.560
So, for example, going back to variable stars, one of the challenges with LSST is that rather than just observing in the same region of the spectrum every night,

00:30:58.560 --> 00:31:05.140
that you'd want to do if you want to, you know, if you want to look at a variable star and see how the brightness changed from one night to the other,

00:31:05.140 --> 00:31:07.540
you'd want to take the exact same observation.

00:31:07.540 --> 00:31:11.080
But LSST is not taking the exact same observation every night.

00:31:11.080 --> 00:31:18.740
It's getting a breadth of different bands throughout the spectrum, everything from infrared to,

00:31:18.740 --> 00:31:20.360
in the near ultraviolet.

00:31:20.360 --> 00:31:30.340
And what that means is that that's really good for things like determining the redshift of galaxies via machine learning, right?

00:31:30.340 --> 00:31:38.500
But it's actually very bad for finding variable stars because now you have to model not only the variability,

00:31:38.500 --> 00:31:42.780
but you have to model the spectral variability over the course of time, too.

00:31:42.840 --> 00:31:44.340
And it gets much more challenging.

00:31:44.340 --> 00:31:55.960
So as this data grows and as the heterogeneity of the data grows, having these sophisticated algorithms,

00:31:55.960 --> 00:32:00.640
whether it's machine learning or some sort of forward modeling or some sort of nonparametric modeling,

00:32:01.180 --> 00:32:03.400
that's becoming increasingly important.

00:32:03.400 --> 00:32:09.840
And it's things that need to happen kind of in real time while you're observing the sky

00:32:09.840 --> 00:32:13.780
because we want to be able to alert people within a minute or so.

00:32:13.780 --> 00:32:17.200
If something changes on the sky and we find an interesting object,

00:32:17.200 --> 00:32:22.680
there's going to be this alert stream so that somebody sitting in a telescope in another part of the world

00:32:22.680 --> 00:32:26.420
could point their telescope there right away and catch this interesting phenomenon.

00:32:27.380 --> 00:32:32.300
Yeah. Wow. I'm really excited to see what comes out of this. That's a big project.

00:32:32.300 --> 00:32:40.720
Yeah, it's going to be huge. It's really going to define the way that we do astronomy over the course of the 2020s.

00:32:40.720 --> 00:32:47.320
Yeah, for sure. So let's talk about some of the libraries that you might be using to answer questions here.

00:32:47.320 --> 00:32:53.320
So the two major ones, and I guess one is kind of a subset of the other, is AstroPy and AstroML.

00:32:53.760 --> 00:32:57.260
Yeah, so AstroPy is actually the big community standard.

00:32:57.260 --> 00:33:03.420
And it's been a really cool project to watch and to be involved in because it started a few years ago

00:33:03.420 --> 00:33:08.960
where everyone had their own little Python library to do things.

00:33:08.960 --> 00:33:11.860
This grew out, I should step back.

00:33:11.860 --> 00:33:14.680
Ten years ago, most people were using IDL.

00:33:14.680 --> 00:33:20.860
And so the community evolved these sets of routines in IDL to do a lot of the common tasks.

00:33:21.000 --> 00:33:27.460
And as more and more people moved over to Python because of – well, I'll go into that later.

00:33:27.460 --> 00:33:32.800
But as more and more people moved into Python because of its advantages,

00:33:32.800 --> 00:33:39.220
people built a whole bunch of different tools to do different things, and it was this sort of smattering.

00:33:39.220 --> 00:33:43.200
And the Space Telescope Science Institute people, the folks behind Hubble,

00:33:43.320 --> 00:33:46.700
came together in around 2012, 2011, and said,

00:33:46.700 --> 00:33:50.760
we should consolidate all this and create one Uber package to rule them all.

00:33:50.760 --> 00:33:57.380
And AstroPy was born, and it's actually accomplished its goal.

00:33:57.380 --> 00:33:59.040
Pretty much everyone is using it now.

00:33:59.040 --> 00:34:04.140
So that's an incredible package and really, really well done,

00:34:04.140 --> 00:34:07.440
and awesome software engineering behind it.

00:34:07.440 --> 00:34:09.180
Lots of buy-in from the community.

00:34:09.180 --> 00:34:13.440
AstroML is something that I started along the same time.

00:34:13.440 --> 00:34:19.240
I didn't have as broad of a vision, but I just wanted to bring together functionality

00:34:19.240 --> 00:34:24.940
and examples of doing machine learning specifically for astronomy in Python.

00:34:25.780 --> 00:34:29.860
And we actually wrote the package to accompany our book, which is –

00:34:29.860 --> 00:34:37.640
it's a Princeton Press book on statistical modeling, machine learning, and et cetera in Python for astronomy.

00:34:37.640 --> 00:34:39.700
Yeah, that's great.

00:34:39.700 --> 00:34:43.580
And so what kind of things do you cover in your book about –

00:34:43.580 --> 00:34:46.820
like what problems are solved or presented or data sets, things like that?

00:34:47.300 --> 00:34:52.600
Yeah, in that book, what we do is we walk through a lot of the –

00:34:52.600 --> 00:34:59.340
it's meant to be an intro graduate text on statistics and machine learning with astronomers in mind.

00:34:59.340 --> 00:35:06.060
So we walk through all the basics of data mining, statistics, machine learning,

00:35:06.060 --> 00:35:15.460
all the while using these data sets drawn from astronomy and problem situations that astronomical researchers will run into.

00:35:16.340 --> 00:35:23.800
And along the way, we also provide code snippets and provide figures with the full figure source available online

00:35:23.800 --> 00:35:31.820
so that if people want to actually use these techniques, they can grab our scripts and start modifying them from there and see where it goes.

00:35:31.820 --> 00:35:34.460
So AstroML is what drives that a little bit.

00:35:34.460 --> 00:35:39.120
In our next edition of the book, which might happen in the next year or so,

00:35:39.120 --> 00:35:43.580
my big task is going to be to incorporate AstroPy

00:35:43.580 --> 00:35:46.640
because we actually wrote that book before AstroPy existed.

00:35:46.640 --> 00:35:51.040
And so it's already a little bit outdated and I want to make sure that everyone's –

00:35:51.040 --> 00:35:53.460
I'm pointing everyone to the tools that are in AstroPy.

00:35:53.460 --> 00:35:54.960
Yeah, that makes a lot of sense.

00:35:54.960 --> 00:35:56.220
So you created the book and you're like,

00:35:56.220 --> 00:35:59.300
we really should make this like a package that people can just use,

00:35:59.300 --> 00:36:03.800
the techniques and whatnot, and then now it's a little more mature, right?

00:36:03.800 --> 00:36:04.340
Yep.

00:36:04.340 --> 00:36:04.780
Yep.

00:36:05.580 --> 00:36:09.400
Do you know of any discoveries that were done as a result of AstroML?

00:36:09.400 --> 00:36:14.220
It's been referenced in a lot of papers.

00:36:14.220 --> 00:36:19.220
I don't know offhand if there's anything that came exactly from that,

00:36:19.220 --> 00:36:25.640
but it's definitely been used for a lot of the incremental building of knowledge over the last few years,

00:36:25.640 --> 00:36:26.760
and it's been fun to see that.

00:36:26.760 --> 00:36:28.640
Yeah, I'm sure that's really rewarding.

00:36:28.640 --> 00:36:29.160
That's awesome.

00:36:29.160 --> 00:36:36.200
Another big thing in the astronomy community is forward modeling and Bayesian approaches.

00:36:36.200 --> 00:36:41.740
So I alluded to earlier the fact that machine learning is a little bit difficult

00:36:41.740 --> 00:36:49.180
because the statistical similarity of the samples is not always a good assumption.

00:36:49.180 --> 00:36:53.780
So the way that astronomers tend to get around that is to use forward modeling.

00:36:53.780 --> 00:36:59.760
So you have some model for your system based on the physics that you know,

00:36:59.760 --> 00:37:06.700
and you can look at the noise properties and the selection effects of your observations to constrain that model,

00:37:06.700 --> 00:37:11.600
and then that model will tell you about the data that you observe.

00:37:11.600 --> 00:37:15.400
And that tends to work really well in a Bayesian setting.

00:37:15.660 --> 00:37:22.360
So a huge push in the last few years in astronomy has been to use tools like Markov Chain Monte Carlo

00:37:22.360 --> 00:37:31.180
to do Bayesian analysis and to do these really large high-dimensional models to learn about the data.

00:37:31.180 --> 00:37:39.340
So there's one package that's been pretty impactful there is the MC package, E-M-C-E-E.

00:37:40.300 --> 00:37:46.520
And that's a package for doing Markov Chain Monte Carlo, doing Bayesian estimation,

00:37:46.520 --> 00:37:54.700
written by an astronomer and used for, it's been cited, I think, thousands of times in the astronomy community

00:37:54.700 --> 00:37:57.360
because so many people are doing that style of analysis.

00:37:57.360 --> 00:37:58.920
Yeah, that's really amazing.

00:37:58.920 --> 00:38:05.340
Yeah, I guess the whole how do you solve these prediction problems more quickly is really important.

00:38:05.340 --> 00:38:08.820
And Monte Carlo simulations are really good at that.

00:38:09.620 --> 00:38:09.900
Mm-hmm.

00:38:09.900 --> 00:38:13.760
And particularly the Bayesian approaches.

00:38:13.760 --> 00:38:17.640
Machine learning tends to be more of a frequentist approach,

00:38:17.640 --> 00:38:22.380
and the Bayesian forward modeling approaches give you some advantage.

00:38:22.380 --> 00:38:28.860
And when you have some a priori idea about what's driving your observations,

00:38:28.860 --> 00:38:33.780
you can take advantage of that more in a Bayesian context than in a machine learning context.

00:38:34.740 --> 00:38:38.680
So you wrote this book, The Statistics, Data Mining, Machine Learning, and Astronomy,

00:38:38.680 --> 00:38:42.440
and you survived that process and you decided to come back for more.

00:38:42.440 --> 00:38:44.660
And you're just about to finish up a book, right?

00:38:44.660 --> 00:38:45.380
Another one.

00:38:45.380 --> 00:38:48.600
Yeah, I'm just, I'm finishing one that's, it's an O'Reilly book.

00:38:48.600 --> 00:38:51.520
So think, you know, cute little animal on the cover.

00:38:51.520 --> 00:38:52.520
You've probably seen that.

00:38:52.520 --> 00:38:53.240
What's your animal?

00:38:53.580 --> 00:38:56.140
The animal is a Mexican bearded lizard.

00:38:56.140 --> 00:39:00.680
Yeah, this one is the Python Data Science Handbook.

00:39:00.680 --> 00:39:05.920
So the reason I did this is I, for years I've been approached by people who are, you know,

00:39:05.920 --> 00:39:08.320
in research or in tech or something like that.

00:39:08.320 --> 00:39:11.220
And they say, hey, I know how to use MATLAB.

00:39:11.220 --> 00:39:13.320
I know how to use R.

00:39:13.320 --> 00:39:17.240
But I want to learn how to do Python and I want to learn how to analyze data in Python.

00:39:17.240 --> 00:39:23.540
And I hadn't found a really good resource to point them to except for kind of collections of videos online.

00:39:23.540 --> 00:39:25.680
So I decided to write it.

00:39:25.680 --> 00:39:30.780
And it's taken much longer than I thought because life gets in the way.

00:39:30.780 --> 00:39:35.840
But we're at the point where it's, I'm doing the final edits right now.

00:39:35.840 --> 00:39:37.340
So it should be released pretty soon.

00:39:37.340 --> 00:39:38.600
Yeah, that's great.

00:39:38.600 --> 00:39:39.980
Congratulations on that.

00:39:39.980 --> 00:39:43.540
I'll be sure to link to it as well from the show notes so everyone can find it.

00:39:43.540 --> 00:39:48.700
One thing I'm particularly excited about this book, I wrote it all in the form of Jupyter Notebooks

00:39:48.700 --> 00:39:53.860
and got the publisher to agree to let me make the Jupyter Notebooks public.

00:39:53.860 --> 00:39:57.440
So you can buy the printed version of the book or you'll be able to go on GitHub

00:39:57.440 --> 00:40:00.520
and just work through the Jupyter Notebooks for free.

00:40:00.520 --> 00:40:02.420
Wow, that really is cool.

00:40:02.420 --> 00:40:02.940
Yeah.

00:40:02.940 --> 00:40:03.940
Okay.

00:40:03.940 --> 00:40:06.720
But you should buy the book to support all the work.

00:40:06.720 --> 00:40:08.720
Definitely support the project.

00:40:08.720 --> 00:40:09.240
That's cool.

00:40:09.240 --> 00:40:13.140
But it's very cool that basically it's a live book, right?

00:40:13.140 --> 00:40:16.740
Like if you have the data and you have the code and you can run it, you can explore it.

00:40:16.740 --> 00:40:17.520
Yeah.

00:40:17.520 --> 00:40:24.000
And we're working on, too, getting a hosted version of it up there on some cloud service.

00:40:24.000 --> 00:40:29.460
So you could just basically click and have a live executable textbook at your fingertips.

00:40:29.460 --> 00:40:31.340
Yeah, it's interesting.

00:40:31.460 --> 00:40:33.880
I think a lot of things are going that way, right?

00:40:33.880 --> 00:40:38.980
The days of just a printed book and a zip file are fading, let's say.

00:40:38.980 --> 00:40:39.700
Yeah, yeah.

00:40:39.700 --> 00:40:41.940
Because there's so much better ways of doing it now.

00:40:41.940 --> 00:40:59.080
This portion of Talk Python To Me is brought to you by DataSchool.

00:40:59.080 --> 00:41:03.660
Have you thought about making a career change into the exciting world of data science, but

00:41:03.660 --> 00:41:04.660
don't know how to get started?

00:41:04.820 --> 00:41:10.340
DataSchool helps data science beginners like you to analyze interesting data sets and build

00:41:10.340 --> 00:41:13.780
machine learning models to predict the future, all using Python.

00:41:13.780 --> 00:41:19.160
You don't need a PhD or a background in mathematics, just a keen interest in using data to answer

00:41:19.160 --> 00:41:19.760
your questions.

00:41:19.760 --> 00:41:24.840
DataSchool has created a data science learning path exclusively for Talk Python listeners.

00:41:25.240 --> 00:41:29.520
So visit talkpython.fm/DataSchool to launch your data science career.

00:41:29.520 --> 00:41:34.260
DataSchool is run by my friend Kevin Markham, so I know that you're going to get excellent

00:41:34.260 --> 00:41:34.760
content.

00:41:34.760 --> 00:41:37.900
Check it out at talkpython.fm/DataSchool.

00:41:37.900 --> 00:41:47.720
So let's talk a little bit about where you work and what you do, because you are breaking

00:41:47.720 --> 00:41:55.000
some rules around how people in academia and scientists work with programming technology and how

00:41:55.000 --> 00:41:56.500
programmers are involved.

00:41:56.500 --> 00:41:57.840
And I think that's really interesting.

00:41:57.840 --> 00:42:02.020
So you're at the University of Washington, but you're at this place called the eScience

00:42:02.020 --> 00:42:03.100
Institute, right?

00:42:03.100 --> 00:42:03.520
Yeah.

00:42:03.520 --> 00:42:04.780
So I'm in the eScience Institute.

00:42:04.780 --> 00:42:06.940
I've been here since the beginning of 2014.

00:42:06.940 --> 00:42:17.440
And the goal of the eScience Institute is to basically further computational research around

00:42:17.440 --> 00:42:17.980
campus.

00:42:18.620 --> 00:42:26.480
And so it's existed for a while, but we really got a big boost in 2014 when I came on.

00:42:26.480 --> 00:42:32.840
We got this joint grant between New York University, UC Berkeley, and UW.

00:42:33.360 --> 00:42:36.700
And we all created some version of this Data Science Institute.

00:42:36.700 --> 00:42:39.520
And so it's a five-year grant to support what we're doing.

00:42:39.520 --> 00:42:46.580
And the goals are basically to see how we can reshape the culture of academia to take more

00:42:46.580 --> 00:42:55.320
advantage of data science tools, to train people better, to provide career paths for software-focused

00:42:55.320 --> 00:42:55.960
researchers.

00:42:56.800 --> 00:43:02.360
And so, for example, the job that I have right now where I, what I do day-to-day is I spend

00:43:02.360 --> 00:43:07.860
a lot of time consulting with researchers around the university, helping them figure out their

00:43:07.860 --> 00:43:08.560
data challenges.

00:43:09.400 --> 00:43:15.720
I mentor students who kind of have one foot in their home domain, their science, and one

00:43:15.720 --> 00:43:18.260
foot in like a data science program.

00:43:18.260 --> 00:43:24.040
And I work a lot on maintaining the software that astronomers and other scientists use.

00:43:24.040 --> 00:43:29.780
And this is a position that's not really, it's, I feel like, sort of a stepchild in academia because

00:43:29.780 --> 00:43:33.020
no one really understands that type of position.

00:43:33.020 --> 00:43:39.360
It doesn't fit into the model of graduate student postdoc faculty.

00:43:39.360 --> 00:43:44.120
So we have a number of people that are in a similar position to me that are working on

00:43:44.120 --> 00:43:44.380
this.

00:43:44.380 --> 00:43:52.260
And it's been super fun to see what comes out of this and the kind of novel trainings and

00:43:52.260 --> 00:43:54.260
novel approaches to research that we can do.

00:43:54.260 --> 00:43:59.880
And particularly fun because it's not only happening at UW, it's happening at NYU and UC Berkeley as

00:43:59.880 --> 00:44:00.100
well.

00:44:00.100 --> 00:44:04.680
And we can compare notes with those institutions and see how things are going.

00:44:04.680 --> 00:44:06.700
That sounds like such a fabulous job.

00:44:06.700 --> 00:44:08.980
Yeah, it's good for the time being.

00:44:08.980 --> 00:44:12.740
I mean, I'm worrying that I'm peaking early because it's so fun.

00:44:12.740 --> 00:44:14.380
I don't know what will come next.

00:44:14.380 --> 00:44:15.380
It's all downhill from here.

00:44:15.380 --> 00:44:16.380
No, no, that's really cool.

00:44:16.380 --> 00:44:22.600
One of the things you pointed out in your PyData talk is that every field is entering a data-rich

00:44:22.600 --> 00:44:23.020
era.

00:44:23.220 --> 00:44:25.740
So there's all these biologists, sociologists.

00:44:25.740 --> 00:44:32.480
You're basically there to help support like the biologists, sociologists, chemists, all the

00:44:32.480 --> 00:44:36.540
people who are hitting the limits of how much data they can handle.

00:44:36.540 --> 00:44:37.640
Yeah, absolutely.

00:44:37.640 --> 00:44:44.160
And the way we're doing this is we have a number of different ways to engage with people on campus.

00:44:44.160 --> 00:44:46.400
So one is we have these open office hours.

00:44:46.600 --> 00:44:52.060
So just like you used to go see your professor during class, we have office hours that are oriented

00:44:52.060 --> 00:44:56.440
towards researchers who, if they have a challenge, they can come talk to one of our people.

00:44:56.440 --> 00:45:01.940
And we have people with expertise in everything from statistics, machine learning, to software

00:45:01.940 --> 00:45:04.240
engineering, to cloud computing and scalability.

00:45:04.240 --> 00:45:07.200
Another thing we do is we run these incubator programs.

00:45:07.200 --> 00:45:12.880
So it's sort of designed off these startup incubators that are coming from the Silicon Valley, where

00:45:12.880 --> 00:45:17.420
instead of incubating their startup idea, we're incubating their research idea and letting

00:45:17.420 --> 00:45:23.160
researchers work shoulder to shoulder with a data scientist who has an expertise that complements

00:45:23.160 --> 00:45:23.540
theirs.

00:45:23.540 --> 00:45:30.720
And we also have graduate fellowships where students have one foot in their own department,

00:45:30.720 --> 00:45:36.740
one foot in e-science and are taking not only, say, astronomy courses, but also database,

00:45:36.740 --> 00:45:42.960
machine learning, statistics, computer science courses and getting a credit on their PhD for

00:45:42.960 --> 00:45:43.220
that.

00:45:43.220 --> 00:45:44.100
Yeah.

00:45:44.100 --> 00:45:48.560
What I thought that was really fascinating is, you know, having gone through some part of

00:45:48.560 --> 00:45:53.120
a PhD program, just there's so many things you've got to take and learn.

00:45:53.120 --> 00:45:56.400
And you're so busy learning your specialty, right?

00:45:56.400 --> 00:45:58.140
Like biology, if that's where your PhD was.

00:45:58.140 --> 00:46:04.820
That is, it's really hard to be a good data science software type person as well.

00:46:04.820 --> 00:46:05.820
Yeah, absolutely.

00:46:05.820 --> 00:46:12.380
I think, you know, you said that these guys in these cohorts, they basically get half of

00:46:12.380 --> 00:46:16.500
their requirements for their PhD program waived.

00:46:16.500 --> 00:46:17.100
Yep.

00:46:17.100 --> 00:46:21.820
So that they can focus the other half on sort of complementing this with data science and

00:46:21.820 --> 00:46:22.320
programming, right?

00:46:22.320 --> 00:46:23.540
Yeah, that's the idea.

00:46:23.540 --> 00:46:27.780
And then what comes with that is they get, they have their home department advisor, but

00:46:27.780 --> 00:46:31.700
they're also matched with a co-advisor that's more methodological.

00:46:31.700 --> 00:46:38.920
And so it leads to not only the student growing a lot, but it leads to some interesting interdisciplinary

00:46:38.920 --> 00:46:40.580
collaborations around campus.

00:46:40.580 --> 00:46:45.980
And we've had a number of pretty cool grants that have been awarded based on some of these

00:46:45.980 --> 00:46:46.500
partnerships.

00:46:47.700 --> 00:46:49.780
Yeah, that sounds really quite amazing.

00:46:49.780 --> 00:46:52.380
I wish that was around when I was in school.

00:46:52.380 --> 00:46:54.040
Yeah, I do too.

00:46:54.040 --> 00:46:56.740
I had to pick a lot of this stuff up on my own.

00:46:56.740 --> 00:46:58.500
It would have been nice to have something like this.

00:46:58.500 --> 00:46:59.320
Yeah.

00:46:59.320 --> 00:47:02.580
If anyone out there is listening and they're maybe in a position where they're like, oh,

00:47:02.580 --> 00:47:03.180
this is interesting.

00:47:03.180 --> 00:47:03.980
How do we do this?

00:47:03.980 --> 00:47:04.260
Right?

00:47:04.320 --> 00:47:08.200
Like another thing that I thought you pointed out was really interesting is it's in a beautiful

00:47:08.200 --> 00:47:08.760
location.

00:47:08.760 --> 00:47:10.540
And you said that that was really important.

00:47:10.540 --> 00:47:12.100
Yeah, yeah, definitely.

00:47:12.100 --> 00:47:21.140
So we have this data science studio that it's an old library branch location on campus.

00:47:21.140 --> 00:47:26.980
And we're on the sixth floor of this tower with where we have the whole floor with 360 degree

00:47:26.980 --> 00:47:30.720
views looking out over Mount Rainier and the Olympic Mountains and things like this.

00:47:31.180 --> 00:47:34.920
And it's important not just so that I can have an awesome view while I'm writing code,

00:47:34.920 --> 00:47:40.040
but it's important because we want people around campus to interact with each other.

00:47:40.040 --> 00:47:44.340
And so we want to be a place where people would like to come and just hang out.

00:47:44.340 --> 00:47:50.900
So it's getting back to the, we call it the water cooler effect.

00:47:51.140 --> 00:47:57.880
The people who are around in the 60s and 70s working on, working on computationally intensive

00:47:57.880 --> 00:48:04.180
science, talk about the days when everyone would go to the mainframe on campus and you'd

00:48:04.180 --> 00:48:06.540
be sitting there waiting to put your punch cards in.

00:48:06.540 --> 00:48:12.660
And, you know, a hydrologist would be talking to an astrophysicist and finding out that they're,

00:48:12.660 --> 00:48:15.460
they're solving the same equations with their programs.

00:48:15.460 --> 00:48:20.980
So they would, you know, have that sort of talk and, and as the campus moved towards desktop

00:48:20.980 --> 00:48:24.180
oriented computing, those sorts of opportunities went away.

00:48:24.180 --> 00:48:29.240
And I think we're, we're better off if we can have that, that sort of connection.

00:48:29.240 --> 00:48:34.240
So one of the cool things about our space here is it's a, it's a space that's open to anyone on

00:48:34.240 --> 00:48:39.180
campus for, for just hanging out and working, but also for scheduling meetings.

00:48:39.180 --> 00:48:43.300
So we have people from all different departments that are scheduling their group meetings here,

00:48:43.300 --> 00:48:48.080
coming in, hanging out, having coffee, and then meeting someone from the other side of campus

00:48:48.080 --> 00:48:52.520
who's solving the same differential equation in their completely different field.

00:48:52.520 --> 00:48:53.740
Yeah, that's great.

00:48:53.740 --> 00:48:54.180
Yeah.

00:48:54.180 --> 00:48:54.860
Very, very nice.

00:48:54.860 --> 00:48:56.840
Like I said, I wish that existed when I was in school.

00:48:58.780 --> 00:48:59.300
All right.

00:48:59.300 --> 00:49:03.180
So we're kind of getting near the end of the show and I have a couple of questions

00:49:03.180 --> 00:49:05.420
that I wanted to run by you.

00:49:05.420 --> 00:49:07.780
One is kind of almost metaphysical.

00:49:07.780 --> 00:49:15.620
So I just heard the other day that there was some study done that there was, we underestimated

00:49:15.620 --> 00:49:19.500
the number of galaxies by like a factor of 20 or something amazing, right?

00:49:19.500 --> 00:49:19.940
Yeah.

00:49:19.940 --> 00:49:20.280
Yeah.

00:49:20.280 --> 00:49:27.080
That was really interesting that, you know, already there's so many galaxies out there and every

00:49:27.080 --> 00:49:29.900
galaxy has, you know, so many stars and so many planets.

00:49:29.900 --> 00:49:33.200
What do you think the chances that there's intelligent life outside the earth?

00:49:33.200 --> 00:49:35.680
Not necessarily like there's people visiting the earth, right?

00:49:35.680 --> 00:49:36.620
Just out there somewhere.

00:49:36.620 --> 00:49:37.720
We will never meet them.

00:49:37.720 --> 00:49:39.220
That's hard to say.

00:49:39.220 --> 00:49:44.260
you know, at this point, so, so one thing at UW, we have this, astrobiology group,

00:49:44.260 --> 00:49:48.100
which sounds like a funny area of study because what are they studying?

00:49:48.100 --> 00:49:48.400
Right.

00:49:48.400 --> 00:49:54.240
But, but they're working on really interesting things, combining, what we know about biology,

00:49:54.240 --> 00:50:00.060
about geophysics, about planetary astronomy, and looking for locations around the universe

00:50:00.060 --> 00:50:02.320
where, life might exist.

00:50:02.620 --> 00:50:08.560
And so they study extremophiles around here, like organisms that live on, on deep sea vents

00:50:08.560 --> 00:50:11.680
and, in acidic boiling water environments, things like that.

00:50:11.680 --> 00:50:19.840
but one thing that's come out of that group is, this, this notion that, simple

00:50:19.840 --> 00:50:25.300
life, you know, microbial life, probably could exist just about anywhere.

00:50:25.300 --> 00:50:29.820
And I tend to think that, you know, I would be, I would be pretty surprised if we

00:50:29.820 --> 00:50:34.080
don't find some sort of microbial life elsewhere in our, our solar system.

00:50:34.080 --> 00:50:39.280
but the other thing to come out of that as we study the dynamics of planets and

00:50:39.280 --> 00:50:43.740
things like this is that there are a lot of things about earth in particular that make

00:50:43.740 --> 00:50:50.880
it very special and a lot of, coincidences that, that are, that would be hard to duplicate.

00:50:50.880 --> 00:50:57.400
You know, things like the fact that Jupyter exists, keeps us from having a large number

00:50:57.400 --> 00:50:58.920
of asteroid impacts on earth.

00:50:58.920 --> 00:51:00.160
That's kind of a big shield.

00:51:00.160 --> 00:51:07.720
And, you know, that, that asteroid impacts, as, as we know from geological paleontological

00:51:07.720 --> 00:51:11.720
history is, they can be pretty bad for life on earth.

00:51:11.720 --> 00:51:17.400
So, so the type of stability that we have on earth, particularly over the last, tens,

00:51:17.960 --> 00:51:21.940
hundreds of millions of years, I suspect that's rather rare.

00:51:21.940 --> 00:51:25.520
And that, that makes me think that intelligent life might be rather rare.

00:51:25.520 --> 00:51:30.820
You know, I think, I think the seas of Europa, when we get, when we get something that can

00:51:30.820 --> 00:51:35.180
burrow through the ice and look down there, I really hope there's something swimming around

00:51:35.180 --> 00:51:37.660
down there because it would be pretty cool.

00:51:37.660 --> 00:51:38.720
It'd be very cool.

00:51:38.720 --> 00:51:40.980
I hope I hope I get to see that someday.

00:51:40.980 --> 00:51:41.560
That'd be awesome.

00:51:41.560 --> 00:51:42.620
All right.

00:51:42.620 --> 00:51:46.120
So another one, my wife's a professor here at Portland State University.

00:51:46.120 --> 00:51:49.300
So I hang out with some other, her colleagues and stuff.

00:51:49.300 --> 00:51:55.820
And one of her colleagues had this student, she's teaching like a numerical methods for

00:51:55.820 --> 00:51:58.180
partial differential equations, you know, something like that.

00:51:58.180 --> 00:52:06.440
Her, her, you know, she's using Python and a lot of things like NumPy and so on in her

00:52:06.440 --> 00:52:07.480
class for the computation.

00:52:07.480 --> 00:52:10.920
And one of her students came and said, Hey, I just, I know MATLAB.

00:52:10.920 --> 00:52:11.780
Can I just use MATLAB?

00:52:11.780 --> 00:52:13.460
Why do I need to learn this Python thing?

00:52:13.600 --> 00:52:16.400
What would you tell that student if you got that question?

00:52:16.400 --> 00:52:17.740
Yeah, that's a good question.

00:52:17.740 --> 00:52:23.020
so number one, I think, use the tool that's most effective for your research.

00:52:23.020 --> 00:52:29.200
And for example, if you, if there are programs in MATLAB that, don't exist in Python and,

00:52:29.200 --> 00:52:33.480
and they were, they're required from your, for your research, there's no reason to learn a

00:52:33.480 --> 00:52:35.160
new tool just because it's a new tool.

00:52:35.340 --> 00:52:39.020
But on the other hand, there, there's some distinct advantages to Python.

00:52:39.020 --> 00:52:45.740
And I alluded to this earlier when I talked about the field of astronomy shifting from 90% IDL over

00:52:45.740 --> 00:52:47.960
the last 10 years to probably 90% Python.

00:52:47.960 --> 00:52:53.980
And the advantages, that I see are number one, it's openness, right?

00:52:54.000 --> 00:52:55.540
It's, it's open and it's free.

00:52:55.540 --> 00:53:01.360
So that means, one thing that has come up with IDL, is there, there are site licenses

00:53:01.360 --> 00:53:03.540
required for every instance that you run.

00:53:03.540 --> 00:53:07.720
You know, it's, it's a pay to play type of operating system or type of, interpreter.

00:53:07.720 --> 00:53:14.020
And so when people started running, running parallelized jobs, taking advantage of all the

00:53:14.020 --> 00:53:18.480
computers in the department, there were times where a grad student would start a job and it

00:53:18.480 --> 00:53:23.160
would use all the site licenses for the entire department and research in the department ground

00:53:23.160 --> 00:53:24.160
to a halt, right?

00:53:24.160 --> 00:53:28.920
so you don't have that problem in Python because there's no site licenses with Python.

00:53:28.920 --> 00:53:31.500
So Python can be cheaper to use.

00:53:31.500 --> 00:53:39.760
the other thing about it is that, to, to serve students well, you know, the, the

00:53:39.760 --> 00:53:47.060
number of academic jobs versus the number of, undergrad degrees or PhDs granted is extremely

00:53:47.060 --> 00:53:47.520
small.

00:53:47.520 --> 00:53:51.120
So most of our students are going to be going out into the world and working somewhere other

00:53:51.120 --> 00:53:52.360
than an academic department.

00:53:52.360 --> 00:53:57.880
And, people in, in the outside world and the tech world, they're much more excited about

00:53:57.880 --> 00:54:02.580
someone with Python chops than someone with IDL chops or MATLAB chops, just because that's,

00:54:02.580 --> 00:54:04.400
you know, the way the world has gone.

00:54:04.400 --> 00:54:07.420
So that, that's another good reason to move to Python.

00:54:07.420 --> 00:54:12.680
the other thing that I love about Python is the culture of open source.

00:54:13.260 --> 00:54:20.360
So particularly now, 10 years ago, it was different, but now, just about anything you want to

00:54:20.360 --> 00:54:25.380
do in Python, you can go out there and there's somebody who has made an open source library

00:54:25.380 --> 00:54:30.900
for it, put it on a place like a GitHub or Bitbucket and, made it available.

00:54:30.900 --> 00:54:33.820
And, and often these libraries are really, really well done.

00:54:33.820 --> 00:54:39.640
There's just been this culture of, of well-designed open source, particularly in the scientific Python

00:54:39.640 --> 00:54:40.240
community.

00:54:40.380 --> 00:54:45.860
And it means that, you know, you can, you can do a, an amazing number of things just

00:54:45.860 --> 00:54:48.720
out of the box with Python and the scientific installation.

00:54:48.720 --> 00:54:50.100
Yeah.

00:54:50.100 --> 00:54:52.040
What do you think that means for reproducibility?

00:54:52.040 --> 00:55:01.420
Like I want to store this thing, of code, the interpreter, maybe even like a Linux Docker

00:55:01.420 --> 00:55:04.580
image of the thing that I use to generate my paper.

00:55:04.980 --> 00:55:06.020
Yeah, that's huge.

00:55:06.020 --> 00:55:09.840
And, you know, it comes back to the, in the beginning, I was telling you how I got into

00:55:09.840 --> 00:55:11.360
the Python open source world.

00:55:11.360 --> 00:55:15.580
I realized that there was, I thought it was ridiculous that I'd spent this whole summer

00:55:15.580 --> 00:55:18.340
building this tool that then no one was going to be able to use.

00:55:18.340 --> 00:55:24.220
And, the tools in Python for enabling that sort of reproducibility, even having like an

00:55:24.220 --> 00:55:26.860
executable paper are, are huge.

00:55:26.860 --> 00:55:32.140
And I think it's really helping science drive, drive itself forward because we don't need

00:55:32.140 --> 00:55:34.360
to reinvent the wheel every time we do a new study.

00:55:34.360 --> 00:55:38.740
All right, Jake, I think that we have to leave it there for the topics, but I do have two final

00:55:38.740 --> 00:55:40.200
questions for you before I let you go.

00:55:40.200 --> 00:55:40.740
Okay.

00:55:40.740 --> 00:55:46.340
I just saw on PyPI that we passed 90,000 distinct packages.

00:55:46.340 --> 00:55:50.320
There's so many amazing things you can just pip install.

00:55:50.320 --> 00:55:55.080
And, in your field, you probably get exposed to really interesting things that maybe not

00:55:55.080 --> 00:55:59.540
everybody knows about, tell us about like one of your favorite PyPI packages that you might

00:55:59.540 --> 00:55:59.960
recommend.

00:55:59.960 --> 00:56:06.960
Well, I mentioned this earlier, but the MC, for Markov chain Monte Carlo, I think that's

00:56:06.960 --> 00:56:11.620
just an incredible package and allows you to do so much as far as, Beijing modeling.

00:56:11.620 --> 00:56:17.040
I could talk about one of my packages, but yeah, well, you know, it's your own packages are not

00:56:17.040 --> 00:56:17.520
off limits.

00:56:17.520 --> 00:56:20.080
Like Astro ML is all right, you know?

00:56:20.080 --> 00:56:20.680
Yeah.

00:56:20.680 --> 00:56:25.020
So one, one that I'm working on recently, which is a lot of fun is this, Altair package.

00:56:25.020 --> 00:56:28.900
And what, what it is, is a Python interface to Vega light.

00:56:28.900 --> 00:56:37.640
And Vega light is a, a visualization grammar aimed at statistical visualizations, that

00:56:37.640 --> 00:56:42.020
basically, outputs interactive, JavaScript plots.

00:56:42.020 --> 00:56:47.180
And so we've been, we've been writing this Python wrapper and trying to make a nice API to,

00:56:47.180 --> 00:56:51.140
to create Vega light grammars and Vega light, visualizations.

00:56:51.140 --> 00:56:56.640
I'm pretty excited about this because, you know, there's, there's so many options for plotting

00:56:56.640 --> 00:56:57.720
out there right now.

00:56:57.720 --> 00:57:04.900
There's, you know, there's matplotlib, there's bokeh, there's plotly, there's hollow views, there's

00:57:04.900 --> 00:57:07.380
ggplot wrapper for Python.

00:57:07.700 --> 00:57:11.780
There's, I'm going to miss something and someone's going to get mad at me.

00:57:11.780 --> 00:57:14.400
seaborn, you know, things like that.

00:57:14.400 --> 00:57:21.420
but the one thing that, that Altair, that's interesting about Altair is that it interfaces

00:57:21.420 --> 00:57:23.120
to this Vega light grammar.

00:57:23.120 --> 00:57:29.480
And that grammar, I think has the, has the possibility of becoming sort of a lingua franca between these

00:57:29.480 --> 00:57:31.140
various visualization packages.

00:57:31.360 --> 00:57:35.440
And the Vega light, if you, if you've heard of D3, which is driving a lot of interactive

00:57:35.440 --> 00:57:40.360
visualization on the web, Vega and Vega light are come out of the same research group.

00:57:40.360 --> 00:57:45.960
So it's people who really know what they're doing as far as, as visualization design.

00:57:45.960 --> 00:57:47.000
Yeah, that's cool.

00:57:47.000 --> 00:57:47.920
That's a great pedigree.

00:57:47.920 --> 00:57:48.820
Yeah.

00:57:48.820 --> 00:57:53.440
So, you know, and I, now I get to write the Python classes that output this stuff and

00:57:53.440 --> 00:57:54.120
it's pretty fun.

00:57:54.120 --> 00:57:54.940
Great.

00:57:54.940 --> 00:57:55.860
What's the package called?

00:57:55.860 --> 00:57:58.440
It's called Altair, A-L-T-A-I-R.

00:57:58.440 --> 00:57:59.400
All right.

00:57:59.400 --> 00:57:59.720
Awesome.

00:58:00.300 --> 00:58:00.560
All right.

00:58:00.560 --> 00:58:03.480
And when you write some Python code, what editor do you use?

00:58:03.480 --> 00:58:07.500
I, I go back and forth these days between Emacs and Atom.

00:58:07.500 --> 00:58:14.040
I actually like the Emacs key bindings, but I like the way that Atom, arranges an

00:58:14.040 --> 00:58:17.620
entire project and lets you, lets you see all the files.

00:58:17.620 --> 00:58:18.520
Yeah.

00:58:18.520 --> 00:58:18.920
Yeah.

00:58:18.920 --> 00:58:19.540
Those are both nice.

00:58:19.540 --> 00:58:19.820
Cool.

00:58:19.820 --> 00:58:20.780
All right.

00:58:20.780 --> 00:58:21.920
So any final call to action?

00:58:21.920 --> 00:58:28.020
I heard you had an announcement about your, PhD cohorts, like 50, 50 program.

00:58:28.020 --> 00:58:29.220
Yeah.

00:58:29.300 --> 00:58:29.440
Yeah.

00:58:29.440 --> 00:58:35.760
So, we, we just put out this announcement for our 20, 2017 PhD fellowships or postdoctoral

00:58:35.760 --> 00:58:36.500
fellowships.

00:58:36.500 --> 00:58:41.560
So, this is looking for people who have recently finished their PhD who are interested

00:58:41.560 --> 00:58:46.060
in continuing research in their own field, but also adding some sort of computational or

00:58:46.060 --> 00:58:47.480
data science element to it.

00:58:47.700 --> 00:58:50.500
And it's similar to the graduate program I described earlier.

00:58:50.500 --> 00:58:56.340
You have, you apply to have one foot in your domain department, one foot in the e-science

00:58:56.340 --> 00:59:01.500
institute with, with two advisors, one from the domain and one in a methodological area.

00:59:01.500 --> 00:59:07.880
And we have just, a great set of postdocs here who are doing some, really phenomenal

00:59:07.880 --> 00:59:08.960
work with that.

00:59:08.960 --> 00:59:15.500
And it, so if you're, if you're, graduating PhD students and this e-science institute or data

00:59:15.500 --> 00:59:16.800
science stuff sounds good.

00:59:16.800 --> 00:59:21.140
I'd encourage you to apply to applications or do sometime mid January.

00:59:21.140 --> 00:59:22.500
All right.

00:59:22.500 --> 00:59:23.900
That's plenty of time to get them in there.

00:59:23.900 --> 00:59:24.400
Cool.

00:59:24.400 --> 00:59:24.740
Yep.

00:59:24.740 --> 00:59:25.560
All right.

00:59:25.560 --> 00:59:26.540
And when's your book coming out?

00:59:26.540 --> 00:59:29.900
probably, probably January.

00:59:29.900 --> 00:59:31.920
I think, I don't know at this point.

00:59:31.920 --> 00:59:36.840
It depends how, how, how quickly I get this, this corrected manuscript back to them.

00:59:36.840 --> 00:59:38.640
Yeah, of course.

00:59:38.640 --> 00:59:41.720
I saw that you can get like an early access version of it, right?

00:59:41.720 --> 00:59:42.660
Yeah.

00:59:42.660 --> 00:59:43.800
The early access is there.

00:59:43.800 --> 00:59:48.880
So if you want to take a look at the pre-release right now, you can, you can go buy it and they'll

00:59:48.880 --> 00:59:51.220
update you to the released version when it comes out.

00:59:51.220 --> 00:59:52.180
All right.

00:59:52.180 --> 00:59:52.760
Sounds great.

00:59:53.700 --> 00:59:56.940
So Jake, it's been super interesting talking about astronomy with you.

00:59:56.940 --> 00:59:58.780
Thanks for coming on the show and sharing your story.

00:59:58.780 --> 00:59:59.460
Yeah.

00:59:59.460 --> 01:00:00.380
Thanks for having me.

01:00:00.380 --> 01:00:00.980
You bet.

01:00:00.980 --> 01:00:01.440
Bye-bye.

01:00:01.440 --> 01:00:05.680
This has been another episode of talk Python to me.

01:00:05.680 --> 01:00:08.340
Today's guest has been Jake Vanderplast.

01:00:08.340 --> 01:00:11.480
And this episode has been sponsored by GoCD and data school.

01:00:11.480 --> 01:00:13.340
Thank you both for supporting the show.

01:00:13.340 --> 01:00:18.400
GoCD is the on-premise open source continuous delivery server.

01:00:18.400 --> 01:00:22.520
Want to improve your deployment workflow, but keep your code and builds in-house.

01:00:23.000 --> 01:00:28.880
Check out GoCD at talkpython.fm/gocd and take control over your process.

01:00:28.880 --> 01:00:34.080
Data school is here to help you become effective with Python's data science tools quickly.

01:00:34.080 --> 01:00:35.920
Skip years at the university.

01:00:35.920 --> 01:00:40.860
Check out the talk Python to me learning path at talkpython.fm/data school.

01:00:40.860 --> 01:00:43.760
Are you or a colleague trying to learn Python?

01:00:43.760 --> 01:00:48.420
Have you tried books and videos that just left you bored by covering topics point by point?

01:00:48.420 --> 01:00:57.040
Well, check out my online course Python Jumpstart by building 10 apps at talkpython.fm/course to experience a more engaging way to learn Python.

01:00:57.580 --> 01:01:04.360
And if you're looking for something a little more advanced, try my write Pythonic code course at talkpython.fm/pythonic.

01:01:04.740 --> 01:01:11.020
You can find the links from this episode at talkpython.fm/episodes slash show slash 81.

01:01:11.020 --> 01:01:13.140
Be sure to subscribe to the show.

01:01:13.140 --> 01:01:15.320
Open your favorite podcatcher and search for Python.

01:01:15.320 --> 01:01:16.560
We should be right at the top.

01:01:16.560 --> 01:01:25.840
You can also find the iTunes feed at /itunes, Google Play feed at /play, and direct RSS feed at /rss on talkpython.fm.

01:01:25.840 --> 01:01:30.960
Our theme music is Developers, Developers, Developers by Corey Smith, who goes by Smix.

01:01:30.960 --> 01:01:37.660
Corey just recently started selling his tracks on iTunes, so I recommend you check it out at talkpython.fm/music.

01:01:37.660 --> 01:01:43.000
You can browse his tracks he has for sale on iTunes and listen to the full-length version of the theme song.

01:01:43.000 --> 01:01:45.080
This is your host, Michael Kennedy.

01:01:45.080 --> 01:01:46.380
Thanks so much for listening.

01:01:46.380 --> 01:01:48.180
I really appreciate it.

01:01:48.180 --> 01:01:49.720
Smix, let's get out of here.

01:01:49.720 --> 01:01:51.460
Stay tuned.

01:01:51.460 --> 01:01:51.460
Stay tuned.

01:01:51.460 --> 01:01:51.540
Stay tuned.

01:01:51.540 --> 01:01:53.280
Stay tuned.

01:01:53.280 --> 01:01:53.280
Stay tuned.

01:01:53.280 --> 01:01:53.540
Stay tuned.

01:01:53.540 --> 01:01:55.280
Stay tuned.

01:01:55.280 --> 01:01:57.020
I'll be right back.

01:01:57.020 --> 01:01:59.700
I'll pass the mic back to who rocked it best.

01:01:59.700 --> 01:01:59.760
I'll pass the mic back to who rocked it best.

01:01:59.760 --> 01:01:59.760
I'll pass the mic back to who rocked it best.

01:01:59.760 --> 01:02:11.760
I'll pass the mic back to who rocked it best.

01:02:11.760 --> 01:02:11.920
you

01:02:11.920 --> 01:02:12.420
you

01:02:12.420 --> 01:02:42.400
Thank you.

