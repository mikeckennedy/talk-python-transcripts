WEBVTT

00:00:00.001 --> 00:00:04.240
Data science has been one of the major driving forces behind the explosion of Python in recent

00:00:04.240 --> 00:00:09.780
years. It's now used for AI research, it controls some of the most powerful telescopes in the world,

00:00:09.780 --> 00:00:14.000
it tracks crop growth and prediction, and so much more. But with all this growth,

00:00:14.000 --> 00:00:18.720
there's an explosion of data science machine learning libraries. That's why I invited Pete

00:00:18.720 --> 00:00:23.400
Garson onto the show. He's going to share his top 10 machine learning libraries for Python.

00:00:23.400 --> 00:00:26.760
After this episode, you should be able to pick the right one for the job.

00:00:27.320 --> 00:00:31.100
This is Talk Python To Me, recorded July 20th, 2017.

00:00:31.100 --> 00:00:50.120
Welcome to Talk Python To Me, a weekly podcast on Python, the language, the libraries, the ecosystem,

00:00:50.120 --> 00:00:55.340
and the personalities. This is your host, Michael Kennedy. Follow me on Twitter where I'm @mkennedy.

00:00:55.340 --> 00:01:00.640
Keep up with the show and listen to past episodes at talkpython.fm and follow the show on Twitter

00:01:00.640 --> 00:01:06.360
via at Talk Python. Talk Python To Me is partially supported by our training courses. Here's an

00:01:06.360 --> 00:01:12.560
unexpected question for you. Are you a C-sharp or .NET developer getting into Python? Do you work at a

00:01:12.560 --> 00:01:17.740
company that used to be a Microsoft shop, but is now finding their way over to the Python space?

00:01:18.120 --> 00:01:23.920
We built a Python course tailor-made for you and your team. It's called Python for the .NET

00:01:23.920 --> 00:01:29.260
developer. This 10-hour course takes all the features of C-sharp and .NET that you think you

00:01:29.260 --> 00:01:34.920
couldn't live without. Unity Framework, Lambda Expressions, ASP.NET, and so on. And it teaches

00:01:34.920 --> 00:01:39.840
you the Python equivalent for each and every one of those. This is definitely the fastest and clearest

00:01:39.840 --> 00:01:46.960
path from C-sharp to Python. Learn more at talkpython.fm/.NET. That's talkpython.fm slash

00:01:46.960 --> 00:01:48.360
D-O-T-N-E-T.

00:01:48.360 --> 00:01:50.940
Pete, welcome to Talk Python.

00:01:50.940 --> 00:01:52.740
Thanks. I'm happy to be here.

00:01:52.740 --> 00:01:58.000
That's great to have you here. And I've done a few shows on machine learning and data science,

00:01:58.000 --> 00:02:02.560
but I'm really happy to do this one because I think it's really accessible to everyone. We're

00:02:02.560 --> 00:02:06.560
going to bring all these different libraries together and kind of just make people aware of

00:02:06.560 --> 00:02:09.360
all the cool things that are out there for data science, machine learning.

00:02:09.360 --> 00:02:15.980
Yeah, it's really crazy actually how many libraries are out there and how active the development is on

00:02:15.980 --> 00:02:20.460
all of them. There's new contributions, new developments all the time. And it seems like

00:02:20.460 --> 00:02:22.860
there's new projects popping up like almost daily.

00:02:22.860 --> 00:02:27.080
Yeah, it's definitely tough to keep up with, but hopefully this adds a little bit of help

00:02:27.080 --> 00:02:30.580
for the reference there. But before we get into all these libraries, let's start with your story.

00:02:30.580 --> 00:02:32.120
How did you get into programming in Python?

00:02:32.120 --> 00:02:37.480
I started programming at a pretty young age, like sort of back before Stack Overflow and

00:02:37.480 --> 00:02:43.840
things like that existed. And I sort of mostly made games. I started with basic, like most people

00:02:43.840 --> 00:02:50.380
probably from a certain age, and then worked into working on Pascal and was making games for my BBS

00:02:50.380 --> 00:02:56.540
back in the day, making online games, utilities and stuff like that. And then for Python, later when I

00:02:56.540 --> 00:03:04.160
worked in games for a long time. And when I worked in games, we were doing like tool automations, like build

00:03:04.160 --> 00:03:10.960
automation, certain workflow automation, build pipelines, all that kind of stuff. So Python was something, a tool that we

00:03:10.960 --> 00:03:15.120
used quite a lot there. So that was where I got my start with Python.

00:03:15.120 --> 00:03:21.680
Oh, yeah, that's really cool. Python is huge in the workflow for games and movies, way more than people on the outside

00:03:21.680 --> 00:03:26.900
realize, I think. Yeah, especially for artists. So like a lot of the tools have Python built into them.

00:03:26.900 --> 00:03:35.780
And so artists will use it for like automating model exports or rigging and that kind of stuff. So it's pretty

00:03:35.780 --> 00:03:40.800
popular in that sense. And then also still even just for like building assets for games.

00:03:40.800 --> 00:03:46.820
Okay, I'm intrigued by your BBS stuff. It occurs to me and it's kind of crazy. There may be younger

00:03:46.820 --> 00:03:54.380
people listening that don't actually know what a BBS is. Okay, so a BBS is short for bulletin board system.

00:03:54.380 --> 00:04:02.380
And it was sort of like in a way the precursor to the internet where you used to host what is

00:04:02.380 --> 00:04:08.400
effectively sort of a website on your home computer. And people would like call your phone number.

00:04:08.400 --> 00:04:12.400
And you'd have it hooked up to your modem. And they would like call your phone number,

00:04:12.760 --> 00:04:18.520
connect to your home computer. So in my case, it was like my computer that I played games on and

00:04:18.520 --> 00:04:24.380
did my homework on and that kind of stuff. And they could connect and send messages to each other

00:04:24.380 --> 00:04:30.620
and download files and play games, very simple games, that kind of thing. So it was like a...

00:04:30.620 --> 00:04:31.700
Yeah, it was so fun.

00:04:31.700 --> 00:04:36.380
Yeah, it was awesome. And like, I really, really enjoyed it. And you had a thing called like echo

00:04:36.380 --> 00:04:41.740
mail back then, which was like this sort of way of like transferring messages all over the world. So,

00:04:41.940 --> 00:04:45.720
you know, somebody would send a message on your BBS, and then it would like call a whole bunch of

00:04:45.720 --> 00:04:50.440
others like in this network. And then somebody in like Australia might answer it. And it would take

00:04:50.440 --> 00:04:56.440
like days to get back because it would be like this chain of people's BBS is calling the next one. So

00:04:56.440 --> 00:05:01.780
yeah, there was no internet. It was the craziest thing. Like, we at our house, my brother and I

00:05:01.780 --> 00:05:08.280
had talked my dad into getting us multiple phone lines so we could work with BBS is like in parallel.

00:05:08.900 --> 00:05:14.580
And you would send these mails and like at night, there would be like a, like a coordination of the

00:05:14.580 --> 00:05:19.400
emails across the globe as these things would like sync up the emails they got queued up. It was the

00:05:19.400 --> 00:05:23.800
weirdest thing. But I loved it. I don't know, what's it? Trade Wars or Planet Wars? One of those games. I

00:05:23.800 --> 00:05:24.400
really loved it.

00:05:24.400 --> 00:05:29.280
For sure. I'm like a huge Trade Wars fan. You can actually play it now. Like there are people who have

00:05:29.280 --> 00:05:35.860
it set up on like websites that have like simulated Telnet stuff. And you can, you can play versions of Trade

00:05:35.860 --> 00:05:40.860
Wars, which I have done recently just to like, don't tell me that you're going to ruin my productivity

00:05:40.860 --> 00:05:46.780
for like the whole day. Yeah. You'll be after this. You'll be like, Oh, Trade Wars 2002. Is it still,

00:05:46.780 --> 00:05:51.620
it's still around? People still play it, but it was such a good game. It's fantastic. Yeah, it was

00:05:51.620 --> 00:05:56.960
fantastic. It's awesome. All right. So that's how you got into this whole thing. Like what do you do

00:05:56.960 --> 00:05:58.540
today? You work at ActiveState, right?

00:05:58.620 --> 00:06:03.400
I do. Yeah. I'm a dev evangelist at ActiveState. So generally that means working with developers,

00:06:03.400 --> 00:06:06.820
language communities, trying to make our distributions better. So at ActiveState,

00:06:06.820 --> 00:06:11.180
we do language distributions. Probably a lot of people in the Python space know us that we do

00:06:11.180 --> 00:06:16.560
ActivePython and it's been around for a long time. We were founding member of the Python Software

00:06:16.560 --> 00:06:21.940
Foundation. And so ActiveState has a pretty long history in the Python community. And before that,

00:06:22.020 --> 00:06:28.600
we were, people probably know us from Perl and now we have a Go distribution and Ruby beta coming out

00:06:28.600 --> 00:06:34.040
soon. So we're sort of expanding to all these different dynamic language ecosystems.

00:06:34.040 --> 00:06:40.780
Sure. That's awesome. So I know that maybe people are a little familiar with some of the advantages of

00:06:40.780 --> 00:06:46.400
these higher order distributions for Python, but maybe give us a sense of like, what is the value of

00:06:46.400 --> 00:06:50.520
these distributions over like going and grabbing Go or grabbing Python and just running with that?

00:06:50.660 --> 00:06:58.000
I think that you've got obviously this sense of curated packages. So there are, you know, in the

00:06:58.000 --> 00:07:02.480
Python distribution, there's like over 300 packages. And so you know that they're going to build, know they're

00:07:02.480 --> 00:07:06.800
going to play nice with each other, know that they have current stable versions, all that kind of stuff.

00:07:06.800 --> 00:07:13.400
And then additionally, you can buy commercial support. So for a lot of our customers, so we have a lot of like

00:07:13.400 --> 00:07:19.660
large enterprise customers, they can't actually adopt a language distribution or a tool like that

00:07:19.660 --> 00:07:24.180
without commercial support. They need to know that somebody has their back. And so that's something

00:07:24.180 --> 00:07:29.440
that we offer on these language distributions for those large customers. But for the community and for

00:07:29.440 --> 00:07:36.520
individual developers, then that is something that having that curated set of packages that you know

00:07:36.520 --> 00:07:40.800
is going to work, that you know is going to play nice. And that also is a, maybe a development team

00:07:40.800 --> 00:07:47.080
lead, you might want a unified install base, so that all your developers have the same development

00:07:47.080 --> 00:07:51.060
environment and they and you know, it's all going to play nice. And so that's something that's one of

00:07:51.060 --> 00:07:51.980
the advantages of those.

00:07:51.980 --> 00:07:57.760
That's really cool. Certainly the ability to install things that have weird compilation stuff. Do you guys

00:07:57.760 --> 00:08:02.260
ship the binaries like pre built for that? So I don't have to have like a Fortran compiler or

00:08:02.260 --> 00:08:02.740
something weird?

00:08:03.080 --> 00:08:08.420
Exactly. Yes. So they're all pre built, all pre compiled. So I mean, a lot of people depending

00:08:08.420 --> 00:08:12.360
on what platform you're on, like on Windows, you're not might not even have a C compiler installed and a

00:08:12.360 --> 00:08:17.340
lot of packages are C based. And so they're pre built, you don't and like you said, you don't need a

00:08:17.340 --> 00:08:22.960
Fortran compiler or some, some exotic build tool to actually make it work. It just works out of the box.

00:08:23.200 --> 00:08:29.900
Yeah. Okay, that's really awesome. And active Python is free. If I'm like a random person, not a huge

00:08:29.900 --> 00:08:31.860
corporate that wants support.

00:08:31.860 --> 00:08:37.720
Exactly. Yeah. If you're just a, you know, a developer, it's free to download and free to use.

00:08:37.720 --> 00:08:43.340
And it even if you are, you know, a large corporation, it's free to use in non production settings. So on your

00:08:43.340 --> 00:08:47.220
own. So it's, you know, you can go and just download it, try it out, see if it works for you.

00:08:47.220 --> 00:08:48.980
Okay, yeah, that sounds sounds awesome.

00:08:49.980 --> 00:08:54.440
How many of the 10 libraries we're going to talk about would come built in? Do you know off the top

00:08:54.440 --> 00:08:54.740
of your head?

00:08:54.740 --> 00:09:03.760
I think that actually almost all of them, but maybe I think cafe is on the list. It's not in the current

00:09:03.760 --> 00:09:08.880
one, but it is on the list to be included. So I think actually like pretty much all of the other

00:09:08.880 --> 00:09:15.020
ones, maybe CNTK as well is still new as well. That's really new. So but you know, we are targeting

00:09:15.020 --> 00:09:19.960
to have as many of these as we possibly can. And so pretty much most of them are included.

00:09:20.160 --> 00:09:23.940
That's awesome. So all the libraries that we're talking about, like one really nice way to just

00:09:23.940 --> 00:09:28.280
get up to speed with them would be grab active, active Python, and you'd be ready to roll.

00:09:28.280 --> 00:09:29.640
Exactly. Yeah, awesome.

00:09:29.640 --> 00:09:33.440
Grab them, install them, you're ready to roll right out of the gate.

00:09:33.440 --> 00:09:40.160
Cool. All right. So let's start at what I would consider the foundation of them. The first library

00:09:40.160 --> 00:09:42.660
that you picked, which is NumPy and SciPy.

00:09:42.860 --> 00:09:47.360
Absolutely. And they are foundational in the sense that a lot of other libraries either

00:09:47.360 --> 00:09:54.180
depend on them or are in fact built like on top of them. Right. So they're, they are sort

00:09:54.180 --> 00:09:59.740
of the base of a lot of these other libraries. And most people might have worked with, with

00:09:59.740 --> 00:10:05.700
NumPy sort of the, its main feature is that sort of n dimensional array structure that it

00:10:05.700 --> 00:10:11.460
includes. And a lot of the data that is shipped to a lot of the other libraries is either supported

00:10:11.460 --> 00:10:17.680
that you can send it a NumPy array, or it requires that you, that you format it that way. So especially

00:10:17.680 --> 00:10:22.680
when you're doing machine learning, you're doing a lot of matrices and a lot of like higher dimensional

00:10:22.680 --> 00:10:28.260
data, depending on how many features you have. It's a really, really useful data structure to have

00:10:28.260 --> 00:10:29.280
in place.

00:10:29.280 --> 00:10:37.220
Yeah. So NumPy is this sort of array like multi-dimensional array like thing that stores and

00:10:37.220 --> 00:10:44.380
does a lot of its processing down in a C level, but has of course, it's programming API and Python,

00:10:44.380 --> 00:10:44.740
right?

00:10:44.740 --> 00:10:51.780
Yes. Yeah, exactly. And a lot of these machine learning libraries do tend to have C level,

00:10:51.780 --> 00:10:59.580
like lowest level implementations with a Python API. And that's predominantly for speed. So when

00:10:59.580 --> 00:11:05.100
you're doing tons and tons and tons of calculations, and you need them to be really, really lightning fast,

00:11:05.440 --> 00:11:09.860
that's the primary reason that they do these things, you know, sort of at the C level.

00:11:09.860 --> 00:11:15.980
All right, absolutely. And so related to this is SciPy. They're kind of grouped under the same

00:11:15.980 --> 00:11:18.900
organization, but they're not the same library exactly, are they?

00:11:18.900 --> 00:11:26.520
No. So SciPy is like a more scientific mathematical computing thing. And it has the more advanced like

00:11:26.520 --> 00:11:32.720
linear algebra and like Fourier transforms, image processing, it has like a physics calculation

00:11:32.720 --> 00:11:38.600
stuff built in. So most like scientific numerical computing functionality is built into SciPy.

00:11:38.600 --> 00:11:43.700
I know that NumPy does have like linear algebra and stuff in it. But I think that the preferred is

00:11:43.700 --> 00:11:47.280
that you use SciPy for all that kind of linear algebra crunching.

00:11:47.280 --> 00:11:52.100
Okay, yeah. So a lot of these things that we're going to talk about will somewhere in them have as

00:11:52.100 --> 00:11:57.320
a dependency or an internal implementation of some variation, or even in maybe in its API,

00:11:57.320 --> 00:12:02.360
like the ability to pass between them, these NumPy arrays and things like that.

00:12:02.360 --> 00:12:02.840
Absolutely.

00:12:02.840 --> 00:12:09.840
Yeah. One other thing that's worth noting, that's pretty interesting. And I think this is a trend

00:12:09.840 --> 00:12:16.620
that's growing. Maybe you guys have more visibility into it than I do. But NumPy in June 13th, 2017,

00:12:16.860 --> 00:12:24.860
so about a month ago at the time of the recording, received a $645,000 grant for the next two years to

00:12:24.860 --> 00:12:28.280
grow it and evolve it and keep it going strong. That's pretty cool.

00:12:28.280 --> 00:12:34.420
It is very cool. And I think that you're starting to see that these open source projects are really

00:12:34.420 --> 00:12:40.820
forming the backbone of most of the machine learning research and actually implementation that you're

00:12:40.820 --> 00:12:45.560
seeing out there in the world. There's not a lot of sort of more closed source behind trade secret

00:12:45.560 --> 00:12:50.100
stuff. A lot of the most bleeding edge development and active development is happening in these open

00:12:50.100 --> 00:12:55.420
source projects. So I think it's great to see them receiving funding and sponsorship like that.

00:12:55.420 --> 00:12:59.920
Yeah, I totally agree. And it's just going to mean more good things for the community and all these

00:12:59.920 --> 00:13:04.860
projects. It's really great to see. One thing I want to touch on for every one of these is to give

00:13:04.860 --> 00:13:11.120
you a sense of how popular they are. And for each one, we'll say the number of GitHub stars and forks.

00:13:11.120 --> 00:13:17.840
And that's not necessarily the exact right measure for the popularity because maybe this is you like

00:13:17.840 --> 00:13:22.080
obviously NumPy is used across many of these other things which have more stars, but people don't

00:13:22.080 --> 00:13:30.660
necessarily contribute directly to NumPy. So on. But for NumPy, NumPy has about 5,000 stars and 2,000

00:13:30.660 --> 00:13:37.120
forks to give you a sense of how popular it is. The next one up, scikit-learn has 20,000 stars and 10,000

00:13:37.120 --> 00:13:44.020
forks. So tell us about scikit-learn. scikit-learn is, again, like we mentioned before, is a thing

00:13:44.020 --> 00:13:50.080
that's built on top of scipy and NumPy and is a very popular library for machine learning in Python.

00:13:50.080 --> 00:13:56.760
And I think it was one of the first, if not the first, I'm not 100% sure, but it's been around for

00:13:56.760 --> 00:14:03.480
quite a long time. And it supports a lot of the sort of most common algorithms for machine learning.

00:14:03.600 --> 00:14:08.820
So that's like classification, regression tools, all that kind of stuff. I actually just saw like a blog post

00:14:08.820 --> 00:14:16.140
come up in my feed today where Airbnb was using scikit-learn to do some kind of like property value

00:14:16.140 --> 00:14:22.380
estimation or something using machine learning. So it's being used very, very widely in a lot of different

00:14:22.380 --> 00:14:22.920
scenarios.

00:14:22.920 --> 00:14:30.540
Oh yeah, that sounds really cool. It definitely is one of the early ones. And it's kind of simpler in the

00:14:30.540 --> 00:14:36.020
sense that it doesn't deal with all the GPUs and parallelization and all that kind of stuff. It just,

00:14:36.020 --> 00:14:42.640
it talks about classification, regression, clustering, dimensionality, and modeling, things like that,

00:14:42.640 --> 00:14:42.840
right?

00:14:42.840 --> 00:14:48.600
Yes, that's right. It doesn't have GPU support. And that can make it a little bit easier to install if

00:14:48.600 --> 00:14:52.720
you, you know, sometimes the GPU stuff can have a lot more dependencies that you need to install to make

00:14:52.720 --> 00:14:58.200
it work. Although that's getting better in the other libraries. And it's like you say,

00:14:58.260 --> 00:15:02.500
it is made and sort of designed to be pretty accessible and pretty easy, you know, because

00:15:02.500 --> 00:15:07.380
it has the sort of baked in algorithms that you can just say, oh, I want to do this and it will

00:15:07.380 --> 00:15:13.020
crunch out your results for you. So I think that that's sort of the sort of ease of use and the sort

00:15:13.020 --> 00:15:19.920
of cleanliness of its API has contributed to its sort of longevity as a, one of the most popular

00:15:19.920 --> 00:15:21.100
machine learning libraries.

00:15:21.100 --> 00:15:27.180
Yeah, absolutely. And it's obviously scikit-learn being part of the scipy whole family. It's built

00:15:27.180 --> 00:15:30.060
on numpy, scipy, and matplotlib.

00:15:30.060 --> 00:15:36.720
Yes. Yes. So yeah, it includes interfaces for all that stuff and for like graphing the output and

00:15:36.720 --> 00:15:43.300
using matplotlib and yeah, using numpy for inputting your data and for getting your data results,

00:15:43.340 --> 00:15:44.060
all that kind of stuff.

00:15:44.060 --> 00:15:52.500
Yeah. Very cool. All right. Next up is Keras at 17.7 thousand stars and 6,000 forks.

00:15:52.500 --> 00:15:56.700
So this one is for deep learning specifically, right?

00:15:56.980 --> 00:16:06.940
Yeah. And so this is for doing rapid development of neural networks in Python. It's one of the

00:16:06.940 --> 00:16:12.140
newest ones, but it's really, really popular. I've had some experience working with it directly

00:16:12.140 --> 00:16:17.840
myself and I was sort of really, really blown away by how simple and straightforward it is.

00:16:18.020 --> 00:16:25.380
So there's like, it creates a layer on top of lower level libs like TensorFlow and Theano and lets you

00:16:25.380 --> 00:16:32.000
just sort of define, I want my network to look like this. So I want it to have this many layers and this

00:16:32.000 --> 00:16:37.880
many nodes per layer. And here are the activation functions. And, you know, here's the optimization

00:16:37.880 --> 00:16:42.220
method that I want to use. And you sort of just define this effectively a configuration,

00:16:42.700 --> 00:16:48.580
and then it will build all of the graph for you, depending on what backend you used.

00:16:48.580 --> 00:16:56.600
And so it's very, very easy to experiment with the like shape of your network and with the different

00:16:56.600 --> 00:17:04.160
activation functions. So it lets you kind of really quickly reach and test, you know, different models

00:17:04.160 --> 00:17:10.140
to see which one works better and to sort of see what one works at all. So it's really easy to use

00:17:10.140 --> 00:17:18.640
and really very effective. I used it to build a little game demo where we like had an AI where

00:17:18.640 --> 00:17:22.840
I trained an AI to play against you to determine when it could shoot at you.

00:17:22.840 --> 00:17:25.300
Was this the demo you had at PyCon?

00:17:25.300 --> 00:17:31.840
It is. Yeah. Yeah. And so we had that demo at PyCon. I since did a blog post about it a little bit.

00:17:31.980 --> 00:17:38.900
And then I actually just recently rewrote it in Go for Go4Con too. So eventually it will be open sourced

00:17:38.900 --> 00:17:44.440
so that people can see. But one of the things that you really notice is that the actual like code for

00:17:44.440 --> 00:17:52.260
Keras to basically define the network and do the sort of machine learning heavy lifting part is very,

00:17:52.260 --> 00:17:57.600
very minimal, like a dozen lines of code or something like that. It's really surprising because you think

00:17:57.600 --> 00:18:02.900
it's like a ton of work, but it makes it super easy. Yeah, that's really cool. And it sounds like

00:18:02.900 --> 00:18:09.480
its goal is to be very easy to get started with. I like the idea of the ability to switch out the

00:18:09.480 --> 00:18:18.760
backend from say TensorFlow to CMTK to Theano. How easy is it to do that? Like if I'm, could I run some

00:18:18.760 --> 00:18:24.860
machine learning algorithms and say, let's try it in TensorFlow and say, do some performance benchmarks

00:18:24.860 --> 00:18:29.540
and stuff? No, no, let's switch it over to Theano and try it here and kind of experiment rather than

00:18:29.540 --> 00:18:34.340
completely rewriting in those various APIs. Exactly. You literally, it's just a configuration

00:18:34.340 --> 00:18:40.160
things. You just, it's almost like a tick box essentially, you know, like it's so easy.

00:18:40.160 --> 00:18:47.280
And so that is absolutely one of the, I think the driving key features of that library that you can

00:18:47.280 --> 00:18:52.000
just pick whichever one suits your purpose or your platform, you know, depending on what's available

00:18:52.000 --> 00:18:57.400
on the platform that you're building for. Cause currently there's not TensorFlow versions for

00:18:57.400 --> 00:19:01.860
every platform on every version of Python and all that kind of stuff. Right. Okay. Well, that's,

00:19:01.860 --> 00:19:07.420
that's pretty cool. So there's two interesting things about this library. One is the fact that it does

00:19:07.420 --> 00:19:13.800
deep learning. So maybe tell people about what deep learning is. How does that relate to like

00:19:13.800 --> 00:19:17.220
standard neural networks or other types of machine learning stuff?

00:19:17.220 --> 00:19:24.460
Well, I think the sort of the simplest way to put it is the idea of like adding these additional layers

00:19:24.460 --> 00:19:35.100
to your network to create a more sophisticated model. So that allows you to create things that can take

00:19:35.100 --> 00:19:44.160
more sophisticated feature domains and then map those to an output more reliably. So, and that's where

00:19:44.160 --> 00:19:48.760
you've seen a lot of advances, for instance, like in like a lot of the image recognition stuff that

00:19:48.760 --> 00:19:55.400
leverages deep learning to be really, really good at identifying images or even doing things like

00:19:55.400 --> 00:20:03.520
style transfer on images where you have a photograph of some scene and then you have some other photograph

00:20:03.520 --> 00:20:09.960
and you're like, I want to transfer the style of the evening to my daytime photograph. And it will just

00:20:09.960 --> 00:20:15.640
do it and it looks like pretty normal. And those are like the most, I guess, popular, common,

00:20:15.640 --> 00:20:18.400
deep learning examples that you see cited.

00:20:18.400 --> 00:20:22.320
Yeah, it makes a lot of sense. And you know, it's, it's easy to think of these as being like,

00:20:22.320 --> 00:20:29.860
I know, Snapchatty, like, sort of superfluous type of examples. But you know, machine learning,

00:20:29.860 --> 00:20:35.680
doing them, like, you know, putting the little cat face on or switching faces or whatever. But,

00:20:35.980 --> 00:20:40.480
you know, there's real meaningful things that can come out of this. Like, for example,

00:20:40.480 --> 00:20:48.120
the detection of tumors in radiology scans, and things like that. And these deep learning models

00:20:48.120 --> 00:20:53.020
can do the image recognition on that and go, yep, that's cancer, you know, maybe better than even

00:20:53.020 --> 00:20:56.900
radiologists can already. And then in the future, it's gonna get crazy.

00:20:57.300 --> 00:21:01.240
Exactly. And it's funny, you mentioned that Stanford Medical about a month ago,

00:21:01.240 --> 00:21:07.700
month and a half ago, actually released like, I don't know how many, like 500,000 radiology scans

00:21:07.700 --> 00:21:14.260
that are like annotated and ready for training machine learning. So that exact use case is intended

00:21:14.260 --> 00:21:21.600
to be like a deep learning problem to be applied. And there are all kinds of additionals of these

00:21:21.600 --> 00:21:27.300
datasets that are coming out. I just saw a post this week about deep learning model that was using

00:21:27.300 --> 00:21:33.900
that was measuring heart monitor data and being more effective than cardiologists kind of thing. So

00:21:33.900 --> 00:21:39.860
It's really crazy. You think of this AI and automation disrupting low end jobs, right? Like,

00:21:39.860 --> 00:21:46.320
at McDonald's, we might have robots making our hamburgers or something silly like that. But if they start

00:21:46.320 --> 00:21:52.040
cutting into radiology and cardiologists, and that's, that's gonna like, it's gonna be a big deal.

00:21:52.040 --> 00:21:57.820
It absolutely is gonna be a big deal. I think people probably start need to start thinking about it. I don't think

00:21:57.820 --> 00:22:04.820
it's necessarily a complete replacement thing. It's not, you know, the radiologist AI can't talk to you

00:22:04.820 --> 00:22:12.460
yet, I guess. And until wait till we get to NLTK, but it can definitely augment and lighten the load

00:22:12.460 --> 00:22:18.100
on professions like medicine that are, you know, perpetually overworked and allow them to be more

00:22:18.100 --> 00:22:22.960
effective, you know, human doctors. So I think like as tools, these things are going to be absolutely

00:22:22.960 --> 00:22:26.980
incredibly revolutionary. Yeah, it's gonna be amazing. You know, do you want a second opinion?

00:22:27.080 --> 00:22:29.660
Let's ask, let's ask the super machine.

00:22:29.660 --> 00:22:35.460
Exactly. But I mean, it's able to one of the strengths of all these machine learning models

00:22:35.460 --> 00:22:43.800
is that the machine learning models are able to visualize higher dimensional complex data sets

00:22:43.800 --> 00:22:50.060
in ways that like humans can't really do. And they have like just intense focus, I guess,

00:22:50.060 --> 00:22:56.280
right? These models, whereas it might be, it's pretty hard for a doctor to read every single paper ever

00:22:56.340 --> 00:23:03.420
written on subject X or to look at 500,000 radiology images even across the course of their career.

00:23:03.420 --> 00:23:08.340
So pretty optimistic where this goes, it's going to be interesting to join all this stuff together.

00:23:08.340 --> 00:23:13.760
The other thing that we're just starting to touch on here, and it's going to appear in a bunch of

00:23:13.760 --> 00:23:21.740
these others. So maybe worth spending a moment on as well is Karis lets you basically seamlessly switch

00:23:21.740 --> 00:23:30.280
from CPU computation and GPU computation. So maybe not everyone knows like the power of non visual GPU

00:23:30.280 --> 00:23:32.140
programming. Maybe talk about that a bit.

00:23:32.340 --> 00:23:38.500
For sure. So your GPU, which is a graphics processing unit. So, you know, if you have a gaming PC at home,

00:23:38.500 --> 00:23:43.140
and you have like, you know what I mean, an Nvidia graphics card or an ATI grout.

00:23:43.140 --> 00:23:46.020
Can run the Unreal Engine like crazy or whatever, right?

00:23:46.020 --> 00:23:50.440
Oh, exactly. So if you have if you play games, and you have a dedicated graphics card, you well,

00:23:50.500 --> 00:23:56.240
even without a dedicated graphics card, but you have a GPU, and there's this thing called general purpose GPU

00:23:56.240 --> 00:24:03.480
programming. So that originally, like a GPU is highly parallel computer has like 1000 cores in it,

00:24:03.480 --> 00:24:09.360
or whatever, something some huge number of cores. Yeah, the one to four or 5000 cores per GPU, right?

00:24:09.620 --> 00:24:15.780
Exactly. Yeah. And so like the intention there was originally that it's because it needs to, in parallel process

00:24:15.780 --> 00:24:21.560
every pixel, or every polygon that's going on the screen, right, and perform like effects. So that's why you can get

00:24:21.560 --> 00:24:28.260
like blur and all this kind of stuff in real time, and real time lighting and all that kind of stuff. So it process

00:24:28.260 --> 00:24:36.160
all that stuff in parallel. But then as the people started to develop SDKs that let you like, well, in addition to

00:24:36.160 --> 00:24:41.800
doing graphics programming, we can just run regular programs on these things. And they're really, really

00:24:41.800 --> 00:24:49.020
fast that cut doing math programs. So we can do that. And so now, basically, a lot of these libraries

00:24:49.020 --> 00:24:54.140
support GPU processing, and it's literally just like a compile flag. Now it's getting a lot easier, you know,

00:24:54.140 --> 00:24:58.820
you still have to make sure you have the drivers and that you you know, you have a GPU that's reasonably

00:24:58.820 --> 00:25:05.980
powerful that's and especially if you're doing a lot of computation. And so then you can basically run

00:25:05.980 --> 00:25:13.780
these giant ml models on your GPU. And again, it's something that's pretty, pretty well suited to

00:25:13.780 --> 00:25:19.700
being parallelized. So that is really great use of GPU. And that's why you're seeing it take off,

00:25:19.700 --> 00:25:25.420
because these models are are easily made parallel. Yeah, they're what are called embarrassingly parallel

00:25:25.420 --> 00:25:29.900
algorithms, right? And just throw them at this, these things with 4000 cores and let them go crazy.

00:25:29.900 --> 00:25:35.220
Yeah, the early days, I mean, still, I guess, when you're doing direct decks or OpenGL, or these things,

00:25:35.220 --> 00:25:39.180
like, it's really all about I want to rotate the screen. So that's like a matrix multiplication

00:25:39.180 --> 00:25:45.000
against all of the vector things. And it's really similar, actually, the type of work it has to do.

00:25:45.000 --> 00:25:49.140
The other thing, I guess, which I don't see appearing anywhere in here, but I'm I suspect

00:25:49.140 --> 00:25:54.800
TensorFlow may have something to do with it, is the new stuff coming from Google, where they have

00:25:54.800 --> 00:26:00.720
like going beyond GPUs for like, AI focused chips. Did you hear about this?

00:26:00.720 --> 00:26:07.400
Yes. So Google has a thing called a TPU, which is a tensor processing unit or whatever. And you can

00:26:07.400 --> 00:26:12.860
that's like a cloud hosted, special piece of hardware that's optimized for doing TensorFlow.

00:26:12.860 --> 00:26:20.340
And so I don't know the exact benchmarks in terms of how that compares to, you know, like some gigantic

00:26:20.340 --> 00:26:27.480
GPU assembly. But obviously, Google thinks that this is a worthwhile investment to build these sort of

00:26:27.480 --> 00:26:32.980
hardware racks in the cloud, and then give people access to run their models on there. So I think

00:26:32.980 --> 00:26:40.100
you're probably going to see more and more specialized, ML targeted hardware that's coming out, whether I

00:26:40.100 --> 00:26:43.600
don't know whether it's like, you'll obviously consumer hardware, like you can go and buy it,

00:26:43.940 --> 00:26:48.520
something for your home computer, but especially in the cloud, you definitely will.

00:26:48.520 --> 00:26:52.440
Yeah, definitely in the cloud. Yeah, it's very interesting. They were talking about real time

00:26:52.440 --> 00:26:55.940
training, not just real time answers. So that sounds pretty crazy.

00:26:55.940 --> 00:27:02.560
This portion of Talk Python To Me has been brought to you by DataCamp. They're calling all data science

00:27:02.560 --> 00:27:07.420
and data science educators. DataCamp is building the future of online data science education.

00:27:07.620 --> 00:27:13.740
They have over 1.5 million learners from around the world who have completed 70 million DataCamp

00:27:13.740 --> 00:27:19.060
exercises to date. Learners get real hands-on experience by completing self-paced, interactive

00:27:19.060 --> 00:27:23.880
data science courses right in the browser. The best part is these courses are taught by top data science

00:27:23.880 --> 00:27:29.060
experts from companies like Anaconda and Kaggle and universities like Caltech and NYU. If you're a data

00:27:29.060 --> 00:27:33.760
science junkie with a passion for teaching, then you too can build data science courses for the masses and

00:27:33.760 --> 00:27:37.440
supplement or even replace your income while you're at it. For more information on becoming

00:27:37.440 --> 00:27:42.120
an instructor, just go to datacamp.com slash create and let them know that Michael sent you.

00:27:42.120 --> 00:27:50.240
So speaking of popular libraries and TPUs, the next up is TensorFlow. That originally came from

00:27:50.240 --> 00:27:56.220
Google and it is crazy at 64,000 stars and 31,000 forks. So tell us about TensorFlow.

00:27:56.220 --> 00:28:02.400
So TensorFlow, obviously, yeah, is this is Google's machine learning library and this is forms the sort of

00:28:02.400 --> 00:28:07.380
slightly lower level than something like Keras and like obviously it's used as a backend.

00:28:07.380 --> 00:28:14.820
You can use it directly as well. And what it does is it represents your model as a computation graph.

00:28:14.820 --> 00:28:22.820
So that's effectively a graph where the nodes are like operations. And this is a way that they found

00:28:22.820 --> 00:28:28.900
is really, really effective to represent these models. And it's a little bit more intimidating to

00:28:28.900 --> 00:28:34.220
get started with mostly because you have to think about building this graph, but you can use it directly

00:28:34.220 --> 00:28:40.320
in Python. Python is actually the recommended language and workflow from Google. So for example,

00:28:40.320 --> 00:28:46.560
you know, when I rewrote the Go version of our little game there, I still had to train and export my model

00:28:46.560 --> 00:28:52.080
from Python. So I use Python to build that, export it. So that's the sort of recommended workflow

00:28:52.080 --> 00:28:56.480
currently from Google for many languages is to use Python as the primary language binding.

00:28:56.480 --> 00:29:01.640
Yeah, that's, that's really interesting and great to see Python. Python appears in so many of these,

00:29:01.640 --> 00:29:08.160
these libraries as a primary way to do it. So there's some interesting stuff about this one.

00:29:08.160 --> 00:29:15.540
Obviously it's super popular. Google has so many use cases for machine learning, just up and down

00:29:15.540 --> 00:29:20.780
their whole, you know, everything that they're doing. So having this like developed internally is really

00:29:20.780 --> 00:29:28.540
cool. It has a flexible architecture that lets it run on CPUs or GPUs, obviously, or mobile devices.

00:29:28.540 --> 00:29:35.080
And it even lets it run like on multiple GPUs and multiple CPUs. Do you have to do anything to make

00:29:35.080 --> 00:29:40.220
that happen? Or do you know how it does that? As far as I can tell that, especially for like this

00:29:40.220 --> 00:29:45.740
switching between CPU and GPU, it's essentially a compile flag. So you have to build like when you build

00:29:45.740 --> 00:29:51.880
the libraries or download one of the nightly builds or whatever, you have to get one of the,

00:29:51.880 --> 00:29:57.960
the versions or that has the enabled GPU support kind of thing built in. And I think that there are

00:29:57.960 --> 00:30:04.700
also now increasingly like CPU optimizations in there. So like for instance, Intel is doing hand

00:30:04.700 --> 00:30:12.620
optimized math kernel stuff that's integrated directly into TensorFlow to make it even faster. So that that's

00:30:12.620 --> 00:30:18.580
something that you can also get in like the latest version as well. So I definitely think speed and

00:30:18.580 --> 00:30:23.800
performance and making that stuff easily accessible to depending on what your hardware is and where

00:30:23.800 --> 00:30:30.020
you're going to deploy it is a big focus for them. Yeah, that's really cool. So do you think this is

00:30:30.020 --> 00:30:33.580
running in the Waymo cars, you know, the Google self driving cars?

00:30:33.580 --> 00:30:37.960
Yeah, I mean, I don't know for sure, but I'd be almost positive of it, you know, from everything that

00:30:37.960 --> 00:30:42.960
I've read and people that I've talked to. I mean, this is Google built this to use not just,

00:30:42.960 --> 00:30:48.260
you know, so there, this is the platform for all of their deep learning and machine learning

00:30:48.260 --> 00:30:53.960
projects. And so I would assume that it's that's TensorFlow is powering that and it's running pretty

00:30:53.960 --> 00:30:58.980
much all of their all of their stuff. Very, very cool. It's probably in Google photos and some other

00:30:58.980 --> 00:31:03.480
things as well. Yeah, Google translate, all those things are all, you know, those things,

00:31:03.860 --> 00:31:08.340
pretty much all of the projects when you start looking at them that Google is running are all

00:31:08.340 --> 00:31:16.060
effectively AI projects. And that's basically all the things that, you know, that just recently,

00:31:16.060 --> 00:31:22.260
like the Google translate, which uses machine learning and like statistical models to do the

00:31:22.260 --> 00:31:27.380
translations is approaching human level accuracy for translation between a lot of the popular

00:31:27.380 --> 00:31:31.920
languages where they have huge, huge data sets to pull from. Yeah, that's crazy. And very,

00:31:32.000 --> 00:31:40.740
cool. So up next, number five is Theano at 6000 stars and 2000 forks. And this one is really kind

00:31:40.740 --> 00:31:46.160
of similar to TensorFlow, but really low level, right? Yeah, so it is, you know, more low level,

00:31:46.160 --> 00:31:51.560
and it is very similar to TensorFlow in the sense that it's also a very high level, high speed math

00:31:51.560 --> 00:31:56.060
library. And I believe it's actually it was originally made by a couple of the guys who then

00:31:56.060 --> 00:32:01.780
went on to Google to make TensorFlow. So it predates TensorFlow by a little bit. But it also has,

00:32:01.780 --> 00:32:08.000
you know, the things that we're, we're talking about here, it has transparent GPU use. And you can do

00:32:08.000 --> 00:32:13.460
things like symbolic differentiation, and a lot of like mathematical things, mathematical operations

00:32:13.460 --> 00:32:21.180
that you want to be highly, highly performant. So it is actually pretty similar to what TensorFlow does,

00:32:21.180 --> 00:32:27.080
and sort of serves a similar purpose. But depending on what you're comfortable with, and what your maybe

00:32:27.080 --> 00:32:32.340
existing projects are, then that is probably going to dictate which one you're using. And if you're

00:32:32.340 --> 00:32:36.820
using something like Harris, then you can just choose this as the back end. And I flip the switch,

00:32:36.820 --> 00:32:42.140
just flip the switch. And there you go. Yeah, it's cool. It also says it has extensive unit testing and

00:32:42.140 --> 00:32:47.420
self verification where it'll detect and diagnose errors, maybe you've set up your model wrong or

00:32:47.420 --> 00:32:51.140
something like that. That's pretty cool. That's pretty cool. Yeah, for sure. I mean, all of these

00:32:51.140 --> 00:32:58.840
libraries are built by super, super smart, accomplished people who are creating things

00:32:58.840 --> 00:33:04.200
that are, you know, solving a real world problem for them and really, you know, sort of pushing things

00:33:04.200 --> 00:33:09.180
forward. And I actually think it's great that there's so many, so many libraries in this space,

00:33:09.180 --> 00:33:15.160
because it really is just making it better for everybody. Yeah, the competition is really cool

00:33:15.160 --> 00:33:22.300
to see the different ways to do this and probably cross pollination. Exactly. Yeah. Yeah. So one of

00:33:22.300 --> 00:33:27.680
the things you have to do for these models is feed them data. And getting data can be a super messy

00:33:27.680 --> 00:33:33.640
thing. And the one library that stands out above all the others about taking transforming,

00:33:34.980 --> 00:33:41.940
redoing, cleaning up data is pandas, right? Absolutely. Yeah. Pandas is, is one of those,

00:33:41.940 --> 00:33:47.720
those libraries that if you're manipulating, especially large sets of data and real world data,

00:33:47.720 --> 00:33:54.700
then this is the one that, that people, you know, repeatedly come back to. And yeah, so pandas is,

00:33:54.700 --> 00:34:00.200
for those that might not know, is like a, you know, data munging data analysis library that lets you

00:34:00.200 --> 00:34:06.120
transform it. One of the hardest parts when you're doing machine learning is actually getting your data

00:34:06.120 --> 00:34:12.960
into a format that can be used effectively by your model. And so a lot of times real world data is

00:34:12.960 --> 00:34:20.520
pretty messy, or it might have gaps in it, or it might not actually be formatted in the right units.

00:34:20.520 --> 00:34:26.320
So it might not be sort of normalized so that you're within the right ranges. And if you feed the models,

00:34:26.320 --> 00:34:32.980
just sort of raw data that hasn't really been either cleaned up or, or formatted correctly,

00:34:32.980 --> 00:34:40.540
then what you might find is that the model doesn't converge or you get what seems like random results

00:34:40.540 --> 00:34:46.200
or things that don't really make sense. And so, you know, spending this time and having a library that

00:34:46.200 --> 00:34:53.100
makes manipulating, especially very large sets of data, very easy, like pandas is super useful.

00:34:53.100 --> 00:34:59.140
And even just for instance, like when I was doing that, that little demo there that, that we talked

00:34:59.140 --> 00:35:05.760
about originally, you know, when I started, I was feeding things raw, raw pixel values for positions

00:35:05.760 --> 00:35:10.120
and velocities and stuff. And it just wasn't working. And it wasn't until I really normalized the data,

00:35:10.120 --> 00:35:15.600
cleaned it up that I had started getting good consistent results. So it's, you know, dealing large

00:35:15.600 --> 00:35:20.620
scale data sets and being able to manipulate them effectively is super important.

00:35:20.620 --> 00:35:28.700
Yeah. At the heart of all these successful AI things, these machine learning algorithms and

00:35:28.700 --> 00:35:33.520
whatnot is a tremendous amount of data. It's why the companies that we talk about doing well are like

00:35:33.520 --> 00:35:40.840
enormous data sucking machines like Google and Microsoft and some of these other ones. Right.

00:35:41.040 --> 00:35:46.580
Exactly. And that's where the power of them comes from is like, you know, Google has access to like just

00:35:46.580 --> 00:35:54.320
massive amount of data that we don't have access to regular people. Or like we were talking about earlier

00:35:54.320 --> 00:36:01.240
with like the radiology images, you need to do need a fairly large set of annotated data. And so that's data

00:36:01.240 --> 00:36:06.580
where, you know, these are case files or whatever that, you know, a doctor has already gone through and said,

00:36:06.720 --> 00:36:13.240
this one was a cancer patient, this one wasn't. And without that kind of annotated data, the models

00:36:13.240 --> 00:36:20.680
can't really learn. They need to know what the answer is. Right. And so that's really, really important.

00:36:20.680 --> 00:36:25.620
Yeah. We have the whole 10,000 hours become an expert for humans. It's that's kind of the equivalent

00:36:25.620 --> 00:36:26.280
for machines.

00:36:26.600 --> 00:36:31.340
Yeah, I guess. Yeah. I don't know what the I don't know what the thing is. It's the machines might need

00:36:31.340 --> 00:36:38.220
more. That's one of the things that is really interesting about humans is that our neural networks

00:36:38.220 --> 00:36:46.020
can learn remarkably quickly without having to walk into traffic 1000 times or do something like that.

00:36:46.020 --> 00:36:49.020
And so there's I don't know, there's some magic going on there or something.

00:36:49.180 --> 00:36:57.200
Yeah, there sure is. All right. Next up is cafe and cafe two. And this originally started out as a

00:36:57.200 --> 00:36:58.320
vision project, right?

00:36:58.320 --> 00:37:05.140
That's right. Yeah, Berkeley. And so this was primarily a vision project. And then there's a sort of successor

00:37:05.140 --> 00:37:13.040
that is backed by Facebook, actually, and is more general purpose and is sort of optimized for web and

00:37:13.040 --> 00:37:18.280
mobile deployment. So obviously, you know, if you want to have machine learning based apps on your

00:37:18.280 --> 00:37:22.940
phone, then having a library that sort of targets that is pretty important.

00:37:22.940 --> 00:37:27.220
Yeah, I'm sure we're going to see more of that. I mean, there are even rumors. I don't know how

00:37:27.220 --> 00:37:34.240
trustworthy they are that the next Apple maybe actually today analysis that the next iPhone will have a

00:37:34.240 --> 00:37:35.300
built in AI chip.

00:37:35.300 --> 00:37:42.440
I remember that they just announced so Apple actually just announced machine language SDK core ML at

00:37:42.440 --> 00:37:51.320
WWDC in June. And so Apple is already targeting these sort of deployed ML models. So, you know, in that

00:37:51.320 --> 00:37:57.020
that library's case, you are effectively choosing a pre-made model. So I want image recognition or I want,

00:37:57.020 --> 00:38:01.820
you know, language parsing in my app. And then you can just feed these sort of pre-trained models.

00:38:01.820 --> 00:38:07.160
But it wouldn't surprise me, you know, they've got the was like the motion chip in your iPhone now.

00:38:07.160 --> 00:38:08.620
Yeah, they got the motion chip. Yeah.

00:38:08.720 --> 00:38:13.600
So it wouldn't surprise me at all that to start seeing that phones are deploying AI chips in there

00:38:13.600 --> 00:38:18.780
to assist with this because most of the sort of things like Siri is a machine learning based thing.

00:38:18.780 --> 00:38:24.520
Right. So yeah. Yeah. It's and it doesn't make sense to go to the cloud all the time. Like that's one of

00:38:24.520 --> 00:38:29.640
the super annoying things about Siri is you ask it a question and it's like six seconds later. Like you

00:38:29.640 --> 00:38:34.480
ask it something simple like what time is it? 10 seconds later, it'll tell you it's such and such.

00:38:34.600 --> 00:38:38.760
Like, is it really that hard? Yeah. Yeah. It's got to go all the way to the cloud and you're in some

00:38:38.760 --> 00:38:43.560
sketchy network area or something. Right. Exactly. And so that I wouldn't be surprised to start seeing

00:38:43.560 --> 00:38:49.640
that stuff deployed onto onto mobile. I think at even at build Microsoft's conference, they started

00:38:49.640 --> 00:38:55.260
talking about edge machine learning where like the machine learning happen is getting pushed to all

00:38:55.260 --> 00:38:59.420
these IOT devices that they're working on as well. So a lot of a lot of attempts in this area.

00:38:59.420 --> 00:39:04.520
For sure. Yeah. And that's the next big thing, right? Is like having IOT based machine learning

00:39:04.520 --> 00:39:10.660
devices. Like, can your fridge learn like your grocery consumption habits and, you know, suggest

00:39:10.660 --> 00:39:13.940
tell you like you're going to run out of milk in two days and you're going to the store today. Maybe

00:39:13.940 --> 00:39:18.580
you should pick some up. I mean, it's going to happen kind of crazy, but it totally will happen.

00:39:18.580 --> 00:39:24.200
And yeah. Yeah. I mean, it doesn't sound as crazy as let's just let a car go drive in a busy

00:39:24.200 --> 00:39:31.080
city on its own. That's true. And yet that's, that's something that exists now, right? Like

00:39:31.080 --> 00:39:36.440
that's, that's a, that's a thing like you can, and maybe it's not fully autonomous, but I mean,

00:39:36.440 --> 00:39:42.000
you could go and buy one like tomorrow, you could buy a car that you can turn on autopilot and like,

00:39:42.000 --> 00:39:49.820
it's crazy. It's fully drive for you. So the future is now, the future is here. It's just not

00:39:49.820 --> 00:39:56.440
evenly distributed. This portion of Talk Python is brought to you by us. As many of you know,

00:39:56.440 --> 00:40:00.760
I have a growing set of courses to help you go from Python beginner to novice to Python expert.

00:40:00.760 --> 00:40:05.860
And there are many more courses in the works. So please consider Talk Python training for you and

00:40:05.860 --> 00:40:10.780
your team's training needs. If you're just getting started, I've built a course to teach you Python

00:40:10.780 --> 00:40:15.620
the way professional developers learn by building applications. Check out my Python jumpstart by

00:40:15.620 --> 00:40:21.680
building 10 apps at talkpython.fm/course. Are you looking to start adding services to your app?

00:40:21.680 --> 00:40:27.600
Try my brand new consuming HTTP services in Python. You'll learn to work with RESTful HTTP services,

00:40:27.600 --> 00:40:32.660
as well as SOAP, JSON and XML data formats. Do you want to launch an online business? Well,

00:40:32.660 --> 00:40:38.160
Matt McKay and I built an entrepreneur's playbook with Python for entrepreneurs. This 16 hour course will

00:40:38.160 --> 00:40:42.460
teach you everything you need to launch your web-based business with Python. And finally,

00:40:42.460 --> 00:40:46.640
there's a couple of new course announcements coming really soon. So if you don't already have an

00:40:46.640 --> 00:40:52.580
account, be sure to create one at training.talkpython.fm to get notified. And for all of you who have bought

00:40:52.580 --> 00:40:59.280
my courses, thank you so much. It really, really helps support the show. One little fact or a quote from

00:40:59.280 --> 00:41:03.720
the cafe webpage that I want to just throw out there because I thought it was pretty cool before we move

00:41:03.720 --> 00:41:12.480
on. They say, speed makes cafe perfect for research experiments and industry deployments. It can process 60 million

00:41:12.480 --> 00:41:20.140
images per day on a single GPU. That's one millisecond per image for inference and four milliseconds per image for

00:41:20.140 --> 00:41:21.780
learning. That's insane.

00:41:21.900 --> 00:41:30.540
So fast. And 60 million images per day is just like, it's crazy. And that's why we were talking

00:41:30.540 --> 00:41:36.180
about the data just a minute ago. And the amount of data being poured into these models is just

00:41:36.180 --> 00:41:42.900
staggering every day. And I don't doubt that they're probably feeding, people are feeding these models

00:41:42.900 --> 00:41:48.580
like that much data every day. And I think they were saying 90% of the world's data that's ever been

00:41:48.580 --> 00:41:53.880
created has been created in the last year. And so it's just one of these things where it gets

00:41:53.880 --> 00:41:59.260
accelerates and accelerates and builds on all this stuff. So I think these things are just going to

00:41:59.260 --> 00:42:01.200
get faster until they're effectively real time.

00:42:01.200 --> 00:42:07.500
Yeah, absolutely. All right. I don't think we said the stars for that one. 20,000 and 11,000 forks.

00:42:08.040 --> 00:42:15.300
So up next is definitely one that data scientists in general just live on. And that's Jupyter.

00:42:15.300 --> 00:42:24.000
For sure. And so this has just become like the standard interchange format for sharing data science,

00:42:24.000 --> 00:42:32.320
whether it's papers or data sets or models, or this has just become the sort of standard,

00:42:32.320 --> 00:42:37.940
I don't know what you're going to call it, lingua franca for exchanging this data. And it's effectively

00:42:37.940 --> 00:42:43.380
a tool for the thing called a Jupyter notebook, which is like kind of like a web pages with like

00:42:43.380 --> 00:42:48.160
embedded programs and embedded data sets. I think that's probably a good way to describe it for those

00:42:48.160 --> 00:42:49.560
who might not have used it before.

00:42:49.560 --> 00:42:54.720
Right. It's like instead of writing a blog post or a paper that's got a little bit of code,

00:42:54.720 --> 00:42:59.440
then a little bit of description, then a picture, which is a graph, it's like live and you can re-execute

00:42:59.440 --> 00:43:03.840
it and tweak it. And it probably plugs into many of these other libraries and it's using that

00:43:03.840 --> 00:43:05.800
somewhere behind the scenes to do that.

00:43:06.120 --> 00:43:13.340
Exactly. Yeah. It's built on the IPython kernel for that's like interactive Python kernel. Yeah. I'm

00:43:13.340 --> 00:43:19.060
sure that there are all kinds of specific uses that can run those notebook or that notebook code and use

00:43:19.060 --> 00:43:20.320
that, that stuff there.

00:43:20.320 --> 00:43:26.660
Cool. Next up is maybe one of the newer kids on the block in this deep learning story from Microsoft,

00:43:26.660 --> 00:43:29.300
actually their cognitive toolkit, C and TK.

00:43:29.300 --> 00:43:35.880
Yeah. And it's, they just released, I think the 2.0 version of it beginning of June or late May.

00:43:35.880 --> 00:43:42.000
And, you know, now it's open source and it's, it's got the Python bindings and it's part of,

00:43:42.000 --> 00:43:46.340
you know, Microsoft's been doing a lot of open source work lately and they've been, you know,

00:43:46.340 --> 00:43:48.520
really, really pushing a lot of their own projects.

00:43:48.520 --> 00:43:56.320
And, it's like we said earlier, it's available as a backend for Keras. So it's similar again to

00:43:56.320 --> 00:44:00.400
TensorFlow and Theano that it's, it's again, focused on that sort of low level

00:44:00.400 --> 00:44:07.260
computation as a directed graph. So similar model, I think this is, you know, obviously emerging as a

00:44:07.260 --> 00:44:11.900
popular and efficient way to represent machine learning models is using that directed graph.

00:44:11.900 --> 00:44:17.900
So it's pretty popular too, right? It's got a decent number of stars and forks and obviously

00:44:17.900 --> 00:44:23.960
as a Keras backend and Microsoft backed library, it's going to be pretty popular and pretty common

00:44:23.960 --> 00:44:24.580
out there.

00:44:24.580 --> 00:44:29.900
Yeah, absolutely. These days, you know, with, Satya Nadella and a lot of the changes at Microsoft,

00:44:29.900 --> 00:44:35.720
I feel like this open source stuff is really taking a new direction, a positive one. And also I think

00:44:35.720 --> 00:44:41.920
their philosophy is if it's good for Azure, it's good for Microsoft. And so this plugs into their

00:44:41.920 --> 00:44:47.200
hosted stuff and interesting ways. And they've got a lot of like cognitive cloud services and things

00:44:47.200 --> 00:44:47.580
like that.

00:44:47.580 --> 00:44:57.260
Yeah. Azure is becoming pretty huge. It's like starting to rival maybe even AWS for, you know,

00:44:57.260 --> 00:45:01.580
a lot of this cloud hosted services and especially around machine learning, like Azure has so many

00:45:01.580 --> 00:45:08.120
different machine learning tools available. And it's really clearly a pretty, pretty big focus for

00:45:08.120 --> 00:45:14.440
Microsoft. And again, it's great to see, you know, more of the, you know, the sort of big guns being

00:45:14.440 --> 00:45:19.380
more open about their development and sharing. I mean, it drives everybody forward and, and, you know,

00:45:19.380 --> 00:45:21.780
just accelerates development across the whole ecosystem.

00:45:21.780 --> 00:45:25.640
Yeah. And they have a number of the Python core developers there. They have Brett Cannon,

00:45:25.640 --> 00:45:30.900
they have Steve Dower, they have, you know, VLAN, like there's some serious people back there working

00:45:30.900 --> 00:45:31.780
on the Python part.

00:45:31.780 --> 00:45:37.020
Exactly. Yeah. They've got a lot of the Python core team there. And, I know a bunch of the

00:45:37.020 --> 00:45:42.860
guys from active state were just at PI data in Seattle and, you know, huge number of the core team

00:45:42.860 --> 00:45:48.540
were there and, you know, just really, really great little conference. They're talking about

00:45:48.540 --> 00:45:52.680
Python and data science. Yeah. I think they have some really interesting language stuff as well.

00:45:52.680 --> 00:45:58.320
So speaking of languages, the, most, certainly the longest running one, probably that's really

00:45:58.320 --> 00:46:03.020
still going strong is NLTK with 5,000 stars and 1.5 thousand forks.

00:46:03.020 --> 00:46:09.100
Yeah. And so NLTK was like the natural language toolkit. And, you know, obviously this is a thing

00:46:09.100 --> 00:46:14.380
for doing natural language parsing, which is, I guess, one of the holy grails of, of machine

00:46:14.380 --> 00:46:19.720
learning is to get it to be really, you know, so you can just speak to your, to your computer

00:46:19.720 --> 00:46:23.400
and completely natural language, and maybe even give it instructions in natural language

00:46:23.400 --> 00:46:28.380
and, and be able to be able to follow your, for your directions and understand what you're

00:46:28.380 --> 00:46:35.580
asking. And so this is like a really popular one in academia for research. They link to and

00:46:35.580 --> 00:46:43.480
include massive corpora of, of work. So that's like gigantic bodies of text in different languages

00:46:43.480 --> 00:46:48.840
and in different styles to be able to train models. So there's, there's also like a pretty

00:46:48.840 --> 00:46:54.740
large, like open data component to this project as well. And, obviously, you know, the use

00:46:54.740 --> 00:46:59.320
case here for natural language is, you know, it's huge for translation. Like we mentioned earlier,

00:46:59.320 --> 00:47:05.360
chatbots, which are now a huge thing for like support. I mean, every website you go onto and

00:47:05.360 --> 00:47:10.320
it pops up, Hey, I'm, you know, Bob and I'm, can I help you today? And it's like, not a really a

00:47:10.320 --> 00:47:16.700
person. It's just a chatbot. And, you know, there's just so many. And then like we were saying, Siri and,

00:47:16.700 --> 00:47:22.560
and Cortana and all those sort of personal assistants where you can say, ask it a natural language question

00:47:22.560 --> 00:47:27.400
and it can come back to you. So this is the sort of almost like foundational library still going

00:47:27.400 --> 00:47:32.240
strong, still tons of active development and research going on with this. Yeah. It's really

00:47:32.240 --> 00:47:37.620
cool. And especially with all the smart home speaker things, Google home, home pod, all that stuff.

00:47:37.620 --> 00:47:42.660
This is just, this is going faster, not slower terms of acceleration, right? It's

00:47:42.660 --> 00:47:49.700
weird talking more and interacting with them way more. Definitely the chatbots. And anytime you have

00:47:49.700 --> 00:47:55.360
text and you want a computer to understand it, this is like a first step for tokenization,

00:47:55.360 --> 00:47:59.940
stemming, tagging, parsing, semantic analysis, all that kind of stuff. Right? Yeah. And that's,

00:47:59.940 --> 00:48:05.920
that's exactly what it outputs. So it will do is like generate parse trees and, and stem it all out and

00:48:05.920 --> 00:48:12.300
then use those, the kind of tokenized version to use that to train your model, not sort of raw text

00:48:12.300 --> 00:48:19.680
characters. And, we really are getting there. I mean, like these days, like for sure, like just the

00:48:19.680 --> 00:48:24.860
recognition part, you know, the tokenization part is very, very good. It's more like the kind of

00:48:24.860 --> 00:48:32.860
semantic meaning. What do you mean when you ask it, you ask Siri for what are the movie times for X or

00:48:32.860 --> 00:48:37.320
something like that? How specific do you have to be for, to get a reasonable answer from her? Yeah.

00:48:37.320 --> 00:48:41.260
It's got to go speech to text and then it probably hits something like this. Exactly. Yeah, exactly.

00:48:41.260 --> 00:48:46.600
That's going to hit a library like this and we're getting there. It's not quite at the Star Trek

00:48:46.600 --> 00:48:51.780
computer do this for me, but it's like way closer than I kind of ever thought we would

00:48:51.780 --> 00:48:57.060
be. It's really pretty impressive sometimes. Yeah, absolutely. It's, it's fun to see this

00:48:57.060 --> 00:49:02.220
stuff evolve. Absolutely. All right, Pete, that's the 10 libraries. And I think these are all really

00:49:02.220 --> 00:49:09.120
great selections and hopefully people have got a lot of exposure and maybe learned about some

00:49:09.120 --> 00:49:13.460
they didn't know about. And I guess encourage, encourage everyone to go out there and try these

00:49:13.460 --> 00:49:17.760
down and if you've got an idea, play with it with one of them or more. For sure. They're also accessible

00:49:17.760 --> 00:49:27.540
now. You know, you don't necessarily have to be ML researcher or a math wizard to actually create

00:49:27.540 --> 00:49:32.380
something that's interesting or experiment or learn a little bit. These libraries all do a really,

00:49:32.380 --> 00:49:39.080
really great job of abstracting away some of the more complicated mathematical parts. And,

00:49:39.080 --> 00:49:45.240
you know, in the case of a lot of them making it reasonably accessible. And so that's where I think

00:49:45.240 --> 00:49:51.680
you're seeing this kind of like democratization trend in machine learning now where this stuff is

00:49:51.680 --> 00:49:56.560
becoming more accessible. It's becoming easier. And I think you're going to see a lot of creativity and a

00:49:56.560 --> 00:50:01.740
lot of innovation come out of people if they just sort of give it a shot and try something out and,

00:50:01.740 --> 00:50:02.840
you know, learn something new.

00:50:03.080 --> 00:50:07.820
Yeah, that's awesome. I totally agree with the democratization of it. And that's also happening

00:50:07.820 --> 00:50:12.560
from a computational perspective, right? Like these are easier to use, but also with the GPUs

00:50:12.560 --> 00:50:18.140
and the cloud and things like that, it's a lot easier. You don't need a supercomputer. You need 500

00:50:18.140 --> 00:50:19.860
bucks or something for a GPU.

00:50:20.200 --> 00:50:25.700
Exactly. That's the, I think all of these sort of things feed into that in together where you have a

00:50:25.700 --> 00:50:33.260
democratization trend in the tools and the source code so that now a, you can have access to Google's

00:50:33.260 --> 00:50:40.940
years and years of AI research via TensorFlow on GitHub. You also, like you said, can go and buy a

00:50:40.940 --> 00:50:48.600
$500 GPU and have basically a supercomputer on your desktop, but also this open data component where

00:50:48.600 --> 00:50:56.640
you can get access to massive data sets like the Stanford image library and, you know, these huge

00:50:56.640 --> 00:51:03.480
NLTK like language corpora that you can then use to train your models where previously that was probably

00:51:03.480 --> 00:51:05.280
impossible to actually access.

00:51:05.280 --> 00:51:09.660
Yeah, that's a really good point because even though you have the machines and you have the algorithms,

00:51:09.660 --> 00:51:14.760
the data, data really makes it work. All right. So I think let's leave it there for the library.

00:51:14.760 --> 00:51:20.440
So those were great. And I'll, I hit you with the final two questions. You're going to write some

00:51:20.440 --> 00:51:22.960
code. What editor, Python code, what editor do you open up?

00:51:22.960 --> 00:51:30.260
Well, obviously ActiveState has Komodo. So I tend to use that a lot for doing a Python code, but I've also

00:51:30.260 --> 00:51:36.760
to be totally fair. I have used VS Code as well, which is getting increasingly popular. So I tend to like

00:51:36.760 --> 00:51:42.900
to cycle between them all because we have an editor product. And so, you know, it's great to keep up to

00:51:42.900 --> 00:51:48.380
date on what all the other ones are doing. So I tend to cycle around a little bit, but yeah, like

00:51:48.380 --> 00:51:50.180
Komodo is sort of my go-to.

00:51:50.180 --> 00:51:54.100
Yeah, that's cool. Yeah. It's definitely important to look and see what the trends are, what other

00:51:54.100 --> 00:51:57.820
people are doing, how can you bring this cool idea back into Komodo, things like that, right?

00:51:57.820 --> 00:51:58.420
Yeah, for sure.

00:51:58.420 --> 00:51:58.820
Yeah.

00:51:58.860 --> 00:52:03.900
All right. And I think we've already hit 10, but do you have another notable PyPI package?

00:52:03.900 --> 00:52:08.740
I don't know. There's, there's so many. I would again, probably give a, a little bit of a shout

00:52:08.740 --> 00:52:14.140
out to, you know, since we're talking about machine learning to Keras, because I do think as an entry

00:52:14.140 --> 00:52:22.280
point to machine learning, it's so accessible. It's so easy to at least get started and get a result with.

00:52:22.380 --> 00:52:25.880
I would give a little shout out to that, that I think that if you're looking to get into this and

00:52:25.880 --> 00:52:28.420
you're looking to try it out, that's a really great place to start.

00:52:28.420 --> 00:52:31.240
Yeah, I totally agree with you. That's, that's where I would start as well.

00:52:31.240 --> 00:52:36.240
All right. Well, it's very interesting to talk about all these libraries with you. I really

00:52:36.240 --> 00:52:39.820
appreciate you coming on the show and sharing this with everyone. Thanks for being here.

00:52:39.820 --> 00:52:40.660
Thank you for having me.

00:52:40.660 --> 00:52:41.200
You bet. Bye.

00:52:41.200 --> 00:52:47.320
This has been another episode of Talk Python To Me. Our guest has been Pete Carson,

00:52:47.320 --> 00:52:52.200
and this episode has been brought to you by DataCamp and us right here at Talk Python Training.

00:52:52.200 --> 00:52:58.240
Want to share your data science experience and passion? Visit datacamp.com slash create

00:52:58.240 --> 00:53:01.300
and write a course for a million budding data scientists.

00:53:01.300 --> 00:53:06.320
Are you or a colleague trying to learn Python? Have you tried books and videos that just left

00:53:06.320 --> 00:53:10.360
you bored by covering topics point by point? Well, check out my online course,

00:53:10.360 --> 00:53:16.600
Python Jumpstart by Building 10 Apps at talkpython.fm/course to experience a more engaging way to

00:53:16.600 --> 00:53:21.560
learn Python. And if you're looking for something a little more advanced, try my Write Pythonic Code

00:53:21.560 --> 00:53:28.480
course at talkpython.fm/pythonic. Be sure to subscribe to the show. Open your favorite podcatcher

00:53:28.480 --> 00:53:33.940
and search for Python. We should be right at the top. You can also find the iTunes feed at /itunes,

00:53:33.940 --> 00:53:39.900
Google Play feed at /play, and direct RSS feed at /rss on talkpython.fm.

00:53:39.900 --> 00:53:44.220
This is your host, Michael Kennedy. Thanks so much for listening. I really appreciate it.

00:53:44.220 --> 00:53:46.200
Now get out there and write some Python code.

00:53:46.200 --> 00:54:06.940
Thank you.

00:54:06.940 --> 00:54:07.000
Thank you.

00:54:07.000 --> 00:54:07.220
Thank you.

