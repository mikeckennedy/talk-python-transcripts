WEBVTT

00:00:00.001 --> 00:00:04.020
We write tests to show us when there are problems with our code, but what if there are intermittent

00:00:04.020 --> 00:00:05.840
problems with the tests themselves?

00:00:05.840 --> 00:00:07.360
That can be a big hassle.

00:00:07.360 --> 00:00:12.000
In this episode, we have Gregory Kapphammer and Owen Perry on the show to share their

00:00:12.000 --> 00:00:14.440
research and advice for taming flaky tests.

00:00:14.440 --> 00:00:19.740
This is Talk Python To Me, episode 429, recorded August 10th, 2023.

00:00:19.740 --> 00:00:35.560
Welcome to Talk Python To Me, a weekly podcast on Python.

00:00:35.560 --> 00:00:37.300
This is your host, Michael Kennedy.

00:00:37.300 --> 00:00:42.420
Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython,

00:00:42.420 --> 00:00:44.780
both on fosstodon.org.

00:00:44.780 --> 00:00:47.400
Be careful with impersonating accounts on other instances.

00:00:47.400 --> 00:00:48.360
There are many.

00:00:48.560 --> 00:00:53.420
Keep up with the show and listen to over seven years of past episodes at talkpython.fm.

00:00:53.420 --> 00:00:57.460
We've started streaming most of our episodes live on YouTube.

00:00:57.460 --> 00:01:03.160
Subscribe to our YouTube channel over at talkpython.fm/youtube to get notified about upcoming

00:01:03.160 --> 00:01:05.000
shows and be part of that episode.

00:01:05.000 --> 00:01:11.300
This episode is brought to you by JetBrains, who encourage you to get work done with PyCharm.

00:01:11.300 --> 00:01:18.200
Download your free trial of PyCharm Professional at talkpython.fm/done dash with dash PyCharm.

00:01:18.380 --> 00:01:20.700
And it's brought to you by Sentry.

00:01:20.700 --> 00:01:22.940
Don't let those errors go unnoticed.

00:01:22.940 --> 00:01:23.940
Use Sentry.

00:01:24.120 --> 00:01:26.780
Get started at talkpython.fm/sentry.

00:01:26.780 --> 00:01:30.300
Owen, Gregory, welcome to Talk Python To Me.

00:01:30.300 --> 00:01:30.860
Hi, Michael.

00:01:30.860 --> 00:01:31.880
It's great to be on the show.

00:01:31.880 --> 00:01:33.420
Thank you for inviting us today.

00:01:33.420 --> 00:01:33.740
Hi there.

00:01:33.740 --> 00:01:34.400
Thanks for having us.

00:01:34.400 --> 00:01:35.980
Really great to have you both on the show.

00:01:35.980 --> 00:01:39.340
It's going to be a bit of a flaky episode, though, wouldn't you say?

00:01:39.340 --> 00:01:41.000
It's definitely going to be flaky.

00:01:41.000 --> 00:01:41.980
Very flaky.

00:01:41.980 --> 00:01:46.100
Looking forward to talking about flaky tests and what we can do about them.

00:01:46.340 --> 00:01:51.240
It's one of these realities of writing lots of unit tests for real world systems, right?

00:01:51.240 --> 00:01:52.680
They end up in weird places.

00:01:52.680 --> 00:01:57.880
So for better or for worse, I've implemented a lot of programs in Python, and many of them

00:01:57.880 --> 00:02:00.540
have test suites with flaky test cases inside of them.

00:02:00.540 --> 00:02:05.860
So I have to confess to everyone, I myself have written programs with many flaky tests.

00:02:06.260 --> 00:02:07.200
As have I.

00:02:07.200 --> 00:02:07.820
As have I.

00:02:07.820 --> 00:02:08.260
All right.

00:02:08.260 --> 00:02:13.440
Before we get into the show itself, maybe just a little bit of background on you two.

00:02:13.440 --> 00:02:14.500
Gregory, you want to go first?

00:02:14.500 --> 00:02:17.700
Just a quick introduction about who you are, how you got into programming in Python.

00:02:17.700 --> 00:02:18.260
Sure.

00:02:18.260 --> 00:02:23.480
My name is Gregory Kapphammer, and I'm a faculty member in the Department of Computer Science

00:02:23.480 --> 00:02:24.780
at Allegheny College.

00:02:24.780 --> 00:02:30.320
And I've actually been programming in Python since I took an AI course in graduate school years

00:02:30.320 --> 00:02:34.460
ago, and we had to implement all of our algorithms in Python.

00:02:34.640 --> 00:02:39.160
Stopped using Python for a short period of time and then picked it back up again once I learned

00:02:39.160 --> 00:02:43.780
about pytest because I found it to be such an awesome test automation framework.

00:02:43.780 --> 00:02:46.740
And I've been programming in Python regularly since then.

00:02:46.740 --> 00:02:47.400
Oh, that's cool.

00:02:47.400 --> 00:02:52.620
What's pretty interesting is people who don't even necessarily do Python sometimes use Python

00:02:52.620 --> 00:02:53.640
to write the tests.

00:02:53.640 --> 00:02:54.500
Yeah, absolutely.

00:02:54.500 --> 00:03:00.420
I have to say I've used a bunch of different test automation frameworks and pytest is by far

00:03:00.420 --> 00:03:02.660
and away my favorite framework out of them all.

00:03:02.660 --> 00:03:03.360
Owen, hello.

00:03:03.580 --> 00:03:07.080
Hi, I'm a PhD student at the University of Sheffield.

00:03:07.080 --> 00:03:10.660
I'm actually coming right to the end of my time as a PhD student.

00:03:10.660 --> 00:03:12.680
So that's quite a journey, isn't it?

00:03:12.680 --> 00:03:13.620
It is quite a journey.

00:03:13.620 --> 00:03:13.900
Yeah.

00:03:13.900 --> 00:03:18.820
And throughout my whole PhD, my main topic has been flaky tests.

00:03:18.820 --> 00:03:19.240
Okay.

00:03:19.360 --> 00:03:24.860
So before I even started my PhD, I had Python experience from just odd undergraduate projects

00:03:24.860 --> 00:03:26.120
that had to be done in Python.

00:03:26.120 --> 00:03:32.500
But for all of my research, I've thought it very important to use real software, real tests

00:03:32.500 --> 00:03:36.960
written by real people to identify flaky tests, find out what causes them, that kind of thing.

00:03:37.080 --> 00:03:40.680
And all of my sort of practical work has been done with Python.

00:03:40.680 --> 00:03:46.240
So for example, I've written pytest plugins to help detect flaky tests.

00:03:46.240 --> 00:03:50.460
And as Greg said, I think pytest is a great framework.

00:03:50.460 --> 00:03:51.440
It's very extensible.

00:03:51.440 --> 00:03:54.200
It's very great for writing plugins, very good API.

00:03:54.200 --> 00:04:00.400
And yeah, I've used lots of different types of Python projects as subjects and experiments.

00:04:00.400 --> 00:04:04.740
So I've seen quite a wide array of different types of software written in Python.

00:04:04.740 --> 00:04:07.460
Have you studied a lot of other people's software,

00:04:07.460 --> 00:04:12.160
interviewed different people to see how they're encountering flaky tests?

00:04:12.160 --> 00:04:14.360
So I've not interviewed anyone exactly,

00:04:14.360 --> 00:04:19.620
but I did do a questionnaire that I sent out on Twitter, LinkedIn, that kind of thing,

00:04:19.620 --> 00:04:25.100
where we just wanted to get as many different kinds of developers talking about flaky tests.

00:04:25.100 --> 00:04:28.280
So we asked them questions like, first of all, what is a flaky test?

00:04:28.280 --> 00:04:29.980
People have slightly different definitions.

00:04:30.420 --> 00:04:33.000
And then I went into, you know, what do you think causes them?

00:04:33.000 --> 00:04:36.380
What impacts do they have on you and your sort of professional workflow?

00:04:36.380 --> 00:04:40.000
And then we talked a little bit about what people do about them as well.

00:04:40.000 --> 00:04:42.680
Well, maybe that's a good place to start.

00:04:42.680 --> 00:04:45.180
And either of you just jump in as you see fit.

00:04:45.180 --> 00:04:47.880
So let's just start with, you know, what is a flaky test?

00:04:47.880 --> 00:04:53.480
I mean, we all know that having unit tests and broader tests, you know, integration tests and so on,

00:04:53.480 --> 00:04:55.960
4R code is generally a good thing.

00:04:55.960 --> 00:04:58.840
I guess if you write them poorly enough, it's not a good thing.

00:04:58.980 --> 00:05:04.020
But, you know, mostly it's recommended advice and there's a spectrum of how strongly it's recommended.

00:05:04.020 --> 00:05:11.060
You know, is it extreme programming, TDD recommended, or is it you have to have some tests to submit a PR level recommended?

00:05:11.240 --> 00:05:12.400
But we think it's great.

00:05:12.400 --> 00:05:17.720
But there are these negative aspects of having tests as well, right?

00:05:17.720 --> 00:05:21.320
It's easy to sell them as a positive, but they become a maintenance burden.

00:05:21.320 --> 00:05:22.960
They become duplicate code.

00:05:22.960 --> 00:05:28.580
And the flakiness, I think, is a particularly challenging part of it.

00:05:28.580 --> 00:05:29.440
Let's start there.

00:05:29.440 --> 00:05:31.060
What's a flaky test for you all?

00:05:31.120 --> 00:05:35.280
So I'll start off and then, Owen, if you'd like to add more details, that would be awesome.

00:05:35.280 --> 00:05:49.220
I would say that a flaky test is a test case that passes or fails in a non-deterministic fashion, even when you're not changing the source code of the program or the test suite for the program.

00:05:49.480 --> 00:06:05.240
So this is a situation where the test case may sometimes pass, then it may fail, and then it may start to pass again, even though, as a developer, you're not making changes to the program under test or the source code of the test suite itself.

00:06:05.440 --> 00:06:06.620
Yeah, that is tricky, right?

00:06:06.620 --> 00:06:08.740
Because just one day the tests start failing.

00:06:08.740 --> 00:06:09.960
We didn't change anything.

00:06:09.960 --> 00:06:10.980
Nothing has changed here.

00:06:10.980 --> 00:06:11.180
Why?

00:06:11.180 --> 00:06:12.940
How could this have anything to do with it, right?

00:06:12.940 --> 00:06:13.420
Owen?

00:06:13.420 --> 00:06:14.340
Sorry, go ahead, Greg.

00:06:14.340 --> 00:06:25.940
The only other thing I was going to add is that flaky test cases could manifest themselves on a developer workstation, or they could also manifest when you're running them in a continuous integration environment as well.

00:06:25.940 --> 00:06:26.560
Yeah, for sure.

00:06:26.560 --> 00:06:28.800
So to just sort of build on what Greg said there a little bit.

00:06:28.800 --> 00:06:31.500
So one interesting thing we found from that developer survey.

00:06:31.860 --> 00:06:38.580
So the definition that Greg gave just then was pretty much the definition we proposed to the respondents to the survey.

00:06:38.580 --> 00:06:40.040
And then we asked them, do you agree?

00:06:40.040 --> 00:06:41.740
If not, what's your definition?

00:06:41.740 --> 00:06:49.760
Most people agreed, but some people said, well, it doesn't just depend on the source code of the program and the code of the test, but also the execution environment.

00:06:49.760 --> 00:06:59.580
So you can take one piece of software and its associated test suite, run it on one computer, it passes, run it on another system, and then for whatever reason, it doesn't.

00:06:59.720 --> 00:07:02.760
So nothing's changed at all except for the execution environment.

00:07:02.760 --> 00:07:12.300
And that's quite prevalent when you're talking about CI, because most of the time these are running on cloud machines, which may not be all exactly the same in spec.

00:07:12.300 --> 00:07:20.300
From the perspective of the developer, it looks as if a test could just fail, but maybe it failed because it was running on a slightly different machine that time it was run.

00:07:20.440 --> 00:07:21.240
I hadn't really thought of that.

00:07:21.240 --> 00:07:23.020
Obviously, the different environments, right?

00:07:23.020 --> 00:07:26.160
Like a CI machine is very different than my MacBook.

00:07:26.160 --> 00:07:32.760
So clearly, it could be the case that a test passes on my machine, not in CI or vice versa.

00:07:32.760 --> 00:07:39.320
But I hadn't really thought about, well, this time it got an AMD cloud processor, and it was already under heavy load.

00:07:39.320 --> 00:07:46.600
And so the timing changed versus the other time it was on like a premium Intel thing in the cloud that had no other thing going on.

00:07:46.600 --> 00:07:47.620
So it behaved differently.

00:07:47.620 --> 00:07:47.960
Yeah.

00:07:47.960 --> 00:07:48.460
That's pretty wild.

00:07:48.460 --> 00:07:49.900
Your point is a good one, Michael.

00:07:49.900 --> 00:08:02.300
It's actually often the case that the speed of the CPU or the amount of memory or the amount of disk on the testing workstation can make a big difference when it comes to manifesting a flaky test.

00:08:02.300 --> 00:08:02.680
Yeah.

00:08:02.680 --> 00:08:05.560
I guess that makes a lot of sense, especially if it's a race condition, right?

00:08:05.560 --> 00:08:09.980
If you're having some sort of parallelism, then that could really come into memory as well, maybe.

00:08:09.980 --> 00:08:12.320
Maybe you run out of memory and get an out of memory exception.

00:08:12.560 --> 00:08:27.980
So when we're talking about flaky tests, one thing that came to mind for me, and I want to bring it up at the start here, because I'm wondering if this classifies as flaky for you, if this is some other kind of not great test, is has to do with science, right?

00:08:27.980 --> 00:08:31.260
So scientific type of computing, mathematical stuff.

00:08:31.260 --> 00:08:41.900
Obviously, you shouldn't say I've got some floating point number equal, equal, you know, some other long, precise, you know, here's my definition of the square root of two as a float equal, equal that, right?

00:08:41.900 --> 00:08:43.000
That might be too precise.

00:08:43.000 --> 00:08:54.620
But what I'm thinking about is if you make the smallest amount of change to some algorithm or the way something works could change some, like maybe you're trying to say, does this, do we get a curve that looks like this?

00:08:54.620 --> 00:08:59.240
Or do we match some kind of criteria on, you know, statistics?

00:08:59.240 --> 00:09:09.380
It could change just a little bit, but the way that you're testing for it to be a match, it changes enough in that regard, even though effectively it kind of means the same thing.

00:09:09.380 --> 00:09:10.340
You know what I'm asking?

00:09:10.520 --> 00:09:12.060
Does that count as a flaky test for you?

00:09:12.060 --> 00:09:16.200
So what you're talking about is a very specific category of flaky test.

00:09:16.200 --> 00:09:17.460
So I would call that a flaky test.

00:09:17.460 --> 00:09:23.900
So yeah, so when you're dealing with programs, like for example, various machine learning packages that are very common in Bison,

00:09:24.020 --> 00:09:31.860
You'll see a lot of test cases that will say assert X is equal to this within a certain tolerance range or something, or is approximately equal to something.

00:09:31.860 --> 00:09:36.340
With these kinds of tests, they are kind of inherently flaky.

00:09:36.340 --> 00:09:40.020
There is a trade-off between how strong you want the test to be.

00:09:40.020 --> 00:09:45.700
So IE how narrow you want that acceptable band to be versus how flaky you want it to be.

00:09:45.700 --> 00:09:47.700
Sorry, how flaky you don't want it to be.

00:09:47.700 --> 00:09:52.380
So the stronger you make the test, the more flaky it's likely to be because that band is narrower.

00:09:52.380 --> 00:09:56.600
But if you just increase that tolerance, then yeah, it won't be as flaky anymore.

00:09:56.600 --> 00:10:01.160
But then maybe it won't catch as many bugs anymore because it's too relaxed of a test.

00:10:01.160 --> 00:10:01.420
Sure.

00:10:01.540 --> 00:10:08.960
And unfortunately, if you're going to reduce a test case to either a pass or a fail, which is something that we have to do, there's no real way around that.

00:10:08.960 --> 00:10:14.180
There has been done work where people have sort of tried to calculate via, there we go.

00:10:14.180 --> 00:10:15.960
It's on a motion sensor.

00:10:15.960 --> 00:10:17.300
Got away with the light.

00:10:17.300 --> 00:10:18.340
It's going dark for you.

00:10:18.340 --> 00:10:20.060
I was like, yeah.

00:10:20.060 --> 00:10:27.780
So there is work people have done where they try to calculate sort of what is the best sort of tolerance threshold in test cases,

00:10:27.780 --> 00:10:30.600
but there's no kind of silver bullet solution for that kind of flakiness.

00:10:30.600 --> 00:10:32.620
Yeah, it's tricky, isn't it?

00:10:32.620 --> 00:10:32.920
Yeah.

00:10:32.920 --> 00:10:33.220
Yeah.

00:10:33.220 --> 00:10:35.620
Pradvan out in the audience has an interesting one.

00:10:35.620 --> 00:10:40.120
It maybe probably lands a little more into the realm directly of the kind of stuff that you're talking about.

00:10:40.120 --> 00:10:48.340
Says at one of my jobs testing, we were testing chained celery tasks that become flaky sometimes since one celery task fails.

00:10:48.340 --> 00:10:51.260
For some reason, the chain task could fail as well.

00:10:51.260 --> 00:10:55.000
Like those kind of external systems are probably at the heart of a lot of this, right?

00:10:55.000 --> 00:10:57.380
Yeah, I think it is often at the heart of it.

00:10:57.380 --> 00:11:07.740
And whether you're using celery or some other work processing queue, or alternatively, if you're interacting with a document database or a relational database,

00:11:07.740 --> 00:11:14.080
in all of those occasions, when you interact with some storage or processing system in your environment,

00:11:14.080 --> 00:11:17.560
you may not have control over that part of your environment.

00:11:17.800 --> 00:11:23.280
And then once again, that's another way in which flakiness can creep into the testing process.

00:11:23.280 --> 00:11:23.640
Right.

00:11:23.640 --> 00:11:29.980
And you maybe don't care about how the celery server is doing, but at the same time, you maybe you haven't mocked that part out.

00:11:29.980 --> 00:11:32.520
You need to somehow, are you doing an end-to-end test or something?

00:11:32.660 --> 00:11:35.420
It becomes part of the reliability of your system.

00:11:35.420 --> 00:11:39.640
Even though maybe that's like a Q&A celery server, not the production server, right?

00:11:39.640 --> 00:11:40.080
Yeah.

00:11:40.080 --> 00:11:43.700
And in fact, you brought up another really good point when it comes to mocking.

00:11:43.700 --> 00:11:50.660
There are many circumstances in which I've had to use one of the various mocking features that Python provides

00:11:50.660 --> 00:11:54.900
in order to stand up my own version of an external service.

00:11:55.020 --> 00:12:03.040
However, that is a trade-off like Owen mentioned previously, because now my test case may be less flaky,

00:12:03.040 --> 00:12:09.780
and yet it's also less realistic, and so therefore may not be able to catch certain types of bugs.

00:12:09.780 --> 00:12:17.040
So now we've seen another example of the trade-off associated with making a test less flaky,

00:12:17.040 --> 00:12:20.260
but perhaps also making it less realistic.

00:12:20.260 --> 00:12:25.000
Yeah, I'm starting to get a sense that there's probably a precision versus stability.

00:12:25.000 --> 00:12:27.620
trade-off that's always at play here.

00:12:27.620 --> 00:12:34.600
Yeah, obviously in the extreme end of the spectrum, you can make any test non-flaky by just deleting it, right?

00:12:34.600 --> 00:12:36.400
Put an ignore attribute on it.

00:12:36.400 --> 00:12:36.740
Exactly.

00:12:36.740 --> 00:12:41.100
So you've got to be careful that you're not optimizing your tests just for passing,

00:12:41.100 --> 00:12:47.380
which if you're trying to get a PR through, then that is a trap you might fall into.

00:12:47.380 --> 00:12:47.900
Yeah, absolutely.

00:12:47.900 --> 00:12:50.140
So you talked about that survey a little bit before.

00:12:50.140 --> 00:12:52.540
Maybe you want to talk about some more of the things you learned from there?

00:12:52.540 --> 00:12:54.820
What were some of the responses there?

00:12:54.820 --> 00:12:56.520
So we've got some really interesting ones, actually.

00:12:56.520 --> 00:13:00.300
I'll just, if you want to find the paper yourself, or if anyone's listening wants to find the paper,

00:13:00.300 --> 00:13:06.140
if you just Google surveying the developer experience of flaky tests, you should be able to find it.

00:13:06.140 --> 00:13:12.020
As I said, we also asked developers what they thought were the most common causes of flaky tests.

00:13:12.440 --> 00:13:16.840
And the cause that got the most votes was setup and teardown.

00:13:16.840 --> 00:13:25.360
So what I mean by that is flakiness being caused by a test case that either doesn't fully set up its executing environment,

00:13:25.360 --> 00:13:27.900
alternatively doesn't clean up after itself.

00:13:27.900 --> 00:13:35.840
And if it doesn't clean up after itself correctly, it could leave some global state behind that could then impact a later test case that's executed, if that makes sense.

00:13:36.040 --> 00:13:36.580
Yeah, absolutely.

00:13:36.580 --> 00:13:39.860
The cleanup one is especially tricky, right?

00:13:39.860 --> 00:13:46.960
We kind of know about setup because you're like, oh, well, we have to do this in order for this file to exist or whatever.

00:13:46.960 --> 00:13:53.940
Teardown part, that becomes really tricky because it could have knock-on effects for tests that either pass and they shouldn't,

00:13:53.940 --> 00:13:58.080
or don't pass because it didn't start fresh, right?

00:14:00.720 --> 00:14:05.020
This portion of Talk Python To Me is brought to you by JetBrains and PyCharm.

00:14:05.020 --> 00:14:09.800
Are you a data scientist or a web developer looking to take your projects to the next level?

00:14:09.800 --> 00:14:12.500
Well, I have the perfect tool for you, PyCharm.

00:14:12.500 --> 00:14:21.780
PyCharm is a powerful integrated development environment that empowers developers and data scientists like us to write clean and efficient code with ease.

00:14:21.780 --> 00:14:28.120
Whether you're analyzing complex data sets or building dynamic web applications, PyCharm has got you covered.

00:14:28.120 --> 00:14:35.280
With its intuitive interface and robust features, you can boost your productivity and bring your ideas to life faster than ever before.

00:14:35.280 --> 00:14:41.460
For data scientists, PyCharm offers seamless integration with popular libraries like NumPy, Pandas, and Matplotlib.

00:14:41.460 --> 00:14:48.580
You can explore, visualize, and manipulate data effortlessly, unlocking valuable insights with just a few lines of code.

00:14:48.580 --> 00:14:53.220
And for us web developers, PyCharm provides a rich set of tools to streamline your workflow.

00:14:53.340 --> 00:15:02.220
From intelligent code completion to advanced debugging capabilities, PyCharm helps you write clean, scalable code that powers stunning web applications.

00:15:02.220 --> 00:15:10.520
Plus, PyCharm's support for popular frameworks like Django, FastAPI, and React make it a breeze to build and deploy your web projects.

00:15:10.520 --> 00:15:15.440
It's time to say goodbye to tedious configuration and hello to rapid development.

00:15:15.980 --> 00:15:16.980
But wait, there's more.

00:15:16.980 --> 00:15:25.700
With PyCharm, you get even more advanced features like remote development, database integration, and version control, ensuring your projects stay organized and secure.

00:15:25.700 --> 00:15:31.160
So whether you're diving into data science or shaping the future of the web, PyCharm is your go-to tool.

00:15:31.160 --> 00:15:33.240
Join me and try PyCharm today.

00:15:33.420 --> 00:15:43.880
Just visit talkpython.fm/done-with-pycharm, links in your show notes, and experience the power of PyCharm firsthand for three months free.

00:15:43.880 --> 00:15:45.060
PyCharm.

00:15:45.060 --> 00:15:46.800
It's how I get work done.

00:15:48.400 --> 00:15:53.380
Well, this kind of leads us into a whole other type of flaky test called a test order dependent test.

00:15:53.380 --> 00:15:54.040
Okay.

00:15:54.040 --> 00:16:04.580
When you have a test case that doesn't clean up after itself properly, then that can potentially mean that later tests that perhaps are targeting similar parts of the program where there might be some state involved.

00:16:04.580 --> 00:16:15.380
They might fail when they should pass or alternatively pass when they should fail just because the assumptions that were there when the developer wrote that test aren't being met anymore because something's changed by another test.

00:16:15.380 --> 00:16:25.900
So what that means is that if you just take an arbitrary test suite, randomize the order, shuffle it, for any large test suite, I can almost guarantee that some tests are going to fail and all you've done is change the order.

00:16:25.900 --> 00:16:29.980
And they're failing because somewhere a test isn't cleaning up after itself properly.

00:16:29.980 --> 00:16:30.200
Yeah.

00:16:30.200 --> 00:16:33.420
Or cleaning up can mean different things, right?

00:16:33.520 --> 00:16:35.560
Cleaning up can mean we didn't change.

00:16:35.560 --> 00:16:41.920
We didn't take away that file we created or put back the file we deleted as part of this test scenario we're working with.

00:16:41.920 --> 00:16:47.840
But it could also be we're testing by talking to a database and we made an insert to it and didn't roll that back.

00:16:47.840 --> 00:16:51.420
Or maybe the most subtle, I guess there's two more levels here.

00:16:51.420 --> 00:16:53.740
One, you could have changed in memory state.

00:16:53.740 --> 00:16:54.060
Yeah.

00:16:54.060 --> 00:16:54.340
Right?

00:16:54.340 --> 00:16:58.300
You could have, like there's a shared variable, which is probably the most common reason.

00:16:58.580 --> 00:17:03.640
Like some shared state of the process just isn't in its starting or expected position.

00:17:03.640 --> 00:17:08.940
But the fourth one, when I said, oh, there's three, but actually I think there's more, is you mocked out something.

00:17:08.940 --> 00:17:11.980
Like I've mocked out what daytime.now means.

00:17:11.980 --> 00:17:13.380
I forgot to put it back.

00:17:13.380 --> 00:17:15.820
So time has stopped or something like that, right?

00:17:15.980 --> 00:17:16.120
Yeah.

00:17:16.120 --> 00:17:21.320
Those are all really good examples of the flakiness that can appear when you have shared state.

00:17:21.320 --> 00:17:27.400
And building on what both you and Owen just said, again, I think there's another tradeoff here.

00:17:27.400 --> 00:17:32.920
One of the tradeoffs is connected to the efficiency of the testing process.

00:17:32.920 --> 00:17:35.400
There's a flakiness of the testing process.

00:17:35.400 --> 00:17:43.260
So if you do a really good job at clearing out state from your database or resetting state in the memory of your process,

00:17:43.260 --> 00:17:49.540
that may take a longer time, but potentially reduce the amount of flakiness that manifests in your tests.

00:17:49.900 --> 00:17:58.740
And then additionally, it's worth noting that when you have test suites with really good setup and really good teardown and cleaning mechanisms,

00:17:58.740 --> 00:18:02.860
those are also more time consuming for us to write as developers,

00:18:02.860 --> 00:18:11.480
which may mean we're spending a lot of time investing in our test suite and perhaps slightly less time actually adding new features to our program.

00:18:11.480 --> 00:18:17.700
And so there's tradeoffs both in terms of developer productivity and the efficiency of the testing process.

00:18:17.700 --> 00:18:18.600
Those both matter.

00:18:18.780 --> 00:18:22.500
Which one matters more to you probably depends on your situation, right?

00:18:22.500 --> 00:18:28.200
If you're a small team and you need to move quick, the developer overhead is probably a serious hassle.

00:18:28.200 --> 00:18:35.940
But if you're a large team and you have 100,000 tests, you want to get answers today, not tomorrow from your test suite.

00:18:36.500 --> 00:18:39.620
The speed of execution probably matters more at that point.

00:18:39.620 --> 00:18:41.560
Yeah, I think that's absolutely the case.

00:18:41.560 --> 00:18:48.020
And so there have been some situations where I have certain test cases that take a really long time to run.

00:18:48.420 --> 00:19:00.020
And so in pytest, I might set a marker and only run those test cases at certain points of time during development on my laptop and then always run them inside a CI.

00:19:00.520 --> 00:19:06.320
And the nice thing about removing those long running test cases is that it can make the testing process faster.

00:19:06.480 --> 00:19:12.520
And I don't have to do my rigorous cleaning approach except when I am running them in CI.

00:19:12.520 --> 00:19:13.760
Yeah, that's an interesting idea.

00:19:13.760 --> 00:19:17.760
Maybe giving them tags and then coming up with a category of speed.

00:19:17.760 --> 00:19:23.060
I mean, I know I've heard of people doing like marking a test as slow, a set of tests as slow.

00:19:23.320 --> 00:19:25.260
Maybe that's not fine grained enough.

00:19:25.260 --> 00:19:29.920
Maybe doing something like fast, less than a second, less than five seconds, less than 10 seconds.

00:19:29.920 --> 00:19:34.140
So I'm willing to run all the ones that run in three seconds or less, but not more than that.

00:19:34.140 --> 00:19:34.580
Right.

00:19:34.580 --> 00:19:37.300
So you could kind of scale it up more than just fast and slow.

00:19:37.300 --> 00:19:43.540
So on the topic of markers, there's several plugins for pytest that enable you to mark a test as flaky.

00:19:43.540 --> 00:19:44.140
Okay.

00:19:44.140 --> 00:19:49.740
Basically, what that then means is that if it fails, it will retry it some number of times.

00:19:49.740 --> 00:19:53.620
And then if it passes at least once, it will call the whole thing a pass.

00:19:53.620 --> 00:19:57.520
So while that means, yeah, you can just make your test suite pass.

00:19:57.520 --> 00:20:12.240
It's like, so for example, so in the, in the survey, we had one respondent tell us sometimes a flaky test always a bad thing because sometimes the fact that a test is non-deterministic is showing that part of the software is non-deterministic when it shouldn't be.

00:20:12.240 --> 00:20:22.360
So if you were to follow this methodology of just rerunning all your flaky tests and ignoring them, if they pass at least once, then you'd miss out on that because you wouldn't be notified that that test failed.

00:20:22.360 --> 00:20:22.640
Yeah.

00:20:22.640 --> 00:20:23.480
That's a good point.

00:20:23.480 --> 00:20:28.360
Maybe it's highlighting a weakness in your infrastructure, your DevOps story.

00:20:28.360 --> 00:20:28.780
Yeah.

00:20:28.780 --> 00:20:30.600
You could say, well, that's out of my control.

00:20:30.600 --> 00:20:31.340
It's not my problem.

00:20:31.340 --> 00:20:39.680
Or you could say, no, actually folks, look, this is pointing out, this is the least stable pillar of our uptime for our app, right?

00:20:39.680 --> 00:20:40.740
Yeah, that's a good point.

00:20:40.740 --> 00:20:47.500
The other thing, since we're talking about randomness, that's important to discuss is the use of property-based testing tools.

00:20:47.500 --> 00:20:57.440
So, for example, I use Hypothesis in order to automatically generate inputs and then send them into my function under test.

00:20:57.440 --> 00:21:10.220
And there may be cases where Hypothesis reveals a bug, and that could, in fact, actually be a bug in my program, even though I've run exactly that same test case frequently in the past.

00:21:10.220 --> 00:21:20.400
And it just happens to be the case that in that run, when I was using a Hypothesis property-based test, it was able to find a potential problem.

00:21:20.400 --> 00:21:37.600
So, in that situation, even though that test didn't fail the last three times, this could still be a silver lining to suggest that there is a problem with my program, and I need to resolve it because Hypothesis has randomly generated an input that I haven't seen previously.

00:21:37.600 --> 00:21:39.400
Yeah, the Hypothesis story is interesting.

00:21:39.400 --> 00:21:43.740
I was thinking about that as well after reading some of the work that you're doing here.

00:21:43.740 --> 00:21:56.980
Thinking things like Hypothesis and parameterized testing and those sorts of things, where they just naturally take some test scenario and run it over and over with a bunch of different inputs, probably uncovers this better than one-off tests, I imagine.

00:21:56.980 --> 00:22:03.120
And Hypothesis also has a mode that lets you run a long-running fuzz testing campaign.

00:22:03.120 --> 00:22:08.460
And in those situations, it doesn't constrain its execution by a specific period of time.

00:22:08.580 --> 00:22:19.500
And I have found that when I let a fuzzing campaign go for a given function, maybe I've described its inputs with a JSON schema, and I'm using the Hypothesis JSON schema plugin.

00:22:19.500 --> 00:22:25.240
I might not find a bug for a long period of time, and then suddenly a bug will crop up.

00:22:25.240 --> 00:22:31.280
And I often use that as an opportunity to rethink my assumptions about the function that I'm testing.

00:22:31.420 --> 00:22:32.820
So you said fuzzing.

00:22:32.820 --> 00:22:33.960
Tell people out there.

00:22:33.960 --> 00:22:36.380
Give people a definition of that if they're unfamiliar with fuzzing.

00:22:36.380 --> 00:22:49.420
So when I think of fuzzing as a process where you're randomly generating inputs that you're going to send to the function under test, and you're frequently doing that in what I would call a campaign,

00:22:49.420 --> 00:22:57.280
which means that the input generation process is going to range perhaps for an extended period of time.

00:22:57.280 --> 00:23:09.560
And you may have different goals during that fuzzing campaign, like covering more of the program under test, or attempting to realize as many crash-inducing inputs as is possible.

00:23:09.700 --> 00:23:11.220
Yeah, that's such a cool idea.

00:23:11.220 --> 00:23:13.820
And that's sort of how a hypothesis works.

00:23:13.820 --> 00:23:21.880
Although I don't know it's really meant for fuzzing in the sense of just we're going to hit it with a whole bunch of stuff at extreme scale until it breaks.

00:23:21.880 --> 00:23:25.260
But it certainly is meant to run it a lot of times with different inputs.

00:23:25.260 --> 00:23:28.400
So sort of the effect, if not necessarily the intent of it.

00:23:28.400 --> 00:23:33.560
Here's another one from the audience that also is one that I hadn't thought about from Marwan.

00:23:33.560 --> 00:23:41.700
It says, sometimes flakiness shows up as conflicts to access a shared resource when more than one CI pipeline is running for the same code.

00:23:41.700 --> 00:23:42.400
That's pretty wild.

00:23:42.400 --> 00:23:43.660
I hadn't really thought about that, right?

00:23:43.660 --> 00:23:45.280
But if you have a lock on something.

00:23:45.280 --> 00:23:49.560
Resource availability in general is quite a common cause of flakiness.

00:23:49.560 --> 00:23:55.300
So this resource could be a file system or a database or anything really.

00:23:55.540 --> 00:24:04.640
Or even something, even if we're not talking about CI, just on a local machine, you've got, like you said, locks and things that aren't supposed to be shared between multiple processes or whatever.

00:24:04.640 --> 00:24:09.600
So yeah, that is a relatively common cause that I've seen in the programs that I've tested.

00:24:09.600 --> 00:24:18.480
Yeah, the thing about that that stands out to me is you might have an assumption that only one process of your app is ever going to be running on a server at a time.

00:24:18.480 --> 00:24:23.940
And yet somehow, because, you know, there were multiple Git commits that you weren't even aware of.

00:24:23.980 --> 00:24:25.440
Now they're running kind of in parallel.

00:24:25.440 --> 00:24:31.200
So you're in this situation that, you know, you never saw coming because you just don't run your app that way.

00:24:31.200 --> 00:24:31.960
You know what I mean?

00:24:31.960 --> 00:24:32.340
Yeah.

00:24:32.340 --> 00:24:45.000
This is actually a really good example of when a flaky test is again a silver lining because it forces you to question your assumptions about how your program will run and when it will run.

00:24:45.000 --> 00:24:48.580
And how much of resources your program is going to consume.

00:24:48.580 --> 00:25:03.100
So in the situation when I never thought about my program running in multiple instances at the same time, if my tests become flaky, that may actually open up a whole new opportunity for me to refactor and improve my program.

00:25:03.100 --> 00:25:03.740
Yeah, that's right.

00:25:03.740 --> 00:25:04.640
You're like, wait a minute.

00:25:04.640 --> 00:25:06.300
I didn't realize this was a problem.

00:25:06.300 --> 00:25:08.180
But yes, maybe it is.

00:25:08.180 --> 00:25:09.380
Let's see.

00:25:09.380 --> 00:25:16.280
So let's talk a little bit about some articles that a couple of the big tech companies wrote here.

00:25:16.280 --> 00:25:34.000
So both Google and Spotify talked about how they're using, how they're experiencing flaky tests and what they're doing to either reduce them or maybe, as you put it, Gregory, some of the silver linings that they're finding in it and some of the tools that are building to help deal with it.

00:25:34.000 --> 00:25:37.720
So over at Google, they say they're running, obviously, a large set of tests.

00:25:38.120 --> 00:25:41.720
I could probably like that is a massive understatement, I imagine.

00:25:41.720 --> 00:25:52.820
But it says they see a continual rate of flakiness of about 1.5% on all test cases, which for them, I imagine, is a lot of tests.

00:25:52.820 --> 00:26:00.920
And so you want to talk a little bit about this, either of you guys, and maybe some of the mitigation strategies they have?

00:26:00.920 --> 00:26:10.240
So from a developer perspective, so this article and others as well, point out an interesting side of flakiness that when you're talking from a purely technical perspective, you don't really consider.

00:26:10.240 --> 00:26:13.760
And that's the kind of the sort of the psychological impact of them.

00:26:13.760 --> 00:26:15.800
So it's a little bit like the boy who cried wolf.

00:26:16.300 --> 00:26:25.740
So if you have a test case that's known to be flaky, you might be tempted to just put some marker on it that says ignore it or it's an expected fail or whatever.

00:26:25.740 --> 00:26:34.660
But then suppose it fails for real sometime and you're ignoring it or you have it quarantined or something, then that means you're missing out on real bugs.

00:26:35.060 --> 00:26:44.900
So as well as just being a hindrance to CI and that kind of thing, it could almost make a developer team lose the discipline to properly investigate every test failure.

00:26:44.900 --> 00:26:50.540
If they've got a test suite that's known to be full of flaky tests, then naturally you're not going to trust it as much.

00:26:50.540 --> 00:26:50.840
Yeah.

00:26:50.840 --> 00:26:53.720
So that's probably one of the biggest problems that flaky tests cause, in my opinion.

00:26:53.720 --> 00:26:57.940
I think the mental aspect of it, how much do I trust it?

00:26:57.940 --> 00:27:00.460
Do I have faith in our test suite?

00:27:00.460 --> 00:27:04.980
Do I have faith in the continuous deployment capabilities of our team?

00:27:04.980 --> 00:27:06.460
Pipelines and things like that.

00:27:06.460 --> 00:27:08.400
That's, I think that's pretty serious.

00:27:08.400 --> 00:27:16.300
There's already a bit of a challenge, I think, on teams to have complete buy-in on making sure the software is self-evaluating.

00:27:16.300 --> 00:27:21.760
You know, like some people will check in code that breaks the build, but they're kind of like yellow, whatever.

00:27:21.760 --> 00:27:25.220
Other people really, you know, somehow they really want the build to work.

00:27:25.220 --> 00:27:28.720
So it's their job to kind of chase that person down and make them fix it.

00:27:28.720 --> 00:27:33.220
And it's always kind of a bit of a struggle, but that's when the tests are awesome, right?

00:27:33.280 --> 00:27:37.240
It just changes to the code, makes it sort of adds this, these breaking builds.

00:27:37.240 --> 00:27:44.040
But if the code is flaky, all of a sudden you can start to see CI as an annoyance because it tells you something's wrong.

00:27:44.040 --> 00:27:45.160
You're like, I know nothing's wrong.

00:27:45.160 --> 00:27:46.360
It's just, it'll go away.

00:27:46.880 --> 00:27:53.620
So maybe speak to the psychological bit of like how flakiness can maybe degrade people's caring about tests at all.

00:27:53.620 --> 00:28:01.400
I would say that overall developers have the risk of losing a confidence in two types of correctness.

00:28:01.400 --> 00:28:08.640
First of all, flaky test cases may cause developers to begin to mistrust and lose confidence in the test suite.

00:28:08.640 --> 00:28:24.280
Then they also may lose confidence in the overall correctness of their program and then may cause them to stop running test cases, which then reduces test quality and maybe even also reduces the quality of the program under test as well.

00:28:24.280 --> 00:28:32.920
So I think regrettably, it's a negative reinforcing cycle where you start to mistrust your tests so you don't run them.

00:28:32.920 --> 00:28:36.620
But then you start to lose confidence in the correctness of your program.

00:28:36.780 --> 00:28:41.960
And now you're not sure what to do because tests are failing for spurious reasons.

00:28:41.960 --> 00:28:43.120
You disable them.

00:28:43.120 --> 00:28:48.280
But then as Owen mentioned previously, you lose the opportunity to get the feedback from those tests.

00:28:48.280 --> 00:28:49.560
It goes both ways, right?

00:28:49.560 --> 00:28:55.620
You don't feel like if it says it's broken, it provides you much value because it might report broken even though it's working.

00:28:55.620 --> 00:28:59.620
But on the flip side, if you were doing, you know, continuous deployment.

00:28:59.620 --> 00:29:05.660
And by that, I mean, I check into a branch, that branch notices the change automatically, it rolls out the new version, right?

00:29:05.660 --> 00:29:09.560
Maybe you merge over to a production branch and it just it takes off.

00:29:09.560 --> 00:29:16.900
The gate to making that not go to production is the CI system that's going to say whether or not the test pass.

00:29:16.900 --> 00:29:22.020
If the test pass and maybe they shouldn't have because, you know, you've got this flakiness.

00:29:22.020 --> 00:29:24.340
Well, that's also also not good.

00:29:24.340 --> 00:29:28.260
That's a situation when you could have just deployed software that wasn't working.

00:29:28.260 --> 00:29:34.440
And then the flip side of that is you have a flaky build and you want to be able to release quickly.

00:29:34.440 --> 00:29:38.720
But because test cases are failing, you don't release your system.

00:29:38.720 --> 00:29:51.620
And so it can really be a hindrance to being able to quickly push your work to production because you frequently have flaky test cases that are causing you to limit the velocity of your development process.

00:29:54.140 --> 00:29:57.020
This portion of Talk Python To Me is brought to you by Sentry.

00:29:57.020 --> 00:29:59.640
You know Sentry for their error tracking service.

00:29:59.640 --> 00:30:05.900
But did you know you can take that all the way through your multi-tiered and distributed app with their distributed tracing feature?

00:30:05.900 --> 00:30:17.280
Distributed tracing is a debugging technique that involves tracking requests of your system, starting from the very beginning, like a user action, all the way to the back end, database and third party services.

00:30:17.280 --> 00:30:22.660
This can help you identify if the cause of an error in one project is due to the error in another.

00:30:22.660 --> 00:30:27.540
Every system can benefit from distributed tracing, but they're especially useful for microservices.

00:30:27.540 --> 00:30:34.360
In this architecture, logs won't give you the full picture, so you can't debug every request in full just by reading the logs.

00:30:34.360 --> 00:30:42.560
Distributed tracing with a platform like Sentry gives you a visual overview about which services were called during the execution of certain requests.

00:30:43.300 --> 00:30:49.720
Aside from debugging and visualizing architecture, distributed tracing also helps you identify performance bottlenecks.

00:30:49.860 --> 00:30:58.620
Through a visual like a Gantt chart, you can see if a particular span in your stack took longer than expected and how it could be causing slowdowns in other parts of your app.

00:30:58.620 --> 00:31:03.600
Learn more and see some examples in the tracing section at docs.sentry.io.

00:31:03.600 --> 00:31:08.300
To take advantage of all the features of the Sentry platform, just create your free account.

00:31:08.460 --> 00:31:16.760
And for all of you Talk Python listeners, use the code Talk Python, all one word, and you'll activate a free month of their premium paid features.

00:31:16.760 --> 00:31:21.360
Get started today at talkpython.fm/sentry dash trace.

00:31:21.360 --> 00:31:24.480
That link is in your podcast player show notes and the episode page.

00:31:24.480 --> 00:31:27.780
Thank you to Sentry for supporting Talk Python To Me.

00:31:30.280 --> 00:31:31.260
You got thoughts on that?

00:31:31.260 --> 00:31:33.580
No, I think Greg's pretty much covered that pretty well.

00:31:33.580 --> 00:31:34.160
Yeah, yeah.

00:31:34.160 --> 00:31:34.580
Excellent.

00:31:34.580 --> 00:31:42.580
So the two things, two takeaways from the Google article is, one, they talked about mitigation strategies.

00:31:42.580 --> 00:31:46.000
And they said they have a tool that monitors the flakiness of tests.

00:31:46.000 --> 00:31:52.380
And if the flakiness is too high, it automatically quarantines the test, takes it out of the critical path, takes it out of CI.

00:31:52.380 --> 00:31:56.320
You know, maybe somebody notices like, hey, there's a new flaky test.

00:31:56.320 --> 00:31:58.160
We need to go find the root cause of that.

00:31:58.160 --> 00:32:00.480
But that's a pretty interesting idea, isn't it?

00:32:00.480 --> 00:32:15.440
Some sort of automation or maybe not quite totally automatic, but some kind of tool that you can run that'll say, this thing has reached a point where maybe its value in the test suite is degraded because it's so flaky that we need to either fix it or just delete it.

00:32:15.440 --> 00:32:23.440
I think the problem with quarantining tests is it only works if the development team is serious about investigating them.

00:32:23.440 --> 00:32:28.940
Otherwise, what could end up happening is quarantining becomes effectively equivalent to just deleting it.

00:32:28.940 --> 00:32:34.820
If they all end up in a special flaky bucket and no one looks at them again, the whole point of the process is kind of moot, really.

00:32:34.820 --> 00:32:43.000
So doing something like that, I think, can be really useful if the developers are willing to actually investigate why these tests are flaky.

00:32:43.220 --> 00:32:43.500
It's true.

00:32:43.500 --> 00:32:48.600
If it becomes just a black box and basically a trash can for tests, then what's the point, right?

00:32:48.600 --> 00:32:48.940
Exactly.

00:32:48.940 --> 00:32:49.240
Yeah.

00:32:49.380 --> 00:32:57.400
It kind of goes back to my talking about like there's some people on the team that really care about the build and continuous integration and all this and other people who just don't.

00:32:57.400 --> 00:33:02.680
So it does come back to the team mentality and people really caring about these things.

00:33:02.680 --> 00:33:13.440
But it's a cool idea if, at least in the optimistic point of view, where assuming everyone wants to make sure these keep working and someone's going to pay attention to this and so on.

00:33:13.440 --> 00:33:19.880
Yeah, it's a difficult one because, I mean, sometimes you can just write a bad test and that test is flaky purely because it's a bad test.

00:33:19.880 --> 00:33:24.640
But other times you can write a good test that's flaky because there's a problem.

00:33:24.640 --> 00:33:32.620
Like I said before, we had one developer say that sometimes a flaky test implies that a part of the program they thought was deterministic was actually non-deterministic.

00:33:32.620 --> 00:33:38.880
So you're potentially throwing away useful information as well as potentially throwing away just poorly written tests.

00:33:38.880 --> 00:33:39.180
Right.

00:33:39.180 --> 00:33:40.820
And it's hard to distinguish between those two.

00:33:40.820 --> 00:33:41.900
I'm sure that it is.

00:33:41.980 --> 00:33:48.140
Yeah, I mean, identifying these, maybe not quarantining them, but identifying them is pretty valuable, I would think.

00:33:48.140 --> 00:33:50.360
And then you can see what lessons come from that, right?

00:33:50.360 --> 00:33:53.840
What you do once you've identified it, I think that is up for debate, right?

00:33:53.840 --> 00:33:54.180
Yeah.

00:33:54.180 --> 00:33:54.660
Okay.

00:33:54.660 --> 00:34:03.040
The other one is Test Flakiness, Methods for Identifying and Dealing with Flaky Tests by Jason Palmer from Spotify, which is also cool.

00:34:03.040 --> 00:34:04.480
This one has pictures, which is fun.

00:34:04.480 --> 00:34:10.080
They've got like a graphical analysis of their tests and the flakiness of it and so on.

00:34:10.280 --> 00:34:12.660
They came up with a thing called Flaky Bot.

00:34:12.660 --> 00:34:22.680
And it's a GitHub integration, GitHub bot that they can make it run and they can ask it to exercise the test really quickly and see if it's flaky.

00:34:22.680 --> 00:34:32.320
And I got the sense that it does that by just running into a bunch of different times with different delays and seeing if it always passes or if it potentially sometimes passes or fails.

00:34:32.320 --> 00:34:42.380
So I think broadly, one of the things that is mentioned in this article and something that's done by a number of pytest plugins as well is rerunning the test suite.

00:34:42.380 --> 00:34:47.420
And so you could imagine rerunning each test case in isolation.

00:34:47.920 --> 00:34:58.980
You could also imagine picking a group of test cases and then rerunning the test cases in that group, either in a random order or in certain fixed orders.

00:34:58.980 --> 00:35:13.560
So rerunning is often a very helpful way for us to detect flaky test cases, whether we rerun the whole test suite, whether we run test cases individually or whether we run test cases in groups.

00:35:14.200 --> 00:35:22.980
Obviously, one of the clear downsides associated with rerunning a test suite is the execution time associated with the rerunning process.

00:35:22.980 --> 00:35:29.020
Yeah, the more you run it, the more likely you're able to detect flakiness if it's only a little bit flaky.

00:35:29.020 --> 00:35:33.880
But at the same time, the longer that goes, the longer it takes, that's also a problem.

00:35:33.880 --> 00:35:40.260
There's another thing in here that I thought was pretty interesting, but I'm struggling to find it in this article for the second.

00:35:40.260 --> 00:35:42.040
Integration, no?

00:35:42.520 --> 00:35:44.160
I thought end-to-end maybe.

00:35:44.160 --> 00:35:51.900
So in the Spotify article, they say that, in their assessment, that end-to-end tests are flaky by nature.

00:35:51.900 --> 00:35:53.000
Write fewer of them.

00:35:53.000 --> 00:35:59.380
So I get the sense, I don't know, I get the sense maybe you all sort of feel this way as well, but I don't necessarily agree with that.

00:35:59.380 --> 00:36:04.800
I think end-to-end tests, if they are flaky, that's telling you something about your program.

00:36:04.800 --> 00:36:12.460
It might not be really precisely narrowing in on it, but it's telling you something about your program if you can write end-to-end tests that are flaky.

00:36:12.460 --> 00:36:13.140
What do you think?

00:36:13.140 --> 00:36:23.860
I think with end-to-end tests, I mean, sort of saying they're flaky by nature may be a little strong, but they're certainly more susceptible to flakiness purely because there's a hell of a lot more going on.

00:36:23.860 --> 00:36:33.100
So I think when we talk about this sort of flakiness and sort of precision trade-off, I think with end-to-end tests, you should be a little bit more forgiving with flakiness purely because there's more going on.

00:36:33.100 --> 00:36:33.300
Yeah.

00:36:33.400 --> 00:36:39.240
So like, for example, for a unit test, you shouldn't really accept any flakiness because that's a very focused test case.

00:36:39.240 --> 00:36:41.000
So yeah, those are my thoughts on that.

00:36:41.000 --> 00:36:41.380
Okay.

00:36:41.380 --> 00:36:43.040
I would agree with what Owen said.

00:36:43.040 --> 00:36:53.700
I still think there is quite a bit of value end-to-end or integration testing because from my perspective, it's increasing the realism of the testing process.

00:36:53.700 --> 00:37:10.780
So I still write end-to-end test cases if I'm building a web API or even if I'm building an application, but I think I have to be willing to tolerate a little bit more flakiness and perhaps even be creative with the various strategies that I adopt when I do rerunning.

00:37:10.780 --> 00:37:19.180
Maybe I need to run some of my integration tests with really good setup and teardown to avoid pollution between test cases.

00:37:19.380 --> 00:37:29.140
Or maybe certain integration test cases have to be run completely in isolation and they can't be run while any other part of the program is being used.

00:37:29.140 --> 00:37:37.200
So in those cases, maybe my integration tests are run less frequently, but I still keep them as a part of my pytest test suite.

00:37:37.200 --> 00:37:37.480
Yeah.

00:37:37.480 --> 00:37:37.920
Interesting.

00:37:37.920 --> 00:37:38.780
Both of you.

00:37:38.780 --> 00:37:48.640
For me, one of the things I do that I think is really valuable is over at Talk Python, we have the courses and the web app that serves up the courses and those people buy them and all that sort of stuff.

00:37:48.720 --> 00:37:54.300
It's like 20,000 lines of Python, maybe more these days, haven't measured it for a long time, but it's, you know, a non-trivial amount.

00:37:54.300 --> 00:37:58.660
And it's got a sitemap of all the pages on there.

00:37:58.660 --> 00:38:09.980
And one of the things I do for the test is just go and find every, pull the sitemap, look at every page on the site and just request it and make sure it doesn't 500 out or 404 or things like that.

00:38:09.980 --> 00:38:12.380
And it just, they all work, right?

00:38:12.720 --> 00:38:16.080
Now there's like 6,000 links in the sitemap.

00:38:16.080 --> 00:38:22.260
So it says, well, these 500 are all really the same thing with just different data behind it.

00:38:22.260 --> 00:38:23.440
So just pick one of those.

00:38:23.440 --> 00:38:27.560
There's a way to kind of winnow it down to, you know, 20 requests and not 6,000.

00:38:27.560 --> 00:38:33.820
But that kind of stuff, there should be no time where there is a 404 on your site.

00:38:33.820 --> 00:38:37.920
It's not an inherent flakiness of testing that there's not a 404.

00:38:37.920 --> 00:38:39.340
There should not be a 404.

00:38:39.340 --> 00:38:39.880
Same thing.

00:38:39.880 --> 00:38:43.380
There should not be a 500, my website, my server crashed.

00:38:43.380 --> 00:38:45.380
It should never crash, right?

00:38:45.900 --> 00:38:49.760
And so those types of integration tests, I think, I think they still add a lot of value, right?

00:38:49.760 --> 00:38:53.260
Because you could miss something like, well, I checked the database models, they were fine.

00:38:53.260 --> 00:38:55.400
I checked the code that works with the database models are fine.

00:38:55.400 --> 00:39:00.240
But the HTML assumed there was a field in the database model we passed to it.

00:39:00.240 --> 00:39:05.200
There is value in the sort of holistic, like, does it still click together story, I think.

00:39:05.200 --> 00:39:07.400
No, I think you're making a really good point, Michael.

00:39:07.400 --> 00:39:13.820
And so what I often do, like, say, for example, I'm adding a new feature or I'm adding a bug fix.

00:39:14.080 --> 00:39:28.620
What I'm going to regularly do to be confident in my changes to the system is run my integration tests maybe every once in a while and run my unit tests very frequently during the refactoring process.

00:39:28.620 --> 00:39:32.860
I can't run them all the time because they regrettably take too long to run.

00:39:32.860 --> 00:39:39.520
But I can run my integration test cases very frequently when I'm adding a bug fix or adding a new feature.

00:39:39.660 --> 00:39:52.640
And then every once in a while do the kind of smoke tests that you mentioned and then the integration or end-to-end testing that we've been discussing so that ultimately I have rapid feedback that gives me confidence.

00:39:52.640 --> 00:39:58.880
And then additionally, I have longer running tests that further give me confidence that my program is working.

00:39:58.880 --> 00:39:59.820
That's a really good point.

00:39:59.820 --> 00:40:03.300
And maybe you don't even run all your unit tests as you're writing that feature.

00:40:03.380 --> 00:40:06.020
Maybe you run a group of them that you think are related.

00:40:06.020 --> 00:40:06.460
Yeah.

00:40:06.460 --> 00:40:15.700
So you're bringing up something that I really love about the Python ecosystem, which is the awesome coverage.py and the pytest-cov plugin.

00:40:15.700 --> 00:40:18.120
Those plugins are really good.

00:40:18.120 --> 00:40:24.460
And what's awesome about coverage.py is that it can track code coverage on a per-test-case basis.

00:40:24.460 --> 00:40:30.580
So one of the things that I will often do is look at what test cases cover what part of the system.

00:40:30.580 --> 00:40:45.300
And as you mentioned, I'll only run those test cases that are covering the parts of the system that I'm changing because that helps me to get very rapid feedback from my unit test cases while I'm adding a new feature to a certain part of my program.

00:40:45.300 --> 00:40:49.180
I didn't realize that coverage.py would tell you that in reverse.

00:40:49.180 --> 00:40:52.460
Like, for this part of your program, these are the five tests.

00:40:52.460 --> 00:40:53.380
That's really cool.

00:40:53.380 --> 00:40:53.620
Yeah.

00:40:53.620 --> 00:40:55.060
So I really like that feature.

00:40:55.060 --> 00:40:58.380
I think it was released in coverage.py 5.0.

00:40:58.380 --> 00:41:01.820
And I've been using it since the feature was available.

00:41:02.180 --> 00:41:18.260
It's incredibly helpful because of the fact that you can look at specific statements in your code and then find out which test cases cover those statements and then choose to rerun those specific tests when you're repeatedly running your test suite.

00:41:18.460 --> 00:41:23.460
And I call that test suite reduction or coverage-based test suite reduction.

00:41:23.460 --> 00:41:31.880
And having what Coverage.py calls the context of coverage is, in my experience, very, very helpful.

00:41:31.880 --> 00:41:32.180
Yeah.

00:41:32.180 --> 00:41:34.440
The bigger your test suite is, the more helpful it is, right?

00:41:34.440 --> 00:41:35.020
Absolutely.

00:41:35.020 --> 00:41:41.380
Owen, did you find that a lot of people were using those kinds of tools to sort of limit the amount of tests they got to run?

00:41:41.380 --> 00:41:49.600
With the sort of programs I was working with, for the purposes of my experiments, I was running the whole test suite in its entirety multiple times to find flaky tests.

00:41:49.740 --> 00:41:52.220
But I did see evidence of that kind of thing being set up.

00:41:52.220 --> 00:41:55.140
So I think it is fairly well-adopted.

00:41:55.140 --> 00:42:06.480
Once again, it's a lot more relevant to very large projects as opposed to small projects where if it only takes you 10 seconds to run the whole test suite, obviously there's not a lot of point in doing...

00:42:06.480 --> 00:42:07.000
Just let it run.

00:42:07.000 --> 00:42:07.300
Yeah.

00:42:07.300 --> 00:42:08.640
But when you've got...

00:42:08.640 --> 00:42:15.200
I've dealt with test suites that take best part of six hours to run end-to-end.

00:42:15.520 --> 00:42:20.120
So having some kind of test selection, test reduction there is essential, really.

00:42:20.120 --> 00:42:24.620
In the winter, it's nice because then your computer can spend six hours heating the room.

00:42:24.620 --> 00:42:28.820
It puts a little less stress on the house heater or office heater.

00:42:28.820 --> 00:42:38.600
Seriously, what do you think about the systems like the IDEs that have their extensions are built right in, where they just constantly run the tests as you make changes?

00:42:38.600 --> 00:42:41.240
They just notice the files have changed, so we're rerunning the tests.

00:42:41.240 --> 00:42:48.300
So I don't use that in an IDE, but I do have something like that set up that runs in a terminal window.

00:42:48.300 --> 00:42:51.780
And I found continuous testing to be quite helpful.

00:42:51.780 --> 00:43:00.900
It's really helpful in cases where maybe I forget to run my test suite while I'm refactoring my program, and it can give me immediate feedback.

00:43:01.060 --> 00:43:13.520
To go back to a comment that I made previously, you can also use different pytest plugins or use something like Hypothesis so that you can run your test suite with random inputs on a regular basis.

00:43:13.520 --> 00:43:23.280
And I have found that's another good way for me to be able to, without having to think too hard, find potential bugs in the functions that I'm testing.

00:43:23.420 --> 00:43:24.020
Okay, interesting.

00:43:24.020 --> 00:43:25.680
Let's talk about some of the tools.

00:43:25.680 --> 00:43:32.460
So you all highlighted a couple of tools that people can use to help find flaky tests.

00:43:32.460 --> 00:43:36.620
So over at Datadog, they've got one for flaky test management.

00:43:36.620 --> 00:43:37.940
Want to tell people about that?

00:43:37.940 --> 00:43:50.700
So many of the tools that are provided by companies like Datadog are offering you a type of dashboard that will help you to better understand the characteristics of your test suite.

00:43:50.700 --> 00:43:55.960
So you can see what are the test cases that tend to be the most flaky.

00:43:55.960 --> 00:44:04.800
I think oftentimes it's hard for us to get a big picture view of our test suite and to understand what is and is not flaky.

00:44:05.440 --> 00:44:15.620
And so therefore having a flaky test management dashboard like Datadog provides can often give me the observability or the visibility that I might miss otherwise.

00:44:15.620 --> 00:44:16.560
That's super cool.

00:44:16.560 --> 00:44:17.340
And let's see.

00:44:17.340 --> 00:44:19.500
There's, I don't know, that's not the one I want to pull up.

00:44:19.500 --> 00:44:22.340
Also at Cypress has flaky test management.

00:44:22.340 --> 00:44:27.640
This is a really interesting approach because I normally use Cypress when I'm testing websites.

00:44:27.640 --> 00:44:29.960
And in my experience, when I'm-

00:44:29.960 --> 00:44:30.120
What is Cypress?

00:44:30.120 --> 00:44:32.040
I'm not familiar that maybe people aren't as well.

00:44:32.040 --> 00:44:33.320
Give us a quick intro first.

00:44:33.320 --> 00:44:34.740
Sure, I'd love to do so.

00:44:34.840 --> 00:44:40.960
So Cypress is a tool that helps you to do testing of your web user interfaces.

00:44:40.960 --> 00:44:46.700
So if you have a web application and you want to be able to test the input to a form,

00:44:46.700 --> 00:44:51.380
or you want to be able to test certain workflows through your web application,

00:44:51.380 --> 00:44:55.720
you can use Cypress and you write your test cases.

00:44:56.480 --> 00:45:02.180
Essentially, it's as if Cypress is running its own Chrome and it can control your test suite.

00:45:02.180 --> 00:45:03.760
It can run your test cases.

00:45:03.760 --> 00:45:09.200
When things fail, it can actually give you snapshots of what failed.

00:45:09.200 --> 00:45:18.520
It can tell you about the browser version that it was using or maybe the mobile ready viewport that it was currently running it at.

00:45:18.720 --> 00:45:28.020
And again, the nice thing about things like what Cypress provides is that it can give you some kind of flaky test case analytics,

00:45:28.020 --> 00:45:31.080
which can show you which are failing and which are passing.

00:45:31.080 --> 00:45:39.780
And it can also say, hey, these that are flaky and then break it out in terms of which ones are the most flaky versus the least flaky.

00:45:39.840 --> 00:45:45.940
Again, primarily in the context of testing web interfaces or web applications.

00:45:45.940 --> 00:45:49.820
Sounds a little bit like Selenium or Playwright, which are both nice.

00:45:49.820 --> 00:45:50.680
It is.

00:45:50.680 --> 00:45:56.880
So I have to say I've had the most flaky tests for my Selenium test cases.

00:45:56.880 --> 00:46:08.360
But when I switch to either Cypress or Playwright, Playwright as well has a way so that you don't have to do these baked in weights inside of your test case,

00:46:08.360 --> 00:46:14.340
which is one of the sources of flakiness that Owen and I have found in a number of real world programs.

00:46:14.340 --> 00:46:16.240
I'd say that's almost one of the most common, actually.

00:46:16.240 --> 00:46:16.820
Okay, Owen.

00:46:16.820 --> 00:46:25.000
So Gregory points out that it could be not exactly there's something wrong with your program or your code or the infrastructure your code depends upon,

00:46:25.000 --> 00:46:31.660
but maybe almost a flaky test framework itself, a flaky test runner scenario,

00:46:31.660 --> 00:46:38.420
where the flakiness is not even necessarily it's in the observation, not in the execution.

00:46:38.420 --> 00:46:39.840
That's pretty interesting.

00:46:39.840 --> 00:46:47.740
Well, yeah, the classic formula for something like that is a test case that says launch something asynchronously, wait one second, check something.

00:46:47.740 --> 00:46:48.620
Yeah.

00:46:48.620 --> 00:46:50.660
You might think that that one second is enough.

00:46:50.660 --> 00:46:56.700
If there's a time when for whatever reason there's some background work going on or it takes a little longer than that, then.

00:46:56.700 --> 00:46:58.680
And all of a sudden it needed one and a half seconds.

00:46:58.680 --> 00:46:59.860
Then you have a flaky test.

00:46:59.860 --> 00:47:00.460
Yeah.

00:47:00.580 --> 00:47:01.080
Yeah, for sure.

00:47:01.080 --> 00:47:06.100
Any of those things where you have to start something and then wait for it to something to happen remotely.

00:47:06.100 --> 00:47:08.060
That's got to be pretty sketchy.

00:47:08.060 --> 00:47:11.140
The usual approach is to sort of have an explicit wait.

00:47:11.140 --> 00:47:16.840
So you'll sort of say, I'm actually going to wait until this is completed, whatever that means.

00:47:16.840 --> 00:47:24.960
But then you run into a situation where, well, what if for whatever reason, this asynchronous thing you're interacting with is timed out or frozen,

00:47:24.960 --> 00:47:27.120
then you're going to end up with a test that's waiting forever.

00:47:27.400 --> 00:47:30.600
So you have to have some kind of upper limit to how long you'll wait for.

00:47:30.600 --> 00:47:31.880
Otherwise you may wait forever.

00:47:31.880 --> 00:47:32.200
Yeah.

00:47:32.200 --> 00:47:33.860
This test case is real slow.

00:47:33.860 --> 00:47:37.020
So once again, there's no kind of silver bullet solution, really.

00:47:37.020 --> 00:47:38.180
It's just trade-offs again.

00:47:38.180 --> 00:47:39.000
Yeah, yeah, yeah.

00:47:39.000 --> 00:47:48.360
What do you all think about things like tenacity where you can go and put a decorator onto a function and just say, retry this with some scenario?

00:47:48.360 --> 00:47:53.180
Or another one that I just learned about is Hynek's stamina, which is cool as well.

00:47:53.460 --> 00:48:03.800
You can say, put a decorator and say, retry a certain number of attempts with, you know, like some kind of back off, an exponential back off where you give it a certain amount of time.

00:48:03.800 --> 00:48:11.400
Like for flaky tests, you see it making sense to say, well, maybe call the functions this way in some of your test cases.

00:48:11.580 --> 00:48:15.000
So I've never actually seen either of these plugins, but they do look quite interesting.

00:48:15.000 --> 00:48:15.400
Yeah.

00:48:15.400 --> 00:48:18.880
I haven't used tenacity either, but I was aware of it.

00:48:18.880 --> 00:48:23.340
And I think you could imagine using tenacity in two distinct locations.

00:48:23.340 --> 00:48:34.240
Maybe you want to put some of these tenacity annotations on your multi-threaded code and then let the test case call those annotated functions.

00:48:34.240 --> 00:48:35.000
Yes, exactly.

00:48:35.000 --> 00:48:36.460
Don't put them in your production code.

00:48:36.460 --> 00:48:42.240
Don't put them on your test, but just have like an intermediate one that you can control the back off and retry count.

00:48:42.240 --> 00:48:42.820
Exactly.

00:48:42.820 --> 00:48:43.440
Exactly.

00:48:43.440 --> 00:48:57.180
And another thing that I think is worth pointing out since we were previously discussing web testing is that there is a way in which it can be a problem with your testing framework, like you previously mentioned, Michael.

00:48:57.180 --> 00:49:02.180
So, for example, Playwright does have a really nice auto-weighting feature.

00:49:02.180 --> 00:49:21.100
And so when I'm testing a web application, I can use Playwright's auto-weight feature, and that will help me to avoid baking in hard-coded weights inside my test because the actual testing framework itself has a way to do auto-weighting.

00:49:21.100 --> 00:49:32.620
So when you say auto-weighting, you can say things like request this page, find this field, put my email address into it, click this button, test that this thing is, you know, now the page has this thing in it.

00:49:32.620 --> 00:49:36.400
But obviously servers don't instantly respond to that, right?

00:49:36.400 --> 00:49:38.020
So you've got to have some sorts of delays.

00:49:38.020 --> 00:49:41.080
So you're talking about the system can kind of track that automatically.

00:49:41.080 --> 00:49:41.620
Yeah.

00:49:41.620 --> 00:49:44.660
So Playwright can actually do some of that on its own.

00:49:44.660 --> 00:50:03.740
So, for example, if you're looking for a certain element in the web page to be available, Playwright has a way that will allow you to ensure that the element is actually attached to the DOM, that it's actually visible, that it hasn't moved around, or that it's not being animated in some way.

00:50:03.740 --> 00:50:15.760
And all of those things are actually part of the testing framework, which makes it incredibly helpful because then I don't have to actually implement all of that when I'm writing my test cases.

00:50:15.760 --> 00:50:16.260
Fantastic.

00:50:16.260 --> 00:50:18.240
We're getting a little short on time here.

00:50:18.240 --> 00:50:19.000
Let's round.

00:50:19.000 --> 00:50:22.040
I want to round it out with a little bit of a survey.

00:50:22.040 --> 00:50:24.620
If I find the right thing.

00:50:24.620 --> 00:50:30.520
Of some of y'all mentioned some of the pytest plugins that might be relevant here.

00:50:30.520 --> 00:50:33.480
So I'm pulling up Awesome pytest, which I'll link to.

00:50:33.480 --> 00:50:36.080
Just an awesome list of pytest things.

00:50:36.080 --> 00:50:44.720
But you've got things like in here, like pytest randomly, which lets you randomly order tests and set a seed and those kinds of things.

00:50:44.720 --> 00:50:46.080
And a bunch of other stuff.

00:50:46.080 --> 00:50:51.200
Do you want to pull out some of these you maybe think are relevant or see if they're at least in your list, the ones you like?

00:50:51.200 --> 00:50:52.640
So I've used randomly before.

00:50:52.640 --> 00:51:07.320
And like how I said earlier, this could be a great way of finding those tests that not necessarily the ones that don't clean up out of themselves properly, but it will certainly show you tests that are potentially impacted by other tests not cleaning up out of themselves.

00:51:07.320 --> 00:51:16.020
So I think if you take almost any large test suite and apply randomly to it, the chances are you are probably going to see some failed tests that weren't failing before you shuffled the order.

00:51:16.020 --> 00:51:18.020
So I think that's quite an interesting plugin.

00:51:18.020 --> 00:51:23.120
And you can use to quickly assess if you've got order dependent tests in your test suite.

00:51:23.180 --> 00:51:36.220
Speaking from experience, the one additional point that I would add is that when I use pytest randomly, I try to make sure I integrate it early into the lifetime of my development process.

00:51:36.220 --> 00:51:51.240
So instead of writing 947 test cases and then trying to use pytest randomly, I try to always make sure that pytest randomly is running in GitHub Actions very early in the development of my application.

00:51:51.480 --> 00:52:06.680
So that when I only have 40 or 50 test cases, I can immediately find those dependent tests that could have flakiness and then begin to be more proactive when it comes to avoiding flakiness very early when I'm launching a new product.

00:52:06.680 --> 00:52:07.400
Yeah, that makes sense.

00:52:07.400 --> 00:52:17.880
One of my follow-up questions was going to be, would you all recommend de facto installing that into and turning that on at least unless you have a reason to disable it for new projects?

00:52:17.880 --> 00:52:18.920
I find it very helpful.

00:52:19.080 --> 00:52:22.340
It's one of the things that I frequently add to a new project.

00:52:22.340 --> 00:52:39.860
So when I'm, I use poetry for a lot of my package management and I have various templates that I use and I often add pytest randomly right away as one of my dev dependencies and then make sure I'm always running my test suite in a random order when I'm running it in GitHub Actions.

00:52:39.860 --> 00:52:47.360
So I can't remember who I heard this off for, where I read it exactly, but I have heard that at Google, other companies as well,

00:52:47.500 --> 00:52:52.460
running the tests in a random order is actually standard practice for the reason, like Greg just said.

00:52:52.460 --> 00:52:57.920
So when you start on a new project, you're starting with this sort of shuffled order test running.

00:52:57.920 --> 00:53:08.840
And I suppose it's kind of like a technical debt then you're kind of paying it off early rather than writing a bunch of tests and then having a big fixing effort when you realize there's a big problem with a whole bunch of them.

00:53:08.920 --> 00:53:16.280
I feel like it's a little similar to linting and things that go through and tell you these are the recommended issues or issues we found.

00:53:16.280 --> 00:53:18.420
We recommend fixing them for your code.

00:53:18.620 --> 00:53:29.020
If you apply that retroactively, like rough or whatever, if you apply that to your project after it's huge, you'll get thousands of lines and nobody wants to spend the next two weeks fixing them.

00:53:29.020 --> 00:53:33.020
But if you just run that as you develop it, go, there's two little things we got to fix.

00:53:33.020 --> 00:53:33.640
No big deal.

00:53:33.640 --> 00:53:35.920
See, it sounds similar to that effect.

00:53:35.920 --> 00:53:36.560
I agree.

00:53:36.560 --> 00:53:36.880
Yeah.

00:53:37.080 --> 00:53:38.260
Here's another one that's interesting.

00:53:38.260 --> 00:53:41.580
High test.socket to disable socket calls during tests.

00:53:41.580 --> 00:53:46.100
So you heard me talk about requesting every page on the sitemap.

00:53:46.100 --> 00:53:50.520
So I'm not necessarily suggesting that you would want to just do this in general.

00:53:50.520 --> 00:53:57.580
But, you know, one of the areas that seems to me that could be result in flakiness for a set of tests is like I depend on an external system.

00:53:57.580 --> 00:54:01.140
Like, oh, I thought we were mocking out the database, but I'm actually talking to the database.

00:54:01.140 --> 00:54:04.940
Or, oh, I thought we were mocking out the API call, but we're talking to it.

00:54:05.240 --> 00:54:11.920
You could probably turn that on for a moment, see which test fails, and just go, well, these three were not supposed to be talking over the network.

00:54:11.920 --> 00:54:15.100
But somehow they fail and we don't let them talk to the network.

00:54:15.100 --> 00:54:16.400
So that might be worth looking into.

00:54:16.400 --> 00:54:17.160
What do you think about that?

00:54:17.160 --> 00:54:17.880
That's a good point.

00:54:17.880 --> 00:54:24.840
I haven't tried that tool, but the way that you've explained it makes it really clear that there would be a lot of utility to using it.

00:54:24.840 --> 00:54:25.180
Yeah.

00:54:25.180 --> 00:54:25.820
Let's see.

00:54:25.820 --> 00:54:27.720
There's probably a couple other ones in here.

00:54:27.720 --> 00:54:29.420
There was one right back here.

00:54:29.420 --> 00:54:32.980
It was called a pytest Picked.

00:54:32.980 --> 00:54:34.840
And I don't really know how I feel about this.

00:54:34.920 --> 00:54:41.540
I don't know if it's precise enough, but you were talking, Greg, about winnowing down the set of tests you were running using coverage.

00:54:41.540 --> 00:54:47.340
And this one says it runs test related to changes detected by version control, just unstaged files.

00:54:47.340 --> 00:54:51.300
I feel like this is a really cool idea, but it does it in the wrong order.

00:54:51.300 --> 00:54:56.920
I feel like it's looking at just the test files that are not changed or that are not committed and rerunning those.

00:54:56.920 --> 00:54:59.100
It should look at the code covered.

00:54:59.100 --> 00:55:06.880
The changes, unstaged production files, and then use code covers to figure out which tests need to be rerun.

00:55:06.880 --> 00:55:07.520
Right.

00:55:07.520 --> 00:55:13.560
It's really cool to use the idea of having the source control tell you what the changes are since your last commit.

00:55:13.920 --> 00:55:17.080
But then this is just applying it to the test files, I think.

00:55:17.080 --> 00:55:20.980
But if it could say, well, now we use coverage to figure out these tests, that would be awesome.

00:55:21.160 --> 00:55:23.720
Yeah, and I regret that I can't remember the name of it.

00:55:23.720 --> 00:55:28.900
There is a tool that does a type of test suite reduction as a pytest plugin.

00:55:28.900 --> 00:55:33.280
And maybe I'll look it up after the show and we can include it in the show notes.

00:55:33.440 --> 00:55:47.160
Of course, the thing that you've got to be careful about is that there could be dependencies between program components that are not evidenced in the source code or the cover relationship, but maybe by access to external resources.

00:55:47.160 --> 00:55:51.360
And so in those cases, the selection process may not work as intended.

00:55:51.360 --> 00:55:51.720
Right.

00:55:51.720 --> 00:55:53.440
This thing changed something in the database.

00:55:53.440 --> 00:55:56.940
Some other part of the code read it, and that makes it crash.

00:55:56.940 --> 00:55:59.000
You didn't actually touch that code over there.

00:55:59.000 --> 00:55:59.680
Something like that.

00:55:59.680 --> 00:56:00.420
Yeah, absolutely.

00:56:00.420 --> 00:56:00.920
Yeah.

00:56:00.920 --> 00:56:01.500
Interesting.

00:56:02.060 --> 00:56:02.980
Maybe one more.

00:56:02.980 --> 00:56:13.380
I don't know too much about this, but Bell points out, says, I remember Anthony had worked on a tool to detect test pollution, which is a kind of related topic.

00:56:13.380 --> 00:56:20.660
And says a test pollution is where a test fails due to the side effects of some other test in the suite.

00:56:20.660 --> 00:56:22.000
That's pretty interesting.

00:56:22.000 --> 00:56:24.940
So maybe that's something for people to look at.

00:56:24.940 --> 00:56:25.440
Have you heard of this?

00:56:25.440 --> 00:56:28.820
I haven't heard of this before, so I can't speak too much about it.

00:56:28.820 --> 00:56:34.820
I've not heard of this specific tool, but I've seen it done in Java as a Java project.

00:56:34.820 --> 00:56:40.240
And yeah, you can do it fairly successfully and you can go quite deep with it as well.

00:56:40.240 --> 00:56:43.640
I mean, it's hard to see exactly how this one works just based on the description.

00:56:43.640 --> 00:56:47.220
But I mean, I think there was an example where it had a global variable.

00:56:47.220 --> 00:56:50.540
But I mean, obviously that's quite a trivial example.

00:56:50.860 --> 00:56:54.500
But I mean, you can get state pollution in ways you really wouldn't expect it.

00:56:54.500 --> 00:57:03.680
So for example, I've seen a test where two tests that were dependent on each other were individual parameterizations of the same test.

00:57:03.680 --> 00:57:11.600
There was a dependency because in the parameterization decorator, someone had used a list object as an argument.

00:57:11.600 --> 00:57:14.800
So then and then in the test, they've modified that list.

00:57:14.800 --> 00:57:17.740
But then the list isn't then recreated for the next test.

00:57:17.740 --> 00:57:19.300
So then the next test gets.

00:57:19.300 --> 00:57:21.120
But that's not a global variable.

00:57:21.120 --> 00:57:24.560
That's just sort of created when that file is executed.

00:57:24.740 --> 00:57:28.320
A weird Python default value behavior.

00:57:28.320 --> 00:57:29.020
Yeah, that is.

00:57:29.020 --> 00:57:29.780
Yeah, it's quite.

00:57:29.780 --> 00:57:32.660
I've seen people complain about that quite a lot.

00:57:32.660 --> 00:57:39.960
So like when you have a function and you put a list or a mutable object as like a default argument, that's quite a common gotcha.

00:57:39.960 --> 00:57:41.340
So it's a similar kind of thing.

00:57:41.340 --> 00:57:41.560
Yeah.

00:57:41.560 --> 00:57:48.060
And it's if you run tools like I talked about LinkedIn, if you run tools like Ruff or others that will, you know, flicate those types of things.

00:57:48.060 --> 00:57:51.220
You know, many of them will warn you like, no, this is a bad idea.

00:57:51.220 --> 00:57:51.900
Don't do that.

00:57:52.220 --> 00:58:05.220
So maybe running, I would imagine running tools like Ruff and other linters that detect these issues might actually reduce the test flakiness by finding some of these, you know, anti patterns that you maybe didn't catch.

00:58:05.220 --> 00:58:06.180
Yeah, it may well do.

00:58:06.180 --> 00:58:06.460
Yeah.

00:58:06.460 --> 00:58:16.540
I think another thing that's important to note when we're talking about a linter like Ruff is that it's so fast to run that there's not really a big cost from a developer's perspective.

00:58:16.540 --> 00:58:17.860
Yeah, it's nearly instant.

00:58:17.860 --> 00:58:18.160
Yeah.

00:58:18.160 --> 00:58:20.020
Again, integrate it early.

00:58:20.020 --> 00:58:21.100
Use it regularly.

00:58:21.720 --> 00:58:22.740
Have it in your IDE.

00:58:22.740 --> 00:58:24.120
Use it in CI.

00:58:24.120 --> 00:58:33.540
And it's one of those things where it might help you to avoid certain coding practices that would ultimately lead to test flakiness creeping into your system.

00:58:33.540 --> 00:58:33.800
Yeah.

00:58:33.800 --> 00:58:34.580
Really good advice.

00:58:34.580 --> 00:58:36.580
It is so fast.

00:58:36.580 --> 00:58:42.360
I ran it against, like I said, 20,000 lines of Python and it went and just it looked like it didn't even do anything.

00:58:42.360 --> 00:58:45.020
I thought, oh, I didn't do it right because nothing happened.

00:58:45.420 --> 00:58:47.400
No, but it's so quick.

00:58:47.400 --> 00:58:52.040
And you can put it, there's plugins for both PyCharm and VS Code that'll just run it.

00:58:52.040 --> 00:58:59.080
And PyCharm even integrates it into its code fixes and all of its behaviors and just reformat code options and stuff.

00:58:59.080 --> 00:58:59.660
It's really good.

00:58:59.820 --> 00:59:02.780
I've been using Ruff recently and I really like it as well.

00:59:02.780 --> 00:59:12.100
Along with the point that you mentioned, I like the fact that you can configure it through the PyProject Toml file, which is where I'm already putting all of my other configurations.

00:59:12.100 --> 00:59:12.640
Yeah.

00:59:12.740 --> 00:59:18.420
And then it also essentially can serve as a language server protocol implementation.

00:59:18.420 --> 00:59:25.460
So even if you don't use the two text editors that you mentioned, you can still get all of the code actions and fixes.

00:59:25.460 --> 00:59:30.580
And because it's so fast, it's really easy to use it even on big code basis.

00:59:30.580 --> 00:59:30.920
Okay.

00:59:30.920 --> 00:59:33.620
One more really quick, because I think this is a good suggestion.

00:59:33.620 --> 00:59:46.220
And this goes back to how I talked about, you know, like the call is coming from inside the house type of problem in that the error could actually be with the test framework and the test code, not just not actually your code.

00:59:46.220 --> 00:59:52.140
So Marwan points out that scoping fixtures incorrectly could be another source of flakiness.

00:59:52.140 --> 00:59:56.340
So the fixture could say, create a generator and pass it over to you.

00:59:56.400 --> 01:00:01.040
But you could say this is a class based one instead of a test instance one.

01:00:01.040 --> 01:00:05.300
And then you get different results depending on the order and all these kinds of things.

01:00:05.300 --> 01:00:05.480
Right.

01:00:05.480 --> 01:00:06.600
That's really interesting.

01:00:06.600 --> 01:00:07.040
I think.

01:00:07.040 --> 01:00:07.740
I would agree.

01:00:07.740 --> 01:00:09.300
I think that's a really good point.

01:00:09.300 --> 01:00:25.080
The other thing that I sometimes need to be very careful about is having auto use test fixtures inside of my code, because then those might be applied everywhere, along with other decorators that are fixtures that are just applied selectively.

01:00:25.600 --> 01:00:33.940
And then I might get a kind of non-determinant process just because of the way that various text fixtures are applied or the order in which they're applied.

01:00:33.940 --> 01:00:34.440
Absolutely.

01:00:34.440 --> 01:00:35.420
All right, Owen.

01:00:35.420 --> 01:00:36.500
Last word on this.

01:00:36.500 --> 01:00:42.120
The other plugin that might be interesting is pytest Xdist or running these distributed.

01:00:42.120 --> 01:00:44.880
Like, what do you think of how's that help or hurt us here?

01:00:44.880 --> 01:00:50.900
Finding your tests in parallel can be obviously a great way to expose concurrency related flakiness.

01:00:51.260 --> 01:01:00.420
Because as you said before, when you're writing the test, you might be writing it under the assumption that you're the only one accessing certain resources or running at a certain time.

01:01:00.420 --> 01:01:00.680
Yeah.

01:01:00.680 --> 01:01:10.680
Another thing that something like this can do as well is it can also expose order dependent tests because so the way it will work is it will create.

01:01:10.680 --> 01:01:14.120
Say you're wanting to run eight tests at a time.

01:01:14.120 --> 01:01:16.320
This plugin will then create 10.

01:01:16.320 --> 01:01:18.820
So you create eight separate processes.

01:01:19.020 --> 01:01:23.240
But within those processes, each one has its own independent Python interpreter.

01:01:23.240 --> 01:01:25.500
So they're running independently of each other.

01:01:25.500 --> 01:01:34.820
But then you could also, by doing that, expose a test case that was expecting a previous test to run, but now isn't because it's running in a different process.

01:01:34.820 --> 01:01:37.300
And that test could then go on to fail.

01:01:37.300 --> 01:01:41.360
So that would then be another issue of inadequate setup from that test.

01:01:41.360 --> 01:01:41.720
Yeah.

01:01:41.720 --> 01:01:44.300
This is something I should probably be running more of as well.

01:01:44.300 --> 01:01:45.240
Like, why not?

01:01:45.240 --> 01:01:47.100
I have 10 cores on this computer.

01:01:47.100 --> 01:01:48.960
Why don't I just have my test run faster?

01:01:48.960 --> 01:01:54.020
It's probably not 10 times faster, but it could do more than just running one thread in serial.

01:01:54.020 --> 01:01:54.960
You could do, yeah.

01:01:54.960 --> 01:02:01.680
But certainly running them in parallel would certainly pull up some of those ordering issues as well as resource contention issues.

01:02:01.680 --> 01:02:07.420
Yeah, so as well as providing a speed up, it's also great because it exposed some problems in your test suite as well.

01:02:07.420 --> 01:02:07.860
Absolutely.

01:02:07.860 --> 01:02:08.700
All right, guys.

01:02:08.700 --> 01:02:11.620
I think we're going to have to leave it there for the time we got.

01:02:11.620 --> 01:02:13.380
But excellent stuff.

01:02:13.380 --> 01:02:16.160
And there's a lot of detail here, isn't there, as you dig into it?

01:02:16.160 --> 01:02:20.940
Yeah, I think flaky tests are something that all of us as developers have encountered.

01:02:20.940 --> 01:02:24.220
We recognize that they limit us as developers.

01:02:24.600 --> 01:02:33.300
But also there's something that if we can automatically detect them or mitigate them in some way, we can remove that hassle from developers.

01:02:33.300 --> 01:02:46.000
So I think what we would like to do, both as researchers and developers, is allow people who write pytest test suites to be more productive and to write better tests that are less flaky.

01:02:46.000 --> 01:02:46.440
Excellent.

01:02:46.440 --> 01:02:52.360
All right, before we wrap up the show, I'll just ask you one quick question I usually do at the end.

01:02:52.360 --> 01:02:57.480
And that is you've got a flaky test related project on PyPI.

01:02:57.480 --> 01:03:00.060
Some library, some package you want to recommend to people.

01:03:00.060 --> 01:03:02.760
Or it could be something other than flaky related.

01:03:02.760 --> 01:03:05.520
But something you want to recommend, some package you've come across lately.

01:03:05.520 --> 01:03:10.100
I was actually going to recommend something that's not connected to flaky test cases.

01:03:10.100 --> 01:03:10.660
Go for it.

01:03:10.660 --> 01:03:18.780
So a lot of the work that I do involves various types of processing of the abstract syntax tree of a Python program.

01:03:18.920 --> 01:03:24.960
And so I thought I might first call out the AST package that's actually a part of Python.

01:03:24.960 --> 01:03:25.560
Go right in, right?

01:03:25.560 --> 01:03:25.920
Yeah.

01:03:25.920 --> 01:03:32.780
Which is built in and an incredibly useful tool, which isn't available in a lot of programming languages.

01:03:32.780 --> 01:03:43.180
The other two packages which are on PyPI, which I'll share about, is number one, libcst, which implements something that's called a concrete syntax tree.

01:03:43.180 --> 01:03:51.720
And it's a super useful tool when you want to be able to make changes to Python code or detect patterns in Python code.

01:03:51.720 --> 01:04:00.580
And you want to be able to fully preserve things like the blank space in the code and the comment strings in the code and things of that nature.

01:04:00.580 --> 01:04:06.060
Libcst is actually the foundation for another tool, which is called FixIt.

01:04:06.060 --> 01:04:14.620
And FixIt is a little bit like Ruff, except that it allows you to very easily write your own linting rules.

01:04:14.680 --> 01:04:26.140
And then finally, the last thing that I would share on this same theme, Michael, is that there's a really fun to use tool by someone who is a core member of the Django project.

01:04:26.140 --> 01:04:29.720
And it's called PyASTGrip.

01:04:29.720 --> 01:04:32.900
And it actually lets you write XPath expressions.

01:04:33.280 --> 01:04:41.700
And then you can use those XPath expressions to essentially query the abstract syntax tree of your Python program.

01:04:41.700 --> 01:04:41.980
Incredible.

01:04:41.980 --> 01:04:42.580
Okay.

01:04:42.580 --> 01:04:47.360
Looks like I guess syntax trees are a little bit like XML, aren't they?

01:04:47.360 --> 01:04:47.660
Okay.

01:04:47.660 --> 01:05:01.880
And if anybody has to do work where they're building an automated refactoring tool or they're building a new linting tool or various types of program analysis tools, the packages that I've mentioned might be very helpful.

01:05:01.880 --> 01:05:02.260
Thank you.

01:05:02.320 --> 01:05:03.240
That was a bunch of good ones.

01:05:03.240 --> 01:05:04.980
Owen, you got anything you want to give a shout out to?

01:05:04.980 --> 01:05:09.220
Well, it's actually a bit spooky because I was also about to recommend Libcst as well.

01:05:09.220 --> 01:05:15.080
So one small library I've used a few times is Radon.

01:05:15.080 --> 01:05:18.680
So that's R-A-D-O-N, I believe it's spelled.

01:05:18.680 --> 01:05:23.320
So this will basically calculate a load of code metrics for you.

01:05:23.320 --> 01:05:23.940
Oh, nice.

01:05:23.940 --> 01:05:24.220
Okay.

01:05:24.340 --> 01:05:31.120
So these are from relatively simple things like number of lines while taking into account comments and that kind of things.

01:05:31.120 --> 01:05:32.940
To more complex metrics.

01:05:32.940 --> 01:05:38.820
So there's this maintainability index, which is basically like a weighted sum of a bunch of other code metrics.

01:05:39.000 --> 01:05:39.800
I really like that one.

01:05:39.800 --> 01:05:40.240
Yeah.

01:05:40.240 --> 01:05:46.140
It's like it combines and says, well, it's like complexity is this, line length is that, function length, like all that kind of stuff.

01:05:46.140 --> 01:05:46.320
Right.

01:05:46.320 --> 01:05:58.380
And I've actually found sort of empirically there is appears to be some correlation in some cases to between having a high soil, having a poor maintainability to having very complex code.

01:05:58.540 --> 01:06:02.720
Having very complex test case code and that test case actually being flaky, which is interesting.

01:06:02.720 --> 01:06:03.620
Yeah, I can believe it.

01:06:03.620 --> 01:06:03.940
Okay.

01:06:03.940 --> 01:06:05.980
That's also a cool, I hadn't heard of Radon.

01:06:05.980 --> 01:06:06.680
That's neat.

01:06:06.680 --> 01:06:07.280
All right, guys.

01:06:07.280 --> 01:06:08.780
Thank you for being on the show.

01:06:08.780 --> 01:06:11.040
It's been a super interesting conversation.

01:06:11.040 --> 01:06:12.180
Final call to action.

01:06:12.180 --> 01:06:18.040
People either have flaky tests and want to get out of them or they want to avoid having them in the first place.

01:06:18.040 --> 01:06:19.620
What do you tell them?

01:06:19.620 --> 01:06:20.680
What are your parting thoughts?

01:06:20.680 --> 01:06:22.940
So my quick parting thought is as follows.

01:06:22.940 --> 01:06:30.400
We'll have some links in the show notes to various papers and tools that Owen and our colleagues and I have developed.

01:06:30.400 --> 01:06:32.640
And we hope that people will try them out.

01:06:32.640 --> 01:06:39.760
It would also be awesome if people can get in contact with us and share some of their flaky test case war stories.

01:06:39.760 --> 01:06:46.560
We would love to learn from you and partner with you to help you solve some of the flaky test case challenges that you have.

01:06:46.560 --> 01:06:48.000
Owen, what else do you want to add?

01:06:48.000 --> 01:06:49.240
I think that's pretty much it for me.

01:06:49.240 --> 01:06:53.700
I'd say probably the most important thing to do would just be just stick with testing.

01:06:53.700 --> 01:07:00.000
Don't let flaky tests put you off test-driven development or anything like that because it's better than not testing.

01:07:00.000 --> 01:07:00.540
Yeah, indeed.

01:07:00.540 --> 01:07:00.980
All right.

01:07:00.980 --> 01:07:01.600
Well, thanks, guys.

01:07:01.600 --> 01:07:02.360
Thanks for being on the show.

01:07:02.360 --> 01:07:03.020
Thank you.

01:07:03.020 --> 01:07:03.420
Thank you.

01:07:03.420 --> 01:07:07.220
This has been another episode of Talk Python To Me.

01:07:07.220 --> 01:07:09.020
Thank you to our sponsors.

01:07:09.020 --> 01:07:10.640
Be sure to check out what they're offering.

01:07:10.640 --> 01:07:12.060
It really helps support the show.

01:07:12.060 --> 01:07:16.800
The folks over at JetBrains encourage you to get work done with PyCharm.

01:07:17.120 --> 01:07:22.360
PyCharm Professional understands complex projects across multiple languages and technologies,

01:07:22.360 --> 01:07:28.000
so you can stay productive while you're writing Python code and other code like HTML or SQL.

01:07:28.000 --> 01:07:33.140
Download your free trial at talkpython.fm/done with PyCharm.

01:07:33.580 --> 01:07:35.480
Take some stress out of your life.

01:07:35.480 --> 01:07:41.260
Get notified immediately about errors and performance issues in your web or mobile applications with Sentry.

01:07:41.260 --> 01:07:46.260
Just visit talkpython.fm/sentry and get started for free.

01:07:46.260 --> 01:07:49.840
And be sure to use the promo code TALKPYTHON, all one word.

01:07:49.840 --> 01:07:51.540
Want to level up your Python?

01:07:51.960 --> 01:07:55.600
We have one of the largest catalogs of Python video courses over at Talk Python.

01:07:55.600 --> 01:08:00.780
Our content ranges from true beginners to deeply advanced topics like memory and async.

01:08:00.780 --> 01:08:03.440
And best of all, there's not a subscription in sight.

01:08:03.440 --> 01:08:06.340
Check it out for yourself at training.talkpython.fm.

01:08:06.340 --> 01:08:08.260
Be sure to subscribe to the show.

01:08:08.260 --> 01:08:11.040
Open your favorite podcast app and search for Python.

01:08:11.040 --> 01:08:12.340
We should be right at the top.

01:08:12.340 --> 01:08:15.300
You can also find the iTunes feed at /itunes,

01:08:15.300 --> 01:08:17.500
the Google Play feed at /play,

01:08:17.500 --> 01:08:21.700
and the direct RSS feed at /rss on talkpython.fm.

01:08:21.700 --> 01:08:25.140
We're live streaming most of our recordings these days.

01:08:25.140 --> 01:08:28.540
If you want to be part of the show and have your comments featured on the air,

01:08:28.540 --> 01:08:32.920
be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

01:08:32.920 --> 01:08:34.820
This is your host, Michael Kennedy.

01:08:34.820 --> 01:08:36.100
Thanks so much for listening.

01:08:36.100 --> 01:08:37.280
I really appreciate it.

01:08:37.280 --> 01:08:39.180
Now get out there and write some Python code.

01:08:39.180 --> 01:08:59.940
I'll see you next time.

