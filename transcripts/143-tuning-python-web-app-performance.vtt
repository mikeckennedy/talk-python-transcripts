WEBVTT

00:00:00.001 --> 00:00:04.120
Do you run a web application or web service? You probably do a couple things to optimize the

00:00:04.120 --> 00:00:09.160
performance of your site. You make sure that the database responds quickly and more. But did you

00:00:09.160 --> 00:00:13.640
know a well of performance improvements actually lives inside your web servers themselves?

00:00:13.640 --> 00:00:19.380
Join Ben Kane and me to discuss how to optimize your Python web application as well as

00:00:19.380 --> 00:00:26.600
uWSGI and Nginx. This is Talk Python to Me, episode 143, recorded December 11, 2017.

00:00:26.600 --> 00:00:45.860
Welcome to Talk Python to Me, a weekly podcast on Python, the language, the libraries, the

00:00:45.860 --> 00:00:50.660
ecosystem, and the personalities. This is your host, Michael Kennedy. Follow me on Twitter where

00:00:50.660 --> 00:00:56.280
I'm at mkennedy. Keep up with the show and listen to past episodes at talkpython.fm and follow the

00:00:56.280 --> 00:01:03.580
show on Twitter via at talkpython. This episode has been sponsored by Rollbar and GoCD. Thank them

00:01:03.580 --> 00:01:08.260
both for supporting the podcast by checking out what they're offering during their segments.

00:01:08.260 --> 00:01:12.880
Hey everyone, before we get to the interview, I want to share a quick update about our Python

00:01:12.880 --> 00:01:16.980
courses with you. Do you work on a software team that needs training and could really use

00:01:16.980 --> 00:01:21.280
a chance to level up their Python? Maybe your entire company is looking to become more proficient.

00:01:21.280 --> 00:01:26.540
We have special offers that make our courses here at Talk Python the best option for everyone

00:01:26.540 --> 00:01:30.640
you work with. Our courses don't require an ongoing subscription like so many corporate

00:01:30.640 --> 00:01:36.320
training options do. And they're roughly priced about the same as a book. We're here to help

00:01:36.320 --> 00:01:42.100
you succeed. Send us a note at sales at talkpython.fm to start a conversation. Now let's get to the

00:01:42.100 --> 00:01:44.380
interview. Ben, welcome to Talk Python.

00:01:44.380 --> 00:01:46.320
Hey, thanks, Michael. Thanks for inviting me.

00:01:46.400 --> 00:01:51.880
It's great to have you here. I love I'm just a sucker for a good performance talk. So I'm really

00:01:51.880 --> 00:01:57.500
excited to talk about all the different layers of Python web apps and performance tuning, mostly

00:01:57.500 --> 00:02:04.860
outside of the Python code itself, right? Yeah, well, I would say one of my my keys is you have to tune

00:02:04.860 --> 00:02:10.640
pretty much everything, the entire stack. So Python code, absolutely. But it's not just that. And

00:02:10.640 --> 00:02:15.600
that's, that's a real important thing to remember is, it's everything, because everything matters.

00:02:15.600 --> 00:02:20.100
Yeah, absolutely. So it's gonna be great to dig into that. But before we do, let's get to your story.

00:02:20.100 --> 00:02:21.360
How'd you get into programming in Python?

00:02:21.360 --> 00:02:28.980
In the 90s, when I was a teenager, I was making anime fan sites, which is fun, just for fun, you know,

00:02:28.980 --> 00:02:35.760
using HTML, CSS, and I wanted to really kick them up a notch. So I learned PHP. And that was,

00:02:35.760 --> 00:02:43.620
you know, kind of the web framework at the time. And I did a little bit of that for fun, mostly some for

00:02:43.620 --> 00:02:50.240
money. But it's funny, in my adult life, I didn't actually make a career out of it, I went into like

00:02:50.240 --> 00:02:56.840
retail. And then it wasn't until a friend said, dude, what are you doing? You can make these awesome

00:02:56.840 --> 00:03:00.240
sites. Why don't you make a career out of this? And I kind of thought about it. I was like, Oh, he's

00:03:00.240 --> 00:03:02.620
right. I should make some money out of this.

00:03:03.140 --> 00:03:04.820
This is way more fun than what I'm doing.

00:03:04.820 --> 00:03:08.120
Exactly, exactly. So fast forward, and here I am.

00:03:08.120 --> 00:03:12.500
Yeah, that's awesome. Cool. And what are you doing day to day these days?

00:03:12.500 --> 00:03:17.840
I'm a staff engineer at American Express. That is, is that's essentially the payment network

00:03:17.840 --> 00:03:23.140
for American Express. So the way I like to summarize it is it's it's really the yes,

00:03:23.140 --> 00:03:29.340
no machines. So when you swipe your card, that transaction has to get somewhere and that's

00:03:29.340 --> 00:03:34.140
somewhere to say yes and no all those systems that it goes through in order to get to that yes and no.

00:03:34.140 --> 00:03:36.000
That's essentially what I work on every day.

00:03:36.000 --> 00:03:41.400
Wow. So a lot of services, a lot of low latency demands, right? The person is standing there with

00:03:41.400 --> 00:03:47.180
their card, like, yeah, you know, oddly, uncomfortably looking at the cashier who's also

00:03:47.180 --> 00:03:51.240
uncomfortably trying not to look at them, just waiting for your systems, right?

00:03:51.580 --> 00:03:55.900
Yeah, exactly. And a whole bunch of them too, right? Because it's not just that one person.

00:03:55.900 --> 00:03:58.800
It's millions of people around the world.

00:03:58.800 --> 00:04:02.240
Yeah. When you think of the number of people are shopping and swiping cards and just using

00:04:02.240 --> 00:04:04.800
payment at any given moment, it's huge, right?

00:04:04.800 --> 00:04:09.260
Yeah. And then when you add things like Black Friday, Cyber Monday, all those big shopping

00:04:09.260 --> 00:04:12.060
holidays, it's it's huge, right? You have to...

00:04:12.060 --> 00:04:13.360
Does that make a big dent for you guys?

00:04:13.360 --> 00:04:16.400
Oh, yeah. The more people are shopping, the more they're swiping the card,

00:04:16.400 --> 00:04:19.940
the more our systems have to be there to say the yes and the no.

00:04:19.940 --> 00:04:23.340
A lot more yes, no questions, huh?

00:04:23.340 --> 00:04:23.840
Exactly.

00:04:23.840 --> 00:04:31.080
Yeah, nice. So when you talked about the PHP stuff, I kind of distracted you from the Python

00:04:31.080 --> 00:04:31.680
part of it.

00:04:31.680 --> 00:04:38.680
With Python, I was actually working at a web hosting company as a sysadmin in about 2005.

00:04:38.680 --> 00:04:44.720
Back then, Perl was really the language of choice for sysadmins. And my buddy, Marcel,

00:04:44.720 --> 00:04:49.780
introduced me to this new language called Python. And from then, it was like love at first sight.

00:04:49.780 --> 00:04:55.200
Like I just, I loved it. I love the syntax of it. I loved how easy it was to work with.

00:04:55.200 --> 00:05:02.180
But then, of course, I moved to a Perl only sysadmin shop. So I was the only one who knew

00:05:02.180 --> 00:05:06.420
Python. Everyone else knew Perl. So I was kind of like, all right, well, I guess I'll just write

00:05:06.420 --> 00:05:12.740
stuff in Perl if I have to. I was really a big fan of Python at the time. So I really didn't do much

00:05:12.740 --> 00:05:21.640
with it then. And then about 2013, 2014, I realized, I was at Amex at that time, I realized that the

00:05:21.640 --> 00:05:27.820
traditional sysadmin role was kind of dying off. And I would say at this point, it's pretty much dead.

00:05:27.820 --> 00:05:32.640
It's just not everyone knows it yet. Yeah, I think it's that much of a change lately.

00:05:32.860 --> 00:05:37.260
Yeah. So like DevOps and stuff like that, it's kind of replacing it Ansible and SaltStack.

00:05:37.260 --> 00:05:42.220
Oh, yeah, exactly. And if you want to stay relevant, you know, I figured you got to learn

00:05:42.220 --> 00:05:49.440
how to program. And that's when I really buckled down and learned how to program again, above like

00:05:49.440 --> 00:05:54.540
scripts, right? I've been writing scripts forever, but more than just a script. I mean, it's a big

00:05:54.540 --> 00:05:58.720
difference between the two. And of course, Python was my language of choice to do it.

00:05:58.720 --> 00:06:04.940
That's really cool. So how did you get interested in performance tuning and that kind of stuff?

00:06:04.940 --> 00:06:10.540
It's really part of my job. With things being the authorizations and that yes and no, like you said,

00:06:10.540 --> 00:06:15.360
people standing at a terminal waiting for an answer. You know, we get a lot of requests,

00:06:15.360 --> 00:06:22.000
and these requests have to be very, very fast. So one of the things that we're very acutely aware of

00:06:22.000 --> 00:06:30.380
is performance. And we actually hold ourselves to certain performance benchmarks. And we constantly

00:06:30.380 --> 00:06:35.160
test, do performance testing during the development cycle, just to make sure that we're meeting those

00:06:35.160 --> 00:06:35.560
benchmarks.

00:06:35.560 --> 00:06:42.180
Do you have like performance requirements or measures in say an automated build or anything like that?

00:06:42.180 --> 00:06:49.380
Yeah, but that's more for developer satisfaction, right? But for us, it's more about how fast those

00:06:49.380 --> 00:06:54.380
transactions get processed. When I think performance tuning, that's my immediate thing.

00:06:54.380 --> 00:07:00.560
Of course, everything else that goes along with it is important. But to me, like that's the thing that

00:07:00.560 --> 00:07:07.300
shines so bright in that. The cool thing is you can take what you learn from that and apply it to all

00:07:07.300 --> 00:07:08.600
sorts of different tools.

00:07:08.600 --> 00:07:14.880
Yeah, absolutely. And as a sysadmin DevOps person, you see these as the whole system,

00:07:14.880 --> 00:07:21.700
right? Like Linux plus the server servers and services plus the app. How does that thing work?

00:07:21.700 --> 00:07:27.000
Right. Or maybe even across, right? Like, how does the front end web servers plus the load balancer plus

00:07:27.000 --> 00:07:31.240
the back end services? How does that thing perform? It's really more what you might want to

00:07:31.240 --> 00:07:33.320
have like the final measure be, right?

00:07:33.320 --> 00:07:39.920
It all matters, right? So you have to really look at the whole picture in order to see how things

00:07:39.920 --> 00:07:43.760
interact or how things change the performance of a system.

00:07:43.760 --> 00:07:48.120
Absolutely. So one thing I kind of wanted to touch on just a little bit at the beginning here is there's

00:07:48.120 --> 00:07:53.580
a little bit of a difference between scalability and straight performance, right?

00:07:53.780 --> 00:08:00.240
If you kind of think about scalability and performance, a lot of times they're really two

00:08:00.240 --> 00:08:06.520
separate problems, but I do think they're very closely related. With scalability or performance,

00:08:06.520 --> 00:08:13.120
you might say like one request versus a million requests. And with performance, it's not just

00:08:13.120 --> 00:08:18.060
necessarily one request, right? You might also have a whole bunch of requests. Maybe it's not a million,

00:08:18.060 --> 00:08:25.160
maybe it is in one instance of an application. And you need to be able to see how many concurrent

00:08:25.160 --> 00:08:28.980
requests can I handle? Because that impacts performance overall.

00:08:28.980 --> 00:08:33.680
Yeah. One of the things I think is kind of funny to think about is you could have an app that responds

00:08:33.680 --> 00:08:39.540
like it takes 10 seconds to process a request, which sounds like performance sucks. But maybe if you throw

00:08:39.540 --> 00:08:44.720
a million requests at it and it only goes up to 11 seconds per request, that's a pretty scalable app.

00:08:44.720 --> 00:08:46.480
It just doesn't perform very well, right?

00:08:46.480 --> 00:08:51.360
Yeah, exactly. You got to make that thing faster, right? But scalability is also,

00:08:51.360 --> 00:08:57.540
you can talk about scale up and scale out too, right? You can scale out the number of instances. So,

00:08:57.540 --> 00:09:04.180
you know, if let's say your application can only handle a couple of thousand requests at a time,

00:09:04.180 --> 00:09:10.240
you know, by adding additional instances, you can add that many number of requests at a time. But then

00:09:10.240 --> 00:09:14.960
there's other trade-offs with that too. You add complexity to the application, which that's a

00:09:14.960 --> 00:09:21.100
trade-off not only in performance and scalability, but also with availability. And that gets a little

00:09:21.100 --> 00:09:21.920
tricky as well.

00:09:21.920 --> 00:09:28.120
Yeah. And some people can get away with like very small amounts of lack of uptime and availability.

00:09:28.120 --> 00:09:30.140
I suspect you guys not so much.

00:09:30.140 --> 00:09:31.080
Yeah, not at all.

00:09:31.080 --> 00:09:33.440
It's frowned upon to have the yes, no machine down.

00:09:33.580 --> 00:09:39.560
I don't know why. I mean, yeah, no, it's highly frowned upon to have that down. We go through

00:09:39.560 --> 00:09:42.700
a lot of effort just to make sure that it's up all the time.

00:09:42.700 --> 00:09:47.660
Yeah, I'm sure. But it totally adds more complexity, which we'll talk about. So,

00:09:47.660 --> 00:09:53.200
I guess one of the things to think about is the difference between performance testing and

00:09:53.200 --> 00:09:55.980
performance tuning. Can you maybe compare those for us?

00:09:56.140 --> 00:09:59.820
Performance testing, at least in my mind, although I think many would agree with me,

00:09:59.820 --> 00:10:05.200
performance testing is really just the execution of tests that measure performance.

00:10:05.200 --> 00:10:12.280
Performance tuning is more of a concerted effort to improve performance. So, you know,

00:10:12.280 --> 00:10:17.760
to give an example, in our development process, we have automated performance tests that run

00:10:17.760 --> 00:10:24.940
all the time. We know whether that benchmark is being met or not. But really, performance tuning

00:10:24.940 --> 00:10:32.320
is adjusting that benchmark. Is that benchmark now a higher benchmark? Does it need to necessarily go

00:10:32.320 --> 00:10:37.020
lower? Although generally, we never go backwards, we always go forwards. You only go backwards if you

00:10:37.020 --> 00:10:37.660
really have to.

00:10:37.660 --> 00:10:42.260
Right. Maybe you add some major feature and it's worth it, but it doesn't actually,

00:10:42.260 --> 00:10:44.100
it does make it go slower because it's doing more or something.

00:10:44.300 --> 00:10:48.940
Yeah. All of these things are about trade-offs. You gain in one area, but you trade in another

00:10:48.940 --> 00:10:55.840
area. So, yeah, it's very important to kind of know the difference. Really, the concerted effort

00:10:55.840 --> 00:11:01.440
with performance tuning, that's important. If your goal is to really squeeze every microsecond out of

00:11:01.440 --> 00:11:07.340
an application, it's important to have that concerted effort. And it's not just about what's the measurement

00:11:07.340 --> 00:11:08.520
tool telling us.

00:11:08.520 --> 00:11:12.280
And if you have graphs or some other kind of reporting, it's really nice to actually go,

00:11:12.280 --> 00:11:17.500
wait a minute, when we deployed that new version yesterday, what's the response time now than what

00:11:17.500 --> 00:11:19.780
it was before or memory usage now or whatever?

00:11:19.780 --> 00:11:26.420
Yeah. Measurement from production is huge, right? Because you can run all the tests you want in

00:11:26.420 --> 00:11:32.800
kind of pre-production environments, but production is where things get crazy. And if you start seeing

00:11:32.800 --> 00:11:38.020
differences in performance there, it really gives you an indication of where you need to start looking

00:11:38.020 --> 00:11:44.680
as well. And it's a really good idea to just measure both. Measure your pre-production

00:11:44.680 --> 00:11:48.740
environments, compare that with your production environments, and see where the differences lie

00:11:48.740 --> 00:11:50.180
and why they are different.

00:11:50.180 --> 00:11:53.560
Yeah, absolutely. Production, that's where reality lives, right?

00:11:53.560 --> 00:11:56.360
Exactly. That's what really matters at the end of the day.

00:11:56.360 --> 00:12:01.180
Exactly. You're building this stuff to run it for millions of people in production. It doesn't matter

00:12:01.180 --> 00:12:04.240
what your tests say. Like that's the final arbiter of how it is.

00:12:04.240 --> 00:12:04.540
Yeah.

00:12:04.540 --> 00:12:10.800
Yeah. So before we get farther into it, let's maybe take a moment and just talk about what a typical

00:12:10.800 --> 00:12:13.800
web stack in Python looks like.

00:12:13.800 --> 00:12:19.820
Really, if you kind of start at the top, you have something like Ngenix, which is a web server,

00:12:19.820 --> 00:12:25.100
and I'm actually skipping a whole bunch of layers, but we'll kind of talk from the web server down.

00:12:25.180 --> 00:12:29.280
In one web server, maybe we're not talking about the scaled out architecture with all the machines

00:12:29.280 --> 00:12:30.180
working together, right?

00:12:30.180 --> 00:12:37.180
Exactly. Exactly. So in your typical one stack kind of approach, you have your web server,

00:12:37.180 --> 00:12:44.500
you have things like Ngenix, Apache, there's several others. Ngenix, one being known for being very

00:12:44.500 --> 00:12:49.560
performant out of the box. And really what those do is they serve those HTTPS requests,

00:12:49.860 --> 00:12:55.020
they serve kind of the static content. And you can even do some like caching with them as well,

00:12:55.020 --> 00:13:01.060
which is interesting. And then you go to like your application server, you have UWSGI,

00:13:01.060 --> 00:13:08.500
G Unicorn, things like that. And those are really there for being the worker processes for your

00:13:08.500 --> 00:13:12.740
running application. So they'll start the application, they'll manage the application,

00:13:12.740 --> 00:13:19.120
make sure it's running, and really make sure there's enough workers of that application to handle

00:13:19.120 --> 00:13:24.540
those requests. And then, of course, you have your app framework as well, your web app framework. So

00:13:24.540 --> 00:13:31.320
Flask, Web2Py, and then Pyramid, Django, there's a whole bunch of those. All of them are kind of a

00:13:31.320 --> 00:13:36.640
little bit different. Some have different areas of expertise, and some have more features, some have

00:13:36.640 --> 00:13:43.340
less features. One of the interesting things with performance, if performance is a big factor for you,

00:13:43.560 --> 00:13:49.060
one of the kind of caveats, or one of the more golden rules I have is, the less features, the more

00:13:49.060 --> 00:13:53.820
performant it's going to be. That's not always true, but it's a good general rule of thumb, at least.

00:13:53.820 --> 00:13:57.820
Yeah, the less you're doing, the more you can do of it. Yeah, that's for sure.

00:13:57.980 --> 00:14:02.960
And then, you also have the database, and that's another whole factor to think about, and whether

00:14:02.960 --> 00:14:08.820
it's SQL, no SQL. Not every web app is going to have that, but a good chunk of them will.

00:14:08.820 --> 00:14:13.840
Yeah, most of them will have some kind of data store, and usually the choice of the database. You

00:14:13.840 --> 00:14:18.620
actually have some really interesting things to say about that. We'll talk about that. One thing I do

00:14:18.620 --> 00:14:22.720
want to maybe take a step back and talk about, because I think it's important to understand for

00:14:22.720 --> 00:14:30.200
people who don't live in web deployments all the time, is you talked about two web servers. You

00:14:30.200 --> 00:14:35.820
talked about Nginx, and you talked about Microwiskey or UWSGI, and why do we need two?

00:14:35.820 --> 00:14:45.920
That's actually kind of important, and I think I will simplify it with Nginx is really good at what

00:14:45.920 --> 00:14:54.280
it does. It's really good handling some proxying. Let's take an example of HTTPS. In order to do that

00:14:54.280 --> 00:15:02.060
SSL handling, right, the SSL handshakes, the decryption, all of that, leveraging Nginx for that

00:15:02.060 --> 00:15:09.520
is very fast. It's good at that. It does that very well, and it does that very fast, and it's tuned

00:15:09.520 --> 00:15:17.220
specifically for that type of task. It also is really good for serving static content. So if you

00:15:17.220 --> 00:15:25.080
take an application server like UWSGI, that's running your Python web app. Well, if you have static content

00:15:25.080 --> 00:15:31.660
with that, offload that kind of workload to Nginx. Let Nginx do the static content, because that's very

00:15:31.660 --> 00:15:36.140
static. It doesn't need to talk to your Python app. If there's no need, then don't do it. But

00:15:36.140 --> 00:15:45.200
UWSGI is more for executing the requests across your Python application as well, and that's really

00:15:45.200 --> 00:15:52.160
kind of your worker process. Now, it all kind of, there's many ways to set up this type of stuff. You can

00:15:52.160 --> 00:15:57.140
do things in many different ways. Some things work better. Some setups work better for certain

00:15:57.140 --> 00:16:02.680
environments, but your typical deployment is going to have kind of all three, web server, the application

00:16:02.680 --> 00:16:08.180
server, and then that web app framework as well. Yeah. And one of the things, I don't know how much

00:16:08.180 --> 00:16:15.520
micro-whiskey suffers from this, but certainly the Python web servers themselves, the pure Python ones,

00:16:15.520 --> 00:16:22.380
can suffer from the fact that you only have the global interpreter log, the GIL. So you can really only do so

00:16:22.380 --> 00:16:30.680
much serving on any one thread at a time. And if you're busy serving up like a large image file,

00:16:30.680 --> 00:16:37.000
you know, you're not processing requests, right? Like, so putting something in there to like offload

00:16:37.000 --> 00:16:43.180
everything except for the true application requests like Nginx is, I find this pretty awesome.

00:16:43.180 --> 00:16:49.260
That's what Nginx is good at. So let it do its job. Yeah. And it can be like a load balancer or proxy

00:16:49.260 --> 00:16:54.600
server. It's really quite advanced what you can do with Nginx. You know, you mentioned run it all in

00:16:54.600 --> 00:17:02.320
kind of one server, one kind of instance, but that's exactly right. You can put Nginx up one level and

00:17:02.320 --> 00:17:08.580
have it do the load balancing across multiple backend applications. And that's really powerful as well.

00:17:08.580 --> 00:17:13.220
Yeah, that's awesome. Another thing that I do at the Nginx level, at least on my site,

00:17:13.660 --> 00:17:20.120
is that's where all the SSL exchange happens, right? Beyond that, like the micro whiskey stuff,

00:17:20.120 --> 00:17:24.520
it doesn't even know that it's encrypted. Well, I guess because it's not, but it's in the data center,

00:17:24.520 --> 00:17:31.620
right? Exactly. Although even today that sometimes you're starting to see even that good, that layer

00:17:31.620 --> 00:17:36.460
get encrypted as well. You know, things change over time. Yeah, I can see in the more machines you

00:17:36.460 --> 00:17:41.360
involve, the more encrypted it is. I guess in my setup, I have micro whiskey and Nginx on the same

00:17:41.360 --> 00:17:45.860
machine. So it's like a loop back. So encryption doesn't make as much sense.

00:17:45.860 --> 00:17:51.840
This portion of Talk Python to Me has been brought to you by Rollbar. One of the frustrating things

00:17:51.840 --> 00:17:57.160
about being a developer is dealing with errors, relying on users to report errors, digging through

00:17:57.160 --> 00:18:02.200
log files, trying to debug issues, or getting millions of alerts just flooding your inbox and

00:18:02.200 --> 00:18:07.460
ruining your day. With Rollbar's full stack error monitoring, you get the context, insight and control

00:18:07.460 --> 00:18:13.580
you need to find and fix bugs faster. Adding Rollbar to your Python app is as easy as pip install Rollbar.

00:18:13.580 --> 00:18:17.820
You can start tracking production errors and deployments in eight minutes or less.

00:18:17.820 --> 00:18:23.240
Are you considering self hosting tools for security or compliance reasons? Then you should really check

00:18:23.240 --> 00:18:28.980
out Rollbar's compliant SaaS option. Get advanced security features and meet compliance without the

00:18:28.980 --> 00:18:36.320
hassle of self hosting, including HIPAA, ISO 27001, Privacy Shield and more. They'd love to give you a demo.

00:18:36.320 --> 00:18:41.700
Give Rollbar a try today. Go to talkpython.fm/Rollbar and check them out.

00:18:41.700 --> 00:18:50.700
If you look at like UWSGI, that one in particular is good chunks of it are written in C, right? And that can

00:18:50.700 --> 00:18:58.200
also help with performance because, you know, it's C. And C is very fast. C is pre-compiled. It's got performance

00:18:58.200 --> 00:19:04.080
in its nature, right? So being able to leverage that is also very useful. And Ngenix is also

00:19:04.080 --> 00:19:09.700
written in C. And there's kind of that, you know, you use your image example. That's a really good

00:19:09.700 --> 00:19:17.620
example, right? That's where you can leverage that aspect of Ngenix to really get that boost

00:19:17.620 --> 00:19:18.260
of performance.

00:19:18.460 --> 00:19:22.580
Right. The threading and parallelism, that's all just, all runs over there in C. And I'm,

00:19:22.580 --> 00:19:24.300
you know, I'm glad I don't have to maintain that.

00:19:24.300 --> 00:19:26.000
Yeah, I would agree with you on that one.

00:19:26.260 --> 00:19:30.600
Let's start by thinking about how you might approach performance tuning. Like, I've got

00:19:30.600 --> 00:19:36.940
an app. It's kind of working pretty well, but certainly it could be better maybe under times

00:19:36.940 --> 00:19:41.820
of load. It's like too slow. Or I'm just thinking, you know, 300 millisecond response time is fine,

00:19:41.820 --> 00:19:46.180
but could we do 25 instead? But how do you think about this tuning problem?

00:19:46.180 --> 00:19:52.640
I like to think of performance tuning as if it's a science experiment. So first step is put

00:19:52.640 --> 00:19:58.100
on a lab coat. And then kind of after you got your lab coat established, really, you know,

00:19:58.100 --> 00:20:04.020
kind of start with the observation. Now, I would say one of the keys here and kind of the next step,

00:20:04.020 --> 00:20:09.280
which is creating questions, you know, with your observations and your questions, it's really good

00:20:09.280 --> 00:20:15.680
to have as many perspectives as possible. With you mentioned kind of the Linux stack, you have web

00:20:15.680 --> 00:20:21.580
servers, you have application servers, you have the actual Python code itself. Many times in many

00:20:21.580 --> 00:20:27.260
areas, you know, some of these things are managed by different people and bringing those people in to

00:20:27.260 --> 00:20:33.580
kind of add their input into observations and what kind of questions can be asked, you know,

00:20:33.580 --> 00:20:39.340
what kind of knobs can be turned, you really start to get multiple perspectives. And that's where things

00:20:39.340 --> 00:20:46.240
get very interesting. Now, with the same thing with science experiments is you only want to change one

00:20:46.240 --> 00:20:52.100
thing. And then kind of that's important as well. So as you're testing and you're validating,

00:20:52.100 --> 00:20:59.500
and you're kind of adjusting as necessary, only making one change at a time is very important,

00:20:59.500 --> 00:21:04.120
because otherwise, if you make too many changes at a time, and this is a real common mistake, I see,

00:21:04.120 --> 00:21:09.840
if you make too many changes at a time, you get a difference, but you don't know which thing caused

00:21:09.840 --> 00:21:14.480
that difference. And sometimes that leads you down like a rabbit hole, right? You start chasing something

00:21:14.480 --> 00:21:18.620
that you thought made a big difference. But in reality, it was something completely different.

00:21:18.620 --> 00:21:21.040
And that's really important.

00:21:21.040 --> 00:21:25.360
Yeah, or one change made it faster and one change made it slower, but you did them at the same time.

00:21:25.360 --> 00:21:26.740
So it looked like it had no effect.

00:21:26.740 --> 00:21:34.740
And another, another key piece is really establishing your baseline. And that's one thing I really talk a

00:21:34.740 --> 00:21:39.840
lot when I'm telling people about performance tuning is the first thing you do, the first thing you do

00:21:39.840 --> 00:21:47.280
before you make any changes is establish a baseline. And then you also establish a base between changes.

00:21:47.280 --> 00:21:51.980
So usually when you have big performance tuning effort, you're not just change one thing, and then

00:21:51.980 --> 00:21:55.740
everyone goes about their day, you want to change multiple things, you want to have some fun with it,

00:21:55.740 --> 00:21:59.660
you just want to see all the little knobs you can turn to make this thing go faster.

00:22:00.400 --> 00:22:27.120
So being able to stop and baseline between each kind of iteration is important. And also being able to go back to a previous state, it's important to test things individually and together in kind of separate tests. And it can take longer. And that's complicated. And people want to tend to want to rush through when they're first kind of getting started with performance tuning.

00:22:27.120 --> 00:22:31.800
And you just got to take your time. And it's key to really measure it very well.

00:22:31.800 --> 00:22:36.240
Well, and with these deployment stacks, or what do you want to call them, you know, you've got

00:22:36.240 --> 00:22:38.960
Nginx, and you can tune Nginx, you've got

00:22:38.960 --> 00:22:46.040
like with year, or G unicorn, or whatever, you can performance tune that. And you've got your Python code. And so measuring them

00:22:46.040 --> 00:22:50.540
separately, I think can be a little bit challenging. While you're talking, it occurred to me that

00:22:50.540 --> 00:22:58.100
it's pretty easy to measure Nginx directly, maybe against a static file. If you have your app running in micro

00:22:58.100 --> 00:23:03.820
whiskey, you could just start hitting it, you showed for both of those scenarios, you have a B Apache benchmark, right?

00:23:03.820 --> 00:23:33.800
An Apache benchmark is actually comes with like the Apache to utils package. And it's a very common benchmarking tool, I would say it's, it's got its own problems, you know, there's, there's some things it does really well, some things it doesn't do, but it's a good general purpose tool. Another one I'm a big fan of is a go bench. It's written in go. It's a little bit different. It approaches benchmarking, web requests a little bit different. In some cases, I've seen it faster. And that's actually an

00:23:33.800 --> 00:23:56.100
interesting problem. It's an interesting problem too, is these benchmarking tools are applications in themselves, and they're running in environments themselves. So often, you can run into situations when your application is tuned so well that your benchmarking tool is actually where your bottlenecks are. And that gets like into a real interesting problem.

00:23:56.280 --> 00:24:04.260
Yeah, so do you do you recommend running the benchmarking tools on a separate machine with like a super low latency connection, like in the same data center or something?

00:24:04.260 --> 00:24:27.200
Yeah, sometimes we've just daisy chain servers to get that, that low latency connection. But yeah, absolutely. If you can run it on a different machine. That's awesome. That's great. You should do that. Sometimes, though, in order to really cut out network latency, we've had to either run it on the same machine or like I said, daisy chain some servers so that they don't go through any switches on the way.

00:24:27.200 --> 00:24:57.180
Yeah, definitely. It's a complicated problem, right?

00:24:57.180 --> 00:25:10.640
That's a lot of things that happens when a request goes through your infrastructure. Because it's easy to go, yeah, this is the config file and I put it up and then it works. But, you know, knowing more about the actual steps before it hits your code is actually pretty interesting.

00:25:10.640 --> 00:25:39.640
Yeah, and that's important for 3am calls as well when you have a problem, right? So the more you know kind of about your application, some of the benefits to really performance tune your application is, you know that this is why it works. And this is how it works. So finding problems is a lot easier. But that also really plays into getting even more performance out there. The more you know about your application, the more you come up with ideas on what to experiment with.

00:25:39.640 --> 00:26:09.620
And where to make those changes and how they would affect it.

00:26:09.620 --> 00:26:18.520
that we didn't have an idea of one thing. And we're like, oh yeah, this one thing is going to make our application just scream. And it had the complete opposite effect.

00:26:18.620 --> 00:26:48.600
Yeah, yeah. So that's a really good point. Thinking about, you know, how good our intuition is what around performance is. And so one thing I wanted to sort of wrap that up with those you have a B and you have a go bench, and you can test your server level things. But if you actually want to test your app in isolation, you know, you maybe don't even want to use the development server, you want to just call it directly. And so you could do things like time, say a unit test, or profile a unit test.

00:26:48.600 --> 00:27:18.580
Or something where it's literally just your code running.

00:27:18.580 --> 00:27:48.560
are occurring, and how long they take to occur. Now, an interesting thing is, when you do profiling, sometimes that also affects changes in performance as well. So you have to take some things with a grain of salt, but it is a really good way to kind of look at the execution of what's happening underneath the covers of all that code. And it helps you to really kind of isolate where within the actual application itself, you might make some performance improvements.

00:27:48.560 --> 00:28:00.460
Right, you do have to be a little bit where I'm cognizant of the sort of observer effect, sort of quantum mechanics style, right? Like, it was doing one thing until I observed it, and I did another thing. Darn it.

00:28:00.560 --> 00:28:20.800
Yeah, and the same is true with monitoring and production as well. I've seen several times where maybe you make this method of monitoring performance statistics, but in doing so, you actually create a load on the system, and that load then starts potentially affecting performance in itself.

00:28:20.800 --> 00:28:46.280
So it's all about balance. That's kind of like a key thing. And, you know, you have to look at these things. Sometimes it's worth it, and sometimes it's not worth it. And you really kind of have to take a look at your application, what you're running, and what those trade-offs are. And there's no real hard-line way to say this is worth it, or this is not worth it. It's all very situational. It depends on the application. It depends on the environment.

00:28:46.280 --> 00:28:48.760
And what's actually happening.

00:28:48.760 --> 00:29:15.900
For sure. One final thing on this profiling bit that I wanted to throw out there is, I'm pretty sure some of the other frameworks have something very, very similar. But in Pyramid, it has this thing called the debug toolbar, which lets you analyze the request and see what's happening. And it has a performance tab. And you can check a box, and it'll actually collect the C profile performance data as you click around the site on a page-by-page basis. And that's really nice to just drop in and see, okay, this page is slow because what?

00:29:15.900 --> 00:29:19.240
Just go request it, and then flip tabs over to the other thing.

00:29:19.240 --> 00:29:26.680
That sounds pretty cool. I'm a big fan of Flask, so I haven't really given Pyramid a try, but that sounds very interesting. I'll have to check that out.

00:29:26.680 --> 00:29:34.620
Yeah, it is pretty cool. I'm a fan of Flask as well. And I think Flask has some kind of debug toolbar, but I don't know if it has the profiling built in, because I just haven't done enough with it.

00:29:34.680 --> 00:29:35.820
Yeah, I haven't looked at that.

00:29:36.020 --> 00:29:54.180
Yeah, another thing that I feel like is often, maybe this is my perception from the outside, and it's just like, I'm looking at this like, I know the database for the site sucks. I know it. That's why it's taking five seconds to load this page. I just do. And so I feel like a lot of people skip the real optimization around the database stuff.

00:29:54.360 --> 00:30:01.200
Like, indexes, indexes, like, if you have a query that doesn't use an index, you need a really good justification for that, for example, in my mind.

00:30:01.200 --> 00:30:23.520
Yeah, absolutely. And when you talk about your traditional SQL databases, although some know SQL databases have indexes as well, you know, when you talk about your traditional SQL database, indexes are incredibly important. And think about what they do, right? And this is kind of goes down to knowing what is underneath the covers of every little piece.

00:30:23.620 --> 00:30:33.420
If you think about what an index is, is a database is an application itself, right? It sounds like this ominous thing, but at the end of the day, it's just an application.

00:30:33.740 --> 00:30:44.800
And really, indexes are a way for the database application to know where on disk is, am I most likely to find this data?

00:30:44.800 --> 00:30:53.900
It's a very fast way to find one little piece of data that then leads you to get all of the different data you need.

00:30:53.900 --> 00:31:02.260
And, you know, in SQL talk, it's really, you know, where's this index key? Let me find that key. And then boom, here's all my row of data.

00:31:02.260 --> 00:31:23.140
It really helps with performance. I would say indexes are very important as well. But sometimes it's also queries. Sometimes queries can be very, very complicated. Indexes are no indexes. And simplifying some of your queries, simplifying some of your database structure can really help out with performance as well.

00:31:23.140 --> 00:31:35.760
Right. And maybe your queries are terrible because your models are not quite right in your database. You know, I mean, there's like all sorts. We can't go too far down here, but it definitely, I think optimizing the database is something to consider, right?

00:31:35.900 --> 00:31:46.960
Yeah, exactly. And one thing to remember is, you know, the database was modeled at the beginning of this application. But in reality is most applications grow over time and the usage grows over time.

00:31:46.960 --> 00:31:59.500
So sometimes those queries get the way they get because, well, we wanted to change and have this ability to pull this data over here, but we didn't want to make major changes to the database model. And sometimes it's just necessary.

00:31:59.500 --> 00:32:00.380
Yeah, for sure.

00:32:00.920 --> 00:32:12.020
So you wrote a really interesting article called Eliminate the DBA. I don't want to go too deeply into it, but you had some really interesting ideas. I definitely want to point people at it. Maybe give us a flyover on that.

00:32:12.020 --> 00:32:24.660
Really, it's Eliminate the DBA for higher availability. And it's a bit of a trolling title, to be honest. A lot of DBAs internally did not like me for that post.

00:32:24.660 --> 00:32:41.040
But really what my point is, is when you're creating a highly available application and highly performant application, the goal is to minimize complexity because complexity leads to problems.

00:32:41.300 --> 00:32:48.000
The more complex an application is, the harder it is to troubleshoot. And not just an application, but an environment in total.

00:32:48.000 --> 00:32:53.280
The harder it is to troubleshoot, the more opportunities for failure are there.

00:32:53.280 --> 00:32:59.460
If you just have an application and there's no database, a database going down doesn't affect that application.

00:32:59.460 --> 00:33:03.760
But if both are there, you have two failure points versus one.

00:33:04.060 --> 00:33:14.380
And that's, you know, really kind of what the article is all about is kind of calling out that if you're going to use a database, make it worthwhile.

00:33:14.380 --> 00:33:17.540
Don't just use a database just to use a database because it's easier.

00:33:17.540 --> 00:33:26.620
That is often kind of really what the design pattern is all about is only use a database if it's absolutely necessary.

00:33:26.620 --> 00:33:31.500
And that's really only when you're talking about super high availability environments.

00:33:31.500 --> 00:33:34.000
Some environments, it doesn't really matter.

00:33:34.000 --> 00:33:36.760
If it's easier to use a database, then use it.

00:33:36.760 --> 00:33:38.060
You've got that web app.

00:33:38.060 --> 00:33:38.960
You've got the database.

00:33:38.960 --> 00:33:39.820
They talk to each other.

00:33:39.820 --> 00:33:41.560
Maybe they're even on the same machine.

00:33:41.560 --> 00:33:42.040
Maybe.

00:33:42.180 --> 00:33:42.580
Exactly.

00:33:42.580 --> 00:33:43.260
Exactly.

00:33:43.260 --> 00:33:47.360
And sometimes it's fine, but other times it isn't.

00:33:47.360 --> 00:33:56.040
And really what that article was all about is knowing when to think about should I or shouldn't I include a database in it.

00:33:56.040 --> 00:33:59.500
Yeah, I liked it because it made me think like at first, like, no, that's not possible.

00:33:59.500 --> 00:34:04.980
And then I'm like, all right, so how is it possible if I think the answer is that you can't do it?

00:34:04.980 --> 00:34:05.880
Do you know what I mean?

00:34:05.880 --> 00:34:07.360
Yeah, it's pretty cool.

00:34:07.360 --> 00:34:08.820
And it all depends on use case.

00:34:08.820 --> 00:34:11.200
Some applications, it's completely not possible.

00:34:11.480 --> 00:34:13.160
In other applications, it is.

00:34:13.160 --> 00:34:24.080
And I'll give you a really good common example, not even like card related, is, you know, my personal blog is actually a statically generated HTML.

00:34:24.080 --> 00:34:27.280
Now, that doesn't mean I write in HTML.

00:34:27.280 --> 00:34:33.460
I write my blog in Markdown, and then I use Python to take that Markdown and generate HTML.

00:34:33.460 --> 00:34:38.700
Some blogs, like if you look at like WordPress, for example, that's got a database backend.

00:34:39.560 --> 00:34:48.440
Now, my stack HTML is going to definitely be a lot less complicated to run than a whole web stack just to write a blog.

00:34:48.440 --> 00:34:48.820
Right.

00:34:48.820 --> 00:34:49.940
You might not even need a server.

00:34:49.940 --> 00:34:52.420
You could potentially drop it on like S3 or something.

00:34:52.420 --> 00:34:53.360
Yeah, exactly.

00:34:53.360 --> 00:34:53.800
Exactly.

00:34:53.800 --> 00:34:55.780
You can get real interesting once it's static pages.

00:34:55.780 --> 00:34:56.560
Right.

00:34:56.640 --> 00:34:57.000
Absolutely.

00:34:57.000 --> 00:34:57.940
All right.

00:34:57.940 --> 00:35:06.320
So there's a lot of stuff we can do at the architectural level, you know, caching, queuing, asyncio, changing the runtime to say PyPy or Cython or something.

00:35:06.320 --> 00:35:09.600
But I want to make sure we touch on all the stacks at a pretty good level.

00:35:09.740 --> 00:35:18.320
So maybe let's move up one level into MicroWSGI and say, this is the thing that runs your Python code.

00:35:18.580 --> 00:35:20.620
What are the knobs and levers that we can turn here?

00:35:20.620 --> 00:35:22.240
There's quite a few.

00:35:22.240 --> 00:35:27.000
One of the simplest ones is actually enabling more threads.

00:35:27.420 --> 00:35:38.680
So threads are interesting because you can actually go, you can go too far and have too many threads and or also go too few as well.

00:35:38.680 --> 00:35:46.020
And really what that is, is if you look at that configuration, it's processes equals a number.

00:35:46.020 --> 00:35:54.940
So one of the things that you kind of want to look at is how many CPUs does my actual machine that I'm running this on have?

00:35:55.240 --> 00:36:00.120
And that's your production machine, not your development machine, because those are two different things.

00:36:00.120 --> 00:36:05.020
And sometimes you have to also adjust for the environment as well.

00:36:05.020 --> 00:36:14.680
And that's something to kind of think about when you're thinking about performance tuning things is what's it run on my laptop is going to be very different than how things run in production.

00:36:14.680 --> 00:36:19.100
In production, you might have a machine with a whole ton of CPUs available.

00:36:19.100 --> 00:36:24.100
And on your laptop, you only have, you know, maybe four or eight, right?

00:36:24.340 --> 00:36:25.340
Depending on your machine.

00:36:25.340 --> 00:36:27.660
And then another thing.

00:36:27.660 --> 00:36:34.880
So kind of the golden rule there is try not to exceed the number of processes for a CPU.

00:36:34.880 --> 00:36:43.480
But I have found in some cases and some workloads, you can actually go up to twice of it and still get a performance increase.

00:36:43.480 --> 00:36:44.880
It's kind of interesting.

00:36:45.220 --> 00:36:51.140
It's really one of those things where you've got to adjust the number and slowly adjust it as you go.

00:36:51.140 --> 00:37:00.980
Start from the lowest and work your way up or potentially work your way down if you've already got something deployed and it's starting to hit some interesting areas.

00:37:01.160 --> 00:37:01.300
Yeah.

00:37:01.300 --> 00:37:10.000
It gets really interesting, too, because basically the parallelism of that is tied to the parallelism of Python, which has its own interesting mixes.

00:37:10.000 --> 00:37:10.400
Right.

00:37:10.400 --> 00:37:14.860
And so if you're doing something that's computational, that takes really long.

00:37:14.860 --> 00:37:15.520
Right.

00:37:15.560 --> 00:37:17.300
Like you're I mean, it doesn't have to be science.

00:37:17.300 --> 00:37:19.820
It could be generating a really large RSS feed.

00:37:19.820 --> 00:37:21.980
For example, some of us has experience with that.

00:37:21.980 --> 00:37:27.180
And, you know, the RSS feed for Doc Python is like 700K.

00:37:27.180 --> 00:37:29.540
And it's quite a bit to generate it at this point.

00:37:29.540 --> 00:37:32.640
At some point, I may have to do something about it, but it's hanging in there just fine.

00:37:32.840 --> 00:37:37.920
But, you know, that that kind of stuff that kind of locks that process up even with the threads.

00:37:37.920 --> 00:37:38.160
Right.

00:37:38.160 --> 00:37:44.700
But if what you're doing is like you're basically I come in, I process requests, I call a database, I wait, I call a web service, I wait and I give it back.

00:37:44.700 --> 00:37:48.600
That one can keep flying because those network I.O. things kind of break it free.

00:37:48.600 --> 00:37:48.880
Right.

00:37:48.880 --> 00:37:49.820
They release the kill.

00:37:49.820 --> 00:37:54.320
And so it gets really it's I think it also depends on how your app is working.

00:37:54.320 --> 00:37:55.540
What kind of app are you running there?

00:37:55.540 --> 00:37:56.760
You're exactly right.

00:37:56.760 --> 00:38:04.420
One thing I actually want to call out just to circle back a little bit, sorry, is there's kind of two adjustments you can make.

00:38:04.420 --> 00:38:10.220
You know, by default, UWSGI processes all have the same CPU affinity.

00:38:10.220 --> 00:38:20.440
So if you do have like a two CPU machine, for example, just changing the processes to four will actually lock all four of those to the same CPU.

00:38:20.520 --> 00:38:30.720
But if you enable threads equals two or enable threads equals true, what that'll actually do is I'll actually split the processes across multiple CPUs.

00:38:30.720 --> 00:38:42.200
And that's actually a very common problem that people run into when doing multithreading is they run into like, oh, well, I'll just add some threads and we're good to go.

00:38:42.200 --> 00:38:45.960
But how it actually lays out in the stack is a little bit different.

00:38:46.160 --> 00:38:53.640
Linux tries to get things running on the same CPU as much as possible to really leverage things like L2 cache.

00:38:53.640 --> 00:38:58.580
But there are ways to split it out to multiple CPUs.

00:38:58.580 --> 00:39:00.560
And sometimes that can really be a big benefit.

00:39:00.560 --> 00:39:10.580
But yeah, depending on what the application does, the reverse can be true as well, where running on that same CPU can give you that performance benefit as well.

00:39:10.580 --> 00:39:21.040
And like your RSS example, I would say, because you're kind of looking all at the same data and generating it, you might even get a benefit running that on one CPU versus two, right?

00:39:21.040 --> 00:39:22.060
Yeah, for sure.

00:39:23.060 --> 00:39:26.280
This portion of Talk Python To Me was brought to you by GoCD.

00:39:26.280 --> 00:39:35.040
GoCD is an on-premise, open-source, continuous delivery tool to help you get better visibility into and control of your team's deployments.

00:39:35.040 --> 00:39:41.580
With GoCD's comprehensive pipeline modeling, you can model complex workflows for multiple teams with ease.

00:39:41.700 --> 00:39:47.200
And GoCD's value stream map lets you track changes from commit to deploy at a glance.

00:39:47.200 --> 00:39:51.380
Say goodbye to deployment panic and hello to consistent, predictable deliveries.

00:39:51.380 --> 00:39:55.720
We all know that continuous integration is super important to the code quality of your applications.

00:39:55.720 --> 00:39:59.340
Choose the open-source, local CI server, GoCD.

00:39:59.340 --> 00:40:03.260
Learn more at talkpython.fm/gocd.

00:40:03.260 --> 00:40:06.060
That's talkpython.fm/gocd.

00:40:06.940 --> 00:40:14.920
You have an example in this, there's an article that you wrote about optimizing Mike Grovitzki or UWSGI, and you have one for Nginx that we'll talk about as well.

00:40:14.920 --> 00:40:24.960
And you start out as your baseline in this one at 347 requests, and just that change knocked it up quite a bit to 1068, which is quite the improvement.

00:40:24.960 --> 00:40:26.020
Yeah, it is.

00:40:26.020 --> 00:40:28.880
And that's as simple as going from one CPU to two.

00:40:28.880 --> 00:40:30.680
Yeah, exactly.

00:40:31.480 --> 00:40:37.860
It seems like, you know, math would say, well, if I have two CPU and I'm getting 347, shouldn't I get around 6800?

00:40:37.860 --> 00:40:41.460
So, you know, 6800 range.

00:40:41.460 --> 00:40:44.220
You know, sometimes you can even go a little bit higher, right?

00:40:44.220 --> 00:40:46.880
With you have less contention.

00:40:46.880 --> 00:40:53.520
Another thing to kind of think about with Linux is there's a task scheduler, right?

00:40:53.520 --> 00:41:01.380
And this task scheduler is figuring out what processes should I give priority to CPU time.

00:41:01.380 --> 00:41:09.960
And when you're all running on a single CPU, you also have other processes that are running against that single CPU.

00:41:09.960 --> 00:41:15.620
So you're going to have conflicts in CPU time that the task scheduler's job is to figure all that out.

00:41:15.620 --> 00:41:21.400
So having two kind of allows you to reduce some of those task scheduler conflicts as well.

00:41:21.400 --> 00:41:22.140
Yeah, it's pretty interesting.

00:41:22.140 --> 00:41:28.920
So the two other major things, one of them I think is somewhat obvious.

00:41:28.920 --> 00:41:30.700
One of them is sort of counterintuitive.

00:41:30.700 --> 00:41:37.240
One, you say, is to disable logging, which you may or may not want to do that based on you might want to have logs for certain reasons.

00:41:37.240 --> 00:41:42.060
But if you can, disabling logging actually has a pretty significant performance change.

00:41:42.060 --> 00:41:45.920
Yeah, because that's disk IO, essentially.

00:41:45.920 --> 00:41:53.540
So every log message, you know, and there's many ways to solve this problem.

00:41:53.540 --> 00:41:58.760
In my article, I kind of took the easy approach by just disabling it because it was an article and it was easy.

00:41:58.760 --> 00:42:12.200
But really what the root of that is, is by disabling logging, I'm telling, you know, micro WSGI to stop writing to disk, essentially for every request.

00:42:12.460 --> 00:42:17.620
So for every request by default, it's just going to write to disk details about that.

00:42:17.620 --> 00:42:21.920
And whether it's asynchronous or synchronous, and those do matter a lot.

00:42:22.020 --> 00:42:26.120
You know, some platforms will default to kind of synchronous logging.

00:42:26.120 --> 00:42:33.220
And what that is, is it makes sure that that data is written to disk before kind of going to the next step.

00:42:33.220 --> 00:42:37.880
And asynchronous is more like, well, let's kick off a, let's throw this in a buffer.

00:42:37.880 --> 00:42:41.980
Let's kick off a thread to write these to disk and let things kind of continue.

00:42:41.980 --> 00:42:44.240
Those can be huge.

00:42:44.240 --> 00:42:52.420
Just going from synchronous to asynchronous can be a big performance increase, but nothing will give you better performance than just disabling it.

00:42:52.420 --> 00:42:54.800
But there's some trade-offs with that, like you said.

00:42:54.800 --> 00:42:57.080
It's hard to optimize faster than doing nothing.

00:42:57.080 --> 00:42:57.520
Yeah.

00:42:57.520 --> 00:43:05.880
One little tip I tend to like is actually using syslog with UDP for logging instead of going to disk.

00:43:05.880 --> 00:43:11.120
So syslog is a very well-established protocol.

00:43:11.580 --> 00:43:15.920
When you're using UDP, you don't have to worry too much about like TCP handshakes.

00:43:15.920 --> 00:43:17.100
It's kind of fire and forget.

00:43:17.100 --> 00:43:27.340
So pushing that to a network place versus disk, which, you know, disk is traditionally slow, although solid state drives have made it a lot faster.

00:43:27.340 --> 00:43:33.540
It's still slower than, you know, memory and going to kind of that network stack can make a big difference.

00:43:33.540 --> 00:43:36.480
And that's kind of an interesting trick.

00:43:36.840 --> 00:43:41.780
You don't necessarily lose your log data, but you also, you know, don't have to go to disk.

00:43:41.780 --> 00:43:49.240
But another kind of key one there is make sure you're not writing too many log entries.

00:43:49.240 --> 00:43:53.560
Kind of finding the right balance of how much logging is there is really important.

00:43:53.560 --> 00:43:56.500
Now, when you're using a framework, the framework is going to do what it's going to do.

00:43:56.580 --> 00:44:00.460
But even within your application, the less logging, the better.

00:44:00.460 --> 00:44:04.960
But at the same time, you have to have this kind of minimum amount of logging in order to support it.

00:44:04.960 --> 00:44:05.680
Yeah, absolutely.

00:44:06.420 --> 00:44:15.200
So the last one that I said was non-intuitive is you can tell the worker process to live only a certain amount of time.

00:44:15.200 --> 00:44:21.080
And after that time to just be killed off and basically start fresh again, which there's some, you know, startup costs.

00:44:21.080 --> 00:44:27.420
And there's like some cost to having this new process come up and it reads like your template files potentially or whatever.

00:44:27.860 --> 00:44:33.960
So it seems like that would be slow, but you actually flipped it to something like restart the worker process every 30 seconds.

00:44:33.960 --> 00:44:35.160
And it was quite fast.

00:44:35.160 --> 00:44:37.320
That really depends on the application.

00:44:37.320 --> 00:44:44.120
If your application is going to hold a lot of data in memory, that could be good or bad, right?

00:44:44.120 --> 00:44:45.840
Depending on how things start up.

00:44:45.840 --> 00:44:53.320
If you have to kind of load things in memory before you can really start serving requests, then that startup time really, that's a hit, right?

00:44:53.320 --> 00:45:09.040
But if your worker process is really just executing something very fast, restarting it, as long as it kind of starts up very quick and you have a very low startup time within the actual application itself, you can get a big benefit.

00:45:09.040 --> 00:45:12.660
The example I had was a very simple, simple application.

00:45:12.660 --> 00:45:17.980
There wasn't a whole lot of kind of data stored in memory or anything like that that you had to kind of build up.

00:45:17.980 --> 00:45:21.920
So the start time was really small and that's where I kind of got that benefit.

00:45:21.920 --> 00:45:24.580
But again, it all depends on the application.

00:45:24.580 --> 00:45:30.140
The benefit can be the lack of memory management or simple allocation because the memory is not fragmented.

00:45:30.140 --> 00:45:36.960
You know, one of the things that Instagram, pretty sure it was Instagram, did, that was a super counterintuitive for performance.

00:45:36.960 --> 00:45:39.300
But at its levels, they turned off the Python GC.

00:45:39.300 --> 00:45:44.060
They left only the reference counting bit, but that doesn't handle cycles and stuff.

00:45:44.060 --> 00:45:46.020
So there's definitely memory leaks when you do that.

00:45:46.020 --> 00:45:46.460
Yeah.

00:45:46.460 --> 00:45:47.860
But if you restart enough.

00:45:47.860 --> 00:45:48.320
Exactly.

00:45:48.320 --> 00:45:50.920
You just go, it's our workload, our data.

00:45:50.920 --> 00:45:53.500
That means we can run for six hours before we run out of memory.

00:45:53.500 --> 00:45:55.180
So let's just restart every hour.

00:45:55.180 --> 00:45:56.020
Something like that.

00:45:56.020 --> 00:45:58.300
And they got like 12% improvement or something.

00:45:58.300 --> 00:45:59.320
I mean, it was really significant.

00:45:59.540 --> 00:46:02.280
That goes into kind of the overall architecture, right?

00:46:02.280 --> 00:46:11.480
It's sometimes it's okay to kill off an application as long as you're doing it gracefully and as long as you have others to take its place.

00:46:11.480 --> 00:46:11.980
Right.

00:46:12.100 --> 00:46:17.180
And that's kind of where that whole microservices approach really lends a hand.

00:46:17.180 --> 00:46:23.120
Because if you break things down really small, then you can run, it's a lot easier to run multiple of them.

00:46:23.120 --> 00:46:32.620
So you can actually handle the distribution of load to other processes when these ones, you want to start taking them down.

00:46:32.620 --> 00:46:33.200
Right.

00:46:33.260 --> 00:46:39.660
And that's where having like Nginx up front doing some of that load balancing really plays into a big hand in that.

00:46:39.660 --> 00:46:39.980
For sure.

00:46:39.980 --> 00:46:41.300
So let's talk about Nginx.

00:46:41.300 --> 00:46:43.140
We've sort of said what it is.

00:46:43.140 --> 00:46:53.280
But just like before you had this baseline analysis, in this case, you had a little under 3000 requests per second, which this is to like a static HTML file or something like that.

00:46:53.280 --> 00:46:53.480
Right.

00:46:53.480 --> 00:46:54.020
Exactly.

00:46:54.260 --> 00:46:54.420
Yeah.

00:46:54.420 --> 00:46:56.420
So it gets all the other stuff out of the way.

00:46:56.420 --> 00:46:57.280
It's not to the app.

00:46:57.280 --> 00:46:58.760
It's just to serve up a thing.

00:46:58.760 --> 00:47:03.260
And so the first thing that you said that you might want to look at is worker threads.

00:47:03.260 --> 00:47:07.040
Again, that's just like with uWSGI, right?

00:47:07.040 --> 00:47:11.960
That's really the number of processes on the system.

00:47:11.960 --> 00:47:25.320
So a really interesting thing about Nginx is by default, it's actually at auto, which tells Nginx to create one worker thread for every CPU available to the system.

00:47:25.320 --> 00:47:30.180
Now, what I actually did was I changed it to two.

00:47:30.180 --> 00:47:32.180
So actually, no, I changed it to four.

00:47:32.180 --> 00:47:38.560
I had two CPUs on the system, which is basically two worker threads per CPU and two processes.

00:47:38.560 --> 00:47:41.320
And I actually got a pretty good boost out of that.

00:47:41.320 --> 00:47:42.660
And it wasn't too bad.

00:47:42.660 --> 00:47:50.040
But then if you changed it to eight, when I kind of was tinkering, and this goes into experimentation, right?

00:47:50.040 --> 00:47:51.000
Measure, measure, measure.

00:47:51.000 --> 00:47:54.020
When I changed it to eight, performance dropped quite a bit.

00:47:54.020 --> 00:47:56.200
So back it goes.

00:47:56.200 --> 00:47:56.700
Yeah.

00:47:56.700 --> 00:47:59.040
So it's a very close balance.

00:47:59.040 --> 00:48:00.520
It's walking a very thin line.

00:48:00.520 --> 00:48:05.320
You know, you can optimize things to work really well in these situations.

00:48:05.540 --> 00:48:10.460
But once you go a little too far, then you start hitting other contention, right?

00:48:10.460 --> 00:48:14.380
And that really breaks down to like that CPU task scheduler.

00:48:14.380 --> 00:48:20.720
And that's actually why I did that in that article is to kind of show sometimes things don't always work out when you just add more numbers.

00:48:20.720 --> 00:48:21.020
Yeah.

00:48:21.100 --> 00:48:26.580
So by messing with the worker threads, you were able to get it to go from a little under 3,000.

00:48:26.580 --> 00:48:29.140
You added another 2,250.

00:48:29.140 --> 00:48:32.680
So not quite doubling, but still quite good.

00:48:32.680 --> 00:48:39.080
And then the other thing you said is like, maybe some of these connections are going to last for a long time or they're sort of backed up.

00:48:39.080 --> 00:48:41.040
Like, what if we let it accept more connections?

00:48:41.040 --> 00:48:47.900
And this is important for requests, but it's super important for like really long, large files, I would imagine, or lots of them.

00:48:47.900 --> 00:48:52.540
People downloaded them or even like WebSockets, these persistent type of things.

00:48:52.540 --> 00:48:53.540
Yeah, exactly.

00:48:53.540 --> 00:48:55.220
And that's exactly it, right?

00:48:55.220 --> 00:49:00.100
Sometimes, and actually in this case, it was, I believe it was a simple REST API.

00:49:00.380 --> 00:49:05.220
So there wasn't really a whole lot of like static connections, but you're right.

00:49:05.220 --> 00:49:09.580
That is a big performance increase when you have those long live connections.

00:49:09.580 --> 00:49:16.040
And sometimes, you know, that's letting Ingenix do some of the work, let Ingenix kind of handle that connectivity.

00:49:16.040 --> 00:49:23.960
By increasing the number of connections per worker, if they're very fast requests to the downstream application,

00:49:24.580 --> 00:49:32.440
you can actually kind of leverage Ingenix handling connectivity with the end client and through the process very quick.

00:49:32.440 --> 00:49:42.680
So by making some changes, I think in that case, like I changed it to like 1024, for example, it went up to like 6,000 requests per second, which was a huge improvement.

00:49:42.680 --> 00:49:43.760
It's really cool.

00:49:43.760 --> 00:49:55.860
Now, another thing in that kind of worker space is to look at the number of open files, which is a very common Linux limitation that people run into.

00:49:55.860 --> 00:50:05.060
In Ingenix, as it's serving static content, for example, or even just the fact that it has logging enabled as well, it's going to have open file handles.

00:50:05.060 --> 00:50:08.540
And by default in Linux, there's a limitation.

00:50:08.780 --> 00:50:16.300
I want to say these days it's 4096, but actually, I think the Ingenix default limitation is much smaller.

00:50:16.300 --> 00:50:17.580
I forget what it is exactly.

00:50:17.580 --> 00:50:29.680
I mean, I think in that example, all I really did was just upped it to 4096, which allowed it to have even more files open, which gave a little bit of a boost, but not too much.

00:50:29.680 --> 00:50:31.500
Yeah, it was still really nice.

00:50:31.500 --> 00:50:35.340
I think it gave it like 300 more or something about it.

00:50:35.340 --> 00:50:36.140
Yeah, which is cool.

00:50:36.140 --> 00:50:44.520
But the other thing you can do is most web workloads are hitting a number of files, 10, 50, 100, or 1,000.

00:50:44.520 --> 00:50:49.420
But after that, how many unique static files do you have in most situations?

00:50:49.420 --> 00:50:55.340
I know there are some that there's tons, but most sites, they've got their CSS and their images and whatever, and it's mostly shared, right?

00:50:55.340 --> 00:50:58.460
So you can also tell it to cache that stuff, right?

00:50:58.460 --> 00:50:59.160
Yeah, you can.

00:50:59.160 --> 00:51:08.400
And I would say one caveat, though, is just because you only have a certain amount of files doesn't mean that that process isn't opening other files.

00:51:08.400 --> 00:51:12.620
Sometimes there's things like shared libraries that it opens, and all of those count as well.

00:51:12.620 --> 00:51:18.860
And even like sockets and things like that, they all kind of count towards the limitations in the OS.

00:51:19.040 --> 00:51:23.420
But in regards to caching, that's actually pretty cool.

00:51:23.420 --> 00:51:34.980
There's some options with NGENX to do like an open file cache, which allows you to increase the default amount of cache for open file handles.

00:51:35.360 --> 00:51:42.760
So NGENX will open up those CSS files and those HTML files, like you were saying, and it will actually load them in memory.

00:51:42.760 --> 00:51:56.720
So that way, when you get a request, even though it's a file on the file system, since NGENX has it open and it's cached in memory, it doesn't have to go to disk for access to that, which makes it a lot faster.

00:51:56.720 --> 00:51:58.500
Yeah, just serve it straight back out of memory.

00:51:58.500 --> 00:51:58.880
That's awesome.

00:51:58.880 --> 00:52:08.640
So in the end, you were able to tune it NGENX just on its own bit from 2,900 up to 6,900.

00:52:08.640 --> 00:52:14.060
That's a serious bit of change by just tweaking a few config settings.

00:52:14.060 --> 00:52:18.840
And then you did the same thing, you know, something similar on the UWSGI level.

00:52:18.840 --> 00:52:21.980
And then, of course, you could, you know, tweak your architecture as well.

00:52:21.980 --> 00:52:26.180
But just making the stuff that contains your app go that much faster, that's pretty awesome.

00:52:26.180 --> 00:52:30.020
There's another article that I wrote about kind of benchmarking Postgres.

00:52:30.020 --> 00:52:38.400
And I had a similar experience there is all I really did in that article was just adjust a shared buffers configuration.

00:52:38.400 --> 00:52:42.900
And what that is, is that's essentially a query cache for Postgres.

00:52:42.900 --> 00:52:49.860
And that change alone for the example I was given had a big, had a big performance increase.

00:52:49.860 --> 00:52:58.320
So, you know, sometimes I kind of call these a little bit of low-hanging fruit because they're just little knobs you can change in existing systems.

00:52:58.320 --> 00:53:02.480
And sometimes those low-hanging fruit can be a really good first step.

00:53:02.480 --> 00:53:04.320
None of those seem super scary, right?

00:53:04.320 --> 00:53:05.920
You just change some config files.

00:53:05.920 --> 00:53:08.480
I mean, if you mess up your config, you will take your website down.

00:53:08.480 --> 00:53:10.200
But, you know, you just put it back.

00:53:10.200 --> 00:53:12.020
It's not super complicated, right?

00:53:12.020 --> 00:53:14.520
The key is test before you put it in production, right?

00:53:15.020 --> 00:53:23.040
So as long as you're testing before production, then, you know, if you make a change and it doesn't work, then, oh, well, who cares, right?

00:53:23.200 --> 00:53:32.940
But really, that's kind of getting into the measuring, establishing your baseline and measuring each little change and how they interact.

00:53:32.940 --> 00:53:34.440
That's real important.

00:53:34.440 --> 00:53:38.940
So, yeah, that way, when you go to production, you know exactly what you're changing.

00:53:38.940 --> 00:53:39.800
Yeah, that sounds good.

00:53:39.800 --> 00:53:43.760
So we're just about out of time for our conversation here.

00:53:43.760 --> 00:53:48.700
But I did want to just ask you, like, how do things like Docker and Kubernetes change this?

00:53:48.700 --> 00:53:49.980
Do they just make it more complicated?

00:53:49.980 --> 00:53:51.160
Do they simplify it?

00:53:51.160 --> 00:53:53.780
Like, what do you think containers mean around this conversation?

00:53:53.780 --> 00:53:55.560
They simplify some things.

00:53:55.560 --> 00:54:00.680
Like, when we're talking like scale-out type approach, it makes it real easy to spin up a new one.

00:54:00.680 --> 00:54:07.440
I was kind of thinking when you asked that question, I think of some of the challenges I've ran into with those.

00:54:07.580 --> 00:54:19.640
One thing that I've run into with, like, Docker is if you just pull down a service and use that service out of the box without changing it, you're going to get kind of a default performance.

00:54:19.640 --> 00:54:30.480
So by using a Docker package, a lot of times you kind of forget about all those little knobs that you have to turn in order to get it fast and performant.

00:54:30.480 --> 00:54:40.440
And then another area that that kind of goes into is Docker is it has some services as well that things run through.

00:54:40.440 --> 00:54:43.000
And Kubernetes is a big example of this.

00:54:43.000 --> 00:54:46.300
So with Kubernetes, you have a software-defined network, right?

00:54:46.700 --> 00:54:52.800
So if you have, like, a cluster, and let's just give kind of a scenario.

00:54:52.800 --> 00:55:03.040
For whatever reason, you had, you know, half your cluster on one side and half your cluster on another, and there's a network latency between getting to that other half.

00:55:03.460 --> 00:55:10.280
With Kubernetes and services, you would have your traffic land on any host within that cluster.

00:55:10.280 --> 00:55:17.360
And then that software-defined networking is responsible for moving that request to the appropriate hosts that might be running that service.

00:55:17.360 --> 00:55:21.020
So if there's some latency in there, you can actually start seeing that.

00:55:21.020 --> 00:55:26.700
And that's actually very obscured away once you start getting into that area.

00:55:26.700 --> 00:55:34.940
It's hard to kind of pin that down because that's so far removed from what you're doing in your application that that can actually be pretty tricky.

00:55:34.940 --> 00:55:40.540
Plus the fact that, you know, you have to go through that software-defined networking means you're taking some penalties there.

00:55:40.540 --> 00:55:43.200
But again, it's probably worth it.

00:55:43.200 --> 00:55:44.220
Yeah, probably.

00:55:44.220 --> 00:55:45.940
Very, very interesting.

00:55:46.480 --> 00:55:54.560
All right, before I get to the final two questions for you, you said that you, we were talking before we hit record about an open source project that you're working on as well.

00:55:54.560 --> 00:55:57.620
You want to give a quick elevator pitch for what that is so people know about it?

00:55:57.620 --> 00:55:58.300
Yeah, absolutely.

00:55:58.300 --> 00:56:03.700
So Automatron is the open source tool that I've created.

00:56:03.700 --> 00:56:07.300
It's kind of a second version of something called Runbook.

00:56:07.300 --> 00:56:15.620
I launched an open source project called Runbook, and things happened where I was like, I need to redo this and start fresh.

00:56:15.740 --> 00:56:17.420
And that became Automatron.

00:56:17.420 --> 00:56:22.560
And really what it is, is it's kind of like if Nagios met IFTTT.

00:56:22.560 --> 00:56:31.860
So where you have these health checks and they monitor, you know, whatever you tell them to monitor, the health checks are really just executables.

00:56:31.860 --> 00:56:39.680
What it will do is it will SSH to the remote server, to the monitored system, run that health check.

00:56:39.860 --> 00:56:44.440
And based on kind of the Nagios return codes, it's either good or bad.

00:56:44.440 --> 00:56:44.700
Right.

00:56:44.700 --> 00:56:47.580
Like SSH in and ask for the free memory or something like that.

00:56:47.580 --> 00:56:48.260
Yeah, exactly.

00:56:48.260 --> 00:56:48.920
Exactly.

00:56:48.920 --> 00:56:51.660
And if it's bad, you know, if you beyond a threshold.

00:56:51.660 --> 00:56:52.920
If it's zero.

00:56:53.200 --> 00:56:54.420
Yeah, yeah, exactly.

00:56:54.420 --> 00:56:57.760
Then really the exit code would indicate a failure.

00:56:57.760 --> 00:57:09.900
And that exit code will actually trigger an action to take place, which is, again, SSHing out to a system and then executing either a command or a script or something like that.

00:57:09.980 --> 00:57:11.500
And this is all built in Python.

00:57:11.500 --> 00:57:18.340
And it actually uses Fabric very heavily, which is a really cool SSH command execution wrapper.

00:57:18.340 --> 00:57:19.360
It's very cool.

00:57:19.360 --> 00:57:20.640
I'm a big fan of it.

00:57:20.640 --> 00:57:22.800
And I used it very heavily there.

00:57:22.800 --> 00:57:25.840
And it's kind of a cool little side project.

00:57:25.840 --> 00:57:31.720
And really what it's there for is I hate on-call and I really wish it would go away.

00:57:31.720 --> 00:57:37.100
And this is one of my ways to hopefully help people make it go away.

00:57:37.100 --> 00:57:41.660
Could a machine just go restart the web server process and just not call me?

00:57:41.660 --> 00:57:42.080
Exactly.

00:57:42.080 --> 00:57:49.800
Or, you know, in some cases when you've kind of designed your environment well enough, you could just maybe reboot the box and who cares?

00:57:49.800 --> 00:57:50.760
Yeah, true.

00:57:50.760 --> 00:57:59.840
At a certain scale, is it worth finding the root cause of a single issue or is it more worth finding the root cause of continuous issues?

00:57:59.840 --> 00:58:06.980
And that's kind of one of the real philosophy changes that that kind of project brings is, in my opinion at least,

00:58:06.980 --> 00:58:12.940
it's worth fighting, you know, more frequent and reoccurring problems.

00:58:12.940 --> 00:58:15.340
And just one-off problems are not worth it.

00:58:15.340 --> 00:58:16.040
Just restart the thing.

00:58:16.040 --> 00:58:17.140
Yep.

00:58:17.140 --> 00:58:18.320
Sounds awesome.

00:58:18.320 --> 00:58:18.820
All right, cool.

00:58:18.820 --> 00:58:19.660
So people check that out.

00:58:19.660 --> 00:58:20.400
We'll put it in the show notes.

00:58:20.400 --> 00:58:22.920
All right, last two questions before we go.

00:58:22.920 --> 00:58:25.520
If you're going to write some Python code, what editor do you use?

00:58:25.520 --> 00:58:28.720
Right now, it's Vim screen and syntax highlighting.

00:58:28.720 --> 00:58:29.480
That's it.

00:58:29.480 --> 00:58:31.240
I'm kind of weird, I believe.

00:58:31.240 --> 00:58:33.900
So, but that's where I feel comfortable.

00:58:33.900 --> 00:58:37.920
And I think that's all those years and kind of operations has led me to that.

00:58:37.920 --> 00:58:38.660
Yeah, cool.

00:58:38.660 --> 00:58:40.840
And then notable PyPI package.

00:58:40.840 --> 00:58:42.940
You already called out Fabric, right?

00:58:42.940 --> 00:58:43.540
That's pretty awesome.

00:58:43.540 --> 00:58:45.040
Yeah, Fabric is an awesome one.

00:58:45.040 --> 00:58:49.440
I actually wanted to call out, since we're talking performance tuning, Frozen Flask.

00:58:49.440 --> 00:58:56.020
Now, it's not one that I've used personally quite a bit, but the whole concept is it allows

00:58:56.020 --> 00:58:59.800
you to pre-generate static pages from a Flask application.

00:58:59.800 --> 00:59:01.420
So it's really cool.

00:59:01.420 --> 00:59:07.260
And the real awesome thing that you can do is you can kind of combine that with certain

00:59:07.260 --> 00:59:11.780
like Ngenix rules to where it'll pre-generate the static HTML.

00:59:11.780 --> 00:59:17.160
And then based on, you know, regular expressions in the paths and things like that, you can have

00:59:17.160 --> 00:59:22.820
Ngenix serve that static HTML without having to go down further in the stack.

00:59:22.900 --> 00:59:27.760
And that's real cool because you can use Frozen Flask to still kind of keep that all within

00:59:27.760 --> 00:59:29.080
your Python application.

00:59:29.080 --> 00:59:32.960
And it's really just at runtime, it'll generate that static HTML.

00:59:32.960 --> 00:59:34.060
It'll freeze it.

00:59:34.060 --> 00:59:38.500
And just Ngenix configuration from there takes it away.

00:59:38.500 --> 00:59:39.280
Yeah, that's pretty awesome.

00:59:39.420 --> 00:59:44.360
So you get the dynamic sort of data-driven bit, but then you could just freeze it, if you

00:59:44.360 --> 00:59:47.500
will, like turn it to static files and then serve it through that way.

00:59:47.500 --> 00:59:47.800
That's cool.

00:59:47.800 --> 00:59:53.540
Most web applications, you know, you have static pages and dynamic pages and really using it

00:59:53.540 --> 01:00:01.560
to establish those static pages and pre-generate them can be a big benefit in kind of production

01:00:01.560 --> 01:00:06.260
workloads, not just from a performance perspective, but cost of what it takes to run it as well.

01:00:06.260 --> 01:00:06.640
For sure.

01:00:06.640 --> 01:00:07.660
That's really interesting.

01:00:07.660 --> 01:00:12.880
And sometimes those sort of landing pages and main catalog pages, that's where like really

01:00:12.880 --> 01:00:14.420
the busy traffic is anyway.

01:00:14.420 --> 01:00:17.640
That's pretty much my use case that I've used things like that.

01:00:17.640 --> 01:00:21.760
I haven't used that one in particular, but I've done some things like that where I just

01:00:21.760 --> 01:00:27.560
pre-fetch pages and save the HTML for, yeah, it was hacky, but you know what?

01:00:27.560 --> 01:00:29.400
It really helped keep things slim.

01:00:29.400 --> 01:00:31.080
So it works.

01:00:31.080 --> 01:00:31.580
That's awesome.

01:00:31.580 --> 01:00:32.700
Cool.

01:00:32.700 --> 01:00:34.640
All right, Ben, final call to action.

01:00:34.640 --> 01:00:35.320
People are excited.

01:00:35.720 --> 01:00:38.560
They realize like there's a few knobs that make their code much faster.

01:00:38.560 --> 01:00:39.220
What do you think?

01:00:39.220 --> 01:00:42.580
They should start by reading your two articles about optimization?

01:00:42.580 --> 01:00:46.460
And really just don't be afraid to just jump right into it.

01:00:46.460 --> 01:00:50.080
Even if you don't know how something works, you know, sometimes just turning that knob and

01:00:50.080 --> 01:00:52.460
then figuring out why it works is a real benefit.

01:00:52.460 --> 01:00:56.260
But for kind of self-promotion, for sure, check out those blog posts.

01:00:56.260 --> 01:00:58.280
You know, you can kind of follow me on Twitter.

01:00:58.280 --> 01:01:00.460
Mad Flojo is my handle.

01:01:00.460 --> 01:01:05.820
I have quite a few posts out there and lots of different stuff and lots of different kind

01:01:05.820 --> 01:01:06.320
of areas.

01:01:06.320 --> 01:01:08.000
But that's definitely a good start.

01:01:08.000 --> 01:01:08.180
Yeah.

01:01:08.180 --> 01:01:13.200
People should check out bencane.com slash archive.html because you have a ton of awesome

01:01:13.200 --> 01:01:13.900
articles there.

01:01:13.900 --> 01:01:15.920
We just chose a few to speak about.

01:01:15.920 --> 01:01:16.240
Awesome.

01:01:16.240 --> 01:01:16.820
Thanks.

01:01:16.820 --> 01:01:17.320
Yeah.

01:01:17.400 --> 01:01:21.140
I have tons of stuff, whether it's, you know, Docker related performance tuning.

01:01:21.140 --> 01:01:28.540
One article I wrote is kind of building self-healing environments using things like salt and just

01:01:28.540 --> 01:01:30.500
some Python code, which is...

01:01:30.500 --> 01:01:30.660
Very cool.

01:01:30.660 --> 01:01:32.420
Well, thanks so much for sharing your experience.

01:01:32.420 --> 01:01:33.400
It was great to chat with you.

01:01:33.400 --> 01:01:33.680
Awesome.

01:01:33.680 --> 01:01:34.360
Thank you.

01:01:34.360 --> 01:01:35.280
Thank you for having me.

01:01:35.280 --> 01:01:35.580
You bet.

01:01:36.520 --> 01:01:39.160
This has been another episode of Talk Python to Me.

01:01:39.160 --> 01:01:44.120
Today's guest has been Ben Kane, and this episode is brought to you by Rollbar and GoCD.

01:01:44.120 --> 01:01:47.240
Rollbar takes the pain out of errors.

01:01:47.240 --> 01:01:52.320
They give you the context and insight you need to quickly locate and fix errors that might have

01:01:52.320 --> 01:01:54.960
gone unnoticed until your users complain, of course.

01:01:54.960 --> 01:02:01.200
As Talk Python to Me listeners, track a ridiculous number of errors for free at rollbar.com slash

01:02:01.200 --> 01:02:02.100
Talk Python to Me.

01:02:03.100 --> 01:02:07.040
GoCD is the on-premise, open-source, continuous delivery server.

01:02:07.040 --> 01:02:11.180
Want to improve your deployment workflow but keep your code and builds in-house?

01:02:11.180 --> 01:02:17.520
Check out GoCD at talkpython.fm/gocd and take control over your process.

01:02:17.520 --> 01:02:20.000
Are you or a colleague trying to learn Python?

01:02:20.000 --> 01:02:24.680
Have you tried books and videos that just left you bored by covering topics point by point?

01:02:24.680 --> 01:02:30.340
Well, check out my online course, Python Jumpstart by Building 10 Apps at talkpython.fm slash

01:02:30.340 --> 01:02:33.320
course to experience a more engaging way to learn Python.

01:02:33.320 --> 01:02:38.080
And if you're looking for something a little more advanced, try my Write Pythonic Code course

01:02:38.080 --> 01:02:40.640
at talkpython.fm/pythonic.

01:02:40.640 --> 01:02:43.360
Be sure to subscribe to the show.

01:02:43.360 --> 01:02:45.560
Open your favorite podcatcher and search for Python.

01:02:45.560 --> 01:02:46.800
We should be right at the top.

01:02:46.800 --> 01:02:52.920
You can also find the iTunes feed at /itunes, Google Play feed at /play, and direct

01:02:52.920 --> 01:02:56.120
RSS feed at /rss on talkpython.fm.

01:02:56.120 --> 01:02:57.980
This is your host, Michael Kennedy.

01:02:57.980 --> 01:02:59.360
Thanks so much for listening.

01:02:59.360 --> 01:03:00.420
I really appreciate it.

01:03:00.420 --> 01:03:02.360
Now get out there and write some Python code.

01:03:02.360 --> 01:03:22.820
I'll see you next time.

