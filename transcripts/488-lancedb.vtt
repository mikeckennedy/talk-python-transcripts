WEBVTT

00:00:00.001 --> 00:00:03.100
LanceDB is a developer-friendly open-source database for AI.

00:00:03.100 --> 00:00:06.940
It's used by well-known companies such as Midjourney and Character.ai.

00:00:06.940 --> 00:00:11.160
On this episode, we have Chang-Chi, the CEO and co-founder of LanceDB,

00:00:11.160 --> 00:00:15.240
on to give us a look at the concept of multimodal data

00:00:15.240 --> 00:00:18.320
and how you can use LanceDB in your own Python apps.

00:00:18.320 --> 00:00:24.440
This is Talk Python to Me, episode 488, recorded November 26, 2024.

00:00:24.440 --> 00:00:27.380
Are you ready for your host, here he is!

00:00:27.600 --> 00:00:30.820
You're listening to Michael Kennedy on Talk Python to Me.

00:00:30.820 --> 00:00:34.500
Live from Portland, Oregon, and this segment was made with Python.

00:00:34.500 --> 00:00:40.580
Welcome to Talk Python to Me, a weekly podcast on Python.

00:00:40.580 --> 00:00:42.800
This is your host, Michael Kennedy.

00:00:42.800 --> 00:00:45.660
Follow me on Mastodon, where I'm @mkennedy,

00:00:45.660 --> 00:00:48.060
and follow the podcast using @talkpython,

00:00:48.060 --> 00:00:51.160
both accounts over at fosstodon.org,

00:00:51.160 --> 00:00:56.060
and keep up with the show and listen to over nine years of episodes at talkpython.fm.

00:00:56.320 --> 00:01:00.640
If you want to be part of our live episodes, you can find the live streams over on YouTube.

00:01:00.640 --> 00:01:06.880
Subscribe to our YouTube channel over at talkpython.fm/youtube and get notified about upcoming shows.

00:01:07.520 --> 00:01:09.420
This episode is brought to you by Sentry.

00:01:09.420 --> 00:01:11.200
Don't let those errors go unnoticed.

00:01:11.200 --> 00:01:13.020
Use Sentry like we do here at Talk Python.

00:01:13.020 --> 00:01:16.400
Sign up at talkpython.fm/sentry.

00:01:16.400 --> 00:01:19.140
And this episode is brought to you by Bluehost.

00:01:19.140 --> 00:01:20.780
Do you need a website fast?

00:01:20.780 --> 00:01:21.640
Get Bluehost.

00:01:21.640 --> 00:01:24.400
Their AI builds your WordPress site in minutes,

00:01:24.400 --> 00:01:27.020
and their built-in tools optimize your growth.

00:01:27.020 --> 00:01:27.980
Don't wait.

00:01:27.980 --> 00:01:31.580
Visit talkpython.fm/bluehost to get started.

00:01:31.580 --> 00:01:32.460
Hey, Chang.

00:01:32.680 --> 00:01:34.220
Welcome to Talk Python To Me.

00:01:34.220 --> 00:01:35.300
Hey, how are you?

00:01:35.300 --> 00:01:36.320
Excited to be here.

00:01:36.320 --> 00:01:37.320
I'm excited to have you here.

00:01:37.320 --> 00:01:43.600
We're going to talk about LanceDB, multidimensional data, all sorts of fun,

00:01:43.600 --> 00:01:45.800
multimodal data, all sorts of fun things.

00:01:45.800 --> 00:01:52.280
So really cool database, kind of in the same categories, SQLite, DuckDB.

00:01:52.760 --> 00:01:56.080
People want a quick mental model, but different kind of data, right?

00:01:56.080 --> 00:01:57.200
Yeah, absolutely.

00:01:57.200 --> 00:02:02.580
So I think we specialize in multimodal data or AI data.

00:02:02.580 --> 00:02:06.860
And what we think about is included in that is, you know,

00:02:06.860 --> 00:02:11.520
the tabular data that we have from previous generations of data engineering, data science,

00:02:11.520 --> 00:02:16.320
but also, you know, embedding vectors, images, videos, PDFs,

00:02:16.540 --> 00:02:23.140
basically anything that don't really fit neatly into like data frames or Excel sheets.

00:02:23.140 --> 00:02:23.480
Yeah.

00:02:23.480 --> 00:02:24.820
Less tabular data.

00:02:24.820 --> 00:02:25.600
That's right.

00:02:25.600 --> 00:02:28.860
If it's not good tabular, then you need something else probably.

00:02:28.860 --> 00:02:29.240
Yep.

00:02:29.240 --> 00:02:29.680
Yep.

00:02:29.680 --> 00:02:30.000
Awesome.

00:02:30.000 --> 00:02:35.640
Which is pretty interesting since I was one of the earliest co-contributors to Pandas.

00:02:35.640 --> 00:02:40.040
So I spent years of my life working on data frames and big data.

00:02:40.040 --> 00:02:41.060
And now I'm working on-

00:02:41.060 --> 00:02:42.900
You've given up on squares and rectangles?

00:02:42.900 --> 00:02:43.560
Oh my gosh.

00:02:44.320 --> 00:02:46.820
Well, never, but there's room to build something more here.

00:02:46.820 --> 00:02:47.580
Yeah, I hear you.

00:02:47.580 --> 00:02:49.280
And also, also I agree.

00:02:49.280 --> 00:02:53.140
So it's going to be super fun to dive into all of that stuff together.

00:02:53.140 --> 00:02:55.340
Before we do though, let's hear your story.

00:02:55.340 --> 00:02:56.440
How do you get into programming?

00:02:56.440 --> 00:03:00.660
I know this is written in Rust, but also has some programming APIs.

00:03:00.660 --> 00:03:03.480
How much Python are you doing these days versus other languages?

00:03:03.480 --> 00:03:04.080
Yeah.

00:03:04.080 --> 00:03:08.380
The core of Lance format and Lance CB are in Rust.

00:03:08.380 --> 00:03:13.940
And then there are Python APIs and wrappers around that, as well as TypeScript.

00:03:13.940 --> 00:03:20.180
And the team spends a significant amount of time thinking about what the Python user experience

00:03:20.180 --> 00:03:26.400
looks like and how best to combine the performance that you get from Rust and the ease of use

00:03:26.400 --> 00:03:27.180
you get from Python.

00:03:27.180 --> 00:03:33.920
And then also making it more as extensible as possible so that even though it's a Python library,

00:03:33.920 --> 00:03:40.120
if you want to add features, for example, you don't have to learn Rust just to be a contributor

00:03:40.120 --> 00:03:41.840
to Lance or Lance DB.

00:03:41.840 --> 00:03:48.520
You know, I think Python has benefited a lot from the various Rust-based projects out there,

00:03:48.520 --> 00:03:49.040
right?

00:03:49.040 --> 00:03:51.860
Obviously, we've got uv and Ruff.

00:03:51.860 --> 00:03:55.600
We've got Pydantic, a bunch of other things that make it faster.

00:03:55.600 --> 00:04:00.940
But there has been some pushback that if everything gets done in Rust, then it's hard for Python

00:04:00.940 --> 00:04:01.880
people to contribute.

00:04:02.280 --> 00:04:06.600
I don't necessarily subscribe to that because previously the answer was when things are too slow,

00:04:06.600 --> 00:04:07.560
we write them in C.

00:04:07.560 --> 00:04:13.980
Maybe it's just traditional Python people also know C, but traditional Python people don't also know Rust.

00:04:13.980 --> 00:04:18.300
I think that's honestly the crux of the issue or the crux of the pushback.

00:04:18.300 --> 00:04:23.380
So I really like to hear that, you know, the extensibility points and you've thought through

00:04:23.380 --> 00:04:26.700
how Python people can extend it and contribute and stuff.

00:04:26.700 --> 00:04:34.460
Yeah, I've spent a lot of time writing like Cython and C to make Python code go faster back in the day.

00:04:34.460 --> 00:04:35.540
I think you're right.

00:04:35.540 --> 00:04:38.160
A lot of the points are the same for Rust.

00:04:38.160 --> 00:04:41.400
And actually, Rust is a lot safer, actually.

00:04:41.400 --> 00:04:46.560
So when we originally started Lance in the very beginning in C++,

00:04:46.560 --> 00:04:51.040
and we spent months writing C++ code for a new format.

00:04:51.040 --> 00:05:01.580
And I think it was the Christmas break of 2022 that that's what I call our Rust pill moment was when we decided to switch over.

00:05:01.580 --> 00:05:08.600
And I think we ended up, we took about three weeks and we rewrote roughly four or five months of C++ code there.

00:05:08.600 --> 00:05:13.880
Overall, I think we actually ended up getting better performance for the most part.

00:05:13.880 --> 00:05:25.760
And the biggest thing was just us having the confidence to move forward very quickly without that, like, in the back of our mind, like, where's the next SEGFAULT coming from?

00:05:25.760 --> 00:05:26.120
Yeah.

00:05:26.120 --> 00:05:26.720
Yeah.

00:05:26.720 --> 00:05:27.800
Oh, absolutely.

00:05:27.800 --> 00:05:29.580
Or the next security vulnerability.

00:05:29.580 --> 00:05:30.480
Exactly.

00:05:30.480 --> 00:05:33.940
Because you did an S print F, not at one of the safe variants or whatever.

00:05:33.940 --> 00:05:35.360
It's been a long time since I've done C++.

00:05:35.360 --> 00:05:37.820
But tell me about that process.

00:05:38.020 --> 00:05:43.560
I don't know how many people are working on the project at this time and how much experience you all had with Rust.

00:05:43.560 --> 00:05:45.920
Why did you make that choice?

00:05:45.920 --> 00:05:46.720
And how did it go?

00:05:46.720 --> 00:05:47.420
Oh, yeah.

00:05:47.420 --> 00:05:49.160
I'm a Rust neophyte.

00:05:49.160 --> 00:05:54.940
Like, before that Christmas of 2022, I've never written a line of Rust.

00:05:54.940 --> 00:05:59.800
Most of our team are also new to Rust.

00:05:59.800 --> 00:06:02.960
Or they're new when they join LanceDB.

00:06:02.960 --> 00:06:09.200
And I think what helps is that most of them were also already proficient in C++, right?

00:06:09.200 --> 00:06:15.260
So the sort of core thinking around algorithms and design is all the same, performance and all that.

00:06:15.260 --> 00:06:20.100
And the Rust language makes it just easier to learn.

00:06:20.100 --> 00:06:20.860
You're more productive.

00:06:20.860 --> 00:06:23.260
And it's just safer.

00:06:23.640 --> 00:06:29.460
And I think one of the surprising benefits that we saw that I don't see people talking about a lot,

00:06:29.460 --> 00:06:35.160
and maybe that's just us not being very good at CMake, is that productivity piece.

00:06:35.160 --> 00:06:42.660
When we were writing lots of C++ code, we spent tons of time just wrestling with CMake to make the builds work.

00:06:42.660 --> 00:06:47.400
And once we moved to Rust, Cargo just made that so easy.

00:06:47.400 --> 00:06:49.360
Basically, we spent zero time wrestling with that.

00:06:49.360 --> 00:06:49.700
Wow.

00:06:49.700 --> 00:06:50.660
That's pretty wild.

00:06:51.080 --> 00:06:53.080
So you all were pretty new to Rust.

00:06:53.080 --> 00:07:00.960
How much do you think that LLMs, ChatGPT, Mistral, Llama, all these things, the availability to just go,

00:07:00.960 --> 00:07:03.840
hey, chat thing, how do you do this in Rust?

00:07:03.840 --> 00:07:04.740
Here it is in C++.

00:07:04.740 --> 00:07:07.080
Did you all find that beneficial?

00:07:07.080 --> 00:07:08.100
Did you all do this?

00:07:08.100 --> 00:07:10.300
We use a lot of coding aid.

00:07:10.300 --> 00:07:14.300
So from Copilot to Cursor, Continue, Zed.

00:07:14.300 --> 00:07:16.880
We try out lots of different tools as well.

00:07:16.880 --> 00:07:24.840
I think certainly the quality of the tooling matters, but at the core of it, there's that model performance.

00:07:24.840 --> 00:07:32.460
And so the models today performs pretty well on Python and TypeScript, but on Rust, much less so.

00:07:32.460 --> 00:07:33.240
Oh, interesting.

00:07:33.580 --> 00:07:33.780
Yeah.

00:07:33.780 --> 00:07:41.540
And so we always joke about, you know, God grant me the confidence of ChatGPT coming up with random Rust syntax.

00:07:41.540 --> 00:07:42.100
Yeah.

00:07:42.100 --> 00:07:43.400
I've had that as well.

00:07:43.400 --> 00:07:47.100
I asked for some, how to do this in Python, because I was feeling lazy or something.

00:07:47.100 --> 00:07:48.580
I just needed it for an example real quick.

00:07:48.580 --> 00:07:50.320
And it said, you use this.

00:07:50.320 --> 00:07:51.820
It looked completely plausible.

00:07:51.820 --> 00:07:55.220
It was converting time zones and date times.

00:07:55.220 --> 00:07:57.480
And just the time zone part of it was driving me nuts.

00:07:57.480 --> 00:07:59.240
I'm like, all right, Chat, let's do this.

00:07:59.240 --> 00:08:00.500
And it just made up stuff.

00:08:00.500 --> 00:08:04.120
So, oh, you're going to use this function or this property based on this.

00:08:04.120 --> 00:08:05.380
No, that doesn't exist.

00:08:05.380 --> 00:08:06.360
I'm like, that doesn't exist.

00:08:06.360 --> 00:08:06.720
Try again.

00:08:06.720 --> 00:08:07.580
Oh, I'm so sorry.

00:08:07.580 --> 00:08:08.620
Let me try another one.

00:08:08.620 --> 00:08:09.700
Yeah.

00:08:09.700 --> 00:08:18.040
What's interesting is I think that like in Python, if you're looking at really well-known libraries out there,

00:08:18.040 --> 00:08:19.700
it generally does much better.

00:08:19.700 --> 00:08:26.620
But even for long-known libraries that don't have as much training corpus or attention focus on it,

00:08:26.620 --> 00:08:29.460
it tends to hallucinate a lot.

00:08:29.460 --> 00:08:38.540
Even for something like Apache Arrow, like PyArrow kind of API, it's the standard for in-memory data.

00:08:38.540 --> 00:08:42.040
But the ChatGPT still makes up APIs for that.

00:08:42.040 --> 00:08:47.160
And if you're looking at in terms of like the effect on the developer community,

00:08:47.780 --> 00:08:53.760
so there are lots of times where I've seen either like Hacker News posts or comments on Hacker News posts

00:08:53.760 --> 00:08:57.280
where there's a choice between two similar libraries.

00:08:57.280 --> 00:09:05.700
And then I think the commenters are like, well, I'm choosing A because ChatGPT or Copilot generates better completions for that one.

00:09:05.700 --> 00:09:06.760
How interesting.

00:09:06.760 --> 00:09:08.380
I've never thought about that.

00:09:08.380 --> 00:09:09.100
But yeah.

00:09:09.100 --> 00:09:09.880
Wow.

00:09:09.880 --> 00:09:17.180
You know, the Python, the PSF and JetBrains Python developer survey that came out,

00:09:17.180 --> 00:09:25.820
one of the really interesting stats is 50% of the people who filled out that survey said they've been doing professional software development for less than two years.

00:09:25.820 --> 00:09:26.300
Oh, wow.

00:09:26.300 --> 00:09:27.280
That is surprising.

00:09:27.480 --> 00:09:27.640
Yeah.

00:09:27.640 --> 00:09:31.580
I mean, 50% of all Python developers, less than two years.

00:09:31.580 --> 00:09:36.620
And I think there's got to be a strong pull for, yeah, I could go research that.

00:09:36.620 --> 00:09:38.620
Let me just ask Chat first and see what it says.

00:09:38.620 --> 00:09:42.540
Or chat, ask Copilot or whatever tools they're using.

00:09:42.540 --> 00:09:43.840
That shouldn't be surprising.

00:09:44.080 --> 00:09:52.100
And also what's interesting, I think I've noticed kind of the same thing with AI versus, you know, previous generations of machine learning.

00:09:52.100 --> 00:09:54.880
In previous generations, it was all Python.

00:09:54.880 --> 00:10:00.420
With AI, there's a huge TypeScript community that has formed around AI.

00:10:00.420 --> 00:10:14.260
And then also just up in previous generations of machine learning, you had to have years of training to learn math and stats and data analytics and data science before you're a productive machine learning engineer.

00:10:14.260 --> 00:10:30.400
And I think today it's with AI, it's definitely not the case that you could quickly through some experimentation and self-learning to become a fairly proficient AI engineer, which is also a new term that's been coined in the last two years.

00:10:30.400 --> 00:10:33.640
It's going to be really interesting to see how this affects the industry.

00:10:33.640 --> 00:10:40.740
On one hand, it's going to supercharge people getting going faster, make them help them become unstuck if they're stuck on a problem.

00:10:41.060 --> 00:10:45.420
But it could also end up hollowing out deep knowledge maybe in the long term.

00:10:45.420 --> 00:10:46.500
I'm not entirely sure.

00:10:46.500 --> 00:10:47.560
We'll see.

00:10:47.560 --> 00:10:49.020
You know, that is very interesting.

00:10:49.020 --> 00:11:04.880
I do think that what we're seeing is there's a lot of focus around sort of model capabilities and then maybe like RAG and, you know, how to marry context and knowledge bases to the model through, you know, vector search and vector databases.

00:11:05.300 --> 00:11:13.820
But what we're seeing a lot is more traditional and large enterprises are now starting to adopt AI in wholesale.

00:11:13.820 --> 00:11:26.740
And the way they're thinking about it is, you know, let's build an excellence center around AI internally and make AI applications easily accessible to the rest of the company across many different groups.

00:11:26.740 --> 00:11:32.520
So you don't have to worry too much about AI infrastructure where the data store and how that interfaces with a model.

00:11:32.880 --> 00:11:39.420
And maybe just think about, you know, more at the business level and the user level, how you want these things to work.

00:11:39.420 --> 00:11:50.060
So I think that deep knowledge around AI statistics, machine learning will probably start to coalesce around these centers of innovation within these large enterprises.

00:11:50.320 --> 00:12:02.200
And of course, within, you know, AI native startups, they, you know, they all have to have much deeper knowledge in advance of the rest of the market to stay ahead of the curve.

00:12:02.200 --> 00:12:03.380
It's interesting.

00:12:03.380 --> 00:12:05.960
And, you know, I started to see these bots.

00:12:05.960 --> 00:12:08.120
We'll get to the database in just a second.

00:12:08.120 --> 00:12:11.040
I started to see these bots as kind of almost replacing search.

00:12:11.040 --> 00:12:14.380
You know, I find myself, maybe I'll just skip search.

00:12:14.380 --> 00:12:19.380
Maybe I'll just ask this thing because I got a good shot of a good answer straight away rather than.

00:12:19.380 --> 00:12:19.820
Right.

00:12:19.820 --> 00:12:21.560
And that kind of ties into this enterprise thing.

00:12:21.560 --> 00:12:24.420
Like, you know, searching Google is a little bit tricky sometimes.

00:12:24.420 --> 00:12:26.020
I use cat or whatever you're using.

00:12:26.020 --> 00:12:31.200
It's tricky because there's SEO tricks going on.

00:12:31.200 --> 00:12:32.400
There's ads.

00:12:32.400 --> 00:12:34.800
There's a lot of stuff where you're like, is this really relevant?

00:12:34.800 --> 00:12:38.120
And I think that's probably even worse than trying to search within your enterprise.

00:12:38.120 --> 00:12:40.100
Like, do we have documents on this or whatever?

00:12:40.220 --> 00:12:43.720
It's probably just like, here's a 175 email email thread.

00:12:43.720 --> 00:12:45.300
Like, no, no, thanks.

00:12:45.300 --> 00:12:46.100
This isn't going to help.

00:12:46.100 --> 00:12:49.620
Let's talk LanceDB and multimodal data.

00:12:49.620 --> 00:12:53.040
And I guess maybe that's the place before we get into the details of Lance.

00:12:53.040 --> 00:12:55.580
What is multimodal data anyway?

00:12:55.580 --> 00:12:56.360
Yeah, absolutely.

00:12:56.360 --> 00:13:03.980
So like I was saying earlier, I think with AI data, you know, vectors and embedding vectors is just scratching the surface.

00:13:03.980 --> 00:13:13.820
One of the most powerful things about AI is that it makes it a lot easier to interact with the multimodal or unstructured data that we have.

00:13:13.820 --> 00:13:25.720
And so, you know, if you're thinking about, you know, images, videos, audio forms, 3D point clouds, there's a lot that's happening in, you know, generation in things like autonomous vehicles.

00:13:25.720 --> 00:13:33.520
But even for traditional enterprises, right, you have just just oodles of like PDF documents or slide decks.

00:13:33.720 --> 00:13:45.700
And there's lots of use cases for a tool that can help those users extract insights and then possibly train models on top of those on top of that kind of data.

00:13:45.700 --> 00:13:55.320
And so if you look at data by by volume, if you look at your average like TPCH data table, it's like what, like 150 bytes or something like that per row.

00:13:55.320 --> 00:14:02.360
And then embeddings, you know, if you just look at the previous generation of open AI, it's like, you know, 25 times that.

00:14:02.360 --> 00:14:09.700
And then if you look at images and videos, you're quickly getting to these tables where even if you have the same number of rows, the data is just huge.

00:14:09.700 --> 00:14:14.960
Right. It's not username, address and a few other things are still simple.

00:14:15.080 --> 00:14:15.200
Yeah.

00:14:15.200 --> 00:14:25.960
When we talk to a lot of our customers who are trying to build this new what they call unstructured data lake, the expectation, they are already coming in with the expectation.

00:14:25.960 --> 00:14:33.520
Okay, this in terms of data volume, this is going to this is going to trounce the existing previous generations of data lakes.

00:14:33.520 --> 00:14:37.660
We had big data before, but now we've got big data in another level, right?

00:14:37.660 --> 00:14:38.960
Yeah, this is even bigger.

00:14:39.360 --> 00:14:49.920
And it's not only that each row is bigger, but, you know, previous generations of tooling, you have, you know, like users or humans manually generating one data point at a time.

00:14:49.920 --> 00:14:55.620
And now, you know, AI is generating its own training data at thousands of tokens per second, right?

00:14:55.620 --> 00:15:00.400
You know, when you're when you're doing completions, that completion itself becomes training data.

00:15:00.400 --> 00:15:06.560
And so the volume at which the data is increasing is also growing at very, very rapidly.

00:15:06.960 --> 00:15:19.080
Yeah, if anyone wants to get a sense for just how intensely the world is focused on this, they just reopened Three Mile Island, the nuclear power plant, purely to plug in a single Azure data center for AI.

00:15:19.080 --> 00:15:22.820
So, yeah, I think we're putting a lot of energy into generating data.

00:15:22.820 --> 00:15:23.760
What would you say?

00:15:23.760 --> 00:15:28.620
Well, I'm glad this data center, even if something goes wrong, the consequences will be much less dire.

00:15:29.460 --> 00:15:29.900
Yes.

00:15:29.900 --> 00:15:35.620
Honestly, I think nuclear is it's worth considering if if you rather than coal or whatever.

00:15:35.620 --> 00:15:37.240
But still, that's a whole different discussion.

00:15:37.240 --> 00:15:38.780
We don't need to go down that hole right now.

00:15:38.780 --> 00:15:40.320
Maybe maybe later at the end.

00:15:40.320 --> 00:15:40.740
Who knows?

00:15:40.740 --> 00:15:43.000
You can turn this show into talk politics instead.

00:15:43.000 --> 00:15:44.920
No, no, no, no.

00:15:44.920 --> 00:15:45.240
Please.

00:15:45.240 --> 00:15:45.540
No.

00:15:45.540 --> 00:15:46.600
It's too early.

00:15:48.460 --> 00:15:51.380
This portion of Talk Python To Me is brought to you by Sentry.

00:15:51.380 --> 00:15:52.480
Code breaks.

00:15:52.480 --> 00:15:53.780
It's a fact of life.

00:15:53.780 --> 00:15:55.920
With Sentry, you can fix it faster.

00:15:55.920 --> 00:16:01.640
As I've told you all before, we use Sentry on many of our apps and APIs here at Talk Python.

00:16:01.980 --> 00:16:07.360
I recently used Sentry to help me track down one of the weirdest bugs I've run into in a long time.

00:16:07.360 --> 00:16:08.300
Here's what happened.

00:16:08.300 --> 00:16:14.340
When signing up for our mailing list, it would crash under a non-common execution pass, like

00:16:14.340 --> 00:16:19.580
situations where someone was already subscribed or entered an invalid email address or something

00:16:19.580 --> 00:16:20.080
like this.

00:16:20.080 --> 00:16:26.060
The bizarre part was that our logging of that unusual condition itself was crashing.

00:16:26.060 --> 00:16:29.260
How is it possible for our log to crash?

00:16:29.260 --> 00:16:31.840
It's basically a glorified print statement.

00:16:32.380 --> 00:16:33.540
Well, Sentry to the rescue.

00:16:33.540 --> 00:16:38.360
I'm looking at the crash report right now, and I see way more information than you'd expect

00:16:38.360 --> 00:16:40.000
to find in any log statement.

00:16:40.000 --> 00:16:43.040
And because it's production, debuggers are out of the question.

00:16:43.040 --> 00:16:49.700
I see the traceback, of course, but also the browser version, client OS, server OS, server

00:16:49.700 --> 00:16:54.940
OS version, whether it's production or Q&A, the email and name of the person signing up.

00:16:54.940 --> 00:16:56.980
That's the person who actually experienced the crash.

00:16:56.980 --> 00:16:59.820
Dictionaries of data on the call stack and so much more.

00:16:59.820 --> 00:17:00.740
What was the problem?

00:17:01.360 --> 00:17:08.100
I initialized the logger with the string info for the level rather than the enumeration.info,

00:17:08.100 --> 00:17:10.460
which was an integer-based enum.

00:17:10.460 --> 00:17:15.780
So the logging statement would crash, saying that I could not use less than or equal to between

00:17:15.780 --> 00:17:17.020
strings and ints.

00:17:17.020 --> 00:17:18.440
Crazy town.

00:17:18.880 --> 00:17:24.620
But with Sentry, I captured it, fixed it, and I even helped the user who experienced that crash.

00:17:24.620 --> 00:17:26.060
Don't fly blind.

00:17:26.060 --> 00:17:27.740
Fix code faster with Sentry.

00:17:27.740 --> 00:17:31.780
Create your Sentry account now at talkpython.fm/sentry.

00:17:32.000 --> 00:17:38.280
And if you sign up with the code TALKPYTHON, all capital, no spaces, it's good for two free

00:17:38.280 --> 00:17:42.880
months of Sentry's business plan, which will give you up to 20 times as many monthly events

00:17:42.880 --> 00:17:44.080
as well as other features.

00:17:45.120 --> 00:17:53.240
Let's talk about something, maybe dissect, kind of like what the H2 or the one-sentence elevator pitch here for LanceDB is.

00:17:53.380 --> 00:17:57.420
So I'll read it out and we'll take it apart and you make sense of it for all of us, okay?

00:17:57.420 --> 00:18:01.920
LanceDB is a developer-friendly, open-source database for AI.

00:18:01.920 --> 00:18:07.580
From hyper-scalable vector search and advanced retrieval for RAG to streaming data,

00:18:07.580 --> 00:18:10.720
interactively exploring large-scale AI sets.

00:18:10.720 --> 00:18:13.300
Best foundation for your AI application.

00:18:13.420 --> 00:18:16.000
Yeah, so open-source for starters.

00:18:16.000 --> 00:18:20.960
Go over here to GitHub, you can see just under 10,000 stars, which is, you know, congratulations.

00:18:20.960 --> 00:18:22.200
That's excellent.

00:18:22.200 --> 00:18:27.200
Actually, across the different bits, but yeah, so really cool.

00:18:27.200 --> 00:18:29.520
And it's under the Apache 2 license.

00:18:29.520 --> 00:18:33.400
That's pretty open for people to do most things what they want, right?

00:18:33.400 --> 00:18:37.600
Maybe not build a closed-source business on top of, but other than that, right?

00:18:37.600 --> 00:18:43.380
The way we're thinking about it is there's a base layer of how do you work with AI?

00:18:43.380 --> 00:18:51.040
And we've made Lance format, which is open-source and it's a columnar format that's optimized for AI.

00:18:51.040 --> 00:19:02.880
And on top of that, we're building the different workloads that allow our users and customers to search, retrieve,

00:19:02.880 --> 00:19:11.660
run analytical queries, run training workloads across the board for all of their AI needs across the enterprise.

00:19:12.380 --> 00:19:21.620
So the repo that you're looking at, which is Lance CB open-source, that's one of the tools that we've built on top of the Lance format.

00:19:21.620 --> 00:19:31.660
And the idea is when you, super common workload is you want to start experimenting with building RAG or agents with memory,

00:19:31.660 --> 00:19:38.520
semantic search, AI-enabled semantic search, like image, text image and video search and things like that.

00:19:38.520 --> 00:19:43.240
Lance CB is sort of the easiest way for you to get started and prototype.

00:19:43.240 --> 00:19:50.500
So like you said in the beginning, the mental model would be something like equivalent to like a SQLite or DuckDB, right?

00:19:50.500 --> 00:19:52.940
So the data is just a file.

00:19:52.940 --> 00:19:55.320
There's no service to manage.

00:19:55.320 --> 00:19:58.120
There's nothing to sort of connect to.

00:19:58.120 --> 00:19:59.540
It's totally open.

00:19:59.540 --> 00:20:02.420
So you can look at it from with other tools that you have.

00:20:02.420 --> 00:20:04.740
And then the actual database runs every day.

00:20:04.740 --> 00:20:05.800
I love this idea.

00:20:05.800 --> 00:20:15.860
I believe, I think things like SQLite and Lance CB, I think this, I just have a file and it, there's a really smart engine on top of that file.

00:20:15.860 --> 00:20:25.040
Instead of a complex server architecture to run and manage and security and firewalls and all of that, migrations, all these things.

00:20:25.040 --> 00:20:29.660
It's really pretty positive, I think, to just say, you can just have a file.

00:20:29.660 --> 00:20:31.740
When your app is running, it's part of your app.

00:20:31.740 --> 00:20:35.920
And when your app shuts down, it saves the data and it shuts down with it, right?

00:20:36.020 --> 00:20:43.060
A couple of interesting things about the, you know, it's just a file is we took that a little bit further than that.

00:20:43.060 --> 00:20:51.760
One is I think that we wanted the file to be open and so that you can inspect it, you can work with it using your other tools, right?

00:20:51.760 --> 00:20:59.440
So when you're building that AI application, one is if you, you have more than just the, that search and the vector lookup.

00:20:59.440 --> 00:21:02.340
There's, you know, you might want to run SQL to look at metadata.

00:21:02.840 --> 00:21:08.740
You might have tensors that, that will upload to PyTorch training workloads and things like that.

00:21:08.740 --> 00:21:10.840
And then we wanted that layer.

00:21:10.840 --> 00:21:15.060
So we wanted that file to be accessible by all your familiar tools, right?

00:21:15.060 --> 00:21:22.380
From like pandas and polars, you know, new data frame engines like DAF, for example, and even distributed engines like Spark and Ray.

00:21:22.380 --> 00:21:29.700
That was one big thing that we focused on with, with Lance Format is making it plug in into the ecosystem.

00:21:30.160 --> 00:21:37.880
For folks who are coming with a Python background and a data background, working with AI would feel familiar.

00:21:37.880 --> 00:21:42.500
Right. A lot of people know how to do SQL to ask questions about their data, even if they're not programmers.

00:21:42.500 --> 00:21:49.220
Maybe a business analyst is like, well, I can write a query, maybe connect that result to some graph or whatever, right?

00:21:49.300 --> 00:21:56.520
This was the primary reason why we chose Apache Arrow as the standard interface and the standard type system.

00:21:56.520 --> 00:21:59.660
It's easy to plug in all these other tools on top.

00:21:59.660 --> 00:22:00.580
So good question.

00:22:00.580 --> 00:22:04.860
Now the audience out there from Carol is, how's the integration with Narwhals?

00:22:04.860 --> 00:22:08.560
I am not familiar enough with Narwhals.

00:22:08.560 --> 00:22:14.420
So Narwhals is a layer that will let you work with multiple data frame libraries.

00:22:14.420 --> 00:22:23.600
So if you wanted to work with say pandas and polars, you can use this and it will adapt depending on what the original data source is.

00:22:23.600 --> 00:22:27.660
It'll adapt everything to a subset of polars commands.

00:22:28.240 --> 00:22:33.120
So maybe the question is, how does it work with polars or pandas, right?

00:22:33.120 --> 00:22:37.260
So maybe if it does with one or the other, then there might be a way to connect it.

00:22:37.260 --> 00:22:39.960
The core interface input output is arrow.

00:22:39.960 --> 00:22:47.560
And then with LanceDB, there's on the output end, you can convert results to pandas data frames or polars data frames.

00:22:47.560 --> 00:22:57.020
And then on the input end to, I think natively, we can take pandas data frames as batches of input, but we convert that to arrow tables.

00:22:57.360 --> 00:23:02.800
And so for others, you know, it's like polars, for example, it's really easy to convert polars to like an arrow table.

00:23:02.800 --> 00:23:08.920
So a lot of that, even if it's not already automatic, it's like dot two arrow one command away.

00:23:08.920 --> 00:23:11.780
Yeah, Carol added, given it works with arrow, it probably just works.

00:23:11.780 --> 00:23:11.960
Yeah.

00:23:11.960 --> 00:23:17.620
When we talk about a file, we don't just mean your local file system, but also object store.

00:23:17.620 --> 00:23:22.540
So if you throw the data set on S3, you can just query it directly.

00:23:22.540 --> 00:23:26.020
Now, you know, you'll hit additional latency because object store.

00:23:26.480 --> 00:23:27.860
But it works.

00:23:27.860 --> 00:23:38.280
So a lot of our users see that as a bridge into production so that, you know, they use LanceDB open source to do the prototyping MVP.

00:23:38.920 --> 00:23:50.500
And then, you know, they, if their use cases fit before they need like a distributed system, it's easy for them to just say, hey, the embedded library lives in my application code.

00:23:50.500 --> 00:23:55.620
And then the I have a shared data layer in S3 or maybe an EFS or something like that.

00:23:55.620 --> 00:24:02.300
And you can run distributed queries very easily still without having to manage your own additional systems.

00:24:02.300 --> 00:24:02.660
Okay.

00:24:02.660 --> 00:24:12.840
Just use really scalable storage layer like S3 or probably anything that has an S3 compatible API, you know, like DigitalOcean.

00:24:12.840 --> 00:24:13.260
Yeah.

00:24:13.260 --> 00:24:18.860
Anything that's like a POSIX compliant interface or an S3 compliant API.

00:24:18.860 --> 00:24:21.680
So even MinIO or something like that, right?

00:24:21.680 --> 00:24:22.940
Like one of your, yeah.

00:24:22.940 --> 00:24:23.260
Yeah.

00:24:23.260 --> 00:24:28.140
I think MinIO published their own integration with LanceDB as an example as well.

00:24:28.180 --> 00:24:28.680
Oh, did they?

00:24:28.680 --> 00:24:29.100
Yeah.

00:24:29.100 --> 00:24:29.880
That's cool.

00:24:29.880 --> 00:24:33.060
Maybe tell people real quick what MinIO is.

00:24:33.060 --> 00:24:35.160
It sounds like you're familiar with it or I can.

00:24:35.160 --> 00:24:35.780
Yeah, go ahead.

00:24:35.780 --> 00:24:44.000
So MinIO is a self-hosted high-end sort of pretty complete version of what you would think of as S3.

00:24:44.000 --> 00:24:50.180
But if you don't want to use S3, you want to host your own, on your own infrastructure with your own storage layer and things like that.

00:24:50.180 --> 00:24:53.640
But you still want to talk S3 APIs and security and all that stuff.

00:24:53.640 --> 00:24:54.840
Yeah, that's what MinIO is.

00:24:54.840 --> 00:24:57.240
It's just a file really, really far.

00:24:57.720 --> 00:25:01.220
And I think the results end up speaking for itself.

00:25:01.220 --> 00:25:02.980
We're seeing really good results.

00:25:02.980 --> 00:25:12.840
And I think it really matches with the way that a lot of our users think about existing tooling and how to add AI to their existing business and applications.

00:25:12.840 --> 00:25:18.460
We might be getting just a little bit ahead of ourselves, but we'll backtrack and talk APIs and how it works and stuff.

00:25:18.460 --> 00:25:26.860
But what's the, you know, if it's just a file, what you said is obviously like a pretty broad statement by that, the way you all have implemented it.

00:25:26.860 --> 00:25:29.680
But what's the go to production story?

00:25:29.680 --> 00:25:30.180
Absolutely.

00:25:30.180 --> 00:25:34.180
If you're using SQLite and that's your middle model, you're like, well, you probably should just switch to Postgres.

00:25:34.180 --> 00:25:34.860
Right.

00:25:34.900 --> 00:25:36.500
But it doesn't sound like that's your answer.

00:25:36.500 --> 00:25:39.060
It sounds like you've thought it through on how to run it at larger scale.

00:25:39.060 --> 00:25:46.380
In terms of large scale production, there's a number of different workloads for both online and offline.

00:25:46.800 --> 00:25:48.800
So this is our commercial offering.

00:25:48.800 --> 00:25:50.560
I'm just calling it LanceDB Enterprise.

00:25:50.560 --> 00:26:03.640
And essentially, it's one distributed system that gives you a huge scale, low latency, high throughput, a system that can be backed by the same Lans data.

00:26:03.640 --> 00:26:05.840
It's just a file in S3.

00:26:06.540 --> 00:26:21.820
So the trick is to create that distributed system in a way that allows you to enjoy the cost efficiency of ObjectStore and the scalability of it while being very, very fast and very performant.

00:26:21.820 --> 00:26:49.780
And so that's the production story for our customers who are putting AI applications into production where they need high throughput or they need very large index sizes or just to manage lots of vectors and create a small portion of it without the same sort of budget constraints as like you would with OpenSearch or other tools that doesn't give you that compute storage separation.

00:26:49.780 --> 00:26:55.880
So it says it has GPU support for building vector indexes, which is pretty awesome.

00:26:55.880 --> 00:27:01.800
So databases, indexes are, they're like the magic speed dust you can sprinkle on them, obviously.

00:27:01.800 --> 00:27:04.940
What's the story with indexing and this GPU support?

00:27:04.940 --> 00:27:10.220
With traditional database indices, it's not very computationally intensive.

00:27:10.220 --> 00:27:14.340
Unfortunately for vector indices and N indices, they're quite so.

00:27:14.340 --> 00:27:21.700
So I think with Rust, you can make CPU based indexing for vector data pretty efficient.

00:27:21.700 --> 00:27:32.440
So if you're looking at, you know, hundreds of thousands or even up to like maybe, you know, 30 or 50 million vectors, CPU indexing is pretty good and can get pretty fast.

00:27:32.640 --> 00:27:42.000
But for our customers that have 15 billion vectors in one table that they need to index one index, that's going to take days.

00:27:42.000 --> 00:27:42.360
All right.

00:27:42.360 --> 00:27:47.420
So with GPU support, you know, we cut that down by more than like 15, 20 X.

00:27:47.420 --> 00:27:59.460
So then it becomes like a, something that they can do repeatedly and have a acceptable feedback loop in terms of, you know, maybe adding new data, refreshing that, that index.

00:27:59.460 --> 00:28:04.180
What is also what I love about the composable data ecosystem.

00:28:04.180 --> 00:28:11.560
And we talk about that, but there's also an extension, which I'd like to add is, is like that composable AI data ecosystem.

00:28:11.560 --> 00:28:20.160
So we talked a little bit about the benefits of like having arrow, which is just, it means if we make the input output APIs arrow, it just works with the rest of the ecosystem.

00:28:20.720 --> 00:28:29.280
This is one of the points where we get additional benefits, where with GPU support, it's easy through the arrow interface to actually talk to PyTorch.

00:28:29.280 --> 00:28:37.880
So we can use a lot of the GPU tooling in PyTorch to build a lot of our accelerated vector indexing tool tools.

00:28:37.880 --> 00:28:48.120
So that way it saves us lots of time messing with CUDA interfaces and, and, and things like that, that are not quite mature in the, the Rust ecosystem yet.

00:28:48.120 --> 00:28:49.280
It's something you don't have to build.

00:28:49.500 --> 00:28:52.520
It just gets better on its own and you just get to upgrade it.

00:28:52.520 --> 00:28:52.700
Right.

00:28:52.700 --> 00:28:53.140
Exactly.

00:28:53.140 --> 00:28:58.660
That's part of the magic of package managers and package repositories and stuff.

00:28:58.660 --> 00:28:59.220
It's really cool.

00:28:59.220 --> 00:29:01.860
So what is the workflow?

00:29:01.860 --> 00:29:05.560
Like, take me through the journey that somebody might go on.

00:29:05.560 --> 00:29:17.700
They have a bunch of data, they want to put it into the database and they want to put the resulting thing into production so that they could use it behind an API or something that they're implementing for their app or something like that.

00:29:17.780 --> 00:29:21.580
Is there a big training block of time that you do?

00:29:21.580 --> 00:29:23.960
And then you move the resulting data somewhere?

00:29:23.960 --> 00:29:29.280
Or do you just, you put it in production and start adding to it over and just keep adding to it over time?

00:29:29.280 --> 00:29:30.120
What's the workflow?

00:29:30.120 --> 00:29:34.520
With the Lansi B Enterprise, like the system that's running, you can just keep adding data to it.

00:29:34.600 --> 00:29:43.480
The indexing and all of that is automatic once you've configured it properly, which is just, you know, here's the schema and like create indices on these columns, right?

00:29:43.480 --> 00:29:46.980
Much like what you would do with a, like a Postgres table.

00:29:47.480 --> 00:29:51.620
Once you have that set up and as you add data, the indexing is automatic.

00:29:51.620 --> 00:30:00.920
And then for small amounts of data, our users typically will say like have Python dictionaries, JSON, or, you know, Pandas data frames.

00:30:00.920 --> 00:30:05.660
They can add those to the database through the API for really large scale data.

00:30:05.660 --> 00:30:13.020
So if you're working with terabytes or even petabytes of data, you don't want to be shoving that through the API like a thousand rows at a time.

00:30:13.320 --> 00:30:15.220
So this is where the open data layer comes in.

00:30:15.220 --> 00:30:27.860
So if you have a large data set and you have, whether it's Spark or Ray, you can use those large distributed systems to write data directly to S3 in the, in Lansi open source format.

00:30:27.860 --> 00:30:35.080
And then the system actually picks it up from object store and takes care of the indexing and compaction and all of that.

00:30:35.080 --> 00:30:38.860
That's sort of like when we're talking about adding data to it, it's, it's, it's both.

00:30:40.400 --> 00:30:43.820
This portion of talk Python to me is brought to you by Bluehost.

00:30:43.820 --> 00:30:47.080
Got ideas, but no idea how to build a website.

00:30:47.080 --> 00:30:49.940
Get Bluehost with their AI design tool.

00:30:49.940 --> 00:30:54.680
You can quickly generate a high quality, fast loading WordPress site instantly.

00:30:54.680 --> 00:30:58.360
Once you've nailed the look, just hit enter and your site goes live.

00:30:58.360 --> 00:30:59.380
It's really that simple.

00:30:59.380 --> 00:31:03.960
And it doesn't matter whether you're a hobbyist entrepreneur or just starting your side hustle.

00:31:03.960 --> 00:31:10.080
Bluehost has you covered with built-in marketing and e-commerce tools to help you grow and scale your website.

00:31:10.080 --> 00:31:11.040
For the long haul.

00:31:11.040 --> 00:31:20.960
Since you're listening to my show, you probably know Python, but sometimes it's better to focus on what you're creating rather than a custom built website and add another month till you launch your idea.

00:31:20.960 --> 00:31:29.580
When you upgrade to Bluehost cloud, you get a hundred percent of time and 24 seven support to ensure your site stays online through heavy traffic.

00:31:29.580 --> 00:31:33.380
Bluehost really makes building your dream website easier than ever.

00:31:33.380 --> 00:31:34.660
So what's stopping you?

00:31:34.660 --> 00:31:35.960
You've already got the vision.

00:31:35.960 --> 00:31:36.740
Make it real.

00:31:36.740 --> 00:31:41.660
Visit talkpython.fm/bluehost right now and get started today.

00:31:41.660 --> 00:31:44.160
And thank you to Bluehost for supporting the show.

00:31:44.700 --> 00:31:51.760
So I'm talking to you on a Mac mini M2 Pro with maxed out RAM and four terabytes of space or something.

00:31:51.760 --> 00:31:54.600
Can I do interesting stuff on my computer?

00:31:54.600 --> 00:31:55.860
Is it too small?

00:31:56.080 --> 00:31:57.460
It's not too small at all.

00:31:57.460 --> 00:32:01.200
One of the things about LAN format is it's all disk, right?

00:32:01.200 --> 00:32:09.260
So most of the magic that we talked about with LanceDB, at least open source, lives in that open source format layer.

00:32:09.260 --> 00:32:11.760
And that format is three things in one.

00:32:11.760 --> 00:32:18.020
One, there's the columnar format, the file format, and then there's a table format, and then there's the indexing.

00:32:18.420 --> 00:32:22.600
So the indexing is what speeds up the NN vector lookups.

00:32:22.600 --> 00:32:33.840
Typically, we've made the index disk base so that if you're looking up a few partitions in the index, it's only loading those partitions.

00:32:33.840 --> 00:32:37.420
And so the RAM requirement is actually quite small.

00:32:37.420 --> 00:32:45.900
And so on your systems, probably the more limiting factor is going to be like the disk size rather than how much memory you have.

00:32:45.900 --> 00:32:46.560
A couple of terabytes.

00:32:46.560 --> 00:32:47.900
I should be able to put some data on that.

00:32:47.900 --> 00:32:48.500
Yeah, exactly.

00:32:48.500 --> 00:32:49.920
A couple of free terabytes, that is.

00:32:49.920 --> 00:32:51.060
All right.

00:32:51.060 --> 00:32:55.040
Let's talk about the architecture a little bit, maybe a look inside.

00:32:55.040 --> 00:33:00.380
Like give us a, you've talked about some of the pieces and formats, but give us a sense here.

00:33:00.380 --> 00:33:12.560
This is sort of an architecture that includes more than just Lance, but it's more about, you know, how our customers are thinking about building their own multimodal data lake.

00:33:12.560 --> 00:33:20.720
So we call this like vector data lake or unstructured data lake, and there's no standardized terminology yet, but this is what I was mentioning before.

00:33:20.720 --> 00:33:21.180
Nice.

00:33:21.180 --> 00:33:26.820
Yeah, I'll just add for the listeners, I'll put a link to a diagram that talks about this a little bit.

00:33:26.820 --> 00:33:27.920
People can check out if they want.

00:33:27.920 --> 00:33:39.740
So the idea is from these companies perspective, they now have, on the one hand, they have lots of data that they can now take advantage of because of new AI models and applications.

00:33:39.740 --> 00:33:57.100
And on the other hand is there's lots of business units across that enterprise that wants to experiment with AI or add AI applications, whether it's agents or, you know, internal tooling or customer success, you know, video and image search.

00:33:57.100 --> 00:34:10.180
And lots of different use cases where there's a lot of like, hey, can you sprinkle some AI magic on my existing business and maybe look for that 10x in terms of ROI.

00:34:10.180 --> 00:34:12.240
You see, we have all this SharePoint data.

00:34:12.240 --> 00:34:13.220
Can you help me out?

00:34:13.220 --> 00:34:15.680
That's literally a conversation.

00:34:16.380 --> 00:34:21.960
I'm sure that it is because SharePoint is bad and you don't want anything to do with it, but people keep putting stuff into it.

00:34:21.960 --> 00:34:22.420
I don't know.

00:34:22.420 --> 00:34:33.240
It's not just about the AI capabilities, but like, how do you build this base layer and make it a lot easier to access for the rest of the folks in the company?

00:34:33.600 --> 00:34:36.980
And most of these companies aren't starting from scratch.

00:34:36.980 --> 00:34:44.840
They already have made significant investments into the data science and analytical tooling, training for their data scientists, analysts.

00:34:44.840 --> 00:34:51.420
And they also made significant investments into their existing data lake for large scale data processing, right?

00:34:51.420 --> 00:34:55.000
So you don't want to make them have to throw all that away.

00:34:55.000 --> 00:35:01.800
And instead, that open data layer is super important to plug into the rest of their existing ecosystem.

00:35:02.380 --> 00:35:13.400
And with Lance, unlike Parquet or JSON or WebDataSet, all of their multi-bono data can live in one place so that they can do search and retrieval on the vectors.

00:35:13.400 --> 00:35:15.340
They can run SQL.

00:35:15.340 --> 00:35:22.480
They can do training and data processing workloads all sort of in one engine and one piece of data.

00:35:22.480 --> 00:35:24.840
So it just makes things a lot simpler.

00:35:24.840 --> 00:35:28.400
It allows us to add a lot more performance optimizations.

00:35:29.180 --> 00:35:42.240
And then it saves, because of the size issue, it also saves these enterprises a lot in terms of cost so that they don't have to keep making different copies of different parts of the data for different parts of that workload.

00:35:42.240 --> 00:35:42.600
Right.

00:35:42.600 --> 00:35:44.340
This thing needs that format of the data.

00:35:44.340 --> 00:35:46.040
That thing needs that format of the data.

00:35:46.040 --> 00:35:46.640
Right.

00:35:46.640 --> 00:35:47.060
Exactly.

00:35:47.060 --> 00:35:56.160
Two examples that really stuck out to me, speaking with one user recently, and they're doing EDA in DuckDB.

00:35:56.460 --> 00:35:57.520
So they have lots of metadata.

00:35:57.520 --> 00:35:58.880
They're doing EDA in DuckDB.

00:35:58.880 --> 00:36:00.360
Quick acronym, please.

00:36:00.360 --> 00:36:00.960
What's EDA?

00:36:00.960 --> 00:36:02.520
Exploratory data analysis.

00:36:02.520 --> 00:36:03.300
Okay, got it.

00:36:03.300 --> 00:36:13.140
They're running SQL in DuckDB, but then they need to fetch, you know, there's like 10 columns of images and like 10 columns of multi-vectors.

00:36:13.140 --> 00:36:23.800
And so when they, at the end of where they finish running that join and filter, it takes just ages to fetch that data.

00:36:23.800 --> 00:36:28.480
So whether, because of the lack of random access support across the board.

00:36:28.480 --> 00:36:37.200
So to be, then you have to think about other tooling, either distributed engine or somehow just load all of your data in memory.

00:36:37.200 --> 00:36:38.780
And that's also not tenable.

00:36:38.780 --> 00:36:48.560
Whereas with Lance format, because you can do random access very quickly, you can actually just quickly move on to the next step in the same Python application.

00:36:48.560 --> 00:37:00.440
And because it works with DuckDB, you can get the row IDs out, fetch the rows, and then quickly move on to downstream processing, whether it's for like train a UMAP or, you know, something like that.

00:37:00.660 --> 00:37:10.740
So that is just one small example where, you know, Lance makes it so that new workloads just works with existing tooling.

00:37:10.740 --> 00:37:12.740
Yeah, that interop layer is really nice.

00:37:12.740 --> 00:37:26.220
And another example is around, and again, this is like, you know, the size changes everything where we spoke with folks that are like, you know, we manage a bunch of image and video data in, let's say, like web data set.

00:37:26.220 --> 00:37:36.260
And we want to iterate on our model and to create new features means we have to download terabytes of web data set, which are basically tarballs.

00:37:36.260 --> 00:37:44.500
Open up the tarballs, like compute the feature from that data, put it back in the tarball and then upload it.

00:37:44.500 --> 00:37:55.380
So to access one small piece and write a small piece, you often are downloading or transferring like 100x that because the large multimodal data.

00:37:55.820 --> 00:38:07.200
Whereas with something like Lance, instead of using web data set, the table format features of Lance allows you to just keep that data in place, just add a new column, which writes a new file.

00:38:07.340 --> 00:38:17.520
And then you can, the readers can just see an updated version of that with new schema and you can move forward, you know, not having to pay that transfer cost.

00:38:17.520 --> 00:38:29.000
And it's not just the infrastructure cost, but also in the time that your engineers are spending on that, managing that process and just running across maybe petabytes of that data.

00:38:29.000 --> 00:38:30.120
That's a crazy amount of data.

00:38:30.120 --> 00:38:30.440
Yeah.

00:38:30.440 --> 00:38:36.260
Let's talk through what it, what it's like to write a little bit of code, you know, connecting to the database, doing some queries or adding some data.

00:38:36.260 --> 00:38:37.980
You guys have a quick start on here.

00:38:37.980 --> 00:38:41.520
Maybe you could just talk us through real quickly on how to get started in Python with it.

00:38:41.520 --> 00:38:47.760
Overall, the goal for the quick start is you have a package that you install in seconds.

00:38:47.760 --> 00:38:59.260
And then depending on how quickly you're running through the quick start that, you know, between like a minute to five minutes, you've got a quote unquote vector database running, embedded vector database running,

00:38:59.260 --> 00:39:02.120
where you can start building applications on top, right?

00:39:02.120 --> 00:39:04.440
So the first step is very familiar.

00:39:04.440 --> 00:39:12.020
You just run pip install Lance DB and that pulls in, you know, Lance format and installs our Rust packages.

00:39:12.020 --> 00:39:15.320
And then you can start connecting to it, right?

00:39:15.320 --> 00:39:21.340
So the default is you just give it a local file path and you say Lance DB dot connect.

00:39:21.720 --> 00:39:30.060
And then that gives you a database connection, which there's two flavors of that, a synchronous for a client and an asynchronous client.

00:39:30.060 --> 00:39:30.760
I love it.

00:39:30.760 --> 00:39:34.340
So the async is basically just connect underscore async.

00:39:34.340 --> 00:39:38.360
And roughly the interfaces are roughly equivalent.

00:39:38.920 --> 00:39:45.660
There's some disparities which we're addressing now, but the main difference is you just sprinkle some await in places.

00:39:45.660 --> 00:39:52.940
The async and await keywords make concurrent programming so much easier than threads and callbacks and all that business.

00:39:52.940 --> 00:39:53.480
For sure.

00:39:53.660 --> 00:39:53.800
Yeah.

00:39:53.800 --> 00:39:59.300
So the connect bit is if people are familiar with SQLite, you just here's a URL to a file.

00:39:59.300 --> 00:40:01.260
That's pretty much you give it that.

00:40:01.260 --> 00:40:06.220
And then you have what's your naming convention on the async API?

00:40:06.220 --> 00:40:09.080
Are all the functions ending in underscore async?

00:40:09.080 --> 00:40:11.060
Or do you create like an async client?

00:40:11.060 --> 00:40:12.300
Or how do you differentiate?

00:40:12.300 --> 00:40:13.240
That's a good question.

00:40:13.240 --> 00:40:13.500
Yeah.

00:40:13.540 --> 00:40:15.020
So we create an async client.

00:40:15.020 --> 00:40:18.620
The method names are the same between the two.

00:40:18.620 --> 00:40:22.120
It's really just that initial connect call that's different.

00:40:22.120 --> 00:40:22.460
I see.

00:40:22.460 --> 00:40:27.180
But that's kind of the factory method for the client, either sync or async that comes back.

00:40:27.180 --> 00:40:29.080
And then you just, you know what you got.

00:40:29.080 --> 00:40:31.600
In the second example, we see that in action.

00:40:31.600 --> 00:40:36.920
First step is let's create a table and let's add some simple data to it.

00:40:36.920 --> 00:40:41.360
Let's say we have two fields, item and a price, right?

00:40:41.420 --> 00:40:46.700
And then an embedding vector here for the example, we're just going to use two digits for that vector.

00:40:46.700 --> 00:40:52.980
In practice, of course, it's going to be like 1,500 or, you know, 3,000 or something like that.

00:40:52.980 --> 00:40:55.280
That's hard to print on your code.

00:40:55.280 --> 00:40:56.180
Right, right.

00:40:56.180 --> 00:40:56.500
Yeah.

00:40:56.500 --> 00:40:59.960
So here we just have, you know, essentially digits of pi here.

00:40:59.960 --> 00:41:05.820
And then you can see whether you have the async connection or the sync connection.

00:41:05.820 --> 00:41:07.860
Both you call create table.

00:41:07.860 --> 00:41:09.200
You add it.

00:41:09.200 --> 00:41:10.480
You give it the table name.

00:41:10.720 --> 00:41:15.780
And then optionally, you can give it data to initialize it.

00:41:15.780 --> 00:41:24.620
So if you have data initially, the schema is inferred from the data that you provide it when you call create table.

00:41:24.980 --> 00:41:32.920
If you go to the next example, you can also initialize an empty table that has with just a schema.

00:41:33.340 --> 00:41:37.660
So here in this quick start, you can have an arrow schema.

00:41:37.660 --> 00:41:43.300
Basically, this is the same schema as the data before, but you create an empty table, and then you can add data to it later.

00:41:43.300 --> 00:41:44.980
I think I should update this.

00:41:45.060 --> 00:41:52.080
But I think what's more convenient is that we've, as you see in the box below, you can actually, we've added pedantic support.

00:41:52.080 --> 00:41:53.000
Beautiful.

00:41:53.000 --> 00:41:56.700
There's a translation layer between pedantic and arrow schemas.

00:41:56.780 --> 00:42:08.980
And so I think for a lot of our Python users, it's much easier to think in terms of pedantic objects as the data model rather than manually dealing with the PyArrow API to create a schema.

00:42:08.980 --> 00:42:17.320
We saw lots of issues where users are like, well, how do I create a fixed size list?

00:42:17.320 --> 00:42:19.820
What is a fixed size list?

00:42:19.820 --> 00:42:24.620
And why does my vector have to be that?

00:42:24.620 --> 00:42:27.440
Should I do float 32 or 64?

00:42:27.440 --> 00:42:33.500
There's lots of stuff that is just much easier to think of in terms of Python types and Python objects.

00:42:33.500 --> 00:42:36.440
I love the Python, the pydantic integration there.

00:42:36.440 --> 00:42:37.260
That's super cool.

00:42:37.260 --> 00:42:45.720
Well, the data layer used for my course's website and the podcast website and stuff is all based on Beanie and Mongo, which is async.

00:42:45.720 --> 00:42:55.920
Basically, you're writing async queries against pydantic models, which is, it's a real, got the validation, but you're not writing directly to the database and just random dictionaries.

00:42:55.920 --> 00:42:58.060
And who knows if it stays consistent.

00:42:58.060 --> 00:43:00.860
So speaking of which, you have a schema here.

00:43:00.860 --> 00:43:05.040
Is this, how hard, how strictly is this enforced, right?

00:43:05.040 --> 00:43:08.160
Is this Postgres level or is this MongoDB level?

00:43:08.160 --> 00:43:10.540
Like, probably should be this.

00:43:10.540 --> 00:43:11.120
Right.

00:43:11.120 --> 00:43:13.500
This is fairly strictly enforced.

00:43:13.500 --> 00:43:17.940
So think, think like arrow tables and, you know, writing to writing arrow tables.

00:43:18.180 --> 00:43:21.800
There's a little bit of like give around like nullability.

00:43:21.800 --> 00:43:28.040
Because a lot of times if you're providing data, the types can be inferred, which is, it's easier to do casting.

00:43:28.320 --> 00:43:37.020
And then if you're inferring like nullable versus nullable, that can get you in trouble when you're inserting data.

00:43:37.020 --> 00:43:53.600
So for example, if you have a non-nullable column that you've declared in schema, but new data coming in, sometimes that translation layer into arrow just automatically turns it into a nullable, even though you didn't give it any nulls in the data.

00:43:53.600 --> 00:43:56.720
And then when you insert it, it'll produce an error.

00:43:56.720 --> 00:44:04.900
Do you respect things like optional typing in Pydantic models to control nullability, like optional float versus float being not nullable?

00:44:04.900 --> 00:44:05.300
Yep.

00:44:05.300 --> 00:44:06.640
So we do a bunch of that as well.

00:44:06.640 --> 00:44:11.060
So in terms of pedantic integration, you know, it's another Rust project.

00:44:11.060 --> 00:44:13.880
I know we've loved working with it for a long time.

00:44:14.060 --> 00:44:24.400
If any listeners are out there that's interested in just messing with like pedantic and arrow, so that translation layer, we'd love to get some help as well.

00:44:24.400 --> 00:44:38.160
So for some of the more like complicated nested types out there, so where that's like lists of lists or lists of fixed with lists and dictionaries and that kind of thing, the translation layer is incomplete.

00:44:38.160 --> 00:44:46.720
But we know a couple of other companies and tools in the ecosystem who've also built their own kind of translation layer.

00:44:46.720 --> 00:44:58.380
So it would be really interesting, I think, if there was a community member who can lead, say like, let's create a, let's get a bunch of projects together and let's create a standardized translation layer.

00:44:58.380 --> 00:45:00.840
Not everyone just doing their own copy, right?

00:45:00.840 --> 00:45:01.540
Right, right.

00:45:01.540 --> 00:45:07.760
And then maybe like either pedantic or maybe arrow, one of the two projects can own that translation layer.

00:45:07.760 --> 00:45:11.040
I think that would be really great for the, for the ecosystem.

00:45:11.040 --> 00:45:12.580
That sounds like a fantastic idea.

00:45:12.580 --> 00:45:20.700
And let's maybe wrap up our little code sample here with talking about querying the database or doing a search because it's fun.

00:45:20.700 --> 00:45:26.060
I know it is fun to put data into a database and define schemas, but the actual purpose is to ask questions, right?

00:45:26.060 --> 00:45:26.680
That's right.

00:45:26.680 --> 00:45:27.140
That's right.

00:45:27.140 --> 00:45:34.300
We wanted to make it so that it's really familiar for people who've worked with databases and data frame engines.

00:45:34.300 --> 00:45:38.620
So the main workload for Lansi B open source is that search API.

00:45:38.620 --> 00:45:50.500
So when you, you can say table.search, you can pass in the vector, the query vector, and then you can call .limit to say how many results we want.

00:45:50.500 --> 00:45:57.640
And then a .2 underscore blah, blah, blah, blah to determine what format you want the results back.

00:45:57.640 --> 00:46:04.320
So you have two pandas here in the, in the example, but you can convert it to polars or you can just get it back as a, as a list.

00:46:04.320 --> 00:46:04.760
Awesome.

00:46:04.760 --> 00:46:06.020
Yeah, that's really cool.

00:46:06.020 --> 00:46:06.760
I love it.

00:46:06.760 --> 00:46:08.280
Oh, and also a couple of other things.

00:46:08.280 --> 00:46:21.340
So here you can also do, if you have a Pydantic model that you use as the table schema, you can also say to pedantic and pass in the model and it'll automatically return a list of pedantic objects back, back to you.

00:46:21.340 --> 00:46:28.280
So this is particularly useful for like multi-step, like agent workflows where you want that structured data.

00:46:28.280 --> 00:46:30.660
It's easy to connect it to the rest of your.

00:46:30.660 --> 00:46:31.000
Right.

00:46:31.000 --> 00:46:37.900
You would just maybe want to take your pedantic and say, turn that into JSON and hand it over to the next agent or something like that.

00:46:37.980 --> 00:46:40.160
If we sort of move forward in that example a little bit.

00:46:40.160 --> 00:46:40.400
Yeah.

00:46:40.400 --> 00:46:49.040
In addition to setting up the schema, there's also the embedding API that's really interesting so that when you create the schema.

00:46:49.040 --> 00:46:54.300
So here's an example where we can create the schema using pedantic.

00:46:54.300 --> 00:47:00.140
So in this block that I've declared the class words, which is a Lance model.

00:47:00.140 --> 00:47:05.480
And the Lance model is just a pedantic base model that knows how to convert itself to, to arrow.

00:47:06.140 --> 00:47:09.520
And then I have two fields in here, text and vector.

00:47:09.520 --> 00:47:19.820
And what we have in Lance CB is an embedding registry where you can hear, I declared a function of embedding function called func.

00:47:19.820 --> 00:47:24.400
And basically I call get registry dot get open AI dot create.

00:47:24.400 --> 00:47:26.340
And I give it the name of the embedding here.

00:47:26.340 --> 00:47:29.580
We're using ADA two, but new models have been released since.

00:47:30.040 --> 00:47:36.380
And I can use the pedantic annotations to say, hey, the text field is the source field for that function.

00:47:36.380 --> 00:47:39.520
So it's text string equals func dot source field.

00:47:39.520 --> 00:47:43.660
And then the function itself knows how many dimensions it is.

00:47:43.660 --> 00:47:48.000
So I don't need to think about like how many dimensions this, the vector, the embedding has.

00:47:48.220 --> 00:47:53.000
Once I've declared the schema, I can call a familiar dot create table workflow.

00:47:53.000 --> 00:47:56.400
And then I can call table dot add to add data to it.

00:47:56.400 --> 00:48:04.720
And here, because I've declared the embedding function in the schema, I don't actually have to generate the embeddings myself.

00:48:04.720 --> 00:48:10.540
So in this example, I'm only passing in the text field as input and Lance CB.

00:48:10.540 --> 00:48:11.740
Oh, that's really cool.

00:48:11.740 --> 00:48:21.280
Yeah, just takes care of calling the open AI API on your behalf and then adding the vectors before, you know, adding the whole batch to the table.

00:48:21.280 --> 00:48:27.100
So open AI was was our first one, but there's dozens of compatible embedding models.

00:48:27.100 --> 00:48:30.520
So pretty much anything you can pull off hugging face.

00:48:30.820 --> 00:48:39.840
And then there's lots of other vendors like cohere, for example, if you were running open source like olama, there's also a integration for that.

00:48:39.840 --> 00:48:43.120
Nice. You can use some of the on machine ones potentially as well.

00:48:43.120 --> 00:48:43.920
Exactly. Yeah.

00:48:43.920 --> 00:48:51.080
So a lot of the ones you can pull off hugging face can just run locally and it exposes the options.

00:48:51.080 --> 00:48:57.480
So if you do have for your Mac mini or or even your MacBook laptop, there are options.

00:48:57.480 --> 00:49:02.520
There are lots of hugging face models where you can specify NPS to actually make it run a little bit faster.

00:49:02.520 --> 00:49:07.000
Oh, nice. That uses the neural processing units or something instead of CPU or GPU.

00:49:07.000 --> 00:49:07.960
OK. Yeah.

00:49:07.960 --> 00:49:09.200
OK, that's cool. I didn't know that.

00:49:09.200 --> 00:49:09.960
Very nice.

00:49:09.960 --> 00:49:18.640
And it looks like I did want to ask you sort of what's the integration with things like open AI or Gemini or other things.

00:49:18.640 --> 00:49:23.680
It looks like a lot of Lance DB is kind of for you building your app that is self-contained.

00:49:23.680 --> 00:49:27.980
But also, you know, here's integration with some of the open AI API stuff.

00:49:27.980 --> 00:49:33.820
What's how much do you depend on using other external AI systems versus just your own?

00:49:33.820 --> 00:49:43.060
Right. So I think typically the embedding model is you either bring your own run locally or you call a third party API.

00:49:43.340 --> 00:49:47.860
And then the actual completion that's outside of the scope of Lance DB.

00:49:47.860 --> 00:49:57.460
So the prompt engineering and the calling the completion model once you have the context retrieved, that's not part of the Lance DB API.

00:49:57.580 --> 00:50:02.120
That said, so we integrate with, let's say, like a Langchain or a Llama Index.

00:50:02.120 --> 00:50:13.260
So if you're comfortable with that layer of sort of AI orchestration or RAG orchestration, you can use their APIs and plug in Lance DB into that.

00:50:13.260 --> 00:50:17.520
So that that takes care of a lot of that, the other parts of the workflow.

00:50:18.100 --> 00:50:37.880
And then if you're in the, let's say, like AWS ecosystem and you're familiar with, you know, Bedrock and things like that, there are a few AWS folks who've built a complete serverless RAG stack where they're using the Bedrock APIs for embedding creation and then for completion.

00:50:37.880 --> 00:50:55.320
But then they have Lance data that's sitting on S3 and they're running Lance DB open source in AWS Lambda function so that you can essentially have, you don't have to manage any servers and just be calling serverless functions to build your RAG application.

00:50:55.320 --> 00:50:56.320
Sounds super cool.

00:50:56.320 --> 00:50:58.440
All right, we're getting short, short on time.

00:50:58.440 --> 00:51:00.780
Let's wrap it up with a couple of things here.

00:51:00.780 --> 00:51:04.260
First of all, we have the open source Lance DB.

00:51:04.260 --> 00:51:07.000
You talked about the enterprise stuff as well.

00:51:07.000 --> 00:51:09.060
Let's just talk business on a little really quick.

00:51:09.060 --> 00:51:10.220
I think it's interesting.

00:51:10.220 --> 00:51:20.720
Always interesting to see companies that have maybe open core or some kind of open source foundation and how you guys are making this work for both contributing to open source, but also eating.

00:51:20.720 --> 00:51:23.260
What's the story here?

00:51:23.260 --> 00:51:26.000
I think it's always a very complicated topic.

00:51:26.000 --> 00:51:31.580
I think the way that we think about it is your journey building AI.

00:51:31.900 --> 00:51:42.360
When you're just starting out and you're doing that prototype and that MVP, a lot of times you don't even know the value of the thing that you want to build or whether it works.

00:51:42.580 --> 00:51:53.360
So why would you want to go through the hassle of managing complicated infrastructure or pay a third party vendor to just have some small amounts of data, right?

00:51:53.360 --> 00:52:04.160
So we wanted to make Lance DB open source super easy for you to just get started and also just bridge you into production in that early stage.

00:52:04.420 --> 00:52:20.160
The cloud and enterprise offerings are essentially for when you go into production and you want to have a super scalable and highly performant vector database where you still don't have to worry about the infrastructure, but you have lots more challenging systems needs.

00:52:20.160 --> 00:52:25.840
Lance DB enterprises, Lance DB cloud is it's a hosted serverless offering.

00:52:25.840 --> 00:52:37.160
So for, you know, small teams that have production needs, but maybe don't have, you know, billions of vectors and, or, or, you know, really challenging security requirements and things like that.

00:52:37.160 --> 00:52:44.320
And Lance DB enterprises, we really have, okay, I need like a thousand to 10,000 queries per second.

00:52:44.320 --> 00:52:52.140
I have just a ridiculous amount of data to catch myself before I curse and, on-prem as well.

00:52:52.140 --> 00:52:52.460
Maybe.

00:52:52.460 --> 00:52:52.900
Yeah.

00:52:52.900 --> 00:52:55.080
I need my data to live on on-prem.

00:52:55.080 --> 00:52:59.140
And so the enterprise comes in, in two different packages.

00:52:59.140 --> 00:53:10.020
One is a managed offering where you can bring your own bucket and then we just run the compute for you and give you a private link or it's, it can be fully, we call it BYOC.

00:53:10.240 --> 00:53:12.980
So it runs within the customer account.

00:53:12.980 --> 00:53:16.160
So nothing leaves the premises except for basic telemetry data.

00:53:16.160 --> 00:53:16.480
Awesome.

00:53:16.480 --> 00:53:17.380
All right.

00:53:17.380 --> 00:53:22.660
And then, you know, quickly close it out with, I guess, what's next?

00:53:22.660 --> 00:53:24.840
Like where are things going here?

00:53:24.840 --> 00:53:28.940
I see Lance DB cloud is in private beta early access mode.

00:53:28.940 --> 00:53:30.180
You know, what are you, where are y'all going?

00:53:30.180 --> 00:53:40.060
There's a couple of really exciting, first on the open source, you know, our vision for, for Lance is to make it the,

00:53:40.140 --> 00:53:42.920
standard for working with AI, right?

00:53:42.920 --> 00:53:54.780
So I think we already have lots of folks who are depending on Lance with image data, video data, large scale training, and we'll continue to, to make that better.

00:53:54.780 --> 00:53:56.960
And then add additional encoding.

00:53:56.960 --> 00:54:05.520
So for folks that not just have those, but, you know, compressing text data and the metadata that they have to make it as efficient as possible.

00:54:05.520 --> 00:54:14.920
And I think there we've, you know, between Lance and Lance DB, I think we just broke through 2.2 million monthly downloads.

00:54:14.920 --> 00:54:19.720
So we're really excited about that and, you know, that community uptake as, as well.

00:54:19.800 --> 00:54:27.520
And, you know, we're getting lots of community collaborators and contributors, and we're looking to grow that community.

00:54:27.520 --> 00:54:40.120
So on the open source side, it's, you know, better APIs, more automation around the data management, like compaction, and then just better encodings for the non-large blob types, right?

00:54:40.180 --> 00:54:43.360
So we're looking to get smaller strings, smaller strings, numerical data, and things like that.

00:54:43.360 --> 00:54:51.320
And then for Lance DB Enterprise, a lot of times, if you have to look at the whole search engine, right?

00:54:51.320 --> 00:55:06.780
So right now, our customers still have to do the chunking and embedding themselves, but we're basically looking to make it much easier for those types of workloads, not just in terms of product activity, but to also save them costs as well in terms of embeddings.

00:55:06.780 --> 00:55:13.400
So a lot of times what we find is our customers will update a table with a new version that's like 80% the same.

00:55:13.400 --> 00:55:30.440
So if we have the right APIs and manage the embedding calls for them in the, we can essentially save 80% of that cost in terms of embedding APIs without complicated sort of like query cache or embedding cache that adds more complexity.

00:55:30.580 --> 00:55:36.320
And then just deepening the, so more complete search engine for AI.

00:55:36.320 --> 00:55:49.160
And then on the offline side, complete features at scale around building that training, pre-processing, and exploratory data analysis workflow for those types of customers.

00:55:49.160 --> 00:55:53.560
Well, it looks like a really cool product and set of services and nice work.

00:55:53.560 --> 00:55:54.280
Thank you.

00:55:54.280 --> 00:55:54.940
Yeah, you bet.

00:55:54.940 --> 00:55:55.640
And thanks for being here.

00:55:55.640 --> 00:55:57.440
You know, final parting thoughts.

00:55:57.440 --> 00:55:58.760
People want to get started with Lance DB.

00:55:58.760 --> 00:55:59.360
What do you tell them?

00:55:59.360 --> 00:56:00.620
Oh, come to our discord.

00:56:00.620 --> 00:56:04.660
I don't know if I have the link, but give it to me and I'll put it in the show notes for people later.

00:56:04.660 --> 00:56:05.620
Yeah, sounds good.

00:56:05.620 --> 00:56:07.120
So come to our discord.

00:56:07.120 --> 00:56:13.220
We're all in there and it's pretty active and we respond fairly quickly.

00:56:13.220 --> 00:56:15.520
So there's not a lot of noise.

00:56:15.520 --> 00:56:24.760
So it's mostly sort of practical topics, debugging and talking about new features, maybe having a little bit of fun with new examples and things like that.

00:56:24.760 --> 00:56:25.060
Awesome.

00:56:25.060 --> 00:56:25.660
All right.

00:56:25.660 --> 00:56:26.920
Well, thanks for being here.

00:56:26.920 --> 00:56:27.500
See you later.

00:56:27.500 --> 00:56:28.080
Thank you.

00:56:28.080 --> 00:56:28.140
Thank you.

00:56:28.140 --> 00:56:31.880
This has been another episode of Talk Python to Me.

00:56:31.880 --> 00:56:33.700
Thank you to our sponsors.

00:56:33.700 --> 00:56:35.300
Be sure to check out what they're offering.

00:56:35.300 --> 00:56:36.720
It really helps support the show.

00:56:36.720 --> 00:56:38.740
Take some stress out of your life.

00:56:38.740 --> 00:56:44.500
Get notified immediately about errors and performance issues in your web or mobile applications with Sentry.

00:56:44.980 --> 00:56:45.980
Just visit talkpython.com.

00:56:45.980 --> 00:56:49.520
Just visit talkpython.fm/sentry and get started for free.

00:56:49.520 --> 00:56:53.100
And be sure to use the promo code talkpython, all one word.

00:56:53.520 --> 00:56:54.520
And this episode is brought to you by Bluehost.

00:56:54.520 --> 00:56:55.720
And this episode is brought to you by Bluehost.

00:56:55.720 --> 00:56:57.320
Do you need a website fast?

00:56:57.320 --> 00:56:58.220
Get Bluehost.

00:56:58.220 --> 00:57:03.580
Their AI builds your WordPress site in minutes and their built-in tools optimize your growth.

00:57:03.800 --> 00:57:04.540
Don't wait.

00:57:04.540 --> 00:57:08.140
Visit talkpython.fm/bluehost to get started.

00:57:08.140 --> 00:57:09.540
Want to level up your Python?

00:57:09.920 --> 00:57:13.580
We have one of the largest catalogs of Python video courses over at Talk Python.

00:57:13.580 --> 00:57:18.760
Our content ranges from true beginners to deeply advanced topics like memory and async.

00:57:18.760 --> 00:57:21.440
And best of all, there's not a subscription in sight.

00:57:21.440 --> 00:57:24.340
Check it out for yourself at training.talkpython.fm.

00:57:24.660 --> 00:57:26.440
Be sure to subscribe to the show.

00:57:26.440 --> 00:57:29.220
Open your favorite podcast app and search for Python.

00:57:29.220 --> 00:57:30.520
We should be right at the top.

00:57:30.520 --> 00:57:35.680
You can also find the iTunes feed at /itunes, the Google Play feed at /play,

00:57:35.680 --> 00:57:39.880
and the direct RSS feed at /rss on talkpython.fm.

00:57:39.880 --> 00:57:42.860
We're live streaming most of our recordings these days.

00:57:42.860 --> 00:57:46.260
If you want to be part of the show and have your comments featured on the air,

00:57:46.260 --> 00:57:50.700
be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

00:57:50.700 --> 00:57:52.760
This is your host, Michael Kennedy.

00:57:52.760 --> 00:57:54.040
Thanks so much for listening.

00:57:54.040 --> 00:57:55.200
I really appreciate it.

00:57:55.200 --> 00:57:57.120
Now get out there and write some Python code.

00:57:57.120 --> 00:58:27.100
We'll see you next time.

