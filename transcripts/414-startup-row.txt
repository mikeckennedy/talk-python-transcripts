00:00:00 At PyCon 2023, there was a section of the expo floor dedicated to new Python-based companies

00:00:06 called Startup Row. I wanted to bring their stories and the experience of talking with

00:00:11 these new startups to you. So in this episode, we talk with the founders of these companies for

00:00:16 about five to 10 minutes each. This is Talk Python To Me, episode 414, recorded on location at PyCon

00:00:23 in Salt Lake City on April 22nd, 2023.

00:00:27 Welcome to Talk Python To Me, a weekly podcast on Python. This is your host, Michael Kennedy.

00:00:44 Follow me on Mastodon, where I'm @mkennedy and follow the podcast using @talkpython,

00:00:50 both on fosstodon.org. Be careful with impersonating accounts on other instances. There are many.

00:00:56 Keep up with the show and listen to over seven years of past episodes at talkpython.fm.

00:01:01 We've started streaming most of our episodes live on YouTube. Subscribe to our YouTube channel over

00:01:06 at talkpython.fm/youtube to get notified about upcoming shows and be part of that episode.

00:01:12 This episode is brought to you by Sentry and us over at Talk Python Training. Please check out what

00:01:19 we're both offering during our segments. It really helps support the show.

00:01:24 We kick off the interviews with Devin Peterson from Ponder. Ponder is taking Moden, a distributed

00:01:29 compute library for Python, and pushing data science compute directly into the database.

00:01:34 Welcome to Talk Python here on Startup Row.

00:01:36 Thank you. Thank you.

00:01:37 Yeah, it's fantastic to have you here. You know, we met yesterday here at PyCon US,

00:01:42 and you were telling me about your project Ponder and how it's built upon Moden, the open source

00:01:49 project. And as I looked around, I'm like, everyone here has a story. And I just thought it'd be so

00:01:53 great to have you on the show along with all the others and just kind of tell your story. You know,

00:01:58 how did you, how did you get here to start up Row at PyCon?

00:02:01 Yeah, it's interesting. So Moden started as my PhD project, and I was doing my PhD at Berkeley,

00:02:07 and I started in the genomics world, trying to build large scale data science tools for,

00:02:13 for, you know, the people who actually do the science. I'm not a biologist myself. I don't know

00:02:18 the first thing about biology, honestly.

00:02:20 But you got some good programming skills, and they can always use that applied to their data,

00:02:23 right?

00:02:23 Right. The problem was we were building tools in Spark, and it was really hard for these Spark-like

00:02:27 APIs to translate natively to the way that they were reasoning about data. And like,

00:02:31 they're using Python. And so there's a very kind of natural way that scientists think about,

00:02:36 you know, interacting with data that's not Spark, right? It's not intuitive, as intuitive in Spark,

00:02:42 even PySpark, right?

00:02:43 A lot of Python people avoid databases as much as they can, at least SQL and directly talking to them like

00:02:48 that.

00:02:48 Yeah, totally. Because like, often the way, when you're exploring data there, you have a mental

00:02:53 model of how you want to interact with the data. And that is not SQL often. Like, it's just the way

00:03:00 that it is.

00:03:01 Yeah.

00:03:01 Yeah.

00:03:01 So yeah, I had a moment there where a data scientist was like, if I don't, I don't want your tool,

00:03:08 can you just make my tool run faster? And so I was like, ah, yes, wait a second, this is actually

00:03:14 a real project. And so we, I started like looking into pandas and looking into like, you know, the,

00:03:19 the world of, you know, databases and, and like the kind of academic space. And nobody had really

00:03:24 dug that deep into pandas because in, in the academic sense, everybody was like, okay, pandas

00:03:29 is just a bad database. That's what database people thought at the time. So we did a bunch of work and

00:03:35 it kind of turned out that's not the case. They're, they're totally new things. And so from there, we built

00:03:41 modem and, and now with ponder, we're kind of extending that to, to basically bridge these

00:03:46 two worlds where you can use Python, but we're, we're generating SQL on the backend and able to

00:03:51 run pandas directly in your database or your data warehouse.

00:03:54 Yeah. Fantastic. So when I first heard about what you're doing at ponder, it, I immediately thought

00:04:00 of Dask and Dask is another popular startup success, open source startup success story with

00:04:06 Matthew Rocklin and Foreman coiled and stuff. And I mean, I think they may have outgrown startup

00:04:11 row, but you know, good for them. Yeah, totally. My first thought was, okay, well, how is this

00:04:16 different than Dask? But the big difference is Dask is grid computing and yours runs in the database.

00:04:21 Yeah. For ponder, definitely open source motor and also integrates with Dask clusters as well. So

00:04:27 Dask has Dask data frame and that runs on Dask clusters. We can also run a mode in open source on,

00:04:33 on Dask clusters. It's very important to us that whatever infrastructure that you

00:04:36 have, you can run pandas on top of that. So ponder is the next level of that, where if you have,

00:04:41 if your data is in the database, it doesn't leave, right? We can just execute it directly there.

00:04:46 And you know, all of your assumptions from Python and pandas hold true in the database,

00:04:51 even though the database actually doesn't like the assumptions that you might have in pandas,

00:04:54 right? We emulate those behaviors and we we've done a lot of work to actually make that feel very

00:04:58 native. So that is a key difference with, with ponder and, and Dask though, is that your data never

00:05:04 leaves the database. So you don't have to have a separate Dask cluster to kind of pull the data

00:05:08 into and execute on it there. You can just run things natively in the database or the data warehouse.

00:05:13 So if you have a large database, you already have a probably powerful database server. Why

00:05:17 suck, transfer all the data off of that, load it into something else, analyze it and throw it away,

00:05:21 right? Just like make it run there.

00:05:23 Exactly. Exactly. Yeah. So maybe a quick elevator pitch type of thing might be like,

00:05:29 you all take pandas and turn it into SQL statements that run on the database,

00:05:33 but people get a program in pandas.

00:05:35 Yes, exactly.

00:05:36 That's exactly it. Yes. Some things that are really, really native in pandas,

00:05:41 like describe, for example, df.describe. Super, super common.

00:05:45 It seems easy. Like it just gives me some summary stats.

00:05:48 Yes, exactly. That's 300 lines of SQL.

00:05:51 No.

00:05:51 Like you wouldn't believe it looking at it though, because you know, it seems so simple and it's,

00:05:57 it is a simple, simple output, right? I want to get some summary statistics for my data,

00:06:01 but SQL is so declarative and the language itself doesn't lend itself well to this type of iterative,

00:06:07 interactive kind of like workflow. So.

00:06:09 Right. And the notebooks remember step by step, they have like a history sort of a memory,

00:06:14 whereas SQLs, every statement is standalone.

00:06:16 Exactly. So all or nothing basically. And you have to do the whole thing up front. And that's

00:06:22 the thing people love about pandas is that you can incrementally build these things up.

00:06:25 So, so we're giving that interface to SQL basically.

00:06:28 Awesome. All right. Well, let's wrap this up with a bit of a bit of a talk,

00:06:32 how you got to startup row. How'd you start this company?

00:06:35 Yeah.

00:06:35 Where are you? Like so many people are excited to take their open source work and instead of making

00:06:40 it their side job or something they do part-time at their company, make it their full-time energy.

00:06:45 And you're there. How'd you do it?

00:06:46 Yeah. So the way that we started was we talked to a lot of companies where they basically asked us,

00:06:54 can you make this work on top of our infrastructure? We didn't support, we, you know,

00:06:58 we only supported in the open source Ray and Dask. And we saw a motion there to have kind of an open

00:07:02 core model. So we follow the open core model where these more enterprise-y features like,

00:07:06 you know, security features and being able to push into data warehouses, right? Like an individual,

00:07:11 you know, consultant may not have, you know, a data warehouse. They probably don't, right? But,

00:07:16 but enterprises do. And these are the types of features that enterprises really care about. So

00:07:19 this open core model, I think, lended itself really well to our business, particularly because,

00:07:24 you know, enterprises will pay for these features. And so, yeah. And then we, we went out and we raised

00:07:30 this, a seed around and, you know, saw the opportunity to come here and be in, in PyCon startup row. And

00:07:36 fortunately, you know, it's, it's a competitive process. Really it is. Yeah. We're, we're very,

00:07:41 we feel very fortunate to be, you know, chosen among the few that are chosen here. But yeah,

00:07:46 that's kind of our journey is, is basically, you know, starting talking like, so for folks out there who are

00:07:51 like interested in this, talk to people who are using this, people who are interested in the

00:07:55 problem that you're solving and figure out where the gaps are and kind of ask questions. Don't be

00:07:59 afraid to ask, like, would you pay for this? Or how much would you pay for this? Those, those questions,

00:08:03 they're uncomfortable to ask. And like, especially the developer who's not used to presenting salesy type

00:08:09 marketing things. You always, salespeople as kind of, yeah, I got it. It's a necessary evil.

00:08:14 Totally. It totally is. Yeah. So, but, but you have to ask, because how do you know if you can kind of

00:08:19 take that next step? Unless you ask, Hey, would you pay $50 a month for this? Would you pay $10 a

00:08:24 month for this? Right. You can't know unless you, unless you really go out there and ask. So that's

00:08:29 what I would encourage folks to do if they're interested in this is, you know, find those gaps

00:08:32 and, and, and really ask the hard questions that are kind of hard, but yeah. Awesome. Well,

00:08:37 congratulations. Thanks for taking the time to talk to us. Thank you. Thank you. Yeah, you bet. Bye.

00:08:41 Next up is Generally Intelligent and Josh Albrecht. Generally Intelligent is an independent

00:08:45 research company developing AI agents with general intelligence that can be safely deployed in the

00:08:51 real world. Josh, welcome to Talk Python To Me. Hey, thanks. Hey, it's great to have you here. Tell

00:08:55 people quickly who you are. Yeah. So I'm Josh, Josh Albrecht. I'm the CTO of Generally Intelligent.

00:09:00 We're an AI research company based in San Francisco. Awesome. I love the humbleness. Generally,

00:09:06 generally intelligent, right? You're not a super genius, but no, it's a clever name. I like it.

00:09:11 Thank you. Yeah. Yeah. And you know, what, what's the problem you're solving here?

00:09:15 So yeah, we, you know, kind of, as it says on the tin, like we're working on artificial general

00:09:19 intelligence. We don't usually like to use that term because it can mean lots of different things to

00:09:22 lots of different people. But in, in general, what we're working on is making more capable,

00:09:27 safer, more robust AI systems. And in particular, we're focused on agents. So systems that can act

00:09:32 on their own. And like right now, mostly what we're focused on is agents that can work kind of in your

00:09:38 browser, on your desktop, in your code editor, those kinds of virtual environments and digital

00:09:42 environments. How much of this are you envisioning running locally versus running on a big cluster in

00:09:47 the cloud? Yeah, I think it'd be nice someday in the future to have things run totally locally. But

00:09:51 right now, a lot of these technologies do require a large cluster of GPUs, which are very expensive.

00:09:57 And most people don't even have, you know, a GPU or have a bunch of GPUs at home. So it's kind of hard

00:10:01 to actually get it running locally. Hopefully someday in the future, we'll be able to do that. But for now,

00:10:05 you'll, you'll probably need internet access to use a lot of these things. Right. Okay. So you're

00:10:08 envisioning a bunch of these agents that have access to an API that can quickly respond. Right,

00:10:15 right. Over there. Yeah. Okay. So give us some ideas, you know. Yeah. So what this looks like

00:10:21 concretely, you can imagine like a coding agent. So one thing you can do with GitHub Copilot right now is

00:10:26 you can write a function declaration and a doc string and have it generate the function. But you can imagine for a

00:10:31 coding agent, you can not only generate the function, but also generate some tests, run those tests,

00:10:35 see errors in those tests, try and fix the errors, kind of do that whole life cycle to ideally give you

00:10:40 a, you know, output that's actually a lot better. And then also, if you're thinking about this as an

00:10:44 agent, maybe it's more of a back and forth. It's not just an autocomplete in your editor,

00:10:47 but it can come back to you and say, you know, I'm sort of uncertain about this part here. What did you

00:10:51 mean? Or, hmm, like, you know, I wrote these tests, but I'm not sure if it's quite what you wanted. Or maybe,

00:10:56 you know, it's kind of running in the background and flagging different things that it sees in your

00:10:58 code base. Like maybe you made some change and it can like detect that you, your doc string is out

00:11:02 of date and kind of flag that for you. So thinking about it more as like an actual pair programmer.

00:11:06 Okay. And is it primarily focused on, yeah. Are you thinking to focus mostly on programming or is it

00:11:11 more abroad? Like I'm looking for a great deal on this classic car, go scour the internet and,

00:11:17 and, and, you know, negotiate it for me. Yeah. Yeah. So, so, you know, the company is generally

00:11:21 intelligent. So we certainly do want to be able to address all these different use cases over time.

00:11:25 I think for us right now, one of the domains that we are interested in is code, especially because

00:11:30 it's so objective. You can know if it's right or wrong, you have tests, that sort of stuff. So it's

00:11:33 a nice playground for, for ourselves and something that we can build for ourselves to kind of iterate

00:11:37 on internally, but we're not exactly sure what the final product will be. We're also training our own

00:11:42 kind of large language models. We might prioritize some stuff around those. So there's lots of

00:11:45 possibilities. We're not wedded to anything yet. Thankfully, we have the luxury to kind of take a little

00:11:49 bit of time to figure that out as a, as a research company. Yeah. That's excellent. What about science?

00:11:53 Yeah. Science is definitely a thing that we're interested in. It's pretty hard. And so, you know,

00:11:58 do we necessarily want these things like, you know, running around making things in test tubes or

00:12:02 whatever? I think that's probably a little bit harder than coding and coding is already pretty

00:12:05 hard. So I think we'll get there. That's some of the stuff that we like personally on the team are

00:12:09 really excited about to see, you know, how can we use these to uncover new cures for diseases or

00:12:13 whatever. I'm really excited for that kind of stuff a little further in the future.

00:12:16 Yeah. That'd be amazing. I was just talking to someone on the expo floor hall here,

00:12:19 about protein folding. Yeah. Right. That kind of stuff. Yeah. It's kind of been elusive for people.

00:12:24 We more or less have just tried to brute force it. Yeah. Right. With the folding at home thing.

00:12:29 Let's just run every computer and just try every possibility, but there's a lot of possibilities.

00:12:32 Yeah. Yeah. Exactly. All right. So where's Python fit in here? What are some of the tools that you're

00:12:36 using? Yeah. So Python is, we love Python. We, we basically write everything in Python or bash,

00:12:41 but you know, mostly Python or Python generates a little bit bash, you know, but it's mostly Python. So yeah,

00:12:46 we use a lot of PyTorch for our models. And then other than that, you know, let's see,

00:12:51 what other libraries do we use? I mean, we use tons of Python libraries like numpy and scikit and,

00:12:55 you know, adders and just, there's, there's so many like wonderful, you know, things that people have

00:13:00 built that we just, yeah, that are just so nice to work with. So we love the Python. You can kind of

00:13:04 take it, open it up, look at all the source and like really understand everything in that full

00:13:07 stack for us doing research. That's really valuable to be able to know everything that's going on.

00:13:11 Yeah. You have these Lego block types of things. Like what if we arranged it like this? You don't

00:13:16 have to write the whole machine learning, but you can click a few pieces together and

00:13:20 yeah, off it goes.

00:13:21 Yeah. Yeah. We build on top of Mosaic, for example, or other open source libraries that,

00:13:25 that people put together for training stuff and kind of adapted for yourself. It's so nice.

00:13:29 you can just pull things in and so easily change everything.

00:13:30 Yeah. Awesome. I must've somehow blinked along the way and I, these large language models just

00:13:36 seem to have come out of nowhere and all of a sudden, you know, AI is one of these things. It's

00:13:40 kind of where I kind of recommended stuff and now all of a sudden it's mind bogglingly good.

00:13:45 Yeah. Do things like TensorFlow and stuff work with these large language models or do you need

00:13:49 other libraries? Yeah. So TensorFlow and PyTorch are probably the two main machine learning libraries

00:13:55 that people do deep learning systems on top of. Pretty sure that, you know, GPT-3 and GPT-4 were

00:14:00 probably trained on top of PyTorch. I think a lot of the stuff at Google, like Palm and BART and those

00:14:05 types of things are trained on TensorFlow, but at the end of the day, they're, they're actually very

00:14:09 similar and they're sort of converging to kind of similar ideas too as well. So it's interesting to

00:14:13 see, to see them evolve.

00:14:14 Yeah. Fantastic. All right. Last question, close out our conversation here is we're sitting here on

00:14:19 startup row. Well, just outside of startup row, I suppose, but it's, you know, there's a bunch of people out

00:14:25 here who are working on open source projects who would like to make it somehow find a way to make

00:14:30 it their passion, their job, spend more time on it, maybe make it a company. How'd you get here?

00:14:35 Tell people your journey.

00:14:36 Yeah. So we got here and a little bit of a different route. So we, a lot of us were working at a previous

00:14:43 company called Sorceress that did apply more of an applied machine learning thing where we are taking

00:14:48 machine learning and applying it to the job of recruiting and trying to figure out, you know,

00:14:51 can we find good people online that might be a good fit for a particular position and,

00:14:54 you know, and reach out to them and get them interested in the job and that sort of stuff.

00:14:58 We went through YC with this in 2017 and we raised our series A and eventually, you know,

00:15:03 it was growing. We had a few million in revenue and customers and everything. And it just, in 2019,

00:15:06 we were looking and it felt like, you know, there's so much really interesting stuff happening in

00:15:10 self-supervised learning and in deep learning and in machine learning. And it feels like, you know,

00:15:14 recruiting is very important, but is this going to be the most important thing in the world? Is this going to

00:15:17 really be the thing that changes the world? Or will there be something a little bit larger in

00:15:21 this more general purpose AI? And the more we thought about it, the more we felt like, you know,

00:15:24 the AI stuff is probably going to have a huge impact. Like we should really be working on that.

00:15:27 So we kind of wound down the previous company, a bunch of us moved over and started up Generally

00:15:31 Intelligent. And then we've been working on stuff ever since then.

00:15:33 Fantastic. Well, I know you've got some really cool stuff where the agents can sort of look at the

00:15:39 code they're writing, think about it, evolve. And it's, it looks like a really interesting take.

00:15:43 So congratulations. And I'll put the link to the, all your work in the show notes. People can check

00:15:49 it out. Yeah. Sounds good. Yeah. Thank you very much. Yeah. Thanks for being here. It's great to

00:15:52 chat. Take care. You bet. This portion of Talk Python To Me is brought to you by Sentry. Is your Python

00:16:00 application fast or does it sometimes suffer from slowdowns and unexpected latency? Does this usually

00:16:07 only happen in production? It's really tough to track down the problems at that point, isn't it?

00:16:11 If you've looked at APM application performance monitoring products before, they may have felt

00:16:16 out of place for software teams. Many of them are more focused on legacy problems made for ops and

00:16:22 infrastructure teams to keep their infrastructure and services up and running. Sentry has just launched

00:16:28 their new APM service. And Sentry's approach to application monitoring is focused on being actionable,

00:16:34 affordable, and actually built for developers. Whether it's a slow running query or latent payment endpoint

00:16:40 that's at risk of timing out and causing sales to tank, Sentry removes the complexity and does the

00:16:46 analysis for you, surfacing the most critical performance issues so you can address them

00:16:50 immediately. Most legacy APM tools focus on an ingest everything approach, resulting in high storage

00:16:57 costs, noisy environments, and an enormous amount of telemetry data most developers will never need to

00:17:03 analyze. Sentry has taken a different approach, building the most affordable APM solution in the market.

00:17:09 They've removed the noise and extract the maximum value out of your performance data while passing

00:17:13 the savings directly onto you, especially for Talk Python listeners who use the code Talk Python.

00:17:19 So get started at talkpython.fm/sentry and be sure to use their code Talk Python all lowercase

00:17:27 so you let them know that you heard about them from us. My thanks to Sentry for keeping this podcast going strong.

00:17:36 Now we talk with Mo Sarat from Wereobots. They're building the database platform for geospatial analytics and AI.

00:17:43 Hey Mo, welcome to Talk Python.

00:17:44 Thank you so much.

00:17:45 Yeah, it's good to have you here. Let's start off with a quick introduction. Who are you?

00:17:49 Absolutely. So my name is Mo and I'm the co-founder and CEO of a company called Wereobots.

00:17:54 Wereobots' grand vision is enable every organization to drive value from data via space and time.

00:18:00 Awesome. I love it. I love it. So yeah, thanks for being here on the show. Let's dive into Wereobots.

00:18:06 What is the problem you're solving? What are you guys building?

00:18:09 Think about like, again, every single data record that is collecting on a daily basis.

00:18:14 Even like we're here right now, we're talking on this podcast at this specific location at this specific time.

00:18:20 So if you think about the space and time aspect, it's actually a very important aspect of every single piece of data that is being collected.

00:18:25 Right. If we're here next week, who knows why we're here? We could be here for a different reason. That might mean something different, right?

00:18:30 Absolutely. Yeah. So that's exactly. So that space and time lens that you can apply to your data can actually also tell you a better story about your data.

00:18:38 You can drive more value, more insights from your data if you apply that space and time lens.

00:18:43 And this is basically what we are, not necessarily like, this is exactly what we focus on in our company.

00:18:48 But more specifically, I mean, we are trying to build like kind of a database infrastructure to enable people to use that space and time lens to drive value from their data.

00:18:58 Okay. Fantastic. Now, when you talk about space and time and data, are we talking records in a time series database?

00:19:05 Are we talking regular database or NoSQL? Or could it be even things like the log file from Nginx about the visitors to my website?

00:19:14 What's the scope?

00:19:15 The scope is actually very wide. So think about any data could be structured, semi-structured, unstructured data that you have.

00:19:22 And as long as it have like kind of a geospatial aspect to it, a geospatial aspect to it means like the record or the document has, was, let's say, created in a specific location or represent an event that happened in a certain location at a certain time.

00:19:37 Or represent like, again, like an object or an asset that you monitor at different locations at different times.

00:19:44 Whatever it is, it can be stored in any of these kind of formats.

00:19:47 As long as it have this kind of geospatial aspect to it, you can definitely apply that kind of geospatial or space-time lens to it.

00:19:55 Right. Okay. So what are some of the questions you might answer with Orlot?

00:19:58 Questions that varies. I mean, so there are, it depends on the type of the data. It depends on the use case.

00:20:03 You have a horizontal technology that enables you to enable so many industry protocols, but I'll give a couple of examples.

00:20:08 Yeah, yeah. Make it concrete for us.

00:20:10 Absolutely. Think about like a logistics company or a delivery company.

00:20:14 Like the most, I mean, well-known delivery company is Amazon, right?

00:20:18 I mean, you go to the app, you purchase an item or a product, and then the whole journey of that product from the supplier to the warehouse to the driver, Amazon driver, all the way that makes it to your door.

00:20:30 There is a whole kind of, everything has a geospatial location to it, attached to it.

00:20:36 The package is moving around. You're located somewhere. Their house is a certain location.

00:20:40 Handling the logistics behind all of that, understanding how things are, you're monitoring all these assets in space and time as it reaches the door.

00:20:50 This whole journey, there's a lot of kind of data processing, data analytics happening that you have to do through, again, the geospatial kind of aspect, the geospatial contextual aspect of things.

00:21:00 So this is one example.

00:21:02 Another example could be if you're like an insurance company and you're insuring homes, for example, and you want to understand what are the nearby kind of climate conditions, natural disaster conditions compared to your home.

00:21:15 There's also the home has a location, these kind of natural disaster, weather changes at different locations all the time.

00:21:21 That will impact how you take decisions about insuring these homes.

00:21:25 Do I buy it? Do insurers want to insure it? What do I have to pay to do that?

00:21:29 Exactly.

00:21:29 That's another example, again, that space and time lens or the geospatial aspect impacts your decision when it comes to taking, it's an important decision that you take in here.

00:21:39 So that's another example.

00:21:40 So these are just a couple of use cases, but there are tons of other use cases and use cases that may not exist even yet.

00:21:47 So there's a lot of movement now into climate tech and ag tech.

00:21:51 And we are like what we're trying to do at Whereabouts is we're building the database infrastructure that enable the next generation climate tech and agriculture technology.

00:22:01 So they can ask the questions that they might have, but you already have the machinery to answer them.

00:22:06 We have machinery to answer them.

00:22:07 And they build their own secret sauce on top of our infrastructure.

00:22:11 So kind of a framework platform.

00:22:13 Absolutely.

00:22:13 Yeah.

00:22:14 Got it.

00:22:14 Yeah.

00:22:14 So Python, where's Python fit in this story?

00:22:16 That's a great question.

00:22:17 So geospatial data or the geospatial aspect of data has existed for so long.

00:22:24 As you said, we live in the space-time continuum.

00:22:26 Everything has a space-time aspect, geospatial aspect.

00:22:28 And that's why developers already have APIs to interact with geospatial data.

00:22:33 And these APIs, the language varies.

00:22:36 So there are some people that use SQL to interact with the data, process the data in either SQL databases or any other kind of SQL processing engine, right?

00:22:45 But a lot of the geospatial developers or people developing with geospatial data, they use Python.

00:22:51 There are so many libraries that use Python to actually...

00:22:55 Example of these libraries is a library called Geopandas.

00:22:57 It's a fantastic library.

00:22:58 It's an extension to Pandas to kind of wrangle and crunch geospatial data.

00:23:03 Ask questions about what things are contained in here, what things are outside of here, how far away is it?

00:23:08 Absolutely.

00:23:08 So this is what Geopandas does.

00:23:09 The only problem is that Geopandas is a library, has a great functionality, but again, it doesn't...

00:23:15 It's not enterprise-ready for the most part.

00:23:17 It doesn't scale, all that kind of stuff.

00:23:19 So what we do at WorderBots is that we provide SQL API to the user to run spatial queries on the data, but we also provide spatial Python API.

00:23:29 Like if you're using Geopandas, you can use the same API, do the heavy lifting enterprise scale kind of processing of the data using our platform, and then do the major Geopandas kind of functionality you're familiar with to, again, do the geospatial processing with it.

00:23:46 So this is how it fits within Python, and actually looking at our...

00:23:50 We have an open source software called Apache Sedona.

00:23:53 It's an Apache under the Apache license, and it has all these APIs, SQL and Python, and Python is the most popular.

00:24:01 So it's been the Python package alone on PyPy is being downloaded a million times over on a monthly basis as we're speaking today.

00:24:11 So definitely Python fits very well within our...

00:24:15 Yeah, that's awesome.

00:24:16 Absolutely, yeah.

00:24:16 So it sounds like your business, WorderBots, is a little bit following the open core model, you say?

00:24:23 Yes.

00:24:24 Let's round out our conversation here with a...

00:24:25 Talking about the business itself, how do you get a startup row?

00:24:28 We follow the open core model.

00:24:30 You're totally right about that.

00:24:31 So we have our open source software, Apache Sedona.

00:24:33 It's available for free open source, very permissive license, the Apache license 2.0.

00:24:37 And it's the open source.

00:24:39 It's also used in operational production in so many use cases.

00:24:41 There are so many contributors outside.

00:24:43 I'm the original creator of it, as well as my partner, Gia.

00:24:46 We're both the original creators, but it's grew beyond us now.

00:24:49 So there are like dozens, like 100 contributors now, something like this.

00:24:53 And we use Sedona as an open core, but we build a whole platform around it.

00:24:57 So if you want to think about like what we do compared to the other data platforms in the market, there are generic data platforms like Snowflake Databricks.

00:25:06 There are more specific specialized data platforms like MongoDB for NoSQL.

00:25:11 There's Neo4j for Graph.

00:25:12 We are...

00:25:13 Webinar Bots is like the data platform for Geospatial.

00:25:16 So this is basically...

00:25:18 And we use Apache Sedona as an open core to enable us to do all of this.

00:25:21 Fantastic.

00:25:22 All right.

00:25:23 Well, congratulations on being here.

00:25:25 Yeah.

00:25:25 I wish you success with the whole project.

00:25:27 And thanks for coming on the show.

00:25:28 Thank you so much.

00:25:29 I appreciate it.

00:25:30 Looking forward to it.

00:25:31 Yeah.

00:25:31 You bet.

00:25:31 Thank you so much.

00:25:32 Yeah.

00:25:32 Bye.

00:25:32 Time to talk to Nip Time, who have created Python programmable spreadsheets that are super powered with Python and AI.

00:25:40 I got to tell you, this product looks super awesome.

00:25:43 It looks so much better than things like Google Sheets or Excel.

00:25:46 And I can't wait to get a chance to play with it.

00:25:48 Hey, guys.

00:25:49 Hello.

00:25:50 Welcome to Talk Python To Me.

00:25:51 Yeah.

00:25:52 It's great to have you here.

00:25:53 First, introduce yourselves.

00:25:54 Thanks for having us.

00:25:55 I'm Dawa.

00:25:56 I've been doing Python professionally for, I don't know, 20 years or so.

00:26:00 I'm Jack.

00:26:02 I'm Dawa's co-founder.

00:26:03 Uh-huh.

00:26:03 Been doing Python a little less than that, but met Dawa about five years ago.

00:26:07 And we founded Nip Time about a year ago.

00:26:10 Yeah.

00:26:10 So, you know, let's dive into it.

00:26:13 Nip Time, what's the product?

00:26:15 What's the problem you're solving?

00:26:16 Yeah.

00:26:17 The proposition that we have is pretty straightforward.

00:26:19 We build a spreadsheet on top of a Jupyter notebook engine, which basically gives you all the data science, superpowers that the notebook gives you in a familiar way.

00:26:31 It's a familiar spreadsheet environment, which means that you can share your work as a Python programmer much easier with people that are not familiar with notebooks because they have the universal data canvas of a spreadsheet.

00:26:42 How interesting, because one of the big challenges data scientists often have is they work in Jupyter, they work in Jupyter, and then some executive wants to share it at a presentation or they want to continue working on it, but they're not developers.

00:26:55 So what do you do?

00:26:56 You write an Excel file and you hand that off and then you re-import it somewhere, maybe?

00:27:01 I don't know.

00:27:01 Yeah, yeah.

00:27:02 The typical flow is indeed very much like you write out the CSV, you email that to the person that is going to put it into Excel.

00:27:10 That person then creates a graph in Excel, screenshot that graph in Excel and sends it to the person that puts it in the presentation and then the CEO can do something with it.

00:27:19 It goes either in PowerPoint or it goes in Word.

00:27:21 Yeah, one of those two, right?

00:27:23 Probably the picture.

00:27:24 But that's a bunch of steps that are disassociated from data.

00:27:28 So that's one problem, right?

00:27:29 That's the one problem.

00:27:30 But since no one really sees your product in action while we're talking here, maybe just a bit of an explanation.

00:27:36 It looks very much like Google Docs or one of the online Excel, I say Doc, I mean Sheets, like one of the online spreadsheet things.

00:27:44 It doesn't look like something embedded into notebooks, right?

00:27:48 Yeah, that's right.

00:27:49 It is a spreadsheet first and foremost.

00:27:51 It looks a lot like Google Sheets, but you can run Python in it.

00:27:54 Yes.

00:27:55 You can run Python both directly in the spreadsheet cells.

00:27:57 You can also define other functionality in Python and then run that with your spreadsheet.

00:28:02 Yeah, I mean, to me, that's where the magic is, right?

00:28:05 Like Excel or Sheets, the spreadsheets more broadly are super useful.

00:28:10 But it's always like, how do I do an if statement in this dreaded thing again?

00:28:14 And how do I do a max with a condition?

00:28:17 You know, just all the programming aspect of going beyond just having raw data is just like, oh boy, this is.

00:28:23 And you just showed me an example where like here, you just write range of a thing and boom, it just writes that out.

00:28:27 Or you write a Python tertiary statement and it just runs.

00:28:32 Right.

00:28:33 Yeah, but also common things in spreadsheets that are hard are data cleaning, right?

00:28:38 You get some data from somewhere and it's not quite right.

00:28:41 And most of the time people end up doing this by hand.

00:28:43 And that's fine the first time you do it.

00:28:46 The second time and the third time it gets very annoying.

00:28:49 While if you just write a little bit of Python, you can clean data like that.

00:28:53 Yeah.

00:28:53 And then the next time you have the data, you just rerun the script and it's clean again.

00:28:57 So that's a very powerful way of doing this thing.

00:29:00 And we have a full Python environment.

00:29:03 It's not just a lightweight, you know, runs in the browser.

00:29:05 You can do pip install anything you want.

00:29:08 So you can connect any API out there, use any data, export any data.

00:29:13 It's a complete environment.

00:29:15 Yeah, how interesting.

00:29:15 Yeah, there's a little window where you can write straight Python, you know, some function that does arbitrary Python.

00:29:22 And then you can invoke it like a function in the spreadsheet, right?

00:29:25 Exactly.

00:29:26 Exactly.

00:29:26 And you can talk to things on the internet.

00:29:28 For example, I could do web scraping there.

00:29:30 Yes.

00:29:31 So you can call an API, like a currency API?

00:29:33 Yeah, exactly.

00:29:34 Okay.

00:29:35 But yeah, any REST call you want to make, you just import requests and go for it.

00:29:40 Wow.

00:29:41 So where does it run?

00:29:42 Is this PyScript, Pyodide?

00:29:44 Is this Sculpt?

00:29:45 Is this Docker on a server?

00:29:47 It's all running in a Docker container.

00:29:49 Okay.

00:29:49 Server side.

00:29:50 That's how it works.

00:29:51 And that's kind of, we do that for maximum flexibility, maximum capability.

00:29:55 So it means that anything you can install, anything you can run on a Jupyter notebook running on Linux, you can run in Neptune.

00:30:02 I see.

00:30:02 So we get full Python 3.11 or 3.10 or whatever it is.

00:30:05 Yep.

00:30:05 Yep.

00:30:06 And, you know, we ship with a bunch of useful packages pre-installed.

00:30:10 But if you want to install something else, you just open up our dependency management window.

00:30:14 Okay.

00:30:15 Install anything else you want to use.

00:30:16 It's all very manageable, very configurable.

00:30:19 Well, it looks super good to me.

00:30:21 What's the user model?

00:30:23 Do I go and create an account on your site and it's kind of like Google Docs or what's the story?

00:30:27 Yep.

00:30:28 Exactly.

00:30:28 You can try it out.

00:30:29 You can go to neptine.com in the upper right.

00:30:31 Just click log in.

00:30:32 You can create an account.

00:30:33 It's totally free to use the free tier.

00:30:36 Yeah.

00:30:36 Give it a shot.

00:30:37 Awesome.

00:30:38 All right.

00:30:39 Final question.

00:30:39 You know, how did you guys get here in Startup Row?

00:30:41 You know, everyone wants to build something amazing with open source, but how did you turn that into a business and something you can put your full time into?

00:30:48 I mean, I guess we're kind of lucky in that when we started, I, you know, I pitched it to a bunch of people that due to no fault of their own got into some money.

00:31:03 And they, they were willing to back us.

00:31:05 And then later we joined YC for the winter batch.

00:31:09 Awesome.

00:31:09 And in that process, we, you know, got a little bit of publicity and were picked up for the Startup Row.

00:31:16 Just to add to that too, based on our experience in Y Combinator, there are lots of open source tools out there that are able to get started on some commercial path just, just based on the community that they're building, based on the users.

00:31:28 Right.

00:31:29 It's a, it's a very good path.

00:31:31 I feel like this whole open core business model has really taken off in the last couple of years where it used to be a PayPal donate button.

00:31:38 And now it's a legitimate offering that businesses will buy.

00:31:41 And it's good.

00:31:42 I think it's very positive.

00:31:43 So I'm really impressed with what you guys built.

00:31:46 I think it's awesome.

00:31:47 I think people really like it.

00:31:48 Yeah.

00:31:49 So good luck.

00:31:50 Thanks for being here.

00:31:51 Thank you so much.

00:31:52 Now up is Nixla.

00:31:53 We have Federico Garza and Christian Chula here to tell us about their time series startup ready to make predictions based on an open source time series ecosystem.

00:32:02 Hey there.

00:32:02 Hello.

00:32:03 Welcome to Talk Python.

00:32:04 Hello.

00:32:04 Hello.

00:32:05 Hello.

00:32:05 Let's start with introductions.

00:32:06 Who are y'all?

00:32:07 So I am Christian Chalew.

00:32:09 I'm a co-founder of Nixla.

00:32:11 Yep.

00:32:11 Hello.

00:32:12 I'm Fede.

00:32:12 I'm CTO and co-founder of Nixla.

00:32:14 Nice to meet you both.

00:32:15 Welcome.

00:32:16 Welcome to the show.

00:32:17 Really great to have you here at PyCon.

00:32:19 And yeah, let's start with the problem y'all are trying to solve.

00:32:23 Okay.

00:32:24 Yeah.

00:32:24 So at Nixla, what we do is time series forecasting.

00:32:27 So as you know, time series forecasting is a very relevant task that a lot of companies and practitioners need to solve.

00:32:36 So essentially predicting future values of something, right?

00:32:40 It could be demand of a product or the weather.

00:32:42 So there are many use cases for forecasting.

00:32:45 It's a very common problem in industry.

00:32:47 And essentially we want to provide tools to developers, engineers, researchers to be able to do this more efficiently and with good practices.

00:32:55 And yeah, that's mostly it.

00:32:58 Right.

00:32:58 Okay.

00:32:58 So is this like a Python API?

00:33:01 Is this a database?

00:33:02 What is the actual product?

00:33:06 The product.

00:33:06 Yeah.

00:33:07 So we have an ecosystem of Python libraries and we have different libraries for different use cases.

00:33:14 For example, we have the stats forecast library, which specializes in statistical econometric models.

00:33:21 And also we have a more complex models and libraries for deep learning and machine learning applications.

00:33:30 Yeah.

00:33:31 Nice.

00:33:31 And if you train some of these models yourself on certain data, things like that, or where do you get the models from?

00:33:38 The idea behind the libraries is that you can use whatever your data is.

00:33:43 The only restriction is that it must be time series data, but you can use whatever data you have.

00:33:50 Okay.

00:33:51 Fantastic.

00:33:52 And where's its data?

00:33:54 Python's at the heart of so much data processing these days.

00:33:57 And, you know, I guess give a shout out to all the different Python packages that are already out there.

00:34:02 Maybe you want to just give a rundown on those and what they're for and then talk about them.

00:34:06 Yeah.

00:34:06 So we have like six packages right now.

00:34:10 They are all libraries on GitHub that you can install or install it with Conda.

00:34:14 And essentially they focus on different ways of approaching forecasting.

00:34:18 And they're essentially libraries built on Python, depending on some of them built on Numba.

00:34:22 Other methods are in.

00:34:24 Oh, you guys are using Numba.

00:34:25 Yeah.

00:34:26 Oh, okay.

00:34:26 And it makes a huge difference?

00:34:28 Yeah.

00:34:28 Yeah.

00:34:28 It makes a huge difference.

00:34:29 All right.

00:34:29 Yeah.

00:34:30 Tell people really, really quickly.

00:34:31 What is Numba?

00:34:32 So Numba is this library which allows you to compile just in time your code.

00:34:38 So it's a lot faster than using just plain Python.

00:34:42 And how easy is it to use?

00:34:44 It's really easy.

00:34:45 Okay.

00:34:46 Yeah.

00:34:46 In fact, we wanted to make our library more efficient and more faster.

00:34:54 And we did it in like two weeks only using Numba.

00:34:58 So it was really easy to use.

00:35:00 Yeah.

00:35:00 Awesome.

00:35:01 Awesome.

00:35:01 And some other packages you see is PyTorch.

00:35:04 So like our deep learning methods, neural forecasting approaches are built on PyTorch or PyTorch Lightning.

00:35:11 Yeah.

00:35:12 Fantastic.

00:35:13 So would you say that your business model is something of an open core model where it's kind of built on top of these libraries?

00:35:19 Absolutely.

00:35:20 Yeah.

00:35:21 Yeah.

00:35:21 So for now, we have been focusing on building these libraries, the community.

00:35:25 We have a very active community on Slack and people that use us and contribute with our code.

00:35:29 And we are building services on top of these libraries like enterprise solutions or hosting computation or even simplifying the usage further.

00:35:39 So for example, APIs where you can just simply pass your data.

00:35:43 I want to know what is going to happen next on this data.

00:35:45 Do you pass us some historical data and ask us to make predictions?

00:35:49 Make predictions and then we produce the predictions.

00:35:51 Okay.

00:35:52 Yeah.

00:35:52 This is one of the types.

00:35:53 Yeah.

00:35:54 So we are working on these different applications and services.

00:35:57 Awesome.

00:35:57 It sounds really cool.

00:35:58 Thanks.

00:35:59 So final question, how do you make your way over here to Startup Row at PyCon?

00:36:03 Like how did you start your company and how did you get here?

00:36:06 Yeah.

00:36:06 It has been a long journey.

00:36:08 I mean, we have been like for a year working on these libraries and services.

00:36:16 And right now we are focusing on building the startup, right?

00:36:20 We want to be able to do this full time for a long time and really, yeah, build something that can help people.

00:36:26 Yeah.

00:36:27 Are you looking to offer an API, like an open AI sort of model or running people's code as a service?

00:36:34 Or where are you thinking you're going?

00:36:35 Yeah.

00:36:36 Yeah.

00:36:36 That's definitely one of the options.

00:36:38 But yeah, we are finishing our funding runs.

00:36:41 And once we finish that, funding helps a lot on software development.

00:36:46 Funding helps a lot on development.

00:36:47 And yeah, so we are exploring different venues and there's very exciting things to come.

00:36:52 All right.

00:36:53 Well, we all wish you the best of luck on your project.

00:36:56 And thanks for taking the time to talk to us.

00:36:58 No, thank you for inviting me.

00:36:59 Yeah, you bet.

00:36:59 Thanks.

00:37:00 Bye.

00:37:00 Bye.

00:37:00 We'll speak with Piero Molina from PrettyBase.

00:37:02 They empower you to rapidly build, iterate, and deploy ML models with their declarative machine learning platform.

00:37:09 Piero, welcome to Talk Python To Me.

00:37:10 Thank you very much for having me.

00:37:12 Yeah, it's fantastic to have you here.

00:37:13 Quick introduction for everyone.

00:37:15 Sure.

00:37:15 So I'm Piero and I'm the CEO of PrettyBase.

00:37:18 Can tell you about PrettyBase in a second.

00:37:20 I'm also the author of Ludwig, which is an open source Python package for training machine learning models.

00:37:26 And yeah.

00:37:27 Awesome.

00:37:28 Well, great to meet you.

00:37:28 Tell us about your company.

00:37:30 Yeah.

00:37:30 So PrettyBase tries to solve the problem of the inefficiency in the development process

00:37:37 of machine learning projects.

00:37:39 Usually they take anywhere from six months to a year or even more, depending on the organization's,

00:37:45 you know, their degree of expertise in developing machine learning projects.

00:37:49 And so with using our platform, companies can get down to like from months to days of development.

00:37:56 And that makes them substantially faster.

00:37:58 Each machine learning project becomes cheaper.

00:38:01 And organizations and teams can do many more machine learning projects.

00:38:06 Yeah.

00:38:06 I mean, training is where the time and the money is spent.

00:38:09 Yeah.

00:38:10 At least computationally.

00:38:11 I mean, paying developers is expensive too.

00:38:12 Right.

00:38:13 But in terms of people say machine learning or AI, it takes all this energy.

00:38:17 And it does take energy to answer questions, but it really takes energy to train the models,

00:38:22 right?

00:38:22 Yeah.

00:38:22 Yeah.

00:38:23 Definitely.

00:38:23 There's training models is a huge part.

00:38:25 Managing the data and putting it in a shape and form that is useful for training the models.

00:38:30 There's also another big piece of the reason why these teams take so long to develop models.

00:38:37 And also, usually there's several people involved in the process.

00:38:42 There are different stakeholders.

00:38:44 Some of them are more machine learning oriented.

00:38:46 Some of them are more engineers.

00:38:47 Some of them may be analysts or product developers that need to use the models downstream.

00:38:52 And so, the handoff of the artifacts and of the whole process between these different people

00:39:00 is also source of a lot of friction.

00:39:03 And with the platform that we are building, we are trying also to reduce the friction as

00:39:06 much as possible.

00:39:07 Yeah.

00:39:07 Sounds great.

00:39:08 Is it about managing that workflow or is it about things like transfer learning and other

00:39:14 more theoretical ideas?

00:39:17 Like where exactly are you doing this?

00:39:20 So, to give you a little bit more of a picture, I would say where we are starting from is from

00:39:25 Ludwig, which is his open source project.

00:39:27 And what Ludwig allows people to do, it allows to define machine learning models and pipelines

00:39:32 in terms of a configuration file.

00:39:35 So, you don't need to write the low-level PyTorch or TensorFlow code.

00:39:39 You can just write a configuration that maps with the schema of your data.

00:39:43 Okay.

00:39:43 And that's literally all you need to get started.

00:39:46 So, it makes it substantially easier and faster to get started training models.

00:39:50 Then, if you are more experienced, you can go down and change more than 700 parameters

00:39:54 that are there and change all the details of training, of the models themselves, the pre-processing,

00:40:01 so you have full flexibility and control.

00:40:02 And you can also go all the way down to the Python code, add your own classes, register them

00:40:09 from the decorator, and they become available in the configuration.

00:40:11 Very cool.

00:40:12 This is what we have in the open source.

00:40:14 Right, right.

00:40:14 And what we're building on top of it is all the...

00:40:17 Again, you can think about this for people who may be familiar with Terraform, for instance.

00:40:22 What Terraform does for infrastructure, so they're finding your infrastructure through a configuration

00:40:26 file.

00:40:27 Ludwig does it for machine learning.

00:40:29 Got it.

00:40:29 That's a good analogy.

00:40:30 Okay.

00:40:30 And so, PratyBase, what does it, it uses this basic concept of models as configuration,

00:40:36 really, and builds on top of it all sorts of infrastructure that is needed for organizations

00:40:41 that are big enterprises to use it in the cloud.

00:40:44 Okay.

00:40:44 So, we have, like, we can deploy on cloud environments.

00:40:48 We abstract away the infrastructure aspect of it, so you can run the training of your models

00:40:53 and inference on either one small CPU machines or a thousand large GPU machines, and you don't

00:40:59 need to think about it, basically.

00:41:00 Oh, cool.

00:41:00 So, I just say train it, and if you happen to have GPUs available, you might use them.

00:41:05 Right, absolutely.

00:41:06 Okay.

00:41:06 Excellent.

00:41:07 So, where does PratyBase fit into this?

00:41:11 Like, where's the business side of this product?

00:41:14 Right, right.

00:41:14 I would say PratyBase makes it easy for teams, really, to develop machine learning products,

00:41:20 right?

00:41:20 As if, Ludwig, you can define your own configuration.

00:41:23 But it's like, you know, a single user experience, if you want, right?

00:41:26 PratyBase becomes like a multi-user experience, where, again, you deploy on the cloud, and you

00:41:32 can connect with data sources.

00:41:33 In Ludwig, you provide, like, a CSV file or a data frame, a Pandas data frame.

00:41:37 With PratyBase, you can, like, connect to Snowflake, to Databricks, to MySQL databases,

00:41:43 to S3 buckets, and do all of those things.

00:41:45 And also, there's a notion of model repositories, because when you start to train a model, the first

00:41:50 one is never the last one that you train.

00:41:52 And so, and an analogy to Git, really.

00:41:55 In Git, you have commits, and you have teams doing different commits and collaborating together.

00:41:59 In our platform, you have multiple models that are configurations, and multiple people

00:42:03 training new different models, spawning from the previous ones.

00:42:06 So there's all a lineage of models that can be compared among each other.

00:42:09 Yeah.

00:42:09 And then the very last piece is that we make it easy to deploy these models with one click of a button.

00:42:14 So you go from the data to the deployed model very, very quickly.

00:42:18 Fantastic.

00:42:18 It sounds great.

00:42:19 So final question.

00:42:20 A lot of people out there working in open source, they'd love to be here on Startup Row,

00:42:25 talking about their startup based on their project.

00:42:28 It sounds like what you built is based on the open core model, which seems to be really,

00:42:32 really successful these days.

00:42:33 You know, tell us a bit about how you got here.

00:42:36 Right.

00:42:36 So basically, I think it started from the open source, really.

00:42:39 I started developing Ludwig when I was working at Uber.

00:42:44 And initially, it was like my own project was a way for myself for being more efficient and

00:42:50 working on the next machine learning project without reinventing the wheel every single time.

00:42:54 And I built that because I'm lazy and I don't want like when I do one thing more than twice,

00:42:59 then I automate it for myself, really.

00:43:01 Productive laziness or something like this.

00:43:04 And so then other people in the company started using it.

00:43:07 And that convinced me that making it open source, also because it was built on top of other open

00:43:12 source projects, would have been a great way to both have people contribute to it and improve

00:43:16 it and also give back to the community.

00:43:17 Because again, I was using myself a lot of open source projects to build it.

00:43:21 Right.

00:43:22 And then from there, I made it so that we donated the project to the Linux Foundation.

00:43:27 So now it's backed by the Linux Foundation.

00:43:29 And also the governance is open as opposed to what it was before when I was at Uber.

00:43:33 And from there, actually, you know, I met a bunch of people, some of my co-founders at

00:43:38 the company, thanks to the project.

00:43:40 And we decided that, so for instance, one of them is Professor Chris from Stanford.

00:43:44 He was developing a similar system that was closed internally at Apple.

00:43:48 And so we said, this thing worked at Uber, worked at Apple, works in the open source,

00:43:52 open source.

00:43:52 Let's make a company out of this.

00:43:54 Right.

00:43:54 Fantastic.

00:43:55 Yeah.

00:43:55 Solving some problems for these big teams.

00:43:57 Right.

00:43:58 Excellent.

00:43:58 Well, best of luck on your company.

00:44:01 Thank you very much, Mike.

00:44:01 Yeah.

00:44:02 Thanks for being here.

00:44:02 Yeah.

00:44:03 Absolutely.

00:44:03 A pleasure.

00:44:04 Thank you so much.

00:44:04 We'll finish up our stroll down startup lane by talking with the folks at Pinecone.

00:44:08 We have Nikhil Rao to talk about the pure Python full stack web app platform that they've

00:44:14 built.

00:44:14 Nikhil, welcome to Talk Python.

00:44:16 Yeah.

00:44:16 Great to be here.

00:44:17 Thanks for having me.

00:44:17 It's great to have you here.

00:44:19 I love going through all the different projects on startup row and talking about them and

00:44:24 shedding a little light on them.

00:44:25 So happy to have you here.

00:44:26 Yeah.

00:44:27 Yeah.

00:44:27 Give a quick introduction on yourself.

00:44:28 Yeah.

00:44:29 So I'm Nikhil.

00:44:29 I'm the CEO, co-founder of Pinecone.

00:44:32 And we're building a way to make web apps in pure Python.

00:44:35 So we have an open source framework and anyone can install this and basically start creating

00:44:39 their apps front end and back end using Python.

00:44:42 Our company went through the recent Y Combinator batch, just ended the winter 23 batch.

00:44:47 And recently we raised our seed ran and starting to hire out and pretty much grow out our project

00:44:51 and company from here.

00:44:52 Okay.

00:44:52 Well, awesome.

00:44:53 Congratulations.

00:44:54 That sounds really cool.

00:44:55 Give us an idea of, I guess, you know, why do you build this, right?

00:44:59 We've got Flask.

00:45:00 We've got Django.

00:45:00 Yeah.

00:45:01 I mean, heck, we even have Ruby if you really want it.

00:45:03 Yeah.

00:45:03 There's a lot.

00:45:04 So previous to this, like you mentioned, there's frameworks like Flask and Django.

00:45:08 And whenever you wanted to, a Python developer wanted to make a web app, they use something

00:45:12 like this, but you always have to pair it with another front end library.

00:45:14 So you can't just make your front end using Python.

00:45:16 You still have to end up using JavaScript, HTML, React, stuff like that for your front end.

00:45:20 And so a lot of people, if you're coming from a Python background, it's a lot of work

00:45:24 to kind of get started with these.

00:45:26 It's a different language, different tool set.

00:45:27 So we really wanted something where Python developers can just use these tools they already know

00:45:31 and be able to make these web apps without having to go learn something completely different.

00:45:35 So as opposed to these tools like Flask and Django, we're very focused on unifying the front

00:45:39 end and back end into one framework.

00:45:40 So you don't need a separate front end and back end.

00:45:42 And that allows us to kind of, the user can just focus on the logic of their app and not

00:45:47 kind of these technical details on the networking and all this other stuff.

00:45:49 Yeah.

00:45:50 It sounds interesting.

00:45:50 I mean, I know many Python people who don't want to do JavaScript.

00:45:54 Yeah.

00:45:55 They don't want to do multiple languages.

00:45:57 But, you know, it's traditionally, at least in the web framework world, you're speaking

00:46:02 many, many languages.

00:46:03 You're speaking HTML, CSS, JavaScript is a big one.

00:46:08 And honestly, I think that there was a period where people were super invested in JavaScript

00:46:13 and thought that was kind of the right way or the necessary way.

00:46:17 And that would take away a lot of what's important about the web framework, right?

00:46:21 Like, well, it doesn't matter if it's Flask or Django, we're just going to return JSON

00:46:25 anyway because it's all Angular.

00:46:26 So who cares, right?

00:46:27 Yeah.

00:46:27 But I don't think that's where people really, most, many people, at least the people choosing

00:46:31 Python want to be.

00:46:33 And so, yeah, how is your stuff different?

00:46:35 So I think exactly what you said before this, to make a serious web app, you always have to

00:46:39 go to JavaScript.

00:46:40 And what we're really trying to do is make everything in Python, including your front end.

00:46:44 And so basically, yeah, we're trying to integrate the two together.

00:46:47 So basically, you don't have to go learn these technical details you didn't want before.

00:46:52 We realized for all the logic of your app, you're using Python anyway.

00:46:55 Like Python's used in so many industries, databases, ML, AI, infrastructure.

00:47:00 And when these people want to make a front end, it is possible to make JavaScript or these

00:47:04 JavaScript front ends.

00:47:05 But that's a lot of overhead.

00:47:06 And before our kind of our framework, there are different low code tools to make front

00:47:11 ends without JavaScript.

00:47:11 But they all kind of have a limit.

00:47:13 And they all have a graduation risk is what we found.

00:47:15 So you can start making your UI.

00:47:17 Can you make any website with them?

00:47:19 Right.

00:47:20 Like Streamlit and Anvil are both notable ones that kind of come to mind.

00:47:24 But neither of them, I like them both a lot, but neither of them are necessarily like, I'm

00:47:28 just going to build a general purpose web app.

00:47:31 They're focused in their particular area.

00:47:33 Yes, exactly.

00:47:34 So I've used tools like Streamlit, Gradio in the past.

00:47:36 And a lot of that was inspiration for Pinecone.

00:47:38 It's really great because it's super easy to get started with.

00:47:41 You don't have to go learn these things, but they all have this kind of ceiling you

00:47:44 hit.

00:47:44 So they're mostly good for like data science apps, dashboard apps.

00:47:48 But as you try to expand your app into like a full stack web app, start adding these new

00:47:51 features, a lot of times you find these frameworks don't really scale with your ideas.

00:47:55 And your two options are either you have to kind of constrain your idea into what these

00:47:59 vendors offer you, or you use that for prototyping.

00:48:02 And when you're making a customer facing production app, you scrap it and go to like a JavaScript

00:48:06 React world.

00:48:07 So what we're really trying to do is make something like these Anvil or Streamlit easy

00:48:11 to get started with for Python developers.

00:48:12 But as you want to expand to these complex cases, you should be able to stay in our framework

00:48:16 and we should be able to handle that also.

00:48:18 Interesting.

00:48:18 So how does the front end interactivity work if it's Python?

00:48:21 Yeah.

00:48:22 And this is also where I think we're a bit different.

00:48:23 We're trying to really leverage a lot of the web dev ecosystem and not recreate everything

00:48:27 from scratch.

00:48:28 So for the front end, we leverage React and Next.js.

00:48:30 So our front end compiles down to a Next.js app.

00:48:32 And from this...

00:48:33 Oh, you're transpiling the Python?

00:48:35 We transpile the Python to Next.js.

00:48:36 And this gives you a lot of great features.

00:48:38 You get single page app features from Next.js, a lot of these performance features.

00:48:41 And that means from our perspective, we don't have to recreate all this stuff.

00:48:45 And also, we don't have to create components one by one.

00:48:48 We just leverage React.

00:48:49 And for what we do in Pinecode for the front end is we just wrap React components and make

00:48:53 them accessible.

00:48:53 So even if we don't offer something, and other local tools, sometimes if they don't offer a

00:48:58 component you need, you may be kind of constrained in what you can build.

00:49:01 We easily have a way for anyone to wrap their own third-party React libraries.

00:49:05 So we're really trying to make the existing stuff out there accessible rather than recreating

00:49:09 it.

00:49:09 Yeah.

00:49:10 So you can sort of extend it with React if you get boxed in.

00:49:13 That's your escape hatch.

00:49:13 Exactly.

00:49:14 Okay.

00:49:15 So that's kind of how our front end works.

00:49:17 And for the back end, we use FastAPI to handle all the states.

00:49:20 So the user state is all on the back end on the server.

00:49:22 And this is what allows us to pretty much keep everything in Python.

00:49:25 So none of the logic is transpiled to JavaScript, only the React.

00:49:28 And all the logic stays in Python.

00:49:30 So you can use any of your existing Python libraries, any existing tools.

00:49:33 You don't have to wait for us to kind of make these integrations.

00:49:36 So it's kind of leveraging React, but also leveraging Python and kind of bringing them together.

00:49:40 What's the deployment look like?

00:49:41 So we're working on an easy deployment.

00:49:43 So you can just type PC deploy.

00:49:45 We'll set up all your infrastructure and you'll get a URL back with your app live.

00:49:48 But also we're an open source framework.

00:49:50 So it's also very easy to self-host and self-deploy.

00:49:52 And so what we're really trying to do is make it accessible and easy, but never kind of lock

00:49:57 you into our framework.

00:49:58 I see.

00:49:58 So I could put like Nginx in front of it or something.

00:50:01 Exactly.

00:50:01 Like, so right now we're still working on our hosting deployment.

00:50:03 So everyone right now who's deployed is hosting on AWS DigitalOcean or a tool like this

00:50:07 with Nginx.

00:50:08 Yeah.

00:50:08 So it integrates just like you would deploy a Flask or React app.

00:50:11 Got it.

00:50:11 But we're really trying to make an optimized hosting service around this later.

00:50:14 Yeah, sure.

00:50:15 It makes sense.

00:50:16 All right.

00:50:16 Sounds like a great product.

00:50:17 Thanks, sir.

00:50:18 Final question here.

00:50:20 You know, how'd you get here?

00:50:21 How'd you start the company?

00:50:22 How'd you land on Startup Row?

00:50:24 I mean, you talked about Y Combinator a little.

00:50:25 Yeah.

00:50:26 So I talked a little bit.

00:50:27 We did the Y Combinator batch.

00:50:28 And really the idea is not only having an open source framework, but having like a business

00:50:33 model around it and being able to create like these features around it.

00:50:36 So we're really focused on kind of being similar to have an open source framework, similar to

00:50:41 like how Vercel has Next.js and their hosted version and kind of bringing that to the Python

00:50:44 community.

00:50:45 So Python is like one of the fastest growing languages.

00:50:47 Obviously, like that's why Python is so big.

00:50:49 And for the web dev part, there's not really a good ecosystem for that.

00:50:53 So when people want to share their ideas, we're really trying to become that de facto

00:50:56 way for Python developers to create their apps and share.

00:50:59 And so, yeah, basically working on our hosting service, growing out our team now and trying

00:51:03 to build up all this like ecosystem around it so people can easily get their ideas out

00:51:07 to the world.

00:51:07 Awesome.

00:51:08 Well, congratulations and thanks for being here.

00:51:10 This has been another episode of Talk Python To Me.

00:51:13 Thank you to our sponsors.

00:51:15 Be sure to check out what they're offering.

00:51:17 It really helps support the show.

00:51:18 Take some stress out of your life.

00:51:21 Get notified immediately about errors and performance issues in your web or mobile applications with

00:51:26 Sentry.

00:51:27 Just visit talkpython.fm/sentry and get started for free.

00:51:32 And be sure to use the promo code talkpython, all one word.

00:51:35 Want to level up your Python?

00:51:37 We have one of the largest catalogs of Python video courses over at Talk Python.

00:51:41 Our content ranges from true beginners to deeply advanced topics like memory and async.

00:51:46 And best of all, there's not a subscription in sight.

00:51:49 Check it out for yourself at training.talkpython.fm.

00:51:52 Be sure to subscribe to the show.

00:51:54 Open your favorite podcast app and search for Python.

00:51:57 We should be right at the top.

00:51:58 You can also find the iTunes feed at /itunes, the Google Play feed at /play,

00:52:03 and the direct RSS feed at /rss on talkpython.fm.

00:52:07 We're live streaming most of our recordings these days.

00:52:11 If you want to be part of the show and have your comments featured on the air,

00:52:14 be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

00:52:19 This is your host, Michael Kennedy.

00:52:20 Thanks so much for listening.

00:52:22 I really appreciate it.

00:52:23 Now get out there and write some Python code.

00:52:25 I'll see you next time.

