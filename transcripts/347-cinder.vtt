WEBVTT

00:00:00.001 --> 00:00:04.840
The team at Instagram dropped the performance bomb on the Python world when they open-sourced

00:00:04.840 --> 00:00:08.060
Cinder, their performance-oriented fork of CPython.

00:00:08.060 --> 00:00:14.320
It contains a number of performance optimizations, including bytecode inline caching, eager evaluation

00:00:14.320 --> 00:00:20.120
of coroutines, a method-at-a-time JIT, and an experimental bytecode compiler that uses

00:00:20.120 --> 00:00:25.220
type annotations to emit type-specialized bytecode that performs better in the JIT.

00:00:25.220 --> 00:00:29.980
While it's not a general-purpose runtime we can all pick up and use, it contains many

00:00:29.980 --> 00:00:34.340
powerful features and optimizations that make their way back to mainline Python.

00:00:34.340 --> 00:00:37.740
We welcome Dino Velen to the show to dive into Cinder.

00:00:37.740 --> 00:00:44.060
This is Talk Python to Me, episode 347, recorded November 29th, 2021.

00:00:57.260 --> 00:01:00.440
Welcome to Talk Python to Me, a weekly podcast on Python.

00:01:00.440 --> 00:01:02.160
This is your host, Michael Kennedy.

00:01:02.160 --> 00:01:06.380
Follow me on Twitter where I'm @mkennedy, and keep up with the show and listen to past

00:01:06.380 --> 00:01:08.340
episodes at talkpython.fm.

00:01:08.340 --> 00:01:11.440
And follow the show on Twitter via at Talk Python.

00:01:11.440 --> 00:01:14.980
We've started streaming most of our episodes live on YouTube.

00:01:14.980 --> 00:01:20.760
Subscribe to our YouTube channel over at talkpython.fm/youtube to get notified about upcoming

00:01:20.760 --> 00:01:22.560
shows and be part of that episode.

00:01:23.400 --> 00:01:26.680
This episode is brought to you by Sentry and TopTal.

00:01:26.680 --> 00:01:28.960
Please check out what they're offering during their segments.

00:01:28.960 --> 00:01:30.420
It really helps support the show.

00:01:30.420 --> 00:01:33.340
Dino, welcome to Talk Python to Me.

00:01:33.340 --> 00:01:34.000
Hi, Michael.

00:01:34.000 --> 00:01:34.640
Thanks for having me.

00:01:34.640 --> 00:01:36.660
I'm really excited to talk to you.

00:01:36.660 --> 00:01:41.300
You've been involved in a lot of projects that I've wanted to talk to you about over the years

00:01:41.300 --> 00:01:42.840
and haven't yet.

00:01:42.840 --> 00:01:44.700
So we're going to touch on a couple of those.

00:01:44.700 --> 00:01:51.060
But we've got some really big news around Sender and some performance stuff that you all over at

00:01:51.060 --> 00:01:53.220
Instagram are doing to try to make Python faster.

00:01:53.220 --> 00:01:56.800
You did a really cool Python keynote, or not keynote, but talk on that.

00:01:56.800 --> 00:02:03.940
So we're going to dive deep into this alternate reality runtime of Cpython called Sender that you

00:02:03.940 --> 00:02:04.260
all have created.

00:02:04.260 --> 00:02:05.020
That's going to be a lot of fun.

00:02:05.020 --> 00:02:05.320
Yeah.

00:02:05.320 --> 00:02:07.100
And it's a slightly alternate reality.

00:02:07.100 --> 00:02:09.240
It's not that much of an alternate reality.

00:02:09.240 --> 00:02:09.980
Just a little bit.

00:02:10.080 --> 00:02:13.360
Before we do, though, let's just hear your story.

00:02:13.360 --> 00:02:15.220
How'd you get into programming in Python?

00:02:15.220 --> 00:02:18.100
I started programming when I was a teenager.

00:02:18.100 --> 00:02:22.080
I got into computers initially really through BBSs.

00:02:22.080 --> 00:02:23.500
Oh, yes.

00:02:23.500 --> 00:02:24.180
Maybe.

00:02:24.180 --> 00:02:25.680
Was this pre-internet?

00:02:25.680 --> 00:02:28.280
This was pre- this is like dial up only.

00:02:28.280 --> 00:02:30.000
You would dial into the BBS?

00:02:30.000 --> 00:02:31.160
Oh, my gosh.

00:02:31.160 --> 00:02:31.520
Yeah.

00:02:31.520 --> 00:02:33.040
Like, you know, I had a modem.

00:02:33.040 --> 00:02:37.440
Someone else had a modem sitting in their home waiting for people to call in.

00:02:37.440 --> 00:02:39.760
You'd log in, send emails.

00:02:39.860 --> 00:02:40.780
Post messages.

00:02:40.780 --> 00:02:43.080
Take your turns on games.

00:02:43.080 --> 00:02:44.140
Log out.

00:02:44.140 --> 00:02:47.320
And someone else could log in and respond to your emails.

00:02:47.320 --> 00:02:48.480
It was so amazing.

00:02:48.480 --> 00:02:54.040
And send email meant wait for another BBS to dial in to connect to that one to like sync

00:02:54.040 --> 00:02:55.540
its local batch of emails.

00:02:55.540 --> 00:02:57.420
It was like peer-to-peer emails.

00:02:57.420 --> 00:02:57.960
So weird.

00:02:58.180 --> 00:02:58.320
Yeah.

00:02:58.320 --> 00:03:01.280
I mean, or like there's a lot of local emails, right?

00:03:01.280 --> 00:03:02.360
Where it's just on.

00:03:02.360 --> 00:03:02.640
Right.

00:03:02.640 --> 00:03:05.840
You're waiting for the other person to have a chance to log in.

00:03:05.840 --> 00:03:10.160
But yeah, there's also that network, like a couple of different big networks.

00:03:10.560 --> 00:03:12.680
It was such a different time.

00:03:12.680 --> 00:03:13.820
It was such a different time.

00:03:13.820 --> 00:03:16.740
I was not super into this as much.

00:03:16.740 --> 00:03:18.220
My brother was really into it.

00:03:18.220 --> 00:03:21.140
We had two phone lines so that we could do more of this.

00:03:21.140 --> 00:03:24.500
Did you ever play Trade Wars or any of the games that were on there?

00:03:24.500 --> 00:03:25.780
Trade Wars was awesome.

00:03:25.780 --> 00:03:27.040
It was so good.

00:03:27.140 --> 00:03:28.800
I think I would still enjoy Trade Wars.

00:03:28.800 --> 00:03:29.640
It was so good.

00:03:29.640 --> 00:03:31.500
I was still playing Trade Wars in college.

00:03:31.500 --> 00:03:37.600
We like formed teams and like we're trying to take over some Trade Wars that was available

00:03:37.600 --> 00:03:41.920
over the internet, actually, that you could like Telnet into and play.

00:03:41.920 --> 00:03:47.020
And a lot of this BBS stuff had sort of found a home over Telnet for a while, hadn't it?

00:03:47.020 --> 00:03:47.340
Yeah.

00:03:47.340 --> 00:03:47.680
Yeah.

00:03:47.680 --> 00:03:54.520
I think the main BBS software that I used there was used in St. Louis where I grew up,

00:03:54.840 --> 00:04:01.060
which was World War IV with, I think it's still around and like available for you to

00:04:01.060 --> 00:04:07.560
like, if you really want to host it on some internet server, but who's going to do that?

00:04:07.560 --> 00:04:08.700
Incredible.

00:04:08.700 --> 00:04:09.480
Okay.

00:04:09.480 --> 00:04:12.980
So how's the BBS story fit into the programming side of things?

00:04:12.980 --> 00:04:19.080
The BBS software that kind of was really popular, you could get a license to it for 50 bucks

00:04:19.080 --> 00:04:22.340
and you got a source code for it along with it.

00:04:22.340 --> 00:04:25.560
And there's a big active modding community.

00:04:25.560 --> 00:04:30.820
And so, you know, I started off like taking people's mods and applying them and then trying

00:04:30.820 --> 00:04:38.820
to make my own mods and like just ended up teaching myself C and initially very poorly

00:04:38.820 --> 00:04:43.160
taught myself C, but then, you know, finally got good at this at some point.

00:04:43.160 --> 00:04:44.440
How fun.

00:04:44.440 --> 00:04:44.800
Yeah.

00:04:44.800 --> 00:04:49.060
Did other people use your mods or were you running your own BBS or anything like that?

00:04:49.280 --> 00:04:50.180
Where'd this surface?

00:04:50.180 --> 00:04:53.580
I did a really bad job at running my own BBS.

00:04:53.580 --> 00:04:59.540
I petitioned my parents for a second phone line, but I also wanted to use it for phone calls.

00:04:59.540 --> 00:05:01.960
So to call my BBS, we had to like dial in.

00:05:01.960 --> 00:05:06.920
And then like I had this device where you could punch four extra codes and it would connect you

00:05:06.920 --> 00:05:07.520
to the modem.

00:05:07.520 --> 00:05:11.440
So that was kind of annoying and didn't make it the world's most popular BBS.

00:05:11.880 --> 00:05:13.400
And it was rather short web.

00:05:13.400 --> 00:05:14.500
Heard some of the automation.

00:05:14.500 --> 00:05:14.980
Yeah.

00:05:14.980 --> 00:05:15.740
Yeah.

00:05:15.740 --> 00:05:17.460
But I've published my mods.

00:05:17.460 --> 00:05:19.380
My friends ran BBSs.

00:05:19.380 --> 00:05:21.360
They pick up some of the mods.

00:05:21.360 --> 00:05:24.700
I don't know that I was the most popular modder out there.

00:05:24.700 --> 00:05:26.260
I should go and see if I could find them.

00:05:26.260 --> 00:05:27.660
That might be terrifying though.

00:05:27.660 --> 00:05:30.040
Yeah, that might be terrifying, but it could also be amazing.

00:05:30.040 --> 00:05:34.980
Let's wrap up the BBS side of things with putting some bookends on the timeframe here.

00:05:35.060 --> 00:05:39.600
What was the beginning baud rate and end baud rate of your BBS time?

00:05:39.600 --> 00:05:43.920
4,400 to 57.6K.

00:05:43.920 --> 00:05:44.400
Yeah.

00:05:44.400 --> 00:05:48.820
So you took it all the way to the end there, but 2,400 probably meant it didn't require

00:05:48.820 --> 00:05:53.160
putting the phone on to a device like the war games.

00:05:53.160 --> 00:05:56.080
Never that bad.

00:05:56.080 --> 00:05:57.240
Fantastic.

00:05:57.240 --> 00:05:58.320
All right.

00:05:58.320 --> 00:05:59.020
How about Python?

00:05:59.020 --> 00:06:00.360
How'd you get to that?

00:06:00.360 --> 00:06:04.860
I got into Python in a very weird way because I started working on a Python implementation.

00:06:04.860 --> 00:06:09.080
Having really never touched or used Python before.

00:06:09.080 --> 00:06:12.780
Obviously, I'd heard about it and it's kind of like significant white space.

00:06:12.780 --> 00:06:13.620
That sounds weird.

00:06:13.620 --> 00:06:21.220
And, but ended up really loving working on it on Iron Python, really loving the language

00:06:21.220 --> 00:06:23.260
and the way it was designed.

00:06:23.260 --> 00:06:30.580
It's a very, it gave me a very weird outlook on Python, I think, just because, you know,

00:06:30.580 --> 00:06:36.780
I knew all sorts of weird corner cases about Python and the language and all the details

00:06:36.780 --> 00:06:37.200
there.

00:06:37.200 --> 00:06:41.860
But then didn't really know much about libraries and things like that.

00:06:41.860 --> 00:06:48.000
And to some extent that continues today, but I get to write a lot more Python code today

00:06:48.000 --> 00:06:48.300
too.

00:06:48.300 --> 00:06:48.620
Sure.

00:06:48.760 --> 00:06:53.720
But like always having been on the implementation side is a little strange.

00:06:53.720 --> 00:06:55.580
It is strange.

00:06:55.580 --> 00:06:58.580
And it is, I guess it would be a weird way to get to know the language.

00:06:58.580 --> 00:07:03.520
So I feel like one of the real big powers of Python is that you can be really effective

00:07:03.520 --> 00:07:05.880
with it with a super partial understanding.

00:07:06.140 --> 00:07:10.820
Like you could have literally no idea how to create a function and you could still do useful

00:07:10.820 --> 00:07:11.760
things with Python.

00:07:11.760 --> 00:07:18.960
Whereas if you're going to jump in and create Iron Python, which we'll talk about in a second,

00:07:18.960 --> 00:07:23.820
you have to start out, what are these meta classes and how do I best implement dynamic,

00:07:23.820 --> 00:07:26.260
you know, dynamic objects and all this stuff.

00:07:26.260 --> 00:07:29.320
That's like the opposite of starting with a partial understanding.

00:07:29.320 --> 00:07:31.680
Well, how do imports work?

00:07:33.260 --> 00:07:34.360
That was a big thing.

00:07:34.360 --> 00:07:34.560
Yeah, yeah, yeah.

00:07:34.560 --> 00:07:38.460
I remember when I learned about that, I'm like, wait, this is like running code.

00:07:38.460 --> 00:07:45.420
It's not like an include file or a statically linked file or, you know, adding a reference

00:07:45.420 --> 00:07:47.400
in .NET or something like that.

00:07:47.400 --> 00:07:48.320
It's nope.

00:07:48.320 --> 00:07:50.100
It just runs whatever's in the script.

00:07:50.100 --> 00:07:53.500
And it happens to be most of the time it defines behaviors, but it doesn't have to.

00:07:53.500 --> 00:07:53.960
Yeah.

00:07:53.960 --> 00:07:57.520
And like, how do you pick what's going to get imported?

00:07:57.520 --> 00:08:03.240
And yeah, the semantics there are so complicated.

00:08:03.240 --> 00:08:03.680
Yeah.

00:08:03.680 --> 00:08:09.160
There are some oddities of Python, but in general, it seems to be working well for people.

00:08:09.160 --> 00:08:13.340
But I can see as implementing it, it could, you know, you could definitely be pulling some

00:08:13.340 --> 00:08:13.720
hair out.

00:08:13.720 --> 00:08:17.900
And I mean, so many things implemented are just, they're super safe.

00:08:17.900 --> 00:08:19.760
Like they make a lot of sense.

00:08:19.760 --> 00:08:26.080
There's just some weird corner cases that you run into that are, it's like, what's going on

00:08:26.080 --> 00:08:26.280
here?

00:08:26.280 --> 00:08:26.460
Yeah.

00:08:26.520 --> 00:08:30.120
When I worked on Iron Python, we couldn't look at the source code that's like Python,

00:08:30.120 --> 00:08:32.840
which made things really interesting.

00:08:32.840 --> 00:08:33.440
Okay.

00:08:33.440 --> 00:08:38.100
Because this predates .NET being open source and all that kind of stuff, right?

00:08:38.100 --> 00:08:38.600
Yeah.

00:08:38.600 --> 00:08:42.260
And so you don't want to be poisoned, poisoned by the ideas.

00:08:42.260 --> 00:08:43.340
Yeah.

00:08:43.740 --> 00:08:44.220
Okay.

00:08:44.220 --> 00:08:44.880
How interesting.

00:08:44.880 --> 00:08:51.780
iPhone was open source, but this was when Microsoft was still very much figuring out how they wanted

00:08:51.780 --> 00:08:53.340
to approach open source.

00:08:53.340 --> 00:08:56.260
And we're still very cagey about it.

00:08:56.260 --> 00:08:57.120
It was very interesting.

00:08:57.120 --> 00:08:58.220
Yeah.

00:08:58.420 --> 00:09:01.280
They've come a long way and many companies have, I would say.

00:09:01.280 --> 00:09:06.080
It's still, there's some idiosyncrasies, I guess there, but certainly it's a different

00:09:06.080 --> 00:09:07.880
time now than it was then.

00:09:07.880 --> 00:09:14.460
This was like what, 2008, 2009 timeframe-ish or 2005 maybe?

00:09:14.460 --> 00:09:14.780
Yeah.

00:09:14.780 --> 00:09:16.660
2005, 2006.

00:09:16.660 --> 00:09:19.500
I think it was around Iron Python 1 out.

00:09:19.500 --> 00:09:21.560
2006 sounds about right.

00:09:21.560 --> 00:09:21.840
Yeah.

00:09:21.840 --> 00:09:22.800
So that's a while ago.

00:09:22.800 --> 00:09:23.280
Yes.

00:09:23.280 --> 00:09:26.320
It doesn't sound that long ago to me, but honestly, it's a while ago.

00:09:26.320 --> 00:09:26.640
Yeah.

00:09:27.020 --> 00:09:29.660
It's like remembering the 90s is not 10 years ago.

00:09:29.660 --> 00:09:31.820
It's true.

00:09:31.820 --> 00:09:33.220
It's definitely true.

00:09:33.220 --> 00:09:34.500
All right.

00:09:34.500 --> 00:09:35.620
How about day to day?

00:09:35.620 --> 00:09:36.200
What are you doing now?

00:09:36.200 --> 00:09:37.200
Your Instagram, right?

00:09:37.200 --> 00:09:41.080
Basically, I work on our fork of CPython, which we call Sender.

00:09:41.080 --> 00:09:48.100
And my job is to make, my entire team's job is to make Instagram run more efficiently.

00:09:48.100 --> 00:09:53.760
I mean, obviously, Instagram is a very large website that has a lot of traffic and it's a

00:09:53.760 --> 00:09:55.840
very large Django app.

00:09:56.240 --> 00:10:04.300
So we just spend our time trying to improve CPython and, you know, very specifically trying

00:10:04.300 --> 00:10:07.260
to improve CPython for Instagram's workload.

00:10:07.260 --> 00:10:12.400
We're very driven by kind of that as our sole direction.

00:10:12.400 --> 00:10:18.520
And so it lets us make some interesting decisions and drive some interesting decisions.

00:10:18.520 --> 00:10:25.540
But it's just really spending the day looking at what we can do to improve performance and

00:10:25.540 --> 00:10:29.420
going off and implementing that and making Instagram a little bit faster.

00:10:29.420 --> 00:10:34.920
So when we talk about Python and Django running Instagram, I put up a little post here of

00:10:34.920 --> 00:10:38.700
something I did yesterday just to have some Instagram stuff to show.

00:10:38.700 --> 00:10:40.140
Is that talking about the website?

00:10:40.140 --> 00:10:42.400
Is that the APIs behind the scenes?

00:10:42.400 --> 00:10:47.020
Like when you say Django runs Instagram, what are we talking about here?

00:10:47.020 --> 00:10:48.120
So it's the website.

00:10:48.120 --> 00:10:49.140
It's the APIs.

00:10:49.480 --> 00:10:55.660
There's obviously some parts that aren't Django, but kind of everything that people's devices

00:10:55.660 --> 00:10:59.240
are interacting with is going through the Django front end.

00:10:59.240 --> 00:11:04.340
And there's also a bunch of like, you know, if we have asynchronous processes that need to

00:11:04.340 --> 00:11:09.520
kick off and run in the background, that's kind of all handled by a Django tier as well.

00:11:09.600 --> 00:11:13.560
So it's a good chunk of what's going on.

00:11:13.560 --> 00:11:13.920
Yeah.

00:11:13.920 --> 00:11:14.380
Nice.

00:11:14.380 --> 00:11:18.200
This is probably one of the, if not the largest Django deployment there is, right?

00:11:18.200 --> 00:11:20.360
This is a lot of, a lot of servers we're talking about, right?

00:11:20.360 --> 00:11:21.740
I would assume so.

00:11:21.740 --> 00:11:22.540
I don't know.

00:11:22.540 --> 00:11:25.060
There might be something else pretty big out there.

00:11:25.060 --> 00:11:25.680
Yeah.

00:11:25.680 --> 00:11:31.500
I feel like the talk at the 2017 PyCon, remember that when we used to go to places where there

00:11:31.500 --> 00:11:33.760
are other people and we would go and like be in the same room and stuff.

00:11:33.760 --> 00:11:34.940
That was so nice.

00:11:34.940 --> 00:11:35.700
I know.

00:11:35.700 --> 00:11:36.160
It was so weird.

00:11:36.160 --> 00:11:42.580
And there was a cool Instagram talk about, I believe that one was about disabling the

00:11:42.580 --> 00:11:44.600
GC or something like that.

00:11:44.600 --> 00:11:48.420
And I feel like they said in that talk, at least at that time, that was one of the largest,

00:11:48.420 --> 00:11:49.940
not the largest Django deployment.

00:11:49.940 --> 00:11:50.240
Yeah.

00:11:50.240 --> 00:11:52.500
And we no longer disable the GC.

00:11:52.500 --> 00:11:53.960
We fixed the memory leak.

00:11:53.960 --> 00:11:54.720
So that's good.

00:11:54.720 --> 00:11:55.680
Okay.

00:11:55.680 --> 00:11:58.920
We're going to talk a lot about memory.

00:11:58.920 --> 00:12:05.500
And honestly, this whole conversation is going to be a bit of a test, an assessment of my

00:12:05.500 --> 00:12:07.260
CPython internals.

00:12:07.260 --> 00:12:12.560
But I think that's okay because a lot of people out there don't know super in-depth

00:12:12.560 --> 00:12:13.900
details about CPython.

00:12:13.900 --> 00:12:17.060
And I can play the person who asks the questions for them.

00:12:17.060 --> 00:12:17.440
Awesome.

00:12:17.440 --> 00:12:17.900
Awesome.

00:12:17.900 --> 00:12:19.460
I can try to answer questions.

00:12:19.460 --> 00:12:21.220
Sure.

00:12:21.220 --> 00:12:23.060
Well, we'll keep it focused on the part that you've been doing.

00:12:23.060 --> 00:12:27.000
But during your talk, you mentioned a couple of things.

00:12:27.420 --> 00:12:34.640
First, you said, okay, well, when we're running over on Django, we're running on, you say UWSGI.

00:12:34.640 --> 00:12:36.520
I feel like this is a micro.

00:12:36.520 --> 00:12:37.980
It used to be like a-

00:12:37.980 --> 00:12:39.080
Micro Whiskey.

00:12:39.080 --> 00:12:39.480
Yeah.

00:12:39.480 --> 00:12:40.160
Micro Whiskey.

00:12:40.160 --> 00:12:40.560
I don't know.

00:12:40.560 --> 00:12:42.020
UWSGI, Micro Whiskey, whatever it is.

00:12:42.020 --> 00:12:42.320
Yeah.

00:12:42.440 --> 00:12:46.980
I feel like all these projects that have interesting names should have a press here to hear how

00:12:46.980 --> 00:12:47.820
it should be pronounced.

00:12:47.820 --> 00:12:49.740
It should be WSGI or Whiskey.

00:12:49.740 --> 00:12:50.180
How do it?

00:12:50.180 --> 00:12:57.560
Anyway, this micro Whiskey you guys are running on and understanding how it creates child processes

00:12:57.560 --> 00:13:03.700
and forks out the work is really important for understanding some of the improvements that

00:13:03.700 --> 00:13:06.600
you've made and some of the areas you've focused on.

00:13:06.600 --> 00:13:12.900
So maybe we could start a little bit by talking about just the infrastructure and how actually

00:13:12.900 --> 00:13:16.520
the execution of Python code happens over at Instagram.

00:13:16.520 --> 00:13:23.360
So in addition to UWSGI, it's running on Linux, which is probably not surprising to you.

00:13:23.360 --> 00:13:26.700
Literally zero people are surprised now.

00:13:26.700 --> 00:13:27.220
Yeah.

00:13:27.480 --> 00:13:28.700
I thought it was a Windows server.

00:13:28.700 --> 00:13:29.200
Come on.

00:13:29.200 --> 00:13:31.780
Or Solaris.

00:13:31.780 --> 00:13:32.080
Yep.

00:13:32.080 --> 00:13:34.260
Or a Raspberry Pi cluster.

00:13:34.260 --> 00:13:34.640
Come on.

00:13:34.640 --> 00:13:36.400
That'd be awesome.

00:13:36.400 --> 00:13:43.640
So like one of the common things that people take advantage on Linux is fork and exec, where

00:13:43.640 --> 00:13:51.160
you start up a master process and then you fork off some trial processes and they can share

00:13:51.160 --> 00:13:53.940
all the memory of that master process.

00:13:53.940 --> 00:14:01.320
So it's a relatively cheap operation to go off and spawn those trial processes and you get

00:14:01.320 --> 00:14:06.860
a lot of sharing between those two processes, which reduces kind of the memory that you need

00:14:06.860 --> 00:14:09.060
to use and all that good stuff.

00:14:09.060 --> 00:14:17.540
And so the way UWSGI is working is that, you know, we are spawning our master process, going

00:14:17.540 --> 00:14:21.700
off importing kind of all of the website.

00:14:21.700 --> 00:14:27.120
Like we try to make sure that everything gets loaded initially and then spawn off a whole bunch

00:14:27.120 --> 00:14:31.120
of worker processes, which are going to actually be serving the traffic.

00:14:31.120 --> 00:14:38.360
And if something happens to one of those worker processes, then the master will come in and

00:14:38.360 --> 00:14:40.340
spawn a new worker to replace it.

00:14:40.340 --> 00:14:40.580
Yeah.

00:14:40.580 --> 00:14:42.780
That kind of goes on and on and on.

00:14:42.780 --> 00:14:45.720
And it's also not just about durability.

00:14:45.720 --> 00:14:48.880
It's also about scalability, right?

00:14:48.920 --> 00:14:54.940
If one of the worker processes is busy working on a request, well, there might be nine others

00:14:54.940 --> 00:15:00.840
and the supervisor process can look and say, OK, well, I got some requests got to be processed

00:15:00.840 --> 00:15:01.160
here.

00:15:01.160 --> 00:15:03.500
This one's not busy and sort of scale it out.

00:15:03.500 --> 00:15:08.000
And that also helps a lot with Pythons, GIL and stuff.

00:15:08.000 --> 00:15:11.600
You can just throw more of these worker processes at it to get more scalability.

00:15:11.600 --> 00:15:15.080
And at some point that kind of hits the database limits anyway.

00:15:15.240 --> 00:15:16.680
So it doesn't really matter that much, right?

00:15:16.680 --> 00:15:16.940
Yeah.

00:15:16.940 --> 00:15:19.140
And I think like you would ski can auto tune.

00:15:19.140 --> 00:15:21.780
I don't know exactly all the details of our settings.

00:15:21.780 --> 00:15:22.380
Yeah.

00:15:22.380 --> 00:15:24.340
There's a lot of advanced settings in there.

00:15:24.340 --> 00:15:24.560
Yeah.

00:15:24.560 --> 00:15:25.060
Yeah.

00:15:25.060 --> 00:15:30.360
Like it can, you know, tune for memory for stall workers.

00:15:30.360 --> 00:15:32.840
It's pretty smart.

00:15:32.840 --> 00:15:33.160
Yeah.

00:15:33.160 --> 00:15:34.400
But like, yeah.

00:15:34.400 --> 00:15:37.360
There's actually a really interesting, I don't know.

00:15:37.360 --> 00:15:38.580
Have you, maybe you've seen this.

00:15:38.580 --> 00:15:45.140
There's a really interesting post called configuring UISG for production deployment over on

00:15:45.140 --> 00:15:50.400
Bloomberg tech talking about all these knobs that they turn to make it work better and

00:15:50.400 --> 00:15:51.300
do these different things.

00:15:51.300 --> 00:15:56.800
And it's super interesting if these, like these tuning knobs are unfamiliar to Python people.

00:15:56.800 --> 00:15:57.300
Yeah.

00:15:57.300 --> 00:15:57.700
Yeah.

00:15:57.700 --> 00:16:01.980
But the important takeaway here is when we're talking about running your code on a single

00:16:01.980 --> 00:16:06.880
server, we're talking about five, 10, 20 copies of the same process running the same

00:16:06.880 --> 00:16:08.240
code with the same interpreter.

00:16:08.240 --> 00:16:09.860
Yeah, exactly.

00:16:10.520 --> 00:16:12.560
You guys pay for bigger cloud instances.

00:16:12.560 --> 00:16:14.560
No, I mean, you have your own data centers, right?

00:16:14.560 --> 00:16:16.640
So you probably get bigger VMs.

00:16:16.680 --> 00:16:21.480
This portion of Talk Python to me is brought to you by Sentry.

00:16:21.480 --> 00:16:24.360
How would you like to remove a little stress from your life?

00:16:24.360 --> 00:16:30.040
Do you worry that users may be encountering errors, slowdowns, or crashes with your app right

00:16:30.040 --> 00:16:30.360
now?

00:16:30.360 --> 00:16:33.400
Would you even know it until they sent you that support email?

00:16:33.400 --> 00:16:38.160
How much better would it be to have the error or performance details immediately sent to you,

00:16:38.280 --> 00:16:43.480
including the call stack and values of local variables and the active user recorded in the

00:16:43.480 --> 00:16:43.800
report?

00:16:43.800 --> 00:16:47.220
With Sentry, this is not only possible, it's simple.

00:16:47.220 --> 00:16:50.780
In fact, we use Sentry on all the Talk Python web properties.

00:16:50.780 --> 00:16:56.560
We've actually fixed a bug triggered by a user and had the upgrade ready to roll out as we got

00:16:56.560 --> 00:16:57.340
the support email.

00:16:57.340 --> 00:16:59.320
That was a great email to write back.

00:16:59.320 --> 00:17:02.720
Hey, we already saw your error and have already rolled out the fix.

00:17:02.720 --> 00:17:04.120
Imagine their surprise.

00:17:04.120 --> 00:17:06.340
Surprise and delight your users.

00:17:06.460 --> 00:17:10.400
Create your Sentry account at talkpython.fm/sentry.

00:17:10.400 --> 00:17:15.880
And if you sign up with the code talkpython, all one word, it's good for two free months

00:17:15.880 --> 00:17:21.160
of Sentry's business plan, which will give you up to 20 times as many monthly events as

00:17:21.160 --> 00:17:22.080
well as other features.

00:17:22.080 --> 00:17:26.480
Create better software, delight your users, and support the podcast.

00:17:26.480 --> 00:17:31.460
Visit talkpython.fm/sentry and use the coupon code talkpython.

00:17:33.640 --> 00:17:37.740
And so that impacts a lot of the decisions that we make.

00:17:37.740 --> 00:17:40.960
We can talk about those more later.

00:17:40.960 --> 00:17:48.380
I think another interesting thing about RUWSD and our deployments in general is that we're

00:17:48.380 --> 00:17:52.480
also redeploying every 10 minutes when developers are...

00:17:52.480 --> 00:17:54.580
Yeah, I saw that and that blows my mind.

00:17:54.580 --> 00:17:57.780
So tell me about this rapid redeployment.

00:17:57.780 --> 00:17:59.260
It blows my mind too.

00:17:59.260 --> 00:18:04.740
And when I started it at Facebook, I guess now Meta, but it was Facebook back then, you

00:18:04.740 --> 00:18:09.320
go through a process called Bootcamp where you spend your first several weeks just learning

00:18:09.320 --> 00:18:10.000
about Facebook.

00:18:10.000 --> 00:18:14.540
And one of the first things you learn is like, Facebook.com redeploys every three to four

00:18:14.540 --> 00:18:14.900
hours.

00:18:14.900 --> 00:18:16.900
I'm like, that's insanely fast.

00:18:16.960 --> 00:18:18.920
And then land on Instagram.

00:18:18.920 --> 00:18:21.240
We were redeploy every 10 minutes.

00:18:21.240 --> 00:18:22.200
It's like, what?

00:18:22.200 --> 00:18:23.920
Yeah, that's incredible.

00:18:23.920 --> 00:18:25.960
Can you talk about why that is?

00:18:25.960 --> 00:18:30.040
Is there just that many improvements in code changes going on?

00:18:30.040 --> 00:18:34.280
Or is there some other like balancing reason that this happens?

00:18:34.280 --> 00:18:35.660
Like a DevOps-y thing?

00:18:35.780 --> 00:18:39.400
I don't know what all the original reasoning is.

00:18:39.400 --> 00:18:41.320
It has a very nice...

00:18:41.320 --> 00:18:46.100
So one of the nice things about deploying a lot is when something goes wrong, it's not

00:18:46.100 --> 00:18:49.340
hard to figure out what caused things to go wrong.

00:18:49.340 --> 00:18:50.660
So you're not looking at...

00:18:50.660 --> 00:18:50.820
Right.

00:18:50.820 --> 00:18:54.380
There's a bunch of small changes and each one gets deployed.

00:18:54.380 --> 00:18:56.920
So you're not going back to the last six months or whatever, right?

00:18:56.920 --> 00:18:58.100
Yeah, exactly.

00:18:58.100 --> 00:18:59.960
Or I mean, even...

00:18:59.960 --> 00:19:04.080
I mean, each of those deployments has a good number of changes in it.

00:19:04.460 --> 00:19:10.360
And even if it was like four hours, it would be a huge number of changes that you have

00:19:10.360 --> 00:19:12.020
to track things down through.

00:19:12.020 --> 00:19:18.940
And it's also like, it's really satisfying from a developer standpoint in that, you know,

00:19:18.940 --> 00:19:21.940
you land your change and it's rolling out in half an hour.

00:19:21.940 --> 00:19:24.420
So I don't think anyone...

00:19:24.420 --> 00:19:28.040
I don't know all the original reasoning, but I don't think anyone would really want to change

00:19:28.040 --> 00:19:32.100
it just because it actually has some significant benefits.

00:19:32.100 --> 00:19:34.680
It makes things interesting and challenging in some ways too.

00:19:34.680 --> 00:19:36.420
But otherwise, I think it's really nice.

00:19:36.420 --> 00:19:36.780
Yeah.

00:19:36.780 --> 00:19:38.400
I...

00:19:38.400 --> 00:19:45.160
It just never ceases to frustrate me or blow my mind how there's these companies just have

00:19:45.160 --> 00:19:46.640
extended downtime.

00:19:46.640 --> 00:19:48.100
Like, I'm not talking...

00:19:48.100 --> 00:19:52.520
We pushed out a new version and in order to switch things in and out of the load balancer,

00:19:52.520 --> 00:19:53.800
there's five seconds of downtime.

00:19:54.680 --> 00:19:58.700
Or we got to run a database migration and it creates a new index and that's going to take,

00:19:58.700 --> 00:20:00.200
you know, one or two minutes.

00:20:00.200 --> 00:20:00.680
I'm talking...

00:20:00.680 --> 00:20:04.940
We're going to be down for six hours on Sunday.

00:20:04.940 --> 00:20:06.960
So please schedule your work around.

00:20:06.960 --> 00:20:09.060
I'm just like, what is wrong with these companies?

00:20:09.060 --> 00:20:15.180
He's like, how is it possible that it takes so long to deploy these things?

00:20:15.180 --> 00:20:23.860
And if they had put in some mechanism to ship small amounts of code with automation, then

00:20:23.860 --> 00:20:26.660
they would just not be in this situation, right?

00:20:26.660 --> 00:20:27.560
Like, they would just...

00:20:27.560 --> 00:20:32.740
They would get pushed somewhere and then something would happen and then they would have a new

00:20:32.740 --> 00:20:34.040
version of the site, right?

00:20:34.040 --> 00:20:40.840
It always baffles me when I end up at a website and it's like, we're currently down for service.

00:20:40.840 --> 00:20:41.580
It's like, what?

00:20:41.580 --> 00:20:42.680
Yes.

00:20:42.680 --> 00:20:44.180
There's a website.

00:20:44.180 --> 00:20:45.460
You're not supposed to do that.

00:20:45.460 --> 00:20:47.480
The most insane thing...

00:20:47.480 --> 00:20:50.060
I'll get off this thing, but it drives me crazy.

00:20:50.060 --> 00:20:53.760
The most insane thing is I've seen websites that were closed on Sunday.

00:20:53.760 --> 00:20:55.480
I'm like, what do you mean it's closed on Sunday?

00:20:55.480 --> 00:20:55.940
Yeah.

00:20:55.940 --> 00:20:58.760
Just go and turn it off when you go home?

00:20:58.760 --> 00:21:01.440
Like, you know, it's open Monday to Friday sort of thing.

00:21:01.440 --> 00:21:05.460
This is like, it was like a government website and I don't know why it had to be closed,

00:21:05.460 --> 00:21:08.040
but apparently it had to be closed.

00:21:08.040 --> 00:21:08.680
Yeah.

00:21:08.680 --> 00:21:12.960
We have engineers standing by Monday through Friday to process your requests by hand.

00:21:12.960 --> 00:21:14.160
Exactly.

00:21:14.160 --> 00:21:15.700
We got to push the button.

00:21:15.700 --> 00:21:16.800
No one's there to push the button.

00:21:16.800 --> 00:21:17.760
Okay.

00:21:17.860 --> 00:21:32.260
So I guess one more setting the stage stories here or things to know is that you run these servers quite close to their limits in terms of like CPU usage and stuff like that.

00:21:32.260 --> 00:21:39.780
And then also you said one of the areas that you focus on is requests per second as your important metric.

00:21:39.780 --> 00:21:41.500
Do you want to talk about those for a moment?

00:21:41.500 --> 00:21:41.900
Sure.

00:21:41.900 --> 00:21:48.180
So I don't know what the overall numbers under normal load are.

00:21:48.180 --> 00:21:52.060
You know, I don't think the CPU load is necessarily super high.

00:21:52.180 --> 00:21:59.480
But what we want to know at the end of the day is like how many requests can we serve under peak load?

00:21:59.680 --> 00:22:11.320
And so what we can actually do is take traffic and route it to a set of servers and drive that traffic up to where the server is under peak load.

00:22:11.320 --> 00:22:23.600
And we see how many requests per second a server is able to serve at that point, which gives us a pretty good idea of kind of what the overall level of efficiency is.

00:22:23.800 --> 00:22:41.640
So when we make a change, we can basically run an A-B test where we take one set of servers that don't have the change, drive them up to peak load and compare it against another set of servers that have the change and drive those set of servers up to peak load.

00:22:41.640 --> 00:22:48.860
And then compare between the two and see how many requests per second we end up getting and what the change is.

00:22:49.280 --> 00:22:53.860
And we can do that to a decent amount of accuracy.

00:22:53.860 --> 00:22:59.440
I think kind of like when we kick off a manual test, we try to strive for within 0.25%.

00:22:59.440 --> 00:23:05.740
When we're doing releases of sender, I think we try to push it a little bit further by doing more runs.

00:23:05.740 --> 00:23:09.120
So we get down to like 0.1% or something like that.

00:23:09.120 --> 00:23:15.580
So we have a pretty good idea of what the performance impact of what those changes are going to end up looking like.

00:23:15.580 --> 00:23:16.800
I think that makes a ton of sense.

00:23:16.900 --> 00:23:19.840
You could do profiling and obviously you do.

00:23:19.840 --> 00:23:20.660
And we do that too.

00:23:20.660 --> 00:23:21.200
Yeah.

00:23:21.200 --> 00:23:25.920
But in the end of the day, there's a bunch of different things, right?

00:23:25.920 --> 00:23:34.400
If I profile against some process and say, well, this went this much faster in terms of CPU, maybe it took more memory.

00:23:35.240 --> 00:23:43.760
And at production scale, it turns into swap, which means it's dramatically, you know, there's a bunch of pushes and pulls in there.

00:23:43.760 --> 00:23:48.180
And this pragmatic, just like, let's just see what it can take now is interesting.

00:23:48.180 --> 00:23:55.900
You all are in this advantaged situation where you have more traffic than any given server can handle, I would imagine.

00:23:56.360 --> 00:23:56.960
Yes.

00:23:56.960 --> 00:23:59.300
So you can tune in.

00:23:59.300 --> 00:24:00.600
We actually run on one server.

00:24:00.600 --> 00:24:01.540
Exactly.

00:24:01.540 --> 00:24:03.460
We have a backup server.

00:24:03.460 --> 00:24:05.240
It hasn't been rebooted in seven years.

00:24:06.180 --> 00:24:14.420
You have the ability to say, well, let's just tune some of our traffic over to this one particular server to sort of see this limit.

00:24:14.840 --> 00:24:19.220
Whereas a lot of companies and products don't, right?

00:24:19.220 --> 00:24:32.660
Like I use this thing called Locust.io, which is just a fantastic Python framework for doing load testing to actually know the upper bound of what my servers can handle.

00:24:32.660 --> 00:24:38.080
Because we get a lot of traffic, but we don't get 30,000 requests a second, lots of traffic, right?

00:24:38.500 --> 00:24:49.060
And so I think this is really neat that you can actually test in production sort of beyond integration tests, not test that it works right, but send real traffic and actually see how it responds.

00:24:49.060 --> 00:24:51.120
Because really, that's the most important thing, right?

00:24:51.120 --> 00:24:52.960
Does it do more or does it do less than before?

00:24:52.960 --> 00:25:01.560
And, you know, you brought up profiling and we still have to use profiling sometimes to like, you know, 0.25%, 0.1%.

00:25:01.560 --> 00:25:03.120
That's still a lot of noise.

00:25:03.120 --> 00:25:22.840
And like, so if there's some little micro optimization, we can still be like, okay, well, what's this function using after the change, you know, kind of across some percentage of the entire fleet, which is kind of amazing because the profiling is just running on production traffic sampled.

00:25:22.840 --> 00:25:27.300
So for smaller things that ends up becoming super important.

00:25:27.300 --> 00:25:34.000
Right. And you're making a ton of changes as we're about to dive into, but they're additive or multiplicative or something like that, right?

00:25:34.000 --> 00:25:42.120
So if you make this thing 1% faster, that 5% faster, this 3% faster, all of a sudden you could end up at 20 to 30% faster in production, right?

00:25:42.120 --> 00:25:42.520
Yeah.

00:25:42.520 --> 00:25:44.200
And how do you do that math?

00:25:44.200 --> 00:25:45.540
Well, we just add up the percents.

00:25:45.540 --> 00:25:47.100
Exactly.

00:25:47.100 --> 00:25:49.700
Where's Cinder? Here we go.

00:25:49.700 --> 00:25:50.440
All right.

00:25:50.500 --> 00:25:54.460
So when I saw this come out, when did you all make this public?

00:25:54.460 --> 00:25:56.760
Shortly before PyCon?

00:25:56.760 --> 00:25:57.900
Yeah, that's right.

00:25:57.900 --> 00:25:58.220
Yeah.

00:25:58.220 --> 00:26:01.480
Which would put it like February, March, something like that.

00:26:01.480 --> 00:26:02.400
Yeah, something like that.

00:26:02.400 --> 00:26:04.520
That sounds exactly right with this eight months ago.

00:26:04.520 --> 00:26:08.620
So this is under the Facebook incubator.

00:26:08.620 --> 00:26:10.280
You guys got to rename this.

00:26:10.280 --> 00:26:11.000
It should be meta.

00:26:11.000 --> 00:26:11.280
Yeah.

00:26:11.280 --> 00:26:13.140
I wonder whose job that is.

00:26:13.140 --> 00:26:14.480
I mean, permalinks.

00:26:14.480 --> 00:26:14.800
Come on.

00:26:14.800 --> 00:26:18.180
So it doesn't matter all that much.

00:26:18.180 --> 00:26:19.920
It's Instagram, I guess.

00:26:19.920 --> 00:26:23.020
But let me just read the first opening bit here.

00:26:23.020 --> 00:26:25.720
I think there's a lot to take away just from the first sentence.

00:26:25.720 --> 00:26:32.500
Cinder is Instagram's internal performance-oriented production version of CPython 3.8.

00:26:32.500 --> 00:26:34.260
So performance-oriented.

00:26:34.260 --> 00:26:35.440
We've been talking about performance.

00:26:35.440 --> 00:26:37.220
We're going to get into a lot of the cool things you've done.

00:26:37.220 --> 00:26:38.640
Production version.

00:26:38.640 --> 00:26:41.000
So you guys are running on this on Cinder?

00:26:41.000 --> 00:26:41.600
Mm-hmm.

00:26:41.600 --> 00:26:42.160
Fantastic.

00:26:42.160 --> 00:26:45.060
And we redeploy like once a week?

00:26:45.060 --> 00:26:48.960
You redeploy the Cinder or CPython runtime, right?

00:26:48.960 --> 00:26:49.420
Yeah.

00:26:49.420 --> 00:26:50.160
Yeah, yeah, yeah.

00:26:50.160 --> 00:26:57.200
So like the source that's up here is, yeah, if you go back and look at maybe a week ago

00:26:57.200 --> 00:26:59.580
is what we're probably running in introduction at any given time.

00:26:59.580 --> 00:26:59.940
Right.

00:26:59.940 --> 00:27:00.240
Okay.

00:27:00.240 --> 00:27:00.800
Fantastic.

00:27:00.800 --> 00:27:08.520
And then CPython 3.8, because you've made a lot of changes to this that can't really move

00:27:08.520 --> 00:27:08.880
forward.

00:27:08.880 --> 00:27:13.440
So you picked the one, I'm guessing that was the most current when you first started,

00:27:13.440 --> 00:27:16.020
most current and stable, and just started working on that, right?

00:27:16.020 --> 00:27:17.400
So we do upgrade.

00:27:17.400 --> 00:27:17.800
Okay.

00:27:17.800 --> 00:27:21.040
It was previously built on CPython 3.7.

00:27:21.040 --> 00:27:21.820
Oh, cool.

00:27:21.820 --> 00:27:22.200
Okay.

00:27:22.200 --> 00:27:29.320
There's hundreds or, I don't know if we're yet up into thousands of changes yet.

00:27:29.320 --> 00:27:33.100
But there's a lot of dips that we've applied.

00:27:33.100 --> 00:27:39.320
It's been, I mean, we've been working on it for, I've been working on it for three years

00:27:39.320 --> 00:27:41.300
now and it predates me.

00:27:41.800 --> 00:27:43.660
So we've upgraded 3.7.

00:27:43.660 --> 00:27:50.040
We're going to upgrade to 3.10 next, which we're actually starting early next year.

00:27:50.040 --> 00:27:52.680
So it's just a big and bold process.

00:27:52.680 --> 00:27:56.780
And you've also contributed some stuff from Cinder to 3.10.

00:27:56.780 --> 00:27:58.780
So that'll be, that'll be interesting as well.

00:27:58.780 --> 00:28:01.960
That probably actually makes it harder to merge rather than easier.

00:28:01.960 --> 00:28:04.780
We hope that makes it easier.

00:28:04.780 --> 00:28:06.700
That is one of the things.

00:28:06.700 --> 00:28:08.580
I guess you could drop that whole section, right?

00:28:08.580 --> 00:28:09.660
You could just say, you know what?

00:28:09.660 --> 00:28:13.500
We don't even need this whole enhancement because that's just part of Python now, right?

00:28:13.500 --> 00:28:13.940
Okay.

00:28:13.940 --> 00:28:14.160
Yeah.

00:28:14.160 --> 00:28:15.860
That is the incentive for us.

00:28:15.860 --> 00:28:17.700
One of the incentives for us to contribute.

00:28:17.700 --> 00:28:19.000
Yeah.

00:28:19.000 --> 00:28:19.460
Yeah.

00:28:19.460 --> 00:28:25.040
Itamar out in the live stream in the audience says, we're close to 2,000 commits.

00:28:25.040 --> 00:28:25.900
Oh my gosh.

00:28:25.900 --> 00:28:26.140
Yeah.

00:28:26.140 --> 00:28:26.600
That's awesome.

00:28:26.600 --> 00:28:34.340
Itamar's going to be, he is now our kind of full-time dedicated resource to help us upstream

00:28:34.340 --> 00:28:34.900
things.

00:28:34.900 --> 00:28:35.620
Oh, fantastic.

00:28:35.620 --> 00:28:36.980
That's, oh, to upstream it.

00:28:36.980 --> 00:28:42.880
Itamar's job is to take the work you're doing here and then work on getting that into CPython

00:28:42.880 --> 00:28:43.140
properly.

00:28:43.140 --> 00:28:43.760
Okay.

00:28:43.760 --> 00:28:46.160
We could have been doing such a better job.

00:28:46.160 --> 00:28:52.540
I think we've upstream some little things, some slightly more significant things, but it's

00:28:52.540 --> 00:28:54.420
something that we really need to be working on more.

00:28:54.420 --> 00:28:55.540
Oh, that's fantastic.

00:28:55.540 --> 00:29:00.980
We've got someone who's dedicated to it and obviously he's not just doing it in a vacuum.

00:29:00.980 --> 00:29:02.120
We're going to help him.

00:29:02.120 --> 00:29:07.160
But having someone drive that and make sure it actually happens is super important.

00:29:07.160 --> 00:29:07.500
Yeah.

00:29:07.500 --> 00:29:08.140
That's really cool.

00:29:08.140 --> 00:29:11.160
I suspect that he and Lucas Lingo will become friends.

00:29:11.160 --> 00:29:12.220
Yeah.

00:29:12.220 --> 00:29:16.060
Lucas will be on the receiving end of that a lot.

00:29:16.060 --> 00:29:16.700
I bet.

00:29:16.940 --> 00:29:20.520
giving it the developer and residents over at CPython.

00:29:20.520 --> 00:29:20.860
Cool.

00:29:20.860 --> 00:29:21.840
All right.

00:29:21.840 --> 00:29:25.980
So I guess let's talk about this.

00:29:25.980 --> 00:29:26.760
Is it supported?

00:29:26.760 --> 00:29:32.040
So right now the story is you guys have put this out here as sort of a proof of concept.

00:29:32.040 --> 00:29:38.180
And by the way, we're using it, but not we expect other teams and companies to take this

00:29:38.180 --> 00:29:39.940
and then like just run on it as well.

00:29:39.940 --> 00:29:40.220
Right.

00:29:40.560 --> 00:29:43.940
This is probably more to like work on the upstreaming side.

00:29:43.940 --> 00:29:44.760
Is that the story?

00:29:44.760 --> 00:29:45.280
Yeah.

00:29:45.280 --> 00:29:47.640
And like let people know what we're doing.

00:29:47.640 --> 00:29:50.840
If someone wants to pick it up and try it, that's great.

00:29:50.840 --> 00:29:59.200
It's just mainly we're focused on our workload and making it faster and can't commit to helping

00:29:59.200 --> 00:30:01.420
people out and making it work for them.

00:30:01.420 --> 00:30:01.760
Right.

00:30:01.760 --> 00:30:07.480
But as you just said, you are working on bringing these changes up to CPython and you already

00:30:07.480 --> 00:30:08.220
have to some degree.

00:30:08.220 --> 00:30:10.540
So that's, you know, that's pretty good.

00:30:10.940 --> 00:30:17.080
I guess it also lets you all take a more specialized, focused view and say, you know what?

00:30:17.080 --> 00:30:21.840
We want to make micro WSGI when it works off child processes.

00:30:21.840 --> 00:30:25.040
We want to make it that happen better and use less memory.

00:30:25.040 --> 00:30:27.340
And we're going to focus on that.

00:30:27.340 --> 00:30:30.120
And if it makes sense to move that to main Python, good.

00:30:30.120 --> 00:30:33.100
If not, then we're just going to keep those changes there.

00:30:33.100 --> 00:30:33.340
Right.

00:30:33.340 --> 00:30:33.620
Yeah.

00:30:33.620 --> 00:30:38.440
And then, I mean, that's happened, I think with like, we've done some work around

00:30:38.440 --> 00:30:44.660
immortalization of the GC heap, which is kind of a big improvement over not collecting.

00:30:44.660 --> 00:30:46.160
We were talking about earlier.

00:30:46.780 --> 00:30:49.100
And that didn't make sense for upstream CPython.

00:30:49.100 --> 00:30:50.300
And so it's okay.

00:30:50.300 --> 00:30:51.740
That's something that we just have to maintain.

00:30:51.740 --> 00:30:52.060
Cool.

00:30:52.060 --> 00:30:53.780
I was so excited when I saw this come out.

00:30:53.780 --> 00:30:58.440
I'm like, wow, this is the biggest performance story I've seen around CPython for quite a

00:30:58.440 --> 00:30:58.640
while.

00:30:58.640 --> 00:31:00.840
And now there have been some other things as well.

00:31:00.840 --> 00:31:02.680
We'll touch on at the end on how they come together.

00:31:03.720 --> 00:31:07.380
But maybe walk us through what is Cinder?

00:31:07.380 --> 00:31:08.740
What is this work?

00:31:08.740 --> 00:31:10.860
And we can dive into some of the areas, maybe.

00:31:10.860 --> 00:31:11.340
Sure.

00:31:11.340 --> 00:31:13.160
You have immortal instances highlighted.

00:31:13.160 --> 00:31:15.240
So we could start talking about that.

00:31:15.240 --> 00:31:16.320
You want to start with JIT first?

00:31:16.320 --> 00:31:16.760
I think.

00:31:16.760 --> 00:31:17.120
Yeah.

00:31:17.120 --> 00:31:17.720
Okay, sure.

00:31:17.720 --> 00:31:18.900
If you think JIT makes sense.

00:31:18.900 --> 00:31:19.620
Yeah, let's talk there.

00:31:19.620 --> 00:31:21.880
So the JIT isn't what I live on day to day.

00:31:21.880 --> 00:31:25.720
We have several other team members who are working on that full time.

00:31:26.140 --> 00:31:29.960
But it's obviously a huge part of the performance story.

00:31:29.960 --> 00:31:34.180
So the JIT right now is it's a method at a time JIT.

00:31:34.180 --> 00:31:36.560
So it compiles each individual method.

00:31:36.560 --> 00:31:39.860
It's, again, very tuned for our workload.

00:31:39.860 --> 00:31:45.280
Kind of you can see here, like some of the descriptions of how to use this thing.

00:31:45.280 --> 00:31:47.400
And it's mentioning this JIT list file.

00:31:47.400 --> 00:31:54.000
So when we're using this in production, what happens is we compile all the functions ahead

00:31:54.000 --> 00:31:59.740
of time inside of the master process before we fork off all this worker processes.

00:31:59.740 --> 00:32:05.400
Because we want all that memory to be shared between the different processes.

00:32:05.400 --> 00:32:09.880
So that's kind of a unusual mode for a JIT to work in.

00:32:09.880 --> 00:32:11.000
Right, right.

00:32:11.000 --> 00:32:13.700
They don't normally think about children processes and forking.

00:32:13.700 --> 00:32:15.760
They just do their own thing, right?

00:32:15.760 --> 00:32:16.180
Yeah.

00:32:16.180 --> 00:32:17.880
It's just like, okay, I have this method.

00:32:17.880 --> 00:32:18.620
It's gone hot.

00:32:18.620 --> 00:32:19.580
It's time to jib it.

00:32:19.580 --> 00:32:22.680
So it's used in this weird way.

00:32:22.680 --> 00:32:28.880
At some point, we need to, I think, add support for kind of normal jitting methods when they

00:32:28.880 --> 00:32:29.340
get hot.

00:32:29.340 --> 00:32:34.920
Like we were at the point where we're talking about using Cinder a little bit beyond Instagram

00:32:34.920 --> 00:32:36.480
within meta.

00:32:36.480 --> 00:32:41.420
And so at that point, people are going to need something that isn't so heavily tuned to

00:32:41.420 --> 00:32:41.840
UWSC.

00:32:42.420 --> 00:32:48.000
The JIT does, it's, you know, entirely, we kind of boom in the full stack.

00:32:48.000 --> 00:32:51.800
So it uses, I think, is it ASM shit?

00:32:51.800 --> 00:32:52.520
Yeah.

00:32:52.520 --> 00:32:56.200
It uses a library to do the X64 code generation.

00:32:56.200 --> 00:33:00.960
Other than that, we go from a high level representation.

00:33:01.460 --> 00:33:06.180
How close is the high level representation to just Python's bytecode?

00:33:06.180 --> 00:33:11.140
There is a pretty good set of overlap.

00:33:11.140 --> 00:33:18.100
There are also a lot of opcodes which kind of turn into multiple smaller things.

00:33:18.660 --> 00:33:24.420
So like off the top of my head, I think like making a function involves setting several

00:33:24.420 --> 00:33:26.900
different attributes on it at the end.

00:33:26.900 --> 00:33:32.900
So there's something that says, make me this function, which is just a single opcode in CPython.

00:33:32.900 --> 00:33:36.940
And there's several different opcodes which are setting those fields on it.

00:33:37.120 --> 00:33:41.280
So it's pretty close, but maybe slightly lower level.

00:33:41.280 --> 00:33:47.620
There's also a lot of opcodes in there for just kind of super low level operations.

00:33:47.620 --> 00:33:52.440
So one of the things, the thing that I spend most of my time working on is static Python.

00:33:52.440 --> 00:33:59.220
And so we added a bunch of things that support primitive math and simple loads and stores of

00:33:59.220 --> 00:34:01.500
fields and lower level things like that.

00:34:01.500 --> 00:34:02.780
So it's a mix.

00:34:02.780 --> 00:34:05.320
Yeah, the static Python that we're going to talk about is super cool.

00:34:05.500 --> 00:34:09.760
And is that possible because the JIT, like you can do whatever you want and then the

00:34:09.760 --> 00:34:12.380
JIT will see that and then adapt correctly?

00:34:12.380 --> 00:34:19.360
The JIT's really important to it because like it takes things that are usually tons of instructions

00:34:19.360 --> 00:34:23.020
and turns them into a single instruction or a couple of instructions.

00:34:23.020 --> 00:34:25.680
It's not 100% required.

00:34:25.680 --> 00:34:32.180
Like we support it in the interpreter loop and kind of our goal is to do no harm and generally

00:34:32.180 --> 00:34:34.380
like at least get the normal performance.

00:34:34.700 --> 00:34:40.200
But the JIT being able to resolve things statically and turn them into simple loads is super important.

00:34:40.200 --> 00:34:47.480
So from HIR, like we turn that into an SSA form and run a bunch of optimizations over it.

00:34:47.480 --> 00:34:52.040
I think one really interesting optimization is rough count removal.

00:34:52.580 --> 00:35:04.160
So we can see kind of these objects are either borrowed or just like that we'd have extra rough counts happening on them that we don't need to actually insert.

00:35:04.160 --> 00:35:07.420
And we can just elide all of those, which is super awesome.

00:35:09.420 --> 00:35:12.720
This portion of Talk Python and me is brought to you by TopTal.

00:35:12.720 --> 00:35:16.880
Are you looking to hire a developer to work on your latest project?

00:35:16.880 --> 00:35:21.080
Do you need some help with rounding out that app you just can't seem to get finished?

00:35:21.080 --> 00:35:24.360
Maybe you're even looking to do a little consulting work of your own.

00:35:24.360 --> 00:35:26.180
You should give TopTal a try.

00:35:26.420 --> 00:35:32.620
You may know that we have mobile apps for our courses over at Talk Python on iOS and Android.

00:35:32.620 --> 00:35:38.460
I actually used TopTal to hire a solid developer at a fair rate to help create those mobile apps.

00:35:38.460 --> 00:35:41.860
It was a great experience and I can totally recommend working with them.

00:35:42.400 --> 00:35:47.340
I met with a specialist who helped figure out my goals and technical skills that were required for the project.

00:35:47.340 --> 00:35:50.600
Then they did all the work to find just the right person.

00:35:50.600 --> 00:35:53.220
I had short interviews with two folks.

00:35:53.220 --> 00:35:57.360
I hired the second one and we released our apps just two months later.

00:35:57.360 --> 00:36:05.640
If you'd like to do something similar, please visit talkpython.fm/TopTal and click that hire top talent button.

00:36:05.640 --> 00:36:07.560
It really helps support the show.

00:36:09.680 --> 00:36:13.080
There's a lot of interesting stuff happening around memory that you all are doing.

00:36:13.080 --> 00:36:14.300
Yes.

00:36:14.300 --> 00:36:20.080
But one of them is this ref count and you make assumptions that are reasonable.

00:36:20.080 --> 00:36:27.820
Like when I'm in a method call of a class, I don't need to increment and then decrement the self object.

00:36:27.820 --> 00:36:28.960
Because guess what?

00:36:28.960 --> 00:36:31.340
The thing must be alive because it's doing stuff, right?

00:36:31.340 --> 00:36:36.800
And then it sounds like also maybe with constants, like the number one doesn't need its ref count changed and stuff like that.

00:36:36.800 --> 00:36:38.360
You notice that and go, you know what?

00:36:38.360 --> 00:36:39.240
We're just going to skip that.

00:36:39.380 --> 00:36:43.420
One of the things we've done is the immortalization of objects.

00:36:43.420 --> 00:36:48.500
And so we can also like the number one is going to be an immortal instance.

00:36:48.500 --> 00:36:53.620
And so in that case, we can be like, okay, yeah, we don't need to deal with ref counts on this.

00:36:53.620 --> 00:37:03.740
Unless, of course, like that number one ends up going off to somewhere that, you know, maybe doesn't understand the ref counting semantics of the JIT.

00:37:03.860 --> 00:37:06.240
In which case, maybe we do have to end up inserting them.

00:37:06.240 --> 00:37:06.540
Right.

00:37:06.540 --> 00:37:13.060
Or like if it's going through like an if else or something where one of the branches, we have to end up ref counting.

00:37:13.300 --> 00:37:14.300
So it's smart.

00:37:14.300 --> 00:37:14.300
So it's smart.

00:37:14.300 --> 00:37:24.320
And it's important because with immortal instances, our ref counts are a little bit more expensive than normal ref counts because we have to check to see if the object is immortal too.

00:37:24.320 --> 00:37:24.800
Right.

00:37:24.800 --> 00:37:26.760
So they're just doing an increment on a number.

00:37:26.760 --> 00:37:27.180
Yeah.

00:37:27.280 --> 00:37:27.520
Okay.

00:37:27.520 --> 00:37:35.040
So this, this immortal instances, this comes back to that memory thing that comes back to the turning off the GC, which you stopped turning it off.

00:37:35.040 --> 00:37:39.860
It sounds like immortal instances are a more nuanced way to solve that same problem.

00:37:39.860 --> 00:37:42.320
This is really about that fork and exact model.

00:37:42.320 --> 00:37:42.560
Yeah.

00:37:42.740 --> 00:37:52.980
So when we fork off those worker processes, they're initially sharing all the memory with the master process unless they happen to go off and write to it.

00:37:53.460 --> 00:38:00.060
And ref counts are a really big source of writing to that shared memory.

00:38:00.060 --> 00:38:09.280
And so what this does is takes all the objects that are present inside of the master process and runs through, marks them all as immortal.

00:38:09.280 --> 00:38:15.320
And then from then on out, the trial process will be like, oh, this thing's immortal.

00:38:15.320 --> 00:38:17.080
I'm not going to change the ref count.

00:38:17.080 --> 00:38:17.400
Okay.

00:38:17.540 --> 00:38:27.000
So this happens, you basically just scan the whole heap right before you do the fork and you're like everything, we're just going to clone this and it becomes unchangeable.

00:38:27.000 --> 00:38:31.260
And then we'll just, at least with regard to its ref count and we'll go from there.

00:38:31.260 --> 00:38:32.080
Yeah.

00:38:32.080 --> 00:38:38.680
And then as long as ideally we also don't, we shouldn't have a lot of global mutable state.

00:38:38.680 --> 00:38:45.700
I think people should be like, you know, if you think about what's in the master process, a process that's like classes and functions.

00:38:46.540 --> 00:38:52.120
And people shouldn't be really going off and mutating those things inside of the worker processes.

00:38:52.120 --> 00:38:52.480
Modules, yeah.

00:38:52.480 --> 00:38:56.040
It seems like something strange is happening if that's going on.

00:38:56.040 --> 00:38:59.800
Maybe let me ask you really quick or let you talk about really quickly this.

00:39:00.340 --> 00:39:12.420
The real benefit here is on Linux, when you fork off these processes, if the memory itself hasn't been changed, that can be shared across the 40 or 60 processes.

00:39:12.420 --> 00:39:19.040
But as soon as that memory changed, it has like a local copy has to be dedicated to that one worker process.

00:39:19.080 --> 00:39:26.360
So silly stuff, simple stuff like I want to pass this string around that happens to be global.

00:39:26.360 --> 00:39:28.720
And then it says, well, it's passed.

00:39:28.720 --> 00:39:35.600
So you've got to add ref to it, which means now you get 60 copies of it all of a sudden.

00:39:36.140 --> 00:39:43.460
Those really simple things, you are able to get lots better memory sharing, which then leads to cache hits versus cache miss and misses.

00:39:43.460 --> 00:39:45.400
And there's like all these knock on effects, right?

00:39:45.400 --> 00:39:45.720
Yeah.

00:39:45.720 --> 00:39:48.100
And it's not just the string itself, right?

00:39:48.100 --> 00:39:50.680
It's the entire page that the string lives on.

00:39:50.880 --> 00:40:01.580
So, you know, you might have a 15 byte string with, you know, a 16 byte object header and you end up copying 4K of memory.

00:40:01.580 --> 00:40:06.560
Because you changed a 6 reference number to a 7 or to a 5.

00:40:06.560 --> 00:40:07.460
Yeah.

00:40:07.460 --> 00:40:08.740
Fascinating.

00:40:08.740 --> 00:40:09.280
Okay.

00:40:09.280 --> 00:40:12.740
Do you think that Python, CPython itself could adopt this?

00:40:12.740 --> 00:40:13.640
Would it make sense?

00:40:13.640 --> 00:40:17.420
We tried to upstream it and there was resistance to it.

00:40:17.420 --> 00:40:22.640
I mean, it is touching something that's very core and it's going to be a bit of a maintenance burden.

00:40:22.640 --> 00:40:29.480
There are other reasons, I think, that people are now talking about wanting to have immortal instances.

00:40:29.480 --> 00:40:33.840
So Eric Snow has been working on sub-interpreters for a long time.

00:40:33.840 --> 00:40:40.600
And I think he has been interested in them recently for sharing objects between interpreters.

00:40:40.600 --> 00:40:47.380
And I think Sam Gross's work on NoGill might have some form of immortal instances as well.

00:40:47.380 --> 00:40:57.320
So maybe the core immortal instances support could land upstream at some point.

00:40:57.320 --> 00:41:05.860
But, you know, maybe the code that actually is walking the heat and is freezing every day, maybe that's very Instagram specific.

00:41:05.860 --> 00:41:08.500
And it doesn't have much value upstream.

00:41:08.500 --> 00:41:18.060
It seems to me that there's probably a set of things that would be good immortal instances for almost any Python process that starts up, right?

00:41:18.060 --> 00:41:22.800
Like before your code runs, everything there probably would be a good candidate for that.

00:41:23.040 --> 00:41:29.960
And, you know, there's potential, like it's kind of scary because ref counts are so frequent.

00:41:29.960 --> 00:41:34.840
And so adding extra code in the ref count process seems risky.

00:41:35.060 --> 00:41:49.780
But if you can't freeze enough stuff that was kind of there before the program started up, that's super core and happening a lot, then maybe it does actually end up making sense for other workloads too.

00:41:49.780 --> 00:41:50.540
Yeah, perhaps.

00:41:50.540 --> 00:41:50.840
Okay.

00:41:50.840 --> 00:41:55.600
So these immortal instances are one of the things that you all have done that's pretty fascinating.

00:41:55.600 --> 00:41:57.560
And also a huge win.

00:41:57.560 --> 00:41:58.920
Something like 5%.

00:41:58.920 --> 00:42:00.200
Yeah, yeah, that's right.

00:42:00.200 --> 00:42:00.760
It says right here.

00:42:00.760 --> 00:42:03.380
Big win in production, 5%.

00:42:03.380 --> 00:42:06.800
And does that mean 5% request per second?

00:42:06.800 --> 00:42:09.740
Is that when you say 5%, is that the metric you're talking about here?

00:42:09.740 --> 00:42:10.000
Yeah.

00:42:10.000 --> 00:42:10.360
Yeah.

00:42:10.360 --> 00:42:21.240
Have you thought about or tested, I'm sure you've thought about, like if this lets you run more worker processes off of increment that, that spawn worker process number?

00:42:21.240 --> 00:42:30.460
I think the developer who worked on this was doing, did look at that number and was looking at tweaking the number of worker processes.

00:42:30.460 --> 00:42:37.180
If I recalling, it got a little bit of pushback from people who were nervous about increasing it.

00:42:37.180 --> 00:42:38.440
Don't mess with this number.

00:42:38.440 --> 00:42:39.760
We never mess with this number.

00:42:39.760 --> 00:42:40.260
What are you doing?

00:42:41.260 --> 00:42:42.780
Yeah, but I hear you.

00:42:42.780 --> 00:42:50.920
I'm just thinking, you know, if it really does create more shared memory, maybe it creates more space on the same hardware for you to actually create more.

00:42:50.920 --> 00:42:57.340
And then that would just possibly allow even a bigger gain in request per second because there's more parallelism.

00:42:57.340 --> 00:43:06.700
I mean, given that it was such a big win, it could have just been that we were already under significant memory pressure and it got us out of significant memory pressure.

00:43:06.700 --> 00:43:08.000
Maybe we had the right number.

00:43:08.000 --> 00:43:09.700
Maybe we had too many hosts.

00:43:09.700 --> 00:43:10.160
I don't know.

00:43:10.160 --> 00:43:11.080
Yeah, yeah, perhaps.

00:43:11.080 --> 00:43:11.520
Perhaps.

00:43:11.520 --> 00:43:16.900
But still, 5% is, as one of the changes, is still a pretty big deal.

00:43:16.900 --> 00:43:17.280
Mm-hmm.

00:43:17.280 --> 00:43:17.820
All right.

00:43:17.820 --> 00:43:21.460
The next one on deck is strict modules.

00:43:21.460 --> 00:43:23.080
Let's talk about strict modules.

00:43:23.080 --> 00:43:27.340
I mean, we've talked about a little bit of things that are kind of related to this.

00:43:27.340 --> 00:43:28.060
You know what I'm saying?

00:43:28.060 --> 00:43:33.780
Like, if you have things that are going off and mutating your things in the master process, it's like, what?

00:43:33.780 --> 00:43:34.640
That's kind of crazy.

00:43:34.640 --> 00:43:38.820
So, strict modules work about performance, actually.

00:43:39.080 --> 00:43:46.040
There's a little bit of performance thought behind it, but now they're really, we're not considering them as a performance feature at all.

00:43:46.040 --> 00:43:48.600
They're more about a reliability feature.

00:43:48.600 --> 00:43:54.720
And so you brought up early on how, like, Python modules, just going off, executing some code.

00:43:55.300 --> 00:43:56.960
Who knows what that code's going to do?

00:43:56.960 --> 00:43:57.320
Right.

00:43:57.320 --> 00:44:03.460
So strict modules is an attempt to tame that process.

00:44:04.420 --> 00:44:09.000
And what we do is we run static analysis over the code.

00:44:09.000 --> 00:44:13.380
I mean, we are basically interpreting the code in a safe interpreter.

00:44:13.940 --> 00:44:22.100
And if the module has any external side effects or depends upon any external side effects, we don't allow it to be imported.

00:44:22.100 --> 00:44:27.600
And so we know that all the modules are side effect free that are strict.

00:44:27.600 --> 00:44:35.600
When you say they're side effect free, does that mean that the importing of them is side effect free or all of the functions are also side effect free?

00:44:35.720 --> 00:44:36.680
The importing of them.

00:44:36.680 --> 00:44:38.500
Their functions can do whatever they want.

00:44:38.500 --> 00:44:38.900
Got it.

00:44:38.900 --> 00:44:41.080
They can call functions from other modules.

00:44:41.080 --> 00:44:43.100
They can call functions from themselves.

00:44:43.100 --> 00:44:50.400
If they call those modules at the top level while doing the import, then those functions need to be side effect free.

00:44:50.400 --> 00:44:51.300
So where does this lead you?

00:44:51.300 --> 00:44:52.240
What do you get out of this?

00:44:52.240 --> 00:44:54.400
We get additional reliability.

00:44:54.400 --> 00:45:04.300
So like, you know, Instagram, as I think maybe we mentioned this being a big Bono monolithic application.

00:45:04.300 --> 00:45:05.780
Maybe we didn't get to that.

00:45:05.780 --> 00:45:07.020
Yeah, I don't think we talked about that.

00:45:07.020 --> 00:45:10.460
But this is not a hundred microservices type of thing, is it?

00:45:10.460 --> 00:45:12.740
No, it's one giant application.

00:45:12.740 --> 00:45:16.780
The thing that gets redeployed every 10 minutes is that giant application.

00:45:16.780 --> 00:45:19.480
That makes the redeployment even more impressive, by the way.

00:45:19.480 --> 00:45:20.960
Right?

00:45:20.960 --> 00:45:27.460
Yeah, I mean, maybe it's nice in that it's one giant application because you just have to redeploy one thing.

00:45:27.460 --> 00:45:29.440
Yeah, exactly.

00:45:29.440 --> 00:45:34.060
It's not a hundred different things you got to keep in sync all at the same time, right?

00:45:34.060 --> 00:45:34.620
Yeah.

00:45:34.620 --> 00:45:36.560
Our PEs make that happen.

00:45:36.560 --> 00:45:39.340
And it just happens behind the scenes as far as I'm concerned.

00:45:39.340 --> 00:45:50.240
So, you know, if you have like, you know, if you import one module and it depends on side effects from another module and then something changes,

00:45:50.240 --> 00:46:00.380
there's the import order, whether that's like state that things are depending upon, suddenly things blow up in production and your site doesn't work and everyone's really sad.

00:46:00.380 --> 00:46:06.100
So this is like, we want to get to a world where our modules are completely safe.

00:46:06.100 --> 00:46:07.640
We've used this.

00:46:07.640 --> 00:46:09.980
We've experimented doing other things with this.

00:46:09.980 --> 00:46:12.280
So like adding a hot reload capability.

00:46:12.280 --> 00:46:15.260
We know the modules are completely side effect free.

00:46:15.440 --> 00:46:22.280
Why not just patch the module in place and like let developers move on without restarting the website.

00:46:22.280 --> 00:46:27.720
It has the potential to kind of really change the way we store modules.

00:46:27.720 --> 00:46:35.680
So that we haven't gone down this route yet where instead of storing modules is a bunch of Python code that needs to run off and execute.

00:46:35.680 --> 00:46:38.860
Could we store modules as like, here's a class definition.

00:46:38.860 --> 00:46:40.480
Here's a function.

00:46:41.280 --> 00:46:45.600
And can we lazily load portions of the modules out of there?

00:46:45.600 --> 00:46:51.040
But we also have a really other different take on lazy loading that's in center now too.

00:46:51.040 --> 00:46:51.700
Okay.

00:46:51.700 --> 00:46:53.700
Yeah, that's pretty interesting.

00:46:53.700 --> 00:47:04.160
Because normally you can't re-import something because maybe you've set some kind of static value on a class.

00:47:04.160 --> 00:47:08.760
You've set some module level variable and that'll get wiped away, right?

00:47:08.760 --> 00:47:11.360
I mean, you can call reload on a module.

00:47:11.360 --> 00:47:16.080
But whether or not that's the safe thing to do, who knows?

00:47:16.080 --> 00:47:17.240
Exactly.

00:47:17.240 --> 00:47:18.040
Exactly.

00:47:18.040 --> 00:47:19.320
All right, cool.

00:47:19.320 --> 00:47:29.040
So I think one of the more interesting areas, probably the two that really stood out to me, are the JIT and StaticPython with the immortal objects being right behind it.

00:47:29.040 --> 00:47:31.260
But StaticPython, this is your area, right?

00:47:31.260 --> 00:47:31.740
What is this?

00:47:31.740 --> 00:47:31.980
Yeah.

00:47:31.980 --> 00:47:40.740
So this is an attempt to leverage the types that we already have throughout our entire code base.

00:47:40.740 --> 00:47:47.380
So Instagram is 100% typed, although there are still some many types flowing around.

00:47:47.660 --> 00:47:50.260
But you can't add code that isn't typed.

00:47:50.260 --> 00:47:52.880
So we know the types of things.

00:47:52.880 --> 00:47:53.260
Right.

00:47:53.260 --> 00:47:59.940
You're talking traditional just colon int, colon str, optional str, that type of typing.

00:47:59.940 --> 00:48:00.280
Yeah.

00:48:00.280 --> 00:48:00.420
Yeah.

00:48:00.420 --> 00:48:10.160
So why not add a compilation step when we're compiling things to PYCs instead of just ignoring the types?

00:48:10.160 --> 00:48:11.780
Why don't we pay attention to the types?

00:48:11.960 --> 00:48:12.140
Yeah.

00:48:12.140 --> 00:48:15.720
So we have a compiler that's written in Python.

00:48:15.720 --> 00:48:21.180
There's actually this old compiler package that started in Python 2.

00:48:21.180 --> 00:48:30.520
There's this external, there's this developer on GitHub, PF Falcon, who upgraded it to Python 3 at some point.

00:48:30.520 --> 00:48:39.500
And we upgraded it to Python 3.8 and made it match CPython identical for bytecode generation.

00:48:39.500 --> 00:48:44.380
So we have this great Python code base to work in, to write a compiler in.

00:48:44.620 --> 00:48:47.300
And we analyze the type annotations.

00:48:47.300 --> 00:48:56.300
And then we have runtime support and a set of new opcodes that can much more efficiently dispatch to things.

00:48:56.300 --> 00:49:04.080
There's a great, my coworker Carl Meyer had this awesome slide of calling a function during a Python talk.

00:49:04.220 --> 00:49:13.580
And it was just like pages, well, it was one page and a very, very tiny font of the assembly of what it takes for CPython to invoke a function.

00:49:13.580 --> 00:49:19.780
And then we're able to just directly call a function using the x64 calling convention.

00:49:19.780 --> 00:49:23.300
So shuffle a few registers around and admit a call instruction.

00:49:23.540 --> 00:49:23.940
That's awesome.

00:49:23.940 --> 00:49:29.980
It surprised me when I first got into Python, how expensive calling a function was.

00:49:29.980 --> 00:49:33.520
Not regardless of what it does, just the act of calling it.

00:49:33.520 --> 00:49:40.980
You know, coming from C# and C++, where you think so good in line by either the compiler or the JIT compiler and all sorts of interesting things.

00:49:40.980 --> 00:49:42.320
You're like, wait, this is expensive.

00:49:42.320 --> 00:49:46.100
I should consider whether or not I'm calling a function in a tight loop.

00:49:46.100 --> 00:49:49.540
There's so many things it has to deal with.

00:49:49.740 --> 00:49:56.740
The JIT has to deal with adding the default values then, and you don't know whether you're going to have to do that until you get to the function.

00:49:56.740 --> 00:50:03.080
It's got to deal with taking keyword arguments and mapping those onto the correct keywords.

00:50:03.080 --> 00:50:07.800
And that's one thing in static Python, we do that at compile time.

00:50:07.800 --> 00:50:13.620
If you're calling the keyword arguments, they turn into positional arguments because we know what we're going to.

00:50:13.620 --> 00:50:19.340
And we can just shuffle those around at compile time and just save a whole bunch of overhead.

00:50:19.520 --> 00:50:20.100
Yeah, that's fantastic.

00:50:20.100 --> 00:50:30.020
So the way people should think of this is maybe like mypyC or Cython, where it looks like regular Python, but then out the other side comes better stuff.

00:50:30.020 --> 00:50:36.580
Except for the difference here is you guys do it at JIT, not some sort of ahead of time pre-deployment type of thing.

00:50:36.580 --> 00:50:36.840
Yeah.

00:50:36.840 --> 00:50:44.920
And so the first thing we did with it was actually we had 40 Cython modules that were inside of the Instagram code base.

00:50:45.240 --> 00:50:49.560
And that was a big developer pain point and that those things had to be rebuilt.

00:50:49.560 --> 00:50:55.760
The tooling for like editing them wasn't as good because you don't get syntax color highlighting.

00:50:56.460 --> 00:50:58.740
And so we were able to just get rid of all of those.

00:50:58.740 --> 00:51:03.380
And those were heavily tuned, like using a bunch of Cython features.

00:51:03.380 --> 00:51:12.360
And so that really kind of proved things out that like, if we need to use low level features, we support things like permanent events if you want to use them.

00:51:12.720 --> 00:51:16.440
Instead of like having boxed variable size ends.

00:51:16.440 --> 00:51:20.800
So that was a good proving that it worked.

00:51:20.800 --> 00:51:32.380
And now I think it's more close to my PyC at runtime as we've been going through and converting other modules to static Python within the Instagram code base.

00:51:32.380 --> 00:51:33.160
Yeah, fantastic.

00:51:33.160 --> 00:51:43.060
You guys say that static Python plus sender JIT achieves seven times performance improvements over CPython on the type version of Richard's benchmark.

00:51:43.060 --> 00:51:45.120
I mean, obviously you got to be specific, right?

00:51:45.120 --> 00:51:46.500
But still, that's a huge difference.

00:51:47.240 --> 00:51:47.500
Yep.

00:51:47.500 --> 00:51:52.680
And some of that's like the ability to use primitive integers.

00:51:52.680 --> 00:52:03.260
Some of that's the ability to use b tables for invoking functions instead of having to do the dynamic lookup, which is something that both my PyC and Cython support.

00:52:03.260 --> 00:52:06.280
So lots of little things end up adding up a lot.

00:52:06.280 --> 00:52:07.380
And so that's just the JIT.

00:52:07.380 --> 00:52:08.820
Yeah, that's fantastic.

00:52:08.820 --> 00:52:13.420
Talk Python to me is partially supported by our training courses.

00:52:13.880 --> 00:52:16.200
We have a new course over at Talk Python.

00:52:16.200 --> 00:52:18.080
HTMX plus Flask.

00:52:18.080 --> 00:52:20.360
Modern Python web apps hold the JavaScript.

00:52:20.360 --> 00:52:24.580
HTMX is one of the hottest properties in web development today.

00:52:24.580 --> 00:52:25.560
And for good reason.

00:52:25.560 --> 00:52:30.660
You might even remember all the stuff we talked about with Carson Gross back on episode 321.

00:52:30.660 --> 00:52:38.140
HTMX, along with the libraries and techniques we introduced in our new course, will have you writing the best Python web apps you've ever written.

00:52:38.140 --> 00:52:39.940
Clean, fast, and interactive.

00:52:40.360 --> 00:52:41.720
All without that front-end overhead.

00:52:41.720 --> 00:52:54.300
If you're a Python web developer that has wanted to build more dynamic, interactive apps, but don't want to or can't write a significant portion of your app in rich front-end JavaScript frameworks, you'll absolutely love HTMX.

00:52:54.300 --> 00:53:00.300
Check it out over at talkpython.fm/HTMX or just click the link in your podcast player show notes.

00:53:02.300 --> 00:53:05.740
You've talked about using primitive integers.

00:53:06.240 --> 00:53:10.240
And I've always thought that Python should support this idea somehow.

00:53:10.240 --> 00:53:22.980
Like, if you're doing some operation, like computing the square root or something, you take two numbers, two integers, and do some math, you know, maybe multiply, you know, square them, and then subtract them or something like that.

00:53:23.700 --> 00:53:30.400
And all of that stuff goes through a really high overhead version of what a number is, right?

00:53:30.400 --> 00:53:39.760
Like, instead of being a four or eight byte thing on a register, it's 50 bytes or something like that.

00:53:39.860 --> 00:53:46.420
As a pi object long thing that gets ref counted, and then, like, somewhere in there is the number bit.

00:53:46.420 --> 00:53:49.340
And that's awesome because it supports having huge numbers.

00:53:49.340 --> 00:53:53.260
Like, you don't ever see negative 2.1 billion when you're adding.

00:53:53.260 --> 00:53:55.940
You increment a number by one in Python, which is great.

00:53:55.940 --> 00:54:02.380
But it also means that at certain times you're doing math is just so much slower because you can't use registers.

00:54:02.380 --> 00:54:05.760
You've got to use, like, complex math, right?

00:54:05.760 --> 00:54:14.380
It sounds like you're doing this, like, let's treat this number as a small number rather than a pi object pointer drive thing.

00:54:14.380 --> 00:54:17.780
You know, JITs can handle this to some degree, right?

00:54:17.780 --> 00:54:24.780
And they can recognize that things are small numbers and generate more efficient code.

00:54:24.780 --> 00:54:27.960
I think when you had Anthony Yanni is talking about Pigeon doing this.

00:54:27.960 --> 00:54:28.160
Yeah.

00:54:28.160 --> 00:54:35.500
You know, if there's still some overhead there for dealing with the cases where you have to bail out and it's not that case.

00:54:35.500 --> 00:54:39.200
It's nice just having the straight line code that's there.

00:54:39.200 --> 00:54:42.800
You can also do type pointers, which, again, kind of handle that.

00:54:42.800 --> 00:54:48.500
Type pointers are kind of difficult on CPython because things expect pi object stars.

00:54:48.500 --> 00:54:55.140
And if that pi object star ever escapes to something that's not your CPython code, it's going to be very unhappy.

00:54:55.140 --> 00:54:55.500
Yeah.

00:54:55.500 --> 00:55:00.180
So this is, I mean, the nice thing is it's a relatively straightforward way to allow it.

00:55:00.580 --> 00:55:07.960
It was actually a little bit controversial in that, like, is this really what Python developers are going to expect?

00:55:07.960 --> 00:55:09.940
And are we going to have the right semantics there?

00:55:09.940 --> 00:55:19.640
And I think we have a to-do item to actually make things raise overflow errors if they do overflow instead of flowing over to negative 2 billion.

00:55:19.640 --> 00:55:20.900
That would be fantastic.

00:55:20.900 --> 00:55:32.500
I would personally rather see an overflow error than have it, you know, wrap around to the negative side or go back to zero if it's unsigned or whatever terrible outcome you're going to get there.

00:55:32.500 --> 00:55:33.200
Yeah.

00:55:33.200 --> 00:55:34.920
It's a much more reasonable behavior.

00:55:34.920 --> 00:55:38.440
We just, I guess we haven't been very motivated to actually go and fix that.

00:55:38.880 --> 00:55:44.080
Well, you're probably not doing the type of processing that would lead to that, right?

00:55:44.080 --> 00:55:51.720
You're probably not doing, like, scientific stuff where all of a sudden, you know, you took a factorial too big or you did some insane thing like that.

00:55:51.720 --> 00:55:55.180
There's probably not a single factorial in the entire code base, I would guess.

00:55:55.180 --> 00:55:55.460
Yeah.

00:55:55.460 --> 00:55:57.100
There's not a lot of math.

00:55:57.100 --> 00:56:07.800
There was, like, some, like, the only place where you've used primitive integers really was in the existing conversion, in the conversion of the existing Cython code where people had resorted to them.

00:56:08.020 --> 00:56:08.140
Right.

00:56:08.140 --> 00:56:11.980
Because it probably started as an int 32 or an int 64, right?

00:56:11.980 --> 00:56:12.320
Yeah.

00:56:12.320 --> 00:56:13.120
Yeah.

00:56:13.120 --> 00:56:15.120
Like, they had that option available to them.

00:56:15.120 --> 00:56:16.080
They used it.

00:56:16.080 --> 00:56:21.060
It's not, like, something that we're going through and sprinkling in in our random Python code.

00:56:21.060 --> 00:56:22.780
Because, like, yeah, we don't do much math.

00:56:22.780 --> 00:56:24.260
It's very object-oriented.

00:56:24.260 --> 00:56:26.040
Lots of function calls.

00:56:26.040 --> 00:56:27.680
Lots of classes.

00:56:27.680 --> 00:56:28.340
Yeah.

00:56:28.340 --> 00:56:29.240
Absolutely.

00:56:29.240 --> 00:56:30.460
All right.

00:56:30.460 --> 00:56:35.380
There's a lot of other good things that you talked about that are not necessarily listed right here.

00:56:35.780 --> 00:56:38.740
sort of, kind of, stuff with async and await.

00:56:38.740 --> 00:56:41.260
It sounds like you guys use async and await a lot.

00:56:41.260 --> 00:56:41.620
Is that right?

00:56:41.620 --> 00:56:41.960
Yeah.

00:56:41.960 --> 00:56:43.980
The entire code base is basically async.

00:56:43.980 --> 00:56:49.760
There was a big conversion, a big push to convert it right as I was starting.

00:56:50.520 --> 00:56:53.240
And now everything basically is async.

00:56:53.240 --> 00:56:55.020
Unless, obviously, it's not as...

00:56:55.020 --> 00:56:55.720
Wait a minute.

00:56:55.720 --> 00:56:57.220
I heard that async and await is slow.

00:56:57.220 --> 00:56:58.240
Why would you ever use that?

00:56:58.240 --> 00:57:00.240
Because it allows additional parallelization.

00:57:00.240 --> 00:57:01.600
Oh, yeah, yeah.

00:57:01.600 --> 00:57:04.440
Because multiple requests can be served by the same worker.

00:57:04.440 --> 00:57:04.800
Sure.

00:57:05.260 --> 00:57:11.280
Well, you know, whenever I hear those, I see examples of, like, we're just calling something as fast as you can.

00:57:11.280 --> 00:57:13.820
And it doesn't really provide...

00:57:13.820 --> 00:57:15.800
There's not an actual waiting, right?

00:57:15.800 --> 00:57:18.320
Like, the async and await is really good to scale the time.

00:57:18.320 --> 00:57:20.140
When you're waiting, do something else.

00:57:20.140 --> 00:57:22.060
And a lot of the examples say, well, this is slower.

00:57:22.060 --> 00:57:23.340
There's, like, no waiting period.

00:57:23.340 --> 00:57:25.360
But you know what is a really good slow thing?

00:57:25.360 --> 00:57:27.060
An external API and a database.

00:57:27.220 --> 00:57:29.260
And it sounds like you guys probably talk to those things.

00:57:29.260 --> 00:57:30.640
And yes.

00:57:30.640 --> 00:57:37.300
And the no waiting case is actually what this eager co-routine evaluation is all about.

00:57:37.300 --> 00:57:39.660
Like, yeah, sometimes we're talking to a database.

00:57:39.660 --> 00:57:44.360
But sometimes you have a function that's like, have I fetched this from the database?

00:57:44.360 --> 00:57:46.560
Okay, here it is.

00:57:46.560 --> 00:57:47.940
I don't have to wait for it.

00:57:47.940 --> 00:57:50.500
Otherwise, I'll go off and fetch it from the database.

00:57:50.500 --> 00:57:50.880
Right.

00:57:50.880 --> 00:57:53.540
If there's an early return before the first await.

00:57:53.540 --> 00:57:54.400
Exactly.

00:57:54.400 --> 00:57:56.660
There's not a huge value to calling this, right?

00:57:56.660 --> 00:57:57.140
Yeah.

00:57:57.140 --> 00:58:00.700
So tell us about this eager co-routine evaluation, which deals with that, right?

00:58:00.700 --> 00:58:01.100
Yeah.

00:58:01.100 --> 00:58:08.780
So this lets us run the function up to the first await and only go off and kind of.

00:58:08.780 --> 00:58:17.580
So normally what happens is you produce your co-routine object, schedule that on your event loop,

00:58:17.580 --> 00:58:19.260
and then eventually it'll get called.

00:58:19.260 --> 00:58:21.800
And now when you call the function, it's going to run.

00:58:21.800 --> 00:58:24.220
It's going to immediately run up to the first await.

00:58:24.720 --> 00:58:28.540
And if it doesn't hit that first await, it's just going to have the value that's produced.

00:58:28.540 --> 00:58:35.900
And you're not going to have to go through this big churn of going through the event loop with this whole co-routine object.

00:58:36.220 --> 00:58:36.920
Yeah, that's fantastic.

00:58:36.920 --> 00:58:37.560
Yeah.

00:58:37.560 --> 00:58:37.640
Yeah.

00:58:37.640 --> 00:58:37.720
Yeah.

00:58:37.720 --> 00:58:37.840
Yeah.

00:58:37.840 --> 00:58:52.220
It is slightly different semantics because now you could have some CPU heavy thing, which is just like not sharing with CPU with other workers, which is a great.

00:58:52.980 --> 00:59:04.740
And I think it can end up kind of, I think there can be some slight differences on what the scheduling happens, like where you could have observable differences, but we haven't had any issues with that.

00:59:05.280 --> 00:59:11.660
So I think it's might be a little bit controversial, but it's such a big when that it makes a lot of sense for us.

00:59:11.660 --> 00:59:13.260
It certainly could change the order.

00:59:13.520 --> 00:59:23.180
If you're doing, here's a whole bunch of co-routines and a bunch of awaits and stuff, and then you ran them in one mode, the sort of standard mode versus this, you would get a different order.

00:59:23.180 --> 00:59:28.980
But, you know, I mean, it sounds like you're going to ultimately put the same amount of CPU load on.

00:59:28.980 --> 00:59:32.940
I mean, async and await runs on one thread anyway, generally.

00:59:32.940 --> 00:59:33.520
Yeah.

00:59:33.640 --> 00:59:39.060
Unless you do something funky to like wrap some kind of thread or something, but in general, it still runs there.

00:59:39.060 --> 00:59:43.420
I would hope that most people aren't super dependent upon the order.

00:59:43.420 --> 00:59:49.480
If you're dependent upon the order and you're doing threading or something like that, you're doing it wrong.

00:59:49.480 --> 00:59:50.040
Yeah.

00:59:50.040 --> 00:59:53.360
The fairness issue might be a bigger issue.

00:59:53.360 --> 00:59:54.280
Yeah, yeah, yeah.

00:59:54.280 --> 00:59:54.680
Yeah.

00:59:54.680 --> 00:59:56.060
For us, it makes a lot of sense.

00:59:56.060 --> 00:59:57.500
Yeah, that's really cool.

00:59:57.500 --> 00:59:58.640
All right.

00:59:58.640 --> 01:00:01.560
Another one was shadow code or shadow byte code.

01:00:01.560 --> 01:00:02.000
Yeah.

01:00:02.440 --> 01:00:04.980
So this is our inline caching implementation.

01:00:04.980 --> 01:00:08.520
We've had this for a few years.

01:00:08.520 --> 01:00:12.740
Python 3.11 is getting something very similar.

01:00:12.740 --> 01:00:16.280
So we kind of expect that our version will be going away.

01:00:16.280 --> 01:00:22.560
We'll have to see if there's any cases that aren't covered or if there's any performance differences.

01:00:22.560 --> 01:00:25.980
But basically, it's nearly identical.

01:00:25.980 --> 01:00:37.720
We have an extra copy of the byte code, which is why it's called shadow byte code, which we can mutate in the background and replace the normal opcodes with specialized ones.

01:00:38.400 --> 01:00:52.620
So if we're doing a load adder and that load adder is an instance of a specific type, we can just say, okay, well, we know that this load adder doesn't have a type descriptor associated with it.

01:00:52.620 --> 01:00:56.900
Descriptor associated with it, like a get set data descriptor.

01:00:57.080 --> 01:01:08.280
We know that the instance has a split dictionary, which is the way CPython shares dictionaries, dictionary layout between instances of classes.

01:01:08.700 --> 01:01:13.500
We know this attribute is at offset two within split dictionary.

01:01:13.500 --> 01:01:22.520
So we just do a simple type check and make sure that the type is still compatible and go off and look in the instance dictionary and pull the value out.

01:01:22.520 --> 01:01:31.280
Instead of going through and looking up all those other things that I've just described, which is kind of what you have to do every single time on a normal load adder.

01:01:31.280 --> 01:01:32.660
Yeah, that's really cool.

01:01:32.660 --> 01:01:35.700
Is this something that could come back to CPython?

01:01:35.700 --> 01:01:40.980
I think the fact that they've gone off and built their own version 3.11 means that's not going to happen.

01:01:40.980 --> 01:01:44.020
But the idea lives there.

01:01:44.020 --> 01:01:44.740
Yes.

01:01:44.740 --> 01:01:45.140
Yeah.

01:01:45.140 --> 01:01:45.520
Yeah.

01:01:45.520 --> 01:01:45.900
Okay.

01:01:45.900 --> 01:01:46.240
Awesome.

01:01:46.240 --> 01:02:02.560
So we're getting short on time here, but maybe you could just highlight really quickly, stepping back one feature point on the asyncio stuff is the send receive without stop iteration stuff that you did.

01:02:02.560 --> 01:02:05.040
And then that getting upstreamed as well already.

01:02:05.040 --> 01:02:05.520
Yeah.

01:02:05.520 --> 01:02:05.580
Yeah.

01:02:05.580 --> 01:02:07.560
So that was adding.

01:02:07.560 --> 01:02:22.740
So I did work on this developer Vladimir Mativ worked on this and that was adding in a, I think he added in a new set of slots for actually achieving this at the end of the day.

01:02:22.740 --> 01:02:23.740
And sender.

01:02:23.740 --> 01:02:23.740
And sender.

01:02:23.740 --> 01:02:23.740
And sender.

01:02:23.740 --> 01:02:23.740
And sender.

01:02:23.740 --> 01:02:23.740
And sender.

01:02:23.740 --> 01:02:27.800
We have a type flag that says this type has these additional slots.

01:02:27.800 --> 01:02:34.460
And so we can call the send function and the receive function and get back and be done.

01:02:34.460 --> 01:02:37.400
That's kind of did this thing return a result?

01:02:37.400 --> 01:02:39.320
Did this thing throw an exception?

01:02:39.320 --> 01:02:41.560
And here's the result.

01:02:41.560 --> 01:02:41.900
Yeah.

01:02:41.900 --> 01:02:48.720
So that instead of producing the stop iteration on every single result, we just return the result.

01:02:48.720 --> 01:02:55.780
And that is obviously big with coroutines because coroutines are generators at the end of the day.

01:02:55.780 --> 01:02:56.080
Yeah.

01:02:56.080 --> 01:02:56.680
That's fantastic.

01:02:57.220 --> 01:03:03.240
Everything can get more efficient by not allocating on sort of hidden behind the scene exceptions, right?

01:03:03.240 --> 01:03:03.640
Yeah.

01:03:03.640 --> 01:03:04.180
All right.

01:03:04.180 --> 01:03:06.140
Well, there's a bunch of cool stuff here.

01:03:06.140 --> 01:03:11.760
And I'm really happy to hear that you and your team and Edomar out there are working on bringing this stuff over.

01:03:11.880 --> 01:03:13.360
Because I was so excited when I saw it.

01:03:13.360 --> 01:03:14.260
And then I saw, is it supported?

01:03:14.260 --> 01:03:15.460
Like, not really.

01:03:15.460 --> 01:03:16.700
You really shouldn't use this.

01:03:16.700 --> 01:03:18.260
I'm like, oh, but it looks so good.

01:03:18.260 --> 01:03:20.940
Like, I want so much of this stuff to be moved over.

01:03:20.940 --> 01:03:21.720
So that's cool.

01:03:21.720 --> 01:03:24.780
And I think some of it will be difficult to move over.

01:03:24.780 --> 01:03:28.220
Like, in moving the entire JIT over, the JIT's written in C++.

01:03:28.220 --> 01:03:35.940
Obviously, the CPython core developers were open to C++ for a JIT at one point in time with unladen swallow.

01:03:35.940 --> 01:03:39.980
Whether or not that feeling has changed, who knows.

01:03:40.460 --> 01:03:42.680
But it's a big piece of code to drop in.

01:03:42.680 --> 01:03:51.260
So one thing that we really want to do going forward is actually get to the point where the big pieces of sender are actually just pip installable.

01:03:51.260 --> 01:03:55.960
So we'll work on getting the hooks that we need upstreamed.

01:03:55.960 --> 01:04:02.480
One thing that the JIT relies on a lot is dictionary watchers that we can do really super fast global loads.

01:04:02.480 --> 01:04:10.440
And we have a bunch of hooks into, like, type modification and function modification that aren't super onerous by any means.

01:04:10.440 --> 01:04:11.060
Yeah.

01:04:11.060 --> 01:04:15.660
So if we can get those upstream, then we can make the JIT just be, here, pip install us.

01:04:15.660 --> 01:04:24.620
And so hopefully we can get those upstreamed in 3.11 and have pip install sender start working.

01:04:24.780 --> 01:04:25.000
Yeah.

01:04:25.000 --> 01:04:26.080
That'd be awesome.

01:04:26.080 --> 01:04:26.400
Yeah.

01:04:26.400 --> 01:04:27.960
So, yeah, really good work on these.

01:04:27.960 --> 01:04:31.640
I guess let's wrap up our conversation here because we're definitely short on time.

01:04:31.640 --> 01:04:39.820
But, you know, there's the other projects, which I'm going to start calling the Shannon Plan that Mark and Guido are working on.

01:04:39.820 --> 01:04:41.180
They've been working on for a year.

01:04:41.700 --> 01:04:48.220
And then there's Pidgin, which, by the way, Anthony Shaw has taken over, but you created Pidgin, right?

01:04:48.220 --> 01:04:48.620
Yep.

01:04:48.620 --> 01:04:49.700
That's awesome.

01:04:49.700 --> 01:04:51.760
Well done on that.

01:04:51.760 --> 01:04:52.960
On a whim at a playtime.

01:04:54.800 --> 01:04:55.200
Exactly.

01:04:55.200 --> 01:04:59.060
And Sam Gross's work on the NoGill stuff.

01:04:59.060 --> 01:05:04.300
All of this seems to be independent, but in the same area as those things.

01:05:04.300 --> 01:05:05.500
Where do you see the synergies?

01:05:05.500 --> 01:05:07.920
Do you see any chance for those to, like, come together?

01:05:07.920 --> 01:05:12.620
Is that through some kind of pip putting the right hooks in there and other people plugging in what they want?

01:05:12.620 --> 01:05:14.080
Or what do you see there?

01:05:14.080 --> 01:05:16.260
It'd be great if these could come together a little bit.

01:05:16.260 --> 01:05:16.620
Yeah.

01:05:16.620 --> 01:05:20.680
In a lot of places, we're working on independent things.

01:05:21.180 --> 01:05:23.660
Obviously, Pidgin is a JIT and we're a JIT.

01:05:23.660 --> 01:05:25.460
With different goals to some degree, right?

01:05:25.460 --> 01:05:26.540
Yeah.

01:05:26.540 --> 01:05:30.500
But, I mean, also very similar and overlapping goals.

01:05:30.500 --> 01:05:37.320
I think there'll probably have to be discussion of, like, what the future of JITs look like in CPython.

01:05:37.320 --> 01:05:39.400
Like, is that something that's part of the core?

01:05:39.400 --> 01:05:43.360
Or is that something that should live on as being external?

01:05:43.360 --> 01:05:48.360
Or is there going to be a grand competition and at one point one of the JITs will win?

01:05:48.360 --> 01:05:49.340
Who knows?

01:05:49.560 --> 01:05:52.520
It's a good discussion that should probably take place.

01:05:52.520 --> 01:05:54.740
The hooks for JITs are there.

01:05:54.740 --> 01:06:03.600
And between what Brett and I added for Pidgin and Mark Shannon's Vector Call work that happened several releases ago,

01:06:03.600 --> 01:06:08.780
I think JITs have a pretty good foundation for booking in and replacing code execution.

01:06:08.780 --> 01:06:15.580
They probably need other books to, you know, get into other things like the Dictionary Watchers that I mentioned.

01:06:15.580 --> 01:06:18.440
But, like, we can keep working on books.

01:06:18.440 --> 01:06:20.220
Other things have less overlap.

01:06:20.220 --> 01:06:32.140
So, hopefully we can all kind of work in our own streets and work to improve things and make those available to Python developers in the best way that's available.

01:06:32.140 --> 01:06:36.920
And not be stomping on each other's shoes or do profiting work too much.

01:06:37.060 --> 01:06:37.740
Yeah, absolutely.

01:06:37.740 --> 01:06:40.040
Well, it's an exciting time.

01:06:40.040 --> 01:06:42.860
I feel like a lot of stuff is sort of coming back to the forefront.

01:06:42.860 --> 01:06:44.680
And it feels like...

01:06:44.680 --> 01:06:45.680
So much performance work.

01:06:45.680 --> 01:06:46.760
Yeah, for sure.

01:06:46.760 --> 01:06:54.180
It feels like the core developers are open to hearing about it and taking on some of the, you know, the disruption and complexity that might come from it.

01:06:54.240 --> 01:06:56.800
But still, it could be valuable, right?

01:06:56.800 --> 01:06:57.160
Mm-hmm.

01:06:57.160 --> 01:06:59.460
It's absolutely going to be valuable.

01:06:59.460 --> 01:07:00.000
Yeah.

01:07:00.000 --> 01:07:03.460
I feel like there's enough pressure from other languages like Go and Rust and stuff.

01:07:03.460 --> 01:07:07.820
Oh, you should come over to our world and forget that Python stuff.

01:07:07.820 --> 01:07:08.880
You're like, hold on, hold on, hold on.

01:07:08.880 --> 01:07:09.700
We can just...

01:07:09.700 --> 01:07:10.440
We can do that too.

01:07:10.440 --> 01:07:11.320
But we've got to...

01:07:11.320 --> 01:07:12.000
We can get faster.

01:07:12.000 --> 01:07:12.360
Yeah.

01:07:12.360 --> 01:07:13.960
Well, this is awesome work.

01:07:13.960 --> 01:07:15.620
Thanks for coming on and sharing.

01:07:15.620 --> 01:07:16.680
Thank you for having me.

01:07:16.960 --> 01:07:18.120
Yeah, you and your team are doing...

01:07:18.120 --> 01:07:20.520
Now, before you get out of here, got the final two questions.

01:07:20.520 --> 01:07:21.540
Okay.

01:07:21.540 --> 01:07:22.540
You're going to write some...

01:07:22.540 --> 01:07:24.520
Let's do notable PyPI package first.

01:07:24.520 --> 01:07:28.460
So is there some library or notable package out there that you come across?

01:07:28.460 --> 01:07:29.460
Like, oh, this thing's awesome.

01:07:29.460 --> 01:07:31.300
People should know about whatever.

01:07:31.300 --> 01:07:33.100
So does it have to be PyPI?

01:07:33.100 --> 01:07:34.200
No, any project.

01:07:34.200 --> 01:07:38.540
So as I said, I have a very weird relationship with Python, right?

01:07:38.540 --> 01:07:42.700
As using mainly from the implementation side.

01:07:42.700 --> 01:07:45.880
So I think my favorite package is the standard library.

01:07:45.880 --> 01:07:46.340
Okay.

01:07:46.700 --> 01:07:47.200
Right on.

01:07:47.200 --> 01:07:51.720
And if I had to pick something out of the standard library, I think one of the coolest parts is

01:07:51.720 --> 01:07:52.140
mock.

01:07:52.140 --> 01:07:59.240
It's been an interesting integration with static Python, but like it, like seeing the way people

01:07:59.240 --> 01:08:03.200
use it and drive their tests, it's kind of really kind of amazing.

01:08:03.200 --> 01:08:04.020
Yeah, I agree.

01:08:04.020 --> 01:08:07.260
It's definitely a very cool one people should certainly be using.

01:08:07.260 --> 01:08:10.740
And now if you're going to write some Python code, you might also have special requirements

01:08:10.740 --> 01:08:12.100
that shift you in one way or the other.

01:08:12.100 --> 01:08:13.480
But what editor are you using?

01:08:13.480 --> 01:08:15.700
Oh, I use VS Code pretty much.

01:08:15.840 --> 01:08:17.000
Well, I use VS Code.

01:08:17.000 --> 01:08:20.880
I use nano when I need to make a quick edit from the command prompt.

01:08:21.120 --> 01:08:21.480
Yeah, cool.

01:08:21.480 --> 01:08:22.780
I'm a fan of nano as well.

01:08:22.780 --> 01:08:23.820
Like, let's just keep it simple.

01:08:23.820 --> 01:08:25.040
It's just give me a nano.

01:08:25.040 --> 01:08:26.380
Let me edit this thing over the shelf.

01:08:26.380 --> 01:08:29.020
It has syntax cover highlighting that.

01:08:29.020 --> 01:08:31.120
It's so advanced.

01:08:31.120 --> 01:08:31.640
It's awesome.

01:08:31.640 --> 01:08:32.240
Cool.

01:08:32.240 --> 01:08:33.240
No, no, I use it as well.

01:08:33.240 --> 01:08:33.820
All right.

01:08:33.820 --> 01:08:35.780
Well, Dino, thank you so much for being here.

01:08:35.780 --> 01:08:36.820
Final call to action.

01:08:36.820 --> 01:08:38.460
People are excited about these ideas.

01:08:38.460 --> 01:08:40.340
Maybe they want to contribute back or try them out.

01:08:40.340 --> 01:08:40.880
What do you say?

01:08:41.060 --> 01:08:42.220
I mean, try out sender.

01:08:42.220 --> 01:08:43.320
Yeah, it's unsupported.

01:08:43.320 --> 01:08:46.260
But, you know, if you have thoughts on it, that's cool.

01:08:46.260 --> 01:08:48.600
You do have instructions on how to build it right here.

01:08:48.600 --> 01:08:49.640
So you could check it out.

01:08:49.640 --> 01:08:50.660
There's a Docker container.

01:08:50.660 --> 01:08:51.480
Yeah.

01:08:51.480 --> 01:08:51.680
Okay.

01:08:51.680 --> 01:08:52.100
Yeah.

01:08:52.620 --> 01:08:55.200
So it's pretty easy to give it a shot.

01:08:55.200 --> 01:09:01.160
You know, like, it might be harder to get it up and running in a perf-sensitive environment.

01:09:01.160 --> 01:09:04.020
If you want to try out Static Python, that'd be cool.

01:09:04.020 --> 01:09:05.120
Or Strict Modules.

01:09:05.120 --> 01:09:08.440
And give us any feedback you have on those.

01:09:08.440 --> 01:09:09.020
Fantastic.

01:09:09.020 --> 01:09:09.780
All right.

01:09:09.780 --> 01:09:10.840
Well, thanks for being on the show.

01:09:10.840 --> 01:09:11.800
Great to chat with you.

01:09:11.800 --> 01:09:12.380
Thank you, Michael.

01:09:12.380 --> 01:09:13.140
Yeah, you bet.

01:09:13.140 --> 01:09:13.500
Bye.

01:09:13.500 --> 01:09:14.020
See ya.

01:09:14.020 --> 01:09:14.300
See ya.

01:09:14.300 --> 01:09:18.280
This has been another episode of Talk Python to Me.

01:09:18.280 --> 01:09:20.100
Thank you to our sponsors.

01:09:20.100 --> 01:09:21.720
Be sure to check out what they're offering.

01:09:21.720 --> 01:09:23.140
It really helps support the show.

01:09:23.140 --> 01:09:25.140
Take some stress out of your life.

01:09:25.140 --> 01:09:30.620
Get notified immediately about errors and performance issues in your web or mobile applications with

01:09:30.620 --> 01:09:30.920
Sentry.

01:09:30.920 --> 01:09:35.920
Just visit talkpython.fm/sentry and get started for free.

01:09:35.920 --> 01:09:39.500
And be sure to use the promo code TALKPYTHON, all one word.

01:09:39.500 --> 01:09:44.100
With TopTal, you get quality talent without the whole hiring process.

01:09:44.100 --> 01:09:47.920
Start 80% closer to success by working with TopTal.

01:09:47.920 --> 01:09:52.500
Just visit talkpython.fm/TopTal to get started.

01:09:52.500 --> 01:09:54.680
Want to level up your Python?

01:09:54.680 --> 01:09:58.740
We have one of the largest catalogs of Python video courses over at Talk Python.

01:09:58.740 --> 01:10:03.900
Our content ranges from true beginners to deeply advanced topics like memory and async.

01:10:03.900 --> 01:10:06.580
And best of all, there's not a subscription in sight.

01:10:06.580 --> 01:10:09.480
Check it out for yourself at training.talkpython.fm.

01:10:09.700 --> 01:10:11.380
Be sure to subscribe to the show.

01:10:11.380 --> 01:10:14.160
Open your favorite podcast app and search for Python.

01:10:14.160 --> 01:10:15.460
We should be right at the top.

01:10:15.460 --> 01:10:20.620
You can also find the iTunes feed at /itunes, the Google Play feed at /play,

01:10:20.620 --> 01:10:24.840
and the direct RSS feed at /rss on talkpython.fm.

01:10:25.680 --> 01:10:28.260
We're live streaming most of our recordings these days.

01:10:28.260 --> 01:10:31.680
If you want to be part of the show and have your comments featured on the air,

01:10:31.680 --> 01:10:36.100
be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

01:10:36.100 --> 01:10:37.940
This is your host, Michael Kennedy.

01:10:37.940 --> 01:10:39.240
Thanks so much for listening.

01:10:39.240 --> 01:10:40.400
I really appreciate it.

01:10:40.400 --> 01:10:42.300
Now get out there and write some Python code.

01:10:42.300 --> 01:11:03.060
I'll see you next time.

01:11:03.060 --> 01:11:33.040
Thank you.

