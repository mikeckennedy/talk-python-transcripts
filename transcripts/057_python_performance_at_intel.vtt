WEBVTT

00:00:00.001 --> 00:00:08.020
When you think about the performance of your software, there's nothing more low-level and fundamental than how your code executes on the CPU itself.

00:00:08.020 --> 00:00:16.820
Many of us study and try to understand how to maximize performance at this low level, but few are in a position to define what actually happens there.

00:00:16.820 --> 00:00:27.740
That's why I'm thrilled to share the work that Intel, the largest PC chip manufacturer, is doing specifically to make Python faster and to make their chips execute Python code even better.

00:00:28.240 --> 00:00:34.360
This week, you'll meet David Stewart, Engineering Manager in the Intel Data Center Software Technology Group at Intel.

00:00:34.360 --> 00:00:38.980
We'll discuss a wide variety of work Intel is doing in open source and Python.

00:00:38.980 --> 00:00:44.900
This is Talk Python To Me, Episode 57, recorded May 2, 2016.

00:00:56.480 --> 00:01:01.660
This music I constructed line by line, just like when I'm coding another software design.

00:01:01.660 --> 00:01:04.860
In both cases, it's about design patterns.

00:01:04.860 --> 00:01:06.360
Anyone can get the job done.

00:01:06.360 --> 00:01:07.880
It's the execution that matters.

00:01:07.880 --> 00:01:09.360
I have many interests.

00:01:09.360 --> 00:01:16.160
Welcome to Talk Python To Me, a weekly podcast on Python, the language, the libraries, the ecosystem, and the personalities.

00:01:16.160 --> 00:01:18.280
This is your host, Michael Kennedy.

00:01:18.540 --> 00:01:20.280
Follow me on Twitter, where I'm @mkennedy.

00:01:20.280 --> 00:01:24.180
Keep up with the show and listen to past episodes at talkpython.fm.

00:01:24.180 --> 00:01:26.760
And follow the show on Twitter via at Talk Python.

00:01:26.760 --> 00:01:31.720
This episode has been brought to you by SnapCI and now Metis.

00:01:31.720 --> 00:01:37.220
Thank them for supporting the show on Twitter via Snap underscore CI and this is Metis.

00:01:37.220 --> 00:01:38.000
That's right.

00:01:38.000 --> 00:01:42.220
Metis and their data science education team have joined the show to keep the episodes coming.

00:01:42.720 --> 00:01:44.700
Be sure to find them on Twitter and tell them thank you.

00:01:44.700 --> 00:01:47.260
David, welcome to the show.

00:01:47.260 --> 00:01:48.940
Thank you very much, Michael.

00:01:48.940 --> 00:01:50.140
It's great to talk with you.

00:01:50.140 --> 00:01:53.540
Yeah, I'm really excited to have you on the show today.

00:01:53.540 --> 00:01:59.040
And I'm looking forward to looking inside what you guys are doing at Intel with Python.

00:01:59.040 --> 00:02:00.160
It's going to be a lot of fun.

00:02:00.600 --> 00:02:03.180
Yeah, it's really been exciting to me.

00:02:03.180 --> 00:02:12.160
I think people have been really responding when I hear that Intel is doubling down on Python just because of its power and popularity.

00:02:12.160 --> 00:02:15.940
So I think it's exciting to be a part of that for sure.

00:02:16.520 --> 00:02:19.320
Yeah, it's really exciting to be part of Python in general.

00:02:19.320 --> 00:02:30.740
I mean, it's really surprising and a pleasant way to me to continue to see this language and ecosystem that's 25 years old gaining momentum and gaining speed.

00:02:30.740 --> 00:02:35.000
Kind of feel like it would have done whatever it did in the first five or ten years, but that's not the case.

00:02:35.000 --> 00:02:35.700
It's great.

00:02:35.880 --> 00:02:36.900
So nice to see Intel.

00:02:36.900 --> 00:02:48.800
Your listeners probably, you're probably already aware of the fact that the top, the majority of the top CF schools in the U.S. are teaching Python as the first introductory language, right?

00:02:48.800 --> 00:02:55.600
That's often a surprise when I talk, I was just talking to a professor at computer science last week and he was like, really?

00:02:55.600 --> 00:02:57.100
They just didn't know about that.

00:02:57.100 --> 00:02:58.820
So, yeah, it's really taken hold.

00:02:58.820 --> 00:03:00.180
Yeah, I think that's great.

00:03:00.180 --> 00:03:02.540
And, you know, to be honest, it makes me a little bit jealous.

00:03:02.540 --> 00:03:12.760
Yes, I, my very first CS, not my first programming class, but my first CS class that was for programmers was in Scheme and Lisp.

00:03:12.760 --> 00:03:14.540
And I would have definitely preferred.

00:03:14.540 --> 00:03:14.740
Oh, my gosh.

00:03:14.740 --> 00:03:17.420
Yeah, I would have definitely preferred to get Python in there.

00:03:17.420 --> 00:03:18.720
But, you know, you get what you get.

00:03:18.720 --> 00:03:19.320
Yeah, no.

00:03:19.320 --> 00:03:21.260
I know what you're talking about.

00:03:21.260 --> 00:03:25.220
Well, you know, if you were talking about Pity here, my first was with Fortran.

00:03:25.360 --> 00:03:33.800
And, in fact, I had to teach Fortran programming to, you know, to engineering students and the like as a part of a graduate teaching assistantship back in the day.

00:03:33.800 --> 00:03:42.140
So, yeah, I didn't necessarily do a lot of programming in Fortran myself, but it was one of those things where you sort of had to cut your teeth on something.

00:03:42.140 --> 00:03:46.400
And there's still people using Fortran these days, but it's probably more than Lisp.

00:03:46.400 --> 00:03:48.900
But, yeah, it's got to be waning.

00:03:49.360 --> 00:03:50.100
I hope so.

00:03:50.100 --> 00:03:52.480
I think we share some common history.

00:03:52.480 --> 00:03:59.520
Like, I started out in, yeah, I started out in an engineering field, not in computer science.

00:03:59.520 --> 00:04:03.000
And when I got there, they said, you have to have a programming class.

00:04:03.000 --> 00:04:04.100
I said, great, can I take C++?

00:04:04.100 --> 00:04:06.120
No, you have to take Fortran first.

00:04:06.120 --> 00:04:09.380
This is the most important language you will ever learn in your life.

00:04:09.380 --> 00:04:11.780
Then you can go take those other less meaningful ones.

00:04:11.780 --> 00:04:15.300
So I took that and then I went to CS.

00:04:15.300 --> 00:04:16.820
I said, can I please take C++?

00:04:16.820 --> 00:04:18.400
No, you have to take Scheme.

00:04:18.400 --> 00:04:20.740
I'm like, why can't I do a real programming?

00:04:20.740 --> 00:04:24.200
Anyway, I'm going to get some hate mail for saying that, but that's okay.

00:04:24.200 --> 00:04:25.240
I'm sure you will.

00:04:25.240 --> 00:04:32.400
No, I really, I do like C++, but I wish I had Python in the beginning.

00:04:32.400 --> 00:04:36.420
That would have inspired me more, even though I was already inspired to do programming.

00:04:36.420 --> 00:04:40.840
Yeah, and I think that it's been particularly interesting.

00:04:40.840 --> 00:04:47.760
There's so many open source projects that are, you know, kind of advanced by some corporate entity or something like that.

00:04:47.760 --> 00:04:50.340
Python is a phenomenal project.

00:04:50.340 --> 00:04:53.100
It's sort of like the Linux kernel itself, right?

00:04:53.100 --> 00:05:01.860
There are plenty of companies involved with Linux, but it's really, you know, with Linus Torvalds being the, you know, the lead,

00:05:01.860 --> 00:05:03.320
it makes a final decision on things.

00:05:03.320 --> 00:05:09.100
And Python, that's one of the strengths, I think, is the kind of approach that Guido uses.

00:05:09.220 --> 00:05:11.980
Guido Van Ropsen, who is the benevolent dictator for Life for Python.

00:05:11.980 --> 00:05:20.880
I think he's really created, you know, just kind of an ethos and a culture around this project that I think is super unique.

00:05:20.880 --> 00:05:29.060
And, in fact, it's a great, I hold up as a great example for people who aspire to, you know, establishing open source projects.

00:05:29.180 --> 00:05:34.480
I mean, hey, this is one of the ways that you talk about, yeah, 25 years of, you know, experience.

00:05:34.480 --> 00:05:38.240
And Guido's, you know, I think a phenomenal leader for that movement.

00:05:38.240 --> 00:05:39.020
Yeah, for sure.

00:05:39.020 --> 00:05:40.380
Yeah, he's doing a great job.

00:05:40.460 --> 00:05:47.580
And I think there's a lot of positive examples of the Python community being a very positive place for open source.

00:05:47.580 --> 00:05:48.520
And that's great.

00:05:48.520 --> 00:05:55.360
So, before we get into the story about what you guys are doing with Python, maybe you could just sort of tell me how you got into programming in Python.

00:05:55.360 --> 00:05:58.220
You said you took Fortran in college.

00:05:58.220 --> 00:05:59.360
So, how did you get to there?

00:05:59.360 --> 00:06:10.440
Yeah, I mean, you know, when I got my bachelor's and master's in computer science, I mean, C was really the most interesting language.

00:06:10.440 --> 00:06:12.720
to me at the time because I was doing a lot in operating systems.

00:06:12.720 --> 00:06:20.820
And so, I've been probably over the years of my career probably more interested in, you know, operating systems in general instead of languages per se.

00:06:20.820 --> 00:06:27.640
And since C is a great language, you know, initially when I worked with it was Unix and then, you know, Linux, obviously.

00:06:27.640 --> 00:06:34.380
I think that's one of the things that it was just when I think in terms of programming, it's still hard for me not to relate back to C.

00:06:34.560 --> 00:06:41.640
But, you know, the Python work came as a result of a couple of things I got involved with back in 2010.

00:06:41.640 --> 00:06:48.560
What happened was I was asked to start up a new project here at Intel to work on embedded Linux.

00:06:48.560 --> 00:06:51.780
And it was kind of an amazing thing.

00:06:51.840 --> 00:06:56.060
The core of this project had this something called BitBake.

00:06:56.060 --> 00:07:02.900
And if you're not familiar with it, what BitBake does is it's a desktop application that will build a complete Linux distribution.

00:07:02.900 --> 00:07:08.240
Now, this is actually quite interesting because what it does is it builds the compiler first, right?

00:07:08.500 --> 00:07:13.700
And it has to bootstrap several different paths of the compiler building to get a functional compiler, right?

00:07:13.700 --> 00:07:22.040
And then it goes and a Linux distribution could be made up of, like, literally a thousand different projects all over the Internet.

00:07:22.040 --> 00:07:26.740
Individual projects with their own source control systems, their own repositories.

00:07:26.740 --> 00:07:33.380
And this system will actually download the sources from all these different projects, goes and patches the source.

00:07:33.380 --> 00:07:37.820
Of course, if you have an open source system, you want to be able to, you know, do patching of those things, right?

00:07:38.120 --> 00:07:46.260
Then it will configure them, build them, build a package repository, build a running Linux image, and then an SDK.

00:07:46.260 --> 00:07:51.400
And the amazing thing is that it does all of this in an hour, like one hour on a desktop computer.

00:07:51.400 --> 00:07:53.360
And you're going, wait, hold on.

00:07:53.360 --> 00:07:56.420
What magic is this thing doing?

00:07:56.420 --> 00:08:01.520
And I started talking to the architect of this thing, and it's a massive Python program.

00:08:01.520 --> 00:08:04.860
So BitBake is actually completely written in Python.

00:08:04.860 --> 00:08:07.160
I was like, you know, I got to confess.

00:08:07.360 --> 00:08:09.780
I was like, holy cow, this is a scripting language?

00:08:09.780 --> 00:08:15.060
And I heard about, you know, things like, you know, you're able to serialize and deserialize objects.

00:08:15.060 --> 00:08:17.960
And it's like, hang on, this is really powerful stuff.

00:08:17.960 --> 00:08:21.720
So I worked on that project for about five years.

00:08:21.720 --> 00:08:23.280
That was a project called the Yocto Project.

00:08:23.280 --> 00:08:26.180
So anybody who knows that, that was something I was involved with.

00:08:26.300 --> 00:08:29.740
And then, you know, things were going, you know, pretty well.

00:08:29.740 --> 00:08:34.500
And I kind of get, you know, a little bit, you know, sedate, I guess, when things are going too well.

00:08:34.500 --> 00:08:36.020
So I always like to have a challenge.

00:08:36.020 --> 00:08:41.340
And at that point, Intel was really interested in, as I said, doubling down on Python,

00:08:41.760 --> 00:08:48.520
because it's primarily the core of a lot of key things that Intel is invested in in other areas.

00:08:48.520 --> 00:08:53.800
And so, and besides its overall popularity as a, you know, as a programming language.

00:08:53.800 --> 00:08:59.640
So that was when I really, you know, got personally very invested in what was going on in Python.

00:08:59.640 --> 00:09:04.460
So I, you know, started a group and we have engineers that are at the core of working on that.

00:09:04.460 --> 00:09:10.000
So that was where I think I got really a lot of Python religion, obviously, at that point,

00:09:10.000 --> 00:09:16.720
was not only seeing it as a user, incredibly powerful, but also be able to now, I think,

00:09:16.720 --> 00:09:19.340
affect it positively on a community basis.

00:09:19.340 --> 00:09:24.320
We're going to dig into some of the cool stuff you guys are doing on a community basis as well.

00:09:24.320 --> 00:09:32.520
But you sort of started out with the philosophy of it's in Intel's interest to sort of understand

00:09:32.520 --> 00:09:38.000
modern cloud computing and the languages that drive that and make that stuff go really well.

00:09:38.000 --> 00:09:38.780
That's correct.

00:09:38.780 --> 00:09:46.000
I mean, if you think about, you know, Intel, obviously, we try and sell processors in the data center space.

00:09:46.000 --> 00:09:47.300
I think we've had some success there.

00:09:47.300 --> 00:09:53.120
One of the things that if you look at overall, how are people programming those data center processors?

00:09:53.400 --> 00:09:58.160
Well, there's obviously a lot of C++, C#, and Java, no question.

00:09:58.160 --> 00:10:06.720
But as we kind of analyzed things, we said, well, gee, there are 7 million PHP programmers out there.

00:10:06.720 --> 00:10:12.740
There's like 9 million JavaScript programmers, a considerable number on, you know, on data center computers.

00:10:12.740 --> 00:10:14.960
And then Python is huge in that respect, too.

00:10:14.960 --> 00:10:22.340
So if you look at the top languages that people are using our processors to run, right, it sort of says, well,

00:10:22.480 --> 00:10:25.440
how do I, you know, I'm all about customer choice in that respect.

00:10:25.440 --> 00:10:29.060
I'd like to make sure whatever customers are using that they get the best, you know,

00:10:29.060 --> 00:10:30.440
possible experience with our processors.

00:10:30.440 --> 00:10:39.160
And so it just feels like whether, you know, it's OpenStack work or some of the things that people are using Python for in,

00:10:39.240 --> 00:10:46.480
like high-performance computing or big data analytics, machine learning.

00:10:46.480 --> 00:10:49.300
There's just a lot of applications of Python.

00:10:49.300 --> 00:10:59.560
And it just seems like for Intel as a company to not be kind of investing that and making it great on our processors,

00:10:59.880 --> 00:11:01.700
it's just not a good idea.

00:11:01.700 --> 00:11:10.460
So one of the things I love about this is that we're being agile with respect to how we invest in the software that's running on the data center on our processors.

00:11:10.460 --> 00:11:16.080
And we're trying to, again, if our customers are using this stuff, we want to make sure that we have the best sort of, you know,

00:11:16.080 --> 00:11:17.360
the best sort of experience for them.

00:11:17.460 --> 00:11:22.300
So hopefully they'll come along and buy our processors on an ongoing basis.

00:11:22.300 --> 00:11:24.200
Yeah, I hope so.

00:11:24.200 --> 00:11:25.140
It sounds great.

00:11:25.140 --> 00:11:31.480
So can you give me some idea of what, like, what are you studying in Python?

00:11:31.480 --> 00:11:42.780
Are you looking at sort of the CPython implementation and how it's working and understanding how that's running on your chips and things like this?

00:11:42.780 --> 00:11:43.660
Or what's the story there?

00:11:43.660 --> 00:11:45.060
Yeah, yeah, yeah.

00:11:45.060 --> 00:11:52.100
But our philosophy is, you know, it's kind of what I call the core software strategy, which basically says this.

00:11:52.100 --> 00:12:00.700
It's like you can go off and, you know, work on speeding up some, you know, customer's application or some other, you know, kind of system.

00:12:00.700 --> 00:12:02.880
You know, that's great.

00:12:02.880 --> 00:12:07.920
But then, you know, at the end of the day, you know, you've optimized that one piece of software.

00:12:07.920 --> 00:12:11.720
You haven't really, you know, optimized anybody else's Python code, right?

00:12:11.720 --> 00:12:15.680
So, for example, let's take Swift.

00:12:15.680 --> 00:12:22.280
It's an open source object storage subsystem that's part of OpenStack, and it's implemented in Python.

00:12:22.280 --> 00:12:29.040
And if you look at that and you go, well, I could go work on the Swift source code, right, the Python source code to make it faster.

00:12:29.120 --> 00:12:34.560
But then it'd be better if I could actually speed up Python instead, that core software.

00:12:34.560 --> 00:12:41.640
And so, in theory, if I speed up Swift, I'll also speed up anything else that's using that accelerated Python, right?

00:12:42.020 --> 00:12:49.260
So, the way we achieve that core software strategy is we've developed some really amazing abilities.

00:12:49.260 --> 00:12:59.440
I mean, the engineers on the team have just done an amazing job to analyze exactly where the processor is spending its time as it's running, you know, major Python code, right?

00:12:59.440 --> 00:13:08.080
So, we're actually able to go at a very deep level, and we had some really very interesting discoveries that we've made at the microarchitectural level.

00:13:08.080 --> 00:13:10.880
And, you know, I can tell you more about that.

00:13:10.880 --> 00:13:12.820
It's kind of interesting.

00:13:13.100 --> 00:13:19.080
I mean, clearly, we see some things that are some challenges from a microarchitecture standpoint.

00:13:19.080 --> 00:13:34.160
So, we go, well, if we're going to improve the performance, we need to do what we can to address those things with the current products and then give feedback to our chip architects to say, hey, man, you need to change the way that the chip is designed to run these languages better.

00:13:34.160 --> 00:13:38.360
So, this is a – it's like a – not an either-or for us.

00:13:38.380 --> 00:13:39.300
It's like a both-and.

00:13:39.300 --> 00:13:49.520
Understanding the architecture at a deep level, we can go in and really speed up Python, and we can also go tell the chip architects to design better chips.

00:13:49.520 --> 00:13:56.900
So – and along those lines, yeah, our philosophy is totally upstreaming all of these things.

00:13:56.900 --> 00:14:04.700
And so, as much as possible, I sort of go – I mean, philosophically, I like to put the cookies on the low shelf so everyone can get at them.

00:14:04.780 --> 00:14:09.980
As much as possible, I want to make sure that we don't just do some amazing sort of things and that we upstream them.

00:14:09.980 --> 00:14:14.160
But then, you know, I want to make sure people know about them, right, so they can take advantage of them too.

00:14:14.160 --> 00:14:20.300
And I want to make it – I want to be a contributor to the community so that they can, you know, take advantage of the hard work that we're doing as well.

00:14:20.300 --> 00:14:22.280
Yeah, that's really excellent.

00:14:22.540 --> 00:14:24.780
So, it manifests sort of in two ways.

00:14:24.780 --> 00:14:31.140
You can improve CPython, and we're just getting started talking about the possibilities there, right?

00:14:31.140 --> 00:14:35.420
And then, how dramatic of a change can you make at the chip level?

00:14:35.420 --> 00:14:42.760
I mean, I think it's really cool that you're, like, thinking about, okay, well, here's our current architecture and the way things are working.

00:14:43.280 --> 00:14:46.300
But when you run Python code, this could be better.

00:14:46.300 --> 00:14:50.120
Would the chip actually detect what type of code it's running?

00:14:50.120 --> 00:14:52.200
Like, does it work at that level?

00:14:52.200 --> 00:14:56.060
Or are they just – you're like, these types of operations could be better?

00:14:56.440 --> 00:15:04.040
Well, actually, our chip architects have a long practice of looking at how current software runs on current chips.

00:15:04.040 --> 00:15:16.200
And we actually have some great tools to be able to analyze that at an instruction level that says, okay, you're running – oh, I don't know whether it's a database or whether it's, you know, Python or something else.

00:15:16.200 --> 00:15:19.840
And you're able to analyze, you know, okay, here's the series of instructions that get run.

00:15:19.840 --> 00:15:29.060
And then they can play a lot of interesting, you know, conceptual games with that and say, well, what if we added this instruction, a new instruction to optimize this thing?

00:15:29.060 --> 00:15:34.060
Or what if we change the way the cache works to, you know, run this stuff better?

00:15:34.060 --> 00:15:48.700
Or, you know, and, you know, we're actually – by having us involved with them, we're actually able to come up with some really interesting ideas and say, well, hey, maybe we could have this kind of, you know, acceleration

00:15:48.700 --> 00:15:53.280
or this kind of idea about how to run these languages better, right?

00:15:53.280 --> 00:16:05.100
So our hope is that, yeah, you know, and so the current chips are really designed from just years of running code, you know, of existing software against it.

00:16:05.100 --> 00:16:09.600
And so, you know, some of it's making up for bad software practices, bad programming practice.

00:16:09.600 --> 00:16:16.760
And it's like, oh, if people were to code this way, maybe if we organized the chip this way, it would run, you know, sort of poor code faster or something like that.

00:16:16.760 --> 00:16:18.260
Not to say that anyone's code is crap.

00:16:18.260 --> 00:16:18.980
I'm not saying that.

00:16:18.980 --> 00:16:20.960
But there's a lot of crappy code that's out there, right?

00:16:20.960 --> 00:16:21.260
Yeah.

00:16:21.260 --> 00:16:23.040
I'm sure none of your listeners have bad code.

00:16:23.040 --> 00:16:24.080
I'm sure they don't.

00:16:24.080 --> 00:16:26.820
But if they listen to other podcasts, they may.

00:16:26.820 --> 00:16:27.560
I'm just teasing.

00:16:28.080 --> 00:16:30.840
I'm sure they're fixing other people's bad code, not their own.

00:16:30.840 --> 00:16:35.620
But, I mean, you know, that's a key, you know, observation in terms of the chip.

00:16:35.620 --> 00:16:46.440
Now, I couldn't necessarily go into a lot of ideas on the podcast, but we're working on a bunch of really intriguing ideas, in my opinion, about ways that we can really run this stuff much, much better.

00:16:46.640 --> 00:16:59.660
I think it's cool to know that you guys out there are actually thinking about, you know, specifically studying these different runtimes like Python and Node.js and PHP and so on.

00:16:59.660 --> 00:17:06.380
And understanding sort of how they're working and not working on your hardware and then adjusting for that.

00:17:07.200 --> 00:17:11.240
When would the first sort of fruits of these labors show up?

00:17:11.240 --> 00:17:18.540
You said in 2010 you got started with the Linux embedded stuff and then later you got into Python.

00:17:18.540 --> 00:17:21.680
So how far down the road does this stuff land?

00:17:21.680 --> 00:17:22.860
Yeah.

00:17:22.860 --> 00:17:31.200
And by the way, the one thing I was going to add to your observation about the work that we do, we've actually been doing this work with Java now for, I think, 15 years.

00:17:31.200 --> 00:17:43.900
And we have this nice little chart that we compare from generation to generation how much the chip speeds it up, but how much our software, you know, JVM's, you know, changes have accelerated in each of those generations.

00:17:43.900 --> 00:17:48.780
And so we're just playing back the same playbook, basically, that we've been using for years with Java.

00:17:48.780 --> 00:18:00.560
What I would say in terms of the fruits of our labor, actually, when we began the group, we had our first performance patches available to the community, I think, within about like three, four months or so.

00:18:00.560 --> 00:18:03.720
And we're on the order of like a 10% performance boost.

00:18:03.720 --> 00:18:06.440
And, you know, you think, well, 10%, that can't be great.

00:18:06.440 --> 00:18:21.280
But, in fact, here's my experience with data center level code, whether it's, you know, massive database benchmarks like TPCC or this, you know, running Swift or some of these other big, big, big customer workloads on a data center.

00:18:21.640 --> 00:18:26.700
If you can boost something, you know, 5% to 10%, you are just like, you are like golden.

00:18:26.700 --> 00:18:27.840
That's like awesome.

00:18:27.840 --> 00:18:35.940
It's very unusual to find a real customer benchmark that gets, you know, some multiple of, you know, many X speed.

00:18:35.940 --> 00:18:37.220
It just doesn't happen, right?

00:18:37.440 --> 00:18:43.840
And so, realistically, if you can improve the throughput by a few percent, usually people are pretty happy about that.

00:18:43.920 --> 00:18:45.700
So, we came up with our first patches.

00:18:45.700 --> 00:19:03.300
Yeah, I'm pretty sure it was about three, four months after we started analyzing things and look at some low-hanging fruit of places where we could pull in some stuff pretty quickly and get the experience with the community, you know, talking with Guido, talking with other community developers to try and really, you know, hone our, you know, our efforts there.

00:19:04.080 --> 00:19:14.880
And the first time we kind of took a look at PyPy, that's when we said, well, shoot, maybe we could get a much, much higher, you know, return with PyPy.

00:19:14.880 --> 00:19:17.520
Maybe we could get a whole lot more than just a few percent here and there.

00:19:17.520 --> 00:19:20.140
So, that became extraordinarily interesting.

00:19:20.140 --> 00:19:20.140
Thank you.

00:19:20.140 --> 00:19:50.120
Thank you.

00:19:50.120 --> 00:20:20.100
Thank you.

00:20:20.100 --> 00:20:25.060
Yeah, that's awesome.

00:20:25.060 --> 00:20:27.440
And that's kind of what I was hinting at, like we're just getting started.

00:20:27.440 --> 00:20:32.580
So, the majority of people who run Python code, they run it on CPython.

00:20:32.580 --> 00:20:37.120
That's the default thing you get if you go to python.org and you download it.

00:20:37.120 --> 00:20:41.740
But there are many other runtimes or implementations.

00:20:41.740 --> 00:20:47.160
The most popular one around performance is probably one called PyPy, P-Y-P-Y.

00:20:47.160 --> 00:20:52.780
But we also have PyStin coming out of Dropbox and Guido's group there.

00:20:53.320 --> 00:20:56.840
We've got Psython, Jython, IronPython.

00:20:56.840 --> 00:20:58.840
There's a lot of options.

00:20:58.840 --> 00:21:07.440
And so, you turn to the probably one of the more established high-performance alternatives to CPython, PyPy, right?

00:21:08.060 --> 00:21:08.620
That's correct.

00:21:08.620 --> 00:21:08.620
That's correct.

00:21:08.620 --> 00:21:16.520
And, you know, for us, it's like I didn't want to be married to anything until we really got a chance to see, you know, kind of what was going on in the landscape.

00:21:16.980 --> 00:21:19.060
I mean, we looked at all of these, frankly.

00:21:19.060 --> 00:21:22.000
And they all have their pros and cons.

00:21:22.000 --> 00:21:26.500
I mean, things like Psython, et cetera, you know, basically creating C code.

00:21:26.500 --> 00:21:29.300
That's a nice model for performance.

00:21:29.300 --> 00:21:34.560
The challenge is that kind of takes away some of the, you know, development speed you get from an actual interpreted language.

00:21:35.320 --> 00:21:42.660
You know, and PyPy for it, and Pidgin and Piston have interesting qualities.

00:21:42.660 --> 00:21:46.920
I don't, you know, the thing that I like about PyPy is as follows.

00:21:46.920 --> 00:21:49.540
I mean, for one thing, it's been around 10 years, right?

00:21:49.540 --> 00:21:55.320
So, it's got, as far as I can tell, the broadest compatibility of any of these efforts.

00:21:56.020 --> 00:21:58.640
It's focused on both Python 2 and Python 3.

00:21:58.640 --> 00:22:11.480
I think that, you know, it really hasn't received a ton of, you know, broad sort of help from folks, you know, like these performance guys that I work with here at Intel.

00:22:11.480 --> 00:22:17.220
I mean, I think they've done an amazing job, absolutely amazing, stunning.

00:22:17.220 --> 00:22:19.160
I am incredibly impressed.

00:22:19.160 --> 00:22:33.840
We had a great sprint with them, you know, basically in March of this year, and face-to-face effort with their core developers, and it was just really very, very effective.

00:22:33.840 --> 00:22:38.980
And then there was another release of PyPy that came out as a result of the sprint.

00:22:38.980 --> 00:22:42.120
You know, so I think we're just getting started working with it.

00:22:42.120 --> 00:22:44.880
The results have been nothing short of stunning.

00:22:45.140 --> 00:22:51.620
I mean, this is what really impressed me because, like I said, we're working with Swift because it's a part of OpenStack.

00:22:51.620 --> 00:22:53.480
We're doing a lot of stuff with OpenStack at Intel.

00:22:53.480 --> 00:22:58.000
You know, Swift is the part of OpenStack that seems to spend a lot of its time in Python.

00:22:58.000 --> 00:23:06.960
As we analyzed it, 70% of the cycles, 70%, 80% of the cycles when you're running Swift is actually in the Python interpreter itself.

00:23:06.960 --> 00:23:10.760
So, it's like, oh, there's a lot of opportunity there.

00:23:11.220 --> 00:23:19.260
Then we look at, we split that down further, and it's like really being, you know, like 30% of those cycles are just in the main interpreter loop of CPython.

00:23:19.260 --> 00:23:24.160
So, we said, well, let's try PyPy, as you said, P-Y-P-Y, and it's incredible.

00:23:24.160 --> 00:23:26.820
We got 111% throughput improvement.

00:23:26.820 --> 00:23:36.120
Now, pause for a second and say, okay, I was looking at a few percent here and there, and here I've gotten more than double the throughput of using PyPy, right?

00:23:36.200 --> 00:23:38.740
It's like compared to CPython, amazing.

00:23:38.740 --> 00:23:42.440
And then it's like an 87% response time improvement.

00:23:42.440 --> 00:23:50.900
And it's like if something's like a software system, I mean, throughput's great because, right, you can scale up more and more users and you get, you know, great throughput.

00:23:50.900 --> 00:23:54.640
But the response time is what people really respond to, right?

00:23:54.660 --> 00:24:05.040
I mean, this is like it means they're accessing files, whether you're like, for example, Wikipedia, all of the, you know, sort of images that people look at in Wikipedia are all managed by Swift, right?

00:24:05.040 --> 00:24:09.160
So, you can look at something like that, and that just brings up your Wikipedia pages faster.

00:24:09.960 --> 00:24:14.640
And I've talked to various customers who are using Swift for their, you know, object storage system.

00:24:14.640 --> 00:24:18.540
And, yeah, this is a huge deal when you get that kind of improvement.

00:24:18.540 --> 00:24:33.620
So, it's like, well, with such amazing, you know, speedups, why wouldn't we try to, you know, see if we can, you know, provide a little, you know, love to the project to see if we can, you know, really do it, you know, continue to make this maybe even the default of how people use Python.

00:24:34.240 --> 00:24:36.320
Okay, yeah, that's really amazing.

00:24:36.320 --> 00:24:39.980
Do you know if you can run all of OpenStack on PyPy?

00:24:39.980 --> 00:24:44.000
Yeah, we're actually in the process of getting that together.

00:24:44.000 --> 00:24:51.000
We have, let's see, so far Swift, Keystone, Nova, and Neutron ported.

00:24:51.000 --> 00:24:59.300
And I would, you know, I would say trying to get like a proof of concept where we have all the core services running in PyPy.

00:25:00.000 --> 00:25:05.720
And I'd like us to really be able to do that maybe before the next OpenStack Summit in the fall in Barcelona.

00:25:05.720 --> 00:25:08.860
So, I'm hoping that we'll be able to show that off.

00:25:08.860 --> 00:25:12.720
Keystone, I said, is another interesting service that's written in Python.

00:25:12.720 --> 00:25:18.200
Keystone, if you're not familiar with it, is the user authentication part of OpenStack.

00:25:18.200 --> 00:25:26.660
So, essentially, every OpenStack service has to go through Keystone to see if you're authorized to do the things you're saying you want to do, right?

00:25:27.020 --> 00:25:30.160
And so, it's a really, you know, centralized part of the project.

00:25:30.160 --> 00:25:33.440
And we were able to speed it up by 37% using PyPy.

00:25:33.440 --> 00:25:36.660
So, that's, again, pretty darn amazing.

00:25:36.660 --> 00:25:42.120
So, I think I'd like to see the entire, yeah, I'd like to see PyPy as the default.

00:25:42.120 --> 00:25:48.380
In fact, years ago, the OpenStack gate had a requirement that everything had to work with PyPy.

00:25:48.500 --> 00:25:50.200
And a year or so ago, that got dropped.

00:25:50.200 --> 00:25:51.560
And I'd like to bring that back.

00:25:51.560 --> 00:25:58.940
And, if possible, try and drive, you know, something that would have the whole, you know, community basically making use of PyPy within OpenStack.

00:25:58.940 --> 00:25:59.200
Yeah.

00:25:59.200 --> 00:26:00.720
Yeah, that's really cool.

00:26:00.720 --> 00:26:04.740
If you're running something like OpenStack, you're running on a lot of machines.

00:26:04.740 --> 00:26:09.180
And efficiency is going to make a big difference in that type of system, right?

00:26:10.200 --> 00:26:10.600
Absolutely.

00:26:10.600 --> 00:26:14.980
And, you know, if you think about this, in some respect, it's not kind of radical.

00:26:14.980 --> 00:26:28.460
Every other interpreted language, whether it's Java, JavaScript, PHP, you name it, Lua, they have all gone the direction of a JIT in order to improve performance.

00:26:29.340 --> 00:26:33.340
And here, let me tell you how this works, right?

00:26:33.340 --> 00:26:36.620
If you've got two, let's say you're adding two integers together, right, in Python.

00:26:36.620 --> 00:26:37.600
Very simple operation.

00:26:37.600 --> 00:26:44.920
You know, if you're running native code on our processors, right, there's one instruction that adds two integers together, right?

00:26:44.920 --> 00:26:50.260
In some cases, it can be even less than one instruction with opcode fusing some features that we have.

00:26:50.620 --> 00:27:00.540
With Python, CPython in particular, we did a measurement and showed that on average, it takes 76 instructions to add two integers together, right?

00:27:00.540 --> 00:27:03.400
So automatically, you can see a stunning difference.

00:27:03.400 --> 00:27:05.200
And, you know, we've got great processors.

00:27:05.200 --> 00:27:07.320
They run instructions very fast.

00:27:07.320 --> 00:27:11.680
But you're running one instruction versus 76 is definitely going to be faster.

00:27:11.680 --> 00:27:14.280
That's a very hard thing to compensate for.

00:27:14.280 --> 00:27:15.520
Yeah.

00:27:15.740 --> 00:27:22.200
So as a result, you know, the processor spends a lot of it because the code footprint is just huge on something like Python.

00:27:22.200 --> 00:27:25.800
And so, you know, you spend a lot of the processor spends a lot of its time.

00:27:25.800 --> 00:27:29.860
We talk about it being cycles stalled in the front end.

00:27:29.860 --> 00:27:33.140
And what that basically means is the first two stages.

00:27:33.140 --> 00:27:37.620
If you think about it, I'm sorry to, let me just mention this briefly.

00:27:37.620 --> 00:27:38.960
I won't geek out on this too much.

00:27:39.040 --> 00:27:49.000
But it's like if you think about the five-stage pipeline for a processor, a modern processor, you get best performance if you can get all the stages of the pipeline running in parallel, right?

00:27:49.000 --> 00:27:55.660
The problem is our processors are spending like half of their time twiddling their thumbs waiting for new instructions to get fetched and decoded.

00:27:56.480 --> 00:28:09.060
So if we can, you know, make a huge difference to that, and a lot of it, frankly, comes down to simply the size of the instructions, the size of the code footprint that we're trying to process, right?

00:28:09.060 --> 00:28:20.120
And so JIT directly addresses that because what a JIT is going to do is it's going to look at the hot interpreted, you know, code and generate native code, right?

00:28:20.120 --> 00:28:28.180
So instead of that 76 instruction for an ad, if it's a hot ad, it will basically run in one instruction because it will be running native code instead.

00:28:28.180 --> 00:28:31.700
So this is not a huge secret.

00:28:31.700 --> 00:28:32.980
It's not magical.

00:28:32.980 --> 00:28:37.360
It's something that is a really, you know, well-worn technique to make this happen.

00:28:37.360 --> 00:28:41.540
And I'm telling you, every other language has gone this direction.

00:28:41.540 --> 00:28:46.940
And as I've, you know, as Guido and I have talked about this as well, he's like, you know, he's, you know, supportive.

00:28:46.940 --> 00:28:48.920
He's like, he says, I don't have a dog in the fight.

00:28:48.920 --> 00:28:54.960
You know, he's basically, he's more, I think, interested in the advances of the language as opposed to these performance things.

00:28:54.960 --> 00:28:57.240
Usually he's not thinking in terms of the performance side.

00:28:57.240 --> 00:28:59.200
But I'm, you know, I'm thinking about the performance side.

00:28:59.200 --> 00:29:11.100
I said, hey, could we really help the overall project by, you know, having a, you know, a great interpreter for Python that really runs really super fast?

00:29:11.100 --> 00:29:18.360
By the way, they're able to achieve greater performance by several other things in terms of the garbage collection that they're able to do.

00:29:18.360 --> 00:29:22.420
In various ways, they have much lower memory footprint as well.

00:29:22.420 --> 00:29:27.260
And so this is something that several of these techniques they're able to use to get really stunning performance.

00:29:27.840 --> 00:29:30.000
Yeah, the PyPy guys are doing a great job.

00:29:30.000 --> 00:29:32.520
And they definitely do some interesting stuff.

00:29:32.520 --> 00:29:34.540
I talked to them on episode 21.

00:29:34.540 --> 00:29:43.640
One of the things that I thought was interesting is the way that they don't immediately JIT everything, but they kind of wait until they find a hotspot.

00:29:43.640 --> 00:29:45.120
And then they go and JIT that.

00:29:45.380 --> 00:29:45.520
Right.

00:29:45.520 --> 00:29:46.020
Right.

00:29:46.020 --> 00:29:52.260
That's, again, a well-worn technique that has a ton of fruit in the Java space, as an example.

00:29:52.260 --> 00:29:52.540
Right.

00:29:52.540 --> 00:29:55.480
They even called their JIT a hotspot is what it's called.

00:29:55.480 --> 00:29:55.780
Right.

00:29:55.780 --> 00:29:59.620
So this is a common technique, and I think it works really well.

00:29:59.620 --> 00:30:00.600
Yeah.

00:30:00.600 --> 00:30:02.380
Yeah.

00:30:02.380 --> 00:30:13.700
I think the challenges that you run into is languages like Python, they're so flexible and you can change the types so much that it's not as easy as JIT in something like Java.

00:30:13.700 --> 00:30:14.020
Right.

00:30:15.020 --> 00:30:15.700
Oh, yeah.

00:30:15.700 --> 00:30:23.180
And particularly when, you know, because it's one of the really attractive things about the language is how, you know, flexible it can be in terms of typing.

00:30:23.180 --> 00:30:27.520
You don't have to spend a lot of time, you know, coming up with the types and documenting all those things.

00:30:27.520 --> 00:30:33.080
And so, you know, coding that is Pythonic, it really has that, you know, kind of power and flexibility.

00:30:33.080 --> 00:30:41.560
Unfortunately, that power and flexibility also comes with a cost and typically, you know, means that you have to spend a lot of code trying to figure out, now, what is the type of this thing?

00:30:41.560 --> 00:30:44.100
And did it change since the last time I looked at it?

00:30:44.100 --> 00:30:44.660
Right.

00:30:44.660 --> 00:30:48.480
So that's a strong consideration when trying to run this stuff fast.

00:30:48.480 --> 00:30:49.660
Yeah.

00:30:49.660 --> 00:30:54.540
I think there's a lot of attention being given to speed and Python lately.

00:30:54.660 --> 00:31:09.740
I've noticed a big uptick in projects and people focused on trying to make Python faster, be that through PyPy or the Microsoft Pigeon or I just spoke to the Euphoria guys about distributed, compiled Python.

00:31:10.020 --> 00:31:13.940
There's a whole bunch of really interesting things.

00:31:13.940 --> 00:31:16.240
And they're not all mutually exclusive.

00:31:16.240 --> 00:31:20.160
I think they're going to come together and some pretty amazing stuff is going to come out of it.

00:31:20.160 --> 00:31:21.500
Yeah, I agree with you.

00:31:21.580 --> 00:31:28.040
I think that one of the strengths of the Python community is the fact that you do have a lot of freedom for innovation.

00:31:28.040 --> 00:31:33.320
And, you know, the uptick in performance projects, I've noticed that as well.

00:31:33.320 --> 00:31:44.840
I think part of that's just because it has become so well adopted that I think if they don't solve the performance thing, you know, I think people will sort of get frustrated and go on to other things.

00:31:45.000 --> 00:31:55.920
You know, for example, in the high-performance computing space, you have people who are scientists and, you know, people studying data and things of that sort as opposed to being programmers, right?

00:31:55.920 --> 00:32:00.140
And they love Python because they can, you know, implement their code using Python.

00:32:00.140 --> 00:32:05.820
But then they go, well, this is performing slowly, so, you know, let me recode it in something else, right?

00:32:05.820 --> 00:32:13.340
And I think our vision would be that people don't have to, you know, change their language to get better performance.

00:32:13.480 --> 00:32:18.440
I'd love it to be a no-compromises, you know, experience with Python, right?

00:32:18.440 --> 00:32:26.480
And then we can bring to bear the best of our, you know, microarchitectural analysis tools, software optimization tools, and, frankly, analysis tools as well.

00:32:26.480 --> 00:32:38.740
We've got a terrific visual profiler called VTune that lets you, you know, pinpoint the exact area in your Python code that is, you know, causing your problems, performance problems.

00:32:38.840 --> 00:32:49.740
We also have, you know, a set of Python libraries that are, particularly if you're using NumPy, SciPy, Pandas, SciKit Learn, a variety of those packages.

00:32:50.300 --> 00:32:54.900
We have an accelerated version of those as well in a Python product.

00:32:54.900 --> 00:33:04.380
So there's a lot of, you know, but our common vision, you know, goes end-to-end in that sort of thing, whether it's the upstream stuff that we're doing or the Python product.

00:33:04.380 --> 00:33:10.600
We just want to make sure that the experience people have with Python is no compromises relative to performance.

00:33:11.180 --> 00:33:11.900
Yeah, that's great.

00:33:11.900 --> 00:33:15.800
And I want to talk about the NumPy, the data science stuff in a moment.

00:33:30.640 --> 00:33:37.840
This portion of Talk Python To Me is brought to you by Metis, offering data science training in New York City, Chicago, San Francisco, and online.

00:33:37.840 --> 00:33:51.240
Led by a deep team of senior data scientists, Metis delivers immersive boot camps, corporate workshops, online training, and part-time professional development courses in data visualization, machine learning, big data, and other data science skills.

00:33:51.240 --> 00:33:57.240
Their full-time boot camp is the only accredited data science boot camp available and includes extensive career support.

00:33:57.880 --> 00:34:08.240
Metis maintains a busy event schedule, so be sure to check them out on Meetup and keep in touch via at ThisIsMetis on Twitter and learn more about them on the web at ThisIsMetis.com.

00:34:08.240 --> 00:34:23.200
So you said that you had worked with the PyPy guys and you actually got them together to do a sprint with some people on your team.

00:34:23.200 --> 00:34:24.980
Is that what the story is?

00:34:24.980 --> 00:34:25.420
Yeah, that's right.

00:34:25.920 --> 00:34:35.280
Yeah, it turns out that all of the core PyPy developers are in Europe, except for Maciek, who's in South Africa.

00:34:35.280 --> 00:34:40.780
And I have a staff of developers working on Python that are in Europe.

00:34:40.780 --> 00:34:42.800
And so we brought everybody together.

00:34:42.800 --> 00:34:45.840
It turns out it's in Bucharest, Romania.

00:34:46.500 --> 00:34:51.140
And so we got everybody together physically in the same room.

00:34:51.140 --> 00:34:57.720
There were actually a couple of folks from the academic world, a couple of universities that wanted to come in and, you know, work.

00:34:57.720 --> 00:35:01.620
Once I got wind of this happening, they said, man, we want to get involved in this, too.

00:35:02.180 --> 00:35:08.940
So, you know, it was a very effective sprint and great to get that teamwork going.

00:35:08.940 --> 00:35:16.880
I feel like we now have a great understanding not only of, you know, PyPy, but also the microarchitectural tools that we have to analyze performance and things of that sort.

00:35:16.940 --> 00:35:23.260
So we can now turn this thing, I think, into a really powerful kind of collaboration with the PyPy project.

00:35:23.260 --> 00:35:24.280
Oh, that's great.

00:35:24.280 --> 00:35:37.920
And I think that's really great that you were able to help get everyone together because that's one of the real big challenges of these open source projects that are somewhat large is just physically getting the people together.

00:35:37.920 --> 00:35:42.040
So there might be people on projects that have never met before, right?

00:35:42.040 --> 00:35:43.500
Absolutely.

00:35:43.500 --> 00:35:49.820
And, you know, I tell you, I've been working in specifically open source projects since like the mid-2000s.

00:35:49.820 --> 00:35:55.340
And it's true that, you know, open source would never have been, you know, wildly successful without the Internet.

00:35:55.340 --> 00:36:09.400
But trust me, when you can get people physically face-to-face, you can get human-to-human contact in a physical, you know, same physical space, you eliminate a lot of inefficiencies and barriers and you get a lot of progress very quickly.

00:36:09.400 --> 00:36:18.880
And so as far as I'm concerned, that's a worthwhile sort of investment to get the smart people together in a room and work on real problems and produce code as a result.

00:36:18.880 --> 00:36:21.780
Yeah, even just getting to know people for a long weekend.

00:36:21.780 --> 00:36:24.500
That can last like a whole year's worth of goodwill from that, right?

00:36:24.500 --> 00:36:25.920
Absolutely.

00:36:25.920 --> 00:36:26.760
No question.

00:36:26.760 --> 00:36:27.740
Yeah, great.

00:36:27.740 --> 00:36:33.880
So that's a huge commitment back to the PyPy group and all of that is open source, of course.

00:36:33.880 --> 00:36:40.800
Have you done anything where you've taken some of this work and research and gotten it back into CPython?

00:36:40.800 --> 00:36:47.460
Yeah, we're trying to be not an either or for me, at least.

00:36:47.460 --> 00:36:52.620
It's like there's still, you know, adoption of PyPy is currently not very high.

00:36:53.500 --> 00:36:55.460
A lot more people are running code on CPython.

00:36:55.460 --> 00:36:57.920
So it's not like an either or for me.

00:36:57.920 --> 00:36:59.120
It's more like a both and.

00:36:59.120 --> 00:37:03.360
And the same thing goes for Python 2 versus Python 3.

00:37:03.360 --> 00:37:17.960
I mean, even though Python 3 is the place where Guido and the rest of the community, the development community, would like everybody just to be doing things on Python 3, reality is core services and OpenStack, a lot of other code is still in Python 2.

00:37:18.640 --> 00:37:24.780
So that's our commitment to the community is as we do performance work, we're doing it for both Python 2 and Python 3.

00:37:25.720 --> 00:37:30.000
And we're really trying to pull off a thing where we can do CPython and PyPy as well.

00:37:30.000 --> 00:37:41.140
So, you know, we have, I think there's some chemical issues that we can, like I said, give a little bit of love to in terms of making it much easier to use PyPy in many of these circumstances.

00:37:41.140 --> 00:37:52.600
So there's some engineering work along those lines as well that we're trying to invest in also to eliminate if there are any deployment issues or any issues with, you know, with the garbage collector or something like that.

00:37:52.640 --> 00:37:55.060
So we're really trying to, you know, really eliminate those.

00:37:55.060 --> 00:38:03.040
And there is a class of code that is challenged partly because PyPy is so fast.

00:38:03.040 --> 00:38:10.040
You know, this is the amazing thing to me is that we've encountered a couple of places where, you know, people have coded a timeout in Python.

00:38:10.460 --> 00:38:13.580
And, you know, the code wasn't exactly correct.

00:38:13.580 --> 00:38:23.080
And so when you switch to PyPy, which is multiple times faster, right, suddenly these timeouts fail because, you know, the code, you know, isn't exactly correct.

00:38:23.080 --> 00:38:27.240
Now, it's not necessarily something I can fix in PyPy.

00:38:27.240 --> 00:38:31.440
I mean, I'd love to figure that out somehow, but I'm not sure that's going to be possible.

00:38:31.440 --> 00:38:36.820
But, you know, we've seen this now in a couple of very interesting instances where people, you know, write their code.

00:38:36.820 --> 00:38:45.460
And, you know, it's, you know, suddenly when you speed it up a lot, it breaks because it made some assumptions about how slow it should be running.

00:38:45.460 --> 00:38:47.740
It got to the weight too quickly, huh?

00:38:47.740 --> 00:38:50.940
It's an odd problem to have.

00:38:50.940 --> 00:38:57.640
But, you know, that is, frankly, when you're going to do a drastic improvement in performance, you may find an issue like that that pops up.

00:38:57.640 --> 00:39:00.140
So in any case, that's something we've seen a little bit of.

00:39:00.140 --> 00:39:05.340
But hopefully people can fix their code because I think that's probably a good thing, right?

00:39:05.340 --> 00:39:06.180
Yeah, absolutely.

00:39:06.560 --> 00:39:07.820
That's obviously the fix.

00:39:07.820 --> 00:39:09.360
Like, hey, let's just not wait so long.

00:39:09.360 --> 00:39:13.920
Anyway, so it sounds to me like you have this multi-pronged approach.

00:39:13.920 --> 00:39:20.220
You're trying to make the processors faster by understanding how they run Python code, trying to make CPython faster, trying to make PyPy faster.

00:39:20.220 --> 00:39:31.660
Another one of the prongs on making things faster or addressing this Python performance story is around something you call the Intel distribution for Python.

00:39:31.660 --> 00:39:33.340
And you kind of hinted at that before, right?

00:39:33.340 --> 00:39:34.160
What's the story there?

00:39:34.820 --> 00:39:40.180
Yeah, and this is, again, we have sort of two-pronged strategy here.

00:39:40.180 --> 00:39:44.960
One strategy says let's do, you know, everything upstream in open source.

00:39:45.160 --> 00:39:51.600
And the other is, is there some way we can kind of pinpoint some of the key pinch points that people have?

00:39:52.420 --> 00:39:57.780
And then it may be that we have some code, you know, internally that will accelerate those things.

00:39:57.780 --> 00:40:05.900
So, you know, in particular when you're running, you know, math functions, this is something that in the evolution of people's Python code, right?

00:40:05.900 --> 00:40:12.640
They write these things initially in Python, and then it's like, wow, this is kind of a performance pinch point.

00:40:12.640 --> 00:40:14.820
Is there something we can kind of do to improve it?

00:40:14.820 --> 00:40:20.920
So a lot of times people recode these things in C, and it's like, okay, and NumPy is a great example of that, right?

00:40:20.920 --> 00:40:25.360
So what we've actually been able to do is say, gee, we have this library here.

00:40:25.440 --> 00:40:26.380
It's called MKL.

00:40:26.380 --> 00:40:28.560
It's called our math kernel libraries, right?

00:40:28.560 --> 00:40:38.840
Where some incredibly smart people have gone at the deepest possible level to have math functions and execute them just as fast as possible, right?

00:40:38.840 --> 00:40:45.940
So they're highly tuned, very, you know, very, very bespoke sort of code, right?

00:40:45.940 --> 00:40:55.140
So, you know, what they've done is done an amazing job hooking up MKL with NumPy, SciPy, and these other popular libraries.

00:40:55.720 --> 00:41:00.480
And so that's code that they actually call the Intel Python distribution or Intel distribution.

00:41:00.480 --> 00:41:07.260
I might have the name wrong, but it's actually in beta now as we're talking, and so that should be something,

00:41:07.260 --> 00:41:12.920
some get in on the beta program, or if they're listening to this at a later time, they may, in fact, be already released.

00:41:12.920 --> 00:41:18.920
So that's another, I think, great alternative, particularly if you're using those scientific libraries to basically make use of that

00:41:18.920 --> 00:41:21.260
and see, you know, if that's going to improve things.

00:41:21.260 --> 00:41:27.540
In some cases, I think they found some code that's, you know, again, it can multithread some of these operations,

00:41:27.540 --> 00:41:31.960
whereas with, you know, regular Python, it's more or less single-threaded.

00:41:31.960 --> 00:41:35.180
And so with multithreading, depending on how many cores you can throw at it,

00:41:35.180 --> 00:41:39.080
they've seen up to 100x on some math functions that they've been able to speed up.

00:41:39.080 --> 00:41:40.740
So that's pretty stunning.

00:41:40.940 --> 00:41:47.840
There's also some data analytics functions that are part of this, and so I think that's a really intriguing option as well.

00:41:47.840 --> 00:41:49.980
Yeah, that does sound intriguing.

00:41:49.980 --> 00:41:56.620
And it sounds to me like what you're actually targeting there is you're targeting the C extensions

00:41:56.620 --> 00:42:02.280
or the C foundations of some of these heavily C-sped-up libraries.

00:42:03.320 --> 00:42:04.800
Yeah, that's exactly right.

00:42:04.800 --> 00:42:12.500
It's a strategy that, again, is like we don't want to have people make a choice between, well, do I want to stick with Python

00:42:12.500 --> 00:42:14.140
or do I want good performance, right?

00:42:14.140 --> 00:42:17.400
We want to be able to make it so that it's a no-compromise situation.

00:42:17.400 --> 00:42:24.140
So, frankly, this is another alternative that lets us, you know, target, you know, some users that really want super high performance

00:42:24.140 --> 00:42:25.620
in this mathematical area.

00:42:25.800 --> 00:42:30.200
And high-performance computing is another area that we've invested in pretty heavily here.

00:42:30.200 --> 00:42:33.340
So this is a way to, you know, get directly into that space and help them out.

00:42:33.340 --> 00:42:40.680
Yeah, and I guess you guys probably understand better than anybody how to exactly line up the math to work on your processors, right?

00:42:40.680 --> 00:42:46.920
Yeah, we really have some amazing tools and analytics to do.

00:42:46.920 --> 00:42:56.720
Yeah, when you sit next to the guys designing the chips, it's usually, you know, much easier to be able to squeeze out that last little bit of performance in any possible case.

00:42:56.720 --> 00:42:57.560
Yeah, yeah.

00:42:57.560 --> 00:42:58.080
Sounds great.

00:42:58.080 --> 00:43:00.000
I'll be sure to link to that in the show notes.

00:43:00.000 --> 00:43:06.440
So one of the things that we talked about last week is you were actually at the OpenStack conference, right?

00:43:06.440 --> 00:43:07.920
Maybe you can give us a...

00:43:07.920 --> 00:43:07.920
That's correct.

00:43:07.920 --> 00:43:08.660
OpenStack Summit.

00:43:08.660 --> 00:43:09.000
Yeah.

00:43:09.000 --> 00:43:09.360
Awesome.

00:43:09.360 --> 00:43:10.740
Give us a rundown on what happened there.

00:43:10.740 --> 00:43:11.620
Yeah.

00:43:11.880 --> 00:43:15.860
This is really exciting because the OpenStack Summit is, you know, every six months.

00:43:15.860 --> 00:43:18.800
This one was in Austin, Texas.

00:43:18.800 --> 00:43:32.120
And what was interesting about this is that OpenStack as a project is going through a lot of really interesting evolution as cloud computing obviously takes over the kind of the fastest growing segment of the data center kind of area, right?

00:43:32.120 --> 00:43:35.780
So software-defined infrastructure is incredibly important.

00:43:35.780 --> 00:43:39.960
And something like 70% of OpenStack is actually written in Python.

00:43:40.160 --> 00:43:51.340
So one of the things that we did was jointly with the project technical leader, or PTL, of Swift, he and I have been collaborating together on PyPy,

00:43:51.340 --> 00:44:00.860
and we were able to jointly present our performance results and showing, hey, here's the effect performance-wise of using PyPy with Swift.

00:44:01.400 --> 00:44:03.820
And again, I think I mentioned the statistics earlier.

00:44:03.820 --> 00:44:05.840
It's like more than double the throughput, right?

00:44:05.840 --> 00:44:14.100
We have data which shows that we can, you know, get 111% throughput improvement and 87% response time improvement.

00:44:14.100 --> 00:44:16.420
You know, that's stuff that we've seen.

00:44:16.420 --> 00:44:20.260
The PTL, John Dickinson, works for a company called SwiftStack.

00:44:20.720 --> 00:44:25.480
And in their product sort of environment, they have a Swift-based product family.

00:44:25.480 --> 00:44:28.340
And so, yeah, they were able to see, you know, similar results.

00:44:28.340 --> 00:44:31.260
And so jointly we were able to present this.

00:44:31.260 --> 00:44:33.720
And people were very excited about this.

00:44:33.720 --> 00:44:39.760
The response was extremely positive, partly because of the opportunity to see the collaboration go on.

00:44:39.760 --> 00:44:47.000
You know, anytime you go in with open source and you can collaborate and talk about how we're working together to make people's lives better,

00:44:47.100 --> 00:44:48.820
I think it's all good, right?

00:44:48.820 --> 00:44:54.600
So that was – but what was funny is I've got to tell you a story.

00:44:54.600 --> 00:44:59.120
When I landed in Austin, so I get to my hotel that I'm staying at for this conference,

00:44:59.120 --> 00:45:02.420
and I'm walking over to the conference venue to get my conference badge.

00:45:02.420 --> 00:45:08.080
The first person I see on the street, I'm sad to say I didn't recognize him,

00:45:08.080 --> 00:45:11.040
but he recognized me and he said, oh, it's Mr. PyPy.

00:45:13.040 --> 00:45:18.780
Somehow, then I found out, okay, why have you recognized me?

00:45:18.780 --> 00:45:24.000
Anyway, so it's – I think the word's kind of getting out that this is something that, you know,

00:45:24.000 --> 00:45:25.080
we see some real value in.

00:45:25.080 --> 00:45:28.500
We'd like to try and help people out through advancing it a little bit.

00:45:28.500 --> 00:45:28.820
Yeah.

00:45:28.820 --> 00:45:34.220
Yeah, you must be being effective if you're getting recognized on the street as Mr. PyPy.

00:45:34.220 --> 00:45:34.680
That's awesome.

00:45:35.720 --> 00:45:41.480
My usual comment is a good reputation takes a long time to build, but a bad reputation is instantaneous.

00:45:41.480 --> 00:45:46.780
So, you know, I think we'll – you know, hopefully we can keep it to be a good reputation as opposed to a bad one.

00:45:46.780 --> 00:45:47.940
Yeah, absolutely.

00:45:47.940 --> 00:45:50.740
Okay, well, that sounds really interesting.

00:45:50.740 --> 00:45:53.400
That's great you were able to share your results with everyone.

00:45:54.520 --> 00:45:58.960
Another thing that you said that was pretty interesting, sort of completely unrelated to this,

00:45:58.960 --> 00:46:05.740
but you've been talking about how the Python community is a really welcoming community.

00:46:05.740 --> 00:46:12.260
And I can tell you from doing training and speaking at conferences that are not just Python conferences

00:46:12.260 --> 00:46:16.880
but have many languages and technologies that when I do Python talks,

00:46:16.880 --> 00:46:22.560
the group of people in the room are more representative of the group of people out in society,

00:46:22.560 --> 00:46:24.020
which is really great.

00:46:24.020 --> 00:46:25.920
Well, I totally agree with you.

00:46:25.920 --> 00:46:31.600
I think it's amazing to see, particularly because I've spent a number of years in other open source projects.

00:46:31.600 --> 00:46:42.420
And so I know at sometimes a painful level how challenging it can be for some people who are not males in particular.

00:46:42.420 --> 00:46:46.180
And this is one of the cases where I don't want to point to particular projects or something like that,

00:46:46.180 --> 00:46:48.000
but I've been involved in a number of projects.

00:46:48.000 --> 00:46:55.880
And it's hard sometimes to find projects that not only have great diversity but inclusion as well.

00:46:55.880 --> 00:46:59.160
And so one of the things that's been very impressive to me,

00:46:59.160 --> 00:47:02.800
the first Python conference I went to actually was EuroPython.

00:47:02.800 --> 00:47:08.200
And Guido made a – actually it was before Guido came on as a keynote speaker.

00:47:08.200 --> 00:47:11.660
The very first keynote of the project, of the conference,

00:47:11.840 --> 00:47:15.920
was a conference given by the founders of Django Girls.

00:47:15.920 --> 00:47:20.540
And it's two female engineers.

00:47:20.540 --> 00:47:28.980
And Django Girls was set up by then as a nonprofit to basically have women teach other women how to code in Python, right?

00:47:28.980 --> 00:47:30.600
And it was a great keynote.

00:47:30.800 --> 00:47:36.020
I mean they actually did a bunch of it as like a fable, you know, kind of talking about –

00:47:36.020 --> 00:47:39.700
essentially talking about how challenging it is for, you know,

00:47:39.700 --> 00:47:44.340
women to really be considered part of the community, right, of any sort of community.

00:47:44.340 --> 00:47:48.660
And so Django Girls, what's amazing is they started up with pretty much nothing.

00:47:48.660 --> 00:47:54.280
And after the first year, they had had 100 workshops all over the world.

00:47:54.480 --> 00:47:57.280
Again, women teaching women how to program in Python, right?

00:47:57.280 --> 00:47:59.780
And this was incredibly impressive.

00:47:59.780 --> 00:48:06.060
I mean Guido actually in his keynote, he – you know, I think he's part of the reason.

00:48:06.060 --> 00:48:07.240
It's like this.

00:48:07.240 --> 00:48:09.580
I think he's very committed.

00:48:09.580 --> 00:48:13.000
He was wearing a PyLadies T-shirt in the conference.

00:48:13.000 --> 00:48:19.520
PyLadies is another, you know, part of the Python community that works really well to help encourage women be involved in Python.

00:48:19.980 --> 00:48:23.320
He – in his keynote, he said – most of it was Q&A from the audience.

00:48:23.320 --> 00:48:26.840
And he said, let's alternate a man and a woman asking questions, right?

00:48:26.840 --> 00:48:30.520
So he made it super clear that that's a priority for him.

00:48:30.520 --> 00:48:31.460
And I asked him about it.

00:48:31.460 --> 00:48:34.220
Is that, you know, something that he – it's important to him?

00:48:34.220 --> 00:48:35.080
He said, yeah, absolutely.

00:48:35.080 --> 00:48:36.680
And so he's making a difference.

00:48:36.680 --> 00:48:39.440
He's putting his own, you know, kind of stamp behind that.

00:48:39.440 --> 00:48:50.640
And I might – on a personal basis, I had – you know, my daughter is – one of my daughters is 24 and she's still trying to find kind of her career direction.

00:48:50.640 --> 00:48:54.840
And she said, Dad, you know, if I wanted to learn programming, you know, what would you suggest?

00:48:54.840 --> 00:48:58.280
And I said, well, I think you should try Python.

00:48:58.280 --> 00:49:02.880
And I said, in fact, you know, you ought to check out this Django Girls thing to see if there's a workshop.

00:49:03.000 --> 00:49:05.900
Well, it turned out, you know, we live in the Portland area.

00:49:05.900 --> 00:49:09.200
There was a Django Girls workshop in another, like, six weeks.

00:49:09.200 --> 00:49:11.640
And I said, oh, look, you could get involved in that.

00:49:11.640 --> 00:49:20.160
And she submitted an application, explained, you know, what she'd be using her knowledge for and how – you know, why she wanted to learn it.

00:49:20.160 --> 00:49:21.760
They accepted her in the workshop.

00:49:21.760 --> 00:49:24.040
And, you know, she was a little nervous.

00:49:24.040 --> 00:49:28.300
She said, gee, I'm not sure I'm smart enough to actually, you know, do well with this, right?

00:49:28.300 --> 00:49:29.080
And I said, no, no, no.

00:49:29.080 --> 00:49:32.460
You know, and I understand sometimes people can feel that way.

00:49:32.500 --> 00:49:42.920
Typically, they've been made to feel not smart, which, you know, I try and do everything I can with my daughters to make them feel like they're – you know, they're both brilliant.

00:49:42.920 --> 00:49:45.000
They have no worry about that.

00:49:45.000 --> 00:49:51.320
But sometimes the environment is one that wants to, you know, make people who are, you know, women not necessarily feel very smart.

00:49:51.320 --> 00:49:53.940
And so I said, you know, I encouraged her.

00:49:53.940 --> 00:49:55.220
She went through the workshop.

00:49:55.220 --> 00:49:56.700
She was very successful with it.

00:49:56.940 --> 00:50:03.300
And I'm hoping, you know, fingers crossed that it would be something she'd be able to, you know, make a living doing if she digs it.

00:50:03.300 --> 00:50:08.220
So, yeah, I love the fact that Django Girls is – you know, has this available.

00:50:08.220 --> 00:50:12.560
And it's, again, part of – I think a core part of the Python community.

00:50:12.560 --> 00:50:20.600
I think it's one of the things that makes it a very attractive community is how important it is for the community to – for women to help, you know, other women.

00:50:20.600 --> 00:50:23.380
And for the entire community to have better inclusion.

00:50:23.380 --> 00:50:25.040
Now, is it perfect?

00:50:25.040 --> 00:50:25.720
No.

00:50:25.720 --> 00:50:28.460
I think there's some challenges still.

00:50:28.460 --> 00:50:33.780
I interviewed one of the – a fellow of the Python Foundation.

00:50:33.780 --> 00:50:36.460
Her name is Terry Oda.

00:50:36.460 --> 00:50:42.660
And I talked to Terry and I said – I asked her how it is from her perspective as a woman.

00:50:42.660 --> 00:50:46.120
I mean, it's one thing for me as a male to say, gee, this seems really good.

00:50:46.180 --> 00:50:47.800
Well, how about a perspective of a woman?

00:50:47.800 --> 00:50:52.040
And she's part of the, you know, the Intel – or, sorry, the Python Foundation.

00:50:52.040 --> 00:50:53.400
She's also an Intel employee.

00:50:53.400 --> 00:50:57.600
And she said, yeah, you know, she said that it's a good environment.

00:50:57.600 --> 00:51:03.420
It has the – you know, it could do better in terms of having more core developers who are women.

00:51:03.420 --> 00:51:11.300
And so there's more, you know, work that can be done to, you know, get more of that inclusion and diversity within, you know, sort of the core development community.

00:51:11.300 --> 00:51:15.860
But it is a great project for its diversity and inclusion.

00:51:15.860 --> 00:51:16.920
Yeah, that's cool.

00:51:16.920 --> 00:51:21.140
I think the Python community is more welcoming than most.

00:51:21.140 --> 00:51:23.920
And I think this is just, you know, more evidence of it.

00:51:23.920 --> 00:51:24.660
So that's cool.

00:51:24.660 --> 00:51:30.680
Hopefully we can inspire some more people to go out and check out some of these projects that are out there and freely available.

00:51:30.680 --> 00:51:31.440
Very cool.

00:51:31.440 --> 00:51:32.900
I agree, yeah.

00:51:32.900 --> 00:51:40.980
So getting near the end of the show, let me ask you a couple of questions that I always ask my guests while I still have you.

00:51:40.980 --> 00:51:44.280
If you're going to write some Python code, what editor do you open up?

00:51:44.720 --> 00:51:45.640
Oh, yeah.

00:51:45.640 --> 00:51:47.320
I'm an old VI guy.

00:51:47.320 --> 00:51:53.000
So, you know, with my roots back in Linux and back in the old days, or Unix, you know, before there was a Linux.

00:51:53.000 --> 00:51:55.500
So, yes, when I edit, I use VI.

00:51:55.500 --> 00:51:56.680
Okay, excellent.

00:51:56.680 --> 00:51:58.000
Yeah, that's definitely a popular one.

00:51:58.000 --> 00:52:04.960
And of all the PyPI packages, you can include Intel ones if you want, there's a ton of them.

00:52:04.960 --> 00:52:07.600
And nobody could possibly know about them all.

00:52:07.600 --> 00:52:10.040
There's, you know, close to 80,000, maybe over.

00:52:10.120 --> 00:52:10.900
I haven't looked in a while.

00:52:10.900 --> 00:52:17.420
What ones do you like that maybe you'd recommend people check out that are not necessarily the most popular?

00:52:17.420 --> 00:52:20.940
I'd ask the question, though, what's my favorite interpreter?

00:52:20.940 --> 00:52:24.840
My favorite interpreter for Python these days is definitely PyPy.

00:52:24.840 --> 00:52:25.980
Yeah, okay.

00:52:25.980 --> 00:52:26.820
That's great.

00:52:26.820 --> 00:52:28.640
It's a very meta answer, right?

00:52:28.720 --> 00:52:31.520
Like, it's PyPy that runs all the packages.

00:52:31.520 --> 00:52:32.600
I'm not a politician.

00:52:32.600 --> 00:52:34.640
I don't even get paid to be a politician.

00:52:34.640 --> 00:52:37.800
But sometimes I know how to, you know, reframe the question.

00:52:37.800 --> 00:52:38.700
Yeah, absolutely.

00:52:38.700 --> 00:52:44.980
Okay, so do you have a final call to action for listeners, things they should do or check out?

00:52:46.080 --> 00:52:51.800
Yeah, I would absolutely have people take another look at PyPy.

00:52:51.800 --> 00:52:53.320
You probably have in the past.

00:52:53.320 --> 00:52:59.040
In particular, we would love to have people check out, if you're using, you know, OpenStack or Swift,

00:52:59.040 --> 00:53:03.880
we would love to get people to, you know, check it out with the latest version of PyPy

00:53:03.880 --> 00:53:10.480
and really try to give us some feedback in the Swift community or in the Python dev community.

00:53:10.480 --> 00:53:12.080
We monitor Python dev.

00:53:12.080 --> 00:53:16.720
And if you have some observations about PyPy that would be helpful, saying, you know, for example,

00:53:16.720 --> 00:53:22.120
if you have some Python code and if you either find you have some, you know, challenges with it,

00:53:22.120 --> 00:53:26.440
we would love to hear that because one of the things we'd love to do is I've got engineers I can, you know,

00:53:26.440 --> 00:53:29.240
try and ask to, you know, basically say, hey, let's go solve these.

00:53:29.240 --> 00:53:34.100
I've given the call to action to our engineers to say, try and, you know, resolve these,

00:53:34.100 --> 00:53:39.240
any sort of compatibility issues or other deployment issues and let's make, you know,

00:53:39.240 --> 00:53:40.780
try and make this a great experience for people.

00:53:40.780 --> 00:53:45.960
So if you can help us out in terms of, you know, get your hands on PyPy, try it against your Python code

00:53:45.960 --> 00:53:50.740
and give us feedback in terms of what might be missing or could work better, we would love to hear that

00:53:50.740 --> 00:53:52.440
and we'd love to dialogue with you on it.

00:53:52.440 --> 00:53:53.100
Okay.

00:53:53.100 --> 00:53:57.220
So absolutely try your code on PyPy and send these guys some feedback.

00:53:57.220 --> 00:53:57.920
That's great.

00:53:58.880 --> 00:54:02.180
All right, David, it's been a super interesting conversation.

00:54:02.180 --> 00:54:08.440
I'm really excited to see the work that you're doing appear in the Intel chips going forward

00:54:08.440 --> 00:54:12.780
and in both the runtimes in CPython and PyPy.

00:54:12.780 --> 00:54:14.060
Thank you, Michael.

00:54:14.060 --> 00:54:16.100
I love talking about this stuff.

00:54:16.100 --> 00:54:20.800
I'm incredibly passionate about it and I'd love to, you know,

00:54:20.800 --> 00:54:25.140
I'd see our love for technology be able to be made use for a lot of people.

00:54:25.320 --> 00:54:29.060
So I'm hopeful that people will get excited about this stuff as a result too.

00:54:29.060 --> 00:54:30.020
Yeah, absolutely.

00:54:30.020 --> 00:54:31.240
Thanks for being on the show.

00:54:31.240 --> 00:54:31.700
Talk to you later.

00:54:31.700 --> 00:54:32.500
Thank you.

00:54:34.880 --> 00:54:48.860
Snap.CI is effective in deep training for data scientists.

00:54:48.860 --> 00:55:00.960
Metis is effective in deep training for data scientists.

00:55:00.960 --> 00:55:04.180
With immersive boot camps, in-person and online courses,

00:55:04.180 --> 00:55:08.420
you can learn the core data science skills you need to take your career to the next level.

00:55:08.420 --> 00:55:11.400
Are you or a colleague trying to learn Python?

00:55:11.400 --> 00:55:16.000
Have you tried books and videos that just left you bored by covering topics point by point?

00:55:16.000 --> 00:55:21.080
Well, check out my online Python course called Python Jumpstart by Building 10 Apps at

00:55:21.080 --> 00:55:25.740
 talkpython.fm/course to experience a more engaging way to learn Python.

00:55:26.400 --> 00:55:31.640
You can find the links from this episode at talkpython.fm/episodes slash show slash 57.

00:55:31.640 --> 00:55:33.860
And be sure to subscribe to the show.

00:55:33.860 --> 00:55:36.180
Open your favorite podcatcher and search for Python.

00:55:36.180 --> 00:55:37.300
We should be right at the top.

00:55:37.300 --> 00:55:40.500
You can also find the iTunes feed at /itunes,

00:55:40.500 --> 00:55:43.260
Google Play feed at /play,

00:55:43.260 --> 00:55:47.720
and direct RSS feed at /rss on talkpython.fm.

00:55:48.220 --> 00:55:51.320
And there's been a few requests for the Ogthera format,

00:55:51.320 --> 00:55:53.480
a more open format than MP3.

00:55:53.480 --> 00:55:56.460
And I added that link as well in the footer of the website.

00:55:56.460 --> 00:56:00.060
Our theme music is Developers, Developers, Developers by Corey Smith,

00:56:00.060 --> 00:56:01.020
who goes by Smix.

00:56:01.020 --> 00:56:04.820
You can hear the entire song on talkpython.fm/music.

00:56:04.820 --> 00:56:07.080
This is your host, Michael Kennedy.

00:56:07.080 --> 00:56:08.700
Thank you so much for listening.

00:56:08.700 --> 00:56:11.000
Smix takes us out of here.

00:56:11.000 --> 00:56:15.100
Stating with my voice, there's no norm that I can feel within.

00:56:15.100 --> 00:56:17.940
Haven't been sleeping, I've been using lots of rest.

00:56:17.940 --> 00:56:20.800
I'll pass the mic back to who rocked his best.

00:56:20.800 --> 00:56:22.560
First developers.

00:56:22.560 --> 00:56:22.700
First developers.

00:56:22.700 --> 00:56:25.460
First developers.

00:56:25.460 --> 00:56:26.280
First developers.

00:56:26.280 --> 00:56:27.460
First developers.

00:56:27.460 --> 00:56:28.520
First developers.

00:56:28.520 --> 00:56:29.520
First developers.

00:56:29.520 --> 00:56:31.520
First developers.

00:56:31.520 --> 00:56:33.120
First of all, first of all, first of all, first of all.

00:56:33.120 --> 00:57:03.100
Thank you.

