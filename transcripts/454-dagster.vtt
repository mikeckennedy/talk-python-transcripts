WEBVTT

00:00:00.001 --> 00:00:05.060
Do you have data that you pull from external sources or that is generated and appears at

00:00:05.060 --> 00:00:06.180
your digital doorstep?

00:00:06.180 --> 00:00:10.980
I bet that data needs processed, filtered, transformed, distributed, and much more.

00:00:10.980 --> 00:00:15.880
One of the biggest tools to create these data pipelines with Python is Dagster.

00:00:15.880 --> 00:00:19.740
And we're fortunate to have Pedram Navid on the show to tell us about it.

00:00:19.740 --> 00:00:23.700
Pedram is the head of data engineering and DevRel at Dagster Labs.

00:00:23.700 --> 00:00:27.880
And we're talking data pipelines this week here at Talk Python.

00:00:28.080 --> 00:00:33.820
This is Talk Python to me, episode 454, recorded January 11th, 2024.

00:00:33.820 --> 00:00:51.500
Welcome to Talk Python to me, a weekly podcast on Python.

00:00:51.500 --> 00:00:53.260
This is your host, Michael Kennedy.

00:00:53.260 --> 00:00:57.740
Follow me on Mastodon, where I'm @mkennedy and follow the podcast using at

00:00:57.740 --> 00:01:00.740
Talk Python, both on fosstodon.org.

00:01:00.740 --> 00:01:05.820
Keep up with the show and listen to over seven years of past episodes at talkpython.fm.

00:01:05.820 --> 00:01:09.540
We've started streaming most of our episodes live on YouTube.

00:01:09.540 --> 00:01:14.280
Subscribe to our YouTube channel over at talkpython.fm/youtube to get notified

00:01:14.280 --> 00:01:17.160
about upcoming shows and be part of that episode.

00:01:18.480 --> 00:01:22.620
This episode is sponsored by Posit Connect from the makers of Shiny.

00:01:22.620 --> 00:01:27.120
Publish, share, and deploy all of your data projects that you're creating using Python.

00:01:27.120 --> 00:01:33.600
Streamlit, Dash, Shiny, Bokeh, FastAPI, Flask, Quattro, Reports, Dashboards, and APIs.

00:01:33.600 --> 00:01:35.980
Posit Connect supports all of them.

00:01:35.980 --> 00:01:40.340
Try Posit Connect for free by going to talkpython.fm/posit.

00:01:40.760 --> 00:01:41.760
Posit Connect.

00:01:41.760 --> 00:01:46.060
And it's also brought to you by us over at Talk Python Training.

00:01:46.060 --> 00:01:50.680
Did you know that we have over 250 hours of Python courses?

00:01:50.680 --> 00:01:51.880
Yeah, that's right.

00:01:51.880 --> 00:01:54.460
Check them out at talkpython.fm/courses.

00:01:56.040 --> 00:02:00.880
Last week, I told you about our new course, Build an AI Audio App with Python.

00:02:00.880 --> 00:02:05.420
Well, I have another brand new and amazing course to tell you about.

00:02:05.420 --> 00:02:10.960
This time, it's all about Python's typing system and how to take the most advantage of it.

00:02:10.960 --> 00:02:15.740
It's a really awesome course called Rock Solid Python with Python Typing.

00:02:15.740 --> 00:02:19.460
This is one of my favorite courses that I've created in the last couple of years.

00:02:19.460 --> 00:02:23.300
Python type hints are really starting to transform Python,

00:02:23.300 --> 00:02:25.680
especially from the ecosystem's perspective.

00:02:25.680 --> 00:02:28.860
Think FastAPI, Pydantic, BearType, etc.

00:02:28.860 --> 00:02:34.160
This course shows you the ins and outs of Python typing syntax, of course,

00:02:34.160 --> 00:02:37.620
but it also gives you guidance on when and how to use type hints.

00:02:37.620 --> 00:02:43.540
Check out this four and a half hour in-depth course at talkpython.fm/courses.

00:02:43.540 --> 00:02:46.440
Now, on to those data pipelines.

00:02:46.440 --> 00:02:50.700
Pedram, welcome to Talk Python to me.

00:02:50.700 --> 00:02:51.760
It's amazing to have you here.

00:02:51.760 --> 00:02:52.960
Michael, great to have you.

00:02:52.960 --> 00:02:53.600
Good to be here.

00:02:53.600 --> 00:02:54.000
Yeah.

00:02:54.000 --> 00:02:57.200
Good to talk about data, data pipelines, automation.

00:02:57.200 --> 00:03:02.420
And boy, oh boy, let me tell you, have I been in the DevOps side of things this week.

00:03:02.420 --> 00:03:07.200
And I'm going to have a special, special appreciation of it.

00:03:07.200 --> 00:03:08.340
I can tell already.

00:03:08.340 --> 00:03:10.380
So excited to talk.

00:03:10.380 --> 00:03:11.100
My good old, sis.

00:03:11.100 --> 00:03:12.620
Indeed.

00:03:13.620 --> 00:03:19.780
So before we get to that, though, before we talk about Dagster and data pipelines and orchestration more broadly,

00:03:19.780 --> 00:03:22.020
let's just give a little bit of background on you.

00:03:22.020 --> 00:03:23.320
Introduce yourself for people.

00:03:23.320 --> 00:03:27.000
How did you get into Python and data orchestration and all those things?

00:03:27.400 --> 00:03:27.540
Of course.

00:03:27.540 --> 00:03:27.820
Yeah.

00:03:27.820 --> 00:03:29.420
So my name is Pedram Navid.

00:03:29.580 --> 00:03:33.360
I'm the head of data engineering and DevRel at Dagster.

00:03:33.360 --> 00:03:34.200
That's a mouthful.

00:03:34.200 --> 00:03:38.140
And I've been a longtime Python user since 2.7.

00:03:38.140 --> 00:03:42.440
And I got started with Python like I do with many things just out of sheer laziness.

00:03:42.880 --> 00:03:48.320
I was working at a bank and there was this rote task, something involving going into servers,

00:03:48.320 --> 00:03:52.200
opening up a text file and seeing if a patch was applied to a server.

00:03:52.200 --> 00:03:57.600
A nightmare scenario when there's 100 servers to check and 15 different patches to confirm.

00:03:57.600 --> 00:03:57.880
Yeah.

00:03:57.880 --> 00:04:01.500
So this kind of predates like the cloud and all that automation and stuff, right?

00:04:01.500 --> 00:04:03.700
So this is definitely before cloud.

00:04:03.700 --> 00:04:06.740
This was like right between Python 2 and Python 3.

00:04:06.740 --> 00:04:09.260
And we were trying to figure out how to use print statements correctly.

00:04:09.260 --> 00:04:10.380
That's when I learned Python.

00:04:10.380 --> 00:04:11.800
I was like, there's got to be a better way.

00:04:12.080 --> 00:04:13.580
And honestly, I've not looked back.

00:04:13.580 --> 00:04:17.360
I think if you look at my entire career trajectory, you'll see it's just

00:04:17.360 --> 00:04:21.480
punctuated by finding ways to be more lazy in many ways.

00:04:21.480 --> 00:04:23.280
Yeah.

00:04:23.280 --> 00:04:24.700
Who was it?

00:04:24.700 --> 00:04:29.580
I think it was Matthew Rocklin that had the phrase something like productive laziness or

00:04:29.580 --> 00:04:30.720
something like that.

00:04:30.720 --> 00:04:36.620
I'm going to find a way to leverage my laziness to force me to build automation.

00:04:36.620 --> 00:04:39.000
So I never, ever have to do this sort of thing again.

00:04:39.000 --> 00:04:40.620
I got that sort of principle.

00:04:40.620 --> 00:04:44.280
It's very motivating to not have to do something and I'll do anything to not do something.

00:04:44.280 --> 00:04:44.600
Yeah.

00:04:44.600 --> 00:04:45.060
Yeah.

00:04:45.060 --> 00:04:45.260
Yeah.

00:04:45.260 --> 00:04:46.020
It's incredible.

00:04:46.020 --> 00:04:51.360
And like that DevOps stuff I was talking about and just, you know, one command and there's

00:04:51.360 --> 00:04:55.700
maybe eight or nine new apps with all their tiers redeployed, updated, resynced.

00:04:55.700 --> 00:05:01.300
And it's, it took me a lot of work to get there, but now I never have to think about it again,

00:05:01.300 --> 00:05:02.380
at least not for a few years.

00:05:02.380 --> 00:05:03.780
And it's amazing.

00:05:03.780 --> 00:05:04.620
I can just be productive.

00:05:04.620 --> 00:05:06.680
It's like right in line, in line with that.

00:05:07.160 --> 00:05:11.680
So what are some of the Python projects you've been, you've worked on, talked about different

00:05:11.680 --> 00:05:13.060
ways to apply this over the years?

00:05:13.060 --> 00:05:13.580
Oh yeah.

00:05:13.580 --> 00:05:18.940
So it started with internal, just like Python projects, trying to automate, like I said, some

00:05:18.940 --> 00:05:23.300
rote tasks that I had and that accidentally becomes, you know, a bigger project.

00:05:23.300 --> 00:05:25.500
People see it and they're like, oh, I want that too.

00:05:25.660 --> 00:05:29.480
And so, well, now I have to build like a GUI interface because most people don't speak

00:05:29.480 --> 00:05:29.800
Python.

00:05:29.800 --> 00:05:34.880
And so that got me into iGUI, I think it was called way back when.

00:05:34.880 --> 00:05:36.060
That was a fun journey.

00:05:36.060 --> 00:05:38.040
And then from there, it's really taken off.

00:05:38.040 --> 00:05:40.600
A lot of it has been mostly personal projects.

00:05:40.600 --> 00:05:45.840
Trying to understand open source was a really big learning path for me as well.

00:05:45.840 --> 00:05:50.460
Really being absorbed by things like SQLAlchemy and requests back when they were coming out.

00:05:50.460 --> 00:05:55.640
Eventually it led to more of a data engineering type of role where I got involved with

00:05:55.640 --> 00:06:00.420
tools like Airflow and trying to automate data pipelines instead of patches on a server.

00:06:00.420 --> 00:06:06.420
That one day led to, I guess, making a long story short, a role at Dexter, where now I

00:06:06.420 --> 00:06:07.820
contribute a little bit to Dexter.

00:06:07.820 --> 00:06:12.060
I work on Dexter, the core project itself, but I also use Dexter internally to build our

00:06:12.060 --> 00:06:13.060
own data pipelines.

00:06:13.060 --> 00:06:19.520
I'm sure it's interesting to see how you all both build Dexter and then consume Dexter.

00:06:19.520 --> 00:06:21.380
Yeah, it's been wonderful.

00:06:21.380 --> 00:06:23.080
I think there's a lot of great things about it.

00:06:23.160 --> 00:06:27.820
One is getting access to Dexter before it's fully released, right?

00:06:27.820 --> 00:06:33.000
So internally, we dog food, new features, new concepts, and we work with the product team,

00:06:33.000 --> 00:06:35.120
the engineering team to say, hey, this makes sense.

00:06:35.120 --> 00:06:35.720
This doesn't.

00:06:35.720 --> 00:06:36.540
This works really well.

00:06:36.540 --> 00:06:37.180
That doesn't.

00:06:37.180 --> 00:06:43.360
And that feedback loop is so fast and so iterative that for me personally, being able to see that

00:06:43.360 --> 00:06:45.360
come to fruition is really, really compelling.

00:06:45.780 --> 00:06:49.700
But at the same time, it's like I get to work at a place that's building a tool for me, right?

00:06:49.700 --> 00:06:51.420
You don't often get that luxury.

00:06:51.420 --> 00:06:53.060
I've worked in ads.

00:06:53.060 --> 00:06:54.540
I've worked in insurance.

00:06:54.540 --> 00:06:55.620
It's like banking.

00:06:55.620 --> 00:06:59.060
It's like, these are nice things, but it's not built for me, right?

00:06:59.060 --> 00:07:01.880
And so for me, that's probably been the biggest benefit, I would say.

00:07:01.880 --> 00:07:02.140
Right.

00:07:02.140 --> 00:07:06.560
If you work in some marketing thing, you're like, you know, I retargeted myself so well

00:07:06.560 --> 00:07:06.820
today.

00:07:06.820 --> 00:07:07.840
You wouldn't believe it.

00:07:07.840 --> 00:07:09.560
I really enjoyed it.

00:07:09.920 --> 00:07:11.880
I've seen the ads that I've created before.

00:07:11.880 --> 00:07:14.960
So it's a little fun, but it's not the same.

00:07:14.960 --> 00:07:15.280
Yeah.

00:07:15.280 --> 00:07:21.460
I've heard of people who are really, really good at ad targeting and finding groups where

00:07:21.460 --> 00:07:26.140
they've like pranked their wife or something or just had an ad that would only show up for

00:07:26.140 --> 00:07:27.000
their wife by running.

00:07:27.000 --> 00:07:29.760
It was like so specific and, you know, freaked them out a little bit.

00:07:29.760 --> 00:07:30.800
That's pretty clever.

00:07:30.800 --> 00:07:32.020
Yeah.

00:07:32.020 --> 00:07:34.040
Maybe it wasn't appreciated, but it is clever.

00:07:34.040 --> 00:07:36.580
Who knows?

00:07:36.580 --> 00:07:37.260
All right.

00:07:37.260 --> 00:07:43.240
Well, before we jump in, you said that, of course, you built GUIs with PyGUI and those

00:07:43.240 --> 00:07:47.740
sorts of things because people don't speak Python back then, two, seven days and whatever.

00:07:47.740 --> 00:07:48.920
Is that different now?

00:07:48.920 --> 00:07:53.000
Not that people speak Python, but is it different in the sense that like, hey, I could give them

00:07:53.000 --> 00:07:58.260
a Jupyter notebook or I could give them Streamlit or one of these things, right?

00:07:58.260 --> 00:08:00.940
Like a little more or less you build and just like plug it in?

00:08:00.940 --> 00:08:01.840
I think so.

00:08:01.840 --> 00:08:05.760
I mean, yeah, like you said, it's not different in that, you know, most people probably still

00:08:05.760 --> 00:08:07.040
to this day don't speak Python.

00:08:07.040 --> 00:08:07.360
Yeah.

00:08:07.360 --> 00:08:10.880
I know we had this like movement a little bit back where everyone was going to learn

00:08:10.880 --> 00:08:12.860
like SQL and everyone was going to learn to code.

00:08:12.860 --> 00:08:18.780
I was never that bullish on that trend because like if I'm a marketing person, I've got 10,000

00:08:18.780 --> 00:08:22.180
things to do and learning to code isn't going to be a priority ever.

00:08:22.180 --> 00:08:27.740
So I think building interfaces for people that are easy to use and speaks well to them is always

00:08:27.740 --> 00:08:28.460
useful.

00:08:28.460 --> 00:08:29.840
That never has gone away.

00:08:29.840 --> 00:08:32.280
But I think the tooling around it has been better, right?

00:08:32.280 --> 00:08:34.220
I don't think I'll ever want to use PyGUI again.

00:08:34.220 --> 00:08:35.300
Nothing wrong with the platform.

00:08:35.300 --> 00:08:36.920
It's just like not fun to write.

00:08:36.920 --> 00:08:39.660
Streamlit makes it so easy to do that.

00:08:39.660 --> 00:08:43.440
So it's like something like Retool and there's like a thousand other ways now that you can

00:08:43.440 --> 00:08:47.800
bring these tools in front of your stakeholders and your users that just wasn't possible before.

00:08:47.800 --> 00:08:48.940
I think it's a pretty exciting time.

00:08:48.940 --> 00:08:51.120
There are a lot of pretty polished tools.

00:08:51.120 --> 00:08:51.480
Yeah.

00:08:51.480 --> 00:08:52.600
It's gotten so good.

00:08:52.600 --> 00:08:52.920
Yeah.

00:08:53.200 --> 00:08:55.320
There are some interesting ones like OpenBB.

00:08:55.320 --> 00:08:55.940
Do you know that?

00:08:55.940 --> 00:08:58.060
The financial dashboard thing.

00:08:58.060 --> 00:08:58.840
I've heard of this.

00:08:58.840 --> 00:08:59.640
I haven't seen it.

00:08:59.640 --> 00:08:59.880
Yeah.

00:08:59.880 --> 00:09:05.580
It's basically for traders, but it's like a terminal type thing that has a bunch of map

00:09:05.580 --> 00:09:11.760
plot lib and other interactive stuff that pops up compared to, say, Bloomberg dashboard

00:09:11.760 --> 00:09:12.640
type things.

00:09:13.200 --> 00:09:18.200
But yeah, that's one sense where like maybe like traders go and learn Python because it's

00:09:18.200 --> 00:09:19.880
like, all right, there's enough value here.

00:09:19.880 --> 00:09:23.200
But in general, I don't think, yeah, I don't think people are going to stop what they're

00:09:23.200 --> 00:09:24.060
doing and learning the code.

00:09:24.060 --> 00:09:25.700
So these new UI things are not.

00:09:25.700 --> 00:09:26.000
All right.

00:09:26.040 --> 00:09:32.640
Let's dive in and talk about this general category first of data pipelines, data orchestration,

00:09:32.640 --> 00:09:33.360
all those things.

00:09:33.360 --> 00:09:36.380
And we'll talk about Dagster and some of the trends and that.

00:09:36.380 --> 00:09:41.580
So let's grab some random internet search for what does a data pipeline maybe look like?

00:09:41.580 --> 00:09:45.800
But, you know, people out there listening who don't necessarily live in that space, which

00:09:45.800 --> 00:09:50.980
I think is honestly many of us, maybe we should, but maybe in our minds, we don't think we live

00:09:50.980 --> 00:09:52.420
in data pipeline land.

00:09:52.420 --> 00:09:53.620
Tell them about it.

00:09:53.620 --> 00:09:54.140
Yeah, for sure.

00:09:54.140 --> 00:09:57.080
It is hard to think about if you haven't done or built one before.

00:09:57.080 --> 00:10:02.820
In many ways, a data pipeline is just a series of steps that you apply to some data set that

00:10:02.820 --> 00:10:08.140
you have in order to transform it to something a little bit more valuable at the very end.

00:10:08.140 --> 00:10:09.480
That's a simplified version.

00:10:09.480 --> 00:10:10.780
The devil's in the details.

00:10:10.780 --> 00:10:13.560
But really, like at the end of the day, you're in a business.

00:10:13.560 --> 00:10:17.380
The production of data sort of happens by the very nature of operating that business.

00:10:17.380 --> 00:10:21.160
It tends to be the core thing that, you know, all businesses have in common.

00:10:21.160 --> 00:10:25.180
And then the other sort of output is you have people within that business who are trying

00:10:25.180 --> 00:10:27.020
to understand how the business is operating.

00:10:27.020 --> 00:10:31.380
And this used to be easy when all we had was a single spreadsheet that we could look at once

00:10:31.380 --> 00:10:31.700
a month.

00:10:31.700 --> 00:10:32.260
Yeah.

00:10:32.260 --> 00:10:35.440
I think businesses have gone a little bit more complex than these days.

00:10:35.440 --> 00:10:36.120
Computers.

00:10:36.120 --> 00:10:36.960
And the expectations.

00:10:36.960 --> 00:10:41.560
Like they expect to be able to see almost real time, not I'll see it at the end of the month.

00:10:41.560 --> 00:10:42.220
Sort of.

00:10:42.220 --> 00:10:42.600
That's right.

00:10:42.600 --> 00:10:43.280
Yeah.

00:10:43.600 --> 00:10:47.320
I think people have gotten used to getting data too, which is both good and bad.

00:10:47.320 --> 00:10:49.540
Good in the sense that now people are making better decisions.

00:10:49.540 --> 00:10:50.200
Bad.

00:10:50.200 --> 00:10:51.680
And then there's more work for us to do.

00:10:51.680 --> 00:10:55.320
And we can't just sit on our feet for half a day, half a month and waiting for the next

00:10:55.320 --> 00:10:56.040
request to come in.

00:10:56.040 --> 00:10:58.840
There's just an endless stream that seems to never end.

00:10:58.840 --> 00:11:00.660
So that's what really Pipeline is all about.

00:11:00.660 --> 00:11:06.920
It's like taking these data and making it consumable in a way that users, tools will understand.

00:11:06.920 --> 00:11:09.200
That helps people make decisions at the very end of the day.

00:11:09.380 --> 00:11:10.880
That's sort of the nuts and bolts of it.

00:11:10.880 --> 00:11:14.440
In your mind, does data acquisition live in this land?

00:11:14.440 --> 00:11:19.980
So for example, maybe we have a scheduled job that goes and does web scraping, calls an

00:11:19.980 --> 00:11:24.180
API once an hour, and that might kick off a whole pipeline of processing.

00:11:24.180 --> 00:11:32.140
Or we watch a folder for people to upload over FTP, like a CSV file or something horrible

00:11:32.140 --> 00:11:32.540
like that.

00:11:32.540 --> 00:11:33.920
You don't even know what it's unspeakable.

00:11:33.920 --> 00:11:38.320
But something like that where you say, oh, a new CSV has arrived for me to get, right?

00:11:38.320 --> 00:11:38.660
Yeah.

00:11:38.780 --> 00:11:42.580
I think that's the beginning of all data pipeline journeys in my mind.

00:11:42.580 --> 00:11:43.460
Very much, right?

00:11:43.460 --> 00:11:46.440
Like an FTP, as much as we hate it, it's not terrible.

00:11:46.440 --> 00:11:52.740
I mean, there are worse ways to transfer files, but I think it's still very much in use today.

00:11:52.740 --> 00:11:57.900
And every data pipeline journey at some point has to begin with that consumption of data from

00:11:57.900 --> 00:11:58.220
somewhere.

00:11:58.220 --> 00:11:58.480
Yeah.

00:11:58.480 --> 00:12:02.960
Hopefully it's SFTP, not just straight FTP, like the encrypted.

00:12:02.960 --> 00:12:06.040
Don't just send your password in the plain text.

00:12:06.040 --> 00:12:06.760
Oh, well.

00:12:07.620 --> 00:12:09.180
I've seen that go wrong.

00:12:09.180 --> 00:12:10.860
That's a story for another day, honestly.

00:12:10.860 --> 00:12:12.380
All right.

00:12:12.380 --> 00:12:14.720
Well, let's talk about the project that you work on.

00:12:14.720 --> 00:12:17.900
We've been talking about it in general, but let's talk about Dagster.

00:12:17.900 --> 00:12:19.540
Like, where does it fit in this world?

00:12:19.540 --> 00:12:20.220
Yes.

00:12:20.220 --> 00:12:24.000
Dagster, to me, is a way to build a data platform.

00:12:24.000 --> 00:12:28.060
It's also a different way of thinking about how you build data pipelines.

00:12:28.460 --> 00:12:32.420
Maybe it's good to compare it with kind of what the world was like, I think, before Dagster

00:12:32.420 --> 00:12:34.540
and how it came about to be.

00:12:34.540 --> 00:12:39.540
So if you think of Airflow, I think it's probably the most canonical orchestrator out there.

00:12:39.540 --> 00:12:43.520
But there are other ways which people used to orchestrate these data pipelines.

00:12:43.520 --> 00:12:45.440
They were often task-based, right?

00:12:45.440 --> 00:12:47.500
Like, I would download file.

00:12:47.500 --> 00:12:48.860
I would unzip file.

00:12:48.860 --> 00:12:50.120
I would upload file.

00:12:50.120 --> 00:12:55.660
These are sort of the words we use to describe the various steps within a pipeline.

00:12:55.660 --> 00:12:58.840
Some of those little steps might be Python functions that you write.

00:12:58.840 --> 00:13:00.660
Maybe there's some pre-built other ones.

00:13:00.660 --> 00:13:00.960
Yeah.

00:13:00.960 --> 00:13:01.920
There might be Python.

00:13:01.920 --> 00:13:03.160
Could be a bash script.

00:13:03.160 --> 00:13:05.440
Could be logging into a server and downloading a file.

00:13:05.440 --> 00:13:09.020
Could be hitting requests to download something from the internet and zipping it.

00:13:09.020 --> 00:13:11.680
Just a various, you know, hodgepodge of commands that would run.

00:13:11.680 --> 00:13:13.620
That's typically how we thought about it.

00:13:13.620 --> 00:13:15.780
For more complex scenarios where your data is bigger,

00:13:15.920 --> 00:13:18.800
maybe it's running against a Hadoop cluster or a Spark cluster.

00:13:18.800 --> 00:13:20.840
The compute's been offloaded somewhere else.

00:13:20.840 --> 00:13:25.620
But the sort of conceptual way you tended to think about these things is in terms of tasks, right?

00:13:25.620 --> 00:13:29.880
Process this thing, do this massive data dump, run a bunch of things,

00:13:29.880 --> 00:13:31.760
and then your job is complete.

00:13:31.760 --> 00:13:36.160
With Airflow, or sorry, with Dagster, we kind of flip it around a little bit on our heads.

00:13:36.160 --> 00:13:38.420
And we say, instead of thinking about tasks,

00:13:38.420 --> 00:13:43.280
what if we flipped that around and thought about the actual underlying assets that you're creating?

00:13:43.680 --> 00:13:46.780
What if you told us not, you know, the steps that you're going to take,

00:13:46.780 --> 00:13:48.320
but the thing that you produce?

00:13:48.320 --> 00:13:52.000
Because it turns out as people and as data people and stakeholders,

00:13:52.000 --> 00:13:54.240
really, we don't care about the task.

00:13:54.240 --> 00:13:56.140
Like, we just assume that you're going to do it.

00:13:56.140 --> 00:14:01.180
What we care about is, you know, that table, that model, that file, that Jupyter notebook.

00:14:01.180 --> 00:14:04.360
And if we model our pipeline through that,

00:14:04.360 --> 00:14:06.160
then we get a whole bunch of other benefits.

00:14:06.160 --> 00:14:08.820
And that's sort of the Dagster sort of pitch, right?

00:14:08.980 --> 00:14:13.300
Like, if you want to understand the things that are being produced by these tasks,

00:14:13.300 --> 00:14:15.120
tell us about the underlying assets.

00:14:15.120 --> 00:14:17.700
And then when a stakeholder says and comes to you and says, you know,

00:14:17.700 --> 00:14:19.480
how old is this table?

00:14:19.480 --> 00:14:20.620
Has it been refreshed lately?

00:14:20.620 --> 00:14:22.640
Well, you don't have to go look at a specific task.

00:14:22.640 --> 00:14:25.820
And remember that task ABC had model XYZ.

00:14:25.820 --> 00:14:29.060
You just go and look up model XYZ directly there, and it's there for you.

00:14:29.060 --> 00:14:31.020
And because you've defined things in this way,

00:14:31.020 --> 00:14:33.420
you get other nice things like a lineage graph.

00:14:33.600 --> 00:14:36.220
You get to understand how fresh your data is.

00:14:36.220 --> 00:14:38.740
You can do event-based orchestration and all kinds of nice things

00:14:38.740 --> 00:14:40.840
that are a lot harder to do in a task world.

00:14:40.840 --> 00:14:44.980
Yeah, more declarative, less imperative, I suppose.

00:14:44.980 --> 00:14:48.200
Yeah, it's been the trend, I think, in lots of tooling.

00:14:48.200 --> 00:14:50.640
React, I think, was famous for this as well, right?

00:14:50.640 --> 00:14:51.540
In many ways.

00:14:51.540 --> 00:14:51.780
Yeah.

00:14:51.780 --> 00:14:54.340
It was a hard framework, I think, for people to sort of

00:14:54.340 --> 00:14:55.560
get their heads around initially,

00:14:55.560 --> 00:14:58.600
because we were so used to like the jQuery declarative,

00:14:58.600 --> 00:15:01.000
or jQuery style of doing things.

00:15:01.000 --> 00:15:03.200
Yeah, how do I hook the event that makes the thing happen?

00:15:03.420 --> 00:15:03.560
Right?

00:15:03.560 --> 00:15:05.900
And React said, let's think about it a little bit differently.

00:15:05.900 --> 00:15:08.020
Let's do this event-based orchestration, really.

00:15:08.020 --> 00:15:11.200
And I think the proof's in the pudding, React's everywhere now,

00:15:11.200 --> 00:15:12.480
and jQuery could be not so much.

00:15:12.480 --> 00:15:13.260
Yeah.

00:15:13.260 --> 00:15:15.000
There's still a lot of jQuery out there,

00:15:15.000 --> 00:15:18.140
but there's not a lot of active jQuery.

00:15:18.140 --> 00:15:19.640
But I imagine there's some.

00:15:19.640 --> 00:15:20.760
There is, there is, yes.

00:15:20.760 --> 00:15:22.140
Yeah, just because people are like,

00:15:22.140 --> 00:15:23.700
you know what, don't touch that, that works.

00:15:23.700 --> 00:15:26.740
Which is probably the smartest thing people can do, I think.

00:15:26.740 --> 00:15:27.600
Yeah, honestly.

00:15:27.600 --> 00:15:29.640
Even though new frameworks are shiny.

00:15:29.640 --> 00:15:32.120
And, you know, if there's any ecosystem

00:15:32.120 --> 00:15:34.600
that loves to chase the shiny new idea,

00:15:34.600 --> 00:15:36.320
it's the JavaScript web world.

00:15:36.320 --> 00:15:36.740
Oh, yeah.

00:15:36.740 --> 00:15:39.360
There's no shortage of new frameworks coming out every time.

00:15:39.360 --> 00:15:40.760
Yeah, I mean, we do too,

00:15:40.760 --> 00:15:42.720
but not as much as like,

00:15:42.720 --> 00:15:43.840
that's six months old.

00:15:43.840 --> 00:15:46.020
That's so old, we can't possibly do that anymore.

00:15:46.020 --> 00:15:46.860
We're rewriting it.

00:15:46.860 --> 00:15:48.340
We're going to do the big rewrite again.

00:15:48.340 --> 00:15:48.660
Mm-hmm.

00:15:48.660 --> 00:15:49.140
Yep.

00:15:49.140 --> 00:15:49.340
Fun.

00:15:49.700 --> 00:15:52.140
Okay, so Dagster is the company,

00:15:52.140 --> 00:15:53.680
but also is open source.

00:15:53.680 --> 00:15:56.100
What's the story around, like,

00:15:56.100 --> 00:15:57.140
can I use it for free?

00:15:57.140 --> 00:15:57.860
Is it open source?

00:15:57.860 --> 00:15:58.540
Do I pay for it?

00:15:58.540 --> 00:15:58.860
100%.

00:15:58.860 --> 00:15:59.020
Okay.

00:15:59.020 --> 00:16:00.560
So Dagster Labs is the company.

00:16:00.560 --> 00:16:03.280
Dagster open source is the product.

00:16:03.280 --> 00:16:04.360
The 100% free,

00:16:04.360 --> 00:16:06.400
we're very committed to the open source model.

00:16:06.400 --> 00:16:09.680
I would say 95% of the things you can get out of Dagster

00:16:09.680 --> 00:16:11.300
are available through open source.

00:16:11.300 --> 00:16:13.720
And we tend to try to release everything through that model.

00:16:14.440 --> 00:16:16.500
You can run very complex pipelines

00:16:16.500 --> 00:16:19.260
and you can deploy it all on your own if you wish.

00:16:19.260 --> 00:16:20.720
There is a Dagster Cloud product,

00:16:20.720 --> 00:16:22.860
which is really the hosted version of Dagster.

00:16:22.860 --> 00:16:24.780
If you want hosted plain,

00:16:24.780 --> 00:16:26.460
we can do that for you through Dagster Cloud,

00:16:26.460 --> 00:16:28.480
but it all runs on the same code base

00:16:28.480 --> 00:16:29.900
and the modeling and the files

00:16:29.900 --> 00:16:31.220
all essentially look the same.

00:16:31.220 --> 00:16:31.580
Mm-hmm.

00:16:31.580 --> 00:16:32.180
Okay.

00:16:32.180 --> 00:16:34.200
So obviously you could get,

00:16:34.200 --> 00:16:35.440
like I talked about at the beginning,

00:16:35.440 --> 00:16:36.940
you could go down the DevOps side,

00:16:36.940 --> 00:16:39.540
get your own open source Dagster set up,

00:16:39.540 --> 00:16:41.340
schedule it, run it on servers,

00:16:41.340 --> 00:16:41.960
all those things.

00:16:41.960 --> 00:16:44.340
But if we just wanted something real simple,

00:16:44.680 --> 00:16:46.500
we could just go to you guys and say,

00:16:46.500 --> 00:16:47.880
hey, I built this with Dagster.

00:16:47.880 --> 00:16:49.080
Will you run it for me?

00:16:49.080 --> 00:16:49.520
Pretty much.

00:16:49.520 --> 00:16:49.720
Yeah.

00:16:49.720 --> 00:16:50.080
Right.

00:16:50.080 --> 00:16:51.100
So there's two options there.

00:16:51.100 --> 00:16:53.100
You can do the serverless model,

00:16:53.100 --> 00:16:54.980
which says, you know, Dagster, just run it.

00:16:54.980 --> 00:16:56.160
We take care of the compute,

00:16:56.160 --> 00:16:57.620
we take care of the execution for you,

00:16:57.620 --> 00:16:58.960
and you just write the code

00:16:58.960 --> 00:17:00.080
and upload it to GitHub

00:17:00.080 --> 00:17:02.820
or any, you know, repository of your choice

00:17:02.820 --> 00:17:04.540
and we'll sync to that and then run it.

00:17:04.540 --> 00:17:06.280
The other option is to do the hybrid model.

00:17:06.280 --> 00:17:08.940
So you basically do the CICD aspect.

00:17:08.940 --> 00:17:11.880
You just say, you push to name your branch.

00:17:11.880 --> 00:17:13.040
If you push to that branch,

00:17:13.080 --> 00:17:15.080
that means we're just going to deploy a new version

00:17:15.080 --> 00:17:16.780
and whatever happens after that,

00:17:16.780 --> 00:17:18.260
it'll be in production, right?

00:17:18.260 --> 00:17:18.660
Exactly.

00:17:18.660 --> 00:17:18.940
Yeah.

00:17:18.940 --> 00:17:21.140
And we offer some templates that you can use

00:17:21.140 --> 00:17:22.400
in GitHub for workflows

00:17:22.400 --> 00:17:23.920
in order to accommodate that.

00:17:23.920 --> 00:17:24.660
Excellent.

00:17:24.660 --> 00:17:25.940
Then I cut you off.

00:17:25.940 --> 00:17:27.020
You're saying something about hybrid.

00:17:27.020 --> 00:17:28.320
Hybrid is the other option.

00:17:28.420 --> 00:17:30.440
For those of you who want to run your own compute,

00:17:30.440 --> 00:17:32.160
you don't want the data leaving your ecosystem,

00:17:32.160 --> 00:17:35.180
you can say, we've got this Kubernetes cluster,

00:17:35.180 --> 00:17:36.280
this ECS cluster,

00:17:36.280 --> 00:17:38.220
but we still want to use the Dagster Cloud product

00:17:38.220 --> 00:17:40.140
to sort of manage the control plane.

00:17:40.140 --> 00:17:41.400
Dagster Cloud will do that.

00:17:41.400 --> 00:17:42.200
And then you can go off

00:17:42.200 --> 00:17:44.180
and execute things on your own environment

00:17:44.180 --> 00:17:45.420
if that's something you wish to do.

00:17:45.420 --> 00:17:46.380
Oh yeah, that's pretty clever

00:17:46.380 --> 00:17:49.100
because running stuff in containers isn't too bad,

00:17:49.100 --> 00:17:50.840
but running container clusters,

00:17:50.840 --> 00:17:52.160
all of a sudden,

00:17:52.160 --> 00:17:54.620
you're back doing a lot of work, right?

00:17:54.840 --> 00:17:55.540
Exactly, yeah.

00:17:55.540 --> 00:17:55.780
Yeah.

00:17:55.780 --> 00:17:58.660
Okay, well, let's maybe talk about Dagster for a bit

00:17:58.660 --> 00:18:01.140
that I want to talk about some of the trends as well,

00:18:01.140 --> 00:18:03.560
but let's just talk through maybe setting up a pipeline.

00:18:03.560 --> 00:18:05.220
Like, what does it look like?

00:18:05.220 --> 00:18:06.540
You know, you talked about in general,

00:18:06.540 --> 00:18:08.420
less imperative, more declarative,

00:18:08.420 --> 00:18:10.140
but what does it look like?

00:18:10.140 --> 00:18:12.260
Be careful about talking about code on audio,

00:18:12.260 --> 00:18:14.420
but you know, just give us a sense

00:18:14.420 --> 00:18:16.640
of what the programming model feels like for us.

00:18:16.640 --> 00:18:17.860
As much as possible,

00:18:17.860 --> 00:18:19.440
it really feels like just writing Python.

00:18:19.440 --> 00:18:21.060
It's pretty easy.

00:18:21.060 --> 00:18:24.560
You add a decorator on top of your existing Python function

00:18:24.560 --> 00:18:25.400
that does something.

00:18:25.400 --> 00:18:27.960
That's a simple decorator called asset.

00:18:27.960 --> 00:18:29.540
And then your pipeline,

00:18:29.540 --> 00:18:31.380
that function becomes a data asset.

00:18:31.380 --> 00:18:33.580
That's how it's represented in the Dagster UI.

00:18:33.580 --> 00:18:35.820
So you could imagine you've got a pipeline

00:18:35.820 --> 00:18:38.480
that gets like maybe Slack analytics

00:18:38.480 --> 00:18:41.160
and uploads that to some dashboard, right?

00:18:41.160 --> 00:18:42.220
Your first pipeline,

00:18:42.220 --> 00:18:44.680
your function would be called something like Slack data,

00:18:44.680 --> 00:18:45.960
and that would be your asset.

00:18:45.960 --> 00:18:48.860
In that function is where you do all the transform,

00:18:48.860 --> 00:18:50.040
the downloading of the data

00:18:50.040 --> 00:18:52.880
until you've really created that fundamental data asset

00:18:52.880 --> 00:18:53.600
that you care about.

00:18:53.600 --> 00:18:55.040
And that could be stored either,

00:18:55.040 --> 00:18:55.480
you know,

00:18:55.480 --> 00:18:55.960
you know,

00:18:55.960 --> 00:18:57.520
data warehouse to F3,

00:18:57.520 --> 00:18:59.120
however you sort of want to persist it,

00:18:59.120 --> 00:18:59.880
that's really up to you.

00:18:59.880 --> 00:19:02.740
And then the resources is sort of where the power,

00:19:02.740 --> 00:19:03.120
I think,

00:19:03.120 --> 00:19:04.520
of a lot of Dagster comes in.

00:19:04.520 --> 00:19:06.360
So the asset is sort of like declaration

00:19:06.360 --> 00:19:07.600
of the thing I'm going to create.

00:19:07.600 --> 00:19:10.280
The resource is how I'm going to,

00:19:10.280 --> 00:19:10.720
you know,

00:19:10.720 --> 00:19:11.460
operate on that,

00:19:11.460 --> 00:19:11.680
right?

00:19:11.680 --> 00:19:14.220
Because sometimes you might want to have a,

00:19:14.220 --> 00:19:15.040
let's say,

00:19:15.040 --> 00:19:16.620
a DuckDB instance locally

00:19:16.620 --> 00:19:18.480
because it's easier and faster to operate.

00:19:18.740 --> 00:19:19.880
But when you're moving to the cloud,

00:19:19.880 --> 00:19:22.780
you want to have a Databricks or a Snowflake.

00:19:22.780 --> 00:19:24.800
You can swap out resources based on environments

00:19:24.800 --> 00:19:27.560
and your asset can reference that resource.

00:19:27.560 --> 00:19:29.940
And as long as it has that same sort of API,

00:19:29.940 --> 00:19:32.300
you can really flexibly change between

00:19:32.300 --> 00:19:33.920
where that data is going to be persistent.

00:19:34.180 --> 00:19:36.680
Does Dagster know how to talk to those different platforms?

00:19:36.680 --> 00:19:40.200
Does it like natively understand DuckDB and Snowflake?

00:19:40.200 --> 00:19:40.540
Yeah.

00:19:40.540 --> 00:19:41.620
So it's interesting.

00:19:41.620 --> 00:19:43.240
People often look to Dagster and like,

00:19:43.240 --> 00:19:43.500
oh,

00:19:43.500 --> 00:19:44.680
does it do X?

00:19:44.680 --> 00:19:46.000
And the question is like,

00:19:46.000 --> 00:19:48.380
Dagster does anything you can do Python with.

00:19:48.380 --> 00:19:49.620
Which is most things.

00:19:49.620 --> 00:19:49.800
Yeah.

00:19:49.800 --> 00:19:50.400
Which is most things.

00:19:50.400 --> 00:19:52.420
So I think if you come from the Airflow world,

00:19:52.420 --> 00:19:54.680
you're very much used to like these Airflow providers.

00:19:54.680 --> 00:19:55.680
And if you want to use...

00:19:55.680 --> 00:19:56.340
That's kind of what I was thinking.

00:19:56.340 --> 00:19:56.600
Yeah.

00:19:56.600 --> 00:19:57.020
Yeah.

00:19:57.020 --> 00:19:58.040
You want to use a Postgres,

00:19:58.040 --> 00:19:59.600
you need to find the Postgres provider.

00:19:59.600 --> 00:20:00.360
You want to use S3,

00:20:00.360 --> 00:20:01.580
you need to find the S3 provider.

00:20:01.580 --> 00:20:02.240
With Dagster,

00:20:02.240 --> 00:20:03.440
you kind of say,

00:20:03.560 --> 00:20:04.380
you don't have to do any of that.

00:20:04.380 --> 00:20:05.540
If you want to use Snowflake,

00:20:05.540 --> 00:20:06.000
for example,

00:20:06.000 --> 00:20:09.100
install the Snowflake connector package from Snowflake,

00:20:09.100 --> 00:20:10.860
and you use that as a resource directly.

00:20:10.860 --> 00:20:13.420
And then you just run your SQL that way.

00:20:13.420 --> 00:20:17.040
There are some places where we do have integrations that help.

00:20:17.040 --> 00:20:18.900
If you want to get into the weeds with IO manager,

00:20:18.900 --> 00:20:21.140
it's where we persist the data on your behalf.

00:20:21.140 --> 00:20:22.520
And so for S3,

00:20:22.520 --> 00:20:23.260
for Snowflake,

00:20:23.260 --> 00:20:23.820
for example,

00:20:23.820 --> 00:20:26.800
there's other ways where we can persist that data for you.

00:20:26.800 --> 00:20:28.820
But if you're just trying to run a query,

00:20:28.820 --> 00:20:30.180
just trying to execute something,

00:20:30.180 --> 00:20:31.640
just trying to save something somewhere,

00:20:31.640 --> 00:20:33.280
you don't have to use that system at all.

00:20:33.440 --> 00:20:37.980
You can just use whatever Python package you would use anyway to do that.

00:20:37.980 --> 00:20:42.400
So maybe some data is expensive for us to get as a company.

00:20:42.400 --> 00:20:46.240
Like maybe we're charged on a usage basis or super slow or something.

00:20:46.240 --> 00:20:48.660
I could write just Python code that goes and say,

00:20:48.660 --> 00:20:48.920
well,

00:20:48.920 --> 00:20:50.460
look in my local database.

00:20:50.460 --> 00:20:51.480
If it's already there,

00:20:51.480 --> 00:20:52.780
use that and it's not too stale.

00:20:52.780 --> 00:20:53.180
Otherwise,

00:20:53.180 --> 00:20:54.780
then do actually go get it,

00:20:54.780 --> 00:20:55.340
put it,

00:20:55.340 --> 00:20:56.060
put it there,

00:20:56.140 --> 00:20:57.320
and then get it back.

00:20:57.320 --> 00:21:00.680
And like that kind of stuff would be up to me to put together.

00:21:00.680 --> 00:21:01.080
Yeah.

00:21:01.080 --> 00:21:05.320
And that's the nice thing is you're not really limited by like anyone's data model

00:21:05.320 --> 00:21:09.220
or world view on how data should be retrieved or saved or augmented.

00:21:09.220 --> 00:21:10.460
You could do it a couple of ways.

00:21:10.460 --> 00:21:11.100
You could say,

00:21:11.100 --> 00:21:12.340
whenever I'm working locally,

00:21:12.340 --> 00:21:17.540
use this persistent data store that we're just going to use for development purposes.

00:21:17.540 --> 00:21:19.480
Fancy database called SQLite,

00:21:19.480 --> 00:21:19.980
something like that.

00:21:20.060 --> 00:21:20.420
Exactly.

00:21:20.420 --> 00:21:20.820
Yes.

00:21:20.820 --> 00:21:21.760
A wonderful database.

00:21:21.760 --> 00:21:22.980
And actually it's,

00:21:22.980 --> 00:21:23.160
yeah,

00:21:23.160 --> 00:21:24.120
it'll work really,

00:21:24.120 --> 00:21:24.540
really well.

00:21:24.540 --> 00:21:25.740
And then you just say,

00:21:25.740 --> 00:21:26.700
when I'm in a different environment,

00:21:26.700 --> 00:21:27.440
when I'm in production,

00:21:27.440 --> 00:21:30.740
swap out my SQLite resource for a name,

00:21:30.740 --> 00:21:33.400
your favorite cloud warehouse resource and go fit,

00:21:33.400 --> 00:21:34.380
fetch that data from there.

00:21:34.380 --> 00:21:36.740
Or I want to use it mini IO locally.

00:21:36.740 --> 00:21:38.680
I want to use S3 on prod.

00:21:38.680 --> 00:21:40.500
It's very simple to swap these things out.

00:21:40.500 --> 00:21:40.780
Okay.

00:21:40.780 --> 00:21:41.280
Yeah.

00:21:41.280 --> 00:21:44.600
So it looks like you build up these assets as y'all call them,

00:21:44.600 --> 00:21:46.120
these pieces of data,

00:21:46.120 --> 00:21:47.780
Python code that accesses them.

00:21:48.160 --> 00:21:54.060
And then you have a nice UI that lets you go and build those out kind of workflow style,

00:21:54.060 --> 00:21:54.320
right?

00:21:54.320 --> 00:21:54.660
Yeah,

00:21:54.660 --> 00:21:55.120
exactly.

00:21:55.120 --> 00:21:57.460
This is where we get into the wonderful world of DAGs,

00:21:57.460 --> 00:22:00.700
which stands for directed acyclic graph.

00:22:00.700 --> 00:22:04.460
I think it stands for a bunch of things that are not connected in a circle,

00:22:04.460 --> 00:22:05.740
but are connected in some way.

00:22:05.740 --> 00:22:06.840
So there can't be any loops,

00:22:06.840 --> 00:22:07.260
right?

00:22:07.260 --> 00:22:08.840
Because then you never know where to start or where to end.

00:22:08.840 --> 00:22:09.660
Could be a diamond,

00:22:09.660 --> 00:22:11.460
but not a circle,

00:22:11.460 --> 00:22:11.920
right?

00:22:11.920 --> 00:22:12.560
Not a circle.

00:22:12.560 --> 00:22:17.380
As long as there's like a path through this data set with the beginning and an end,

00:22:17.660 --> 00:22:21.620
then we can kind of start to model this connected graph of things.

00:22:21.620 --> 00:22:22.940
And then we know how to execute them,

00:22:22.940 --> 00:22:23.100
right?

00:22:23.100 --> 00:22:23.600
We can say,

00:22:23.600 --> 00:22:23.980
well,

00:22:23.980 --> 00:22:26.980
this is the first thing we have to run because that's where all dependencies start.

00:22:26.980 --> 00:22:31.220
And then we can either branch off in parallel or we can continue linearly until everything

00:22:31.220 --> 00:22:31.720
is complete.

00:22:31.720 --> 00:22:33.340
And if something breaks in the middle,

00:22:33.340 --> 00:22:35.100
we can resume from that broken spot.

00:22:35.100 --> 00:22:35.340
Okay.

00:22:35.340 --> 00:22:35.960
Excellent.

00:22:36.340 --> 00:22:38.140
And is that the recommended way?

00:22:38.140 --> 00:22:41.120
Like if I write all this Python code that works on the pieces,

00:22:41.120 --> 00:22:45.440
then the next recommendation would be to fire up the UI and start building it?

00:22:45.440 --> 00:22:46.340
Or do you say,

00:22:46.340 --> 00:22:46.620
ah,

00:22:46.620 --> 00:22:50.240
you should really write it in code and then you can just visualize it or monitor it?

00:22:50.240 --> 00:22:51.840
Everything in Dagster is written as code.

00:22:51.840 --> 00:22:57.040
The UI reads that code and it interprets it as a DAG and then it displays that for you.

00:22:57.040 --> 00:22:58.600
There are some things to do with the UI.

00:22:58.720 --> 00:23:00.120
Like you can materialize assets,

00:23:00.120 --> 00:23:01.220
you can make them run,

00:23:01.220 --> 00:23:02.540
you can do backfills,

00:23:02.540 --> 00:23:03.760
you can view metadata,

00:23:03.760 --> 00:23:06.700
you can sort of enable and disable schedules.

00:23:06.700 --> 00:23:07.760
But the core,

00:23:07.760 --> 00:23:08.880
we really believe this is Dagster,

00:23:08.880 --> 00:23:11.580
like the core declaration of how things are done,

00:23:11.580 --> 00:23:13.140
it's always done through code.

00:23:13.140 --> 00:23:13.400
Okay.

00:23:13.400 --> 00:23:13.960
Excellent.

00:23:13.960 --> 00:23:15.860
So when you say materialize,

00:23:15.860 --> 00:23:17.700
maybe I have an asset,

00:23:17.700 --> 00:23:22.100
which is really a Python function I wrote that goes and pulls down a CSV file.

00:23:22.100 --> 00:23:23.800
The materialize would be,

00:23:23.800 --> 00:23:26.900
I want to see kind of representative data in this,

00:23:26.900 --> 00:23:28.080
in the UI.

00:23:28.280 --> 00:23:29.160
so I could go,

00:23:29.160 --> 00:23:29.600
all right,

00:23:29.600 --> 00:23:30.380
I think this is right.

00:23:30.380 --> 00:23:31.460
Let's keep passing it down.

00:23:31.460 --> 00:23:32.440
Is that what that means?

00:23:32.440 --> 00:23:34.980
Materialize really means just run this particular asset,

00:23:34.980 --> 00:23:36.760
make this asset new again,

00:23:36.760 --> 00:23:37.360
fresh again,

00:23:37.360 --> 00:23:37.760
right?

00:23:37.760 --> 00:23:39.600
As part of that materialization,

00:23:39.600 --> 00:23:41.160
we sometimes output metadata.

00:23:41.160 --> 00:23:42.740
And you can kind of see this on the right.

00:23:42.740 --> 00:23:44.440
If you're looking at the screen here,

00:23:44.440 --> 00:23:46.880
where we talk about what the timestamp was,

00:23:46.880 --> 00:23:47.620
the URL,

00:23:47.620 --> 00:23:51.180
there's a nice little graph of like number of rows over time.

00:23:51.180 --> 00:23:53.880
All that metadata is something you can emit.

00:23:53.880 --> 00:23:56.900
And we emit some ourselves by default with the framework.

00:23:56.900 --> 00:23:58.620
And then as you materialize these assets,

00:23:58.620 --> 00:24:00.660
as you run that asset over and over again,

00:24:00.660 --> 00:24:01.060
over time,

00:24:01.060 --> 00:24:01.800
we capture all that.

00:24:01.800 --> 00:24:04.260
And then you can really get a nice overview of,

00:24:04.260 --> 00:24:04.600
you know,

00:24:04.600 --> 00:24:05.860
this asset's lifetime,

00:24:05.860 --> 00:24:06.260
essentially.

00:24:06.260 --> 00:24:06.680
Nice.

00:24:06.680 --> 00:24:07.480
Yeah.

00:24:07.480 --> 00:24:08.140
I think the asset,

00:24:08.140 --> 00:24:10.320
the metadata is really pretty excellent,

00:24:10.320 --> 00:24:10.560
right?

00:24:10.560 --> 00:24:11.280
Over time,

00:24:11.280 --> 00:24:13.960
you can see how the data's grown and changed.

00:24:14.220 --> 00:24:14.400
Yeah.

00:24:14.400 --> 00:24:15.120
The metadata is,

00:24:15.120 --> 00:24:16.200
is really powerful.

00:24:16.200 --> 00:24:19.240
And it's one of the nice benefits of being in this asset world,

00:24:19.240 --> 00:24:19.440
right?

00:24:19.440 --> 00:24:22.420
Because you don't really want to metadata on like this task that run.

00:24:22.420 --> 00:24:24.860
You want to know like this table that I created,

00:24:24.860 --> 00:24:27.440
how many rows has it had every single time it's run?

00:24:27.440 --> 00:24:29.720
Because if that number drops by like 50%,

00:24:29.720 --> 00:24:30.920
that's a big problem.

00:24:30.920 --> 00:24:31.640
Conversely,

00:24:31.640 --> 00:24:34.300
if the runtime is slowly increasing every single day,

00:24:34.300 --> 00:24:35.260
you might not notice it,

00:24:35.260 --> 00:24:36.620
but over a month or two,

00:24:36.620 --> 00:24:39.420
it went from a 30 second pipeline to 30 minutes.

00:24:39.820 --> 00:24:43.340
maybe there's like a great place to start optimizing that one specific asset.

00:24:43.340 --> 00:24:43.780
Right.

00:24:43.780 --> 00:24:45.960
And what's cool if it's just Python code,

00:24:45.960 --> 00:24:47.620
you know how to optimize that probably,

00:24:47.620 --> 00:24:47.860
right?

00:24:47.860 --> 00:24:48.140
Hopefully.

00:24:48.140 --> 00:24:48.380
Yeah.

00:24:48.380 --> 00:24:49.980
Well,

00:24:49.980 --> 00:24:51.080
as much as you're going to,

00:24:51.080 --> 00:24:51.360
yeah,

00:24:51.360 --> 00:24:51.840
you got,

00:24:51.840 --> 00:24:55.140
you have all the power of Python and you should be able to,

00:24:55.140 --> 00:24:57.920
as opposed to it's deep down inside some framework that you don't really.

00:24:57.920 --> 00:24:58.300
Exactly.

00:24:58.300 --> 00:24:58.960
Yeah.

00:24:58.960 --> 00:24:59.700
You use Python,

00:24:59.700 --> 00:25:00.840
you can benchmark it.

00:25:00.840 --> 00:25:01.440
There's probably,

00:25:01.440 --> 00:25:05.500
you probably knew you didn't write it that well when you first started and you can

00:25:05.500 --> 00:25:07.020
always find ways to improve it.

00:25:07.020 --> 00:25:09.780
So this UI is something that you can just run locally.

00:25:09.780 --> 00:25:10.740
Kind of like Jupyter.

00:25:10.740 --> 00:25:11.440
A hundred percent.

00:25:11.440 --> 00:25:15.980
Just type Degester dev and then you get the full UI experience.

00:25:15.980 --> 00:25:16.960
You get to see the runs,

00:25:16.960 --> 00:25:17.760
all your assets.

00:25:17.760 --> 00:25:18.580
Is it a web app?

00:25:18.580 --> 00:25:19.020
It is.

00:25:19.020 --> 00:25:19.300
Yeah.

00:25:19.300 --> 00:25:20.340
It's a web app.

00:25:20.340 --> 00:25:21.580
There's a Postgres backend.

00:25:21.580 --> 00:25:24.320
And then there's a couple of services that run the web server,

00:25:24.320 --> 00:25:25.040
the GraphQL,

00:25:25.040 --> 00:25:26.260
and then the workers.

00:25:26.260 --> 00:25:26.660
Nice.

00:25:26.660 --> 00:25:27.060
Yeah.

00:25:27.060 --> 00:25:28.360
So pretty serious web app.

00:25:28.360 --> 00:25:28.900
It sounds like,

00:25:28.900 --> 00:25:30.620
but yeah,

00:25:30.620 --> 00:25:36.360
just something you run all probably containers or something.

00:25:36.360 --> 00:25:38.100
You just fire up when you download it.

00:25:38.100 --> 00:25:38.240
Right.

00:25:38.360 --> 00:25:39.760
Locally it doesn't even use containers.

00:25:39.760 --> 00:25:42.500
It's just all pure Python for that.

00:25:42.500 --> 00:25:43.860
But once you deploy,

00:25:43.860 --> 00:25:44.060
yeah,

00:25:44.060 --> 00:25:46.000
I think you might want to go down the container route,

00:25:46.000 --> 00:25:50.020
but it's nice not having to have Docker just to like run a simple test deployment.

00:25:50.020 --> 00:25:50.660
Yeah.

00:25:50.660 --> 00:25:53.280
I guess not everyone's machine has that for sure.

00:25:53.700 --> 00:25:55.440
So question from the audience here.

00:25:55.440 --> 00:25:56.960
Jazzy asks,

00:25:56.960 --> 00:25:59.140
does it hook into AWS in particular?

00:25:59.140 --> 00:26:04.860
Is it compatible with existing pipelines like ingestion lambdas or transform lambdas?

00:26:04.860 --> 00:26:05.200
Yeah,

00:26:05.200 --> 00:26:06.300
you can hook into AWS.

00:26:06.300 --> 00:26:09.140
So we have some AWS integrations built in.

00:26:09.140 --> 00:26:10.280
Like I mentioned before,

00:26:10.280 --> 00:26:14.280
there's nothing stopping you from importing Boto3 and doing anything really you want.

00:26:14.460 --> 00:26:15.900
So a very simple use case,

00:26:15.900 --> 00:26:20.260
like let's say you already have an existing transformation being triggered in AWS through

00:26:20.260 --> 00:26:20.680
some Lambda.

00:26:20.680 --> 00:26:23.400
You could just model that within Dagster and say,

00:26:23.400 --> 00:26:23.640
you know,

00:26:23.640 --> 00:26:25.620
trigger that Lambda Boto3.

00:26:25.620 --> 00:26:26.000
Okay.

00:26:26.000 --> 00:26:30.040
Then the asset itself is really that representation of that pipeline,

00:26:30.040 --> 00:26:32.640
but you're not actually running that code within Dagster itself.

00:26:32.640 --> 00:26:34.400
That's still occurring on the AWS framework.

00:26:34.400 --> 00:26:38.820
And that's a really simple way to start adding a little bit of observability and orchestration

00:26:38.820 --> 00:26:39.880
to existing pipelines.

00:26:39.880 --> 00:26:40.320
Okay.

00:26:40.320 --> 00:26:45.260
That's pretty cool because now you have this nice UI and these metadata in this history,

00:26:45.260 --> 00:26:47.080
but it's someone else's cloud.

00:26:47.080 --> 00:26:47.460
Exactly.

00:26:47.460 --> 00:26:47.840
Yeah.

00:26:47.840 --> 00:26:48.160
Yeah.

00:26:48.160 --> 00:26:50.060
And you can start to pull more information in there.

00:26:50.060 --> 00:26:51.120
And over time,

00:26:51.120 --> 00:26:51.780
you might decide,

00:26:51.780 --> 00:26:52.080
you know,

00:26:52.080 --> 00:26:52.500
this,

00:26:52.500 --> 00:26:53.020
you know,

00:26:53.020 --> 00:26:53.900
Lambda that I had,

00:26:53.900 --> 00:26:54.960
it's starting to get out of hand.

00:26:54.960 --> 00:26:59.740
I want to kind of break it apart into multiple assets where I want to sort of optimize it a

00:26:59.740 --> 00:27:01.400
little way in Dagster can help you along that.

00:27:01.400 --> 00:27:02.160
Yeah.

00:27:02.160 --> 00:27:02.760
Excellent.

00:27:02.760 --> 00:27:04.000
How do you set up,

00:27:04.000 --> 00:27:07.820
like triggers or observability inside Dagster?

00:27:07.820 --> 00:27:09.740
Like Jazzy's asking about S3,

00:27:09.740 --> 00:27:10.820
but like in general,

00:27:10.820 --> 00:27:11.160
right?

00:27:11.160 --> 00:27:13.120
If a row is entered into a database,

00:27:13.120 --> 00:27:16.400
something is dropped in a blob storage or the date changes.

00:27:16.400 --> 00:27:16.840
I don't know.

00:27:16.840 --> 00:27:17.320
Yeah.

00:27:17.320 --> 00:27:18.300
Those are great questions.

00:27:18.300 --> 00:27:19.440
You have a lot of options.

00:27:19.440 --> 00:27:19.960
In Dagster,

00:27:19.960 --> 00:27:23.280
we do model every asset with a couple little flags,

00:27:23.280 --> 00:27:24.960
I think that are really useful to think about.

00:27:24.960 --> 00:27:28.100
One is whether the code of that particular asset has changed.

00:27:28.100 --> 00:27:28.620
Right.

00:27:28.620 --> 00:27:32.260
And then the other one is whether anything upstream of that asset has changed.

00:27:32.260 --> 00:27:37.760
And those two things really power a lot of automation functionality that we can get downstream.

00:27:37.760 --> 00:27:39.460
So let's start with the,

00:27:39.460 --> 00:27:41.440
I think the S3 example is the easiest to understand.

00:27:41.440 --> 00:27:43.820
You have a bucket and there is,

00:27:43.820 --> 00:27:44.160
you know,

00:27:44.160 --> 00:27:45.760
a file that gets uploaded every day.

00:27:45.760 --> 00:27:47.560
You don't know what time that file gets uploaded.

00:27:47.560 --> 00:27:49.140
You don't know when it'll be uploaded,

00:27:49.140 --> 00:27:49.620
but you know,

00:27:49.640 --> 00:27:51.320
at some point it will be in Dagster.

00:27:51.320 --> 00:27:52.660
We have a thing called the sensor,

00:27:52.660 --> 00:27:55.180
which you can just connect to an S3 location.

00:27:55.700 --> 00:27:58.820
You can define how it looks into that file or into that folder.

00:27:58.820 --> 00:28:02.340
And then you would just pull every 30 seconds until something happens.

00:28:02.340 --> 00:28:04.040
When that something happens,

00:28:04.040 --> 00:28:06.060
that triggers sort of an event.

00:28:06.060 --> 00:28:10.060
And that event can trickle at your will downstream to everything that depends on it

00:28:10.060 --> 00:28:11.640
as you sort of connect to these things.

00:28:11.640 --> 00:28:13.440
So it gets you awake from this,

00:28:13.440 --> 00:28:13.680
like,

00:28:13.680 --> 00:28:13.940
Oh,

00:28:13.940 --> 00:28:15.860
I'm going to schedule something to run every hour.

00:28:16.100 --> 00:28:17.180
Maybe the data will be there,

00:28:17.180 --> 00:28:18.080
but maybe it won't.

00:28:18.080 --> 00:28:20.020
And you can have a much more event-based workflow.

00:28:20.020 --> 00:28:21.280
When this file runs,

00:28:21.280 --> 00:28:24.740
I want everything downstream to know that this data has changed.

00:28:24.740 --> 00:28:26.920
And as sort of data flows through these systems,

00:28:26.920 --> 00:28:28.760
everything will sort of work its way down.

00:28:28.760 --> 00:28:29.040
Yeah,

00:28:29.040 --> 00:28:29.600
I like it.

00:28:29.600 --> 00:28:34.200
This portion of Talk Python to Me is brought to you by Posit,

00:28:34.200 --> 00:28:35.400
the makers of Shiny,

00:28:35.400 --> 00:28:37.140
formerly RStudio,

00:28:37.140 --> 00:28:39.480
and especially Shiny for Python.

00:28:39.480 --> 00:28:41.400
Let me ask you a question.

00:28:41.400 --> 00:28:43.080
Are you building awesome things?

00:28:43.080 --> 00:28:44.180
Of course you are.

00:28:44.180 --> 00:28:45.740
You're a developer or data scientist.

00:28:45.880 --> 00:28:46.660
That's what we do.

00:28:46.660 --> 00:28:48.680
And you should check out Posit Connect.

00:28:48.680 --> 00:28:51.520
Posit Connect is a way for you to publish,

00:28:51.520 --> 00:28:52.000
share,

00:28:52.000 --> 00:28:55.660
and deploy all the data products that you're building using Python.

00:28:55.660 --> 00:28:58.840
People ask me the same question all the time.

00:28:58.840 --> 00:28:59.300
Michael,

00:28:59.300 --> 00:29:02.020
I have some cool data science project or notebook that I built.

00:29:02.020 --> 00:29:03.920
How do I share it with my users,

00:29:03.920 --> 00:29:04.660
stakeholders,

00:29:04.660 --> 00:29:05.300
teammates?

00:29:05.300 --> 00:29:10.120
Do I need to learn FastAPI or Flask or maybe Vue or React.js?

00:29:10.120 --> 00:29:11.340
Hold on now.

00:29:11.340 --> 00:29:12.580
Those are cool technologies,

00:29:12.580 --> 00:29:14.240
and I'm sure you'd benefit from them,

00:29:14.240 --> 00:29:15.760
but maybe stay focused on the data,

00:29:15.760 --> 00:29:16.120
project.

00:29:16.120 --> 00:29:18.600
Let Posit Connect handle that side of things.

00:29:18.600 --> 00:29:19.800
With Posit Connect,

00:29:19.800 --> 00:29:23.320
you can rapidly and securely deploy the things you build in Python.

00:29:23.320 --> 00:29:24.140
Streamlit,

00:29:24.140 --> 00:29:24.700
Dash,

00:29:24.700 --> 00:29:25.200
Shiny,

00:29:25.200 --> 00:29:25.700
Bokeh,

00:29:25.700 --> 00:29:26.500
FastAPI,

00:29:26.500 --> 00:29:27.000
Flask,

00:29:27.000 --> 00:29:27.700
Quarto,

00:29:27.700 --> 00:29:28.240
ports,

00:29:28.240 --> 00:29:28.800
dashboards,

00:29:28.800 --> 00:29:29.760
and APIs.

00:29:29.760 --> 00:29:32.040
Posit Connect supports all of them.

00:29:32.040 --> 00:29:37.900
And Posit Connect comes with all the bells and whistles to satisfy IT and other enterprise requirements.

00:29:38.480 --> 00:29:42.200
Make deployment the easiest step in your workflow with Posit Connect.

00:29:42.200 --> 00:29:43.320
For a limited time,

00:29:43.320 --> 00:29:48.420
you can try Posit Connect for free for three months by going to talkpython.fm/posit.

00:29:48.420 --> 00:29:52.040
That's talkpython.fm/P-O-S-I-T.

00:29:52.260 --> 00:29:53.940
The link is in your podcast player show notes.

00:29:53.940 --> 00:29:57.180
Thank you to the team at Posit for supporting Talk Python.

00:29:59.100 --> 00:30:05.140
The sensor concept is really cool because I'm sure that there's a ton of cloud machines,

00:30:05.140 --> 00:30:10.420
people provisioned, just because this thing runs every 15 minutes, that runs every 30 minutes,

00:30:10.420 --> 00:30:16.840
and you add them up and aggregate, we need eight machines just to handle the automation rather than,

00:30:16.840 --> 00:30:19.420
you know, because they're hoping to catch something without too much latency,

00:30:19.420 --> 00:30:21.940
but maybe like that actually only changes once a week.

00:30:21.940 --> 00:30:22.340
Exactly.

00:30:22.340 --> 00:30:27.760
And I think that's where we have to like sometimes step away from the way we're so used to thinking

00:30:27.760 --> 00:30:28.420
about things.

00:30:28.420 --> 00:30:29.580
And I'm guilty of this.

00:30:29.580 --> 00:30:33.600
When I create a data pipeline, my natural inclination is to create a schedule where it's

00:30:33.600 --> 00:30:34.840
a, is this a daily one?

00:30:34.840 --> 00:30:35.500
Is this weekly?

00:30:35.500 --> 00:30:36.300
Is this monthly?

00:30:36.300 --> 00:30:40.180
But what I'm finding more and more is when I'm creating my pipelines, I'm not adding a schedule.

00:30:40.180 --> 00:30:45.180
I'm using Dacister's auto-materialized policies, and I'm just telling it, you figure it out.

00:30:45.180 --> 00:30:46.620
I don't have to think about schedules.

00:30:46.620 --> 00:30:48.540
Just figure out when the things should be updated.

00:30:48.540 --> 00:30:51.080
When it's, you know, parents have been updated, you run.

00:30:51.080 --> 00:30:53.020
When the data has changed, you run.

00:30:53.020 --> 00:30:55.340
And then just like figure it out and leave me alone.

00:30:55.340 --> 00:30:55.620
Yeah.

00:30:55.620 --> 00:30:57.440
And it's worked pretty well for me so far.

00:30:57.440 --> 00:30:57.860
I think it's great.

00:30:57.860 --> 00:31:04.200
I have a search, refresh the search index on the various podcast pages that runs and it

00:31:04.200 --> 00:31:08.140
runs every hour, but the podcast ships weekly, right?

00:31:08.140 --> 00:31:09.960
But I don't know which hour it is.

00:31:09.960 --> 00:31:14.780
And so it seems like that's enough latency, but it would be way better to put just a little

00:31:14.780 --> 00:31:15.520
bit of smarts.

00:31:15.520 --> 00:31:18.420
Like what was the last date that anything changed?

00:31:18.420 --> 00:31:19.940
Was that since the last time you saw it?

00:31:19.940 --> 00:31:20.920
Maybe we'll just leave that alone.

00:31:21.060 --> 00:31:25.780
You know, you're starting to inspire me to go write more code, but pretty cool.

00:31:25.780 --> 00:31:26.300
All right.

00:31:26.300 --> 00:31:33.540
So on the homepage at Dagster.io, you've got a nice graphic that shows you both how to write

00:31:33.540 --> 00:31:37.720
the code, like some examples of the code, as well as how that looks in the UI.

00:31:37.720 --> 00:31:41.380
And one of them is called, says to launch backfills.

00:31:41.380 --> 00:31:43.100
What is this backfill thing?

00:31:43.300 --> 00:31:44.460
Oh, this is my favorite thing.

00:31:44.460 --> 00:31:44.940
Okay.

00:31:44.940 --> 00:31:45.040
Okay.

00:31:45.040 --> 00:31:50.360
So when you first start your data journey as a data engineer, you sort of have a pipeline

00:31:50.360 --> 00:31:53.700
and you build it and it just runs on a schedule and that's fine.

00:31:53.700 --> 00:31:58.060
What you soon find is, you know, you might have to go back in time.

00:31:58.060 --> 00:32:01.700
You might say, I've got this data set that updates monthly.

00:32:01.700 --> 00:32:02.660
Here's a great example.

00:32:02.780 --> 00:32:04.380
AWS cost reporting, right?

00:32:04.380 --> 00:32:10.320
AWS will send you some data around, you know, all your instances and your S3 bucket, all

00:32:10.320 --> 00:32:10.540
that.

00:32:10.540 --> 00:32:14.360
And it'll update that data every day or every month or whatever have you.

00:32:14.360 --> 00:32:18.800
Due to some reason, you've got to go back in time and refresh data that AWS updated due

00:32:18.800 --> 00:32:19.980
to some like discrepancy.

00:32:19.980 --> 00:32:21.580
Backfill is sort of how you do that.

00:32:21.820 --> 00:32:24.400
And it works hand in hand with this idea of a partition.

00:32:24.400 --> 00:32:27.940
A partition is sort of how your data is naturally organized.

00:32:27.940 --> 00:32:31.100
And it's like a nice way to represent that natural organization.

00:32:31.100 --> 00:32:34.820
It has nothing to do with like the fundamental way, how often you want to run it.

00:32:34.820 --> 00:32:39.680
It's more around like, I've got a data set that comes in once a month, it's represented monthly.

00:32:39.680 --> 00:32:43.120
It might be updated daily, but it's the representation of the data is monthly.

00:32:43.120 --> 00:32:44.600
So I will partition it by month.

00:32:44.600 --> 00:32:45.900
It doesn't have to be dates.

00:32:45.900 --> 00:32:47.100
It could be strings.

00:32:47.100 --> 00:32:48.220
It could be a list.

00:32:48.220 --> 00:32:55.380
You could have a partition for every company or every client or every domain you have,

00:32:55.380 --> 00:32:59.780
whatever you sort of think is a natural way to think about breaking apart that pipeline.

00:32:59.780 --> 00:33:04.220
And once you do that partition, you can do these nice things called backfills, which says,

00:33:04.220 --> 00:33:08.620
instead of running this entire pipeline and all my data, I want you to pick that one month

00:33:08.620 --> 00:33:12.840
where your data went wrong or that one month where data was missing and just run the partition

00:33:12.840 --> 00:33:13.840
on that range.

00:33:13.840 --> 00:33:18.180
And so you limit compute, you save resources and get a little bit more efficient.

00:33:18.180 --> 00:33:22.240
And it's just easier to like think about your pipeline because you've got this natural

00:33:22.240 --> 00:33:23.480
built-in partitioning system.

00:33:23.480 --> 00:33:24.360
Excellent.

00:33:24.360 --> 00:33:27.400
So maybe you missed some important event.

00:33:27.400 --> 00:33:31.540
Maybe your automation went down for a little bit, came back up.

00:33:31.540 --> 00:33:33.240
You're like, oh no, we've, we've missed it.

00:33:33.240 --> 00:33:33.580
Right.

00:33:33.580 --> 00:33:36.540
But you want to start over for three years.

00:33:36.540 --> 00:33:39.700
So maybe we could just go and run the last day.

00:33:39.700 --> 00:33:40.480
It's worth it.

00:33:40.480 --> 00:33:40.880
Exactly.

00:33:40.880 --> 00:33:44.720
Or another one would be your vendor says, hey, by the way, we actually screwed up.

00:33:44.720 --> 00:33:48.060
We uploaded this file from two months ago, but the numbers were all wrong.

00:33:48.060 --> 00:33:51.020
So we've uploaded a new version to that destination.

00:33:51.020 --> 00:33:52.800
Can you update your data set?

00:33:52.800 --> 00:33:55.980
One way is to recompute the entire universe from scratch.

00:33:55.980 --> 00:34:00.220
But if you've partitioned things and you can say, no, limit that to just this one partition

00:34:00.220 --> 00:34:01.260
for that month.

00:34:01.260 --> 00:34:04.740
And that one partition can trickle down all the way to all your other assets that depend

00:34:04.740 --> 00:34:05.200
on that one.

00:34:05.200 --> 00:34:10.620
Do you have to pre decide, do you have to think about this partitioning beforehand or can you

00:34:10.620 --> 00:34:11.340
do it retroactively?

00:34:11.340 --> 00:34:12.340
You could do it retroactively.

00:34:12.340 --> 00:34:14.000
And I have done that before as well.

00:34:14.000 --> 00:34:16.340
It really depends on, on where you're at.

00:34:16.660 --> 00:34:18.980
I think it's your first asset ever.

00:34:18.980 --> 00:34:23.500
Probably don't bother with partitions, but it really isn't a lot of work to get them to

00:34:23.500 --> 00:34:23.960
get them started.

00:34:23.960 --> 00:34:24.800
Okay.

00:34:24.800 --> 00:34:25.400
Yeah.

00:34:25.400 --> 00:34:25.940
Really neat.

00:34:25.940 --> 00:34:27.480
I like a lot of the ideas here.

00:34:27.480 --> 00:34:33.020
I like that it's got this visual component that you can see what's going on, inspect it.

00:34:33.020 --> 00:34:34.620
You also see you can debug runs.

00:34:34.620 --> 00:34:35.780
What happens there?

00:34:35.900 --> 00:34:40.180
Like, obviously, when you're pulling data from many different sources, maybe it's not your

00:34:40.180 --> 00:34:41.140
data you're taking in.

00:34:41.140 --> 00:34:42.180
Fields can vanish.

00:34:42.180 --> 00:34:43.260
It can be the wrong type.

00:34:43.260 --> 00:34:44.400
Systems can go down.

00:34:44.400 --> 00:34:45.040
I'm sure.

00:34:45.040 --> 00:34:46.000
Sure.

00:34:46.000 --> 00:34:47.160
The debugging is interesting.

00:34:47.160 --> 00:34:52.520
So what's it looks a little bit kind of like a web browser debug dev tools thing.

00:34:52.520 --> 00:34:54.720
So for the record, my code never fails.

00:34:54.720 --> 00:34:56.320
I've never had a bug in my life.

00:34:56.320 --> 00:34:57.600
But for the other you have.

00:34:57.600 --> 00:34:58.180
Yeah.

00:34:58.180 --> 00:34:59.120
Well, mine doesn't.

00:34:59.200 --> 00:35:02.540
I only do it to make an example and remind me how others.

00:35:02.540 --> 00:35:03.020
Yes.

00:35:03.020 --> 00:35:05.160
If I do, it's intentional, of course.

00:35:05.160 --> 00:35:05.840
Yeah.

00:35:05.840 --> 00:35:07.240
To humble myself a little bit.

00:35:07.240 --> 00:35:07.980
Exactly.

00:35:07.980 --> 00:35:10.760
This view is one of my favorite.

00:35:10.760 --> 00:35:11.960
I mean, so many favorite views.

00:35:11.960 --> 00:35:13.860
But this is, it's actually really fun to watch.

00:35:13.860 --> 00:35:16.300
Watch this actually run when you execute this pipeline.

00:35:16.300 --> 00:35:20.920
But really, like, let's go back to, you know, the world before orchestrators.

00:35:20.920 --> 00:35:22.380
We use cron, right?

00:35:22.380 --> 00:35:24.360
We'd have a bash script that would do something.

00:35:24.360 --> 00:35:27.120
And we'd have a cron job that said, make sure this thing runs.

00:35:27.120 --> 00:35:30.980
And then hopefully it was successful, but sometimes it wasn't.

00:35:30.980 --> 00:35:32.660
And it's a sometimes it wasn't.

00:35:32.660 --> 00:35:34.020
That's always been the problem, right?

00:35:34.020 --> 00:35:35.240
It's like, well, what do I do now?

00:35:35.240 --> 00:35:36.600
How do I know why it failed?

00:35:36.600 --> 00:35:38.340
What was, when did it fail?

00:35:38.340 --> 00:35:40.940
You know, at what point or what steps did it fail?

00:35:40.940 --> 00:35:42.140
That's really hard to do.

00:35:42.140 --> 00:35:48.740
What this debugger really is, is a structured log of every step that's been going on through your pipeline, right?

00:35:48.740 --> 00:35:51.900
So in this view, there's three assets that we can kind of see here.

00:35:51.900 --> 00:35:53.120
One is called users.

00:35:53.120 --> 00:35:54.360
One is called orders.

00:35:54.360 --> 00:35:55.840
And one is to run dbt.

00:35:56.360 --> 00:35:59.400
So presumably there's these two, you know, tables that are being updated.

00:35:59.400 --> 00:36:02.660
And then a dbt job, it looks like that's being updated at the very end.

00:36:02.660 --> 00:36:07.580
Once you execute this pipeline, all the logs are captured from each of those assets.

00:36:07.580 --> 00:36:10.380
So you can manually write your own logs.

00:36:10.380 --> 00:36:16.360
You have access to a Python logger and you can use your info, your error, whatever have you, in log output that way.

00:36:16.360 --> 00:36:17.960
And it'll be captured in a structured way.

00:36:18.040 --> 00:36:21.340
But it also capture logs from the integrations.

00:36:21.340 --> 00:36:24.220
So using dbt, we capture those logs as well.

00:36:24.220 --> 00:36:26.720
You can see it processing every single asset.

00:36:26.720 --> 00:36:33.540
So if anything does go wrong, you can filter down and understand at what step, at what point did something go wrong.

00:36:33.640 --> 00:36:34.040
That's awesome.

00:36:34.040 --> 00:36:36.800
And just the historical aspect.

00:36:36.800 --> 00:36:43.400
Because just going through logs, especially multiple systems, can be really, really tricky to figure out what's the problem.

00:36:43.400 --> 00:36:45.380
What actually caused this to go wrong?

00:36:45.380 --> 00:36:47.260
But come back and say, oh, it crashed.

00:36:47.260 --> 00:36:49.240
Pull up the UI and see.

00:36:49.240 --> 00:36:49.820
All right.

00:36:49.840 --> 00:36:52.160
Well, show me what this run did.

00:36:52.160 --> 00:36:53.140
Show me what this job did.

00:36:53.140 --> 00:36:57.840
And it seems like it's a lot easier to debug than your standard web API or something like that.

00:36:57.840 --> 00:37:02.340
You can click onto any of these assets that get that metadata that we had earlier as well.

00:37:02.340 --> 00:37:08.120
If, you know, one step failed and it's kind of flaky, you can just click on that one step and say, just rerun this.

00:37:08.120 --> 00:37:09.280
Everything else is fine.

00:37:09.280 --> 00:37:10.560
We don't need to restart from scratch.

00:37:10.560 --> 00:37:10.880
Okay.

00:37:10.880 --> 00:37:13.860
And it'll keep the data from before.

00:37:13.860 --> 00:37:15.300
So you don't have to rerun that.

00:37:15.300 --> 00:37:15.540
Yeah.

00:37:15.540 --> 00:37:17.720
I mean, it depends on how you built the pipeline.

00:37:18.100 --> 00:37:23.020
We like to build idempotent pipelines is how we sort of talk about it, the data engineering landscape, right?

00:37:23.020 --> 00:37:27.220
So you should be able to run something multiple times and not break anything in a perfect world.

00:37:27.220 --> 00:37:28.200
That's not always possible.

00:37:28.200 --> 00:37:29.900
But ideally, yes.

00:37:29.900 --> 00:37:38.420
And so we can presume that if users completed successfully, then we don't have to run that again because that data was persisted, you know, database S3 somewhere.

00:37:38.420 --> 00:37:45.000
And if orders was the one that was broken, we can just only run orders and not have to worry about rewriting the whole thing from scratch.

00:37:46.000 --> 00:37:53.660
So idempotent for people who maybe don't know, you run it once or you perform the operation once or you perform it 20 times, same outcome.

00:37:53.660 --> 00:37:55.260
Should have side effects, right?

00:37:55.260 --> 00:37:55.800
That's the idea.

00:37:55.800 --> 00:37:56.260
Yeah.

00:37:56.260 --> 00:37:56.880
That's the idea.

00:37:56.880 --> 00:37:58.600
Easier something done sometimes.

00:37:58.600 --> 00:38:00.080
It sure is.

00:38:00.080 --> 00:38:00.700
Sometimes it's easy.

00:38:00.700 --> 00:38:01.540
Sometimes it's very hard.

00:38:01.540 --> 00:38:07.200
But the more you can build pipelines that way, the easier your life becomes in many ways.

00:38:07.380 --> 00:38:10.560
Generally, not always, but generally true for programming as well, right?

00:38:10.560 --> 00:38:14.300
If you talk to functional programming people, they'll say like, it's an absolute, but.

00:38:14.300 --> 00:38:14.840
Yes.

00:38:14.840 --> 00:38:17.300
Functional programmers love this kind of stuff.

00:38:17.300 --> 00:38:20.700
And it actually does lend itself really well to data pipelines.

00:38:20.700 --> 00:38:29.860
Data pipelines, unlike maybe some of the software engineering stuff, it's a little bit different in that the data changing is what causes often most of the headaches, right?

00:38:30.100 --> 00:38:40.780
It's less so the actual code you write, but more the expectations tend to change so frequently and so often in new and novel and interesting ways that you would often never expect.

00:38:40.780 --> 00:38:48.700
And so the more you can sort of make that function so pure that you can provide any sort of data set and really test really easily.

00:38:48.700 --> 00:38:54.100
These expectations, when they get the easier it is to sort of debug these things and build on them in the future.

00:38:54.100 --> 00:38:54.440
Yeah.

00:38:54.740 --> 00:38:55.860
And cache them as well.

00:38:55.860 --> 00:38:56.280
Yes.

00:38:56.280 --> 00:38:57.080
It's always nice.

00:38:57.080 --> 00:38:57.380
Yeah.

00:38:57.380 --> 00:39:02.040
So speaking of that kind of stuff, like what's the scalability story?

00:39:02.040 --> 00:39:10.580
If I've got some big, huge, complicated data pipeline, can I parallelize them and have them run multiple pieces?

00:39:10.580 --> 00:39:12.640
Like if there's different branches or something like that?

00:39:12.640 --> 00:39:13.120
Yeah, exactly.

00:39:13.120 --> 00:39:19.400
That's one of the key benefits, I think, in writing your assets in this DAG way, right?

00:39:19.400 --> 00:39:22.420
Anything that is parallelizable will be parallelized.

00:39:22.420 --> 00:39:24.280
Now, sometimes you might want to put limits on that.

00:39:24.400 --> 00:39:25.920
Sometimes too much parallelization is bad.

00:39:25.920 --> 00:39:27.940
Your poor little database can't handle it.

00:39:27.940 --> 00:39:33.180
And you can say, you know, maybe a concurrency limit on this one just for today is worth putting.

00:39:33.180 --> 00:39:38.940
Or if you're hitting an API for an external vendor, they might not appreciate 10,000 requests a second on that one.

00:39:38.940 --> 00:39:40.320
So maybe you would slow it down.

00:39:40.320 --> 00:39:41.200
But in essence...

00:39:41.200 --> 00:39:41.880
Or rate limiting, right?

00:39:41.880 --> 00:39:45.000
You can run into too many requests and then your stuff crashes.

00:39:45.000 --> 00:39:45.680
Then you got to start.

00:39:45.680 --> 00:39:46.960
It can be a whole thing.

00:39:46.960 --> 00:39:47.480
It can be a whole thing.

00:39:47.480 --> 00:39:48.500
There's memory concerns.

00:39:48.500 --> 00:39:50.680
But let's pretend the world is simple.

00:39:50.680 --> 00:39:53.880
Anything that can be paralyzed will be through Dexter.

00:39:54.060 --> 00:39:59.620
And that's really the benefit of writing these DAGs is that there's a nice algorithm for determining what that actually looks like.

00:39:59.620 --> 00:39:59.840
Yeah.

00:39:59.840 --> 00:40:03.220
I guess if you have a diamond shape or any sort of split, right?

00:40:03.220 --> 00:40:04.920
Those two things now become...

00:40:04.920 --> 00:40:06.160
Because it's acyclical.

00:40:06.160 --> 00:40:09.200
They can't turn around and then eventually depend on each other again.

00:40:09.200 --> 00:40:09.800
So...

00:40:09.800 --> 00:40:12.260
That's a perfect chance to just go fork it out.

00:40:12.260 --> 00:40:12.660
Exactly.

00:40:12.660 --> 00:40:15.140
And that's kind of where partitions are also kind of interesting.

00:40:15.140 --> 00:40:31.240
If you have a partitioned asset, you could take your data set, partition it to five buckets and run all five partitions at once, knowing full well that because you've written this in a idempotent and partition way, that the first pipeline will only operate on apples and the second one only operates on bananas.

00:40:31.580 --> 00:40:35.080
And there is no commingling of apples and bananas anywhere in the pipeline.

00:40:35.080 --> 00:40:36.320
Oh, that's interesting.

00:40:36.320 --> 00:40:39.820
I hadn't really thought about using the partitions for parallelism, but of course...

00:40:39.820 --> 00:40:39.940
Yeah.

00:40:39.940 --> 00:40:42.740
It's a fun little way to break things apart.

00:40:43.160 --> 00:40:49.520
So if we run this on the Dagster cloud or even on our own, this is pretty much automatic.

00:40:49.520 --> 00:40:51.120
We don't have to do anything.

00:40:51.120 --> 00:40:55.120
Like Dagster just looks at it and says, this looks parallelizable and it'll go or...

00:40:55.120 --> 00:40:55.380
That's right.

00:40:55.380 --> 00:40:55.640
Yeah.

00:40:55.640 --> 00:41:02.400
As long as you've got the full deployment, whether it's OSS or cloud, Dagster will basically parallelize it for you as much as possible.

00:41:02.400 --> 00:41:02.780
Excellent.

00:41:02.780 --> 00:41:04.520
You can set global concurrency limits.

00:41:04.520 --> 00:41:09.280
So you might say, you know, 64 is more than enough, you know, parallelization that I need.

00:41:09.280 --> 00:41:12.700
Or maybe I want less because I'm worried about overloading systems.

00:41:12.920 --> 00:41:13.840
It's really up to you.

00:41:13.840 --> 00:41:15.680
I'm putting this on a $10 server.

00:41:15.680 --> 00:41:16.480
Please don't.

00:41:16.480 --> 00:41:18.460
Please don't kill it.

00:41:18.460 --> 00:41:21.820
Just respect that it's somewhat wimpy, but that's okay.

00:41:21.820 --> 00:41:22.940
It'll get the job done.

00:41:22.940 --> 00:41:23.740
It'll get the job done.

00:41:23.740 --> 00:41:24.280
All right.

00:41:24.280 --> 00:41:31.380
I want to talk about some of the tools and some of the tools that are maybe at play here when working with Dagster and some of the trends and stuff.

00:41:31.380 --> 00:41:38.380
But before that, it may be speak to where you could see people adopt a tool like Dagster, but they generally don't.

00:41:38.380 --> 00:41:42.800
They don't realize like, oh, actually, there's a whole framework for this, right?

00:41:42.800 --> 00:41:50.380
Like I could, sure, I could go and build just on HTTP server and hook into the request and start writing to it.

00:41:50.380 --> 00:41:52.760
But like, maybe I should use Flask or FastAPI.

00:41:52.760 --> 00:42:06.380
Like there's these frameworks that we really naturally adopt for certain situations like APIs and others, background jobs, data pipelines, where I think there's probably a good chunk of people who could benefit from stuff like this.

00:42:06.380 --> 00:42:08.900
But they just don't think they need a framework for it.

00:42:08.900 --> 00:42:10.040
Like Cron is enough.

00:42:10.040 --> 00:42:12.620
Yeah, it's funny because sometimes Cron is enough.

00:42:12.620 --> 00:42:15.620
And I don't want to encourage people not to use Cron.

00:42:15.620 --> 00:42:18.620
But think twice, at least, is what I would say.

00:42:19.340 --> 00:42:26.240
So probably the first like trigger for me of thinking of, you know, is that actually a good choice is like, am I trying to ingest data from somewhere?

00:42:26.240 --> 00:42:28.000
That's something that fails.

00:42:28.000 --> 00:42:33.000
Like, I think we just can accept that, you know, if you're moving data around, the data source will break.

00:42:33.000 --> 00:42:34.920
The expectations will change.

00:42:34.920 --> 00:42:36.240
You will need to debug it.

00:42:36.240 --> 00:42:37.460
You will need to run it.

00:42:37.460 --> 00:42:38.920
And doing that in Cron is a nightmare.

00:42:38.920 --> 00:42:43.820
So I would say definitely start to think about an orchestration system if you're ingesting data.

00:42:43.820 --> 00:42:47.300
If you have a simple Cron job that sends one email, like, you're probably fine.

00:42:47.300 --> 00:42:50.940
I don't think you need to implement all of DAG searches just to do that.

00:42:50.940 --> 00:43:03.720
But the more closer you get to data pipelining, I think the better your life will be if you're not trying to debug a obtuse process that no one really understands six months from now.

00:43:04.200 --> 00:43:04.640
Excellent.

00:43:04.640 --> 00:43:05.140
All right.

00:43:05.140 --> 00:43:08.100
Maybe we could touch on some of the tools that are interesting.

00:43:08.100 --> 00:43:13.420
You see people using, you talked about DuckDB and DBT, a lot of D's starting here.

00:43:13.420 --> 00:43:19.220
But give us a sense of like some of the supporting tools you see a lot of folks using that are interesting.

00:43:19.220 --> 00:43:19.940
Yeah, for sure.

00:43:19.940 --> 00:43:25.620
I think in the data space, probably DBT is one of the most popular choices.

00:43:25.620 --> 00:43:34.140
And DBT, in many ways, it's nothing more than a command line tool that runs a bunch of SQL in a DAG as well.

00:43:34.140 --> 00:43:37.660
So there's actually a nice fit with Dijkstra and DBT together.

00:43:37.660 --> 00:43:45.080
DBT is really used by people who are trying to model that business process using SQL against typically a data warehouse.

00:43:45.080 --> 00:43:57.320
So if you have your data in, for example, Postgres, Snowflake, Databricks, Microsoft SQL, these types of data warehouses, generally you're trying to model some type of business process.

00:43:57.700 --> 00:44:00.260
And typically people use SQL to do that.

00:44:00.260 --> 00:44:06.160
Now you can do this without DBT, but DBT has provided a nice clean interface to doing so.

00:44:06.160 --> 00:44:12.120
It makes it very easy to connect these models together, to run them, to have a development workflow that works really well.

00:44:12.120 --> 00:44:15.240
And then you can push it to prod and have things run again in production.

00:44:15.240 --> 00:44:16.860
So that's DBT.

00:44:16.860 --> 00:44:18.520
We find it works really well.

00:44:18.520 --> 00:44:21.480
And a lot of our customers are actually using DBT as well.

00:44:21.480 --> 00:44:27.020
There's DuckDB, which is a great, it's like the SQLite for columnar databases, right?

00:44:27.020 --> 00:44:27.260
Yeah.

00:44:27.260 --> 00:44:28.020
It's in process.

00:44:28.020 --> 00:44:29.080
It's fast.

00:44:29.080 --> 00:44:30.740
It's written by the Dutch.

00:44:30.740 --> 00:44:32.180
There's nothing you can't like about it.

00:44:32.180 --> 00:44:32.920
It's free.

00:44:32.920 --> 00:44:33.560
We love that.

00:44:33.560 --> 00:44:36.240
It feels very comfortable in Python itself.

00:44:36.240 --> 00:44:37.240
It does.

00:44:37.240 --> 00:44:37.920
It's so easy.

00:44:37.920 --> 00:44:39.320
Yes, exactly.

00:44:39.320 --> 00:44:42.560
The Dutch have given us so much and they've asked nothing of us.

00:44:42.560 --> 00:44:44.440
So I'm always very thankful for them.

00:44:44.440 --> 00:44:45.060
It's fast.

00:44:45.060 --> 00:44:45.820
It's so fast.

00:44:45.820 --> 00:44:56.500
It's like if you've ever used Pandas for processing large volumes of data, you've occasionally hit memory limits or inefficiencies in doing these large aggregates.

00:44:56.500 --> 00:45:08.640
I won't go into all the reasons of why that is, but DuckDB sort of changes that because it's a fast serverless sort of C++ written tooling to do really fast vectorized work.

00:45:08.640 --> 00:45:10.600
And by that, I mean like it works on columns.

00:45:10.600 --> 00:45:13.620
So typically in like SQLite, you're doing transactions.

00:45:13.960 --> 00:45:18.160
You're doing single row updates, writes, inserts, and SQLite is great at that.

00:45:18.160 --> 00:45:25.840
Where typical transactional databases fail or aren't as powerful is when you're doing aggregates, when you're looking at an entire column, right?

00:45:25.840 --> 00:45:26.920
Just the way they're architected.

00:45:26.920 --> 00:45:37.940
If you want to know the average, the median, the sum of some large number of columns, and you want to group that by a whole bunch of things, you want to know the first date someone did something and the last one.

00:45:37.940 --> 00:45:42.200
Those types of vectorized operations, DuckDB is really, really fast at doing.

00:45:42.660 --> 00:45:50.220
And it's a great alternative to, for example, Pandas, which can often hit memory limits and be a little bit slow in that regard.

00:45:50.220 --> 00:45:53.700
Yeah, it looks like it has some pretty cool aspects, transactions, of course.

00:45:53.700 --> 00:45:59.000
But it also says direct Parquet, CSV, and JSON querying.

00:45:59.220 --> 00:46:10.200
So if you've got a CSV file hanging around and you want to ask questions about it or JSON or some of the data science stuff through Parquet, turn an indexed proper query engine against it.

00:46:10.200 --> 00:46:12.300
Don't just use a dictionary or something, right?

00:46:12.300 --> 00:46:12.560
Yeah.

00:46:12.720 --> 00:46:22.300
It's great for reading a CSV, zip files, tar files, Parquets, partition Parquet files, all that stuff that usually was really annoying to do and operate on.

00:46:22.300 --> 00:46:24.180
You can now install DuckDB.

00:46:24.180 --> 00:46:25.700
It's got a great CLI, too.

00:46:25.700 --> 00:46:37.060
So before you go and program your entire pipeline, you just run DuckDB and you start writing SQL against CSV files and all this stuff to really understand your data and just really see how quick it is.

00:46:37.060 --> 00:46:48.820
I used it on a bird data set that I had as an example project and there was millions of rows and I was joining them together and doing massive group buys and it was done in like seconds.

00:46:48.820 --> 00:46:52.560
And it's just hard for me to believe that it was even correct because it was so quick.

00:46:52.560 --> 00:46:53.700
So it's wonderful.

00:46:53.700 --> 00:46:57.200
I must have done that wrong somehow because it's done.

00:46:57.200 --> 00:46:57.940
It shouldn't be done.

00:46:57.940 --> 00:46:58.200
Yeah.

00:46:58.200 --> 00:46:58.600
Yeah.

00:46:58.600 --> 00:47:06.320
And the fact it's in process means there's not a babysit, a server for you to babysit patch, make sure it's still running.

00:47:06.520 --> 00:47:08.220
It's accessible, but not too accessible.

00:47:08.220 --> 00:47:09.200
All that, right?

00:47:09.200 --> 00:47:12.140
It's a pip and sell away, which is always, we love that, right?

00:47:12.140 --> 00:47:12.880
Yeah, absolutely.

00:47:12.880 --> 00:47:25.320
You mentioned, or I guess I mentioned Parquet, but also Apache Arrow seems like it's making its way into a lot of different tools and sort of foundational sort of high memory, high performance in memory processing.

00:47:25.320 --> 00:47:26.300
Have you used this, Eddie?

00:47:26.300 --> 00:47:29.920
I've used it, especially through like working through different languages.

00:47:29.920 --> 00:47:34.120
So moving data between Python and R is where I last used this.

00:47:34.120 --> 00:47:35.600
I didn't know Arrow's great at that.

00:47:35.920 --> 00:47:40.320
I believe Arrow is like the underneath some of the Rust to Python as well.

00:47:40.320 --> 00:47:41.620
It's working there.

00:47:41.620 --> 00:47:47.120
So typically I don't use Arrow like directly myself, but it's in many of the tooling I use.

00:47:47.120 --> 00:47:47.740
Right.

00:47:47.740 --> 00:47:48.800
It's a great product.

00:47:48.800 --> 00:47:52.220
And like so much of the ecosystem is now built on Arrow.

00:47:52.380 --> 00:47:55.940
Yeah, I think a lot of it is, I feel like the first time I heard about it was through Polars.

00:47:55.940 --> 00:47:56.560
That's right.

00:47:56.560 --> 00:47:56.920
Yeah.

00:47:56.920 --> 00:48:04.820
I'm pretty sure, which is another Rust story for kind of like Pandas, but a little bit more fluent, lazy API.

00:48:04.820 --> 00:48:05.380
Yes.

00:48:05.380 --> 00:48:07.240
We live in such great times, to be honest.

00:48:07.240 --> 00:48:12.960
So Polars is a Python bindings for Rust, I believe is kind of how I think about it.

00:48:13.200 --> 00:48:20.300
It does all the transformation in Rust, but you've had this Python interface to it and it makes things, again, incredibly fast.

00:48:20.300 --> 00:48:22.520
I would say similar in speed to DuckDB.

00:48:22.520 --> 00:48:24.500
They both are quite comparable sometimes.

00:48:24.900 --> 00:48:29.440
Yeah, it also claims to have vectorized and columnar processing and all that kind of stuff.

00:48:29.440 --> 00:48:30.580
Yeah, it's pretty incredible.

00:48:30.580 --> 00:48:40.920
So not a drop-in replacement for Pandas, but if you have the opportunity to use it and you don't need to use the full breadth of what Pandas offers, because Pandas is quite a huge package.

00:48:40.920 --> 00:48:41.740
There's a lot it does.

00:48:41.740 --> 00:48:45.540
But if you're just doing simple transforms, I think Polars is a great option to explore.

00:48:45.540 --> 00:49:01.740
Yeah, I talked to Richie Vink, who is part of that, and I think they explicitly chose to not try to make it a drop-in replacement for Pandas, but tried to choose an API that would allow the engine to be smarter and go like, I see you're asking for this, but the step before you wanted this other thing.

00:49:01.740 --> 00:49:06.940
So let me do that transformation all in one shot and a little bit like a query optimization engine.

00:49:06.940 --> 00:49:07.220
Yeah.

00:49:07.220 --> 00:49:08.060
What else is out there?

00:49:08.060 --> 00:49:09.780
We've got time for just a couple more.

00:49:09.780 --> 00:49:12.300
If there's anything that you're like, oh, yeah, people use this all the time.

00:49:12.300 --> 00:49:15.960
Obviously, the databases, you've said, Postgres, Snowflake, et cetera.

00:49:15.960 --> 00:49:17.140
Yeah, there's so much.

00:49:17.140 --> 00:49:20.740
So another little one I like, it's called DLT, DLT Hub.

00:49:20.740 --> 00:49:23.200
It's getting a lot of attraction as well.

00:49:23.200 --> 00:49:25.520
And what I like about it is how lightweight it is.

00:49:25.520 --> 00:49:29.360
I'm such a big fan of lightweight tooling that's not, you know, massive frameworks.

00:49:29.360 --> 00:49:32.820
Loading data is, I think, still kind of yucky in many ways.

00:49:32.820 --> 00:49:33.460
It's not fun.

00:49:33.460 --> 00:49:36.420
And DLT makes it a little bit simpler and easier to do so.

00:49:36.420 --> 00:49:45.520
So that's what I would recommend people just look into if you got to either ingest data from, you know, some API, some website, some CSV file.

00:49:45.520 --> 00:49:47.480
It's a great way to do that.

00:49:47.480 --> 00:49:52.500
It claims it's the Python library for data teams loading data into unexpected places.

00:49:52.500 --> 00:49:53.420
Very interesting.

00:49:53.420 --> 00:49:54.480
Yes, that's great.

00:49:54.480 --> 00:49:55.760
Yeah, this looks cool.

00:49:55.760 --> 00:49:56.440
All right.

00:49:56.880 --> 00:50:02.920
Well, I guess maybe let's talk about, let's talk business and then we can talk about what's next and then we'll probably be out of time.

00:50:02.920 --> 00:50:04.420
I'm always fascinated.

00:50:04.420 --> 00:50:11.920
I think there's starting to be a bit of a blueprint for this, but companies that take a thing, they make it and they give it away and then they have a company around it.

00:50:11.920 --> 00:50:14.560
And, you know, congratulations to you all for doing that.

00:50:14.560 --> 00:50:14.940
Right.

00:50:14.940 --> 00:50:22.740
And a lot of it seems to kind of center around the open core model, which I don't know if that's exactly how you would characterize yourself.

00:50:23.140 --> 00:50:32.260
But maybe you talk about the business side, because I know there's many successful open source projects that don't necessarily result in full time jobs or companies if people were to want that.

00:50:32.260 --> 00:50:33.960
It's a really interesting place.

00:50:33.960 --> 00:50:38.860
And I don't think it's one that anyone has truly figured out well.

00:50:38.860 --> 00:50:41.980
I can say this is the way forward for everyone, but it is something we're trying.

00:50:41.980 --> 00:50:44.180
And I think for Dexter, I think it's working pretty well.

00:50:44.180 --> 00:50:49.400
And what I think is really powerful about Dexter is like the open source product is really, really good.

00:50:49.740 --> 00:50:55.040
And it hasn't really been limited in many ways in order to drive like cloud product consumption.

00:50:55.040 --> 00:50:55.700
Yeah, sure.

00:50:55.700 --> 00:50:58.660
We really believe that there's actual value in that separation of these things.

00:50:58.660 --> 00:51:01.400
There are some things that we just can't do in the open source platform.

00:51:01.400 --> 00:51:10.600
For example, there's pipelines on cloud that involve, you know, ingesting data through our own systems in order to do reporting, which just doesn't make sense to do on the open source system.

00:51:10.600 --> 00:51:12.620
It makes the product way too complex.

00:51:12.820 --> 00:51:19.260
But for the most part, I think Dexter open source, like we really believe that like just getting it in the hands of developers is the best way to prove the value of it.

00:51:19.260 --> 00:51:23.200
And if we can build a business on top of that, I think we're all super happy to do so.

00:51:23.200 --> 00:51:27.000
It's nice that we get to sort of drive both sides of it.

00:51:27.000 --> 00:51:29.700
To me, that's like one of the more exciting parts, right?

00:51:29.700 --> 00:51:36.980
A lot of the development that we do in Dexter open source is driven by people who are paid through, you know, what happens on Dexter cloud.

00:51:36.980 --> 00:51:44.880
And I think from what I can tell, there's no better way to build open source product than to have people who are adequately paid to develop that product.

00:51:44.880 --> 00:51:48.480
Otherwise, it can be, you know, a labor of love, but one that doesn't last for very long.

00:51:48.480 --> 00:51:53.700
Yeah, whenever I think about building software, there's 80% of it that's super exciting and fun, 10%.

00:51:53.700 --> 00:52:04.740
And then there's that little sliver of like really fine polish that if it's not just your job to make that thing polished, you're just for the most part, just not going to polish that bit, right?

00:52:04.740 --> 00:52:07.840
It's tough. UI, design, support.

00:52:07.840 --> 00:52:11.760
There's all these things that go into making software like really extraordinary.

00:52:11.760 --> 00:52:13.700
That's really, really tough to do.

00:52:13.700 --> 00:52:16.940
And I think I really like the open source business model.

00:52:16.940 --> 00:52:24.640
I think for me, being able to just try something, not having to talk to sales and being able to just deploy locally and test it out and see if this works.

00:52:24.640 --> 00:52:28.180
And if I choose to do so, deploy it in production.

00:52:28.180 --> 00:52:34.060
Or if I bought the cloud product and I don't like the direction that is going, I can even go into open source as well.

00:52:34.060 --> 00:52:35.220
That's pretty compelling to me.

00:52:35.220 --> 00:52:36.460
Yeah, for sure it is.

00:52:36.860 --> 00:52:50.520
And I think the more moving pieces of infrastructure, the more uptime you want and all those types of things, the more somebody who's maybe a programmer but not a DevOps infrastructure person but needs to have it there, right?

00:52:50.520 --> 00:52:52.500
Like that's an opportunity as well, right?

00:52:52.500 --> 00:52:54.560
For you to say, look, you can write the code.

00:52:54.880 --> 00:53:00.260
We made it cool for you to write the code, but you don't have to like get notified when the server's down or whatever.

00:53:00.260 --> 00:53:01.660
Like we'll just take care of that for you.

00:53:01.660 --> 00:53:02.420
That's pretty awesome.

00:53:02.420 --> 00:53:02.780
Yeah.

00:53:02.780 --> 00:53:04.840
And there's efficiencies of scale as well, right?

00:53:04.840 --> 00:53:08.960
Like we've learned the same mistakes over and over again, so you don't have to, which is nice.

00:53:08.960 --> 00:53:12.740
I don't know how many people who want to maintain servers, but people do.

00:53:12.740 --> 00:53:15.440
And they're more than welcome to if that's how they choose to do so.

00:53:15.440 --> 00:53:16.680
Yeah, for sure.

00:53:16.680 --> 00:53:17.260
All right.

00:53:17.260 --> 00:53:18.420
Just about out of time.

00:53:18.420 --> 00:53:22.500
Let's close up our conversation with where are things going for Dagster?

00:53:22.500 --> 00:53:23.880
Like what's on the roadmap?

00:53:24.300 --> 00:53:25.420
What are you excited about?

00:53:25.420 --> 00:53:26.340
Oh, that's a good one.

00:53:26.340 --> 00:53:29.640
I think we've actually published our roadmap line somewhere.

00:53:29.640 --> 00:53:31.640
If you search Dagster roadmap, it's probably out there.

00:53:31.640 --> 00:53:36.780
I think for the most part, that hasn't changed much going into 2024, though we may update it.

00:53:36.780 --> 00:53:37.620
There it is.

00:53:37.620 --> 00:53:40.280
We're really just doubling down on what we've built already.

00:53:40.280 --> 00:53:45.880
I think there's a lot of work we can do on the product itself to make it easier to use, easier to understand.

00:53:45.880 --> 00:53:49.100
My team specifically is really focused around the education piece.

00:53:49.100 --> 00:53:54.240
And so we launched Dagster University's first module, which helps you really understand the core.

00:53:54.240 --> 00:54:01.900
Our next module is coming up in a couple months, and that'll be around using Dagster with DBT, which is our most popular integration.

00:54:01.900 --> 00:54:03.920
We're building up more integrations as well.

00:54:03.920 --> 00:54:09.040
So I've built a little integration called Embedded ELT that makes it easy to ingest data.

00:54:09.300 --> 00:54:12.560
But I want to actually build an integration with DLT as well, DLT Hub.

00:54:12.560 --> 00:54:13.680
So we'll be doing that.

00:54:13.680 --> 00:54:17.340
And there's more coming down the pipe, but I don't know how much I can say.

00:54:17.340 --> 00:54:23.340
Look forward to an event in April where we'll have a launch event on all that's coming.

00:54:23.340 --> 00:54:23.680
Nice.

00:54:23.680 --> 00:54:26.260
Is that an online thing people can attend or something?

00:54:26.260 --> 00:54:26.560
Exactly.

00:54:27.020 --> 00:54:30.620
Yeah, there'll be some announcement there on the Dagster website on that.

00:54:30.620 --> 00:54:32.980
Maybe I will call it one thing that's actually really fun.

00:54:32.980 --> 00:54:34.700
It's called Dagster Open Platform.

00:54:34.700 --> 00:54:39.000
It's a GitHub repo that we launched a couple months ago, I want to say.

00:54:39.000 --> 00:54:41.460
We took our internal...

00:54:41.460 --> 00:54:42.220
I should go back one more.

00:54:42.220 --> 00:54:42.840
Sorry.

00:54:42.840 --> 00:54:45.720
It's like GitHub, Dagster Open Platform and GitHub.

00:54:45.720 --> 00:54:47.560
I have it somewhere.

00:54:47.560 --> 00:54:48.540
Yeah.

00:54:48.540 --> 00:54:51.200
It's here under the organization?

00:54:51.700 --> 00:54:54.080
Yes, it should be somewhere in here.

00:54:54.080 --> 00:54:54.760
There it is.

00:54:54.760 --> 00:54:56.580
Dagster Open Platform on GitHub.

00:54:56.580 --> 00:55:00.420
And it's really a clone of our production pipelines for the most part.

00:55:00.420 --> 00:55:03.180
There's some things we've chosen to ignore because they're sensitive.

00:55:03.180 --> 00:55:06.760
But as much as possible, we've defaulted to making it public and open.

00:55:06.760 --> 00:55:09.040
And the whole reason behind this was because, you know,

00:55:09.040 --> 00:55:12.480
as data engineers, it's often hard to see how other data engineers write code.

00:55:12.480 --> 00:55:14.880
We get to see how software engineers write code quite often,

00:55:14.880 --> 00:55:19.220
but most people don't want to share their platforms for various good reasons.

00:55:19.640 --> 00:55:23.300
Also, there's like smaller teams or maybe just one person.

00:55:23.300 --> 00:55:29.680
And then like those pipelines are so integrated into your specific infrastructure, right?

00:55:29.680 --> 00:55:32.220
So it's not like, well, here's a web framework to share, right?

00:55:32.220 --> 00:55:36.760
Like, here's how we integrate into that one weird API that we have that no one else has.

00:55:36.760 --> 00:55:38.860
So it's no point in publishing it to you, right?

00:55:38.860 --> 00:55:39.980
That's typically how it goes.

00:55:39.980 --> 00:55:43.200
Or they're so large that they're afraid that there's like some, you know,

00:55:43.200 --> 00:55:46.080
important information that they just don't want to take the risk on.

00:55:46.080 --> 00:55:46.280
Yep.

00:55:46.340 --> 00:55:49.620
And so we built like something that's in the middle where we've taken as much as we can

00:55:49.620 --> 00:55:50.780
and we've publicized it.

00:55:50.780 --> 00:55:52.040
And you can't run this on your own.

00:55:52.040 --> 00:55:53.320
Like, that's not the point.

00:55:53.320 --> 00:55:56.560
The point is to look at the code and see, you know, how does Dagster use Dagster?

00:55:56.560 --> 00:55:57.660
And what does that kind of look like?

00:55:57.660 --> 00:55:57.980
Nice.

00:55:57.980 --> 00:55:58.280
Okay.

00:55:58.280 --> 00:55:58.820
All right.

00:55:58.820 --> 00:56:01.760
Well, I'll put a link to that in the show notes and people can check it out.

00:56:01.760 --> 00:56:02.280
Appreciate it.

00:56:02.340 --> 00:56:02.480
Yeah.

00:56:02.480 --> 00:56:05.060
I guess let's wrap it up with the final call to action.

00:56:05.060 --> 00:56:06.520
People are interested in Dagster.

00:56:06.520 --> 00:56:07.400
How do they get started?

00:56:07.400 --> 00:56:07.960
What do you tell them?

00:56:07.960 --> 00:56:08.500
Oh, yeah.

00:56:08.500 --> 00:56:11.140
Dagster.io is probably the greatest place to start.

00:56:11.140 --> 00:56:13.520
You can try the cloud product.

00:56:13.520 --> 00:56:17.720
We have free self-serve or you can try the local install as well.

00:56:17.720 --> 00:56:22.200
If you get stuck, a great place to join is our Slack channel, which is up on our website.

00:56:22.200 --> 00:56:27.640
There's even a Ask AI channel where you can just talk to a Slack bot that's been trained

00:56:27.640 --> 00:56:29.260
on all our GitHub issues and discussions.

00:56:29.260 --> 00:56:34.560
And it's surprisingly good at walking you through, you know, any debugging, any issues or even advice.

00:56:34.560 --> 00:56:36.000
That's pretty excellent, actually.

00:56:36.000 --> 00:56:36.340
Yeah.

00:56:36.340 --> 00:56:37.280
It's real fun.

00:56:37.280 --> 00:56:37.860
It's really fun.

00:56:37.860 --> 00:56:40.680
And if that doesn't work, we're also there in the community where you can just chat to

00:56:40.680 --> 00:56:41.140
us as well.

00:56:41.140 --> 00:56:42.000
Cool.

00:56:42.000 --> 00:56:43.140
All right.

00:56:43.140 --> 00:56:45.040
Well, Pedram, thank you for being on the show.

00:56:45.040 --> 00:56:47.320
Make sure all the work on Dagster and sharing it with us.

00:56:47.320 --> 00:56:47.820
Thank you, Michael.

00:56:47.820 --> 00:56:48.120
You bet.

00:56:48.120 --> 00:56:48.660
See you later.

00:56:48.660 --> 00:56:52.000
This has been another episode of Talk Python to Me.

00:56:52.000 --> 00:56:53.760
Thank you to our sponsors.

00:56:53.760 --> 00:56:55.420
Be sure to check out what they're offering.

00:56:55.420 --> 00:56:56.840
It really helps support the show.

00:56:56.840 --> 00:57:01.740
This episode is sponsored by Posit Connect from the makers of Shiny.

00:57:01.740 --> 00:57:06.240
Publish, share, and deploy all of your data projects that you're creating using Python.

00:57:06.240 --> 00:57:12.820
Streamlit, Dash, Shiny, Bokeh, FastAPI, Flask, Quarto, Reports, Dashboards, and APIs.

00:57:12.820 --> 00:57:15.220
Posit Connect supports all of them.

00:57:15.220 --> 00:57:20.900
Try Posit Connect for free by going to talkpython.fm/posit, P-O-S-I-T.

00:57:22.340 --> 00:57:23.600
Want to level up your Python?

00:57:23.600 --> 00:57:27.720
We have one of the largest catalogs of Python video courses over at Talk Python.

00:57:27.720 --> 00:57:32.820
Our content ranges from true beginners to deeply advanced topics like memory and async.

00:57:32.820 --> 00:57:35.480
And best of all, there's not a subscription in sight.

00:57:35.480 --> 00:57:38.400
Check it out for yourself at training.talkpython.fm.

00:57:38.680 --> 00:57:40.460
Be sure to subscribe to the show.

00:57:40.460 --> 00:57:43.240
Open your favorite podcast app and search for Python.

00:57:43.240 --> 00:57:44.560
We should be right at the top.

00:57:44.560 --> 00:57:49.700
You can also find the iTunes feed at /itunes, the Google Play feed at /play,

00:57:49.700 --> 00:57:53.900
and the direct RSS feed at /rss on talkpython.fm.

00:57:53.900 --> 00:57:56.880
We're live streaming most of our recordings these days.

00:57:56.880 --> 00:58:00.280
If you want to be part of the show and have your comments featured on the air,

00:58:00.520 --> 00:58:04.640
be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

00:58:04.640 --> 00:58:06.760
This is your host, Michael Kennedy.

00:58:06.760 --> 00:58:08.060
Thanks so much for listening.

00:58:08.060 --> 00:58:09.220
I really appreciate it.

00:58:09.220 --> 00:58:11.120
Now get out there and write some Python code.

00:58:11.120 --> 00:58:32.000
I'll see you next time.

