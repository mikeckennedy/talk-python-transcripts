WEBVTT

00:00:00.001 --> 00:00:08.500
Game theory is the study of competing interests, be it individual actors within an economy or healthy versus cancer cells within a body.

00:00:08.500 --> 00:00:19.500
Our guests this week, Vince Knight, Mark Harper, and Owen Campbell, are here to discuss their Python project built to study and simulate one of the central problems in game theory, the prisoner's dilemma.

00:00:19.500 --> 00:00:25.620
This is Talk Python To Me, episode 104, recorded March 8th, 2017.

00:00:25.620 --> 00:00:55.460
Welcome to Talk Python To Me, a weekly podcast on Python, the language, the libraries, the ecosystem,

00:00:55.460 --> 00:00:56.580
and the personalities.

00:00:56.580 --> 00:00:58.680
This is your host, Michael Kennedy.

00:00:58.680 --> 00:01:00.680
Follow me on Twitter where I'm @mkennedy.

00:01:00.680 --> 00:01:07.100
Keep up with the show and listen to past episodes at talkpython.fm and follow the show on Twitter via at Talk Python.

00:01:07.100 --> 00:01:12.480
This episode is brought to you interruption free by Talk Python Training.

00:01:12.480 --> 00:01:14.440
That's right, I'm talking about my courses.

00:01:14.440 --> 00:01:22.460
Whether you want to get started learning Python with the Jumpstart course, learn over 50 Pythonic coding techniques, or consume a bunch of HTTP services,

00:01:22.460 --> 00:01:23.760
I've got a course for you.

00:01:23.760 --> 00:01:29.440
If you're not already subscribed, be sure to be a friend of the show at talkpython.fm/friends.

00:01:29.440 --> 00:01:36.980
I have two new courses I'll be announcing in just the next couple of weeks, as well as a major feature, so you probably don't want to miss out on that.

00:01:36.980 --> 00:01:42.440
And finally, if you work for a company with 200 developers or more, we should talk.

00:01:42.440 --> 00:01:45.980
I have a very special first-time offer for enterprises.

00:01:46.200 --> 00:01:52.340
All right, now let's get on to learning how we can minimize our prison sentence by studying the prisoner's dilemma.

00:01:52.340 --> 00:01:56.040
Oh, and Vincent, Mark, welcome to Talk Python.

00:01:56.040 --> 00:01:57.040
Thanks for having us.

00:01:57.040 --> 00:01:57.740
Thanks for having us.

00:01:57.740 --> 00:01:58.880
Yeah, pleasure to be here.

00:01:58.880 --> 00:02:07.000
Yeah, it's great to make your acquaintance, and I'm really excited to talk about game theory and trade-offs and solving all these sort of standoff problems.

00:02:07.000 --> 00:02:08.000
It's going to be so much fun.

00:02:08.360 --> 00:02:16.220
But before we actually get into what is game theory, the library that you guys are working on, and things like that, let's start with your story.

00:02:16.220 --> 00:02:18.220
Vincent, how did you get into programming in Python?

00:02:18.220 --> 00:02:25.160
I'm a mathematician, and so I used various bits of commercial software during my PhD, so Mathematica, the main one.

00:02:25.360 --> 00:02:31.640
And then I started looking for a free alternative, and that's how I stumbled on SageMath.

00:02:31.640 --> 00:02:39.380
And from SageMath kind of fell into Python and slowly, slowly grew from there, and now I teach Python.

00:02:39.380 --> 00:02:40.660
That's really awesome.

00:02:40.660 --> 00:02:43.040
And so it's great that you started with SageMath.

00:02:43.040 --> 00:02:47.320
I've had William Stein on the show, and I think SageMath is quite the cool project.

00:02:47.320 --> 00:02:55.220
It's really generated a lot of neat things like Cython that people are doing totally different stuff with today.

00:02:55.220 --> 00:02:58.440
Do you teach classes with SageMath, or do you just teach Python?

00:02:58.440 --> 00:02:59.280
We used to.

00:02:59.280 --> 00:03:08.640
I used to teach all our first years a combination of Python and SageMath, but actually for a number of reasons, I changed that over to just teaching with Python now.

00:03:08.640 --> 00:03:14.840
So the Python Simpy library kind of does a lot of what our first years need to do and the various things about the installation.

00:03:14.840 --> 00:03:19.500
That's getting better, especially with Cloud.SageMath, but we've just moved to just teaching Python now.

00:03:19.500 --> 00:03:19.680
Sure.

00:03:19.680 --> 00:03:20.420
Okay, great.

00:03:20.420 --> 00:03:21.400
Owen, how about you?

00:03:21.400 --> 00:03:22.060
What's your story?

00:03:22.060 --> 00:03:31.580
Well, I'm a ZX81 generation, so I was bought one of those things as a small child, and I've never really looked back.

00:03:31.580 --> 00:03:36.900
Once I realized that people were prepared to pay me to do this stuff, that was the career for me.

00:03:36.900 --> 00:03:39.940
Like, you know, I could play with this toy every day and I get paid for it?

00:03:39.940 --> 00:03:41.080
Oh my gosh, I'm in.

00:03:41.080 --> 00:03:41.980
Count me in.

00:03:41.980 --> 00:03:42.460
Exactly.

00:03:42.460 --> 00:03:51.360
I don't think I've ever quite reached the level of joy of the day I made it draw a space invader, but, you know, once or twice I get close, and one day I hope to get there.

00:03:51.360 --> 00:03:52.560
Nice, nice.

00:03:52.560 --> 00:03:54.000
I'm relatively new to Python.

00:03:54.000 --> 00:03:55.140
I actually started.

00:03:55.140 --> 00:04:05.500
My eldest son is one of Vince's students, and when he started at Cardiff and said, hey, it's Python they're teaching us here, he's got a younger brother.

00:04:05.500 --> 00:04:10.500
And I thought, right, well, we'll get a head start on, I'd better learn this stuff so that I can teach him.

00:04:10.500 --> 00:04:21.460
And so I've only been, I must have used every language under the sun until then, but it's only in the last three, four years or so that I've done any Python work.

00:04:21.580 --> 00:04:29.500
Now that's a really cool coincidence, and I think that's a great, it's a great service you're doing for your son to sort of like get him ready for all those things, right?

00:04:29.500 --> 00:04:38.120
If you go into almost any kind of university with programming skills of some sort, you're at such an advantage, I think.

00:04:38.280 --> 00:04:42.800
Well, I'm sure he'll think so too now that he hears you saying it on this podcast.

00:04:42.800 --> 00:04:47.720
I've seen it work out well for a lot of people.

00:04:47.720 --> 00:04:49.680
All right, Mark, how about you?

00:04:49.680 --> 00:04:50.820
Sure, I'm a data scientist.

00:04:50.820 --> 00:04:52.580
I currently work at Google.

00:04:53.320 --> 00:04:57.480
Before that, I was an academic in bioinformatics, which we used a lot of Python in.

00:04:57.480 --> 00:05:01.920
And I started with Python before that when I was a graduate student in mathematics.

00:05:03.040 --> 00:05:08.160
I started writing C++ way back in high school, and that was okay for a while.

00:05:08.160 --> 00:05:14.620
But then I looked for languages that were maybe more efficient with my time and less efficient with the machine's time.

00:05:14.620 --> 00:05:20.500
So I had a very brief stint with Perl before I found Python, and then I never looked back.

00:05:20.500 --> 00:05:21.440
Oh, that's awesome.

00:05:21.440 --> 00:05:23.480
And you're using Python day-to-day at Google?

00:05:23.480 --> 00:05:24.880
Among other languages, yes.

00:05:24.880 --> 00:05:25.360
Yeah.

00:05:25.360 --> 00:05:26.040
Okay, cool.

00:05:26.040 --> 00:05:28.580
Yeah, you said you started with C++.

00:05:28.580 --> 00:05:35.980
I remember when I was at university, they told me I had to start with Fortran, and I pleaded to let me do C++ instead.

00:05:35.980 --> 00:05:37.540
I'm like, this will be so much more useful.

00:05:37.540 --> 00:05:37.960
I'm sure.

00:05:37.960 --> 00:05:38.820
No, no.

00:05:38.820 --> 00:05:41.560
Fortran is the most important language you'll ever learn.

00:05:41.560 --> 00:05:43.540
I'm not sure I still agree with them.

00:05:43.540 --> 00:05:45.680
I was told it was a high-level language.

00:05:45.680 --> 00:05:52.420
Yeah, there was a two-year period where C++ was the language of the future before they decided that that was Java.

00:05:52.420 --> 00:05:55.860
You guys all worked together on this project.

00:05:55.860 --> 00:05:57.260
How did you come to meet each other?

00:05:57.260 --> 00:06:00.260
One, I think, the university connection there.

00:06:00.260 --> 00:06:02.740
But how did you end up working on this project together?

00:06:02.740 --> 00:06:10.300
I don't know if it's worth Mark starting, because I think the link was – I can't actually remember exactly how Mark kind of got started with it.

00:06:10.300 --> 00:06:13.320
But, yeah, the link between Owen and I was through Owen's son.

00:06:13.320 --> 00:06:19.700
I think I found your GitHub page because it was shared on an EGT blog on Google+.

00:06:19.700 --> 00:06:21.300
Ah, yeah, yeah, yeah.

00:06:21.300 --> 00:06:26.020
And then I went over and I poked around a little bit and realized I could add a couple strategies right away.

00:06:26.020 --> 00:06:27.860
And I did that.

00:06:27.860 --> 00:06:30.780
And then I came back a couple months later and started working more on it.

00:06:30.780 --> 00:06:31.300
Yeah.

00:06:31.300 --> 00:06:32.580
And it just kind of snowballed from there.

00:06:32.580 --> 00:06:32.940
Okay.

00:06:32.940 --> 00:06:34.280
Yeah, really interesting.

00:06:34.520 --> 00:06:39.500
I think GitHub has – their slogan is like social coding or something like that.

00:06:39.500 --> 00:06:42.860
But I really do think it's connected people in some pretty powerful ways.

00:06:42.860 --> 00:06:43.580
It's amazing.

00:06:43.580 --> 00:06:56.260
For me, it's just been – this whole product has just been such an amazing learning experience because, as you can tell from Mark and Owen, they've got a far greater experience and knowledge of software engineering and coding and things like that.

00:06:56.260 --> 00:06:59.940
And so everything I've learned is from coding with these two guys, really.

00:06:59.940 --> 00:07:03.860
It's just kind of skyrocketed my own knowledge of writing code.

00:07:03.860 --> 00:07:04.440
So, yeah.

00:07:04.440 --> 00:07:06.320
And that's thanks to GitHub, in a way.

00:07:06.320 --> 00:07:06.880
Yeah, absolutely.

00:07:06.880 --> 00:07:07.440
Thanks to GitHub.

00:07:07.440 --> 00:07:07.860
Very cool.

00:07:07.860 --> 00:07:11.240
So let's start with what is game theory.

00:07:11.240 --> 00:07:20.000
So the thing that we're going to talk about is your library that solves or simulates, I guess, is really the way to think of it, a certain problem in game theory.

00:07:20.000 --> 00:07:20.940
But let's start at a high level.

00:07:20.940 --> 00:07:21.560
What's game theory?

00:07:21.560 --> 00:07:22.880
Okay.

00:07:22.880 --> 00:07:33.580
So it's loosely – loosely, it's kind of the study of interactions between different individuals or agents in some shape or form.

00:07:33.580 --> 00:07:38.600
And so it's kind of the study of how strategic interactions form.

00:07:38.600 --> 00:07:43.740
You know, like if I know what you know, what should I do at a very loose level?

00:07:43.740 --> 00:07:46.880
And hard to say when it really began.

00:07:46.880 --> 00:07:50.540
There's kind of different – people tell you different things about when it started.

00:07:50.540 --> 00:07:53.680
But the famous game theorist is John Nash.

00:07:53.680 --> 00:07:58.240
And he kind of found a particular solution concept to game theory.

00:07:58.240 --> 00:07:59.300
So, yeah.

00:07:59.300 --> 00:08:02.660
John Nash won the Nobel Prize for his work in economics.

00:08:02.660 --> 00:08:03.260
Right.

00:08:03.260 --> 00:08:03.460
That's right.

00:08:03.460 --> 00:08:04.580
I originally thought it was a Fields Medal.

00:08:04.580 --> 00:08:06.100
You guys said, no, no, no, no, no, no.

00:08:06.100 --> 00:08:07.240
Very cool.

00:08:07.240 --> 00:08:13.180
And he's also the subject of that movie, A Beautiful Mind, which was a great movie.

00:08:13.180 --> 00:08:16.400
So maybe tell us, like, what example was he working with?

00:08:16.400 --> 00:08:18.020
Do you remember what his work was on?

00:08:18.020 --> 00:08:18.420
Yeah.

00:08:18.420 --> 00:08:25.840
So he worked on Nash Equilibria, which is kind of a solution concept for in a strategic situation.

00:08:25.840 --> 00:08:33.640
There'll be a set of actions that all the players can take from which no one has a reason to do anything different.

00:08:33.640 --> 00:08:36.060
So there'll be a point at which we can't really improve.

00:08:36.420 --> 00:08:43.540
And his work was to show using a really clever proof that that point always exists.

00:08:43.540 --> 00:08:44.820
So it was an existence proof.

00:08:44.820 --> 00:08:49.620
And that's kind of snowballed as a solution concept for these strategic interactions.

00:08:49.620 --> 00:08:55.360
And so, yeah, in the movie, he has this kind of moment in the bar where he sees everything.

00:08:55.360 --> 00:09:06.140
And that's kind of the notion that if we all do what's best for ourselves, we'll actually not necessarily do what's best for everyone, which is a big theme in game theory.

00:09:06.140 --> 00:09:06.180
Right.

00:09:06.180 --> 00:09:07.480
Economics as well.

00:09:07.480 --> 00:09:14.920
And we can get into these local minima, right, where, like, nobody is benefiting from what is happening as much as they could be.

00:09:14.920 --> 00:09:16.920
But somebody has to move first.

00:09:16.920 --> 00:09:22.100
And so game theory kind of explores that space and finds these minimas.

00:09:22.100 --> 00:09:27.640
So if you consider these strategies or those strategies, you'll end up in this situation.

00:09:27.640 --> 00:09:29.340
And it's interesting in economics.

00:09:29.340 --> 00:09:31.140
It's interesting in public policy.

00:09:31.140 --> 00:09:33.420
And it's, you know, all sorts of places.

00:09:33.420 --> 00:09:33.680
Right.

00:09:33.680 --> 00:09:36.080
Mark, did you want to add something to the game theory thing?

00:09:36.160 --> 00:09:37.020
I think it's a good summary.

00:09:37.020 --> 00:09:40.720
The paper that Nash wrote is really famous because it's only one page long.

00:09:40.720 --> 00:09:44.180
And it has a huge number of citations.

00:09:44.180 --> 00:09:47.500
So it's kind of a story goes pretty deep.

00:09:47.500 --> 00:09:55.600
But, yeah, I think just adding on what you guys said, it's really fascinating to me that there can be two stable equilibria in a game.

00:09:55.600 --> 00:09:59.620
One which is really bad for everyone and one is really good for everyone.

00:09:59.620 --> 00:10:03.720
And whether or not those positions are stable or not is another big part of it.

00:10:04.220 --> 00:10:05.380
It's one thing to be at that equilibrium.

00:10:05.380 --> 00:10:08.340
It's another thing if you can be kind of easily nudged off of it.

00:10:08.340 --> 00:10:09.620
And that's the class.

00:10:09.620 --> 00:10:09.940
Yeah.

00:10:09.940 --> 00:10:10.160
Yeah.

00:10:10.160 --> 00:10:13.560
How hard is it and how seated are people in their ways?

00:10:13.560 --> 00:10:15.820
You know, you can see that looking at society sometimes.

00:10:15.820 --> 00:10:17.800
You're like, well, why are things structured this way?

00:10:17.800 --> 00:10:20.560
And you just, if people just did this, right?

00:10:20.620 --> 00:10:23.520
And so I think this is really interesting to explore that.

00:10:23.520 --> 00:10:28.520
Like one of the classical problems in this space and what is the focus of your library.

00:10:28.520 --> 00:10:30.700
So remind me, what's the exact name of your library?

00:10:30.700 --> 00:10:31.620
Axelrod?

00:10:31.620 --> 00:10:32.980
The Axelrod Library.

00:10:32.980 --> 00:10:33.760
Just Axelrod.

00:10:33.760 --> 00:10:34.040
Okay.

00:10:34.180 --> 00:10:36.160
I wasn't sure if there was like a dash or something.

00:10:36.160 --> 00:10:37.540
So the Axelrod Library.

00:10:37.540 --> 00:10:40.420
Let's start out with, what's the origin of that name?

00:10:40.420 --> 00:10:46.800
Axelrod is the name of a researcher who did an influential tournament several decades ago

00:10:46.800 --> 00:10:51.380
around the iterated Prisoner's Dilemma where some of the classic strategies like...

00:10:51.380 --> 00:10:53.220
So what's the Prisoner's Dilemma?

00:10:53.220 --> 00:10:55.660
Well, so it's named after Robert Axelrod.

00:10:55.660 --> 00:10:56.580
That's his full name.

00:10:56.580 --> 00:11:01.300
And the Prisoner's Dilemma is a particular situation in game theory where...

00:11:01.300 --> 00:11:02.940
A lot of academics run into this problem, right?

00:11:02.940 --> 00:11:04.420
They find themselves...

00:11:04.420 --> 00:11:05.140
Just kidding.

00:11:05.140 --> 00:11:05.860
Right, right.

00:11:05.860 --> 00:11:10.100
So the story behind the Prisoner's Dilemma is you've got two prisoners and I guess you've

00:11:10.100 --> 00:11:15.080
arrested both of them and they both have to decide whether they're going to rat on the other guy.

00:11:15.080 --> 00:11:15.320
Right.

00:11:15.320 --> 00:11:19.340
Some kind of co-conspirator, co-criminal sort of thing, right?

00:11:19.340 --> 00:11:19.700
Right.

00:11:20.000 --> 00:11:21.240
And if you both just...

00:11:21.240 --> 00:11:21.740
A gang.

00:11:21.740 --> 00:11:22.280
Right.

00:11:22.280 --> 00:11:27.180
And if you both just don't say anything, then they have some evidence and they can maybe

00:11:27.180 --> 00:11:29.220
get you with a low-level crime.

00:11:29.220 --> 00:11:32.520
But if one of you rats out on the other one, then they'll use that confession against them.

00:11:32.520 --> 00:11:37.620
So it's in the prosecution or the police's best interest to get you both to confess.

00:11:37.620 --> 00:11:41.560
And it's in both of your own individual best interest to say nothing.

00:11:41.560 --> 00:11:45.560
But they provide you with a big incentive to confess on the other person.

00:11:45.560 --> 00:11:49.920
They say, well, we'll let you go without any penalty if you'll just confess to the other

00:11:49.920 --> 00:11:50.300
person.

00:11:50.300 --> 00:11:52.160
So it's in your rational...

00:11:52.160 --> 00:11:55.180
It's in your own personal interest now to confess on the other person.

00:11:55.180 --> 00:12:00.120
But the problem is it's in their best interest to confess, to testify against you as well.

00:12:00.120 --> 00:12:02.800
And if you both confess, then it's bad for both of you, right?

00:12:02.800 --> 00:12:06.840
This is like one of the local minimums that you can get stuck in that's really bad.

00:12:06.840 --> 00:12:07.260
Right.

00:12:07.260 --> 00:12:07.500
Exactly.

00:12:07.500 --> 00:12:09.540
You both go into jail because you both don't want each other, yeah?

00:12:09.540 --> 00:12:10.300
Okay.

00:12:10.380 --> 00:12:16.680
And so the actual Rod library or the actual Rod project was around modeling the prisoner's

00:12:16.680 --> 00:12:17.200
dilemma, correct?

00:12:17.200 --> 00:12:17.600
Yeah.

00:12:17.600 --> 00:12:20.560
So one of the things is how do people end up cooperating?

00:12:20.560 --> 00:12:24.240
If we're all incentivized not to cooperate with each other, yet we look around, we see all

00:12:24.240 --> 00:12:26.720
these situations where people are cooperating.

00:12:27.160 --> 00:12:34.180
So can we devise strategies that when we play this game repeatedly that coerce or convince

00:12:34.180 --> 00:12:37.620
our partners that they're better off cooperating with us than defecting against us?

00:12:37.620 --> 00:12:37.900
Yeah.

00:12:37.900 --> 00:12:38.220
Okay.

00:12:38.380 --> 00:12:42.740
So let's think about some of the things that you can do with this library.

00:12:42.740 --> 00:12:45.800
So the actual Rod competition was held, you said, in the 80s.

00:12:45.800 --> 00:12:49.860
And you said the algorithms were lost or the strategies were lost to time.

00:12:49.860 --> 00:12:50.640
Is that true?

00:12:50.640 --> 00:12:52.160
There's some that can still be found.

00:12:52.160 --> 00:12:55.700
But I actually exchanged a very brief, when we started this library, I exchanged a very

00:12:55.700 --> 00:12:59.520
brief email conversation with Robert Axelrod saying, I don't suppose you happen to have

00:12:59.520 --> 00:13:01.260
any of these strategies kicking about.

00:13:01.260 --> 00:13:05.220
And from his very first tournament, he simply said, no, they're not around.

00:13:05.420 --> 00:13:08.560
And some of the others are written in, what, in Fortran, I believe?

00:13:08.560 --> 00:13:09.000
Yes.

00:13:09.000 --> 00:13:11.660
And they're not very well documented or anything like that.

00:13:11.660 --> 00:13:13.820
So they're quite difficult to follow.

00:13:13.820 --> 00:13:19.800
So some have published descriptions in various books and papers.

00:13:19.800 --> 00:13:24.720
But again, they're not usually sufficiently detailed to allow you to completely reproduce

00:13:24.720 --> 00:13:25.400
the strategy.

00:13:25.400 --> 00:13:25.840
Yeah.

00:13:25.840 --> 00:13:26.780
It's not.

00:13:26.780 --> 00:13:28.360
It's one thing to describe the concept.

00:13:28.360 --> 00:13:29.800
It's another to implement it, right?

00:13:29.800 --> 00:13:30.200
Yeah.

00:13:30.200 --> 00:13:30.620
Exactly.

00:13:30.620 --> 00:13:35.400
We had one poor individual at the PyCon UK Sprint last year who came to the

00:13:35.400 --> 00:13:40.160
work on the project and spent the entire day wading through the Fortran code for some

00:13:40.160 --> 00:13:42.600
strategy that he was trying to reimplement.

00:13:42.600 --> 00:13:47.420
I don't think he was expecting to spend the day working on Fortran at a PyCon.

00:13:47.420 --> 00:13:50.720
Well, how was your PyCon experience?

00:13:50.720 --> 00:13:53.400
Well, people were nice, but all I did was Fortran.

00:13:53.400 --> 00:13:54.300
It was dreadful.

00:13:54.300 --> 00:13:58.440
Let's talk a little bit about what this library does.

00:13:58.440 --> 00:14:01.660
It's like, so you can set up these things called strategies, right?

00:14:01.660 --> 00:14:05.080
And you can have players and tournaments.

00:14:05.080 --> 00:14:07.180
Maybe give us a sense for what the whole thing does.

00:14:07.180 --> 00:14:13.120
Well, in the end, each player that's participating has a very simple choice to make, which is either

00:14:13.120 --> 00:14:17.040
to cooperate with their fellow prisoner or to rat on them.

00:14:17.200 --> 00:14:20.540
So a simple choice between two options.

00:14:20.540 --> 00:14:28.840
And so what we do in the library is to say all of the different strategies that we know

00:14:28.840 --> 00:14:31.080
about, we play them against one another.

00:14:31.080 --> 00:14:32.820
And first of all, we do that in pairs.

00:14:32.820 --> 00:14:38.340
So we'll put a pair of players against one another and they'll play the game repeatedly over and

00:14:38.340 --> 00:14:39.140
over and over again.

00:14:39.400 --> 00:14:45.000
And then we'll just match up all the different combinations of pairs for however many players

00:14:45.000 --> 00:14:46.140
we're interested in.

00:14:46.140 --> 00:14:50.280
And we'll tot up all the scores and see who comes out on top.

00:14:50.280 --> 00:14:51.080
Okay, excellent.

00:14:51.080 --> 00:14:55.840
So give us a sense for some of the, you have some clever names for the different strategies

00:14:55.840 --> 00:14:56.840
or players, right?

00:14:56.840 --> 00:14:58.500
Strategy and player, it's kind of the same thing.

00:14:58.500 --> 00:15:02.500
So you've got like the basic ones, the cooperator and the defector.

00:15:02.500 --> 00:15:03.580
But what else?

00:15:03.580 --> 00:15:09.420
Well, probably the most famous one is the tit for tat strategy, because in Axelrod's

00:15:09.420 --> 00:15:13.620
original tournament, one of the interesting results that came out of his work was that

00:15:13.620 --> 00:15:16.200
this strategy was one of the most successful.

00:15:16.200 --> 00:15:20.260
I'm sure the other two will correct me, but possibly even the most successful of the lot.

00:15:20.260 --> 00:15:20.480
Yep.

00:15:20.480 --> 00:15:21.140
In his tournaments.

00:15:21.140 --> 00:15:21.380
Yep.

00:15:21.380 --> 00:15:21.640
Yep.

00:15:21.640 --> 00:15:21.920
All right.

00:15:21.920 --> 00:15:22.760
So what's tit for tat?

00:15:22.760 --> 00:15:27.120
If you rat on me next time, I'm ratting on you.

00:15:27.120 --> 00:15:27.480
I'm going to rat on you.

00:15:27.480 --> 00:15:27.900
Yeah.

00:15:27.900 --> 00:15:28.320
Yeah.

00:15:28.320 --> 00:15:30.380
And if you're nice to me, I'll be nice to you.

00:15:30.380 --> 00:15:38.420
So it simply, it looks at what its opponent did the last time they played and simply plays

00:15:38.420 --> 00:15:41.840
exactly the same choice that they made when it comes around to the next turn.

00:15:41.840 --> 00:15:47.860
So of course, if the opponent is being cooperative with it, then it will cooperate.

00:15:47.860 --> 00:15:53.160
So it is quite a cooperative strategy and it did very well in Axelrod's original tournament.

00:15:53.160 --> 00:15:53.760
Interesting.

00:15:53.760 --> 00:15:59.640
And then you've got the grudger, which if you ever turn on the grudger, you're going to regret

00:15:59.640 --> 00:16:00.040
it, right?

00:16:00.040 --> 00:16:00.920
Well, what else?

00:16:00.920 --> 00:16:03.200
There was a few other interesting ones.

00:16:03.200 --> 00:16:03.580
Yeah.

00:16:03.580 --> 00:16:08.780
There's a lot of variants of strategies where if you cross them once, they'll never cooperate

00:16:08.780 --> 00:16:09.440
with you again.

00:16:09.440 --> 00:16:11.400
Or maybe if you cross them, they'll forgive you once.

00:16:11.400 --> 00:16:13.580
But if you cross them twice, then they're like, okay.

00:16:13.580 --> 00:16:14.220
Interesting.

00:16:14.220 --> 00:16:17.460
And so implementing these strategies is actually pretty straightforward, right?

00:16:17.460 --> 00:16:20.880
It's like you could reasonably write a strategy in like five lines of Python.

00:16:20.880 --> 00:16:21.160
Yeah.

00:16:21.160 --> 00:16:21.720
Absolutely.

00:16:21.980 --> 00:16:27.700
And that's been one of the aims of the library is to make that as simple and straightforward

00:16:27.700 --> 00:16:28.940
as we possibly can.

00:16:29.100 --> 00:16:35.980
Because it's a great way for somebody who's new to programming or new to Python or new

00:16:35.980 --> 00:16:41.320
to making contributions to open source projects to get involved with something and to see an

00:16:41.320 --> 00:16:42.180
effect very quickly.

00:16:42.180 --> 00:16:47.020
You can invent a strategy that's not been thought of before very easily.

00:16:47.020 --> 00:16:51.080
You can code it up in a few lines and you can get it into the library and get it participating

00:16:51.080 --> 00:16:51.880
in the tournament.

00:16:51.880 --> 00:16:55.540
And it has been a first contribution for many, many people to open source.

00:16:55.800 --> 00:17:01.980
I gave a class on repeated games one morning and that afternoon, one of my students had written

00:17:01.980 --> 00:17:05.920
their first open source contribution with a little bit of help, but not much.

00:17:05.920 --> 00:17:06.620
So, yeah.

00:17:06.620 --> 00:17:07.500
I think that's really great.

00:17:07.500 --> 00:17:12.320
You know, a lot of people are super interested in contributing to open source, but I think it

00:17:12.320 --> 00:17:16.340
feels super daunting because they see all the things they're like, oh, Django, I'd love

00:17:16.340 --> 00:17:16.600
to.

00:17:16.600 --> 00:17:18.400
Oh my gosh, this is a big project.

00:17:18.400 --> 00:17:22.740
And if I break it, oh boy, Pinterest will be mad at me or whatever, right?

00:17:23.340 --> 00:17:28.420
Whereas here, really at its core, to submit, to contribute a strategy, all you're doing

00:17:28.420 --> 00:17:33.720
is writing a relatively simple, well, it can be simple or complicated, class method.

00:17:33.720 --> 00:17:35.760
Just one method is kind of all you need to do.

00:17:35.760 --> 00:17:35.980
Yeah.

00:17:35.980 --> 00:17:38.520
So, you know, if people are there listening and they're like, hey, I'd love to get started

00:17:38.520 --> 00:17:38.840
on this.

00:17:38.840 --> 00:17:40.780
You've got some interesting game theory.

00:17:40.780 --> 00:17:41.940
Definitely check this out.

00:17:41.940 --> 00:17:44.180
I'll be linking to the GitHub repo in the show notes.

00:17:44.180 --> 00:17:44.780
That'll be awesome.

00:17:44.780 --> 00:17:45.280
Cool.

00:17:45.280 --> 00:17:46.520
And we'd love to have them.

00:17:46.520 --> 00:17:47.660
We look forward to seeing them.

00:17:47.660 --> 00:17:48.800
That's awesome.

00:17:48.800 --> 00:17:51.020
You guys are going to be busy with pull requests, I think.

00:17:51.020 --> 00:17:52.360
Thanks.

00:17:52.360 --> 00:17:52.800
Fantastic.

00:17:52.800 --> 00:17:54.120
How many strategies do you have?

00:17:54.120 --> 00:17:54.300
Yeah.

00:17:54.300 --> 00:17:55.740
How many strategies do you have so far?

00:17:55.740 --> 00:18:00.900
In the current release, which is 2.6, we got 184 strategies.

00:18:00.900 --> 00:18:01.560
Wow.

00:18:01.560 --> 00:18:03.220
But a lot of those are parametrized.

00:18:03.220 --> 00:18:08.360
So, you know, you can, for example, a random strategy, we have a strategy that acts completely

00:18:08.360 --> 00:18:08.820
randomly.

00:18:08.820 --> 00:18:13.820
You can also pass a parameter of making it random where they select to cooperate with a

00:18:13.820 --> 00:18:14.460
given probability.

00:18:14.460 --> 00:18:18.880
So, in a way, we have pretty much an infinite number of strategies.

00:18:18.880 --> 00:18:22.520
So, you have like a two-thirds cooperator, one-third defector-like?

00:18:22.520 --> 00:18:22.940
Yeah.

00:18:22.940 --> 00:18:24.980
And much more complicated ones as well.

00:18:24.980 --> 00:18:25.480
Of course.

00:18:25.480 --> 00:18:30.080
So, what are some of the more, not convoluted, but involved ones, I guess?

00:18:30.080 --> 00:18:31.400
That's a good question.

00:18:31.400 --> 00:18:36.880
There's several that we've implemented from the literature that try to break deadlocking

00:18:36.880 --> 00:18:37.480
situations.

00:18:37.480 --> 00:18:43.320
So, one of the weaknesses of tit-for-tat is it can get in these confect, or these cooperate-defect

00:18:43.320 --> 00:18:47.720
cycles with an opponent that accidentally defects, or if the tournament is noisy or something

00:18:47.720 --> 00:18:48.240
like that.

00:18:48.240 --> 00:18:53.240
And so, if you can give it a small probability that it will forgive a defection, they can

00:18:53.240 --> 00:18:54.700
get back to cooperating again.

00:18:54.700 --> 00:18:59.660
So, there's a lot of strategies that implement little things like that, that attempt to get

00:18:59.660 --> 00:19:01.420
the players back on track.

00:19:01.660 --> 00:19:02.120
I see.

00:19:02.120 --> 00:19:06.540
Just kind of break, because if you just go really clearly down the cooperate-defect,

00:19:06.540 --> 00:19:10.060
grudge, you end up in these equilibrium really quickly, right?

00:19:10.060 --> 00:19:15.660
And so, as you make concessions over time or take more of it into account, it can probably

00:19:15.660 --> 00:19:16.480
be more interesting, huh?

00:19:16.480 --> 00:19:20.420
I was going to say, Mark, if you want to talk about even some of the more complex ones where

00:19:20.420 --> 00:19:24.700
we're using neural networks and we're training these strategies using genetic algorithms and

00:19:24.700 --> 00:19:25.320
things like that.

00:19:25.320 --> 00:19:25.960
Now, that is cool.

00:19:25.960 --> 00:19:26.160
Sure.

00:19:26.260 --> 00:19:28.400
You're probably best placed to talk about those.

00:19:28.400 --> 00:19:28.780
Sure.

00:19:28.780 --> 00:19:29.060
Yeah.

00:19:29.060 --> 00:19:32.300
So, one of the great things about the library is that we have so many strategies already,

00:19:32.300 --> 00:19:36.900
and you can use these to train new strategies using typical machine learning methods.

00:19:36.900 --> 00:19:42.300
So, we had a couple of really interesting contributions, I guess, almost a year ago from

00:19:42.300 --> 00:19:47.180
Martin Jones, who's, I guess, somewhat well-known in the Python community.

00:19:47.180 --> 00:19:53.260
And he submitted a strategy that used a lookup table that he had trained to find the optimal

00:19:53.260 --> 00:19:55.320
response against all the other strategies in the library.

00:19:55.720 --> 00:19:58.040
And he also submitted the first one that used the neural network.

00:19:58.040 --> 00:20:03.640
So, we've extended that method of taking a strategy based on some kind of machine learning

00:20:03.640 --> 00:20:09.180
algorithm, training it against the other strategies, and then adding it back to the tournament to

00:20:09.180 --> 00:20:09.740
see how it does.

00:20:09.740 --> 00:20:13.040
And right now, those are amongst the best players in the library in terms of performance.

00:20:13.040 --> 00:20:13.880
Oh, wow.

00:20:13.880 --> 00:20:14.260
Okay.

00:20:14.260 --> 00:20:14.900
How interesting.

00:20:14.900 --> 00:20:16.420
And is it adaptive?

00:20:16.420 --> 00:20:18.360
Or does it somehow identify?

00:20:18.360 --> 00:20:21.100
Like, okay, I think that thing I'm playing against, that's a grudger.

00:20:21.100 --> 00:20:23.020
So, I'm going to do a grudging thing.

00:20:23.020 --> 00:20:24.100
That's a great question.

00:20:24.300 --> 00:20:29.040
So, some of the strategies that try to adapt on the fly don't always do so well.

00:20:29.040 --> 00:20:32.660
But what you'll see is the trained strategies typically learn things.

00:20:32.660 --> 00:20:37.520
Like, if you defect too many times in a row, then they'll just decide that they're going

00:20:37.520 --> 00:20:38.100
to give up on you.

00:20:38.100 --> 00:20:40.700
That you're one of these strategies that is never going to cooperate with them.

00:20:40.820 --> 00:20:43.020
So, they'll just defect with you for the rest of the time.

00:20:43.020 --> 00:20:48.380
And we've got some population dynamics based ones as well, which we can talk about a little

00:20:48.380 --> 00:20:48.620
more.

00:20:48.620 --> 00:20:51.560
But they have to learn to play against themselves and the other players.

00:20:51.560 --> 00:20:58.020
And they end up evolving like a handshake so they can recognize other variants of themselves,

00:20:58.020 --> 00:21:00.700
but exclude opponents that aren't like them.

00:21:00.700 --> 00:21:01.180
Really?

00:21:01.180 --> 00:21:02.220
Wow.

00:21:02.340 --> 00:21:03.600
That is so interesting.

00:21:03.600 --> 00:21:06.240
What's really cool is that that kind of comes out of the training.

00:21:06.240 --> 00:21:07.700
So, we just train them.

00:21:07.700 --> 00:21:13.220
And so, we're not kind of designing that handshake, but we're seeing that handshake evolve, which

00:21:13.220 --> 00:21:13.740
is kind of cool.

00:21:13.740 --> 00:21:14.060
For sure.

00:21:14.180 --> 00:21:14.380
Okay.

00:21:14.380 --> 00:21:14.720
Yeah.

00:21:14.720 --> 00:21:15.660
That's incredible.

00:21:15.660 --> 00:21:16.840
I didn't really expect that.

00:21:16.840 --> 00:21:24.220
Like, you know, at Google last year, they came up with those two neural networks that would

00:21:24.220 --> 00:21:26.240
exchange encrypted data.

00:21:26.240 --> 00:21:31.720
And a third one that was trying to break it, the Adam Eve, the Alice Bob and Eve one, that

00:21:31.720 --> 00:21:37.120
one, that basically invented a way to talk to each other to do encryption.

00:21:37.120 --> 00:21:42.240
And it sounds like this is kind of like that a little bit in the sense that they like know

00:21:42.240 --> 00:21:43.200
how to play against each other.

00:21:43.280 --> 00:21:45.180
I mean, I'm not saying it's necessarily the same thing, but.

00:21:45.180 --> 00:21:47.460
I think it's in the similar vein.

00:21:47.460 --> 00:21:47.740
Yeah.

00:21:47.740 --> 00:21:51.040
It's a result of the same kinds of reinforcement training strategies.

00:21:51.040 --> 00:21:51.560
Okay.

00:21:51.560 --> 00:21:52.440
Interesting.

00:21:52.440 --> 00:21:55.700
But each of our matches is typically only 200 turns long.

00:21:55.700 --> 00:21:58.540
So, they've only got 200 bits of information to do this.

00:21:58.540 --> 00:22:01.600
So, not quite in the same league as what Google are doing.

00:22:01.600 --> 00:22:03.820
That's right.

00:22:03.820 --> 00:22:06.000
And the worst thing that'll happen is they'll win a tournament.

00:22:06.000 --> 00:22:09.760
We're not letting them just get the computers to talk to each other without us knowing what's

00:22:09.760 --> 00:22:10.120
going on.

00:22:10.120 --> 00:22:10.560
So, we're okay.

00:22:10.560 --> 00:22:11.260
Yeah.

00:22:11.980 --> 00:22:14.940
The singularity probably won't come from these strategies.

00:22:14.940 --> 00:22:16.320
Who knows?

00:22:16.320 --> 00:22:17.860
I mean, maybe.

00:22:17.860 --> 00:22:24.620
But if it's ever found on GitHub by one of these other AIs, it's going to bring it back

00:22:24.620 --> 00:22:25.720
in and it's going to be trouble.

00:22:25.720 --> 00:22:27.500
I don't know.

00:22:27.500 --> 00:22:30.380
If we can just get society into this local minimum, we have them.

00:22:31.240 --> 00:22:32.380
All right.

00:22:32.380 --> 00:22:35.180
So, the idea is you have these strategies, which are really interesting.

00:22:35.180 --> 00:22:39.420
And then you kind of do like a survival technique, right?

00:22:39.420 --> 00:22:42.960
You have them compete against each other over and over and over, 200 times, you say?

00:22:42.960 --> 00:22:49.200
And then you can draw really interesting conclusions like, well, we had this even distribution at

00:22:49.200 --> 00:22:50.920
the beginning of each type.

00:22:50.920 --> 00:22:53.160
But at the end, we had way more grudgers or whatever.

00:22:53.160 --> 00:22:54.740
Maybe talk about that a little bit.

00:22:55.120 --> 00:22:55.300
Yeah.

00:22:55.300 --> 00:22:59.160
There are a couple of variations of tournaments that you can play.

00:22:59.160 --> 00:23:02.400
And some of them play these population dynamics.

00:23:02.760 --> 00:23:09.560
So, in various different ways, at the end of each round, your score determines the size

00:23:09.560 --> 00:23:11.880
of your population going into the following round.

00:23:11.880 --> 00:23:17.680
And at the end of it, we look to see, well, who's died out, who survived through to the

00:23:17.680 --> 00:23:18.020
end?

00:23:18.020 --> 00:23:21.300
Has one strategy dominated the final population?

00:23:21.300 --> 00:23:26.740
Or have we ended up in an equilibrium of three equal populations with three different strategies?

00:23:27.400 --> 00:23:32.540
And that's actually, if you've seen the logo for the library, that sort of multicolored

00:23:32.540 --> 00:23:38.240
plot, that's a plot that comes out of one of these population dynamic tournaments.

00:23:38.240 --> 00:23:43.660
And it's showing one or two of those strategies are surviving very nicely through to the end

00:23:43.660 --> 00:23:44.280
and dominating.

00:23:44.280 --> 00:23:48.900
And a couple of them have died out and don't have very much population left at all.

00:23:48.900 --> 00:23:49.180
Yeah.

00:23:49.180 --> 00:23:51.980
And you have really nice visualizations of the results.

00:23:51.980 --> 00:23:53.140
I really like those.

00:23:53.140 --> 00:23:55.880
So, some of the tournaments you have, you say, round robin.

00:23:55.880 --> 00:23:58.880
You have spatial ones and probabilistic endings.

00:23:58.880 --> 00:24:00.360
What are those?

00:24:00.360 --> 00:24:05.860
Well, as I said earlier, the first thing that we do is to pitch two players against one another

00:24:05.860 --> 00:24:08.520
and then they compete for a certain number of rounds.

00:24:08.520 --> 00:24:14.640
Well, in most of the tournaments, that number of rounds is fixed and it's the same for each

00:24:14.640 --> 00:24:16.380
pair that plays.

00:24:16.380 --> 00:24:22.860
But in the probabilistic ending, you set a probability so that in any given round, there's a probability

00:24:22.860 --> 00:24:25.900
that that might be the last one in this match.

00:24:25.900 --> 00:24:30.200
So, the length of matches between different pairs varies.

00:24:30.200 --> 00:24:35.100
Whereas in all the other tournaments, they're all playing for exactly the same number of rounds.

00:24:35.100 --> 00:24:35.800
Okay, cool.

00:24:35.800 --> 00:24:39.700
Have you guys ever done anything like trying to study career criminals?

00:24:39.700 --> 00:24:41.560
So, ones that go back to jail and back.

00:24:41.640 --> 00:24:45.780
So, if you lose a round, you have to like sit out a round or something.

00:24:45.780 --> 00:24:51.360
Only the ones that win or they somehow get released back into the population based on how long they

00:24:51.360 --> 00:24:52.340
were sent to jail for.

00:24:52.340 --> 00:24:54.300
I think you need to put a PR through.

00:24:54.300 --> 00:24:55.940
I was going to say, yeah.

00:24:55.940 --> 00:24:57.000
PR is very welcome.

00:24:57.000 --> 00:24:57.640
Yeah.

00:24:58.120 --> 00:24:59.320
We don't have that.

00:24:59.320 --> 00:25:05.060
But actually, the library would allow for that to be implemented because we have these structures

00:25:05.060 --> 00:25:07.080
for how the tournaments are in these different tournaments.

00:25:07.080 --> 00:25:14.460
And so, you could theoretically build a tournament that has some sort of dropping out phase or things

00:25:14.460 --> 00:25:15.060
like that.

00:25:15.060 --> 00:25:16.160
Yeah, you could do that.

00:25:16.160 --> 00:25:16.700
Oh, yeah.

00:25:16.700 --> 00:25:17.220
Sounds interesting.

00:25:17.220 --> 00:25:17.420
Yeah.

00:25:17.420 --> 00:25:20.840
And there's lots of variants of the game that have been studied in the literature where other

00:25:20.840 --> 00:25:25.140
than corroborate and defect, you can also like intentionally punish your opponent by taking some

00:25:25.140 --> 00:25:26.880
of their points away or some of their money away.

00:25:26.880 --> 00:25:31.940
And these are, you know, usually done with small studies of undergraduate students to see if it

00:25:31.940 --> 00:25:32.980
improves the equilibrium.

00:25:32.980 --> 00:25:34.980
And we haven't gone down that.

00:25:34.980 --> 00:25:36.220
It was something we've talked about before.

00:25:36.220 --> 00:25:38.640
And like Ben said, it would be relatively easy to add to the library.

00:25:38.640 --> 00:25:43.120
But I think we've had so much fun with just the regular prisoner's dilemma that we haven't

00:25:43.120 --> 00:25:44.040
messed with that part of it.

00:25:44.040 --> 00:25:46.820
Yeah, you haven't had to go out there because it's still interesting to study.

00:25:46.820 --> 00:25:48.480
I do have access to students.

00:25:48.480 --> 00:25:49.180
That's true.

00:25:49.180 --> 00:25:58.240
We also have a whole set of strategies that we sort of collectively refer to as the cheaters.

00:25:58.240 --> 00:26:03.160
They don't conform to the rules of the tournament that Axelrod originally did.

00:26:03.160 --> 00:26:05.600
They do all sorts of things within the code.

00:26:05.600 --> 00:26:08.420
So they'll try to manipulate their opponent's code.

00:26:08.420 --> 00:26:12.420
They'll try to manipulate the history of the scores.

00:26:12.420 --> 00:26:14.240
And so we allow those strategies in.

00:26:14.240 --> 00:26:19.160
And so there's quite an interesting little tournament that just plays them.

00:26:19.160 --> 00:26:22.580
And sees, well, which is the best cheating strategy?

00:26:22.580 --> 00:26:28.600
So who's best at writing strategies that manipulate the internals of Python and mess about with

00:26:28.600 --> 00:26:30.220
their opponent's class methods?

00:26:30.220 --> 00:26:30.480
Yeah.

00:26:30.480 --> 00:26:31.640
So that's really interesting.

00:26:31.640 --> 00:26:34.520
The normal ones, they're provided.

00:26:34.520 --> 00:26:35.740
I mean, everybody's provided.

00:26:35.740 --> 00:26:40.220
But the normal ones look at like, what did my player, my opponent do last time?

00:26:40.220 --> 00:26:41.060
Did they defect?

00:26:41.060 --> 00:26:42.600
Or did they cooperate?

00:26:42.720 --> 00:26:43.740
And I'll do something.

00:26:43.740 --> 00:26:50.420
But the sneaky ones, the insider trading ones, if you will, they say, you know what?

00:26:50.420 --> 00:26:51.620
I actually have a self pointer.

00:26:51.620 --> 00:26:53.640
And my self pointer has the opponent.

00:26:53.640 --> 00:26:58.340
My opponent has a method that's called like respond.

00:26:58.880 --> 00:27:02.720
And you can just overwrite that method to say, return C.

00:27:02.720 --> 00:27:03.860
Exactly.

00:27:03.860 --> 00:27:05.180
Exactly.

00:27:05.180 --> 00:27:12.320
And it was a bit of a headache at one point, because as the library matured and became quite

00:27:12.320 --> 00:27:14.780
a good research tool, well, a very good research tool.

00:27:14.780 --> 00:27:19.720
We had these cheater strategies that were kind of all good fun and games at the start.

00:27:19.920 --> 00:27:25.000
But I think we made a good decision earlier on to bring in this classification of strategies.

00:27:25.000 --> 00:27:27.860
So you can't stumble on the cheaters by accident.

00:27:27.860 --> 00:27:29.420
But if you want to go get them, you can.

00:27:29.420 --> 00:27:33.340
And at the same time, there's loads of other things that we can classify strategies on.

00:27:33.340 --> 00:27:36.500
So for example, how long is their memory?

00:27:36.500 --> 00:27:39.020
Do they only look at what they did in the previous round?

00:27:39.020 --> 00:27:40.840
Or do they look all the way back to the beginning of the game?

00:27:40.840 --> 00:27:42.980
And you can kind of go and get strategies based on that.

00:27:42.980 --> 00:27:43.640
Oh, interesting.

00:27:43.640 --> 00:27:47.360
So basically, they provided a list of opponent responses.

00:27:47.360 --> 00:27:52.420
And you could say, like, always, you know, trim that list to like 10 or 5 or 1 types of

00:27:52.420 --> 00:27:53.080
responses, right?

00:27:53.080 --> 00:27:57.740
Only consider strategies that only look at, you know, that much memory, so to speak.

00:27:57.740 --> 00:27:58.160
Okay.

00:27:58.160 --> 00:27:59.780
Because that's another interesting thing.

00:27:59.780 --> 00:28:02.520
Talking about these sophisticated strategies that make use of neural nets and stuff.

00:28:02.520 --> 00:28:06.600
There's a lot of talk in the literature, in the research literature, about whether or not

00:28:06.600 --> 00:28:07.900
memory matters, you know?

00:28:07.900 --> 00:28:12.580
And so this particular paper that made a lot of noise recently, well, in 2012, was it?

00:28:12.580 --> 00:28:16.920
But they were like, actually, you know, any strategy can be beaten by a strategy that only

00:28:16.920 --> 00:28:17.960
has very little memory.

00:28:17.960 --> 00:28:22.800
And we're kind of seeing with the vast amount of strategies we've got that we can train strategies

00:28:22.800 --> 00:28:27.720
that can adapt and can learn and can be very powerful and very sophisticated.

00:28:27.720 --> 00:28:28.280
I see.

00:28:28.280 --> 00:28:33.620
So maybe like if you're going to write a strategy yourself where it's procedural and you write

00:28:33.620 --> 00:28:37.980
the code, like maybe it doesn't make sense to remember 200 steps and try to like, how

00:28:37.980 --> 00:28:38.680
do I fit that in?

00:28:38.680 --> 00:28:42.120
But if you give it to a deep learning neural network and go, here, this is what they did.

00:28:42.120 --> 00:28:43.780
What does that mean?

00:28:43.780 --> 00:28:47.740
Maybe it can actually find within that the patterns that we don't see.

00:28:47.740 --> 00:28:48.240
That's right.

00:28:48.240 --> 00:28:50.640
Mark wrote a paper on that with the coolest title ever.

00:28:50.640 --> 00:28:51.720
It's called The Art of War.

00:28:51.720 --> 00:28:52.100
So you could...

00:28:52.100 --> 00:28:54.620
That's awesome.

00:28:54.620 --> 00:28:58.000
We came up with a strategy that used a lot of memory that was very effective.

00:28:58.000 --> 00:29:02.940
So I think there's this, there's been this kind of longstanding folk idea in the game

00:29:02.940 --> 00:29:06.880
theory community that complicated strategies are more trouble than they're worth.

00:29:06.880 --> 00:29:09.900
And that's because in the beginning, Tit for Tat was winning all these tournaments.

00:29:09.900 --> 00:29:12.580
And it's a really simple strategy that only looks at their previous round.

00:29:12.580 --> 00:29:15.060
And people tried to do all these complicated heuristics and stuff like that.

00:29:15.060 --> 00:29:17.620
And it just didn't work because the other strategies were like, I don't know what this

00:29:17.620 --> 00:29:20.200
other guy's doing, but it's complicated.

00:29:20.200 --> 00:29:22.180
So I'm just going to defect against him and call it good.

00:29:22.180 --> 00:29:24.740
I was just a human.

00:29:24.740 --> 00:29:25.120
Come on.

00:29:25.120 --> 00:29:26.000
I can't, I can't be.

00:29:26.000 --> 00:29:26.440
Right.

00:29:26.440 --> 00:29:27.300
Right.

00:29:27.300 --> 00:29:31.440
But yeah, we've definitely seen some success with these longer, these strategies that can

00:29:31.440 --> 00:29:33.300
be trained to do something sophisticated.

00:29:33.300 --> 00:29:33.780
Yeah.

00:29:33.780 --> 00:29:37.380
And how, how intense are the AIs that, that you're putting in?

00:29:37.380 --> 00:29:38.860
Like, is this deep learning?

00:29:38.860 --> 00:29:40.420
Is this just a neural network?

00:29:40.420 --> 00:29:46.300
Does it involve things like TensorFlow and like GPUs or is it, is it pretty simple as far

00:29:46.300 --> 00:29:46.740
as those go?

00:29:46.740 --> 00:29:51.400
So the way we train all of them right now, I did play with using TensorFlow, but some of

00:29:51.400 --> 00:29:54.640
the other strategies, the only way we can really do it is with an evolutionary algorithm.

00:29:54.640 --> 00:29:56.640
Just mutate and see if it's good.

00:29:56.640 --> 00:30:01.920
And so all the strategies we train now use that method, even the neural network and the

00:30:01.920 --> 00:30:03.140
neural networks we use are pretty simple.

00:30:03.140 --> 00:30:05.040
They're, they're basically one layer deep.

00:30:05.040 --> 00:30:09.960
Can you give them access to, or have you given them access to previous tournaments and the

00:30:09.960 --> 00:30:14.600
whole outcome and letting them look at that or like letting them see more than the tournament

00:30:14.600 --> 00:30:15.240
that they're in?

00:30:15.480 --> 00:30:16.800
I haven't currently trained one.

00:30:16.800 --> 00:30:19.480
So I did train a neural network that tries to predict the next move.

00:30:19.480 --> 00:30:23.300
And I let it look at a lot of games for that, but then I had trouble turning that into a

00:30:23.300 --> 00:30:23.920
real strategy.

00:30:23.920 --> 00:30:30.280
And it turns out it was better just to take a neural network and directly evolve the parameters

00:30:30.280 --> 00:30:31.500
that do well on a tournament.

00:30:31.500 --> 00:30:36.020
And in that case, it doesn't actually look at the outcomes of third party games.

00:30:36.260 --> 00:30:36.460
I see.

00:30:36.460 --> 00:30:37.060
All right.

00:30:37.060 --> 00:30:41.140
You guys talk about a thing called Moran processes for population dynamics.

00:30:41.140 --> 00:30:43.380
What's the, what's the story there?

00:30:43.380 --> 00:30:44.200
What's that about?

00:30:44.200 --> 00:30:45.140
I'm not familiar with that.

00:30:45.140 --> 00:30:45.540
Sure.

00:30:45.540 --> 00:30:49.300
So this is a, I got the classic example of a population dynamic.

00:30:49.300 --> 00:30:52.600
You basically have a population of, in our case, in different strategies.

00:30:52.600 --> 00:30:57.540
But if you were thinking like animals, it would be the, you'd have a bunch of animals that

00:30:57.540 --> 00:31:01.100
have their different sizes, or they have some different characteristic, different eye color

00:31:01.100 --> 00:31:01.860
or something like that.

00:31:01.860 --> 00:31:06.260
And then they all interact with each other and they accrue fitness points.

00:31:06.260 --> 00:31:10.500
In this case, they're playing the prisoner's dilemma and whatever their score is, is their

00:31:10.500 --> 00:31:10.920
fitness.

00:31:10.920 --> 00:31:15.680
And then you select one of them to reproduce proportional to its fitness or score.

00:31:16.040 --> 00:31:20.460
And you randomly select one to, to die of natural causes, let's say.

00:31:20.460 --> 00:31:28.580
And so one step at a time, the population evolves as one strategy does better than the other

00:31:28.580 --> 00:31:28.980
ones.

00:31:28.980 --> 00:31:33.240
I mean, it depends on the intrinsics of the strategies themselves and also the makeup of

00:31:33.240 --> 00:31:33.700
their opponents.

00:31:33.700 --> 00:31:36.780
So if they play well with certain opponents, then they're better off and a population has

00:31:36.780 --> 00:31:38.040
a lot of those kinds of opponents.

00:31:38.040 --> 00:31:38.540
I see.

00:31:38.540 --> 00:31:43.260
So it's underlying dynamics that actually decide how the population of one type of thing,

00:31:43.260 --> 00:31:45.680
like the grudgers or whatever, grow or shrink over time.

00:31:45.820 --> 00:31:46.180
Yeah, exactly.

00:31:46.180 --> 00:31:50.460
And so the questions around that are, is a population stable to invasion by a mutant

00:31:50.460 --> 00:31:51.440
type, a different type?

00:31:51.440 --> 00:31:55.000
Or is a strategy good at taking over a population of another type?

00:31:55.000 --> 00:31:55.900
Oh, interesting.

00:31:55.900 --> 00:31:56.180
Okay.

00:31:56.180 --> 00:32:00.600
So yeah, something that sounds like it could be a cool variation on tournaments, maybe you've

00:32:00.600 --> 00:32:07.000
done this, is what if you introduce like an invasive species type?

00:32:07.000 --> 00:32:10.480
Like you run the tournament for like three fourths of the way through, and then you introduce

00:32:10.480 --> 00:32:15.260
some other strategy that wasn't initially there after like others have been killed off.

00:32:15.260 --> 00:32:15.940
Have you tried this?

00:32:15.940 --> 00:32:17.540
Oh, there's your second PR, I think.

00:32:17.540 --> 00:32:19.440
We're doing well here.

00:32:19.440 --> 00:32:20.020
Yeah.

00:32:20.020 --> 00:32:20.540
Yeah.

00:32:20.540 --> 00:32:20.800
Yeah.

00:32:20.800 --> 00:32:24.040
I think it would be cool because, because a lot of them, like if you look at the graphs,

00:32:24.040 --> 00:32:26.420
it's super hard to talk about pictures on the audio.

00:32:26.540 --> 00:32:32.020
But if you look at some of the diagrams, and I'll link to your presentation at PyCon, you'll

00:32:32.020 --> 00:32:35.020
see that in the beginning, there'll be all these different types, and some of them will

00:32:35.020 --> 00:32:36.660
die out after a while.

00:32:37.020 --> 00:32:41.060
And maybe the strategies that won when they were around are not the same strategies that

00:32:41.060 --> 00:32:42.320
win if some new type comes, right?

00:32:42.320 --> 00:32:42.960
Yeah.

00:32:42.960 --> 00:32:43.420
Okay.

00:32:43.420 --> 00:32:49.300
Like if you throw a cooperator into a room full of defectors, it won't do very well, because

00:32:49.300 --> 00:32:51.480
they'll rip it apart, basically.

00:32:51.480 --> 00:32:53.700
Exactly.

00:32:53.700 --> 00:32:54.960
Exactly.

00:32:54.960 --> 00:32:55.720
Okay.

00:32:55.720 --> 00:32:56.840
Quite interesting.

00:32:56.840 --> 00:32:57.140
Yeah.

00:32:57.140 --> 00:32:57.960
All right.

00:32:57.960 --> 00:33:00.220
So what did you use to visualize those results?

00:33:00.220 --> 00:33:01.300
Those graphs are really nice.

00:33:01.300 --> 00:33:03.040
Is that Matplotlib or something else?

00:33:03.040 --> 00:33:04.200
That's all Matplotlib.

00:33:04.200 --> 00:33:04.280
Matplotlib.

00:33:04.280 --> 00:33:04.800
Yeah, yeah.

00:33:04.800 --> 00:33:09.820
That was at the start, we made it so that you could easily get various outputs of the

00:33:09.820 --> 00:33:10.000
library.

00:33:10.000 --> 00:33:11.500
It's all just done in Matplotlib.

00:33:11.500 --> 00:33:11.920
Okay.

00:33:11.920 --> 00:33:12.280
Yeah.

00:33:12.280 --> 00:33:16.820
And there's two sub-libraries in the repository, one that does something called fingerprinting,

00:33:16.820 --> 00:33:21.260
which is this idea from the literature that gives you an image that corresponds to a strategy,

00:33:21.260 --> 00:33:23.640
which those are kind of neat to look at.

00:33:23.640 --> 00:33:29.180
And then there's another one that's been around for a while that looks at how often each strategy

00:33:29.180 --> 00:33:31.160
cooperates against each different given opponent.

00:33:31.680 --> 00:33:35.660
So we have a lot of other visualizations and they're mostly with Matplotlib as well, with

00:33:35.660 --> 00:33:37.260
the occasional Seaborn, I think, here and there.

00:33:37.260 --> 00:33:37.560
Okay.

00:33:37.560 --> 00:33:42.840
At the start, out of the three of us, I think it's fair to say I was the keenest to keep

00:33:42.840 --> 00:33:45.040
the library very light on dependencies.

00:33:45.040 --> 00:33:51.760
And so I was kind of a roadblock for quite a while with Mark and Owen more sensibly saying,

00:33:51.760 --> 00:33:54.520
no, let's use all these awesome libraries to make the code better.

00:33:54.520 --> 00:33:55.240
And I was like, no, no, no.

00:33:55.240 --> 00:33:57.300
Let's pure Python, pure Python.

00:33:57.300 --> 00:34:03.060
And so Matplotlib was introduced, but it had this really nasty little fix so that you could

00:34:03.060 --> 00:34:04.380
still install the library without it.

00:34:04.380 --> 00:34:09.560
And then eventually I understood what was the way to do things.

00:34:09.560 --> 00:34:13.780
And so a lot of code got a lot neater when we started making use of all the great libraries.

00:34:13.780 --> 00:34:14.360
That's nice.

00:34:14.360 --> 00:34:16.780
The fingerprinting is another good little story.

00:34:17.820 --> 00:34:20.440
I put an issue on the library.

00:34:20.440 --> 00:34:22.840
It must be about 12 months back now.

00:34:22.840 --> 00:34:24.420
One of the early ones, yeah.

00:34:24.420 --> 00:34:30.160
Just a bright little idea, I thought, which was that we were starting to get people contributing

00:34:30.160 --> 00:34:32.600
strategies other than the three of us.

00:34:32.600 --> 00:34:39.220
And I was starting to find it tricky to work out, is this strategy the same as one that we've

00:34:39.220 --> 00:34:40.120
already got?

00:34:40.120 --> 00:34:44.440
And this is back in the days when I think we only had about 30 or 40 strategies.

00:34:44.440 --> 00:34:50.840
And I was starting to find it tricky then because, of course, it's not enough simply to look

00:34:50.840 --> 00:34:57.980
at the code because the same decisions can be coded in all sorts of different ways.

00:34:57.980 --> 00:35:05.300
So it struck me that we could do with a better way of trying to decide whether a given strategy

00:35:05.300 --> 00:35:09.260
was at least similar to one that we'd already seen.

00:35:09.260 --> 00:35:15.660
And I naively typed that up into an issue on the repository and said something like, maybe

00:35:15.660 --> 00:35:20.700
we should just find a handful of strategies that we play them against and we can do something

00:35:20.700 --> 00:35:26.360
clever mathematically to work out how similar they look, thinking that my friends, the mathematicians

00:35:26.360 --> 00:35:27.880
would go, yeah, that's a great idea.

00:35:27.880 --> 00:35:29.060
I'll do that this afternoon.

00:35:30.560 --> 00:35:34.060
Oh, no, they say this is a significant mathematical challenge.

00:35:34.060 --> 00:35:40.340
And 12 months later, here we are with some poor student of Vince's doing it as his final

00:35:40.340 --> 00:35:40.900
year project.

00:35:40.900 --> 00:35:46.300
The interesting thing is that that poor student is actually Owen's son.

00:35:46.300 --> 00:35:49.780
He's now doing his final year project with me.

00:35:49.780 --> 00:35:54.920
And so he's implemented this graphical approach from the literature, which has its strengths

00:35:54.920 --> 00:35:55.380
and weaknesses.

00:35:55.380 --> 00:35:59.560
But actually, I think this might be news to both Mark and Owen, as well as all the listeners,

00:35:59.560 --> 00:36:05.380
is he's just been working on a cleverer way of doing it, building a statistical model that

00:36:05.380 --> 00:36:07.300
will spit out if two strategies are different or not.

00:36:07.300 --> 00:36:10.660
So yeah, hopefully we'll be able to talk about that a bit more at some point.

00:36:10.660 --> 00:36:13.460
Well, I think that's really funny that you just throw that out there.

00:36:13.460 --> 00:36:16.800
Like, ah, you guys could just like compare strings or something and see.

00:36:16.800 --> 00:36:19.600
No, no, this is actually a lot of work.

00:36:19.600 --> 00:36:25.340
But on the other hand, it's super cool that something constructive as a project is coming

00:36:25.340 --> 00:36:25.840
out of it, right?

00:36:25.840 --> 00:36:27.260
Like, that's interesting.

00:36:27.260 --> 00:36:31.760
And things have worked the other way, too, where I've gone like, oh, we got to do this

00:36:31.760 --> 00:36:32.020
thing.

00:36:32.020 --> 00:36:33.620
It's probably going to be really, really hard.

00:36:33.620 --> 00:36:38.120
And then Owen and Mark are going, oh, you just need to use a class, something, something,

00:36:38.120 --> 00:36:38.480
something.

00:36:38.480 --> 00:36:39.800
And like, it's two lines of Python.

00:36:39.800 --> 00:36:40.680
I'm like, oh, okay, cool.

00:36:40.680 --> 00:36:42.740
I can actually do that in a less comprehension.

00:36:42.740 --> 00:36:43.140
We're good.

00:36:44.960 --> 00:36:49.020
Have you thought of involving like deep learning and the classification of similarities?

00:36:49.020 --> 00:36:50.140
Like, that would be interesting.

00:36:50.140 --> 00:36:54.580
I did write a blog post a couple of years ago where I derived a bunch of features of the

00:36:54.580 --> 00:36:57.060
strategies and the tournaments and did a little bit with it.

00:36:57.060 --> 00:37:01.460
But that was back right when we had just about 100 strategies and we weren't calculating as

00:37:01.460 --> 00:37:01.800
many things.

00:37:01.800 --> 00:37:04.380
So that's definitely worth a revisit, see if we can do more.

00:37:04.380 --> 00:37:08.560
That's actually kind of, that was the plan with this poor final year student.

00:37:08.560 --> 00:37:14.360
But actually, we've kind of got some really promising results using rather naive statistical

00:37:14.360 --> 00:37:14.760
models.

00:37:14.760 --> 00:37:18.560
So we need to look into that a bit more, but it doesn't look like we need to be too

00:37:18.560 --> 00:37:19.060
sophisticated.

00:37:19.060 --> 00:37:23.020
Again, because of the way the tournament is now and the, sorry, the way the library is

00:37:23.020 --> 00:37:27.320
now, it can just create so much data for us, which is a real strength.

00:37:27.320 --> 00:37:28.180
Oh, that's excellent.

00:37:28.180 --> 00:37:33.080
I think this is a real service, not just for Owen's son and your student in this particular

00:37:33.080 --> 00:37:39.240
case, but I think that it's, this kind of stuff is really valuable to students who are

00:37:39.240 --> 00:37:42.120
coming out of school and they want to get a job, right?

00:37:42.120 --> 00:37:44.240
If they come in and say, well, what do you know about programming?

00:37:44.240 --> 00:37:49.800
Would you like to see my project on GitHub and maybe the paper that was published built

00:37:49.800 --> 00:37:50.260
off of it?

00:37:50.260 --> 00:37:52.360
And here's my optimizations I've done.

00:37:52.360 --> 00:37:56.340
And like, if you can provide that to students, then they're in such a better place when they

00:37:56.340 --> 00:37:58.040
go out and say, Hey, I want a job.

00:37:58.580 --> 00:38:00.900
We actually had a student turn up today or yesterday.

00:38:00.900 --> 00:38:01.360
I forget.

00:38:01.360 --> 00:38:04.140
And she, you asked, didn't you, Owen?

00:38:04.140 --> 00:38:05.600
Like, how did everyone find this library?

00:38:05.600 --> 00:38:06.920
Because we've had quite a few people now.

00:38:06.920 --> 00:38:12.280
And she said, Oh, actually, as part of my course, I have to contribute to an open source project.

00:38:12.280 --> 00:38:16.620
And, you know, she'd found the actual project and seen the documentation and the community.

00:38:16.620 --> 00:38:18.600
And she's decided to choose us, which is great.

00:38:18.720 --> 00:38:19.000
Oh, wow.

00:38:19.000 --> 00:38:19.640
That's really cool.

00:38:19.640 --> 00:38:23.280
Think of how the world is going to be after 10 years of this kind of stuff.

00:38:23.280 --> 00:38:26.680
Like all the students have to contribute to open source and they have to somehow be really

00:38:26.680 --> 00:38:27.360
aware of this.

00:38:27.360 --> 00:38:29.940
And they're totally familiar with, that's your pull request.

00:38:29.940 --> 00:38:30.720
They know what that means.

00:38:30.720 --> 00:38:35.520
Like that's just, you know, it's already got so much momentum, but I could see a world

00:38:35.520 --> 00:38:40.580
where it's like, we'd look back at these as quaint days when open source was small.

00:38:40.580 --> 00:38:44.860
Another good kind of side to this project is in terms of academic research.

00:38:44.860 --> 00:38:48.300
So going back to the fact that Axelrod's original tournaments can't really be reproduced and there's

00:38:48.300 --> 00:38:49.560
loads of papers that can't be reproduced.

00:38:49.560 --> 00:38:55.300
That's like a problem in modern research where so much software is needed and written, but

00:38:55.300 --> 00:38:58.140
it's not necessarily written in a sustainable way.

00:38:58.140 --> 00:39:02.760
And so the Axelrod project is a really good example of maybe I'm a bit biased, obviously,

00:39:02.760 --> 00:39:06.820
but I'd say best practice in terms of, you know, our code isn't only written and documented

00:39:06.820 --> 00:39:11.480
well, but it's extremely well tested and it's constantly being refactored and improved.

00:39:11.480 --> 00:39:14.520
And it's all, you know, a wonderful example of how you should be doing research.

00:39:14.520 --> 00:39:14.800
Yeah.

00:39:14.800 --> 00:39:17.340
You guys have a nice read the docs and all sorts of stuff, right?

00:39:17.340 --> 00:39:21.720
But just to go into Owen's point, it's pretty rare for academic code to be even sufficiently

00:39:21.720 --> 00:39:25.460
documented to use it again, let alone enhance it or anything like that.

00:39:25.460 --> 00:39:29.020
It's just because the journals don't require it a lot of times when you publish it.

00:39:29.020 --> 00:39:35.700
And two, I think the way we have our test suite, we really try to test every stated behavior of

00:39:35.700 --> 00:39:40.720
every strategy so that we know that it is as close to the real deal as we can get it.

00:39:40.720 --> 00:39:41.720
That is really awesome.

00:39:41.720 --> 00:39:42.560
And I think it's great.

00:39:42.560 --> 00:39:47.660
And you have sort of the case study in this example of the way not to do it and the way

00:39:47.660 --> 00:39:49.960
to do it all wrapped into one, right?

00:39:50.020 --> 00:39:53.400
With the actual rod original tournaments and people are like, we don't even know if we

00:39:53.400 --> 00:39:54.680
can reproduce these strategies.

00:39:54.680 --> 00:39:57.240
And yet there's, there are papers and stuff written on it, right?

00:39:57.240 --> 00:39:57.860
Absolutely.

00:39:57.860 --> 00:40:00.220
And so you're like, well, how do you validate that science, right?

00:40:00.220 --> 00:40:05.800
That's been a real eye opener to me as somebody that's, that's got no academic background whatsoever

00:40:05.800 --> 00:40:10.320
since the day I graduated and vowed never to sit an exam again in my life.

00:40:10.320 --> 00:40:19.220
To come back into it, albeit on the fringes, however many years later, and find that there

00:40:19.220 --> 00:40:24.060
are papers and papers and papers written on the back of software that isn't accessible.

00:40:24.480 --> 00:40:27.560
And I look at that and say, well, how is that reproducible?

00:40:27.560 --> 00:40:28.700
How is that scientific?

00:40:28.700 --> 00:40:35.240
And it's been, it's been a real shock to me to find that we in this little project have

00:40:35.240 --> 00:40:38.460
so much of a struggle to try and reproduce other people's results.

00:40:38.460 --> 00:40:38.700
Yeah.

00:40:38.700 --> 00:40:41.360
But I think what you guys are doing, I mean, obviously it's open source.

00:40:41.360 --> 00:40:43.900
It's got version control all the way through.

00:40:43.900 --> 00:40:46.920
So somebody can go back, well, this paper was written on this version.

00:40:46.920 --> 00:40:50.200
It's not just, I still have the library, but I have exactly the same code.

00:40:50.200 --> 00:40:51.500
I think that's interesting.

00:40:51.660 --> 00:40:56.500
You know, I was talking to Kyle Cranmer from the Large Hadron Collider, and he was talking

00:40:56.500 --> 00:41:01.100
about how this is the problem in particle physics as well, just like reproducible science in general.

00:41:01.100 --> 00:41:06.680
And they do interesting things like have kind of an escrow service to hold the code tied to

00:41:06.680 --> 00:41:08.160
the paper and stuff like that.

00:41:08.160 --> 00:41:13.160
So I think, I think there's people trying different things from different angles, but yeah, very nice

00:41:13.160 --> 00:41:13.420
work.

00:41:13.420 --> 00:41:17.560
So what's the computational story here?

00:41:17.560 --> 00:41:21.520
You have 183 strategies, 200 rounds.

00:41:21.520 --> 00:41:23.580
They all fight with each other in certain ways.

00:41:23.580 --> 00:41:26.860
Like how long does it take to run a typical tournament?

00:41:26.860 --> 00:41:30.540
Is it pretty quick or is this like, you got to go have lunch?

00:41:30.540 --> 00:41:35.780
Well, the full tournament on all the strategies takes about an hour on a typical machine.

00:41:35.780 --> 00:41:37.440
So does that fit with that?

00:41:37.440 --> 00:41:40.520
Vince has some nice 16 core machines that can really chew through it.

00:41:40.680 --> 00:41:40.920
Yeah.

00:41:40.920 --> 00:41:45.520
So on that, it's still just under an hour for the full noisy tournament, which is the

00:41:45.520 --> 00:41:46.780
one that takes the longest amount of time.

00:41:46.780 --> 00:41:48.540
Because that's another thing that's inbuilt in the library.

00:41:48.540 --> 00:41:50.400
It's completely paralyzed.

00:41:50.400 --> 00:41:53.900
So you can play all these matches in parallel and the library takes care of that for you.

00:41:53.900 --> 00:41:55.680
So you can use up as many cores as you can get.

00:41:55.680 --> 00:41:55.900
Okay.

00:41:55.900 --> 00:41:56.480
That's cool.

00:41:56.640 --> 00:42:00.060
And some of the classifications, if the strategies are deterministic, then we don't have to

00:42:00.060 --> 00:42:00.900
repeat those matches.

00:42:00.900 --> 00:42:06.120
But if the tournament's noisy or the strategies are stochastic, then we have to repeat those

00:42:06.120 --> 00:42:06.800
more times.

00:42:06.800 --> 00:42:10.780
So depending on exactly what strategies you choose, it can be extremely fast if they're

00:42:10.780 --> 00:42:15.000
all deterministic or it can take a lot more time if it's like a noisy tournament.

00:42:15.000 --> 00:42:15.340
Okay.

00:42:15.340 --> 00:42:20.840
So have you guys tried to do any fancy tricks to make it faster or something like that?

00:42:20.840 --> 00:42:26.160
You talked about figuring out if it's deterministic and then going, okay, it already knows

00:42:26.160 --> 00:42:26.940
the answer to this.

00:42:26.940 --> 00:42:32.000
I would want to very cautiously say that I think we're already doing a fair few of these

00:42:32.000 --> 00:42:36.700
tricks, you know, like the parallelization, like vectorizing things where we can.

00:42:36.700 --> 00:42:39.420
But again, if anyone wants to take a look and make it faster.

00:42:39.420 --> 00:42:43.660
Writing the multiprocessing is several days of my life I'll never get back and I don't

00:42:43.660 --> 00:42:44.380
wish to repeat.

00:42:44.380 --> 00:42:45.920
Yeah.

00:42:45.920 --> 00:42:49.060
And as someone else who worked on that part of it, wow, yes.

00:42:49.060 --> 00:42:50.300
Yeah.

00:42:50.300 --> 00:42:53.060
I think we all kind of broke our knuckles on that for a bit.

00:42:53.060 --> 00:42:53.900
Yeah, that was...

00:42:53.900 --> 00:42:55.040
Yeah, that sounds really cool.

00:42:55.160 --> 00:42:57.140
Of course you want it to go as fast as possible, right?

00:42:57.140 --> 00:43:01.380
Like, you know, maybe somehow bringing some NumPy scipy into it.

00:43:01.380 --> 00:43:04.000
I don't really know or who knows.

00:43:04.000 --> 00:43:07.860
But it seems like an interesting computational challenge in its own right.

00:43:07.860 --> 00:43:08.200
Yeah.

00:43:08.200 --> 00:43:10.540
And you guys have battled it some, it sounds like.

00:43:10.540 --> 00:43:10.760
Yeah.

00:43:10.760 --> 00:43:13.020
And part of it is memory overhead as well.

00:43:13.220 --> 00:43:16.460
If you're batching out the matches, you can do them in parallel.

00:43:16.460 --> 00:43:21.660
But if you do too many of them, then you exhaust your memory, you hit swap, and then you're

00:43:21.660 --> 00:43:22.520
done for, you know?

00:43:22.520 --> 00:43:23.160
Yeah.

00:43:23.160 --> 00:43:27.320
That was a big bit of work when all of a sudden we realized that like we were using more than

00:43:27.320 --> 00:43:29.300
16 gigs of RAM and we kind of did it all.

00:43:29.300 --> 00:43:32.040
So now it's basically got no memory fingerprint anymore.

00:43:32.040 --> 00:43:34.160
But we do it all to file.

00:43:34.160 --> 00:43:35.560
But I'd forgotten about that.

00:43:35.560 --> 00:43:35.800
Yeah.

00:43:35.800 --> 00:43:36.240
Yeah.

00:43:36.240 --> 00:43:38.280
Yeah.

00:43:38.280 --> 00:43:41.440
It sounds just like an interesting algorithmic problem.

00:43:41.440 --> 00:43:46.040
And then, you know, you've got the known structure to deal with and stuff.

00:43:46.040 --> 00:43:49.700
But then people are throwing these strategies at you that you have no idea on how they're

00:43:49.700 --> 00:43:50.200
going to behave.

00:43:50.200 --> 00:43:51.620
And so that's like an interesting twist.

00:43:51.620 --> 00:43:53.160
It just, it sounds like a fun problem to me.

00:43:53.160 --> 00:43:53.600
Yeah.

00:43:53.600 --> 00:43:55.660
We do label the strategies too if they have a long run time.

00:43:55.660 --> 00:44:00.300
So some of them take 10 times, 100 times longer in whatever computations they do than

00:44:00.300 --> 00:44:01.120
say tit for tat.

00:44:01.120 --> 00:44:05.320
So that's another thing that we've studied, optimizing specific strategies.

00:44:05.740 --> 00:44:07.360
There's a couple of really cool strategies in the library.

00:44:07.360 --> 00:44:08.380
They're called meta strategies.

00:44:08.380 --> 00:44:14.840
And they basically take a large group of the other strategies in the library, maybe all

00:44:14.840 --> 00:44:17.500
of them, and then take the consensus vote or something like that.

00:44:17.500 --> 00:44:21.360
So they, by definition, are 100 times slower than the other ones.

00:44:21.360 --> 00:44:22.020
Yeah.

00:44:22.020 --> 00:44:23.060
Because they just keep rerunning.

00:44:23.060 --> 00:44:24.580
That's a pretty interesting idea.

00:44:24.580 --> 00:44:27.920
It's almost like the politician thing.

00:44:27.920 --> 00:44:31.740
You see what people are asking or what they want and just like, I'm just going to do that.

00:44:31.740 --> 00:44:35.720
Well, however the wind is blowing, that's how we're going to cooperate here or whatever.

00:44:36.680 --> 00:44:37.040
Yeah.

00:44:37.040 --> 00:44:40.440
So are there interesting combinations that work well?

00:44:40.440 --> 00:44:41.940
Have you tried that?

00:44:41.940 --> 00:44:46.080
Instead of running all of them, go like, I think this set of them or that set, like this

00:44:46.080 --> 00:44:50.820
mixture of like, I'll ask these 10 and see what I get.

00:44:50.820 --> 00:44:51.980
I have played with it a lot.

00:44:52.080 --> 00:44:55.960
So that was one of the best strategies in the early days, these meta strategies.

00:44:55.960 --> 00:44:59.940
And of course, you know, if you give them like the top five other strategies in the library,

00:44:59.940 --> 00:45:00.740
then it does well.

00:45:00.740 --> 00:45:06.020
But one change we made recently is rather than take the best performer of all the strategies

00:45:06.020 --> 00:45:11.120
in its set, if you randomly take one of the top 10 performers, it actually works better.

00:45:11.120 --> 00:45:15.280
And if you always go after that top one, which is sort of a weird quirk of like an ensemble

00:45:15.280 --> 00:45:16.200
machine learning method.

00:45:16.200 --> 00:45:16.520
Yeah.

00:45:16.520 --> 00:45:16.940
Yeah.

00:45:16.940 --> 00:45:17.740
Very interesting.

00:45:17.740 --> 00:45:20.000
You can study a lot of cool stuff with us.

00:45:20.000 --> 00:45:20.580
It sounds like.

00:45:20.580 --> 00:45:25.760
So maybe give us some stories like where this has been used.

00:45:25.760 --> 00:45:32.140
Like, obviously, if any of us or any two of us, I should say, get like captured by the police

00:45:32.140 --> 00:45:35.460
and charged with a crime, we'll know what to do.

00:45:35.460 --> 00:45:38.800
We'll have a more informed view on what to do.

00:45:38.800 --> 00:45:40.880
But, you know, it's not just for prisoners, right?

00:45:40.880 --> 00:45:42.340
It applies to other things as well.

00:45:42.340 --> 00:45:42.660
Yeah.

00:45:42.660 --> 00:45:46.920
And Owen added a pretty interesting strategy not that long ago where you can play as a

00:45:46.920 --> 00:45:47.120
human.

00:45:47.120 --> 00:45:51.580
So you can pick the strategy you want to play against and you can submit C or D and practice

00:45:51.580 --> 00:45:53.800
your one-on-one skills versus these strategies.

00:45:53.800 --> 00:45:55.580
Nice.

00:45:55.580 --> 00:45:58.260
That could have been an Atari game when it first came out.

00:45:58.260 --> 00:46:06.880
One of the kind of applications of this study is to understand the evolution of social norms.

00:46:06.880 --> 00:46:11.880
So, you know, like as Mark said at the beginning, you know, why when you look around, are we cooperative?

00:46:11.880 --> 00:46:18.720
And then that kind of steps onto other fields of evolutionary game theory that are then applied

00:46:18.720 --> 00:46:21.580
in like mathematical oncology and things like that.

00:46:21.580 --> 00:46:25.660
So these ideas are all interconnected and have wide-ranging applications.

00:46:25.660 --> 00:46:26.020
Yeah.

00:46:26.020 --> 00:46:26.260
Okay.

00:46:26.260 --> 00:46:26.580
Cool.

00:46:26.580 --> 00:46:31.780
You said that there's some applications in mathematical oncology for cancer.

00:46:31.780 --> 00:46:32.760
Yeah.

00:46:32.760 --> 00:46:33.220
What's that?

00:46:33.220 --> 00:46:38.380
That's kind of not immediately looking at the prison's dilemma, but the same ideas kind

00:46:38.380 --> 00:46:46.120
of transpire that you can see the spread of a tumor inside a health body in the same way

00:46:46.120 --> 00:46:51.360
that you can think of introducing a particular strategy into another population.

00:46:51.360 --> 00:46:57.280
And so things like Moran processes and evolutionary dynamics and population dynamics have been used

00:46:57.280 --> 00:46:59.340
to understand, yeah, the spread of cancer.

00:46:59.340 --> 00:47:00.940
And so mathematical oncology.

00:47:00.940 --> 00:47:01.240
Yeah.

00:47:01.240 --> 00:47:06.360
You can kind of think of cancer as a cooperator that has switched strategies to a defector

00:47:06.360 --> 00:47:08.120
and is now trying to take over.

00:47:08.120 --> 00:47:11.920
And so can it invade that population or not?

00:47:11.920 --> 00:47:12.440
Oh, wow.

00:47:12.440 --> 00:47:12.880
Okay.

00:47:12.880 --> 00:47:17.840
It's really interesting when these things make connections to somewhere where they're totally

00:47:17.840 --> 00:47:18.640
unexpected, right?

00:47:18.640 --> 00:47:19.160
Yeah.

00:47:19.160 --> 00:47:22.880
Like people working, you know, I'm sure Nash wasn't thinking about cancer when he worked on

00:47:22.880 --> 00:47:24.260
his game theory ideas.

00:47:24.260 --> 00:47:26.380
Nash got his Nobel Prize in economics, right?

00:47:26.380 --> 00:47:32.020
And the ideas that he was thinking about when he first wrote his paper and his thesis, they

00:47:32.020 --> 00:47:34.440
kind of just rolled really.

00:47:34.440 --> 00:47:35.620
And he didn't really see it coming.

00:47:35.620 --> 00:47:36.860
Well, I'll say that.

00:47:36.860 --> 00:47:38.000
I don't know that for sure.

00:47:38.000 --> 00:47:40.480
But it certainly wasn't what he was thinking of when he was doing the work.

00:47:40.480 --> 00:47:40.860
Yeah.

00:47:40.860 --> 00:47:41.220
Yeah.

00:47:41.220 --> 00:47:41.420
Yeah.

00:47:41.420 --> 00:47:41.920
Very interesting.

00:47:42.140 --> 00:47:42.260
Yeah.

00:47:42.260 --> 00:47:43.520
And it's still using economics now.

00:47:43.520 --> 00:47:48.680
So there's this idea, this like first approximation that everyone does what's best for them.

00:47:48.680 --> 00:47:51.620
Like they take all the available information and they say, it's best for me to invest in

00:47:51.620 --> 00:47:52.020
this thing.

00:47:52.020 --> 00:47:53.320
And that's not what most people do.

00:47:53.320 --> 00:47:57.060
Most people just copy someone else's strategy or someone else's advice.

00:47:57.060 --> 00:48:01.740
So it's more of an evolutionary process than rational process that we would think of it.

00:48:01.740 --> 00:48:02.040
Sure.

00:48:02.200 --> 00:48:07.660
Does it appear in politics or is politics too slow of a process, right?

00:48:07.660 --> 00:48:13.200
Like every four years, every six years, like is that enough for people to see these strategies

00:48:13.200 --> 00:48:13.640
and evolve?

00:48:13.640 --> 00:48:14.200
Or what do you think?

00:48:14.200 --> 00:48:17.520
Well, I know there's papers that are written in political science that use this kind of stuff,

00:48:17.520 --> 00:48:20.840
but I don't know how well applied to the real world they are.

00:48:20.840 --> 00:48:21.980
They might just be theoretical.

00:48:21.980 --> 00:48:26.960
I know there were lots of blog posts written about game theory and Brexit.

00:48:26.960 --> 00:48:28.980
So some combination.

00:48:28.980 --> 00:48:29.480
Yeah.

00:48:29.480 --> 00:48:30.980
I was just saying, well, I was thinking of Brexit.

00:48:30.980 --> 00:48:32.960
I was thinking of the US presidential election.

00:48:32.960 --> 00:48:35.260
Like both of these are unexpected.

00:48:35.260 --> 00:48:38.500
And I'm wondering if game theory has interesting things to say about them.

00:48:38.500 --> 00:48:39.360
Don't blame us.

00:48:39.360 --> 00:48:40.700
I'm not blaming you guys.

00:48:40.700 --> 00:48:43.960
I was actually in London when the Brexit vote happened.

00:48:43.960 --> 00:48:46.420
And it was very interesting to walk the streets of London the day after.

00:48:46.420 --> 00:48:47.360
It's quite something.

00:48:47.360 --> 00:48:49.400
A really, really neat project.

00:48:49.400 --> 00:48:52.500
And I love how it applies to all these different things.

00:48:52.500 --> 00:48:57.880
And I don't know if Mark and Vince have noticed, but whilst we've been chatting on our Gitter

00:48:57.880 --> 00:49:00.880
channel, we've now had somebody new come on saying,

00:49:00.880 --> 00:49:01.800
that they're a student.

00:49:01.800 --> 00:49:05.660
They're on a course where they're required to contribute to an OSS project.

00:49:05.660 --> 00:49:10.200
And they like the look of our project and they'd like to know how to get involved.

00:49:10.200 --> 00:49:11.160
So that's awesome.

00:49:11.160 --> 00:49:12.400
I could tell you what to say.

00:49:12.400 --> 00:49:15.420
I didn't think this was a live broadcast, but now I'm beginning to.

00:49:17.780 --> 00:49:18.340
Gosh, yeah.

00:49:18.340 --> 00:49:19.740
Talk Python works fast.

00:49:19.740 --> 00:49:20.400
Yeah.

00:49:20.400 --> 00:49:22.860
Well, maybe it's just someone's hacked into my computer.

00:49:22.860 --> 00:49:26.620
So what you need to tell them.

00:49:26.620 --> 00:49:30.580
I think that's as much as it's on the one side of where the library is gone.

00:49:30.700 --> 00:49:36.100
And it's a really sophisticated, rich research tool that has various applications and things.

00:49:36.100 --> 00:49:40.780
But one of the other things that we've kind of really been careful to is sometimes at the

00:49:40.780 --> 00:49:47.700
cost of efficiency, we have prioritized readability and entry of newcomers, not only to Python,

00:49:47.700 --> 00:49:48.460
but the programming.

00:49:48.980 --> 00:49:53.380
And I think that's something that we're all pretty proud of, that the library is a good place for newcomers.

00:49:53.380 --> 00:49:59.760
In fact, at one point, the winning strategy, it's no longer the case, but at one point,

00:49:59.760 --> 00:50:06.480
the winning strategy was written by a particular high schooler in the UK, which is actually Owen's other son.

00:50:06.480 --> 00:50:08.380
So cool.

00:50:08.380 --> 00:50:11.920
There's a lot of connections to Owen's children here.

00:50:11.920 --> 00:50:12.460
How interesting.

00:50:12.460 --> 00:50:13.960
They get you into all sorts of trouble.

00:50:13.960 --> 00:50:17.520
So, Owen, here's what you need to tell that person and get her.

00:50:17.640 --> 00:50:22.200
There's this simple problem about fingerprinting, and they should be able to knock that out right away to get started.

00:50:22.200 --> 00:50:25.460
All right.

00:50:25.460 --> 00:50:29.580
I think maybe we're kind of getting low on time here for talking about the project.

00:50:29.580 --> 00:50:33.160
There's one other thing I wanted to ask you guys about here.

00:50:33.160 --> 00:50:38.500
And you said that in the notes, you'd put down something for a thing called ugly fruitcake.

00:50:38.500 --> 00:50:40.340
What is ugly fruitcake?

00:50:40.340 --> 00:50:44.760
That's my son's username on Twitter.

00:50:44.760 --> 00:50:47.340
Awesome.

00:50:47.340 --> 00:50:50.220
Well done, Mr. Ugly Fruitcake.

00:50:50.220 --> 00:50:52.480
No, that's awesome.

00:50:52.480 --> 00:50:54.640
Excellent.

00:50:54.640 --> 00:50:55.000
Excellent.

00:50:55.000 --> 00:50:55.440
Okay.

00:50:55.440 --> 00:50:58.120
So I guess we'll kind of have to leave it there.

00:50:58.120 --> 00:51:00.100
We're getting low on time before we're talking about it.

00:51:00.100 --> 00:51:02.720
But it's been really, really fun to talk about.

00:51:02.720 --> 00:51:06.020
Let's talk two things really quick before we drop this, I suppose.

00:51:06.020 --> 00:51:06.860
I still want to ask you.

00:51:06.860 --> 00:51:09.560
One, what's the Python 3 versus Python 2 story?

00:51:09.560 --> 00:51:11.780
Like, what support is there for each version?

00:51:12.120 --> 00:51:14.500
So for a long time, we were supporting both.

00:51:14.500 --> 00:51:20.800
And eventually, we kind of made what I think is one of the best decisions we've made so far is we dropped Python 2.

00:51:21.220 --> 00:51:23.160
Was that on the 1st of December last year?

00:51:23.160 --> 00:51:23.900
I seem to remember.

00:51:23.900 --> 00:51:31.060
And that's just allowed us to clean up our code base so much and pull people away from legacy Python.

00:51:31.060 --> 00:51:33.000
So yeah, we're using...

00:51:33.000 --> 00:51:33.420
That's awesome.

00:51:33.420 --> 00:51:34.740
I think that's a really great move.

00:51:34.740 --> 00:51:37.020
And also, you were talking about the readability and the simplicity.

00:51:37.020 --> 00:51:42.540
Like, as a newcomer to Python, you're often, like, hit in the face with this dichotomy.

00:51:42.540 --> 00:51:44.780
And if you can avoid that, that's really nice.

00:51:44.780 --> 00:51:47.960
And even as not a newcomer, it was still challenging sometimes.

00:51:47.960 --> 00:51:53.720
We've gotten into weird cases where iterators and generators were interacting across the different versions.

00:51:53.720 --> 00:51:54.700
Right, right.

00:51:54.700 --> 00:51:54.960
Yeah.

00:51:54.960 --> 00:51:56.800
Or just mistakes because of division.

00:51:59.100 --> 00:51:59.820
Yeah, absolutely.

00:51:59.820 --> 00:52:03.200
And then also, Jaxelrod, am I saying that correctly?

00:52:03.200 --> 00:52:03.860
Yeah.

00:52:03.860 --> 00:52:04.860
What is Jaxelrod?

00:52:04.860 --> 00:52:05.980
Oh, Django Axelrod.

00:52:05.980 --> 00:52:12.140
That's a longer-term attempt, really, to build a web app on top of this library.

00:52:12.140 --> 00:52:19.260
So instead of installing it yourself and trying to set up tournaments on your own machine,

00:52:19.260 --> 00:52:26.480
the idea here is to say, well, it would be good to be able to tuck this library away somewhere on a much bigger machine

00:52:26.480 --> 00:52:31.880
and put a web front end on it, and in that front end, you could define the tournaments that you want,

00:52:31.880 --> 00:52:36.160
submit them to the server, it would run them, and then spew the results back at you

00:52:36.160 --> 00:52:38.360
for you to get out and do some analysis with.

00:52:38.360 --> 00:52:44.000
That's a project that we've kicked off, and the reason for the name purely is because we're using Django

00:52:44.000 --> 00:52:46.540
and the Django REST framework to try to do it at the moment.

00:52:46.540 --> 00:52:49.560
But it's in fairly early stages at the moment.

00:52:49.560 --> 00:52:54.360
So again, if anybody listening fancies getting involved, it would be an excellent area to get into.

00:52:54.360 --> 00:52:57.780
All right. You want to do some web work and turn interested in game theory?

00:52:57.780 --> 00:52:58.680
Here's your chance, right?

00:52:58.680 --> 00:52:59.220
Yeah, absolutely.

00:52:59.220 --> 00:52:59.640
It's very cool.

00:52:59.640 --> 00:53:03.240
Okay, nice. Is that publicly available or just something you guys have?

00:53:03.240 --> 00:53:08.620
No, no, no. It's another one of the repositories on GitHub under the actual Python organization.

00:53:08.620 --> 00:53:09.420
All right. Awesome.

00:53:09.420 --> 00:53:15.720
The organization GitHub profile has not only the library, but it has lots of really interesting collection of notebooks,

00:53:15.720 --> 00:53:19.180
collection of visualizations, the fingerprints, lots of interesting repositories there,

00:53:19.180 --> 00:53:21.120
as well as Jack Solrod and this rest of the way.

00:53:21.120 --> 00:53:24.120
Okay, excellent. Yeah, I'll definitely link to those as well. That's great.

00:53:24.120 --> 00:53:28.440
All right, guys, it's been super interesting. I really enjoyed our conversation.

00:53:28.440 --> 00:53:31.180
But before I let you go, quick round of questions.

00:53:31.180 --> 00:53:34.700
Owen, if you're going to write some Python code, where do you open up?

00:53:34.700 --> 00:53:36.020
I'm a sublime text man.

00:53:36.020 --> 00:53:38.980
And you said that you like the Anaconda plug-in, right?

00:53:38.980 --> 00:53:43.420
I do. It sits there and tells me where I've gone wrong. Shows me all my mistakes, yeah.

00:53:44.160 --> 00:53:45.660
You like people to yell at you, huh?

00:53:45.660 --> 00:53:51.020
As long as it does it quietly so I can ignore it when I feel like it, that's fine.

00:53:51.020 --> 00:53:56.560
Yeah, no, I think things like that that really give you that extra insight are super, super valuable.

00:53:56.560 --> 00:53:58.320
So, cool. Mark, how about yourself?

00:53:58.320 --> 00:54:02.940
So, I usually use PyCharm lately. I started using it about a year ago, and I really liked it.

00:54:02.940 --> 00:54:07.460
Before that, I just used kind of a bare-bones text editor for the most part.

00:54:07.600 --> 00:54:11.900
Okay, nice. And you probably use a lot of Jupyter, like, notebooks and stuff as well at work.

00:54:11.900 --> 00:54:17.620
Yeah, in general. I also teach a data science class that's based on Jupyter notebooks, so I'm really comfortable.

00:54:17.620 --> 00:54:19.240
Yeah, I like Jupyter a lot, actually.

00:54:19.240 --> 00:54:21.740
Yeah, it's pretty neat. It's definitely got a lot of momentum.

00:54:21.740 --> 00:54:22.520
Vince?

00:54:22.520 --> 00:54:25.000
I also use Jupyter a bunch, but my editor's done.

00:54:25.000 --> 00:54:25.720
Okay, awesome.

00:54:25.720 --> 00:54:32.500
And PyPI package, we just passed over 100,000 PyPI packages, which I think is...

00:54:32.500 --> 00:54:32.720
Yay!

00:54:32.720 --> 00:54:33.820
That's awesome.

00:54:33.820 --> 00:54:36.100
Yeah, that's so cool.

00:54:36.100 --> 00:54:40.140
So, how about, like, the favorite ones? Like, same order. Owen, how about you?

00:54:40.140 --> 00:54:45.580
I think my favorite at the moment has got to be Conch, the shell, the Python shell.

00:54:45.580 --> 00:54:54.440
So, I've finally got to the point now where it doesn't matter what operating system I'm on, I've got exactly the same shell, and its config files are written in Python.

00:54:54.440 --> 00:54:55.500
It's a thing of beauty.

00:54:55.500 --> 00:54:59.380
That is beautiful. And that's X-O-N-S-H, Conch.

00:54:59.380 --> 00:54:59.880
It is.

00:54:59.880 --> 00:55:00.860
It's very cool, yeah. Mark?

00:55:00.860 --> 00:55:06.520
I think Request is my favorite. It's so easy if you're doing any kind of, like, web scraping or anything like that.

00:55:06.520 --> 00:55:11.840
I like a lot of the different ones on there. And, of course, Axelrod has got to be my favorite one.

00:55:11.840 --> 00:55:15.940
Yeah, why not throw that out there? For sure, for sure. And Vince?

00:55:16.440 --> 00:55:22.660
I have to mention QCiW. That's written by a PhD student of mine, so I just want to test that he actually listened to the end.

00:55:22.660 --> 00:55:28.220
It's a library for simulating queuing processes, but I think my favorite has to be TQDM.

00:55:28.460 --> 00:55:37.460
It's a brilliant little library that you can use to wrap any iterator, and it'll generate a progress bar for you for free.

00:55:37.460 --> 00:55:38.940
Oh, that is cool.

00:55:38.940 --> 00:55:45.100
Yeah, it's really, really cool, because you don't have to do anything. You just get these progress bars, which sometimes when I'm doing something, that might take a little bit of time.

00:55:45.100 --> 00:55:50.580
I'm not sure how long. I can just quickly import TQDM, and straight away I get an idea of, oh, no, this isn't going to take an hour.

00:55:50.580 --> 00:55:52.860
This is going to take a couple of years. I've got to change something.

00:55:52.860 --> 00:55:54.680
Exactly. Fabulous.

00:55:54.680 --> 00:56:00.240
Okay, great recommendations, guys. Thanks. And final call to action, people should get started with Axelrod?

00:56:00.240 --> 00:56:06.760
Yeah, come on over. Come check us out in our Gitter room and just say hi and submit a strategy or anything else.

00:56:06.760 --> 00:56:08.700
But yeah, we really welcome contributors.

00:56:08.700 --> 00:56:13.220
Yeah, I can say look at the code. It looks definitely easy to jump into, especially if you want to do the strategies.

00:56:13.220 --> 00:56:21.780
And I feel like there's a chance to plug in more interesting, intriguing things that people are not expecting and probably get some cool results.

00:56:21.780 --> 00:56:22.880
For sure. Absolutely.

00:56:23.260 --> 00:56:26.120
All right, guys. Well, this has been a great chatting with you. Thanks for being on the show.

00:56:26.120 --> 00:56:27.040
Thank you so much.

00:56:27.040 --> 00:56:28.440
Thanks for having us. Thank you.

00:56:28.440 --> 00:56:29.760
Yeah, it's been a pleasure. Thank you.

00:56:29.760 --> 00:56:30.620
Yes, it has. Bye.

00:56:33.040 --> 00:56:35.880
This has been another episode of Talk Python To Me.

00:56:35.880 --> 00:56:40.580
Today's guests have been Vince Knight, Mark Harper, and Owen Campbell.

00:56:40.580 --> 00:56:44.000
And this episode has been sponsored by Talk Python Training.

00:56:44.000 --> 00:56:47.420
Check out all the courses at training.talkpython.fm.

00:56:47.420 --> 00:56:49.780
Be sure to subscribe to the show.

00:56:49.780 --> 00:56:51.980
Open your favorite podcatcher and search for Python.

00:56:51.980 --> 00:56:53.220
We should be right at the top.

00:56:53.600 --> 00:57:02.540
You can also find the iTunes feed at /itunes, Google Play feed at /play, and direct RSS feed at /rss on talkpython.fm.

00:57:02.540 --> 00:57:07.620
Our theme music is Developers, Developers, Developers by Corey Smith, who goes by Smix.

00:57:07.620 --> 00:57:14.320
Corey just recently started selling his tracks on iTunes, so I recommend you check it out at talkpython.fm/music.

00:57:14.460 --> 00:57:19.680
You can browse his tracks he has for sale on iTunes and listen to the full-length version of the theme song.

00:57:19.680 --> 00:57:21.740
This is your host, Michael Kennedy.

00:57:21.740 --> 00:57:23.040
Thanks so much for listening.

00:57:23.040 --> 00:57:24.240
I really appreciate it.

00:57:24.240 --> 00:57:26.360
Smix, let's get out of here.

00:57:44.080 --> 00:57:48.700
We'll see you next time.

00:57:48.700 --> 00:58:18.680
Thank you.

