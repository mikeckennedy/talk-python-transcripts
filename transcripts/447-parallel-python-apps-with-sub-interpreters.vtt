WEBVTT

00:00:00.001 --> 00:00:02.500
It's an exciting time for the capabilities of Python.

00:00:02.500 --> 00:00:05.880
We have the Faster CPython initiative going strong,

00:00:05.880 --> 00:00:09.200
the recent async work, the adoption of typing,

00:00:09.200 --> 00:00:12.160
and on this episode, we discuss a new isolation

00:00:12.160 --> 00:00:15.000
and parallelization capability coming to Python

00:00:15.000 --> 00:00:16.340
through sub-interpreters.

00:00:16.340 --> 00:00:19.360
We have Eric Snow, who spearheaded the work

00:00:19.360 --> 00:00:21.220
to get them added to Python 3.12

00:00:21.220 --> 00:00:24.140
and is working on the Python API for them in 3.11,

00:00:24.140 --> 00:00:27.640
along with Anthony Shaw, who's been pushing the boundaries

00:00:27.640 --> 00:00:30.240
of what you can already do with sub-interpreters.

00:00:30.240 --> 00:00:33.620
This is Talk Python to Me, episode 446,

00:00:33.620 --> 00:00:36.060
recorded December 5th, 2023.

00:00:36.060 --> 00:00:54.160
Welcome to Talk Python to Me, a weekly podcast on Python.

00:00:54.160 --> 00:00:55.900
This is your host, Michael Kennedy.

00:00:55.900 --> 00:00:58.560
Follow me on Mastodon, where I'm @mkennedy,

00:00:58.560 --> 00:01:00.980
and follow the podcast using @talkpython,

00:01:00.980 --> 00:01:03.380
both on fosstodon.org.

00:01:03.380 --> 00:01:05.940
Keep up with the show and listen to over seven years

00:01:05.940 --> 00:01:08.460
of past episodes at talkpython.fm.

00:01:08.460 --> 00:01:12.180
We've started streaming most of our episodes live on YouTube.

00:01:12.180 --> 00:01:15.320
Subscribe to our YouTube channel over at talkpython.fm

00:01:15.320 --> 00:01:18.340
slash YouTube to get notified about upcoming shows

00:01:18.340 --> 00:01:19.800
and be part of that episode.

00:01:19.800 --> 00:01:24.340
This episode is sponsored by PyBites Developer Mindset Program.

00:01:24.340 --> 00:01:27.420
PyBytes' core mission is to help you break the vicious cycle

00:01:27.420 --> 00:01:30.780
of tutorial paralysis through developing real-world applications.

00:01:30.780 --> 00:01:34.320
The PyBites Developer Mindset Program will help you build the confidence

00:01:34.320 --> 00:01:37.080
you need to become a highly effective developer.

00:01:37.080 --> 00:01:39.620
And it's brought to you by Sentry.

00:01:39.620 --> 00:01:41.860
Don't let those errors go unnoticed.

00:01:41.860 --> 00:01:42.880
Use Sentry.

00:01:42.880 --> 00:01:45.720
Get started at talkpython.fm/sentry.

00:01:45.720 --> 00:01:49.220
Anthony, Eric, hello, and welcome to Talk Python.

00:01:49.220 --> 00:01:49.740
Great.

00:01:50.140 --> 00:01:50.480
Hey, guys.

00:01:50.480 --> 00:01:51.920
It's really good to have you both here.

00:01:51.920 --> 00:01:55.340
You both have been on the show before, which is awesome.

00:01:55.340 --> 00:01:58.720
And Eric, we've talked about sub-interpreters before,

00:01:58.720 --> 00:02:01.880
but they were kind of a dream almost at the time.

00:02:01.880 --> 00:02:02.520
That's right.

00:02:02.520 --> 00:02:04.500
Now they feel pretty real.

00:02:04.500 --> 00:02:05.200
That's right.

00:02:05.200 --> 00:02:06.640
Yeah, it's been a long time coming.

00:02:07.100 --> 00:02:11.920
And I think the last time we talked, I've always been hopeful, but it seemed like it was getting

00:02:11.920 --> 00:02:12.340
closer.

00:02:12.340 --> 00:02:18.980
So with 312, we were able to land Per Interpreter Gill, which kind of was the last piece, the

00:02:18.980 --> 00:02:20.520
foundational part I wanted to do.

00:02:20.520 --> 00:02:22.860
A lot of cleanup, a lot of work that had to get done.

00:02:22.860 --> 00:02:24.960
But that last piece got in for 312.

00:02:25.320 --> 00:02:25.720
Excellent.

00:02:25.720 --> 00:02:26.340
Excellent.

00:02:26.340 --> 00:02:26.940
So good.

00:02:26.940 --> 00:02:30.360
And maybe let's just do a quick check-in with you all.

00:02:30.360 --> 00:02:31.580
It's been a while.

00:02:31.580 --> 00:02:34.240
You know, Anthony, start with you, I guess.

00:02:34.240 --> 00:02:37.460
Quick intro for people who don't know you, although I don't know how that's possible.

00:02:37.460 --> 00:02:39.140
And then just what you've been up to.

00:02:39.140 --> 00:02:39.880
Yeah.

00:02:39.880 --> 00:02:41.020
I'm Anthony Shaw.

00:02:41.360 --> 00:02:44.180
I work at Microsoft to lead the Python advocacy team.

00:02:44.180 --> 00:02:54.220
And I do lots of Python stuff, open source, testing things, building tools, blogging, building

00:02:54.220 --> 00:02:55.580
projects, sharing things.

00:02:55.580 --> 00:02:58.100
You have a book, something about the inside of Python?

00:02:58.100 --> 00:02:58.380
Oh, yeah, I have a book as well.

00:02:58.380 --> 00:02:58.660
Yeah?

00:02:58.660 --> 00:02:59.660
I forget about that.

00:02:59.660 --> 00:03:00.580
Yeah.

00:03:00.580 --> 00:03:04.460
Yeah, there's a book called CPython Internals, which is a book all about the Python compiler

00:03:04.460 --> 00:03:05.580
and how it works.

00:03:05.580 --> 00:03:08.940
And you suppress the memory of writing it, like it was too traumatic.

00:03:08.940 --> 00:03:09.940
It's down there.

00:03:09.940 --> 00:03:11.080
I keep forgetting.

00:03:11.080 --> 00:03:15.680
Yeah, that book was for 3.9.

00:03:15.680 --> 00:03:22.520
And people keep asking me if I'm going to update it for 3.13, maybe, because things keep changing.

00:03:22.520 --> 00:03:23.060
Yeah.

00:03:23.060 --> 00:03:27.820
Things are definitely changing at a more rapid pace than they were a few years ago as well.

00:03:27.820 --> 00:03:29.640
So that maybe makes it more challenging.

00:03:29.640 --> 00:03:34.920
Yeah, recently, I've been doing some more research as well.

00:03:34.920 --> 00:03:39.380
So I just finished my master's a few months ago and I started my PhD.

00:03:39.380 --> 00:03:43.260
And I'm looking at parallelism in Python as one of the topics.

00:03:43.260 --> 00:03:49.840
So I've been quite involved in subinterpreters and the free threading project and some other

00:03:49.840 --> 00:03:50.420
stuff as well.

00:03:50.420 --> 00:03:50.880
Awesome.

00:03:51.140 --> 00:03:52.700
Congratulations on the master's degree.

00:03:52.700 --> 00:03:53.540
That's really great.

00:03:53.540 --> 00:03:53.820
Thanks.

00:03:53.820 --> 00:03:56.080
And I didn't realize you were going further.

00:03:56.080 --> 00:03:56.940
So Eric.

00:03:56.940 --> 00:03:58.260
Eric Snow.

00:03:58.260 --> 00:04:03.380
So I've been working on Python as a core developer for over 10 years now.

00:04:03.700 --> 00:04:07.420
But I've been participating for even longer than that.

00:04:07.420 --> 00:04:09.000
And it's been good.

00:04:09.000 --> 00:04:15.300
I've worked on a variety of things, a lot of stuff down in the core runtime of CPython.

00:04:15.300 --> 00:04:21.780
And I've been working on this, trying to find a solution for multi-core Python since, really,

00:04:21.780 --> 00:04:22.700
since 2014.

00:04:23.100 --> 00:04:28.660
So I've been slowly, ever so slowly, working towards that goal.

00:04:28.660 --> 00:04:31.980
And we've made it with 3.12 and there's more work to do.

00:04:31.980 --> 00:04:34.760
But that's a lot of the stuff that I've been working on.

00:04:34.760 --> 00:04:38.640
I'm at Microsoft, but don't work with Anthony a whole lot.

00:04:39.200 --> 00:04:46.380
I work on the Python performance team with Guido and Grant Booger and Mark Shannon here

00:04:46.380 --> 00:04:47.040
at Kedrill.

00:04:47.040 --> 00:04:50.320
And we're just working generally to make Python faster.

00:04:50.320 --> 00:04:53.000
So my part of that has involved sub-interpreters.

00:04:53.000 --> 00:04:53.580
Awesome.

00:04:53.580 --> 00:05:00.720
Interestingly enough, it's only really this year that I've been able to work on all the sub-interpreter

00:05:00.720 --> 00:05:01.920
stuff full-time.

00:05:01.920 --> 00:05:04.400
Before that, I was working mostly on other stuff.

00:05:04.400 --> 00:05:07.640
So this year has been a good year for me.

00:05:07.640 --> 00:05:08.620
Yeah, I would say.

00:05:08.620 --> 00:05:11.340
That must be really exciting to get the, like, you know what?

00:05:11.340 --> 00:05:12.840
Why don't you just keep, just do that?

00:05:12.840 --> 00:05:13.940
That'd be awesome for us.

00:05:13.940 --> 00:05:15.120
Yeah, it's been awesome.

00:05:15.120 --> 00:05:20.280
Well, maybe since you're on the team, let's a quick check-in on FasterCPython.

00:05:20.280 --> 00:05:23.220
It's made a mega difference over the last couple of releases.

00:05:23.220 --> 00:05:25.100
Yeah, it's kind of interesting.

00:05:25.100 --> 00:05:28.360
Mark Shannon has a, definitely has a vision.

00:05:28.360 --> 00:05:31.900
He's developed a plan as of, like, years ago.

00:05:31.900 --> 00:05:37.040
But we finally were able to get him, put him in a position where he could do something

00:05:37.040 --> 00:05:37.580
about it.

00:05:37.580 --> 00:05:40.040
And we've all been kind of pitching in.

00:05:40.040 --> 00:05:46.080
A lot of it has to do with just applying some of the general ideas that are out there regarding

00:05:46.080 --> 00:05:47.800
dynamic languages and optimization.

00:05:47.800 --> 00:05:54.680
Things have been applied to other things like HHVM or various of the JavaScript runtimes.

00:05:55.300 --> 00:06:02.100
And so a lot of specialization, adaptive specialization, a few other techniques.

00:06:02.100 --> 00:06:09.000
But right now, so a lot of that stuff we're able to get in for 3.11.

00:06:09.000 --> 00:06:13.760
In 3.12, there wasn't quite as much impactful stuff.

00:06:13.760 --> 00:06:18.680
We're kind of gearing up to effectively add a JIT into CPython.

00:06:18.680 --> 00:06:24.880
And that's required a lot of kind of behind-the-scenes work to get things in the right places.

00:06:25.520 --> 00:06:29.600
So we're somewhat targeting 3.13 for that.

00:06:29.600 --> 00:06:36.840
So right now, I think with where things are at, we're kind of break-even performance-wise.

00:06:37.080 --> 00:06:39.700
But there's a lot of stuff that we can do.

00:06:39.700 --> 00:06:45.900
A lot of optimization work that really hasn't even been done yet that'll take that performance

00:06:45.900 --> 00:06:47.780
improvement up pretty drastically.

00:06:47.780 --> 00:06:50.840
It's kind of hard to say where we're going to be.

00:06:50.920 --> 00:06:57.160
But for 3.13, it's looking pretty good for at least some performance improvement because

00:06:57.160 --> 00:06:59.920
of the JITing and optimization work.

00:06:59.920 --> 00:07:01.160
That's exciting.

00:07:01.160 --> 00:07:03.980
Yeah, we have no real JIT at the moment, right?

00:07:03.980 --> 00:07:05.280
Not in CPython.

00:07:05.280 --> 00:07:05.800
Yeah.

00:07:05.800 --> 00:07:07.060
I mean, I know there's number.

00:07:07.060 --> 00:07:08.900
In Anthony, yeah, it is best.

00:07:08.900 --> 00:07:10.460
I know.

00:07:10.460 --> 00:07:15.340
Well, that's actually super exciting because I feel like that could be another big boost,

00:07:15.340 --> 00:07:15.860
potentially.

00:07:15.860 --> 00:07:20.600
You know, with the JIT, you come to all sorts of things like inlining of small methods and

00:07:20.600 --> 00:07:23.800
optimization based on type information and, yeah.

00:07:23.800 --> 00:07:24.060
Yep.

00:07:24.060 --> 00:07:24.840
All that stuff.

00:07:24.840 --> 00:07:30.720
One of the most exciting parts for me is that a lot of this work, not long after I joined

00:07:30.720 --> 00:07:37.120
the team, so two years ago, two and a half years ago, somewhere in there, pretty early

00:07:37.120 --> 00:07:43.040
on, we started reaching out to other folks, other projects that were interested in performance

00:07:43.040 --> 00:07:45.680
and performance of Python code.

00:07:45.680 --> 00:07:49.720
And we've worked pretty hard to collaborate with them.

00:07:49.940 --> 00:07:57.420
So like the team over at Meta, they have a lot of interest in making sure Python is very

00:07:57.420 --> 00:07:57.760
efficient.

00:07:57.760 --> 00:08:03.780
And so we've actually worked pretty closely with them and they're able to take advantage

00:08:03.780 --> 00:08:05.680
of a lot of the work that we've done, which is great.

00:08:05.680 --> 00:08:06.080
Yeah.

00:08:06.080 --> 00:08:10.280
There seems to be some synergy between the sender team and the faster CPython team.

00:08:10.280 --> 00:08:11.560
So awesome.

00:08:11.560 --> 00:08:19.240
But let's focus on a part that is there, but not really utilized very much yet, which is

00:08:19.240 --> 00:08:20.500
the sub-interpreter.

00:08:20.500 --> 00:08:22.600
So back on, when is this?

00:08:22.600 --> 00:08:23.440
2019.

00:08:23.440 --> 00:08:29.240
Eric, I had you on and we talked about, can sub-interpreters free us from Python's gil?

00:08:29.640 --> 00:08:35.760
And then since then, this has been accepted, but it's Anthony's fault that we're here.

00:08:35.760 --> 00:08:41.240
Because Anthony posted over on Mastodon, hey, here's a new blog post, me running Python parallel

00:08:41.240 --> 00:08:42.860
applications with sub-interpreters.

00:08:42.860 --> 00:08:48.400
How about we use Flask and FastAPI and sub-interpreters and make that go fast?

00:08:48.400 --> 00:08:55.460
And that sounded more available in the Python level than I kind of realized the sub-interpreter

00:08:55.460 --> 00:08:56.060
stuff was.

00:08:56.220 --> 00:08:58.380
So that's super exciting, both of you.

00:08:58.380 --> 00:09:02.880
Yeah, it's been fun to play with it and try and build applications on it and stuff like

00:09:02.880 --> 00:09:03.380
that.

00:09:03.380 --> 00:09:08.840
And working with Eric probably over the last couple of months on things that we've discovered

00:09:08.840 --> 00:09:12.780
in that process, especially with C extensions.

00:09:12.780 --> 00:09:12.980
Daytime?

00:09:12.980 --> 00:09:13.620
Daytime?

00:09:13.620 --> 00:09:14.440
Yeah.

00:09:14.440 --> 00:09:16.260
Yeah, that's one.

00:09:16.260 --> 00:09:18.020
With C extensions.

00:09:18.020 --> 00:09:24.140
And I think that some of those challenges are going to be the same with free threading as

00:09:24.140 --> 00:09:24.320
well.

00:09:24.760 --> 00:09:30.400
So it's how C extensions have state, where they put it, whether that's thread safe.

00:09:30.400 --> 00:09:36.900
And as soon as you kind of open up the possibility of having multiple gills in one process, then

00:09:36.900 --> 00:09:40.300
what challenges does that create?

00:09:40.300 --> 00:09:41.000
Absolutely.

00:09:41.000 --> 00:09:43.720
Well, I guess maybe some nomenclature first.

00:09:43.720 --> 00:09:47.240
Not no-gil Python or sub-interpreter Python.

00:09:47.240 --> 00:09:48.300
Free-threaded.

00:09:48.300 --> 00:09:49.240
Is that what we're calling it?

00:09:49.240 --> 00:09:50.280
What's the name?

00:09:50.340 --> 00:09:52.120
How do we speak about this?

00:09:52.120 --> 00:09:58.020
It's not quite settled, but I think a lot of people have taken to referring to it as

00:09:58.020 --> 00:09:58.800
free-threaded.

00:09:58.800 --> 00:09:59.800
I can go with that.

00:09:59.800 --> 00:10:04.340
I mean, people still talk about no-gil, but free-threaded is probably the best bet.

00:10:04.340 --> 00:10:07.700
Are you describing what it does and why you care?

00:10:07.700 --> 00:10:09.420
Or are you describing the implementation, right?

00:10:09.480 --> 00:10:13.720
Like the implementation is it has no gill, so it can be free-threaded, or it has sub-interpreters,

00:10:13.720 --> 00:10:15.040
so it can be free-threaded.

00:10:15.040 --> 00:10:17.100
But really what you want is the free-threaded part.

00:10:17.100 --> 00:10:19.500
You don't care actually about the GIL too much, right?

00:10:19.500 --> 00:10:20.440
Kind of.

00:10:20.660 --> 00:10:21.740
Well, it's interesting.

00:10:21.740 --> 00:10:26.220
With sub-interpreters, it really isn't necessarily a free-threaded model.

00:10:26.220 --> 00:10:33.420
It's kind of free-threaded only in the part at which you're moving between interpreters.

00:10:33.420 --> 00:10:37.520
So you only have to care about it when you're interacting between interpreters.

00:10:37.520 --> 00:10:39.360
The rest of the time, you don't have to worry about it.

00:10:39.360 --> 00:10:46.040
Where with the no-gil is more kind of what we think of as free-threading, where everything

00:10:46.040 --> 00:10:47.360
is unsafe.

00:10:47.720 --> 00:10:50.420
And for people who don't know, the no-gil stuff is what's coming out of the sender team

00:10:50.420 --> 00:10:51.200
and from Sam Gross.

00:10:51.200 --> 00:10:55.640
And that was also approved, but with the biggest caveat I've ever seen on an approved pep.

00:10:55.640 --> 00:10:56.120
Sure.

00:10:56.120 --> 00:10:58.020
Like, we approve this.

00:10:58.020 --> 00:11:01.540
We also reserve the right to completely undo it and not approve it anymore.

00:11:01.540 --> 00:11:06.580
But it's also a compiler flag that is an optional off-by-default situation.

00:11:06.580 --> 00:11:08.560
So it should be interesting.

00:11:08.560 --> 00:11:11.820
Yeah, we can maybe compare and contrast them a bit later as well.

00:11:11.820 --> 00:11:12.740
Yeah, absolutely.

00:11:12.740 --> 00:11:16.260
Well, let's start with what is an interpreter.

00:11:16.540 --> 00:11:18.120
So then how do we get to subinterpreters?

00:11:18.120 --> 00:11:20.540
And then what work did you have to do?

00:11:20.540 --> 00:11:23.640
I heard there was a few global variables that were being shared, Eric.

00:11:23.640 --> 00:11:24.200
Yeah.

00:11:24.200 --> 00:11:30.420
Maybe let's give people a quick rundown of what is this and how is this new feature in 3.12

00:11:30.420 --> 00:11:31.220
changing things?

00:11:31.220 --> 00:11:31.660
Yeah.

00:11:31.660 --> 00:11:39.080
Subinterpreters, in a Python process, when you run Python, everything that happens, all

00:11:39.080 --> 00:11:45.340
the machinery that's running your Python code is running with a certain amount of global state.

00:11:45.340 --> 00:11:50.520
And historically, you can think of it as, you know, across the whole process.

00:11:50.520 --> 00:11:52.120
You've got a bunch of global state.

00:11:52.360 --> 00:11:59.440
If you look at all the stuff like in the sys module, sys.modules or sys.whatever, all those

00:11:59.440 --> 00:12:02.440
things are shared across the whole runtime.

00:12:02.440 --> 00:12:07.520
So if you have different threads, for instance, running it, they all share that stuff, even though

00:12:07.520 --> 00:12:10.000
you're going to have different code running in each thread.

00:12:10.000 --> 00:12:15.940
So all of that runtime state is everything that Python needs in order to run.

00:12:15.940 --> 00:12:23.100
But what's interesting is that the vast majority of it, you can think of as the actual interpreter.

00:12:23.100 --> 00:12:29.280
And so that state, if we treat it as isolated and we're very careful about it, then we can

00:12:29.280 --> 00:12:30.420
have multiple of them.

00:12:30.420 --> 00:12:36.140
That means that when your Python code runs, that it can run with a different set of this

00:12:36.140 --> 00:12:42.280
global state, different modules imported, different things going on, different threads that are

00:12:42.280 --> 00:12:45.740
unrelated and really don't affect each other at all.

00:12:45.740 --> 00:12:51.880
And then with that in mind, you can take it one step farther and say, well, let's completely

00:12:51.880 --> 00:12:55.960
isolate those and not even have them share a gill, right?

00:12:55.960 --> 00:12:59.320
And then at that point, that's where the magic kind of happens.

00:12:59.320 --> 00:13:04.940
So that's kind of my first goal in this whole project was to get to that point.

00:13:05.160 --> 00:13:10.340
Because once you get there, then it opens up a lot of possibilities when it comes to

00:13:10.340 --> 00:13:12.260
concurrency and parallelism.

00:13:12.260 --> 00:13:12.700
Yeah.

00:13:12.700 --> 00:13:16.620
Then Anthony can start running with his blog posts and showing off good things.

00:13:16.620 --> 00:13:17.680
Yeah, absolutely.

00:13:17.680 --> 00:13:26.080
This portion of Talk Python to Me is brought to you by the Pybytes Python Developer Mindset

00:13:26.080 --> 00:13:26.620
Program.

00:13:26.620 --> 00:13:31.760
It's run by my two friends and frequent guests, Bob Belderbos and Julian Sequira.

00:13:32.260 --> 00:13:35.640
And instead of me telling you about it, let's hear them describe their program.

00:13:35.640 --> 00:13:43.180
In a world where AI, machine learning and large language models are revolutionizing how we live

00:13:43.180 --> 00:13:46.080
and work, Python stands at the forefront.

00:13:46.080 --> 00:13:50.400
Don't get left behind in this technological evolution.

00:13:51.180 --> 00:13:52.100
Tutorial paralysis?

00:13:52.100 --> 00:13:52.100
Tutorial paralysis?

00:13:52.100 --> 00:13:54.000
That's a thing of the past.

00:13:54.000 --> 00:13:59.940
With Pybytes Coaching, you move beyond endless tutorials to become an efficient, skilled Python

00:13:59.940 --> 00:14:00.380
developer.

00:14:00.380 --> 00:14:05.460
We focus on practical, real-world skills that prepare you for the future of tech.

00:14:05.460 --> 00:14:12.360
Join us at Pybytes and step into a world where Python isn't just a language, but a key to unlocking

00:14:12.360 --> 00:14:15.060
endless possibilities in the tech landscape.

00:14:15.060 --> 00:14:20.460
Check out our 12-week PDM program and embark on a journey to Python mastery.

00:14:20.460 --> 00:14:22.280
The future is Python.

00:14:22.280 --> 00:14:24.820
And with Pybytes, you're one step ahead.

00:14:24.820 --> 00:14:43.660
One thing I don't know the answer to, but might be interesting, is Python has a memory management

00:14:43.660 --> 00:14:48.720
story in front of the operating system, virtual memory that's assigned to the process with pools,

00:14:48.720 --> 00:14:51.240
arenas, blocks, those kinds of things.

00:14:51.580 --> 00:14:54.080
What's that look like with regard to sub-interpreters?

00:14:54.080 --> 00:14:59.760
Does each sub-interpreter have its own chunk or set of those for the memory it allocates,

00:14:59.760 --> 00:15:02.560
or is it still a shared one thing per process?

00:15:02.560 --> 00:15:04.980
It's per interpreter.

00:15:04.980 --> 00:15:07.440
This is something that was very global.

00:15:07.440 --> 00:15:13.780
And like you pointed out earlier, this whole project was all about taking all sorts of global

00:15:13.780 --> 00:15:18.280
state that was actually stored in C global variables all over the place, right?

00:15:18.360 --> 00:15:25.200
And pulling those in together into one place and moving those down from kind of the process

00:15:25.200 --> 00:15:27.940
global state down into each interpreter.

00:15:27.940 --> 00:15:34.220
So one of those things was all of the allocator state that we have for objects.

00:15:34.220 --> 00:15:39.060
And Python kind of has this idea of different levels of allocators.

00:15:39.060 --> 00:15:44.860
The object allocator is what's used heavily for Python objects, of course, but some other

00:15:44.860 --> 00:15:45.500
state as well.

00:15:45.500 --> 00:15:51.640
And the object allocator is the part that has all the arenas and everything, like you were

00:15:51.640 --> 00:15:51.920
saying.

00:15:51.920 --> 00:15:52.400
Yeah.

00:15:52.400 --> 00:15:58.120
So part of what I did before we could make the GIL per interpreter, we had to make the

00:15:58.120 --> 00:15:59.860
allocator state per interpreter.

00:15:59.860 --> 00:16:04.960
Well, the reason I think that it's interesting asking about it, one, because of the gill, obviously,

00:16:04.960 --> 00:16:09.460
but the other one is, it seems to me like these sub interpreters could be used for a little

00:16:09.460 --> 00:16:13.640
bit of stability or isolation or run some kind of code.

00:16:13.640 --> 00:16:16.820
And when that line exits, I want the memory freed.

00:16:16.820 --> 00:16:18.440
I want models unloaded.

00:16:18.440 --> 00:16:19.980
I wanted to go back to the way it was.

00:16:19.980 --> 00:16:21.040
You know what I mean?

00:16:21.040 --> 00:16:25.740
Whereas normally in Python, even if the memory becomes free, right, it's still got some of

00:16:25.740 --> 00:16:27.220
that, like, well, we allocated this stuff.

00:16:27.220 --> 00:16:28.780
Now we're holding it to refill it.

00:16:28.840 --> 00:16:34.320
And then, you know, you don't unimport modules and the modules can be pretty intense, actually,

00:16:34.320 --> 00:16:37.960
if, you know, if they start allocating a bunch of stuff themselves and so on.

00:16:37.960 --> 00:16:41.080
What do you guys think about this as an idea, as an aspect of it?

00:16:41.080 --> 00:16:44.500
Yeah, there's one example that's been coming across recently.

00:16:44.500 --> 00:16:46.220
And this is a pattern.

00:16:46.220 --> 00:16:48.180
I think it's a bit of an anti-pattern, actually.

00:16:48.180 --> 00:16:56.180
But some Python packages, they store, like, some state information in the module level.

00:16:56.180 --> 00:17:02.740
So an example is a SDK that I've been working with, which has just been rewritten to stop

00:17:02.740 --> 00:17:03.480
it from doing this.

00:17:03.480 --> 00:17:08.600
But you would put the API key of the SDK, you would import it.

00:17:08.600 --> 00:17:12.640
So you'd import X and then do, like, X dot API key equals.

00:17:13.220 --> 00:17:21.420
So it basically stores the API key in the module object, which is fine if you've got,

00:17:21.420 --> 00:17:24.240
you've imported the module once and you're using it once.

00:17:24.240 --> 00:17:30.680
But what you see is that if you put that in a web application, it just assumes that everyone

00:17:30.680 --> 00:17:31.760
uses the same key.

00:17:31.760 --> 00:17:38.060
So, you know, you can't have, you can't import that module and then connect to it with different

00:17:38.060 --> 00:17:40.480
API keys, like you'd have different users or something.

00:17:40.860 --> 00:17:40.980
Right.

00:17:40.980 --> 00:17:43.700
So you had some kind of multi-tenancy, right?

00:17:43.700 --> 00:17:49.620
Where, like, they would say, you know, enter their ChatGPT, open AI key, and then they could

00:17:49.620 --> 00:17:50.980
work on behalf of that, right?

00:17:50.980 --> 00:17:52.660
Like, potentially something like that, right?

00:17:52.660 --> 00:17:53.560
Yeah, exactly.

00:17:53.560 --> 00:17:55.480
So that's kind of like an API example.

00:17:55.480 --> 00:17:59.940
But there are other examples where, let's say you're loading data or something and it's,

00:17:59.940 --> 00:18:05.740
and it stores some temporary information somewhere in like a class attribute or even like a

00:18:05.740 --> 00:18:06.800
module attribute like that.

00:18:06.800 --> 00:18:13.560
If you've got one piece of code loading data and then in another thread in a web app or

00:18:13.560 --> 00:18:18.340
just in another thread generally, you're reading another piece of data and they're sharing state

00:18:18.340 --> 00:18:20.260
somehow and you've got no isolation.

00:18:20.260 --> 00:18:25.600
Some of that is due to the way that people have written the Python code or the extension

00:18:25.600 --> 00:18:31.540
code has kind of been built around, oh, we'll just put this information here.

00:18:31.680 --> 00:18:33.660
And they haven't really thought about the isolation.

00:18:33.660 --> 00:18:40.840
Sometimes it's because on the C level, especially that because the GIL was always there, they've

00:18:40.840 --> 00:18:42.160
never had to worry about it.

00:18:42.160 --> 00:18:47.440
So, you know, let's, you could just have a counter, for example, or there's an object, which is a

00:18:47.440 --> 00:18:50.600
dictionary that is a cache of something.

00:18:50.600 --> 00:18:55.900
and you just put that, as a static variable, and you just read and write from

00:18:55.900 --> 00:18:56.000
it.

00:18:56.000 --> 00:18:59.680
You've never had to worry about thread safety because the girl was there to kind of protect

00:18:59.680 --> 00:19:00.000
you.

00:19:00.000 --> 00:19:04.480
You probably shouldn't have built it that way, but it didn't really matter because it worked.

00:19:04.480 --> 00:19:06.480
what about this, Anthony?

00:19:06.480 --> 00:19:10.140
What if we can, if we can write it on one line, it'll probably be safe, right?

00:19:10.140 --> 00:19:13.280
If we can fit it just one line of Python code, it'd be okay.

00:19:13.280 --> 00:19:14.300
Yeah.

00:19:14.300 --> 00:19:16.020
Yeah.

00:19:16.020 --> 00:19:16.760
Dictionary.ad.

00:19:16.760 --> 00:19:17.400
What's wrong there?

00:19:17.400 --> 00:19:18.300
Dictionary.get.

00:19:18.300 --> 00:19:18.820
It's fine.

00:19:18.820 --> 00:19:19.360
Yeah.

00:19:19.740 --> 00:19:25.740
So yeah, what we're saying within sub interpreters, I think what's the concept that people will

00:19:25.740 --> 00:19:31.260
need to kind of understand is where the, where the isolation is, because there are different

00:19:31.260 --> 00:19:32.980
models for running parallel code.

00:19:32.980 --> 00:19:38.540
And at the moment we've got co-routines, which is asynchronous.

00:19:38.540 --> 00:19:40.500
So it can run concurrently.

00:19:40.500 --> 00:19:44.520
So that's, if you do async and a wait, or if you use the old, co-routine decorator,

00:19:44.920 --> 00:19:49.680
um, you've also got things like generators, which are kind of like a concurrent pattern.

00:19:49.680 --> 00:19:52.700
you've got threads that you can create.

00:19:52.700 --> 00:19:58.860
all of those live within the same interpreter and they share the same information.

00:19:58.860 --> 00:20:04.960
So you don't have to, if you create a thread inside that thread, you can read a variable

00:20:04.960 --> 00:20:06.220
from outside of that thread.

00:20:06.220 --> 00:20:08.060
and it doesn't complain.

00:20:08.060 --> 00:20:10.700
you don't need to create a lock at the moment.

00:20:10.700 --> 00:20:13.440
although in some situations you probably should.

00:20:13.880 --> 00:20:19.740
and you don't need to reimport modules and stuff like that, which, yeah, which can

00:20:19.740 --> 00:20:20.280
be fine.

00:20:20.280 --> 00:20:25.000
And then at the other extreme, you've got multi-processing, which is a module in the standard

00:20:25.000 --> 00:20:30.520
library that allows you to create extra Python processes and then kind of gives you like an

00:20:30.520 --> 00:20:33.020
API to talk to them and share information between them.

00:20:33.020 --> 00:20:38.060
And that's kind of like the app, the other extreme, which is you've got on it is the, you

00:20:38.060 --> 00:20:39.920
know, the ultimate level of isolation.

00:20:40.100 --> 00:20:42.080
You've got a whole separate Python process.

00:20:42.080 --> 00:20:46.420
but instead of interacting with it via the command line, you've kind of got this

00:20:46.420 --> 00:20:52.460
nicer API where you can almost treat it like it's in the same process as the one you're

00:20:52.460 --> 00:20:53.040
running from.

00:20:53.040 --> 00:20:53.660
Yeah.

00:20:53.660 --> 00:20:57.740
It's kind of magical actually that that at all where you get a return value from a process,

00:20:57.740 --> 00:20:58.280
for example.

00:20:58.280 --> 00:20:58.800
Right.

00:20:59.060 --> 00:20:59.340
Yeah.

00:20:59.340 --> 00:21:03.880
but the thing is, if you peel back the covers a little bit, then like how it

00:21:03.880 --> 00:21:08.060
sends information to the other Python process involves a lot of pickles.

00:21:08.060 --> 00:21:11.380
and it's not particularly efficient.

00:21:11.380 --> 00:21:17.740
And also a Python process has a lot of extra stuff that you maybe necessarily didn't even

00:21:17.740 --> 00:21:18.180
need.

00:21:18.180 --> 00:21:22.440
Like you get all this isolation from having it, but you have to import all the modules again.

00:21:22.560 --> 00:21:26.160
You have to create the arenas again, or the memory allocation.

00:21:26.160 --> 00:21:31.020
You have to do all the startup process again, which takes a lot of times, like at least 200

00:21:31.020 --> 00:21:31.440
million seconds.

00:21:31.440 --> 00:21:32.760
The Python code again, right?

00:21:32.760 --> 00:21:33.640
At least the PYC.

00:21:33.640 --> 00:21:34.040
Yeah.

00:21:34.040 --> 00:21:34.620
Yeah.

00:21:34.620 --> 00:21:35.280
Yeah, exactly.

00:21:35.280 --> 00:21:38.400
So you basically created like a whole separate Python.

00:21:38.920 --> 00:21:45.080
and if you do that just to run a small chunk of code, then it's not probably the best

00:21:45.080 --> 00:21:45.760
model at all.

00:21:45.760 --> 00:21:46.200
Yeah.

00:21:46.200 --> 00:21:51.460
You have a nice graph as well that shows like sort of the rate as you add more work and you

00:21:51.460 --> 00:21:52.540
need more parallelism.

00:21:52.540 --> 00:21:53.680
We'll get to that.

00:21:53.680 --> 00:21:54.280
I'm sure.

00:21:54.280 --> 00:21:59.680
you know, one thing that struck me coming to Python from other languages like C, C++,

00:21:59.680 --> 00:22:06.460
C#, there's very little locks and events, threading, coordinating stuff in Python.

00:22:06.680 --> 00:22:12.740
And I think that there's probably a ton of Python code that actually is not actually thread

00:22:12.740 --> 00:22:17.540
safe, but people kind of get away with it because the context switching is so coarse grained,

00:22:17.540 --> 00:22:18.000
right?

00:22:18.000 --> 00:22:19.720
Like you say, well, the gills there.

00:22:19.720 --> 00:22:24.120
So you only run one instruction at a time, but like this temporary invalid state you enter

00:22:24.120 --> 00:22:27.640
to as part of like, just your code running, like took money out of this account.

00:22:27.640 --> 00:22:29.220
And then I'm going to put it into that account.

00:22:29.220 --> 00:22:33.800
Those are multiple Python lines and there's nothing saying they couldn't get interrupted between

00:22:33.800 --> 00:22:34.520
one to the other.

00:22:34.520 --> 00:22:35.900
And then things are busted.

00:22:35.900 --> 00:22:36.160
Right.

00:22:36.220 --> 00:22:40.560
I feel there's, there's some concern about adding this concurrency, like, oh, we're having

00:22:40.560 --> 00:22:41.240
to worry about it.

00:22:41.240 --> 00:22:42.980
Like you probably should be worrying about it now.

00:22:42.980 --> 00:22:47.860
Not as much necessarily, but it's, I feel like people are getting away with it because

00:22:47.860 --> 00:22:51.020
it's so rare, but it's not, it's a non-zero possibility.

00:22:51.020 --> 00:22:51.880
What do you guys think?

00:22:51.880 --> 00:22:52.300
Yeah.

00:22:52.300 --> 00:22:53.880
I mean, those are real concerns.

00:22:53.880 --> 00:23:01.140
It's as there's been lots of discussion with the no-go work about really what, what matters,

00:23:01.300 --> 00:23:05.160
what we need to care about really what impact it's going to have.

00:23:05.160 --> 00:23:15.820
And I mean, it's probably going to have some impact on people with Python code, but it'll especially have impact on people that maintain extension modules.

00:23:15.820 --> 00:23:16.400
Yeah.

00:23:16.400 --> 00:23:21.920
But it really is all the pain that comes with free threading.

00:23:21.920 --> 00:23:27.840
that's what it introduces with the benefits as well, of course.

00:23:28.680 --> 00:23:42.320
But what's interesting, I'd like to think of sub interpreters kind of provide the same facility, but they force you to be explicit about what gets shared and they force you to do it in a thread safe way.

00:23:42.320 --> 00:23:45.920
So it's, you can't do it without thread safety.

00:23:46.360 --> 00:23:58.580
And so it's not an issue and it doesn't hurt that people really haven't used sub interpreters extensively up till now, whereas threads are kind of something that's been around for quite a while.

00:23:58.580 --> 00:24:09.560
Yeah, it has been in, well, it's sub interpreters have traditionally just been a thing you can do from C extensions or the C API, which really limits them from being used in just a standard.

00:24:09.560 --> 00:24:20.580
Like I'm working on my web app, so let's just throw in a couple of sub interpreters, you know, but in 313, is that when we're looking at having a Python level API for creating, interacting with?

00:24:20.580 --> 00:24:30.740
Yeah, I've been working on a PEP for that PEP 554 recently created a new PEP to replace that one, which is PEP 734.

00:24:30.740 --> 00:24:31.720
That's the one.

00:24:31.720 --> 00:24:34.880
So that's the one that I'm targeting for 313.

00:24:35.460 --> 00:24:37.960
And I don't know, it's pretty straightforward.

00:24:37.960 --> 00:24:46.880
Create interpreters and kind of look at them and with an interpreter, run some code, you know, pretty basic stuff.

00:24:46.880 --> 00:24:52.460
And then also because sub interpreters aren't quite so useful if you can't cooperate between them.

00:24:52.460 --> 00:25:00.600
But there's also a queue type that, you know, you push stuff on and you pop stuff off and just pretty basic.

00:25:00.700 --> 00:25:05.180
So you could write something like await queue.pop or something like that.

00:25:05.180 --> 00:25:06.040
Excellent.

00:25:06.040 --> 00:25:06.680
Yeah.

00:25:06.680 --> 00:25:08.080
So yeah, this is really cool.

00:25:08.080 --> 00:25:13.500
And the other thing that I wanted to talk about here, looks like you already have it in the pep, which is excellent.

00:25:13.500 --> 00:25:14.660
Somehow I missed that before.

00:25:14.660 --> 00:25:23.100
Is that there's an, we have thread pool executors, we have multi-processing pool executors, and this would be an interpreter pool executor.

00:25:23.100 --> 00:25:24.740
What's the thinking there?

00:25:25.240 --> 00:25:28.000
People are already familiar with using concurrent features.

00:25:28.000 --> 00:25:41.560
So if we can present the same API for sub interpreters, it makes it really easy because you can set it up with multi-processing or threads and switch it over to one of the other pool types without a lot of fuss.

00:25:41.560 --> 00:25:42.040
Right.

00:25:42.040 --> 00:25:45.300
Basically, with a clever import statement, you're good to go, right?

00:25:45.300 --> 00:25:54.420
From whatever import, like multi-processing pool executor as pool executor or interpreter pool executor as pool executor, and then the rest of the code could stay potentially.

00:25:54.420 --> 00:25:54.980
Yeah.

00:25:54.980 --> 00:25:55.980
What I expect-

00:25:55.980 --> 00:25:59.800
The communication, like you gotta, it's gotta kind of be a basic situation, right?

00:25:59.800 --> 00:26:01.000
Because there are assumptions.

00:26:01.400 --> 00:26:01.480
Yeah.

00:26:01.480 --> 00:26:01.880
Yeah.

00:26:01.880 --> 00:26:08.880
And it should work mostly the same way that you already use it with threads and multi-processing.

00:26:08.880 --> 00:26:09.940
But we'll see.

00:26:09.940 --> 00:26:17.060
There's some limitations with sub interpreters currently that I'm sure we'll work on solving as we can.

00:26:17.060 --> 00:26:19.200
So we'll see.

00:26:19.200 --> 00:26:28.660
It may not be quite as efficient as I'd like at first with the interpreter pool executor, because we'll probably end up doing some pickling stuff, kind of like multi-processing does.

00:26:29.040 --> 00:26:31.260
Although I expect it'll be a little more efficient.

00:26:31.260 --> 00:26:35.140
This portion of Talk Python To Me is brought to you by Sentry.

00:26:35.140 --> 00:26:39.160
You know Sentry for the air monitoring service, the one that we use right here at Talk Python.

00:26:39.160 --> 00:26:42.900
But this time, I want to tell you about a new and free workshop.

00:26:42.900 --> 00:26:47.280
Heaming the Kraken, managing a Python monorepo with Sentry.

00:26:47.280 --> 00:26:58.720
Join Salma Alam Nayour, senior developer advocate at Sentry, and David Winterbottom, head of engineering at Kraken Technologies, for an inside look into how he and his team develop

00:26:58.720 --> 00:27:07.240
deploy and maintain a rapidly evolving Python monorepo with over 4 million lines of code that powers the Kraken utility platform.

00:27:07.740 --> 00:27:21.060
In this workshop, David will share how his department of 500 developers, who deploy around 200 times a day, use Sentry to reduce noise, prioritize issues, and maintain code quality without relying on a dedicated Q&A team.

00:27:21.300 --> 00:27:29.480
You'll learn how to find and fix root causes of crashes, ways to prioritize the most urgent crashes and errors, and tips to streamline your workflow.

00:27:29.480 --> 00:27:35.380
Join them for free on Tuesday, February 27th, 2024 at 2 a.m. Pacific time.

00:27:35.840 --> 00:27:39.820
Just visit talkpython.fm/sentry dash monorepo.

00:27:39.820 --> 00:27:42.000
That link is in your podcast player show notes.

00:27:42.000 --> 00:27:51.560
2 a.m. might be a little early here in the U.S., but go ahead and sign up anyway if you're a U.S. listener, because I'm sure they'll email you about a follow-up recording as well.

00:27:51.560 --> 00:27:54.480
Thank you to Sentry for supporting this episode.

00:27:55.480 --> 00:27:58.740
I was going to save this for later, but I think maybe it's worth talking about now.

00:27:58.740 --> 00:28:10.600
So first of all, Anthony, you wrote a lot about and have actually had some recent influence on what you can pass across, say, the starting code and then the running interpreter that's kind of like the sub-interpreter doing extra work.

00:28:10.600 --> 00:28:13.180
Want to talk about what data exchange there is?

00:28:13.180 --> 00:28:13.680
Yeah.

00:28:13.680 --> 00:28:23.840
So when you're using any of these models, multiprocessing, sub-interpreters, or threading, I guess you've got two or three things to worry about.

00:28:23.940 --> 00:28:25.860
One is how do you create it in the first place?

00:28:25.860 --> 00:28:27.780
So how do you create a process?

00:28:27.780 --> 00:28:29.560
How do you create an interpreter?

00:28:29.560 --> 00:28:30.500
How do you create a thread?

00:28:30.500 --> 00:28:33.720
The second thing is how do you send data to it?

00:28:33.720 --> 00:28:37.340
Because normally the reason you've created them is because you need it to do some work.

00:28:37.340 --> 00:28:42.680
So you've got the code, which is when you spawn it, when you create it.

00:28:42.680 --> 00:28:48.680
The code that you want it to run, but that code needs some sort of input, and that's probably going to be Python objects.

00:28:48.680 --> 00:28:53.560
It might be reading files, for example, or listening to a network socket.

00:28:53.760 --> 00:28:57.260
So it might be getting its input from somewhere else.

00:28:57.260 --> 00:28:59.440
But typically you need to give it parameters.

00:28:59.440 --> 00:29:05.260
Now, the way that works in multiprocessing is mostly reliant on pickle.

00:29:06.040 --> 00:29:20.380
If you start a process and you give it some data, either as a parameter or you create a queue and you send data down the queue or the pipe, for example, it pickles the data.

00:29:20.380 --> 00:29:22.780
So you can put a Python object in.

00:29:22.780 --> 00:29:23.980
It uses the pickle module.

00:29:24.140 --> 00:29:29.480
It converts it into a byte string and then it basically converts the byte string on the other end back into objects.

00:29:29.820 --> 00:29:35.320
That's got its limitations because not everything can be pickled.

00:29:35.320 --> 00:29:52.320
And also some objects, especially if you've got an object which has got objects in it and it's deeply nested or you've got a big complicated dictionary or something that's got all these strange types in it which can't necessarily be rehydrated just from a byte string.

00:29:53.320 --> 00:30:01.180
An alternative, actually, I do want to point out because for people who come across this issue quite a lot, there's another package called Dill on PyPI.

00:30:01.180 --> 00:30:05.040
So if you think of pickle, think of Dill.

00:30:05.040 --> 00:30:07.980
Dill is very similar to pickle.

00:30:08.260 --> 00:30:14.580
It has the same interface, but it can pickle slightly more exotic objects than pickle can.

00:30:14.580 --> 00:30:24.400
So often if you find that you've tried to pickle something, you try to share it with a process or a sub interpreter and it comes back and says, this can't be pickled.

00:30:24.400 --> 00:30:27.320
You can try Dill and see if that works.

00:30:27.320 --> 00:30:37.340
So, yeah, that's the typical way of doing it is that you would pickle an object and then on the other end, you would basically unpickle it back into another object.

00:30:38.000 --> 00:30:40.300
The downside of that is that it's pretty slow.

00:30:40.300 --> 00:30:41.280
It's equivalent.

00:30:41.280 --> 00:30:52.220
Like if you use the JSON module in Python, it's kind of similar, I guess, to converting something into JSON and then converting it from JSON back into a dictionary on the other end.

00:30:52.220 --> 00:30:55.920
Like it's not a super efficient way of doing it.

00:30:55.920 --> 00:30:58.480
So sub interpreters have another mechanism.

00:30:58.480 --> 00:31:01.160
I haven't read PEP 734 yet.

00:31:02.660 --> 00:31:08.360
So I don't know how much of this is in the new PEP, Eric, or if it's in the queue.

00:31:08.360 --> 00:31:09.960
But there's a...

00:31:09.960 --> 00:31:11.500
It's much the same.

00:31:11.500 --> 00:31:12.520
Okay, it's much the same.

00:31:12.840 --> 00:31:15.360
So there's another mechanism with sub interpreters.

00:31:15.880 --> 00:31:21.900
Because they share the same process, whereas multiprocessing doesn't, they're separate processes.

00:31:21.900 --> 00:31:29.340
Because they share the same process, you can basically put some data in a memory space, which can be read from a separate interpreter.

00:31:29.340 --> 00:31:31.080
Now you need to be...

00:31:31.080 --> 00:31:32.760
Well, Python needs to be really careful.

00:31:32.760 --> 00:31:37.780
You don't need to worry too much about it because that complexity is done for you.

00:31:37.780 --> 00:31:41.000
But there are certain types of objects that you can put in as parameters.

00:31:41.000 --> 00:31:51.500
You can send either as startup variables for your sub interpreter, or you can send via like a pipe, basically, backwards and forwards between the interpreters.

00:31:51.500 --> 00:32:04.900
And these are essentially all the immutable types for Python, which is like string, unicode strings, byte strings, bool, none, integer, float, and tuples.

00:32:04.900 --> 00:32:07.440
And you can do tuples of tuples as well.

00:32:07.440 --> 00:32:11.980
And it seems like the tuple part had to...

00:32:11.980 --> 00:32:13.360
Something that you added recently, right?

00:32:13.360 --> 00:32:15.880
It says, I implemented tuple sharing just last week.

00:32:15.880 --> 00:32:17.540
Yeah, that's in now.

00:32:17.540 --> 00:32:19.420
I really wanted to use it.

00:32:19.420 --> 00:32:21.900
So I thought, well, instead of keep...

00:32:21.900 --> 00:32:23.340
I kept complaining that it wasn't there.

00:32:23.340 --> 00:32:28.700
So I thought instead of complaining, I might as well talk to Eric and work out how to implement it.

00:32:28.700 --> 00:32:29.740
Yeah, that's awesome.

00:32:29.740 --> 00:32:31.460
But yeah, you can't share dictionaries.

00:32:31.460 --> 00:32:31.960
That's one thing.

00:32:31.960 --> 00:32:32.560
Yeah, exactly.

00:32:32.560 --> 00:32:36.280
So one thing that I thought that might be awesome, are you familiar with message spec?

00:32:36.280 --> 00:32:37.480
You guys seen message spec?

00:32:37.480 --> 00:32:46.960
It's like Pydantic in the sense that you create a class with types, but the parsing performance is quite a bit, like much, much faster.

00:32:46.960 --> 00:32:53.540
80 times faster than Pydantic, 10 times faster than Marshuro and Seatters and so on.

00:32:53.540 --> 00:32:57.560
And faster still even than, say, JSON or UJSON.

00:32:57.560 --> 00:33:04.260
So maybe it makes sense to use this, turn it into its serialization formats, bytes, send the bytes over and then pull it back.

00:33:04.260 --> 00:33:04.700
I don't know.

00:33:04.700 --> 00:33:06.040
It might give you a nice structured way.

00:33:06.060 --> 00:33:06.920
You can share bytes strings.

00:33:06.920 --> 00:33:18.160
So you can stick something into Pickle or you can use like msgspec or something like that to serialize something into a bytes string and then receive it on the other end and rehydrate it.

00:33:18.160 --> 00:33:19.080
Or even Pydantic.

00:33:19.080 --> 00:33:20.580
Like Pydantic is awesome as well.

00:33:20.580 --> 00:33:24.500
This is meant to be super fast with a little bit of less behavior, right?

00:33:24.740 --> 00:33:25.000
Yeah.

00:33:25.000 --> 00:33:26.980
So this is kind of a design thing.

00:33:26.980 --> 00:33:31.660
I think people need to consider when they're like, great, I can run everything in parallel now.

00:33:32.560 --> 00:33:36.540
But you have to kind of unwind and think about how you've designed your application.

00:33:36.540 --> 00:33:40.300
Like at which point do you fork off the work?

00:33:40.300 --> 00:33:40.740
Yeah.

00:33:40.740 --> 00:33:43.340
And how do you split the data?

00:33:43.340 --> 00:33:53.100
You can't just kind of go into it assuming, oh, we'll just have a pool of workers and we've kind of got this shared area of data that everybody just reads for.

00:33:53.740 --> 00:33:57.300
Yeah, I'll pass it a point or two of million entry list and I'll just run with it.

00:33:57.300 --> 00:34:01.120
Yeah, because I mean, in any language, you're going to get issues if you do that.

00:34:01.120 --> 00:34:07.920
Even if you've got shared memory and it's easier to read and write to the different spaces, you're going to get issues with locking.

00:34:07.920 --> 00:34:10.460
And I think it's also important with free threading.

00:34:10.460 --> 00:34:17.300
If you read the spec or kind of follow what's happening with free threading, it's not like the gills disappeared.

00:34:17.300 --> 00:34:20.320
The gills been replaced with other locks.

00:34:20.320 --> 00:34:21.460
Yeah.

00:34:21.580 --> 00:34:23.980
So there are still going to be locks.

00:34:23.980 --> 00:34:25.140
You can't just have no locks.

00:34:25.140 --> 00:34:27.680
Especially cross threads, right?

00:34:27.680 --> 00:34:34.280
Like it moves some of the reference counting stuff into like, well, it's fast on the default thread, the same thread.

00:34:34.280 --> 00:34:39.560
But if it goes to another, it has to kick in another more thread safe case that potentially is slower and so on.

00:34:39.560 --> 00:34:39.840
Yeah.

00:34:39.840 --> 00:34:45.500
So yeah, the really important thing with sub interpreters is that they have their own GIL.

00:34:45.500 --> 00:34:48.520
So each one has its own lock.

00:34:48.700 --> 00:34:52.280
So they can run fully in parallel just as they could with multi-processing.

00:34:52.280 --> 00:34:57.140
So I feel like a closer comparison with sub interpreters is multi-processing.

00:34:57.140 --> 00:34:58.060
Yeah, absolutely.

00:34:58.060 --> 00:35:00.620
Because they basically run fully in parallel.

00:35:00.620 --> 00:35:05.000
If you start four of them and you have four cores, each core is going to be busy doing work.

00:35:05.680 --> 00:35:10.840
You start them, you give them data, you can interact with them whilst they're running.

00:35:10.840 --> 00:35:16.760
And then when they're finished, they can close and they can be destroyed and cleaned up.

00:35:16.760 --> 00:35:19.560
So it's much closer to multi-processing.

00:35:20.040 --> 00:35:27.620
But the big difference is that the overhead both on the memory and CPU side of things is much smaller.

00:35:27.620 --> 00:35:31.400
Separate processes with multi-processing are pretty heavyweight.

00:35:31.400 --> 00:35:32.680
They're big workers.

00:35:33.800 --> 00:35:37.920
And then the other thing that's pretty significant is the time it takes to start one.

00:35:37.920 --> 00:35:42.620
So starting a process with multi-processing takes quite a lot of time.

00:35:42.620 --> 00:35:48.440
And it's significantly, I think it's like 20 or 30 times faster to start a sub interpreter.

00:35:48.440 --> 00:35:51.160
You have a bunch of graphs for it somewhere.

00:35:51.160 --> 00:35:51.720
There we go.

00:35:51.720 --> 00:35:52.260
Yeah.

00:35:52.520 --> 00:35:53.860
So I scrolled past it.

00:35:53.860 --> 00:35:54.240
There we go.

00:35:54.240 --> 00:35:58.000
It's not exactly the same, but kind of captures a lot of it there.

00:35:58.000 --> 00:36:04.520
So one thing that I think is exciting, Eric, is the interpreter pool, sub interpreter pool.

00:36:04.520 --> 00:36:09.520
Because a lot of the difference between the threading and the sub interpreter performance

00:36:09.520 --> 00:36:14.960
is that startup of the new arenas and like importing the standard library,

00:36:14.960 --> 00:36:16.680
all that kind of stuff that still is going to happen.

00:36:16.680 --> 00:36:21.860
But once those things are loaded up in the process, they could be handed work easily, right?

00:36:21.940 --> 00:36:24.700
And so if you've got a pool of, you know, like say that you have 10 cores,

00:36:24.700 --> 00:36:27.400
you've got 10 of them just chilling or however many, you know,

00:36:27.400 --> 00:36:29.460
you've sort of done enough work to like do in parallel,

00:36:29.460 --> 00:36:33.060
then you could have them laying around and just send like,

00:36:33.060 --> 00:36:34.440
okay, now I want you to run this function.

00:36:34.440 --> 00:36:35.540
And now I want you to run this.

00:36:35.540 --> 00:36:38.240
And that one means go call that API and then process it.

00:36:38.240 --> 00:36:41.700
I think you could get the difference between threading and sub interpreters

00:36:41.700 --> 00:36:44.960
a lot lower by having them kind of reuse basically.

00:36:44.960 --> 00:36:45.500
Yep.

00:36:45.500 --> 00:36:46.160
Absolutely.

00:36:46.160 --> 00:36:51.160
There's some of the key difference,

00:36:51.360 --> 00:36:54.640
I think is mostly that when you have mutable data,

00:36:54.640 --> 00:36:57.240
whereas with threads, you can share it.

00:36:57.240 --> 00:37:02.160
So threads can kind of talk to each other through the data that they share with each other.

00:37:02.160 --> 00:37:05.740
Whereas with sub interpreters, there are a lot of restrictions.

00:37:05.740 --> 00:37:08.780
And I expect we'll work on that to an extent,

00:37:08.780 --> 00:37:11.100
but it's also part of the programming model.

00:37:11.100 --> 00:37:12.480
And like Anthony was saying,

00:37:12.480 --> 00:37:15.580
if you really want to take advantage of parallelism,

00:37:15.580 --> 00:37:17.000
you need to think about it.

00:37:17.200 --> 00:37:22.020
You need to actually be careful about your data and how you're splitting up your work.

00:37:22.020 --> 00:37:25.220
I think there's going to be design patterns that we come to know,

00:37:25.220 --> 00:37:26.900
or conventions we come to know,

00:37:26.900 --> 00:37:29.400
like, let's suppose I need some calculation,

00:37:29.400 --> 00:37:31.460
and I'm going to use it in a for loop.

00:37:31.460 --> 00:37:34.160
You don't run the calculation if it's the same over and over.

00:37:34.160 --> 00:37:35.840
Every time through the loop, you run it,

00:37:35.840 --> 00:37:37.020
and then you use the result, right?

00:37:37.020 --> 00:37:39.640
So in this, you know, a similar thing here would be like,

00:37:39.640 --> 00:37:41.980
well, if you're going to process a bunch of data,

00:37:41.980 --> 00:37:43.440
and the data comes from, say, a database,

00:37:43.440 --> 00:37:46.080
don't do the query and hand it all the records.

00:37:46.420 --> 00:37:48.640
Just tell it, go get that data from the database.

00:37:48.640 --> 00:37:51.280
That way it's already serialized in the right process,

00:37:51.280 --> 00:37:55.460
and there's not this cross serialization to either pickling

00:37:55.460 --> 00:37:57.660
or whatever mechanism you come up with, right?

00:37:57.660 --> 00:38:00.360
But like, try to think about when you get the data,

00:38:00.360 --> 00:38:02.440
can you delay it until it's in the sub process,

00:38:02.440 --> 00:38:05.200
or sub interpreter rather, and so on, right?

00:38:05.200 --> 00:38:06.480
Yeah, definitely.

00:38:06.480 --> 00:38:10.240
One interesting thing is that pep734,

00:38:10.240 --> 00:38:15.940
I've included memory view as one of the types that's supported.

00:38:15.940 --> 00:38:20.800
So basically, you can take a memory view of any kind of object

00:38:20.800 --> 00:38:23.120
that implements the buffer protocol,

00:38:23.120 --> 00:38:26.380
so like numpy arrays and stuff like that,

00:38:26.380 --> 00:38:29.460
and pass that memory view through to another interpreter,

00:38:29.460 --> 00:38:31.980
and you can use it, and it doesn't make a copy or anything.

00:38:31.980 --> 00:38:34.880
It actually uses the same underlying data.

00:38:34.880 --> 00:38:36.380
They actually get shared.

00:38:36.380 --> 00:38:37.320
Oh, that's interesting.

00:38:37.320 --> 00:38:41.460
Yeah, and I think there's even more room for that

00:38:41.460 --> 00:38:45.280
with other types, but we're starting small.

00:38:45.280 --> 00:38:49.680
But the key thing there is that, like you're saying,

00:38:49.680 --> 00:38:54.580
I mean, with coming up with different models and patterns

00:38:54.580 --> 00:38:57.560
and libraries, I'm sure they'll come up

00:38:57.560 --> 00:39:01.440
as people feel out really what's the easiest way

00:39:01.440 --> 00:39:02.740
to take advantage of these features,

00:39:03.040 --> 00:39:05.840
and that's the sort of thing that will apply

00:39:05.840 --> 00:39:08.880
not just to general free-threaded, like, no-gil,

00:39:08.880 --> 00:39:10.200
but also sub-interpreters.

00:39:10.200 --> 00:39:11.380
Definitely.

00:39:11.380 --> 00:39:12.440
It's going to be exciting.

00:39:12.440 --> 00:39:15.540
So I guess I want to move on and talk about

00:39:15.540 --> 00:39:16.600
working with this in Python

00:39:16.600 --> 00:39:18.940
and the stuff that you've done, Anthony,

00:39:18.940 --> 00:39:20.780
but maybe a quick comment from the audience.

00:39:21.160 --> 00:39:23.720
Jazzy asks, is this built on top of a queue,

00:39:23.720 --> 00:39:24.900
which is built on top of a linked list?

00:39:24.900 --> 00:39:27.620
Because I'm building this, and my research led me

00:39:27.620 --> 00:39:28.480
to these data structures.

00:39:28.480 --> 00:39:32.340
I guess that's the communication across sub-interpreter,

00:39:32.340 --> 00:39:34.160
cross-interpreter communication.

00:39:34.160 --> 00:39:36.840
Yeah, with sub-interpreters, like in PEPS 734,

00:39:36.840 --> 00:39:40.460
a queue implements the same interfaces

00:39:40.460 --> 00:39:42.620
as the queue from the queue module.

00:39:42.860 --> 00:39:46.240
But there's no reason why people couldn't implement

00:39:46.240 --> 00:39:48.120
whatever data structure they want

00:39:48.120 --> 00:39:50.440
for communicating between sub-interpreters,

00:39:50.440 --> 00:39:52.320
and then that data structure's in charge

00:39:52.320 --> 00:39:55.460
of preserving thread safety and so forth.

00:39:55.460 --> 00:39:56.380
Yeah, excellent.

00:39:56.380 --> 00:39:57.500
Yeah, it's not a standard queue.

00:39:57.500 --> 00:39:58.800
It's like a concurrent queue

00:39:58.800 --> 00:40:00.200
or something along those lines.

00:40:00.200 --> 00:40:00.720
Yeah.

00:40:00.720 --> 00:40:01.120
Yeah.

00:40:01.380 --> 00:40:04.140
All right, so all of this we've been talking about here

00:40:04.140 --> 00:40:07.740
is we're looking at this cool interpreter pool

00:40:07.740 --> 00:40:08.540
executor stuff.

00:40:08.540 --> 00:40:12.220
That's in draft format, Anthony, for 3.13.

00:40:12.220 --> 00:40:16.240
And somehow I'm looking at this running Python parallel applications

00:40:16.240 --> 00:40:18.220
and sub-interpreters that you're running.

00:40:18.220 --> 00:40:20.280
What's going on here?

00:40:20.280 --> 00:40:21.080
How do you do this magic?

00:40:21.080 --> 00:40:23.760
You need to know the secret password.

00:40:23.760 --> 00:40:28.800
So in Python 3.12,

00:40:30.260 --> 00:40:36.480
the C API for creating sub-interpreters was included,

00:40:36.480 --> 00:40:38.460
and a lot of the mechanism

00:40:38.460 --> 00:40:42.080
for creating sub-interpreters was included.

00:40:42.080 --> 00:40:44.800
So there's also a...

00:40:44.800 --> 00:40:47.960
In CPython, there's a standard library,

00:40:47.960 --> 00:40:49.540
which I think everybody kind of knows.

00:40:49.540 --> 00:40:53.220
And then there are some hidden modules,

00:40:53.220 --> 00:40:55.760
which are mostly used for testing.

00:40:55.760 --> 00:40:58.760
So not all of them get bundled,

00:40:58.760 --> 00:40:59.800
I think, in the distribution.

00:41:00.080 --> 00:41:03.340
I think a lot of the test modules get taken out.

00:41:03.340 --> 00:41:06.720
But there are some hidden modules you can use for testing

00:41:06.720 --> 00:41:09.280
or because a lot of the test suite for CPython

00:41:09.280 --> 00:41:11.040
has to test C APIs,

00:41:11.040 --> 00:41:14.140
and nobody really wants to write unit tests in C.

00:41:14.140 --> 00:41:16.380
So they write the tests in Python,

00:41:16.380 --> 00:41:19.080
and then they kind of create these modules

00:41:19.080 --> 00:41:20.980
that basically just call the C functions.

00:41:20.980 --> 00:41:24.240
And so you can get the test coverage and do the testing from Python code.

00:41:24.240 --> 00:41:29.500
So I guess what was from PEP6...

00:41:29.500 --> 00:41:30.700
I can't remember.

00:41:30.700 --> 00:41:32.500
I look at how many PEP6...

00:41:32.500 --> 00:41:34.160
Eric will probably know.

00:41:34.160 --> 00:41:36.800
What is now PEP734,

00:41:36.800 --> 00:41:42.240
but the Python interface to create sub-interpreters,

00:41:42.240 --> 00:41:45.000
a version of that was included in 3.12.

00:41:45.000 --> 00:41:49.300
So you can import this module called underscore XX sub-interpreters.

00:41:49.300 --> 00:41:51.640
And it's called underscore XX

00:41:51.640 --> 00:41:54.200
because it kind of indicates that it's experimental

00:41:54.200 --> 00:41:57.200
and it's underscore because you probably shouldn't be using it.

00:41:57.200 --> 00:41:59.540
It's not safe for work to me.

00:41:59.540 --> 00:42:01.420
I mean, I don't know.

00:42:02.700 --> 00:42:08.420
But it provides a good way of people actually testing this stuff

00:42:08.420 --> 00:42:13.100
and seeing what happens if I import my C extension from a sub-interpreter.

00:42:13.100 --> 00:42:15.900
So that's kind of some of what I've been doing,

00:42:15.900 --> 00:42:20.460
is looking at, okay, what can we try and do in parallel?

00:42:20.460 --> 00:42:28.340
And this blog post, I wanted to try a WSGI or an ASGI web app.

00:42:28.340 --> 00:42:32.340
And the typical pattern that you have at the moment,

00:42:32.560 --> 00:42:35.640
and I guess how a lot of people would be using parallel code,

00:42:35.640 --> 00:42:37.240
but without really realizing it,

00:42:37.240 --> 00:42:42.060
is when you deploy a web app for Django Flask or FastAPI,

00:42:42.060 --> 00:42:46.120
you can't have one guild per web server

00:42:46.120 --> 00:42:48.860
because if you've got one guild per web server,

00:42:48.860 --> 00:42:51.040
you can only have one user per website,

00:42:51.040 --> 00:42:53.200
which is not great.

00:42:53.200 --> 00:42:57.100
So the way that most web servers implement this

00:42:57.100 --> 00:42:59.360
is that they have a pool of workers.

00:43:00.720 --> 00:43:04.500
G-Unicorn does that by spawning Python processes

00:43:04.500 --> 00:43:06.740
and then using the multi-processing module.

00:43:06.740 --> 00:43:11.680
So it basically creates multiple Python processes all listening to the same socket.

00:43:11.680 --> 00:43:16.680
And then when a web request comes in, one of them takes that request.

00:43:16.680 --> 00:43:19.480
It also then inside that has a thread pool.

00:43:20.260 --> 00:43:24.900
So even basically a thread pool is better for concurrent code.

00:43:24.900 --> 00:43:30.100
So G-Unicorn normally is used in a multi-worker, multi-thread model.

00:43:30.100 --> 00:43:32.160
That's how we kind of talk about it.

00:43:32.160 --> 00:43:35.780
So you'd have the number of workers that you have CPU cores,

00:43:35.780 --> 00:43:39.120
and then inside that you'd have multiple threads.

00:43:39.120 --> 00:43:43.480
So it kind of means you can handle more requests at a time.

00:43:43.480 --> 00:43:47.860
If you've got eight cores, you can handle at least eight requests at a time.

00:43:47.980 --> 00:43:52.180
However, because most web code can be concurrent on the backend,

00:43:52.180 --> 00:43:54.900
like you're making a database query,

00:43:54.900 --> 00:43:57.640
or you're reading some stuff from a file like that,

00:43:57.640 --> 00:44:00.440
that doesn't necessarily need to hold the GIL.

00:44:00.440 --> 00:44:04.220
So you can run it concurrently, which is why you have multiple threads.

00:44:04.220 --> 00:44:06.880
So even if you've only got eight CPU cores,

00:44:06.880 --> 00:44:11.960
you can actually handle 16 or 32 web requests at once,

00:44:12.320 --> 00:44:16.940
because some of them will be waiting for the database server to finish running at SQL query,

00:44:16.940 --> 00:44:20.340
or the API that it called to actually reply.

00:44:20.340 --> 00:44:25.780
So what I wanted to do with this experiment was to look at the multi-worker,

00:44:25.780 --> 00:44:27.520
multi-thread model for web apps,

00:44:27.520 --> 00:44:31.720
and say, okay, could the worker be a sub-interpreter?

00:44:32.600 --> 00:44:35.320
And like, what difference would that make?

00:44:35.320 --> 00:44:38.020
So instead of using multi-processing for the workers,

00:44:38.020 --> 00:44:40.400
could I use sub-interpreters for the workers?

00:44:40.400 --> 00:44:45.120
So even though the Python interface in 3.12 is experimental,

00:44:45.120 --> 00:44:48.400
I basically wanted to adapt Hypercorn,

00:44:48.400 --> 00:44:53.120
which is a web server for ASCII and WSGI apps in Python.

00:44:53.120 --> 00:44:55.480
I wanted to adapt Hypercorn

00:44:55.480 --> 00:44:59.000
and basically start Hypercorn workers from a sub-interpreter pool,

00:44:59.000 --> 00:45:04.660
and then seeing if I can run Django, Flask, and FastAPI in a sub-interpreter.

00:45:04.660 --> 00:45:07.240
So a single process, single Python process,

00:45:07.240 --> 00:45:11.380
but running across multiple cores and listening to web requests

00:45:11.380 --> 00:45:15.220
and basically running and serving web requests with multiple gills.

00:45:15.220 --> 00:45:16.820
So that was the task.

00:45:16.820 --> 00:45:19.140
In the article, you said you had started with G Unicorn,

00:45:19.140 --> 00:45:26.340
and they just made too many assumptions about the web workers being truly sub-processes.

00:45:26.340 --> 00:45:28.240
But Hypercorn was a better fit, you said.

00:45:28.240 --> 00:45:32.420
Yeah, it was easier to implement this experiment in Hypercorn.

00:45:32.420 --> 00:45:35.300
It had like a single entry point.

00:45:35.300 --> 00:45:39.160
Because when you start an interpreter, when you start a sub-interpreter,

00:45:39.160 --> 00:45:41.960
you need to import the modules that you want to use.

00:45:41.960 --> 00:45:46.520
You can't just say, run this function over here.

00:45:46.620 --> 00:45:50.960
You can, but if that function relies on something else that you've imported,

00:45:50.960 --> 00:45:52.980
you need to import that from the new sub-interpreter.

00:45:52.980 --> 00:45:58.680
So what I did with this experiment was basically start a sub-interpreter

00:45:58.680 --> 00:46:01.500
that imports Hypercorn, listens to the sockets,

00:46:01.500 --> 00:46:03.780
and then is ready to serve web requests.

00:46:03.780 --> 00:46:04.480
Interesting.

00:46:04.480 --> 00:46:04.940
Okay.

00:46:04.940 --> 00:46:07.340
And at a minimum, you got it working, right?

00:46:07.340 --> 00:46:09.740
Yeah, it did a hello world.

00:46:09.740 --> 00:46:13.200
So we got that working.

00:46:13.200 --> 00:46:14.720
So I was, yeah, pleased with that.

00:46:16.100 --> 00:46:19.080
And then kind of started doing some more testing of it.

00:46:19.080 --> 00:46:22.440
So, you know, how many concurrent requests can I make at once?

00:46:22.440 --> 00:46:23.780
How does it handle that?

00:46:23.780 --> 00:46:25.980
What does my CPU core load look like?

00:46:25.980 --> 00:46:27.220
Is it distributing it well?

00:46:27.220 --> 00:46:31.040
And then kind of some of the questions are, you know,

00:46:31.040 --> 00:46:34.880
how do you share data between the sub-interpreters?

00:46:35.960 --> 00:46:42.440
So the minimum I had to do was each sub-interpreter needs to know which web socket should I be listening to.

00:46:42.440 --> 00:46:46.360
So like which network socket, once I've started, what port is it running on?

00:46:46.360 --> 00:46:49.040
And is it running on multiple ports?

00:46:49.040 --> 00:46:50.360
And which one should I listen to?

00:46:50.360 --> 00:46:52.280
So yeah, that's the first thing I had to do.

00:46:52.280 --> 00:46:52.700
Nice.

00:46:52.700 --> 00:46:53.120
Yeah.

00:46:53.140 --> 00:47:02.800
Maybe just tell people real quick about just like what are the commands like at the Python level that you look at in order to create an interpreter, run some code on it and so on.

00:47:02.800 --> 00:47:04.760
What's this weird world look like?

00:47:04.760 --> 00:47:05.720
Derek, do you want to cover that?

00:47:06.000 --> 00:47:07.940
Yeah, there is a whole lot.

00:47:07.940 --> 00:47:17.660
I mean, if we talk about PEP 734, you have an interpreters module with a create function in it that returns you an interpreter object.

00:47:17.660 --> 00:47:24.300
And then once you have the interpreter object, it has a function called run or a method.

00:47:24.300 --> 00:47:27.740
The interpreter object also has a method called exec.

00:47:27.740 --> 00:47:29.260
I'm trying to remember.

00:47:29.260 --> 00:47:33.920
It was exec sync because it's synchronous with the current thread.

00:47:34.780 --> 00:47:40.000
And whereas exec run will create a new thread for you and run things in that there.

00:47:40.000 --> 00:47:41.860
So they're kind of different use cases.

00:47:41.860 --> 00:47:43.720
But it's basically the same thing.

00:47:43.720 --> 00:47:44.620
You have some code.

00:47:44.620 --> 00:47:53.200
Currently supports just you give it a string with all your code on it, like you load it from a file or something.

00:47:53.200 --> 00:47:56.860
You know, basically it's a script that's going to run in that sub integer.

00:47:56.860 --> 00:48:00.000
Alternately, you can give it a function.

00:48:00.440 --> 00:48:05.300
And as long as that function isn't a closure, doesn't have any arguments and stuff like that.

00:48:05.300 --> 00:48:08.960
So it's just like really basic, basically a script, right?

00:48:08.960 --> 00:48:12.620
If you got something like that, you can also pass that through.

00:48:12.620 --> 00:48:14.380
And then it runs it.

00:48:14.380 --> 00:48:16.600
And that's just about it.

00:48:16.600 --> 00:48:21.240
If you want to get some results back, you're going to have to manually pass them back kind of like you do with threads.

00:48:21.820 --> 00:48:24.160
But that's something people already understand pretty well.

00:48:24.160 --> 00:48:24.540
Right.

00:48:24.540 --> 00:48:25.660
And create one of those channels.

00:48:25.660 --> 00:48:29.340
And then you just wait for it to exit and then read from the channel or something like that.

00:48:29.340 --> 00:48:29.700
Yeah.

00:48:29.700 --> 00:48:30.100
Yeah.

00:48:30.100 --> 00:48:33.300
And so there's a way to say things like just run.

00:48:33.300 --> 00:48:36.120
And there's also a way to say create an interpreter.

00:48:36.120 --> 00:48:38.660
And then you could use the interpreter to do things.

00:48:38.660 --> 00:48:43.720
And that lets you only pay the process like startup cost once, right?

00:48:43.980 --> 00:48:44.220
Yeah.

00:48:44.220 --> 00:48:44.720
Yeah.

00:48:44.720 --> 00:48:49.080
And you can also, you can call that the run multiple times.

00:48:49.080 --> 00:48:52.360
And each time it kind of adds on to what ran before.

00:48:52.720 --> 00:49:02.900
So if you run some code that modifies things or import some modules and that sort of thing, those will still be there the next time you run some code in that interpreter.

00:49:02.900 --> 00:49:10.460
Which is nice because then if you've got some startup stuff that you need to do one time, you can do that ahead of time right after you create the interpreter.

00:49:10.460 --> 00:49:16.400
But then in kind of your loop in your worker, then you run again and all that stuff is ready to go.

00:49:16.680 --> 00:49:22.520
Oh, that's interesting because when I think about, say, my web apps, a lot of them talk to MongoDB and use Beanie.

00:49:22.520 --> 00:49:28.300
And you go to Beanie and you tell it to like create a connection or a MongoDB client pool.

00:49:28.300 --> 00:49:30.060
And it does all that stuff.

00:49:30.060 --> 00:49:31.800
And then you just ambiently talk to it.

00:49:31.800 --> 00:49:33.620
Like go to that, you know, kind of like Django or whatever, right?

00:49:33.620 --> 00:49:35.620
Go to that class and do a query on it.

00:49:35.620 --> 00:49:40.740
You could run that startup code like once potentially and have that pool just hanging around for subsequent work.

00:49:40.740 --> 00:49:41.700
Nice.

00:49:41.700 --> 00:49:42.440
All right.

00:49:42.440 --> 00:49:44.560
Let's see some more stuff.

00:49:45.040 --> 00:49:47.960
So you said you got it working pretty well, Anthony.

00:49:47.960 --> 00:49:51.480
And you said one of the challenges was trying to get it to shut down, right?

00:49:51.480 --> 00:49:52.540
Mm-hmm.

00:49:52.540 --> 00:49:52.900
Yeah.

00:49:52.900 --> 00:49:53.260
Yeah.

00:49:53.260 --> 00:50:01.420
So in Python, when you start a Python process, you can press Ctrl-C to quit, which is a keyboard interrupt.

00:50:01.420 --> 00:50:05.920
That kind of sends the interrupt in that process.

00:50:07.060 --> 00:50:16.400
All of these web servers have got like a mechanism for cleanly shutting down because you don't want to just, if you press Ctrl-C, you don't want to just terminate the processes.

00:50:16.400 --> 00:50:22.620
Because when you write an ASCII app in particular, you can have like events that you can do.

00:50:22.780 --> 00:50:32.620
So people who've done FastAPI probably know the on-event decorator that you can put and say, when my app starts up, create a database connection pool.

00:50:32.620 --> 00:50:35.340
And when it shuts down, then go and clean up all this stuff.

00:50:35.340 --> 00:50:48.620
So if the web servers decided to shut down for whatever reason, whether you've pressed Ctrl-C or it just decided to close for whatever reason, it needs to tell all the workers to shut down cleanly.

00:50:49.620 --> 00:51:00.180
So signals, like the signals module, doesn't work between sub-interpreters because it kind of sits in the interpreter state, from what I understand.

00:51:00.180 --> 00:51:14.880
So what I did was basically use a channel so that the main worker, like the coordinator, when that had a shutdown request, it would send a message to all of the sub-interpreters to say, okay, can you stop now?

00:51:15.240 --> 00:51:28.160
And then it would kick off a job, basically tell Hypercorn, in this case, to shut down cleanly, call any shutdown functions that you might have, and then log a message to say that it's shutting down as well.

00:51:28.160 --> 00:51:38.100
Because the other thing is, with web servers, if it just terminated immediately, and then you looked at your logs, and you were like, okay, why did the website suddenly stop working?

00:51:38.100 --> 00:51:44.920
And there was no log entries, and it just went from, I'm handling requests, to just, you know, absolute silence.

00:51:44.920 --> 00:51:44.960
Yes.

00:51:44.960 --> 00:51:46.960
That also wouldn't be very helpful.

00:51:46.960 --> 00:51:50.720
So it needs to write log messages, it needs to call, like, shutdown functions and stuff.

00:51:50.720 --> 00:51:51.280
Right.

00:51:51.280 --> 00:52:01.880
So what I did was, and this is, I guess, where it's kind of a bit of a turtles all the way down, but inside the sub-interpreter, I start another thread.

00:52:02.920 --> 00:52:09.120
Because if you have a poller which listens to a signal on a channel, that's a blocking operation.

00:52:09.120 --> 00:52:15.260
So, you know, at the bottom of my sub-interpreter code, I've got, okay, run Hypercorn.

00:52:15.260 --> 00:52:18.400
So it's going to run, it's going to listen to Socket's web requests.

00:52:18.760 --> 00:52:30.020
But I need to also be able to run concurrently in the sub-interpreter a loop which listens to the communication channel and sees if a shutdown request has been sent.

00:52:30.020 --> 00:52:39.020
So this is kind of a, maybe an implementation detail of how interpreters work in Python, but interpreters have threads as well.

00:52:39.200 --> 00:52:42.480
So you can start threads inside interpreters.

00:52:42.480 --> 00:52:49.820
So similar to what I said with G-Unicorn and Hypercorn, how you've got multi-worker, multi-thread, like each worker has its own threads.

00:52:49.820 --> 00:52:53.300
In Python, interpreters have the threads.

00:52:53.300 --> 00:52:59.200
So you can start a sub-interpreter and then inside that sub-interpreter, you can also start multiple threads.

00:52:59.200 --> 00:53:03.220
And you can do coroutines and all that kind of stuff as well.

00:53:03.220 --> 00:53:08.100
So basically what I did is to start a sub-interpreter which also starts a thread,

00:53:08.440 --> 00:53:12.680
and that thread listens to the communication channel and then waits for a shutdown request.

00:53:12.680 --> 00:53:12.980
Right.

00:53:12.980 --> 00:53:16.000
Tells Hypercorn, all right, you're done.

00:53:16.000 --> 00:53:17.180
We're out of here.

00:53:17.180 --> 00:53:17.660
Yeah.

00:53:17.660 --> 00:53:18.040
Okay.

00:53:18.040 --> 00:53:18.680
Interesting.

00:53:18.680 --> 00:53:21.460
Here's an interesting question from the audience, from Chris.

00:53:21.460 --> 00:53:27.020
Well, it says, when you, we talked about the global kind of startup, like if you run that once, it'll already be set.

00:53:27.020 --> 00:53:31.920
And, you know, does that make code somewhat non-deterministic in a sub-interpreter?

00:53:31.920 --> 00:53:33.860
I mean, if you explicitly work with it, no.

00:53:33.860 --> 00:53:36.280
But if you're doing the pool, like which one do you get?

00:53:36.280 --> 00:53:37.880
Is it initialized or not?

00:53:38.160 --> 00:53:44.740
Eric, do you have an idea of a startup function that runs in the interpreter pool executor type thing?

00:53:44.740 --> 00:53:47.820
Or is it just they get doled out and they run what they run?

00:53:47.820 --> 00:53:54.200
With concurrent features, it's already kind of a pattern.

00:53:54.200 --> 00:53:58.600
You have an initialized function that you can call that'll do the right thing.

00:53:58.600 --> 00:54:04.680
And then you have your task that the worker's actually running.

00:54:05.240 --> 00:54:14.340
So with the, I don't know, I wouldn't say it's non-deterministic unless you have no control over it.

00:54:14.340 --> 00:54:23.720
I mean, it's if you want to make sure that state progresses in an expected way, then you're going to run your own sub-interpreters, right?

00:54:23.760 --> 00:54:35.160
But if you have no control over the sub-interpreters, you're just like handing off to some library that's using sub-interpreters, I would think it'd be somewhat not quite so important about whether it's deterministic or not.

00:54:35.420 --> 00:54:40.700
I mean, each time it runs, there are a variety of things.

00:54:40.700 --> 00:54:58.820
The whole thing could be kind of reset or you could make sure that anything that runs it, any part of your code that runs is careful to keep its state self-contained and therefore preserve determinist behavior that way.

00:54:58.820 --> 00:55:04.940
One thing I do a lot is I'll write code that'll say, you know, if this is already initialized, don't do it again.

00:55:04.940 --> 00:55:07.180
So I talked about like the database connection thing.

00:55:07.180 --> 00:55:11.800
If somebody were to call it twice, it'll say, well, it looks like the connection's already not none.

00:55:11.800 --> 00:55:13.120
So we're good.

00:55:13.120 --> 00:55:13.780
Right.

00:55:13.780 --> 00:55:20.960
You could just always run the startup code with one of these like short circuit things that says, hey, it looks like this interpreter, this is already done.

00:55:20.960 --> 00:55:21.600
We're good.

00:55:21.600 --> 00:55:25.720
But, you know, that would probably handle a good chunk of it right there.

00:55:25.720 --> 00:55:28.380
But we're back to this thing that Anthony said, right?

00:55:28.380 --> 00:55:31.620
Like we're going to learn some new programming patterns potentially.

00:55:31.620 --> 00:55:32.200
Yeah.

00:55:32.200 --> 00:55:33.060
Quite interesting.

00:55:33.060 --> 00:55:40.000
So we talked at the beginning about how sub interpreters have their own memory and their own module loads and all those kinds of things.

00:55:40.000 --> 00:55:42.220
And that might be potentially interesting for isolation.

00:55:42.220 --> 00:55:48.840
Also kind of tying back to Chris's comment here, this isolation is pretty interesting for testing.

00:55:48.840 --> 00:55:49.700
Right, Anthony?

00:55:49.700 --> 00:55:50.640
Like my test.

00:55:50.640 --> 00:55:56.500
So another thing you've been up to is working with trying to run pie test sessions in sub interpreters.

00:55:56.500 --> 00:55:57.500
Tell people about that.

00:55:57.800 --> 00:55:58.000
Yeah.

00:55:58.000 --> 00:56:00.920
So I started off with a web worker.

00:56:00.920 --> 00:56:06.260
One of the things I hit with a web worker was that I couldn't start Django applications.

00:56:07.660 --> 00:56:13.120
And realized the reason was the date time module.

00:56:13.120 --> 00:56:18.440
So the Python standard library, some of the modules are implemented in Python.

00:56:18.440 --> 00:56:20.400
Some of them are implemented in C.

00:56:20.400 --> 00:56:22.640
Some of them are a combination of both.

00:56:22.640 --> 00:56:31.060
So some modules you import in the standard library have like a C part that's been implemented in C for performance reasons typically.

00:56:31.060 --> 00:56:36.060
Or because it needs some special operating system API that you can't access from Python.

00:56:36.060 --> 00:56:38.200
And then the front end is Python.

00:56:38.200 --> 00:56:48.260
So there is a list basically of standard library modules that are written in C that have some sort of global state.

00:56:48.660 --> 00:57:00.760
And then the core developers have been going down that list and fixing them up so that they can be imported from a sub interpreter or just marking them as not compatible with sub interpreters.

00:57:00.760 --> 00:57:08.880
One such example was the read line module that Eric and I were kind of working on last week and the week before.

00:57:08.880 --> 00:57:13.380
Read line is used for, I guess, listening to like user input.

00:57:13.380 --> 00:57:20.100
So if you run the input built in, like read line is one of the utilities it uses to listen to keyboard input.

00:57:20.100 --> 00:57:27.300
If you start, let's say you started five sub interpreters at the same time and all of them did a read line listened for input.

00:57:27.300 --> 00:57:29.880
Like what would you expect the behavior to be?

00:57:29.880 --> 00:57:35.500
Which when you type in the keyboard, where would you expect the letters to come out?

00:57:35.500 --> 00:57:37.280
So it kind of poses an interesting question.

00:57:37.980 --> 00:57:46.360
So read line is not compatible with sub interpreters, but it discovered like it was actually sharing a global state.

00:57:46.360 --> 00:57:50.240
So when it initialized, it would install like a callback.

00:57:50.240 --> 00:57:59.900
And what that meant was that even though it said it's not compatible, if you started multiple sub interpreters that imported read line, it would crash Python itself.

00:58:00.760 --> 00:58:04.960
The date time module is another one that needs fixing.

00:58:04.960 --> 00:58:07.600
It installs a bunch of global state.

00:58:07.600 --> 00:58:10.380
So yeah, date time was another one.

00:58:10.380 --> 00:58:22.140
So what I wanted to do is to try and test some other C extensions that I had and just basically write a pytest extension, a pytest plugin, I guess,

00:58:22.280 --> 00:58:28.180
which you've got an existing pytest suite, but you want to run all of that in a sub interpreter.

00:58:28.180 --> 00:58:40.120
And the goal of this is really that you're developing a C extension, you've written a test suite already for pytest, and you want to run that inside a sub interpreter.

00:58:40.780 --> 00:58:54.380
So I'm looking at this from a couple of different angles, but I want to really try and use sub interpreters in other ways, import some C extensions that have never even considered the idea of sub interpreters and just see how they respond to it.

00:58:54.380 --> 00:58:57.420
Like read line was a good example.

00:58:57.420 --> 00:59:02.520
Like I think it was a, this won't work, but the fact that it crashed is bad.

00:59:02.520 --> 00:59:03.820
How is it going to crash, right?

00:59:03.820 --> 00:59:04.940
Like what's happening there?

00:59:05.280 --> 00:59:05.560
Yeah.

00:59:05.560 --> 00:59:09.240
So it should have, it should have kind of just said, this is not compatible.

00:59:09.240 --> 00:59:14.080
And that's kind of uncovered a, and this is all super experimental as well.

00:59:14.080 --> 00:59:21.340
So like, this is not, you know, you've, you've had to import the underscore XX module to even try this.

00:59:21.340 --> 00:59:26.220
So yeah, there's, there's read line date time was another one.

00:59:26.220 --> 00:59:33.380
And so I put this sort of pytest extension together so that I could run some existing test suites inside sub interpreters.

00:59:33.920 --> 00:59:40.780
And then the next thing that I looked at doing was CPython has a huge test suite.

00:59:40.780 --> 00:59:54.920
So basically how all of Python itself is tested, the parser, the compiler, the evaluation loop, all of the standard library modules have got pretty good test coverage.

00:59:54.920 --> 01:00:03.540
So like when you compile Python from source or you make changes on GitHub, like it runs the test suite to make sure that your changes didn't break anything.

01:00:04.780 --> 01:00:14.840
Now the next thing I kind of wanted to look at was, okay, can we, to try and kind of get ahead of the curve really on sub interpreter adoption.

01:00:14.840 --> 01:00:26.140
So in 3.13 when PEP 7.3.4 lands, can we try and test all of the standard library inside a sub interpreter and see if it has any other weird behaviors.

01:00:26.700 --> 01:00:40.600
And this test will probably apply to free threading as well, to be honest, because I think anything that you're, you're doing like this, you're importing these C extensions, which always assume that there was a big GIL in place.

01:00:40.960 --> 01:00:44.640
If you take away that assumption, then you get these strange behaviors.

01:00:44.640 --> 01:00:54.860
So yeah, the next thing I've been working on is basically running the CPython test suite inside sub interpreters and then seeing what kind of weird behaviors pop up.

01:00:54.860 --> 01:01:00.680
I think it's a great idea because obviously CPython is going to need to run code in a sub interpreter, run R code, right?

01:01:00.680 --> 01:01:06.520
So at a minimum, the framework interpreter, all the runtime bits, that should hang together, right?

01:01:06.520 --> 01:01:07.040
Yeah.

01:01:07.040 --> 01:01:10.040
There are some modules that it doesn't make sense to run in sub interpreters.

01:01:10.160 --> 01:01:11.300
Readline was an example.

01:01:11.300 --> 01:01:13.680
Some TK interrupt, maybe?

01:01:13.680 --> 01:01:14.380
Yeah.

01:01:14.380 --> 01:01:15.420
Yeah, possibly.

01:01:15.420 --> 01:01:17.320
Maybe not, actually.

01:01:17.320 --> 01:01:17.680
I don't know.

01:01:17.680 --> 01:01:18.200
Yeah.

01:01:18.200 --> 01:01:27.460
If you think about like, if you got, when you're doing GUI programming, right, you're going to have kind of your core stuff running the main thread, right?

01:01:27.460 --> 01:01:36.100
And then you hand off, you may have sub threads doing some other work, but the core of the application, think of it as running in the main thread.

01:01:36.100 --> 01:01:38.580
I think of applications in that way.

01:01:38.660 --> 01:01:46.240
And there are certain things that you do in Python, standard library modules that really only make sense with that main thread.

01:01:46.240 --> 01:01:51.420
So supporting those in sub interpreters isn't quite as meaningful.

01:01:51.420 --> 01:01:51.820
Yeah.

01:01:52.520 --> 01:01:54.280
I can't remember all the details.

01:01:54.280 --> 01:02:05.220
So I feel like there are some parts of Windows itself, some UI frameworks there that required that you access them on the main program thread, not on some background thread as well, because it would freak things out.

01:02:05.220 --> 01:02:06.960
So it seems like not unreasonable.

01:02:07.440 --> 01:02:08.160
Yeah, same is true.

01:02:08.160 --> 01:02:12.480
Like the signal module, I remember at exit, a few others.

01:02:12.480 --> 01:02:13.100
Excellent.

01:02:13.100 --> 01:02:13.720
All right.

01:02:13.720 --> 01:02:15.840
Well, I guess let's, we're getting short on time.

01:02:15.840 --> 01:02:17.540
Let's wrap it up with this.

01:02:17.680 --> 01:02:24.680
So the big thing to keep an eye on really here is PEP 734, because that's when this would land.

01:02:24.680 --> 01:02:28.600
You're no longer with the underscore XX sub interpreter.

01:02:28.600 --> 01:02:32.120
You're just working with interpreters sub module.

01:02:32.120 --> 01:02:32.580
Yeah.

01:02:32.580 --> 01:02:33.020
313.

01:02:33.580 --> 01:02:33.780
Yeah.

01:02:33.780 --> 01:02:35.200
So right now it's in draft.

01:02:35.200 --> 01:02:37.200
Like, what's it looking like?

01:02:37.200 --> 01:02:42.300
If it'll be in 313, it'll be in 313 alpha something, some beta something.

01:02:42.300 --> 01:02:46.580
Like, when is this going to start looking like a thing that is ready for people to play with?

01:02:46.580 --> 01:02:55.020
So I, yeah, this PEP, I went through and did a massive cleanup of PEP 554, which is why I made a new PEP for it.

01:02:55.820 --> 01:03:06.940
I simplified a lot of things, clarified a lot of points, had lots of good feedback from people and ended up with what I think is a good API, but it was a little different in some ways.

01:03:06.940 --> 01:03:12.760
So I've had the implementation for PEP 554 mostly done and ready to go for years.

01:03:12.760 --> 01:03:25.800
And so it was a matter, it's been a matter of now that I have this, this updated PEP up, going back to the implementation, tweaking it to match, and then making sure everything still feels right.

01:03:25.800 --> 01:03:28.160
I try and use it in a few cases.

01:03:28.160 --> 01:03:33.400
And if everything looks good, then go ahead and I'll start a discussion on that.

01:03:33.400 --> 01:03:38.400
I'm hoping within the next week or two to start up a round of discussion about this PEP.

01:03:38.400 --> 01:03:45.820
And hopefully we won't have a whole lot of back and forth so I can get this over to the steering councils in the near future.

01:03:45.820 --> 01:03:48.780
Well, the hard work has been done already, right?

01:03:48.780 --> 01:03:49.120
Yeah.

01:03:49.120 --> 01:03:52.980
The C layer is there and it's accepted and it's in there.

01:03:52.980 --> 01:03:56.640
Now it's just a matter of what's the right way to look at it from Python, right?

01:03:56.640 --> 01:04:07.360
And one thing to keep in mind is that I'm planning on backporting the module to Python 3.12 just so that we have a print interpreter Gale in 3.12.

01:04:07.360 --> 01:04:10.600
So it'd be nice if people could really take advantage of it.

01:04:11.020 --> 01:04:14.520
So for that one, we'd have to pip install it or would it be added as...

01:04:14.520 --> 01:04:14.700
Yeah.

01:04:14.700 --> 01:04:15.400
Pip install.

01:04:15.400 --> 01:04:15.980
Okay.

01:04:15.980 --> 01:04:18.540
I probably won't support before 3.12.

01:04:18.540 --> 01:04:23.060
I mean, subinterpreters have been around for decades, but only through the C API.

01:04:23.060 --> 01:04:28.920
But that said, I doubt I'll backport this module past 3.12.

01:04:28.920 --> 01:04:30.620
So just 3.12 and up.

01:04:30.620 --> 01:04:32.920
That's more than I expected anyway.

01:04:32.920 --> 01:04:33.580
So that's pretty cool.

01:04:33.580 --> 01:04:34.120
All right.

01:04:34.120 --> 01:04:35.800
Final thoughts, you guys.

01:04:35.800 --> 01:04:37.460
What do you want to tell people about this stuff?

01:04:38.300 --> 01:04:40.820
Personally, I'm excited for where everything's going.

01:04:40.820 --> 01:04:45.120
It's taken a while, but I think we're getting to a good place.

01:04:45.120 --> 01:04:51.800
It's interesting with all the discussion about no-gil, it's easy to think, oh, then why do we need subinterpreters?

01:04:51.800 --> 01:04:54.880
Or if we have subinterpreters, why do we need no-gil?

01:04:54.880 --> 01:04:56.960
But they're kind of different needs.

01:04:56.960 --> 01:05:04.480
The most interesting thing for me is that what's good for no-gil is good for subinterpreters and vice versa.

01:05:04.480 --> 01:05:12.720
That no-gil probably really wouldn't be possible without a lot of the work that we've done to make a per-interpreter GIL possible.

01:05:12.720 --> 01:05:16.160
So I think that's one of the neat things.

01:05:16.160 --> 01:05:19.800
The future's looking bright for Python multicore.

01:05:19.800 --> 01:05:24.380
And I'm excited to see where people go with all these things that we're adding.

01:05:24.380 --> 01:05:29.020
When's the subinterpreters programming design patterns about coming out?

01:05:29.020 --> 01:05:33.380
Yeah, my thoughts are...

01:05:33.380 --> 01:05:40.280
Subinterpreters are mentioned in my book, actually, when it was like Python 3.9, I think.

01:05:42.360 --> 01:05:45.740
Because it was possible then, but it's changed quite a lot since.

01:05:45.740 --> 01:05:50.540
They, I guess, kind of some thoughts to leave people with.

01:05:50.540 --> 01:05:57.780
I think if you're a maintainer of a Python package or a C extension module in a Python package,

01:05:57.780 --> 01:06:03.660
there's going to be a lot more exotic scenarios for you to test coming in the next year or so.

01:06:05.100 --> 01:06:12.120
And some of those uncover things that you might have done or just kind of relied on the GIL with global state,

01:06:12.120 --> 01:06:17.200
where that's not really desirable anymore and you're going to get bugs down the line.

01:06:17.200 --> 01:06:22.780
So I think with any of that stuff as a package maintainer, you want to test as many scenarios as you can

01:06:22.780 --> 01:06:26.440
so that you can catch bugs and fix them before your users find them.

01:06:26.960 --> 01:06:32.180
So if you are a package maintainer, there's definitely some things that you can start to look at now to test.

01:06:32.180 --> 01:06:39.460
That's available in 313 Alpha 2 is at least probably the one I've tried, to be honest.

01:06:39.460 --> 01:06:44.600
And if you're a developer, not necessarily a maintainer,

01:06:44.600 --> 01:06:51.160
then I think this is a good time to start reading up on parallel programming

01:06:51.160 --> 01:06:54.720
and how you need to design parallel programs.

01:06:55.480 --> 01:07:01.240
And those kind of concepts are the same across all languages and Python would be no different.

01:07:01.240 --> 01:07:05.800
We just have different mechanisms for starting parallel work and joining it back together.

01:07:05.800 --> 01:07:10.500
But if you're interested in this and you want to run more code in parallel,

01:07:10.500 --> 01:07:18.940
there's definitely some stuff to read and some stuff to learn about in terms of signals, pipes, queues,

01:07:20.260 --> 01:07:27.260
sharing data, how you have locks and where you should put them, how deadlocks can occur, things like that.

01:07:27.260 --> 01:07:29.680
So all of that stuff is the same in Python as anywhere else.

01:07:29.680 --> 01:07:31.980
We just have different mechanisms for doing it.

01:07:31.980 --> 01:07:32.600
All right.

01:07:32.600 --> 01:07:35.160
Well, people have some research work.

01:07:35.160 --> 01:07:38.860
And I guess a really, really quick final question, Eric, and then we'll wrap this up.

01:07:38.860 --> 01:07:42.940
Following up on what Anthony said, like test your stuff, make sure it works in a sub-interpreter.

01:07:42.940 --> 01:07:47.680
If for some reason you're like, my code will not work in a sub-interpreter and I'm not ready yet,

01:07:47.680 --> 01:07:54.060
is there a way to determine that your code is being run in a sub-interpreter rather than regularly from your Python code?

01:07:54.060 --> 01:07:59.380
Yeah. If you have an extension module that supports sub-interpreters,

01:07:59.380 --> 01:08:04.920
then you will have updated your module to use what's called multi-phase init.

01:08:04.920 --> 01:08:09.160
And that's something that shouldn't be too hard to look up.

01:08:09.160 --> 01:08:10.900
I think I talked about it in the PEP.

01:08:10.900 --> 01:08:17.300
If you implement multi-phase init, then you've already done most of the work to support a sub-interpreter.

01:08:17.300 --> 01:08:23.820
If you haven't, then your module can't be imported in a sub-interpreter.

01:08:23.980 --> 01:08:28.180
It'll actually fail with an import error if you try and import it in a sub-interpreter,

01:08:28.180 --> 01:08:31.560
or at least a sub-interpreter that has its own GIL.

01:08:31.560 --> 01:08:35.600
There are ways to create sub-interpreters that still share GIL and that sort of thing.

01:08:35.600 --> 01:08:39.140
But you just won't be able to import it at all.

01:08:39.140 --> 01:08:43.620
So like the readline module can't be imported in sub-interpreters.

01:08:43.620 --> 01:08:51.960
The issue that Anthony ran into is kind of a subtle side effect of the check that we're doing.

01:08:53.900 --> 01:08:57.800
But really, it boils down to if you don't implement multi-phase init,

01:08:57.800 --> 01:09:00.600
then you won't be able to import the module.

01:09:00.600 --> 01:09:02.260
You'll just get an importer.

01:09:02.260 --> 01:09:04.660
So that's, I mean, it makes it kind of straightforward.

01:09:04.660 --> 01:09:06.100
Yeah, sounds good.

01:09:06.100 --> 01:09:07.780
More opt-in than opt-out.

01:09:07.780 --> 01:09:08.160
Yep.

01:09:08.160 --> 01:09:08.680
Right on.

01:09:08.680 --> 01:09:09.320
All right, guys.

01:09:09.320 --> 01:09:12.740
Thank you both for coming back on the show and awesome work.

01:09:12.740 --> 01:09:15.760
This is looking close to the fetish line and exciting.

01:09:15.760 --> 01:09:16.440
Thanks, Michael.

01:09:16.760 --> 01:09:16.940
Yep.

01:09:16.940 --> 01:09:17.460
See y'all.

01:09:18.860 --> 01:09:21.500
This has been another episode of Talk Python to Me.

01:09:21.500 --> 01:09:23.320
Thank you to our sponsors.

01:09:23.320 --> 01:09:24.920
Be sure to check out what they're offering.

01:09:24.920 --> 01:09:26.340
It really helps support the show.

01:09:26.340 --> 01:09:29.640
Are you ready to level up your Python career?

01:09:29.640 --> 01:09:34.560
And could you use a little bit of personal and individualized guidance to do so?

01:09:34.980 --> 01:09:41.340
Check out the PyBytes Python Developer Mindset program at talkpython.fm/pdm.

01:09:41.980 --> 01:09:43.680
Take some stress out of your life.

01:09:43.680 --> 01:09:49.160
Get notified immediately about errors and performance issues in your web or mobile applications with

01:09:49.160 --> 01:09:49.460
Sentry.

01:09:49.460 --> 01:09:54.460
Just visit talkpython.fm/sentry and get started for free.

01:09:54.460 --> 01:09:58.060
And be sure to use the promo code talkpython, all one word.

01:09:58.060 --> 01:09:59.800
Want to level up your Python?

01:09:59.800 --> 01:10:03.840
We have one of the largest catalogs of Python video courses over at Talk Python.

01:10:03.840 --> 01:10:09.020
Our content ranges from true beginners to deeply advanced topics like memory and async.

01:10:09.020 --> 01:10:11.680
And best of all, there's not a subscription in sight.

01:10:11.680 --> 01:10:14.600
Check it out for yourself at training.talkpython.fm.

01:10:14.600 --> 01:10:16.700
Be sure to subscribe to the show.

01:10:16.700 --> 01:10:19.480
Open your favorite podcast app and search for Python.

01:10:19.480 --> 01:10:20.780
We should be right at the top.

01:10:20.780 --> 01:10:25.960
You can also find the iTunes feed at /itunes, the Google Play feed at /play,

01:10:25.960 --> 01:10:30.160
and the direct RSS feed at /rss on talkpython.fm.

01:10:30.160 --> 01:10:33.120
We're live streaming most of our recordings these days.

01:10:33.120 --> 01:10:36.520
If you want to be part of the show and have your comments featured on the air,

01:10:36.520 --> 01:10:40.900
be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

01:10:40.900 --> 01:10:43.000
This is your host, Michael Kennedy.

01:10:43.000 --> 01:10:44.300
Thanks so much for listening.

01:10:44.300 --> 01:10:45.460
I really appreciate it.

01:10:45.460 --> 01:10:47.360
Now get out there and write some Python code.

01:10:47.360 --> 01:10:47.960
I'll see you next time.

01:10:47.960 --> 01:10:48.560
Bye.

01:10:48.560 --> 01:10:48.560
Bye.

01:10:48.560 --> 01:10:48.600
Bye.

01:10:48.600 --> 01:10:48.780
Bye.

01:10:48.780 --> 01:10:49.400
Bye.

01:10:49.400 --> 01:10:49.440
Bye.

01:10:49.440 --> 01:10:49.780
Bye.

01:10:49.780 --> 01:10:50.780
Bye.

01:10:50.780 --> 01:10:50.780
Bye.

01:10:50.780 --> 01:10:50.780
Bye.

01:10:50.780 --> 01:10:50.780
Bye.

01:10:50.780 --> 01:10:50.780
Bye.

01:10:50.780 --> 01:10:51.780
Bye.

01:10:51.780 --> 01:10:52.780
Bye.

01:10:52.780 --> 01:10:52.780
Bye.

01:10:52.780 --> 01:10:52.780
Bye.

01:10:52.780 --> 01:10:53.780
Bye.

01:10:53.780 --> 01:11:23.760
Thank you.

