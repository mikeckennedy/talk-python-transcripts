00:00:00 When you think about processing tabular data in Python, what library comes to mind?

00:00:04 Pandas, I'd guess.

00:00:05 But there are other libraries out there, and Polar's is one of the more exciting new ones.

00:00:10 It's built in Rust, embraces parallelism, and can be 10 to 20 times faster than Pandas out of the box.

00:00:17 We have Polar's creator, Richie Vink, here to give us a look at this exciting new data frame library.

00:00:23 This is Talk Python to Me, episode 402, recorded January 29, 2023.

00:00:29 Welcome to Talk Python to Me, a weekly podcast on Python.

00:00:46 This is your host, Michael Kennedy.

00:00:48 Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython, both on fosstodon.org.

00:00:55 Be careful with impersonating accounts on other instances.

00:00:58 There are many.

00:00:59 Keep up with the show and listen to over seven years of past episodes at talkpython.fm.

00:01:04 We've started streaming most of our episodes live on YouTube.

00:01:08 Subscribe to our YouTube channel over at talkpython.fm/youtube to get notified about upcoming shows and be part of that episode.

00:01:17 This episode is brought to you by TypePy.

00:01:19 TypePy is here to take on the challenge of rapidly transforming a bare algorithm in Python into a full-fledged decision support system for end users.

00:01:26 Check them out at talkpython.fm/taipy, T-A-I-P-Y.

00:01:31 And it's also brought to you by User Interviews.

00:01:34 Earn extra income for sharing your software developer opinion.

00:01:38 Head over to talkpython.fm/userinterviews to participate today.

00:01:43 Hey, Richie.

00:01:44 Welcome to Talk Python To Me.

00:01:46 Hey, Michael.

00:01:46 Thanks for having me.

00:01:47 Great to be here.

00:01:48 I feel like maybe I should rename my podcast TalkRust to me or something.

00:01:52 I don't know.

00:01:52 Rust is taking over.

00:01:54 Yeah.

00:01:55 As the low-level part of how do we make Python go fast?

00:01:59 There's some kind of synergy with Rust.

00:02:01 What's going on there?

00:02:02 Yeah, there is.

00:02:03 I'd say Python already was low-level languages that succeeded, that made Python a success.

00:02:09 I mean, like NumPy, Pandas, everything that was reasonable fast was so because of C or Cyton, which is also C.

00:02:16 But Rust, different from C, Rust has made low-level programming a lot more fun to use and a lot more safe.

00:02:24 And especially if you regard multi-trend programming, parallel programming, concurrent programming, it is a lot easier in Rust.

00:02:32 It opens a lot of possibilities.

00:02:34 Yeah, my understanding.

00:02:35 I've only given a cursory look to Rust to sort of scan some examples.

00:02:40 And we're going to see some examples of code in a little bit, actually, related to pullers.

00:02:45 But it's kind of a low-level language.

00:02:47 It's not as simple as Python.

00:02:48 No.

00:02:49 It's a JavaScript.

00:02:51 But it is easier than C, C++, not just in the syntax, but it does better memory tracking for you and the concurrency especially, right?

00:03:00 Yeah, well, so Rust brings a whole new thing to the table, which is called ownership and a borer checker.

00:03:07 And Rust is really strict.

00:03:08 There are things in Rust you cannot do in C or C++ because at a time there can only be one owner of a piece of memory.

00:03:15 And other people can.

00:03:16 You can lend out this piece of memory to other users, but then they cannot mutate it.

00:03:21 So there can be only one owner which is able to mutate something.

00:03:24 And this restriction makes Rust a really hard language to learn.

00:03:29 But once it's clicked, once you went over that steep learning curve, it becomes a lot easier because it doesn't allow you things that you could do in C and C++.

00:03:39 But those things were also things you shouldn't do in C and C++ because they probably led to sac faults and to memory issues.

00:03:46 And this borer checker also makes writing concurrent programming safe.

00:03:51 You can have many threads reading a variable all they want.

00:03:55 They can read concurrently.

00:03:56 It's when you have writers and readers that this whole thread safety, critical section, take your locks or the locks re-entering.

00:04:04 All of that really difficult stuff comes in.

00:04:06 And so it sounds like an important key to make sure.

00:04:10 Yeah, and Rust and the same borer checker also knows when memory has to be freed and not.

00:04:16 But it doesn't have to, unlike in Go or Yaka where you have a garbage collector, it doesn't have to do garbage collection and it doesn't have to do reference counting like Python does.

00:04:25 It does so by just statically.

00:04:27 So at compile time, it knows when something is out of scope and not used anymore.

00:04:31 And this is real power.

00:04:32 I guess the takeaway for listeners who are wondering, you know, why is Rust seemingly taking over so much of the job that C and variations of C, right?

00:04:41 Like you said, Cython have traditionally played in Python.

00:04:44 It's easier to write modern, faster, safer code.

00:04:47 Yeah.

00:04:48 Yeah.

00:04:48 And it's more fun too, right?

00:04:50 Yeah, definitely.

00:04:51 And it's a language which has got its tools right.

00:04:54 So it's got a package manager, which is really great to use.

00:04:57 So it's got a real creates.io, which is similar to the PyPy index.

00:05:01 It feels like a modern language.

00:05:03 Yeah.

00:05:03 Builds low-level, more low-level code.

00:05:06 You can also write high-level stuff like REST APIs, which is, I must say, also for high-level stuff, I like to write it in Rust because of the safety guarantees and also the correctness guarantees.

00:05:18 If my program compiles on Rust, I'm much more certain it is correct than when I write my Python program, which is dynamic and types are not enforced.

00:05:27 So it's always a bit graying on that side.

00:05:30 Python is great to use, but it's harder to write correct code in Python.

00:05:34 Yeah.

00:05:34 And you can optionally write very loose code or you could opt in to things like type hints and even mypy and then you get closer to the static languages, right?

00:05:45 Are you a fan of Python typing?

00:05:47 Definitely.

00:05:48 But because they're optional, they are as strong as the weakest link.

00:05:52 So one library which you use, if it doesn't do this type correct or doesn't do it, it breaks.

00:05:58 It's quite brittle because it's optional.

00:06:01 I hope we get something that really enforces it and really can check it.

00:06:05 I don't know if it's possible because of the dynamic nature of Python.

00:06:09 Python can do so many things just dynamically and statically.

00:06:13 We just cannot know probably.

00:06:15 I don't know how far it can go.

00:06:17 But yeah.

00:06:18 In Polis as well, we use mypy type hints, which prevent us from having a lot of bugs and also make the IDE experience much nicer.

00:06:28 Yeah.

00:06:29 Type hints are great.

00:06:30 They really help you also think about your library.

00:06:32 I think you really see a shift in modern Python and Python 10 years ago where it was more dynamic.

00:06:39 The dynamic of Python were more seen as a strength than currently, I believe.

00:06:46 Yeah, I totally agree.

00:06:47 And I feel like when type hints first came out, you know, this was, yes, wow, at this point, kind of early Python 3.

00:06:54 But it didn't feel like it at the time, you know.

00:06:56 Python 3 had been out for quite a while.

00:06:58 When type hints were introduced, I feel like that was Python 3, 4.

00:07:02 But anyway, that was put it maybe six years into the life cycle of Python 3.

00:07:06 But still, I feel like a lot of people were suspicious of that at the moment.

00:07:10 You know, they're like, oh, what is this weird thing?

00:07:12 We're not really sure we want to put these types into our Python.

00:07:16 And now, a lot less.

00:07:17 There's a lot less of those reactions.

00:07:19 Yeah, I see.

00:07:20 Yeah, yeah.

00:07:21 I see Python having two, probably more.

00:07:24 But I often see Python as the really fun, nice, huge duct tape language where I can, in my, for instance, in Jupyter Notebook, I can just hack away and try interactively what happens.

00:07:35 And for such code, type hints don't matter.

00:07:38 But once I write more of a library or product or tool, then type hints are really great.

00:07:44 I believe they came about that Dropbox really needed them.

00:07:47 They had a huge Python code.

00:07:49 These are really trouble maintaining it without the plugins.

00:07:52 But I'm not really sure.

00:07:53 Yeah, and I heard some guy who has something to do with Python used to work there.

00:07:55 Yeah, yeah, yeah.

00:07:56 Guido used to work there, I think even at that time.

00:07:59 All right, so a bit of a diversion from how I often start the show.

00:08:02 So let's just circle back real quick and get your story.

00:08:05 How did you get into programming and Python and Rust as well, I suppose?

00:08:08 I got into programming.

00:08:09 I just wanted to learn programming.

00:08:11 A friend of mine who did, who programmed a lot of PHP said, learn Python, like that.

00:08:16 He gave me an interactive website where I could do some puzzles and I really got hooked to it.

00:08:23 It was a fun summer.

00:08:25 I was programming a lot.

00:08:27 I started automating.

00:08:28 My job was a civil engineer at the moment.

00:08:31 Then I started.

00:08:32 There was a lot of mundane tasks which were repetitive and I just found ways to automate my job.

00:08:37 And eventually I was doing that for a year or three, four.

00:08:40 And then I got into data science and I switched jobs.

00:08:43 I became a data scientist and later a data engineer.

00:08:47 Yeah, so that was Python mostly.

00:08:48 I've always been looking for more languages.

00:08:52 I've been playing with Haskell, I've been playing with Go, I've been playing with JavaScript.

00:08:56 I've been playing with Scala.

00:08:59 And then I found Rust.

00:09:01 And Rust really, really made me happy.

00:09:03 Like you learn a lot about how computers work.

00:09:07 So I had a new renaissance of the first experience with Python.

00:09:10 Another summer with Rust.

00:09:12 And I've been doing a lot of toy projects.

00:09:14 Like writing and interpreting.

00:09:16 I don't know.

00:09:17 A lot of projects.

00:09:18 And Polus became one of those hobby projects just to use Rust more.

00:09:23 Now it's got quite the following.

00:09:25 And we're going to definitely dive into that.

00:09:27 But let me pull it up.

00:09:28 It does right here.

00:09:29 13,000 GitHub stars.

00:09:31 That's a good number of people using that project.

00:09:35 Yeah.

00:09:36 Crazy, isn't it?

00:09:37 Yeah, it is.

00:09:37 It is.

00:09:38 On GitHub stars, it's the fastest growing data tool, I believe.

00:09:42 Wow.

00:09:42 Incredible.

00:09:43 You must be really proud of that.

00:09:45 Yeah.

00:09:46 Yeah.

00:09:46 If you would have told me this two years ago, I wouldn't have believed it.

00:09:50 But it happens slow enough so you can get accustomed to that.

00:09:54 Yeah.

00:09:54 That's cool.

00:09:55 Kind of like being a parent.

00:09:57 The challenges of the kids are small.

00:09:59 They're intense, but there are only a few things they need when they're small.

00:10:03 And you kind of grow with it.

00:10:04 So a couple of thoughts.

00:10:06 One, you had the inverse style of learning to program that I think a lot of computer science

00:10:11 people do.

00:10:12 And certainly that I did.

00:10:13 It could also just be that I learned it a long time ago.

00:10:16 But when I learned programming, it was, I'm going to learn C and C++.

00:10:20 And then you're kind of allowed to learn the easier languages.

00:10:24 But you will learn your pointers.

00:10:26 You'll have your void star star and you're going to like it.

00:10:29 You're going to understand what a pointer to a pointer means.

00:10:31 And we're going to get, I mean, you know, you start inside and you have the most complex,

00:10:37 closest to the machine.

00:10:38 You work your way out.

00:10:39 You kind of took this opposite.

00:10:40 Like, let me learn Python where it's much more high level.

00:10:43 It's much, you know, if you choose to be often say very much more away from the hardware and

00:10:48 the ideas of memory and threads and all that.

00:10:51 And then you went to Rust.

00:10:52 So was it kind of an intense experience?

00:10:54 We're like, oh my gosh, this is intense.

00:10:56 Or had you studied enough languages by then to become comfortable?

00:11:00 Well, yeah, yeah, no.

00:11:01 So the going from high level to low language, I think it makes natural sense.

00:11:06 If you've learned it yourself, there's no professor telling me you learn your pointers.

00:11:11 So I think this also helped a lot because at that point, you're really accustomed to programming,

00:11:17 to algorithms.

00:11:18 Yeah.

00:11:18 So you can, I believe you should learn one thing, one new thing at a time, and then you

00:11:23 can really own that knowledge.

00:11:25 But Rust, I wouldn't say you should learn Rust as a first language.

00:11:29 It would be really terrible because you need, that would be terrible.

00:11:33 But other languages also don't help you much because the borrow checker is quite unique.

00:11:39 It doesn't let you do things you can do in other languages.

00:11:42 So what you learned there, the languages that allow you to do that, they just hurt you because

00:11:48 you were...

00:11:49 They encourage the wrong behavior, right?

00:11:51 Well, yeah.

00:11:52 So nine out of ten times, it turns out by the compiler not letting you do that one

00:11:58 thing, that one thing you wanted was probably really bad to begin with.

00:12:01 It led to really...

00:12:03 So in Rust, your code is always a lot flatter.

00:12:05 It's always really clear who owns the memory, how deep your nesting is.

00:12:10 It's always one D deeper.

00:12:12 Most of the times, it's not that complicated.

00:12:15 You make things really flat and really easy to reason about.

00:12:19 And in the beginning of a project, it seems, okay, a bit over-constraining.

00:12:24 But when...

00:12:25 I mean, software will become complex and complicated, and then you're happy that the compiler not

00:12:30 to do this...

00:12:31 Yeah, absolutely.

00:12:31 ...in this direction.

00:12:32 It seems like a better way, honestly.

00:12:34 You know, you get a sense of programming in a more simple language that doesn't ask so

00:12:39 many low-level concepts of you, and then you're ready.

00:12:43 You can add on these new ones.

00:12:44 So I feel like a lot of how we teach programming and how people learn programming is a little

00:12:49 bit backwards, to be honest.

00:12:50 Yeah.

00:12:50 Enough on that.

00:12:51 So you were a civil engineer for a while, and then you became a data scientist, and now

00:12:56 you've created this library.

00:12:57 Still working as a data scientist now?

00:12:59 No, no.

00:12:59 I got sponsored two years ago for two days a week, and yeah, just use that time to develop

00:13:07 Olar.

00:13:07 And currently, I start all my day jobs and go full-time with Olar.

00:13:13 I'm trying to live on sponsorships, which is not really working.

00:13:17 It's not enough at this time, but I hope to start a foundation and get some proper sponsors

00:13:22 in.

00:13:22 Yeah, that'd be great.

00:13:23 Yeah.

00:13:24 That's awesome.

00:13:25 It's still awesome that you're able to do that, even if you still needed to grow a

00:13:29 little bit.

00:13:30 Yeah.

00:13:30 We'll have you on a podcast and let other people know out there who maybe are using your

00:13:34 library.

00:13:35 Maybe they can put a little sponsorship in GitHub sponsors.

00:13:39 I feel like GitHub sponsors really made it a lot easier for people to support.

00:13:44 Because there used to be PayPal donate buttons and other things like that.

00:13:50 And one, those are not really recurring.

00:13:52 And two, you've got to go find some place and put your credit card.

00:13:55 Many of us already have a credit card registered at GitHub.

00:13:59 It's just a matter of checking a box and monthly it'll just go.

00:14:02 It's kind of like the app store versus buying independent apps.

00:14:05 It just cuts down a lot of friction.

00:14:06 I feel like it's been really positive, mostly for open source.

00:14:10 Yeah, I think it's good as a way to say thank you.

00:14:13 It isn't enough to pay the bills.

00:14:16 I think for most developers, it isn't.

00:14:17 But I hope we get there.

00:14:19 I think companies who use it should give a bit more back.

00:14:24 I mean, they have a lot of money.

00:14:25 I agree.

00:14:25 It's really, really ridiculous that there are banks and VC funded companies and things like that that have, not necessarily in terms of the VC ones, but definitely in terms of financial and other large companies that make billions and billions of dollars in profit on top of open source technology.

00:14:43 And many of them don't give anything back, which is, it's not criminal because the licenses allow it, but it's certainly borders on immoral to say all this money and not at all support the people who are really building the foundations that we build upon.

00:14:58 Most of my sponsors are developers.

00:15:00 Yeah.

00:15:00 Yeah.

00:15:01 So, yeah.

00:15:02 Let's hope it changes.

00:15:04 I don't know.

00:15:05 Yeah.

00:15:05 Well, I'll continue to beat that drum.

00:15:07 This portion of Talk Python to Me is brought to you by TypePi.

00:15:12 TypePi is the next generation open source Python application builder.

00:15:16 With TypePi, you can turn data and AI algorithms into full web apps in no time.

00:15:21 Here's how it works.

00:15:22 You start with a bare algorithm written in Python.

00:15:25 You then use TypePi's innovative tool set that enables Python developers to build interactive end user applications quickly.

00:15:33 There's a visual designer to develop highly interactive GUIs ready for production.

00:15:37 And for inbound data streams, you can program against the TypePi core layer as well.

00:15:42 TypePi core provides intelligent pipeline management, data caching, and scenario and cycle management facilities.

00:15:48 That's it.

00:15:49 You'll have transformed a bare algorithm into a full-fledged decision support system for end users.

00:15:54 TypePi is pure Python and open source.

00:15:57 And you install it with a simple pip install TypePi.

00:16:00 For large organizations that need fine-grained control and authorization around their data, there is a paid TypePi Enterprise Edition.

00:16:07 But the TypePi core and GUI described above is completely free to use.

00:16:11 Learn more and get started by visiting talkpython.fm/TypePi.

00:16:16 That's T-A-I-P-I.

00:16:17 The link's in your show notes.

00:16:19 Thank you to TypePi for sponsoring the show.

00:16:22 Let's talk about your project.

00:16:23 So, Polars and the RS is for Rust, I imagine, at the end.

00:16:28 Yeah.

00:16:28 But tell us about the name Polars, like Polar Bear, but Polars.

00:16:32 Yeah.

00:16:32 So, I started writing a data frame library.

00:16:35 And initially, it was only for Rust.

00:16:37 It was my idea.

00:16:38 Until you get it.

00:16:41 Until you saw all the people doing data science in Python.

00:16:43 You're like, whoa.

00:16:44 Yeah, yeah, yeah.

00:16:44 What can I do for these people, right?

00:16:45 Yeah, yeah.

00:16:46 And I wanted to give a wink to the Pondus project.

00:16:49 But I wanted a bear that was better, faster, I don't know, stronger.

00:16:53 So, luckily, a panda bear isn't the most frightful bear.

00:16:58 So, I had a few to choose.

00:17:00 But the Grizzly, yeah, the Polars has the RS.

00:17:04 So, that's a lucky coincidence.

00:17:06 Yeah.

00:17:06 Yeah, so the subtitle here is Lightning Fast Data Frame Library for Rust and Python.

00:17:12 And you have two APIs that people can use.

00:17:15 We'll get to dive into those.

00:17:16 Yeah.

00:17:17 Because we've written in Rust, it's a complete data frame library in Rust.

00:17:21 And you can expose it to many frontends.

00:17:23 So, it's already frontend in Rust, Python, Node.js.

00:17:27 R is coming up.

00:17:28 And normal JavaScript is coming up.

00:17:30 And Ruby, there is also a Polar in Ruby.

00:17:33 How interesting.

00:17:35 So, for the JavaScript one, are you going to use WebAssembly?

00:17:38 Yeah.

00:17:39 Right?

00:17:39 Which is pretty straightforward because Rust comes from Mozilla.

00:17:42 WebAssembly, I believe, also originated.

00:17:44 They kind of originated as a somewhat tied together story.

00:17:47 Yeah.

00:17:48 So, Rust C++ C can compile to WebAssembly.

00:17:51 It's not really straightforward because the WebAssembly virtual machine isn't like your normal OS.

00:17:57 So, there are a lot of things harder.

00:17:59 But we are working on the challenges there.

00:18:02 Okay.

00:18:02 Well, that's pretty interesting.

00:18:03 But for now, you've got Python and you've got Rust.

00:18:05 And that's great.

00:18:07 Let's...

00:18:07 I think a lot of people listening, myself included, when I started looking into this,

00:18:12 immediately go to, it's like pandas, but rust.

00:18:15 You know?

00:18:16 It's like pandas, but instead of C at the bottom, it's rust at the bottom.

00:18:20 And that's somewhat true, but mostly not true.

00:18:23 So, let's start with you telling us, you know, how is this like pandas and how is it different

00:18:28 from pandas?

00:18:29 Yeah.

00:18:29 So, it's not like pandas.

00:18:32 I think it's different on two ways.

00:18:34 So, we have the API and we have the implementation.

00:18:37 And which one should I start with?

00:18:39 Bottom up?

00:18:40 That's, I think, bottom up.

00:18:41 Yeah, bottom up.

00:18:42 Sure.

00:18:42 Yeah.

00:18:42 All right.

00:18:43 So, that was my critique from pandas and that they didn't start bottom up.

00:18:48 They took whatever was there already, which were good for that purpose.

00:18:53 And pandas built on NumPy.

00:18:55 And NumPy is a great library.

00:18:57 But it's built for numerical processing and not for relational processing.

00:19:01 Relational data is completely different.

00:19:04 You have string data, you have message data.

00:19:05 And this data is going to be just put as Python object in those NumPy arrays.

00:19:11 And if you know anything about memory, then in this array, you have a pointer where each

00:19:17 Python object is somewhere else.

00:19:18 So, if you traverse this memory, every pointer you hit, you must look it up somewhere else.

00:19:23 But memory is not in cache.

00:19:24 So, you have a cache miss, which is a 200x slowdown per element to traverse.

00:19:28 Yeah.

00:19:29 So, for people listening, what you're saying the 200x slowdown is the L1, L2, L3 caches,

00:19:35 which all have different speeds and stuff.

00:19:37 The caches that are near the CPU versus main memory is like 200 to 400 times slower.

00:19:43 Yeah.

00:19:43 Not staging off a disk or something.

00:19:45 It's really different, right?

00:19:47 It's really a big deal.

00:19:47 It's a big deal.

00:19:48 It's terribly slow.

00:19:49 It also, Python has a GIL.

00:19:51 It also blocks multi-threading.

00:19:54 If you want to read the string, you cannot do this from different threads.

00:19:57 If you want to modify the string, there's only one thread that can access Python here.

00:20:01 So, they also didn't take into account anything from databases.

00:20:07 So, databases are basing from the 1950s.

00:20:11 There's been a lot of research in databases and how we do things fast, write a query, and

00:20:17 then optimize this query.

00:20:18 Because the user that uses your library is not the expert.

00:20:21 It doesn't write optimized query.

00:20:23 No.

00:20:23 But we have a lot of information, so we can optimize this query and execute this in a very efficient

00:20:29 way.

00:20:30 That's an interesting idea.

00:20:31 Yeah.

00:20:32 And Pandas just executes it and gives you what you ask.

00:20:35 And what you ask is probably not the answer.

00:20:37 Yeah, that's interesting because as programmers, when I have my Python hat on, I want my code

00:20:43 to run exactly as I wrote it.

00:20:45 I don't want it to get clever and change it.

00:20:48 If I said do a loop, do a loop.

00:20:50 If I said put it in a dictionary, put it in a dictionary.

00:20:53 But when I write a database query, be that against Postgres with relational or MongoDB,

00:20:59 there's a query planner.

00:21:01 And the query planner looks at all the different steps.

00:21:04 Should we do the filter first?

00:21:06 Can we use an index?

00:21:07 Can we use a compo?

00:21:08 Which index should we choose?

00:21:09 All of those things, right?

00:21:11 And so what you tell it and what happens, you don't tell it how to do finding the data

00:21:16 of the database.

00:21:17 You just give it, here's kind of the expressions that I need, the predicates that I need you

00:21:22 to work with.

00:21:23 And then you figure it out.

00:21:24 You're smart.

00:21:25 You're the database.

00:21:26 So one of the differences I got from reading what you've got here so far is it looks like,

00:21:32 I don't know if it goes as far as this database stuff that we're talking about, but there's

00:21:35 a way for it to build up the code it's supposed to run.

00:21:39 And it can decide things like, you know, these two things could go in parallel or things along

00:21:44 those lines, right?

00:21:44 Yeah.

00:21:45 Yeah.

00:21:45 Well, it is actually very similar.

00:21:47 It is a factorized query engine.

00:21:49 And you can, the only thing that doesn't make us a database is that we don't have any, we don't bother

00:21:54 with, with file structures.

00:21:57 Right.

00:21:57 Like the persistence and transactions and all.

00:22:00 Yeah.

00:22:00 So we have different kinds of databases.

00:22:02 You have OLAP and OLTP, transactional modeling, which works often on one.

00:22:07 So if you do a REST API query and you modify one user ID, then you're transactional.

00:22:12 And if you do OLAP, that's more analytical.

00:22:15 And then you do large aggregations of large whole tables.

00:22:18 And then you need to process all the data.

00:22:20 And those different database designs lead to different query optimizers.

00:22:24 And Polis is focused on OLAP.

00:22:26 But yeah, we, so as you described, you've got two ways of programming things.

00:22:30 One is procedural, which Python mostly is.

00:22:33 So you tell exactly if you want to get a cup of coffee, how many steps it should take

00:22:38 forward, then rotate 90 degrees, take three steps and rotate 90 degrees.

00:22:42 You can put, write down the whole algorithm how to get a coffee.

00:22:46 Or you could just say, get me a coffee and I'd like some sugar.

00:22:49 And then let the, let the algorithm, let the query engine decide how to best get it.

00:22:54 Right.

00:22:54 And that's more declarative.

00:22:55 You describe the end result.

00:22:58 And as it turns out, this is also very readable because you declare what you want and the

00:23:02 intent is readable in the, in the query.

00:23:05 And if you're doing more procedural programming, you describe what you're doing and the intent

00:23:10 often needs to come from comments.

00:23:12 Like what are we trying to do when we follow this out?

00:23:15 Right.

00:23:15 Yeah.

00:23:15 That makes a lot of sense.

00:23:16 And that's why, yeah, sorry.

00:23:18 And that's why the, so the first thing is we write from, we write a database engine,

00:23:23 a query engine from scratch and really think about multiprocessing, about cache, caches,

00:23:29 about also out of core.

00:23:31 We can process data that doesn't fit into memory.

00:23:33 So we really built this from scratch with all those things in mind.

00:23:37 And then in, at first we wanted to expose the Pondos API.

00:23:41 And then we noticed how bad it was for writing fast data.

00:23:45 The Pondos API just isn't really good for this declarative analyzing of what the user wants

00:23:51 to do.

00:23:51 So we just cut it off and took the freedom to design an API that makes most sense.

00:23:56 Oh, that's interesting.

00:23:58 I didn't realize that you had started trying to be closer to Pondos than you ended up.

00:24:02 Yeah.

00:24:02 Well, it was very short-lived, I must say.

00:24:05 It was painful.

00:24:07 Yeah.

00:24:07 And that's not necessarily saying Pondos are bad, I don't think.

00:24:10 It's approaching the problem differently and it has different goals, right?

00:24:14 Yeah.

00:24:14 So maybe we could look at an example of some of the code that we're talking about.

00:24:19 I guess also one of the other differences there is much of this has to do with what you

00:24:25 would call, I guess you refer to them as lazy APIs or streaming APIs, kind of like a generator.

00:24:31 Yeah.

00:24:31 So if you think about a join, for instance, in Pondos, if you would write a join and then

00:24:36 take only one to the first 100 rows of that result, then it would first do the join and

00:24:43 then that might produce 1 million or 10 million rows.

00:24:46 And then you take only 100 of them.

00:24:49 And then you have materialized a million, but you take only a fraction of that.

00:24:52 By having that lazy, you can optimize for the whole query at a time and just see, oh,

00:24:58 we do this join, but we only need 100 rows.

00:25:00 So that's how we materialize 100 rows.

00:25:02 So it gets you more realistic approach.

00:25:04 That's really cool.

00:25:04 I didn't realize it had so many similarities to databases, but yeah, it makes a lot of sense.

00:25:09 All right.

00:25:10 Let's look at maybe a super simple example you've got on fuller.rs.

00:25:17 What country is RS?

00:25:18 I always love how different countries that often have nothing to do with domain names

00:25:23 get grabbed because they have a cool ending like Libya that was .ly for a while.

00:25:28 It still is, but it was used frequently like Bitly and stuff.

00:25:31 Do you know what RS is?

00:25:32 I believe it's Serbia.

00:25:34 Serbia.

00:25:34 Okay.

00:25:35 I'm not sure.

00:25:35 Yeah.

00:25:36 Yeah.

00:25:36 Very cool.

00:25:36 All right.

00:25:37 So polar.rs.

00:25:39 It's like polar.rs.

00:25:40 Over here, you've got on the homepage here, the landing page, and then through the documentation

00:25:45 as well, you've got a lot of places where you're like, show me the Rust API.

00:25:48 Or show me the Python API.

00:25:50 People can come and check out the Rust code.

00:25:52 It's a little bit longer because it's that kind of language, but it's not terribly more

00:25:58 complex.

00:25:58 But maybe talk us through this little example here on the homepage in Python just to give

00:26:04 people a sense of what the API looks like.

00:26:06 Yeah.

00:26:06 So we start with a scanned CSV, which is a lazy read, which is, so a read CSV tells what

00:26:13 you do, and then it reads the CSV and you get the data frame.

00:26:16 In a scanned CSV, we started a computation graph.

00:26:20 We call this a lazy frame.

00:26:21 A lazy frame is actually just, it remembers the steps of the operations you want to do.

00:26:26 Then it sends it to Polar, but it looks at this very plan and optimize it and will think of

00:26:32 how to execute it.

00:26:33 And we have different engines.

00:26:34 So you can have an engine that's more specialized for data that doesn't fit into memory, an engine

00:26:39 that's more specialized for data that does fit into memory.

00:26:42 So we start with a scan and then we do a dot filter and we want to use verbs.

00:26:48 Verbs, that's the declarative part.

00:26:50 In Pandas, we often do indexes.

00:26:52 And those indexes are ambiguous in my opinion because you can pass in a numpy array with

00:26:59 booleans, but you can also pass in a numpy array with integers.

00:27:02 So you can do slicing.

00:27:04 You can also pass in a numpy array, a list of strings, and then you do column selection.

00:27:09 So it has three functions.

00:27:10 One thing that I find really interesting about Pandas is it's so incredible.

00:27:15 And people who are very good with Pandas, they can just make it fly.

00:27:19 They can make it really right expressions that are super powerful.

00:27:23 But it's not obvious that you should have been able to do that before you see it.

00:27:27 You know, there's a lot of not quite magic, but stuff that doesn't seem to come really straight

00:27:32 out of the API directly.

00:27:34 You know, you pass in like some sort of like a Boolean expression that involves a vector

00:27:41 and some other test into the brackets.

00:27:44 Like, wait, how did I know I could do that?

00:27:45 Whereas this, your API is a lot more of a fluent API where you say, you know, PD, you'd say

00:27:52 PL, PL.scan, CSV.filter.groupby.aggregate.collect.

00:27:57 And it kind of just flows together.

00:27:59 Does that mean that the editors and IDEs can be more helpful suggesting what happens at each

00:28:05 step?

00:28:06 Yes, we are really strict on types.

00:28:08 So we also only return a single type from a method.

00:28:12 And we only, a .filter just expects a Boolean expression that produces a Boolean, not an integer,

00:28:18 not a string.

00:28:18 So we want our methods from reading or code, you should be able to understand what should

00:28:25 go in there.

00:28:25 That's really important to me.

00:28:27 It should be unambiguous.

00:28:28 It should be consistent.

00:28:29 And your knowledge of the API should expand to different parts of the API.

00:28:34 And that's where, I think we're going to talk about this later, but that's where expressions

00:28:38 really come in.

00:28:39 This portion of Talk Python to Me is brought to you by User Interviews.

00:28:45 As a developer, how often do you find yourself talking back to products and services that you

00:28:51 use?

00:28:51 Sometimes it may be frustration over how it's working poorly.

00:28:55 And if they just did such and such, it would work better.

00:28:58 And it's easy to do.

00:29:00 Other times it might be delight.

00:29:02 Wow, they autofilled that section for me.

00:29:04 How did they even do that?

00:29:05 Wonderful.

00:29:06 Thanks.

00:29:06 While this verbalization might be great to get the thoughts out of your head, did you

00:29:11 know that you can earn money for your feedback on real products?

00:29:14 User Interviews connects researchers with professionals that want to participate in research studies.

00:29:19 There is a high demand for developers to share their opinions on products being created for

00:29:25 developers.

00:29:25 Aside from the extra cash, you'll talk to people building products in your space.

00:29:30 You will not only learn about new tools being created, but you'll also shape the future of

00:29:35 the products that we all use.

00:29:36 It's completely free to sign up and you can apply to your first study in under five minutes.

00:29:41 The average study pays over $60.

00:29:43 However, many studies specifically interested in developers pay several hundreds of dollars

00:29:49 for a one-on-one interview.

00:29:50 Are you ready to earn extra income from sharing your expert opinion?

00:29:54 Head over to talkpython.fm/user interviews to participate today.

00:29:59 The link is in your podcast player show notes.

00:30:02 Thank you to user interviews for supporting the show.

00:30:06 I just derailed you a little bit here as you were describing this.

00:30:10 So you start out with scanning a CSV, which is sort of creating and kicking off a data frame equivalent here.

00:30:18 A lazy frame.

00:30:18 And then you, a lazy frame.

00:30:20 Okay.

00:30:20 And then you say a dot filter and you give it an expression like this column is greater than five.

00:30:25 Right.

00:30:25 Right.

00:30:26 Or some expression that we would understand in Python.

00:30:28 And that's the filter statement.

00:30:30 Right.

00:30:30 Yeah.

00:30:30 And then we follow the group by argument and then an aggregation where we say, okay, take all columns and sum them.

00:30:37 And this again is an expression.

00:30:39 And these are really easy expressions.

00:30:41 And then we take this lazy frame and we materialize it into a data frame that can collect on it.

00:30:47 And collect means, okay, all those steps you recorded, now you can do your magic, query optimizer, get all the stuff.

00:30:54 And what this will do here, it will recognize that, okay, we've taken the iris.csv, which got different columns.

00:31:01 And now in this case, it won't.

00:31:02 So if you would have finished with a select where we only select a few columns, it would have recognized, oh, we don't need all those columns in the CSV file.

00:31:11 We only take the ones we need.

00:31:12 What it will do, it will push the filter, the predicate, down to the scan.

00:31:17 So during the reading of the CSV, we will take this predicate.

00:31:20 We say, okay, the sample length is larger than five.

00:31:24 The rows that don't match this predicate will not be materialized.

00:31:27 So if you have a really large CSV file, this will really, let's say you have a CSV file with tens of gigabytes, but your predicate only selects 5% of that.

00:31:37 Then you only materialize 5% of the 10 gigabytes.

00:31:41 Yeah, so 500 megs instead of 10 gigabytes or something like that, or 200 megs, whatever it is, quite a bit less.

00:31:47 That's really interesting.

00:31:48 And this is all part of the benefits of what we were talking about with the lazy frames, lazy APIs, and building up all of the steps before you say go.

00:31:59 Because in Pandas, you would say read CSV.

00:32:00 So, okay, it's going to read the CSV.

00:32:02 Now what?

00:32:03 Yes.

00:32:03 Right?

00:32:04 And then you apply your filter if that's the order you want to do it in, and then you group, and so on and so on, right?

00:32:09 Right.

00:32:10 It's interesting in that it does allow more database-like behavior behind the scenes.

00:32:15 Yeah.

00:32:15 Yeah.

00:32:15 In the end, in my opinion, the data frame should be seen as a table in a database.

00:32:21 It's the final view of computation.

00:32:25 Like, you can see it as a materialized view.

00:32:27 We have some data on this, and we want to get it into another table, which we would feed into our machine learning models or whatever.

00:32:37 And we do a lot of operations on them before we get there.

00:32:41 So, I wouldn't see a data frame as a data.

00:32:44 It's not only a data structure.

00:32:46 It's not only a list or a dictionary.

00:32:48 There are lots of steps before we get into those tables we eventually need.

00:32:53 Right.

00:32:53 So, here's an interesting challenge.

00:32:56 There's a lot of visualization libraries.

00:33:00 There are a lot of other data science libraries that know and expect Pandas data frames.

00:33:07 So, like, okay, what you do is you send me the Pandas data frame here, or we're going to patch Pandas so that if you call this function on the data frame, it's going to do this thing.

00:33:15 And they may say, Richie, fantastic job you've done here in Polars, but my stuff is already all built around Pandas, so I'm not going to use this.

00:33:23 But it's worth pointing out there's some cool Pandas integration, right?

00:33:27 Yeah.

00:33:27 Yeah.

00:33:27 So, Polars doesn't want to do plotting.

00:33:30 I don't think it should be in a data frame library.

00:33:33 Maybe another library can do it on top of Polars if they feel like it.

00:33:38 It shouldn't be in Polars, in my opinion.

00:33:39 But often when you do plotting, you're plotting, the number of rows will not be billions.

00:33:44 I mean, there's no plotting engine that can deal with that.

00:33:47 So, you will be reducing your big data set to something small, and then you can send it to a plot account.

00:33:53 There's hardly a monitor that has enough pixels to show you that.

00:33:58 Right.

00:33:58 Right.

00:33:58 So, yeah.

00:33:59 We can call it to Pandas, and then we transform our Polars data frame to Pandas, and then you can integrate with scikit-learn.

00:34:06 And we often find that progressively rewriting some Pandas code into Polars already is cheaper than keeping it in Pandas.

00:34:14 If you go from Pandas to Polars, do a join in Polars, and then back to Pandas, we probably made up for those double copies.

00:34:22 Pandas does a lot of internal copies.

00:34:23 If you do a reset index, it copies all data.

00:34:26 If you do, there are a lot of internal copies in Pandas which are implicit.

00:34:29 So, I wouldn't worry about an explicit copy in the end of your ETL to go to plotting when the data is already small.

00:34:37 Right, right.

00:34:37 So, let's look at the benchmarks because it sounds like, to a large degree, even if you do have to do this conversion in the end, many times, it still might even be quicker.

00:34:47 So, you've got some benchmarks over here, and you compared.

00:34:50 I'm going to need some good vision for this one.

00:34:52 You compared Polars, Pandas, Dask, and then two things which are too small for me to read.

00:34:58 Tell us what you compared.

00:34:59 Modding and facts.

00:35:00 Modding and facts.

00:35:00 Okay.

00:35:01 And for people listening, you go out here and look at these benchmarks right off the homepage.

00:35:07 There's like a little tiny purple thing and a whole bunch of really tall bar graphs up the rest.

00:35:13 Yeah.

00:35:13 And the little tiny thing that you can kind of miss if you don't look carefully, that's the time it takes for Polars.

00:35:20 And then all the others are up there in like 60 seconds, 100 seconds.

00:35:24 And then Polars is like a quarter of a second.

00:35:26 So, you know, it's easy to miss it in the graph.

00:35:29 But the quick takeaway here, I think, is there's some fast stuff.

00:35:32 Yeah.

00:35:32 Yeah.

00:35:33 We're often orders of magnitudes faster than Pandas.

00:35:35 So, it's not uncommon to hear it's 10 to 20x times faster.

00:35:39 Especially if you do write proper Pandas and proper Polars, it's probably 20x if we deal with IO as well.

00:35:47 So, what we see here are the TPCH benchmarks.

00:35:49 And TPCH is a database query benchmark standard, which this is used by every query engine to show how fast it is.

00:35:58 And those are really hard questions that really flex the muscles of a query engine.

00:36:05 So, you have joints on several tables, different group buys, different nested group buys, etc.

00:36:10 And, yeah, I really tried to make those other tools faster.

00:36:14 So, in memory, Dask, and Modin, it was really hard to make stuff faster than Pandas, except for Polars.

00:36:21 On a few occasions.

00:36:23 Once we include IO, all those tools first needed to go via Pandas.

00:36:28 And, yeah.

00:36:29 What this sort of shows is that we have Pandas, which is a single-threaded data frame engine.

00:36:36 And then we have tools that parallelize Pandas.

00:36:39 And it's not always, they don't, just parallelizing Pandas doesn't make it faster.

00:36:44 So, if we have a filter or a element-wise multiplication, parallelization is easy.

00:36:50 You just split it up in chunks and do your parallelization.

00:36:53 And then those tools win.

00:36:55 You have 10 cores, you can start 10 threads, and they can take one-tenth of the data and start to answer yes or no for the filter question, for example.

00:37:02 A lot of people don't realize that a lot of data frame operations are not embarrassingly parallel.

00:37:07 A group by is definitely not embarrassingly parallel.

00:37:11 A filter, or sorry, a join needs a shuffle.

00:37:14 It doesn't, it's not embarrassingly parallel.

00:37:16 And that's why you see those tools being slower than Pandas, because they're string data, and then you have a problem with those.

00:37:24 Or we need to do multiprocessing, and we need to send those Python objects to another project, and we copy data, which is slow.

00:37:31 Or we need to do multi-threading, and we're bound by the gill, and we're single-threaded.

00:37:35 And then there is the expensive shuffle.

00:37:36 Yeah.

00:37:37 I think there's some interesting parallels for Dask and Polars.

00:37:41 On these benchmarks, at least, you're showing much better performance than Dask.

00:37:46 I've had Matthew Rockland on a couple times to talk about Dask and some of the work they're doing there, Coiled.

00:37:51 And it's very cool, and one of the things that I think Dask is interesting for is allowing you to scale your code out to multi-cores on your machine, or to even distributed grid computing, or process data that doesn't fit in memory, and they can, behind the scenes, juggle all that for you.

00:38:08 I feel like Polars kind of has a different way, but attempts to solve some of those problems as well.

00:38:14 Yeah.

00:38:15 The Polars has full control over everything.

00:38:18 So it's built from the ground up, and it controls the IO, it controls their own memory, it controls which strap gets which data.

00:38:24 And in Dask, it goes through, it takes this other tool and then parallelizes that, but it is limited by what this other tool also is limited by.

00:38:34 But I think, so on a single machine, it has those challenges.

00:38:38 I think Dask distributed doesn't have these challenges.

00:38:41 And I think for distributed, it can work really well.

00:38:44 Yeah.

00:38:44 The interesting part with Dask, I think, is that it's kind of like Pandas, but it scales in all these interesting ways.

00:38:50 Across cores, bigger memory, but also across machines, and then, you know, across cores, across machines, like all that.

00:38:56 Yeah, and Dask.

00:38:57 I feel like Dask is a little bit, maybe it's trying to solve like a little bit bigger computer problem.

00:39:02 Like how can we use a cluster of computers to answer these questions?

00:39:06 The documentation also says it themselves.

00:39:08 They say that they're probably not faster than Pandas on a single machine.

00:39:12 So they're more for the large, the big data.

00:39:16 But Pandas wants to be, and a lot faster on a single machine, but also wants to be able to do out-of-core processing on a single machine.

00:39:22 So if you, we don't support all queries yet, but we want to, we already do basic joins, group by sorts, predicates, element-wise operations.

00:39:32 And then we can process, I process 500 gigabytes on my laptop.

00:39:37 That's pretty good.

00:39:39 Your laptop probably doesn't have 500.

00:39:41 No, no, no, no.

00:39:41 It's 16 gigs.

00:39:42 Yeah.

00:39:43 Nice.

00:39:44 It's probably actually a value to, as you develop this product, to not have too massive of a computer to work on.

00:39:51 If you had a $5,000 workstation, you know, you might be a little out of touch with many people using your code.

00:39:59 Yeah.

00:39:59 And so, although I think there, I think spoilers like scaling on a single machine makes sense for different reasons as well.

00:40:08 I think a lot of people talk about distributed, but if you think about the complexity of distributed,

00:40:13 you need to send data, shuffle data over the network to other machines.

00:40:16 So there are a lot of people using Polars in our Discord who have one terabyte of RAM and say,

00:40:23 it's cheaper and a lot faster than Spark because they can, one, Polars is faster on a single machine and one, two, they have a beefy machine with like 120 cores and they don't have to go over the network to parallelize.

00:40:37 And yeah.

00:40:38 So I think times are changing.

00:40:40 I think also scaling out data on a single machine is getting more and more.

00:40:44 It is one of the areas in which it's interesting is GPUs.

00:40:48 Do you have any integration with GPUs or any of those sorts of things?

00:40:51 No.

00:40:51 Not suggesting that necessarily is even a good idea.

00:40:54 I'm just wondering if it does.

00:40:55 No, I get this question, but I'm not really convinced I can get the memory.

00:40:59 I can get the data fast enough into the memory.

00:41:02 Like we want to process gigabytes of data.

00:41:05 And the challenge already on the CPU is getting the data from cache from memory fast enough on a CPU.

00:41:12 This is, I don't know.

00:41:14 I don't know.

00:41:14 Yeah.

00:41:15 So maybe we could talk really quickly about platforms that it runs on.

00:41:19 You know, I just, this is the very first show that I'm doing on my M2 Pro processor, which is fun.

00:41:25 I literally been using it for like an hour and a half, so I don't really have much to say, but it looks neat.

00:41:30 Anyway, you know, that's very different than an Intel machine, which is different than a Raspberry Pi, which is different than, you know, some version of Linux running on ARM or on AMD.

00:41:40 So where, where do these, what's the reach?

00:41:43 Well, we support it.

00:41:45 We support it.

00:41:46 We don't.

00:41:47 So Polaris also has a lot of like SIMD optimizations.

00:41:50 SIMD stands for a single instruction log data where, for instance, if you do a floating point operation, instead of doing a single floating point at a time,

00:41:58 you can fill in those vector lanes into your CPU, which can fit eight floating points and in a single operation can compute eight at a time.

00:42:06 And then you have eight times the parallelism on a single core.

00:42:09 Those instructions are only activated for Intel.

00:42:13 So we don't have these instructions activated for ARM, but we do compile to ARM.

00:42:18 How it performs?

00:42:19 I think it performs fast.

00:42:22 Yeah.

00:42:23 But so if the standard machines, right?

00:42:25 macOS, Windows, Linux, or where all that to go.

00:42:28 And it ships as a wheel.

00:42:30 So you don't have to have any, you don't have to have Rusty or anything like that hanging around.

00:42:33 We also have Conda, but Conda is always a bit lagging behind.

00:42:38 So I'd advise to install from pip because we can, we control this employment.

00:42:44 Yeah, exactly.

00:42:44 You push it out to IPI and that's what pip sees and it's going to go, right?

00:42:49 Pretty much instantly.

00:42:50 I guess it's worth pointing out while we're sitting here is, not that thing I highlighted this.

00:42:55 You do have a whole section in your user guide, the Polar's book called Coming from Pandas that actually talks about the differences,

00:43:02 not just how do I do this versus, you know, this operation in Pandas versus Polar's, but it also talks about some of the philosophy,

00:43:09 like this lazy concepts that we've spoken about and query optimization.

00:43:14 I feel like we covered it pretty well.

00:43:16 Yeah.

00:43:17 Unless there's maybe some other stuff that you want to throw in here really quick,

00:43:20 but I mostly just want to throw this out as resource because I know many people are coming from Pandas and they may be interested in this.

00:43:26 And this is probably a good place to start.

00:43:28 I'll link to it in the show notes.

00:43:29 I think the most controversial one is that we don't have the multi-index.

00:43:33 You don't have anything other than zero-based, zero-one-two-three.

00:43:36 You know, where is it in the array type of data?

00:43:38 Yeah.

00:43:38 Well, we can, we will support data structures that make lookups faster, like index in a database sense,

00:43:45 but it will not involve the, it will not change the semantic query.

00:43:50 That's an important thing.

00:43:51 Okay.

00:43:52 Yeah.

00:43:52 So I encourage people who are mostly Pandas people that come down here and, you know, look through this.

00:43:57 It's pretty straightforward.

00:43:59 Another thing that I think is interesting and we're talking about maybe is we could touch a little bit on some of the,

00:44:06 how can I, and your user guide, you've got, how can I work with IO?

00:44:10 How can I work with time series?

00:44:12 How can I work with multiprocessing and so on?

00:44:14 What do you think is good to highlight out of here?

00:44:16 The user guide is a bit outdated.

00:44:18 So I think it's a hero.

00:44:20 So the, for instance, IO is changing.

00:44:23 Polar just writes as its own IO readers.

00:44:28 So we've written our own CSV reader, JSON reader, or K, IPC, Arrow, and that's all in our control.

00:44:36 But for interaction with databases, it's often a bit more complicated.

00:44:41 Deal with different drivers, different ways.

00:44:44 And currently we do this with connector X, which is really great and allows us to read from a lot of different databases.

00:44:50 But it doesn't allow us to write from databases yet.

00:44:52 And this is happy.

00:44:54 This is not really changing.

00:44:55 I want to explain a bit why.

00:44:57 So Polar is built upon the arrow memory specification.

00:45:01 And the arrow memory specification is sort of the standard of how memory for data, how memory for columnar data should look into, how columnar data should be represented in memory.

00:45:12 And this is becoming a new standard.

00:45:14 And Spark is using it.

00:45:17 Dremel, Pandas itself.

00:45:18 For instance, if you read a parquet in Pandas, it reads first into arrow memory and then copies that into Pandas memory.

00:45:26 So the arrow memory specification is becoming a standard, and this is a way to share data to processes, also to other libraries within the process without copying data.

00:45:37 We can just swap out pointers if we know that we both support arrow.

00:45:41 Oh, so arrow defines basically in memory it looks like this.

00:45:47 And if you both agree on that, we can just swap out pointers.

00:45:50 Right, because a .NET object, a C++ object, and a Python object, those don't look like anything similar to any of them, right, in memory.

00:45:59 And, yeah.

00:46:00 So this is from the Apache Arrow project, yeah.

00:46:04 And this is really, really used by a lot of different tools already.

00:46:09 And currently there is coming the ADBC, which is the Apache Arrow database connector, which will solve all those problems,

00:46:16 because then we can write, read and write from a lot of databases in Arrow, and then it will be really fast and really easy for us to do.

00:46:23 So luckily we, that's one of those foundations of Polar's I'm really happy about, because supporting Arrow and using Arrow memory gives us a lot of interaction, interlock with other libraries.

00:46:37 Yeah, that's interesting.

00:46:38 Yeah, that's interesting.

00:46:38 When you think of pandas, you know, it's kind of built on top of NumPy as its core foundation, and it can exchange NumPy arrays with other things to do that.

00:46:47 So Apache Arrow is kind of your base.

00:46:51 Yeah, well, it's kind of full circle, because Apache Arrow is started by Wes McKinney.

00:46:56 Wes McKinney being known as the creator of pandas.

00:47:00 And when he got out of pandas, he thought, okay, the memory representation of NumPy is just not, we should not use it.

00:47:07 And then he was inspired to build Apache Arrow, which learned from pandas.

00:47:12 And yeah.

00:47:13 So that's how you learn about these projects, right?

00:47:16 This is how you realize, oh, we had put this thing in place, maybe we work better, right?

00:47:20 You work on a project for five years, and you're like, if I got a chance to start over, but it's too late now.

00:47:26 But every now and then, you do actually get a chance to start over.

00:47:30 Yeah, yeah.

00:47:30 I didn't realize that Wes was involved with both.

00:47:33 I mean, I knew from pandas, but I didn't realize he's part of mine.

00:47:36 Yeah, he's the CEO of Voltron, which, no, he started Apache Arrow.

00:47:41 And that's, Apache Arrow is sort of super big, like used everywhere, but sort of middleware.

00:47:46 Like it's end users are developers, and end users are developers who build tools, and not developers who use libraries.

00:47:53 That's something like that.

00:47:54 Right.

00:47:55 You might not even know that you're using it.

00:47:57 You just use, I just use pullers.

00:47:59 And oh, by the way, it happens to internally be better because of this.

00:48:04 Yeah.

00:48:04 Yeah, very cool.

00:48:05 Okay, let's see.

00:48:06 We've got a little bit of time left to talk about it.

00:48:08 So, for example, some of these, how can I?

00:48:10 Let me just touch on a couple that are nice here.

00:48:13 So you talked about connector X.

00:48:15 You talked about the database, but it's like three lines of code to define a connection string, define a SQL query, and then just, you can just say pl.readsql.

00:48:24 Yeah.

00:48:25 And there you go.

00:48:26 You call it data frame, or what do you call the thing you get back here?

00:48:29 So reading is always a data frame.

00:48:31 Scanning will be a place.

00:48:32 Got it.

00:48:33 Okay.

00:48:33 Is there a scan SQL as well?

00:48:35 No, this might happen in the future.

00:48:38 The challenge is, are we going to push back our optimizations?

00:48:42 So we write a bonus query, and then we must translate that into SQL, into the SQL we send to the database.

00:48:51 But that needs to be consistent over different databases.

00:48:54 That's a whole other rabbit hole we might get into.

00:48:57 I'm not sure.

00:48:58 Because you can already do many of these operations in the SQL query that you're sending over, right?

00:49:05 You have sort of two layers of query engines and optimizers and query plans.

00:49:09 And it's not like you can't add on additional filters, joins, sorts, and so on before it ever gets back.

00:49:17 It would be terrible if someone writes select star from table and then writes the filters in polars.

00:49:23 And then the database has sent all those data over the network.

00:49:26 So, yeah, ideally, we'd be able to push those predicates down into the SQL.

00:49:32 Yeah, but you know somebody's going to do it because they're more comfortable writing polar API in Python than they are writing T-SQL.

00:49:39 Yeah, you will not.

00:49:40 Yeah.

00:49:41 If it's possible, someone will write it.

00:49:43 It's not optimal.

00:49:44 That's right.

00:49:45 That is right.

00:49:46 Let's see what else can you do here.

00:49:48 So we've already talked about the CSV files.

00:49:51 And this is the part that I was talking about where you've got the toggle to see the Rust code and the Python code.

00:49:57 So I think people might appreciate that.

00:49:59 Parquet files.

00:50:00 So Parquet files is a more efficient format.

00:50:04 Maybe talk about using Parquet files versus CSV and why you want to get rid of your CSV and store these intermediate files and then load them.

00:50:13 Parquet is a really fast CSV reader.

00:50:15 I really did my best on that one.

00:50:18 But if you can use Parquet or Arrow IPC because your data is typed, there's no ambiguity upon reading.

00:50:27 We know which type it is.

00:50:28 Right.

00:50:29 Because CSV files, even though it might be representing a date, it's still a string.

00:50:33 And we need to parse it.

00:50:34 It's slow to parse it.

00:50:37 There's also we can just so Parquet interacts really nicely with query optimization.

00:50:44 So we can select just a single column from the file without touching any of the other columns.

00:50:48 We can read statistics.

00:50:50 And so Parquet file can write statistics, which knows, okay, this page has got this maximum value, this minimum value.

00:50:57 And if you have written a photos query, which says, oh, so only give me the result where the value is larger than this.

00:51:04 And we see that the statistics say it cannot be in this file.

00:51:09 We can just skip the whole column.

00:51:10 We don't have to read.

00:51:11 Yeah.

00:51:12 Oh, interesting.

00:51:12 Wow.

00:51:13 So there are a lot of optimizations, which.

00:51:16 So the best work is work you don't have to do.

00:51:18 And Parquet allows.

00:51:20 Exactly.

00:51:20 Or you've done it when you created the file and you never do it again or something like that.

00:51:25 Yeah.

00:51:25 Yeah.

00:51:26 So you've got a read Parquet, a scan Parquet.

00:51:29 I suppose that's the data frame versus lazy frame.

00:51:32 And then you also have the ability to write them.

00:51:33 That's pretty interesting.

00:51:34 JSON, multiple files.

00:51:36 Yeah.

00:51:37 Yeah.

00:51:37 There's just a whole bunch of how do I, how can I rather, a bunch of neat things.

00:51:41 What else would you like?

00:51:42 I think the most important thing I want to touch on is the expression API.

00:51:46 So that's a bit, you go a bit higher.

00:51:49 So just follow up.

00:51:51 Polis expressions.

00:51:51 They got their own chapter.

00:51:53 One of the goals of the Polis API is to keep the API service small, but give you a lot of things you can do.

00:52:01 And that's where the Polis expressions come in.

00:52:03 So Polis expressions are expressions of what you want to do, which are run and parallelized on a query engine.

00:52:10 And you can combine them indefinitely.

00:52:12 So an expression takes a series and produces a series.

00:52:15 And because the input is the same as the output, you can combine them.

00:52:19 And as you can see, we can do pretty complicated stuff.

00:52:22 And you can keep chaining them.

00:52:25 And this is the same like how I'd like to see it.

00:52:28 For instance, the Python vocabulary is quite small.

00:52:31 So we have a while, we have a loop, we have a variable assignment.

00:52:34 But if you, I think it fits into maybe two pieces of paper.

00:52:39 But with this, you can write any program you want with the combination of all those, all those, yeah, this vocabulary.

00:52:46 Yeah.

00:52:46 And that's what we want to do with the Polis expressions as well.

00:52:49 So you've got a lot of small building blocks, which can be combined into.

00:52:55 Yeah.

00:52:55 So somebody could say, I want to select a column back.

00:52:58 But then I don't want the actual values.

00:53:01 I want the unique ones, a uniqueness.

00:53:03 So if there's duplicate, remove those.

00:53:06 And then you can do a dot account.

00:53:07 Then you can add an alias, which gives it a new, which basically defines the column name.

00:53:12 Yeah, you could read it as, well, it's not named.

00:53:14 You could read it as an ads.

00:53:16 So take column names as unique names to in SQL.

00:53:19 But as is a keyword environment.

00:53:21 So I'm not allowed to use that.

00:53:22 Right.

00:53:23 It means something else, yeah.

00:53:25 That's interesting.

00:53:26 Okay.

00:53:27 Yeah.

00:53:28 So people, they use these expressions to do lots of transformations and filtering and things like that.

00:53:35 Yeah.

00:53:35 So these expressions can be used in a select on different places.

00:53:39 But the knowledge of expressions extrapolates to different locations.

00:53:43 So you can do it in a select statement.

00:53:45 And then you select column name.

00:53:46 You select this expression and you get a result.

00:53:49 But you can also do this in a group by aggregation.

00:53:51 And then the same logic applies.

00:53:53 It runs on the same engine and we make sure everything is consistent.

00:53:57 And this is really powerful because it's so expressive.

00:54:01 People don't have to use custom apply with lambda.

00:54:05 Because when you use a lambda, it's a black box to us.

00:54:08 It will be slow because it's Python and we don't know what happens.

00:54:11 So a lambda is, it will be slow.

00:54:13 It will kill parallelization because it deals.

00:54:15 But yeah, a lambda is three times that.

00:54:18 Right.

00:54:19 It gets in the way of a lot of your optimizations and a lot of your speed is there.

00:54:24 That's why we want to make this expression API very complete.

00:54:28 So you don't need them as much.

00:54:30 Yeah.

00:54:30 So people are wanting to get this, get seriously into this.

00:54:33 They should check out chapter three expressions, right?

00:54:35 And just go through there.

00:54:36 Probably, especially, you know, sort of browse through the Python examples that they can see where, go back and see what they need to learn more about.

00:54:44 But it's a very interesting API.

00:54:46 The speed is very compelling.

00:54:48 I think it's a cool project.

00:54:50 Like I said, how many people we got here?

00:54:52 13,000 people using it already.

00:54:54 So that's a big community.

00:54:56 Yeah.

00:54:56 So if you're interested in project, we have a discord where, where you can chat with us and ask questions and see how you can best do things.

00:55:04 It's pretty active there.

00:55:05 Cool.

00:55:05 The discord's linked right off the homepage.

00:55:07 So that's awesome.

00:55:08 People can find it there.

00:55:09 Contributions.

00:55:10 People want to make contributions.

00:55:12 I'm sure you're willing to accept PRs and other feedback.

00:55:15 Before you put in a really large PR, please first open an issue with a, with a, with a, to start the discussion.

00:55:24 This is, this contribution is welcome.

00:55:26 And we also have a few getting started good for new contributors.

00:55:30 Okay.

00:55:31 Yes.

00:55:31 You've, you've tagged or labeled some of the issues as look here.

00:55:35 If you want to get, get into this.

00:55:37 Yeah.

00:55:37 I must say, I think we're an interesting project to, to contribute to because we're, you can, it's not, not everything is set in stone.

00:55:45 So there are still places where you can play.

00:55:49 I'm not sure.

00:55:50 There's still interesting work to be done.

00:55:52 It's not completely 100% polished.

00:55:55 Yeah.

00:55:56 Yeah.

00:55:57 On the periphery.

00:55:57 Yeah.

00:55:59 Yeah.

00:55:59 Yeah.

00:55:59 Very cool.

00:56:00 Let's wrap it up with a comment from the audience here.

00:56:02 Ajit says, excellent content guys.

00:56:04 It certainly helps me kickstart my journey from pandas to pullers.

00:56:08 Awesome.

00:56:09 Awesome.

00:56:09 Glad, glad to help.

00:56:11 I'm sure it will.

00:56:11 Many people do that.

00:56:12 So Richie, let's close it out with final call action.

00:56:16 People are interested in this project.

00:56:17 They want to start playing and learning pullers.

00:56:19 Maybe try it out on some of their code that is hand us at the moment.

00:56:23 What do they do?

00:56:23 I'd recommend if you have a new project, just start in pullers.

00:56:26 Because you can also rewrite some pandas, but the most fun experience will just start a new project in pullers.

00:56:34 Because then you can really enjoy what pandas offers.

00:56:38 Learn the expression API.

00:56:39 Learn how you use it declaratively.

00:56:41 And yeah, then it will be most fun.

00:56:45 Absolutely.

00:56:45 Sounds great.

00:56:46 And like we did point out, it has the to and from hand us data frames.

00:56:51 So you can work on a section of your code and still have it consistent, right?

00:56:55 With other parts that have to be handed.

00:56:57 You can progressively rewrite some performance heavy parts.

00:57:01 Or I also think pullers is really strict on the schema, on the types.

00:57:06 It's also if you write any ETL, you will be really happy to do that in pullers.

00:57:11 Because you can check the schema of a lazy frame before executing it.

00:57:15 Then you know the vtypes before running the query.

00:57:18 And if the data comes in and it doesn't apply to this schema, you can fail fast.

00:57:23 And instead of having strange outputs.

00:57:25 Oh, that's interesting.

00:57:26 Because you definitely don't want zero when you expected something else.

00:57:31 Because it could parse or other weird whatever, right?

00:57:33 Yeah.

00:57:34 So this was my...

00:57:35 So missing data in pullers doesn't change the schema.

00:57:39 So pullers is really...

00:57:41 The schema is defined by the operations and the data.

00:57:44 And not by the values in the data.

00:57:47 So you can statically check your data.

00:57:49 Excellent.

00:57:50 All right.

00:57:51 Well, congratulations on a cool project.

00:57:53 I'm glad we got to share with everybody.

00:57:55 Thanks for coming on the show.

00:57:56 Bye.

00:57:56 You bet.

00:57:56 Bye.

00:57:57 This has been another episode of Talk Python to Me.

00:58:00 Thank you to our sponsors.

00:58:02 Be sure to check out what they're offering.

00:58:04 It really helps support the show.

00:58:05 TypeI is here to take on the challenge of rapidly transforming a bare algorithm in Python

00:58:10 into a full-fledged decision support system for end users.

00:58:13 Get started with TypeI Core and GUI for free at talkpython.fm/typeI.

00:58:19 T-A-I-P-Y.

00:58:20 Earn extra income from sharing your software development opinion at user interviews.

00:58:25 Head over to talkpython.fm/user interviews to participate today.

00:58:30 Want to level up your Python?

00:58:32 We have one of the largest catalogs of Python video courses over at Talk Python.

00:58:36 Our content ranges from true beginners to deeply advanced topics like memory and async.

00:58:41 And best of all, there's not a subscription in sight.

00:58:44 Check it out for yourself at training.talkpython.fm.

00:58:47 Be sure to subscribe to the show.

00:58:49 Open your favorite podcast app and search for Python.

00:58:52 We should be right at the top.

00:58:53 You can also find the iTunes feed at /itunes, the Google Play feed at /play,

00:58:58 and the direct RSS feed at /rss on talkpython.fm.

00:59:03 We're live streaming most of our recordings these days.

00:59:06 If you want to be part of the show and have your comments featured on the air,

00:59:09 be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

00:59:14 This is your host, Michael Kennedy.

00:59:15 Thanks so much for listening.

00:59:17 I really appreciate it.

00:59:18 Now get out there and write some Python code.

00:59:20 I'll see you next time.

00:59:40 Thank you.

