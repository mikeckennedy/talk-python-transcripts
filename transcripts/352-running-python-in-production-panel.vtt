WEBVTT

00:00:00.001 --> 00:00:02.740
Do we talk about running Python in production enough?

00:00:02.740 --> 00:00:08.100
I can tell you the Talk Python infrastructure, courses, podcasts, APIs, and so on get a fair

00:00:08.100 --> 00:00:13.080
amount of traffic, but they look nothing like what Google or Instagram or insert big tech

00:00:13.080 --> 00:00:15.280
name here's deployments look like.

00:00:15.280 --> 00:00:20.960
Yet, mostly, we hear about interesting feats of engineering at massive scale that, while

00:00:20.960 --> 00:00:24.960
impressive, often is outside the world of most Python devs.

00:00:25.280 --> 00:00:29.940
On this episode, we have three great guests who do think we should talk more about small

00:00:29.940 --> 00:00:31.560
to medium-sized Python deployments.

00:00:31.560 --> 00:00:34.020
Emily Morehouse, Hennick, and Glyph.

00:00:34.020 --> 00:00:35.600
I think you'll enjoy the conversation.

00:00:35.600 --> 00:00:38.140
They each bring their own interesting perspectives.

00:00:38.140 --> 00:00:44.680
This is Talk Python To Me, episode 352, recorded January 12, 2022.

00:00:44.680 --> 00:01:01.520
Welcome to Talk Python To Me, a weekly podcast on Python.

00:01:01.520 --> 00:01:03.240
This is your host, Michael Kennedy.

00:01:03.240 --> 00:01:07.460
Follow me on Twitter, where I'm @mkennedy, and keep up with the show and listen to past

00:01:07.460 --> 00:01:09.440
episodes at talkpython.fm.

00:01:09.440 --> 00:01:12.560
And follow the show on Twitter via at Talk Python.

00:01:12.760 --> 00:01:16.140
We've started streaming most of our episodes live on YouTube.

00:01:16.140 --> 00:01:21.840
Subscribe to our YouTube channel over at talkpython.fm/youtube to get notified about upcoming

00:01:21.840 --> 00:01:23.680
shows and be part of that episode.

00:01:23.680 --> 00:01:27.920
This episode is brought to you by SignalWire and Tonic.ai.

00:01:27.920 --> 00:01:30.380
Please check out what they're offering during their segments.

00:01:30.380 --> 00:01:31.880
It really helps support the show.

00:01:31.880 --> 00:01:35.760
Emily, Hennick, Glyph, welcome to Talk Python To Me.

00:01:35.760 --> 00:01:38.640
It's so good to have you here on the show.

00:01:38.780 --> 00:01:42.440
And I'm super excited about talking about running Python in production.

00:01:42.440 --> 00:01:47.880
And I'm glad that it's more than just one of you, because I think everyone has their own

00:01:47.880 --> 00:01:50.900
perspective and their own context and whatnot.

00:01:50.900 --> 00:01:55.800
You know, do you work for a small company or a large company or do you have many people on

00:01:55.800 --> 00:01:57.000
the team or just a couple, right?

00:01:57.000 --> 00:02:01.700
And I think that really influences what you might cover, some of the decisions you might

00:02:01.700 --> 00:02:02.520
make and so on.

00:02:02.520 --> 00:02:06.120
But before we get to that, let's just start off with a little bit of background.

00:02:06.640 --> 00:02:08.500
Emily, it's been a while since you've been on the show.

00:02:08.500 --> 00:02:10.640
Maybe, you know, tell people a bit about yourself.

00:02:10.640 --> 00:02:11.000
Yep.

00:02:11.000 --> 00:02:12.000
So I'm Emily Morehouse.

00:02:12.000 --> 00:02:14.880
I am the director of engineering at Cuddlesox.

00:02:14.880 --> 00:02:18.320
We are a digital product development company.

00:02:18.320 --> 00:02:23.880
So we kind of touch on anything from web, mobile, IoT, DevOps.

00:02:23.880 --> 00:02:27.800
We really touch the whole stack and get to work with a lot of different industries.

00:02:28.360 --> 00:02:33.420
And then I also am a Python core developer and the PyCon chair for this year.

00:02:33.420 --> 00:02:33.680
Yeah.

00:02:33.680 --> 00:02:41.740
You just happened to grab that role right when, you know, COVID hit and conferences got insanely

00:02:41.740 --> 00:02:43.220
complicated and uncertain.

00:02:43.220 --> 00:02:45.120
How did you juggle all that?

00:02:45.120 --> 00:02:46.360
It was a really big bummer.

00:02:46.540 --> 00:02:52.020
I think because PyCon is really such an important piece of our community and getting to see people

00:02:52.020 --> 00:02:54.440
in person and connect on a regular basis.

00:02:54.440 --> 00:02:58.700
But of course, we wanted to do the thing that was going to keep everyone the most safe.

00:02:59.280 --> 00:03:04.960
So kind of last minute going online and then doing a full online conference and then still

00:03:04.960 --> 00:03:09.220
hoping that things will settle down this year so that in person is something that feels very

00:03:09.220 --> 00:03:10.060
safe for us to do.

00:03:10.060 --> 00:03:11.060
But I was grateful.

00:03:11.060 --> 00:03:15.100
I actually wound up staying on for this third year as chair just so I could have that sort

00:03:15.100 --> 00:03:18.840
of like last hurrah of hopefully getting to be in person.

00:03:18.840 --> 00:03:19.300
Yeah.

00:03:19.300 --> 00:03:19.700
Yeah.

00:03:19.700 --> 00:03:20.540
Yeah.

00:03:20.540 --> 00:03:21.140
I hope so.

00:03:21.140 --> 00:03:21.580
Awesome.

00:03:21.580 --> 00:03:24.620
Well, thanks for all your hard work on it, even if we didn't get a meet in Pittsburgh.

00:03:24.620 --> 00:03:26.060
Glyph, you want to go next?

00:03:26.060 --> 00:03:26.800
I'm Glyph.

00:03:26.800 --> 00:03:29.620
I am probably best known for Twisted.

00:03:29.620 --> 00:03:33.980
I've worked on a variety of production Python environments in my career.

00:03:33.980 --> 00:03:40.820
I am currently in the process of exploring some options for something independent, all

00:03:40.820 --> 00:03:44.120
of which are secret and I can't really say anything about here yet.

00:03:44.120 --> 00:03:46.220
Most recently of Pilot.com.

00:03:46.220 --> 00:03:50.160
They are a fantastic company and I very much recommend people work there.

00:03:50.160 --> 00:03:53.420
And we also ran a ton of Python in production over there.

00:03:53.500 --> 00:03:58.440
I'm sure that folks in the Python podcasting space have probably heard me occasionally before,

00:03:58.440 --> 00:04:00.100
so I won't belabor my introduction.

00:04:00.100 --> 00:04:00.520
Yeah.

00:04:00.520 --> 00:04:01.100
Sounds good.

00:04:01.100 --> 00:04:02.440
Well, good to have you here again.

00:04:02.440 --> 00:04:03.560
Henrik, welcome.

00:04:03.560 --> 00:04:03.840
Welcome.

00:04:03.840 --> 00:04:04.340
Yes.

00:04:04.340 --> 00:04:05.660
Thank you for having me.

00:04:05.660 --> 00:04:09.240
I think more or less my first Python podcast in my life.

00:04:09.240 --> 00:04:10.780
Well, it's awesome to have you.

00:04:10.780 --> 00:04:16.660
You do so much writing and you have such an influence on through like conferences and presentations.

00:04:16.660 --> 00:04:18.180
You should definitely be here.

00:04:18.280 --> 00:04:20.320
So it's long overdue.

00:04:20.320 --> 00:04:20.700
Welcome.

00:04:20.700 --> 00:04:23.660
It's great to hear because I don't think about it like myself.

00:04:23.660 --> 00:04:24.860
But so I'm Henrik.

00:04:24.860 --> 00:04:27.480
I work for a small web hosting company.

00:04:27.480 --> 00:04:29.080
We are like a traditional web hoster.

00:04:29.080 --> 00:04:33.600
I've been working there for now like 14 years, which is a very long time, which is like a thousand

00:04:33.600 --> 00:04:35.360
years in Silicon Valley time, I think.

00:04:35.360 --> 00:04:37.960
Everything I do, I do in Python basically there.

00:04:38.380 --> 00:04:45.040
Like from web services to simple web pages over proxies, applications, you name it.

00:04:45.040 --> 00:04:46.300
We use Python for it.

00:04:46.300 --> 00:04:46.940
Oh, fantastic.

00:04:46.940 --> 00:04:51.860
Do you like control VMs and provision VMs and stuff and that kind of thing?

00:04:51.860 --> 00:04:53.200
Or what type of work are you doing?

00:04:53.200 --> 00:04:57.060
We are more on the, no, we don't sell VMs.

00:04:57.060 --> 00:05:00.020
We don't sell anything where you can have root on it.

00:05:00.020 --> 00:05:02.100
So we're selling a platform basically.

00:05:02.100 --> 00:05:03.900
Like mostly PHP.

00:05:03.900 --> 00:05:06.140
Like it is what it is.

00:05:06.240 --> 00:05:06.740
Yeah, sure.

00:05:06.740 --> 00:05:08.600
At least you can control it all in Python.

00:05:08.600 --> 00:05:09.060
Yeah.

00:05:09.060 --> 00:05:10.000
Well, fantastic.

00:05:10.000 --> 00:05:13.840
And so the reason we're all here today actually is it's your fault, Henrik.

00:05:13.840 --> 00:05:18.180
So you wrote this article back about a year ago.

00:05:18.180 --> 00:05:19.420
Yeah, check the date.

00:05:19.420 --> 00:05:20.140
Yeah, I know.

00:05:20.140 --> 00:05:24.240
We've been talking about getting the four of us together for a while, haven't we?

00:05:24.240 --> 00:05:26.280
It's been a bit of a journey.

00:05:26.280 --> 00:05:31.920
So we didn't mean for it to be that long, but I don't think there's anything that is dated

00:05:31.920 --> 00:05:32.620
about this.

00:05:32.620 --> 00:05:33.840
So it's totally fine.

00:05:33.960 --> 00:05:36.900
What I meant is that it was like right before Corona.

00:05:36.900 --> 00:05:42.780
Like when I was, I was writing this article on my phone while listening to a podcast in

00:05:42.780 --> 00:05:45.620
a van going to a husky farm in Finland.

00:05:45.620 --> 00:05:48.120
A husky farm like the dogs?

00:05:48.120 --> 00:05:48.600
Yeah.

00:05:48.600 --> 00:05:48.940
Yeah.

00:05:48.940 --> 00:05:50.520
I did a husky safari basically.

00:05:50.520 --> 00:05:52.920
Like riding husky sleds, mushing.

00:05:52.920 --> 00:05:53.300
Yeah.

00:05:53.300 --> 00:05:55.060
Oh, that sounds interesting.

00:05:55.060 --> 00:05:55.580
It was.

00:05:55.580 --> 00:05:57.100
It was my last trip before Corona.

00:05:57.360 --> 00:06:02.380
So, which makes it kind of funny because I talk about conferences later on, which, yeah.

00:06:02.380 --> 00:06:03.720
Yeah.

00:06:03.720 --> 00:06:08.400
That kind of got put on hold, but well, we can replace it with things like this and other

00:06:08.400 --> 00:06:08.920
conversations.

00:06:09.360 --> 00:06:13.620
So in this article, you talked about the title is Python in production.

00:06:13.620 --> 00:06:15.100
Of course, we'll link to it in the show notes.

00:06:15.100 --> 00:06:19.940
And you said you were missing a key part from the public Python discourse and would like to

00:06:19.940 --> 00:06:20.580
help change that.

00:06:20.580 --> 00:06:24.700
Basically, we should have more conversations about running in productions.

00:06:24.700 --> 00:06:26.760
So let me see if I get this right.

00:06:26.800 --> 00:06:28.720
You're a huge proponent of microservices.

00:06:28.720 --> 00:06:32.620
Everything should just be a bunch of small microservices and everyone else is doing it

00:06:32.620 --> 00:06:32.820
wrong.

00:06:32.820 --> 00:06:33.940
I'm just kidding.

00:06:33.940 --> 00:06:36.380
You actually don't take a different view than that.

00:06:36.380 --> 00:06:41.500
But we're going to go through, you know, sort of some of the themes in your article, some

00:06:41.500 --> 00:06:42.600
of the other ones out there.

00:06:42.600 --> 00:06:47.640
And then also just all of us talk about sort of what we're doing in production, some of the

00:06:47.640 --> 00:06:48.960
considerations and trade-offs.

00:06:48.960 --> 00:06:50.580
So a lot of fun there.

00:06:50.580 --> 00:06:53.460
Let's talk about cloud computing first.

00:06:53.660 --> 00:06:58.040
I'll pick one of these out of here and then we can touch on the whole microservice monolith

00:06:58.040 --> 00:06:59.400
thing as well.

00:06:59.400 --> 00:07:01.260
You know, Glyph, I'll just pick you.

00:07:01.260 --> 00:07:03.400
We have this trade-off, right?

00:07:03.400 --> 00:07:07.440
We could go and create a VM, set up our own system, and it's totally portable.

00:07:07.440 --> 00:07:08.820
It could run in a data center.

00:07:08.820 --> 00:07:10.180
It could run in the cloud or whatever.

00:07:10.180 --> 00:07:16.580
Or we could go all in with Lambda and other, you know, very specialized services tied to AWS

00:07:16.580 --> 00:07:19.060
or to Azure or whatever.

00:07:19.060 --> 00:07:21.720
What are your thoughts on this whole running in the cloud story?

00:07:21.920 --> 00:07:28.300
There's as many dimensions to that question as there are services in AWS, which is to

00:07:28.300 --> 00:07:31.440
say like a higher order than ALF null infinity.

00:07:31.440 --> 00:07:37.200
The way that I look at that question is you have to look at each service kind of one at

00:07:37.200 --> 00:07:37.660
a time.

00:07:37.660 --> 00:07:42.840
And the way that I would generally suggest people decide whether or not it's a good idea to adopt

00:07:42.840 --> 00:07:49.180
a particular service has to do with the way that their operational footprint works in terms

00:07:49.180 --> 00:07:50.540
of overhead.

00:07:50.540 --> 00:07:56.140
So for example, should you run your own Postgres or should you use RDS?

00:07:56.140 --> 00:08:03.460
Almost always you should just use RDS because the operational footprint of that is Amazon configures

00:08:03.460 --> 00:08:08.940
and manages the tremendous complexity of operating a relational data store in production.

00:08:08.940 --> 00:08:14.100
And when you need to set that up for development, because like part of production is having a

00:08:14.100 --> 00:08:19.040
development environment that matches closely enough that it works well and that you can test

00:08:19.040 --> 00:08:25.340
things and know for sure that you're getting like adequate test coverage is you can just run a

00:08:25.340 --> 00:08:26.300
Postgres Docker container.

00:08:26.460 --> 00:08:29.060
And functionally for your infrastructure, it's probably the same.

00:08:29.060 --> 00:08:35.280
You rarely need to do like deep configuration that is like meaningful to your application

00:08:35.280 --> 00:08:39.780
between your local development Postgres container and your RDS instance.

00:08:39.780 --> 00:08:44.680
As that slider moves further and further towards like, well, actually our data store configuration

00:08:44.680 --> 00:08:46.380
is really part of our application.

00:08:46.380 --> 00:08:49.040
Well, actually we need to like tune things very closely.

00:08:49.040 --> 00:08:51.460
Even running your own data store might make sense.

00:08:51.460 --> 00:08:56.300
That's a pretty unusual circumstance, but that you can look at that for other

00:08:56.300 --> 00:08:58.560
aspects of your platform as well.

00:08:58.560 --> 00:09:01.440
Routing, like load balancing, caching.

00:09:01.440 --> 00:09:03.340
Is that part of your app?

00:09:03.340 --> 00:09:04.940
Do you care about it in development?

00:09:04.940 --> 00:09:09.860
If it breaks in an interesting way, is that going to like take you down?

00:09:09.860 --> 00:09:13.320
And look at each of those issues for each cloud service.

00:09:13.320 --> 00:09:16.380
How much, how expensive is it going to be for you to replicate it in development?

00:09:16.380 --> 00:09:20.220
And how accurate is that reflection going to be of what's in production?

00:09:20.220 --> 00:09:25.820
And always try to pick the answer that is the lowest overhead for your particular team

00:09:25.820 --> 00:09:26.320
configuration.

00:09:26.320 --> 00:09:32.760
That said, the tricky part is this changes as your team grows.

00:09:32.760 --> 00:09:33.480
Right.

00:09:33.480 --> 00:09:35.240
And as your service gets more complex.

00:09:35.240 --> 00:09:39.000
One of the things that I imagine I'm going to touch on a few times is that probably the

00:09:39.000 --> 00:09:44.860
most interesting Python in production experience I had was running pilot.com's application, which

00:09:44.860 --> 00:09:47.380
started in a very different place than it ended up.

00:09:47.500 --> 00:09:49.140
And I'm very happy with the way that that went.

00:09:49.140 --> 00:09:55.880
But we started in an incredibly like super macro service, just one big Python container

00:09:55.880 --> 00:10:04.360
and like nothing else towards picking up application load balancers and lambdas and all kinds of

00:10:04.360 --> 00:10:08.600
other stuff that eventually became part of that infrastructure as the team grew.

00:10:08.760 --> 00:10:12.720
We had more of a dedicated infrastructure, like operational footprint that we could actually

00:10:12.720 --> 00:10:13.940
manage because it was staffed.

00:10:13.940 --> 00:10:14.820
Yeah, that makes a lot of sense.

00:10:14.820 --> 00:10:17.060
It definitely changes as your team changes.

00:10:17.060 --> 00:10:18.260
Emily, what do you think?

00:10:18.260 --> 00:10:19.460
What's your view on this?

00:10:19.460 --> 00:10:21.780
How much of the cloud should you bite off?

00:10:21.900 --> 00:10:23.680
And where's the trade-offs?

00:10:23.680 --> 00:10:25.480
I agree with a lot of the things that Gliff said.

00:10:25.480 --> 00:10:30.000
I think that once you get to the point where you're adding complexity to your local development

00:10:30.000 --> 00:10:32.460
environment, that's usually a red flag for me.

00:10:32.460 --> 00:10:36.300
And you have to be doing something that gives you a lot of value.

00:10:36.300 --> 00:10:42.440
So for example, we'll use like Firestore as an offline database for mobile applications.

00:10:42.440 --> 00:10:48.280
And whether you're using that or you're using CouchDB, whatever choice you're making there,

00:10:48.280 --> 00:10:50.640
you're going to have some sort of added complexity locally.

00:10:51.220 --> 00:10:54.600
So yeah, I totally agree with like RDS is a really easy swap.

00:10:54.600 --> 00:11:03.140
And if it's RDS on AWS or moving to GCP or Peroku or whatnot, your portability between

00:11:03.140 --> 00:11:05.620
those different ecosystems is going to be pretty straightforward.

00:11:05.620 --> 00:11:13.660
But if you want to rewrite your Lambda functions to use Cloud Run instead, like it's not just a

00:11:13.660 --> 00:11:14.960
drag and drop sort of thing.

00:11:14.960 --> 00:11:15.240
Right.

00:11:15.240 --> 00:11:18.440
You don't just change a connection string and now it's all good again.

00:11:18.440 --> 00:11:18.780
Yeah.

00:11:19.180 --> 00:11:23.680
And I think it's going to depend on, you know, like where the company is at.

00:11:23.680 --> 00:11:28.160
Again, for us, like we're often looking at what our client needs.

00:11:28.160 --> 00:11:32.560
So if they're a client that doesn't have a technical team that they just kind of want to

00:11:32.560 --> 00:11:36.840
like let this run on its own and they don't want to manage it, like that's going to impact

00:11:36.840 --> 00:11:38.900
whether we choose Heroku or AWS.

00:11:38.900 --> 00:11:43.380
So like really looking at where you're at now and what sort of support you need in the future

00:11:43.380 --> 00:11:46.340
is a big question to ask for that decision.

00:11:46.340 --> 00:11:51.520
Which way does that gauge go for you if they are kind of hands-off and not super technical?

00:11:51.520 --> 00:11:53.480
Do you give them Heroku or what do you give them?

00:11:53.480 --> 00:11:53.680
Yep.

00:11:53.680 --> 00:11:55.420
If they're non-technical, we give them Heroku.

00:11:55.420 --> 00:11:59.520
If they are technical, even then that kind of depends.

00:11:59.520 --> 00:12:04.480
A lot of times it's AWS or GCP, just kind of depending on what other pieces of the ecosystem

00:12:04.480 --> 00:12:05.180
that they need.

00:12:05.360 --> 00:12:08.920
Or since we work with a lot of existing tech companies, a lot of times they say,

00:12:08.920 --> 00:12:12.220
hey, we are a GCP company or an AWS company.

00:12:12.220 --> 00:12:14.160
And then we just say, cool, we can do either.

00:12:14.160 --> 00:12:14.600
Right.

00:12:14.600 --> 00:12:17.900
And if they're already there, you might as well just keep going with that, right?

00:12:17.900 --> 00:12:18.520
Yeah, definitely.

00:12:18.520 --> 00:12:19.400
Yeah, absolutely.

00:12:19.400 --> 00:12:26.180
But I will say that like 95% of the time, anything that's like web application or API based,

00:12:26.180 --> 00:12:30.020
it's going to be in a Docker container anyway, just to give us that portability.

00:12:30.020 --> 00:12:30.260
Sure.

00:12:30.260 --> 00:12:31.600
Hynek, thoughts?

00:12:31.760 --> 00:12:37.580
I have very, very little first-hand experience with cloud services because we use none.

00:12:37.580 --> 00:12:41.420
It would be kind of odd for a cloud hosting company to use somebody else's cloud.

00:12:41.420 --> 00:12:43.320
Although I know that people are doing it.

00:12:43.320 --> 00:12:44.260
It totally happens.

00:12:44.260 --> 00:12:47.480
People are more or less reselling the big cloud.

00:12:47.480 --> 00:12:47.960
That's true.

00:12:47.960 --> 00:12:48.160
Yeah.

00:12:48.160 --> 00:12:48.640
Wrapper.

00:12:48.640 --> 00:12:49.060
Yeah.

00:12:49.060 --> 00:12:55.780
We run our own hardware in own cages in a shared data center, like military grade.

00:12:55.780 --> 00:12:59.820
I once almost got tasered because I took a photo from the outside.

00:12:59.820 --> 00:13:01.640
Security came running immediately.

00:13:01.640 --> 00:13:08.440
So the one thing we run outside is Sentry because that makes sense.

00:13:08.440 --> 00:13:10.300
And I didn't want to run it anymore myself.

00:13:10.300 --> 00:13:16.340
But there's also the thing that Europeans in general and Germans in particular are not very

00:13:16.340 --> 00:13:22.280
excited when you put their data on other people's servers, particularly of US companies.

00:13:22.860 --> 00:13:29.760
So often it is also like a competitive advantage for us to just say our data literally does not leave Berlin.

00:13:29.760 --> 00:13:30.120
Right.

00:13:30.120 --> 00:13:36.020
That's certainly something that here in the US, it's easy to kind of forget about.

00:13:36.020 --> 00:13:36.220
Right.

00:13:36.220 --> 00:13:38.580
Oh, these just the thing of, oh, it's just the big clouds.

00:13:38.580 --> 00:13:38.820
Right.

00:13:38.820 --> 00:13:39.780
These are big cloud companies.

00:13:39.780 --> 00:13:42.240
But really, those are US cloud companies.

00:13:42.240 --> 00:13:42.480
Right.

00:13:42.520 --> 00:13:46.600
And if you're in Europe or somewhere else, that's another angle to think about.

00:13:46.600 --> 00:13:46.780
Right.

00:13:46.780 --> 00:13:49.720
I have maybe one thing to add as a user.

00:13:49.720 --> 00:13:53.400
That as a user, I don't care that US East 1 is down.

00:13:53.400 --> 00:13:57.060
But you will know.

00:13:57.060 --> 00:13:57.640
You'll still know.

00:13:57.640 --> 00:13:57.760
Yeah.

00:13:57.760 --> 00:13:59.260
This is what I see happening.

00:13:59.260 --> 00:13:59.480
Right.

00:13:59.480 --> 00:14:01.440
Again, people rely too much on cloud services.

00:14:01.820 --> 00:14:06.860
that half of the internet is down when some of that many people are not talking about.

00:14:06.860 --> 00:14:07.880
Yeah, for sure.

00:14:07.880 --> 00:14:10.400
So maybe we can wrap this up real quick.

00:14:10.400 --> 00:14:12.280
But I do want to add just one thing.

00:14:12.280 --> 00:14:17.700
A roller out there in the audience says, I prefer my $5 DigitalOcean VM and just my Nginx,

00:14:17.700 --> 00:14:19.940
MongoDB, Postgres, Python.

00:14:19.940 --> 00:14:25.840
So how often do you think about, you know, especially, I guess, you, Emily, how often do you think about

00:14:25.840 --> 00:14:26.380
price?

00:14:26.380 --> 00:14:29.720
You know, if you look at the price for AWS, it can add up.

00:14:30.080 --> 00:14:31.840
And for some companies, that doesn't matter.

00:14:31.840 --> 00:14:32.220
Right.

00:14:32.220 --> 00:14:34.420
Just having infrastructure is great.

00:14:34.420 --> 00:14:39.840
But for others, you know, maybe they don't have revenue yet or they're not profitable and

00:14:39.840 --> 00:14:41.340
they're really trying to squeeze by.

00:14:41.340 --> 00:14:44.040
Like, where do you land on like, let's save you some money.

00:14:44.040 --> 00:14:49.200
We could do this for $10, but we're going to do it for $500 because it's the right architecture

00:14:49.200 --> 00:14:50.840
to run this distributed thing.

00:14:50.840 --> 00:14:52.580
You know, what are your thoughts?

00:14:52.580 --> 00:14:53.880
What do you tell your customers?

00:14:53.880 --> 00:14:54.320
Sure.

00:14:54.320 --> 00:15:00.060
The cost benefit often comes down to picking an ecosystem that's going to be super stable.

00:15:00.060 --> 00:15:03.940
for them where we can say, hey, yeah, this is going to cost you a few hundred dollars

00:15:03.940 --> 00:15:04.940
a month.

00:15:04.940 --> 00:15:11.240
But if you need somebody to step in and start to manage your system for you, you know, a

00:15:11.240 --> 00:15:16.540
couple of developer hours will easily outshine their cost for their cloud hosting.

00:15:16.700 --> 00:15:18.320
So that kind of puts it in perspective.

00:15:18.320 --> 00:15:23.340
A lot of these clients are already people who have spent multiple thousands, you know,

00:15:23.340 --> 00:15:27.220
tens to hundreds of thousands of dollars actually building their software in the first place.

00:15:27.220 --> 00:15:32.160
So as long as you're, you know, on a much lower order of magnitude, you're typically okay.

00:15:32.160 --> 00:15:33.020
Yeah, that's true.

00:15:33.020 --> 00:15:37.400
If you, especially if you're not technical and you've got to hire someone in to kind of

00:15:37.400 --> 00:15:38.720
fix it, then yeah.

00:15:39.100 --> 00:15:41.440
Then any sort of failure is a big problem.

00:15:41.440 --> 00:15:46.020
This portion of Talk Python To Me is brought to you by SignalWire.

00:15:46.020 --> 00:15:47.920
Let's kick this off with a question.

00:15:47.920 --> 00:15:51.500
Do you need to add multi-party video calls to your website or app?

00:15:51.500 --> 00:15:56.320
I'm talking about live video conference rooms that host 500 active participants, run in the

00:15:56.320 --> 00:16:01.340
browser and work within your existing stack and even support 1080p without devouring the

00:16:01.340 --> 00:16:04.040
bandwidth and CPU on your user's devices.

00:16:04.600 --> 00:16:08.980
SignalWire offers the APIs, the SDKs and edge networks around the world for building the

00:16:08.980 --> 00:16:14.400
realest of real-time voice and video communication apps with less than 50 milliseconds of latency.

00:16:14.400 --> 00:16:20.600
Their core products use WebSockets to deliver 300% lower latency than APIs built on REST,

00:16:20.600 --> 00:16:25.180
making them ideal for apps where every millisecond of responsiveness makes a difference.

00:16:25.180 --> 00:16:29.020
Now, you may wonder how they get 500 active participants in a browser-based app.

00:16:29.020 --> 00:16:34.100
Most current approaches use a limited but more economical approach called SFU or selective

00:16:34.100 --> 00:16:38.480
forwarding units, which leaves the work of mixing and decoding all those video and audio

00:16:38.480 --> 00:16:41.440
streams of every participant to each user's device.

00:16:41.440 --> 00:16:47.200
Browser-based apps built on SFU struggle to support more than 20 interactive participants.

00:16:47.200 --> 00:16:52.400
So SignalWire mixes all the video and audio feeds on the server and distributes a single unified

00:16:52.400 --> 00:16:53.920
stream back to every participant.

00:16:53.920 --> 00:16:58.520
So you can build things like live streaming fitness studios where instructors demonstrate

00:16:58.520 --> 00:17:00.060
every move from multiple angles.

00:17:00.560 --> 00:17:05.200
Or even live shopping apps that highlight the charisma of the presenter and the charisma of

00:17:05.200 --> 00:17:06.980
the products they're pitching at the same time.

00:17:06.980 --> 00:17:12.280
SignalWire comes from the team behind FreeSwitch, the open-source telecom infrastructure toolkit

00:17:12.280 --> 00:17:17.620
used by Amazon, Zoom, and tens of thousands of more to build mass-scale telecom products.

00:17:17.620 --> 00:17:22.080
So sign up for your free account at talkpython.fm/signalwire.

00:17:22.280 --> 00:17:26.820
And be sure to mention Talk Python To Me to receive an extra 5,000 video minutes.

00:17:26.820 --> 00:17:29.020
That's talkpython.fm/signalwire.

00:17:29.020 --> 00:17:31.120
And mention Talk Python To Me for all those credits.

00:17:32.440 --> 00:17:35.520
One thing that I'd love to add on to that is, well, two things.

00:17:35.520 --> 00:17:42.240
First of all, on the notion of price and sort of related to which technologies you should use,

00:17:42.240 --> 00:17:47.820
I think the question you constantly need to be asking is really never ask about one side of

00:17:47.820 --> 00:17:48.300
the equation.

00:17:48.300 --> 00:17:50.780
Never ask about features or price.

00:17:50.780 --> 00:17:54.160
You always want to be looking at a price-performance ratio.

00:17:54.400 --> 00:17:59.600
And that performance shouldn't necessarily, in fact, usually should not be like metrics like gigabytes

00:17:59.600 --> 00:18:02.840
or throughput or anything like that.

00:18:02.840 --> 00:18:06.100
You should be looking at, do you need what the product offers?

00:18:06.100 --> 00:18:08.080
And what is it going to save you time on?

00:18:08.080 --> 00:18:12.720
So like when Emily says, if it's going to save you development time, that is gold.

00:18:12.720 --> 00:18:16.540
You always want to go err on the side of saving development time.

00:18:16.540 --> 00:18:17.700
Developers are very expensive.

00:18:17.700 --> 00:18:18.640
We're very finicky.

00:18:18.640 --> 00:18:21.560
And anything that you develop, you also need to maintain.

00:18:21.560 --> 00:18:25.980
So one of the big benefits of cloud services is think of that price, not just in terms

00:18:25.980 --> 00:18:28.660
of like development cost, but are you going to need to maintain it?

00:18:28.660 --> 00:18:31.400
And on the flip side, do you need this thing at all?

00:18:31.400 --> 00:18:36.800
Quite like Hinnick was talking about having, you know, just not using cloud at all.

00:18:36.800 --> 00:18:41.720
And yet he successfully develops and deploys many services in production and they all seem

00:18:41.720 --> 00:18:42.160
fine.

00:18:42.160 --> 00:18:47.080
And so I think that there's often kind of this tool obsession where we look at features,

00:18:47.080 --> 00:18:49.760
features, features, and assume that features equals benefits.

00:18:49.760 --> 00:18:52.120
But features are only benefits to you if you need them.

00:18:52.120 --> 00:18:54.860
If you don't need them, they're additional costs.

00:18:54.860 --> 00:18:58.280
You have to learn stuff about every single one of those features.

00:18:58.280 --> 00:19:04.420
The time your developers spend learning the surface of the AWS API is a cost you have to

00:19:04.420 --> 00:19:04.880
think about.

00:19:04.880 --> 00:19:09.380
And so like, it's quite often just better to not use the cloud because it's cheaper to

00:19:09.380 --> 00:19:10.740
not figure it all out.

00:19:10.740 --> 00:19:13.920
If you know how expensive it's going to be in your own infrastructure.

00:19:14.320 --> 00:19:18.360
Or as Emily pointed out, if it's in a Docker container, you may run it in the cloud now,

00:19:18.360 --> 00:19:21.820
but it's fairly portable and not super tied to it.

00:19:21.820 --> 00:19:24.380
Paul Everett out in the audience has a quick question.

00:19:24.380 --> 00:19:27.420
He says, we talk about pinning dependencies to control change.

00:19:27.420 --> 00:19:31.480
But when we talk about cloud computing, we say let things like AWS handle it.

00:19:31.480 --> 00:19:35.900
You know, you're sort of, that's kind of a dependency on their compatibility of each one

00:19:35.900 --> 00:19:37.320
of those services you take on.

00:19:37.680 --> 00:19:42.000
What do you think about, you know, stability of those APIs and those services over time?

00:19:42.000 --> 00:19:46.240
I think Azure maybe is a little more changing than AWS.

00:19:46.240 --> 00:19:49.340
Things come and go, but still it's something to consider, right?

00:19:49.340 --> 00:19:51.840
It's another thing you've got to deal with churn on, I guess.

00:19:51.840 --> 00:19:52.100
Yeah.

00:19:52.100 --> 00:19:56.260
And I think it depends on like how quickly you're jumping on new services.

00:19:56.260 --> 00:20:01.820
So we actually started using ECS right after it was released and it's come a long way since

00:20:01.820 --> 00:20:02.120
then.

00:20:02.120 --> 00:20:05.380
And both like the product and our perficiency has come a long way.

00:20:05.520 --> 00:20:10.980
So I think that taking any sort of like new service with a heavy dose of skepticism and

00:20:10.980 --> 00:20:15.580
making sure that it's something that is really stable, that's going to fulfill the need.

00:20:15.580 --> 00:20:17.000
It's an important thing to look at.

00:20:17.000 --> 00:20:17.240
Yeah.

00:20:17.240 --> 00:20:18.060
That's a really good point.

00:20:18.060 --> 00:20:18.440
All right.

00:20:18.440 --> 00:20:18.940
Let's move on.

00:20:18.940 --> 00:20:20.060
We touched on this enough.

00:20:20.060 --> 00:20:22.520
I think let's talk about microservices.

00:20:22.520 --> 00:20:26.200
I was joking with Hennick about that at the beginning.

00:20:26.200 --> 00:20:28.160
So we have this spectrum.

00:20:28.160 --> 00:20:34.260
We can have just one code base, one process that runs in micro WSGI or G Unicorn or whatever,

00:20:34.260 --> 00:20:40.800
where we could have a little Lask API that does user management and login, another part

00:20:40.800 --> 00:20:43.700
that does, you know, the catalog or whatever.

00:20:43.700 --> 00:20:47.400
We could have a bunch of these little services, these microservices, and then put them all

00:20:47.400 --> 00:20:47.720
together.

00:20:47.720 --> 00:20:50.120
I have thoughts on this, but where are your thoughts?

00:20:50.400 --> 00:20:53.700
My opinion at this point is, I think, quite public.

00:20:53.700 --> 00:21:00.660
I think you should start with a monolith first for this simple reason that microservices come

00:21:00.660 --> 00:21:02.640
with a lot of adjacent complexity.

00:21:02.640 --> 00:21:05.860
We basically just talked about with cloud services, right?

00:21:05.980 --> 00:21:09.600
Like you cannot have microservices without service discovery.

00:21:09.600 --> 00:21:14.740
You cannot have microservices without tracing because every error becomes like a, I thought

00:21:14.740 --> 00:21:17.340
it's once called a distributed murder mystery.

00:21:17.420 --> 00:21:27.600
When you're trying to find the fault of it, there are things that people don't think about, like retries need to be kept.

00:21:27.600 --> 00:21:34.040
Otherwise, you're going to get exponential growth and you denial service yourself and stuff like that.

00:21:34.040 --> 00:21:41.520
So, I mean, it's a general good idea to have as few moving parts as possible, which already someone said about cloud computing.

00:21:42.020 --> 00:21:46.260
And because more moving parts are always harder to make reliable, right?

00:21:46.260 --> 00:21:50.300
And Martin Foller calls it the microservice premium, which I like.

00:21:50.300 --> 00:21:52.720
And it's again, it's a trade-off.

00:21:52.720 --> 00:21:55.040
Is this premium worth it to you?

00:21:55.040 --> 00:22:02.220
And I think you need a lot more experience to make this trade-off than many people think.

00:22:02.220 --> 00:22:03.960
That's my experience.

00:22:03.960 --> 00:22:04.300
Yeah.

00:22:04.300 --> 00:22:10.380
You probably start simple and then with just two or three pieces and then you end up with 20 and you're like, well, how do we get here?

00:22:10.380 --> 00:22:10.740
Yeah.

00:22:10.920 --> 00:22:11.080
Yeah.

00:22:11.080 --> 00:22:12.720
And they don't really know why, right?

00:22:12.720 --> 00:22:20.380
Like if people want boundaries, like I cannot speak for huge teams because one of the peculiarities of my job is that our team is very small with a lot of responsibility.

00:22:20.380 --> 00:22:25.520
But there are big teams that have boundaries that do not come with a network partition.

00:22:25.520 --> 00:22:27.520
So, yeah, it's a trade-off.

00:22:27.520 --> 00:22:31.220
And I think that most people need more experience to actually make this trade-off.

00:22:31.220 --> 00:22:34.140
Yeah, I would, I very much agree with that.

00:22:34.340 --> 00:22:42.260
particularly the people who tend to be asking this question are often asking it because they're at a small company.

00:22:42.260 --> 00:22:43.680
They have a small team.

00:22:43.680 --> 00:22:48.800
They saw a really cool white paper or a presentation by somebody at Google or Netflix.

00:22:48.800 --> 00:22:52.580
And they're like, wow, all this stuff about microservices sounds great.

00:22:52.580 --> 00:22:58.860
Fault isolation and distributed tracing and, you know, use whatever language you want.

00:22:58.940 --> 00:23:00.360
And I'll get back to that one in a second.

00:23:00.360 --> 00:23:06.800
And again, like I said before, you will need to think about everything in terms of cost benefit and not just benefit.

00:23:07.280 --> 00:23:16.260
And the folks at Google and Netflix are talking about 10,000 person teams and like, how do you manage complexity at that scale?

00:23:16.260 --> 00:23:20.420
How do you deal with the problems that come with that type of organization?

00:23:20.420 --> 00:23:28.740
And so asking like, I also, even the shape of this debate has long bothered me because the terms that we're using, right?

00:23:28.740 --> 00:23:31.620
Do we want monoliths or microservices?

00:23:32.360 --> 00:23:40.160
Asking that question is like saying, well, when we have a wheel, do we want it to be like an 18-wheeler, like truck wheel?

00:23:40.160 --> 00:23:43.520
Or do we want it to be like one of those wheels that comes on like a micromachines car?

00:23:43.520 --> 00:23:46.360
And the answer is, I don't know.

00:23:46.360 --> 00:23:49.620
Like, what are you putting on the vehicle that this wheel is going to be attached to?

00:23:49.620 --> 00:23:51.040
Like, it really depends.

00:23:51.040 --> 00:23:54.240
The question of like, how big do you want your service to be?

00:23:54.240 --> 00:23:57.440
Which is, I think, a better question than do you want a microservice or a monolith?

00:23:57.440 --> 00:24:02.720
It comes down to what is the surface area to volume ratio of your team?

00:24:02.720 --> 00:24:06.000
It takes about 12 people to run a microservice, I would say.

00:24:06.000 --> 00:24:10.740
Like a dozen people per service is roughly what you should be thinking about.

00:24:10.740 --> 00:24:18.720
Plus, there's the fixed overhead of a microservice architecture where you need service discovery and tracing and logging and like a whole bunch of other stuff,

00:24:18.720 --> 00:24:29.040
which you probably need in today's modern fancy cloud environment if you're going to be able to leverage services like Honeycomb and CloudWatch and like just manage all your logs and not have to deal with that yourself.

00:24:29.040 --> 00:24:34.480
Then you're talking about maybe a six-person team that can just do infra.

00:24:34.480 --> 00:24:41.020
And if you are at a three-person startup and thinking, huh, that sounds like a lot of people, you want a monolith.

00:24:41.020 --> 00:24:53.940
You want one piece of code you can maintain because the benefit of having something like a very fine-grained microservice architecture is that you can say, we're going to have each team own its little piece.

00:24:54.440 --> 00:25:07.260
And we're going to move a lot of the complexity of orchestrating from code to configuration because our operations team can like deal with the configuration and manage their process around testing configurations.

00:25:07.260 --> 00:25:11.880
But if you're a small development team, you want code you can write unit tests for.

00:25:11.880 --> 00:25:16.260
You want code you can run and understand the way that you run and understand everything else.

00:25:16.260 --> 00:25:22.900
And if you push everything into YAML files and INI files and cloud APIs, what does your development environment look like anymore?

00:25:22.900 --> 00:25:24.180
How do you test those changes?

00:25:24.180 --> 00:25:26.800
Right. Even developing on your own machine becomes tricky.

00:25:26.800 --> 00:25:28.000
Yeah, exactly. Yeah.

00:25:28.000 --> 00:25:33.980
Yeah. I think the big takeaway is that the team needs to match what microservices are optimized for.

00:25:33.980 --> 00:25:34.860
Yeah, exactly.

00:25:34.860 --> 00:25:35.420
That's a really good point.

00:25:35.420 --> 00:25:42.940
Emily, I suspect the people you suggest use Heroku probably are not receiving 10 microservices over there, are they?

00:25:42.940 --> 00:25:44.020
No, definitely not.

00:25:44.020 --> 00:25:50.280
I think one of the things that we typically have built in is a certain amount of ability to scale independently.

00:25:50.280 --> 00:25:53.720
And so we typically do have two different workers on Heroku.

00:25:53.960 --> 00:25:58.100
So we're going to have, you know, a Django application and a Celery task worker.

00:25:58.240 --> 00:26:00.860
And then we know that we can kind of scale those two independently.

00:26:00.860 --> 00:26:05.540
But that's where we really see the division of load.

00:26:05.660 --> 00:26:14.220
So that's my lens that I look at monolith versus microservices with is really, do I need to scale them independently?

00:26:14.220 --> 00:26:19.600
Yeah. And I certainly think having some kind of, we're going to kick off the long running work over there.

00:26:19.600 --> 00:26:21.540
So it's not happening as part of a web request.

00:26:21.540 --> 00:26:23.040
That certainly makes sense.

00:26:23.280 --> 00:26:32.840
Even things just like sending a group of people an email often takes too long if the group is large enough before the request will time out and wreck your servers and all sorts of stuff.

00:26:32.840 --> 00:26:33.940
So yeah, it makes sense.

00:26:33.940 --> 00:26:35.980
But I think developer experience is a really big one.

00:26:35.980 --> 00:26:46.640
We have had the pleasure of working with another like very large tech company, like multiple hundreds of employees, and they had a monolithic Rails app.

00:26:46.740 --> 00:26:53.660
And it was absolute hell to work with as a developer because you're constantly like having hundreds of PRs open at a time.

00:26:53.660 --> 00:26:57.800
You open a PR and within 20 minutes, you have a merge conflict with somebody else.

00:26:57.800 --> 00:27:06.460
And you really, at that point, it makes sense from a developer perspective too, to be able to divide it up and say, okay, like, this is your area of expertise.

00:27:06.460 --> 00:27:07.780
This is your area of expertise.

00:27:07.780 --> 00:27:09.480
And at least set up those partitions.

00:27:09.480 --> 00:27:12.120
But you can do that in a monolith as well.

00:27:12.120 --> 00:27:16.920
It just takes a little bit of like awareness and slice in the problem a little differently.

00:27:16.920 --> 00:27:17.540
Yeah.

00:27:17.540 --> 00:27:26.040
So let me think of a, let me throw out an idea that I just thought of that I'm not sure I'm advocating this or even that it is a good idea.

00:27:26.040 --> 00:27:35.280
But one of the problems you have is sort of you have this big monoliths, like you were touching on, Emily, is a lot of people are changing the same bits of code and they're kind of in all over the place.

00:27:35.280 --> 00:27:37.840
So the microservices is one way to solve that problem.

00:27:38.000 --> 00:27:51.100
What about trying to think about, you know, could we package up some of the functionality of what our application does or our area into Python packages that we can then install and use in the main app?

00:27:51.100 --> 00:27:51.840
That idea?

00:27:51.840 --> 00:27:52.500
Good idea?

00:27:52.500 --> 00:27:55.620
I mean, it's interesting, but it's also another level of complexity, right?

00:27:55.620 --> 00:27:55.900
Yeah.

00:27:55.900 --> 00:27:56.320
Yeah.

00:27:56.320 --> 00:27:58.480
You got to run your own private server or something.

00:27:58.480 --> 00:28:02.860
I would actually say that that is a prerequisite to microservices.

00:28:03.920 --> 00:28:10.120
Specifically, one of the things that people often forget about the whole service architecture thing, and this comes down to the future.

00:28:10.120 --> 00:28:14.840
Remember, I said I was going to talk about choose your own language kind of thinking before.

00:28:14.840 --> 00:28:25.320
One of the things that people often choose microservice architecture for badly is this idea that they want to experiment with different programming languages because they want to use the right tool for the right job.

00:28:25.560 --> 00:28:34.920
And number one, just the cognitive overhead of jumping between Haskell and OCaml and Python and Rust on your back end is way, way higher than most people think.

00:28:35.120 --> 00:28:47.720
But even forgetting about the sort of human cognitive overhead, your service architecture, your service fabric needs to be logging and recording metrics and dealing with load balancers and dealing with data stores in a consistent way.

00:28:47.720 --> 00:29:00.260
And that task by itself is complex enough that you probably need a library, which means every supported language in your environment needs to have packages installed that are maintained by your infrastructure team and not by your application teams.

00:29:00.260 --> 00:29:12.220
And that means before you can even think about microservices, you have to be able to split your workflow into multiple different package repositories, different source control, different teams, different CI.

00:29:12.220 --> 00:29:19.660
Like you need to be able to do that first before you can reasonably split things across like multiple actual services.

00:29:19.900 --> 00:29:29.020
That doesn't necessarily mean you need to like not have a quote unquote monolith because you can put a monorepo into that kind of multiple package, multiple library workflow.

00:29:29.020 --> 00:29:36.040
And that's also fine, but you do need to be able to have multiple work streams going that end up in the same service package.

00:29:37.620 --> 00:29:40.920
This portion of Talk Python To Me is brought to you by Tonic.ai.

00:29:40.920 --> 00:29:48.240
Creating quality test data for developers is a complex, never-ending chore that eats in the valuable engineering resources.

00:29:48.240 --> 00:29:50.440
Random data doesn't do it.

00:29:50.440 --> 00:29:54.200
And production data is not safe or legal for developers to use.

00:29:54.200 --> 00:30:00.980
What if you could mimic your entire production database to create a realistic dataset with zero sensitive data?

00:30:00.980 --> 00:30:04.020
Tonic.ai does exactly that.

00:30:04.020 --> 00:30:11.960
With Tonic, you can generate fake data that looks, acts, and behaves like production data because it's made from production data.

00:30:11.960 --> 00:30:18.940
Using their universal data connectors and a flexible API, Tonic integrates seamlessly into your existing pipelines

00:30:18.940 --> 00:30:24.680
and allows you to shape and size your data to scale, realism, and degree of privacy that you need.

00:30:24.680 --> 00:30:31.620
Their platform offers advanced subsetting, secure de-identification, and ML-driven data synthesis

00:30:31.620 --> 00:30:36.040
to create targeted test data for all your pre-production environments.

00:30:36.040 --> 00:30:44.240
Your newly mimicked datasets are safe to share with developers, QA, data scientists, and, heck, even distributed teams around the world.

00:30:44.240 --> 00:30:48.860
Shorten development cycles, eliminate the need for cumbersome data pipeline work,

00:30:48.860 --> 00:30:52.900
and mathematically guarantee the privacy of your data with Tonic.ai.

00:30:53.360 --> 00:31:00.560
Check out their service right now at talkpython.fm/tonic or just click the link in your podcast player's show notes.

00:31:00.560 --> 00:31:06.220
Be sure to use our link, talkpython.fm/tonic, so they know you heard about them from us.

00:31:08.000 --> 00:31:10.500
Another thing is versioning across the services, right?

00:31:10.500 --> 00:31:19.420
The definition, like if you're using SQLAlchemy, the user shape has to match all the parts that talk to the database that might touch a user object or something like that.

00:31:19.420 --> 00:31:29.060
So I guess I'll wrap it up with a quick thought that for me, microservices feel like I'm trading code and developer complexity for operational and DevOps complexity.

00:31:29.140 --> 00:31:36.220
And I personally feel much more comfortable managing and working with code complexity than like infrastructure complexity.

00:31:36.220 --> 00:31:45.140
But if your team and your organization is all about managing infrastructure complexity and DevOps, and you have a bunch of junior devs, maybe microservices make sense.

00:31:45.140 --> 00:31:45.620
I don't know.

00:31:45.620 --> 00:31:46.560
It kind of depends.

00:31:46.560 --> 00:31:50.660
But my vote's for the monolith side because I'd rather manage the software complexity.

00:31:50.660 --> 00:31:52.960
Let's talk about security.

00:31:52.960 --> 00:31:55.660
I mean, we've had some crazy stuff.

00:31:55.660 --> 00:31:58.000
I think it's log4jmeme.

00:31:58.000 --> 00:31:59.760
There's a .com.

00:31:59.760 --> 00:32:01.220
I think that is a .com.

00:32:01.220 --> 00:32:01.760
Yeah.

00:32:01.760 --> 00:32:11.360
So like security is clearly on the, it's something we always have to think about, but something, right, that recently log4j obviously being a Java, not Python thing.

00:32:11.360 --> 00:32:15.080
But it's the one thing that makes me nervous about running stuff in production.

00:32:15.080 --> 00:32:27.800
Honestly, we've got stuff like status cake or various other things that you can fire up or uptime or whatever that will tell you if your site is down and send you notifications and things like that.

00:32:27.800 --> 00:32:29.920
But the security one is a little bit scary.

00:32:29.920 --> 00:32:32.260
So how do you approach thinking about this?

00:32:32.260 --> 00:32:37.940
You know, Henrik, maybe you go first hosting other people's code is like a next level is like meta security.

00:32:38.280 --> 00:32:38.780
Yeah.

00:32:38.780 --> 00:32:48.340
The one flag I want to wave here is defense in depth, which is something that's very dear to my heart and which I feel is, yeah, which could be a bit more popular.

00:32:48.340 --> 00:32:48.400
Sure.

00:32:48.400 --> 00:32:52.620
Because every significant attack nowadays is a multi-stage one.

00:32:52.620 --> 00:32:56.780
It doesn't matter if it's like owning your Chrome or owning a server.

00:32:56.780 --> 00:32:58.320
It's usually multiple stages.

00:32:58.960 --> 00:33:07.280
So you shouldn't make it as hard as possible to the attackers, even though they have entered your infrastructure at this point.

00:33:07.280 --> 00:33:11.560
So for me, it means that I treat our own network, which is very private.

00:33:11.560 --> 00:33:15.400
You need a VPN to get in and everything as if it has intruders inside it.

00:33:15.400 --> 00:33:16.780
And that's our standard.

00:33:16.980 --> 00:33:24.000
So you should hash your passwords as the maintainer of Argon2CFFI, of course, using Argon2, but use whatever.

00:33:24.000 --> 00:33:32.460
We use TLS even in private networks, because I know that cloud providers have virtual LANs, which are also often encrypted.

00:33:32.460 --> 00:33:34.080
But still, it's just another layer.

00:33:34.080 --> 00:33:43.040
You cannot have enough layers to protect yourself at this point, because if someone intrudes you, you don't want them to sniff passwords out of your traffic and things like that.

00:33:43.040 --> 00:33:44.120
The list goes on.

00:33:44.300 --> 00:33:45.420
Yeah, it definitely does.

00:33:45.420 --> 00:33:47.220
Emily, security thoughts?

00:33:47.220 --> 00:33:51.460
Yeah, luckily, this isn't one that we've necessarily had to worry too much about.

00:33:51.460 --> 00:33:55.980
I think the worst thing that's happened for us is DDoS attack, and that's about it.

00:33:55.980 --> 00:34:04.360
So for me, I think definitely staying on top of dependency management, keeping things up to date, which is not necessarily always the easiest thing.

00:34:04.360 --> 00:34:11.140
Especially the Python 2.3 transition, upgrading Django is sometimes a little bit more complex than you'd want it to be.

00:34:11.260 --> 00:34:16.380
But I think that getting those security updates and making sure that you're on an LTS version is really important.

00:34:16.380 --> 00:34:18.020
LTS being long-term support.

00:34:18.020 --> 00:34:18.800
Yeah, absolutely.

00:34:18.800 --> 00:34:19.100
Yeah.

00:34:19.100 --> 00:34:24.540
So one of the things that I think, you know, I made the, I was picking on the log4j thing.

00:34:24.640 --> 00:34:30.800
But one of the problems that made this a little bit harder, I mean, this is going to be something we live with for a long time.

00:34:30.800 --> 00:34:31.580
It's going to be a nightmare.

00:34:31.580 --> 00:34:33.200
But the consequences of it.

00:34:33.560 --> 00:34:53.760
But one of the problems was that the fixed version of log4j, which for those who didn't know, the log4j problem is if you can basically get a server to print a message with any of your input, like this URL was invalid, or this email tried to log in and failed, you can own the computer.

00:34:54.040 --> 00:34:55.680
Which is really, really bad.

00:34:55.680 --> 00:34:57.620
So it has to be fixed, like straight away.

00:34:57.620 --> 00:35:02.440
But the problem was, the fixed version was on a newer version, like Java 8.

00:35:02.440 --> 00:35:09.340
So if you were running Java 7, you had to both not just upgrade your library, but then upgrade your whole runtime, which might be problematic.

00:35:09.960 --> 00:35:18.400
So Emily, that makes me think that, you know, you probably, one of the good rules to go by is don't let your frameworks get too far out of date.

00:35:18.400 --> 00:35:24.420
Like you don't have to be on the latest Django, but don't stay on Django 1 when Django 4 is about to be released.

00:35:24.420 --> 00:35:31.180
Or don't stay on Python 3.6 when you could be on the newer one or something like that.

00:35:31.180 --> 00:35:32.020
How do you all feel about that?

00:35:32.020 --> 00:35:45.640
Yeah, and I think that, like looking at it from an open source maintainer's perspectives, making sure that you have the ability to kind of hotfix previous versions and be very clear about which versions you're supporting and which ones you're not.

00:35:45.640 --> 00:35:55.300
That way, you make it easy for a user to know, like, yeah, I'm not going to get the security update and I need to upgrade and have that done ahead of time to stay on top of things.

00:35:55.300 --> 00:35:55.560
Yeah.

00:35:55.560 --> 00:35:57.920
And AgraGliff, old frameworks, what do you think?

00:35:57.920 --> 00:36:07.340
I think that this is a, I don't think I can find this right away, but I remember one of my more popular tweets was one of those sort of two buttons memes where the guy can't choose.

00:36:07.340 --> 00:36:18.900
And on one side, you've got get owned because your dependencies are out of date and you have no way to immediately update them or get owned because you're automatically updating from an upstream that you don't know if you can trust or not.

00:36:18.900 --> 00:36:34.500
And that's kind of, so the way that I sort of split that difference in practice is you really want to make sure that it's not just about like regularly upgrading because you can always say like, oh yeah, we'll regularly upgrade.

00:36:35.060 --> 00:36:38.140
But you've only got a fixed budget for security, right?

00:36:38.140 --> 00:36:44.940
Like it's possible to spend all of your time spinning your wheels trying to increase the cost for attackers across every possible access.

00:36:44.940 --> 00:36:46.080
I'm sorry, access.

00:36:46.080 --> 00:36:46.360
Yeah.

00:36:46.360 --> 00:36:48.800
But eventually you got to ship features and deliver stuff.

00:36:48.800 --> 00:36:49.740
Like that's really.

00:36:49.740 --> 00:36:50.420
Exactly.

00:36:50.420 --> 00:36:51.080
It's a balance.

00:36:51.080 --> 00:36:51.300
Yeah.

00:36:51.300 --> 00:36:58.640
Thinking about Hinnick's suggestion of defense in depth, the way that I like to think about that is you want to raise the bar to a certain minimal level in a bunch of different areas.

00:36:58.640 --> 00:37:05.420
And then not get too obsessed with getting that last 5% of security on each possible access.

00:37:05.420 --> 00:37:14.700
So for dependencies, the way that I think about that is have the, you know, very widely available automation that already does this stuff like depend a bot.

00:37:14.700 --> 00:37:24.000
Make sure you are getting those PRs automatically pin everything so that you're never dealing with a library upgrade in the middle of future development.

00:37:24.000 --> 00:37:28.260
Your library upgrade work should always be, I am upgrading this library now.

00:37:28.260 --> 00:37:41.180
And 90% of the time, if you have that set up where you've got a build that is running on every PR that builds your whole dependency structure, that's got everything pinned and hashes pinned and everything.

00:37:41.180 --> 00:37:45.540
And then is also regularly receiving these PR updates.

00:37:45.880 --> 00:37:49.900
Most attackers who are doing things with supply chain attacks aren't all that clever.

00:37:49.900 --> 00:37:55.980
And so they will just end up trying to pop your CI and you'll see that you'll get some kind of error.

00:37:55.980 --> 00:37:57.420
You'll probably notice.

00:37:57.420 --> 00:38:10.920
Not necessarily attackers can be very sophisticated, but like you want to have everything, every library running in your nicely isolated CI environment on your GitHub actions or whatever first.

00:38:11.060 --> 00:38:17.320
And again, you want that those changes to all be like same code with the old dependencies, same code with new dependency.

00:38:17.320 --> 00:38:22.200
And every so often you'll get one very expensive upgrade that you really got to do and you got to make time for.

00:38:22.200 --> 00:38:30.420
If you're not upgrading all the super easy, almost free, like just hit the green button on the PR that's working.

00:38:30.840 --> 00:38:37.060
If you're not doing that all the time, then when you do get to those big upgrades, you will be upgrading 50 dependencies at a time.

00:38:37.060 --> 00:38:37.600
Yeah, it's going to be rough.

00:38:37.600 --> 00:38:38.120
Yeah.

00:38:38.120 --> 00:38:41.340
You really need to think about the cost to you as the defender.

00:38:41.340 --> 00:38:52.620
And the way that you reduce that cost on dependency upgrades is spending just a little bit of time every week, tending that automation and looking for those upgrades that are really going to take some development work.

00:38:52.980 --> 00:39:02.220
And there are fewer than you might think, like quite often feels like a really big task because most people get stuck in this, oh no, it's time to do the dependency upgrades this quarter.

00:39:02.220 --> 00:39:08.920
And you have like a seven week project that you're like trying to figure out which dependency is the one that's making all your tests fail.

00:39:08.920 --> 00:39:11.680
And you're like changing 50 lines in your requirements.txt all at once.

00:39:11.980 --> 00:39:21.780
If you don't do that, and by the time you're upgrading the one library that really does have a big breaking API change, it's not actually that hard as long as everything else is already up to date.

00:39:21.780 --> 00:39:22.040
Yeah.

00:39:22.040 --> 00:39:26.800
And usually there's one or two libraries that are massive, like Gingo or Flask.

00:39:26.800 --> 00:39:30.900
And then it's all the little dependencies that probably require no effort on your part.

00:39:30.900 --> 00:39:36.020
One other thing that I switched to, you talked about Dependabot, which is really great.

00:39:36.020 --> 00:39:46.220
And I have Dependabot turned on for my stuff, but I started using pip compile where it'll go and basically you give it an in file and it'll build your requirements.txt.

00:39:46.220 --> 00:39:50.360
And it'll even say like, this thing is here because it depends.

00:39:50.360 --> 00:39:51.620
You know, you installed Flask.

00:39:51.620 --> 00:39:53.640
That's why it's dangerous is here, for example.

00:39:53.640 --> 00:40:00.900
And I really like having that because it's just, you know, this week I'll go and I'll run it and it'll tell me what the new requirements are.

00:40:00.900 --> 00:40:02.780
I'm actively knowing that's happening.

00:40:02.780 --> 00:40:04.820
I'll sort of process it and go with it.

00:40:04.820 --> 00:40:05.760
What do you all think?

00:40:05.760 --> 00:40:08.320
Emily, are you using Dependabot or something else?

00:40:08.320 --> 00:40:09.900
Yeah, we do use Dependabot.

00:40:09.900 --> 00:40:17.120
I think another interesting like integral piece of being able to upgrade your dependencies is having tests that give you the confidence that you can.

00:40:17.120 --> 00:40:24.580
So do you feel confident to say, yes, our upgraded dependencies passed our test suite, therefore we can upgrade?

00:40:24.580 --> 00:40:27.480
Or is there something that is going to require somebody else to go in and look at it?

00:40:27.480 --> 00:40:31.340
But yeah, I think we've come a really long way in terms of dependency management.

00:40:31.340 --> 00:40:42.200
I do not miss the giant requirements.txt files that had all the dependencies of dependencies of dependencies all the way down and you didn't know what you would install versus another library.

00:40:42.200 --> 00:40:42.680
Yeah, exactly.

00:40:42.680 --> 00:40:58.620
I also like things like pip compile because it lets you have a little bit more control over the child dependencies versus relying on a top level library like Django to specify their own requirements in a way that works for your organization.

00:40:59.020 --> 00:40:59.800
Yeah, absolutely.

00:40:59.800 --> 00:41:10.980
So I also use Dependabot, like I said, and if you go and run pip-tools and then you commit that back to the repo, Dependabot will notice that that's been upgraded and it will automatically close the PR.

00:41:10.980 --> 00:41:13.540
So there's kind of a nice match between them as well.

00:41:13.540 --> 00:41:19.140
Yeah, the answers to all these questions are really are so much simpler than they used to be, you know, five, six years ago.

00:41:19.240 --> 00:41:33.780
I've been using requires.io for probably almost a decade and Dependabot is much better, largely because it defaults just sending you those individual upgrades and you can really tune how much stuff it's going to try to do at once.

00:41:34.320 --> 00:41:48.260
So yeah, mostly the answer on this is like, make something that builds your Docker container or whatever your fully realized application artifact is, run it all the time and have part of that process be like, freeze your dependencies, pin everything.

00:41:48.260 --> 00:41:51.960
You need to make sure that everything is pinned so that you get very reliable, repeatable builds.

00:41:51.960 --> 00:42:00.040
But having done that, you can really just bask in the, you know, cornucopia of tools that we have available to do this now that make it all pretty easy.

00:42:00.040 --> 00:42:01.940
Yeah, it's definitely getting easier and easier.

00:42:02.300 --> 00:42:03.960
Let's talk about performance a little bit.

00:42:03.960 --> 00:42:14.660
I guess maybe I'll show just really quickly, I'll throw out there that at sneak.io, S-N-Y-K.io, they've got some stuff where you can put in like a package or something like that.

00:42:14.660 --> 00:42:15.300
Let's see.

00:42:15.300 --> 00:42:19.080
I don't remember where you go to do it, but you can basically put in like a Python package.

00:42:19.080 --> 00:42:20.960
It'll give you a security report for it.

00:42:20.960 --> 00:42:25.120
So that's, I don't know how accurate that turns out to be for everything, but it's something.

00:42:25.120 --> 00:42:26.700
But let's talk about performance.

00:42:26.700 --> 00:42:28.640
And one thing I'd like to touch on is.

00:42:28.640 --> 00:42:31.560
Actually, can we just say one more thing about security before we move on?

00:42:31.560 --> 00:42:32.000
Yeah, go for it.

00:42:32.040 --> 00:42:35.760
Because Hennick mentioned encryption and not trusting your local network at the beginning.

00:42:35.760 --> 00:42:39.840
And one other thing I wanted to mention, speaking of tools that are much better than they used to be.

00:42:39.840 --> 00:42:49.360
One popular idiom is to like have a production mode that has all your encryption on and a development mode where you like just turn it all off for convenience.

00:42:50.040 --> 00:43:00.160
And I'm a big fan of setting up like an entry in your hosts file for each developer and having some infrastructure for provisioning certificates for individual developer machines.

00:43:00.400 --> 00:43:02.120
So that encryption is just always on.

00:43:02.120 --> 00:43:02.780
So that encryption is just always on.

00:43:02.780 --> 00:43:05.320
Nobody ever makes a connection without TLS on it.

00:43:05.320 --> 00:43:10.200
Even your like local API stubs still have some kind of TLS on them.

00:43:10.200 --> 00:43:13.280
Because it's actually not all that hard, particularly with Let's Encrypt.

00:43:13.280 --> 00:43:17.500
You get a couple of DNS plugins and you can easily vend those certificates to your dev team.

00:43:17.620 --> 00:43:19.180
It takes a couple of days of work at most.

00:43:19.180 --> 00:43:28.820
And having done that, not only are you more secure because you just don't even have that switch anymore to like accidentally be sending everything in plain text.

00:43:29.020 --> 00:43:32.120
But also you spot a surprising number of configuration issues.

00:43:32.120 --> 00:43:38.780
And like you get to see how your like for real certificates work while you're doing development.

00:43:38.780 --> 00:43:45.360
And that can really help a lot of developers like understand what's going on with the somewhat tricky world of HTTPS.

00:43:45.360 --> 00:43:48.020
So it's just running a little bit closer to production.

00:43:48.020 --> 00:43:48.400
Yeah.

00:43:48.400 --> 00:43:48.740
All right.

00:43:48.740 --> 00:43:49.980
Let's talk about performance.

00:43:49.980 --> 00:43:52.900
And I'll just put this up on the screen because I think it's a fun thing.

00:43:52.900 --> 00:43:54.320
Have you all seen Locus.io?

00:43:54.320 --> 00:43:56.940
Yeah, we use it regularly for load testing.

00:43:56.940 --> 00:43:57.360
Do you?

00:43:57.360 --> 00:43:57.720
Yeah.

00:43:57.920 --> 00:43:59.080
Maybe tell people about it real quick.

00:43:59.080 --> 00:44:01.060
I've used it once or twice and it's fantastic.

00:44:01.060 --> 00:44:01.700
Yeah, definitely.

00:44:01.700 --> 00:44:08.440
So it's a tool that allows you to essentially write a script that will emulate requests to your server.

00:44:08.440 --> 00:44:14.040
And then you can give it a variety of, wow, mom brain with no sleep makes me forget words.

00:44:14.040 --> 00:44:17.220
But basically just like different parameters that you can specify.

00:44:17.220 --> 00:44:24.620
So it makes it really easy to say like, here is the approximate randomized behavior for a single user.

00:44:24.780 --> 00:44:29.260
And then scale it up to hundreds or thousands of users and see how your server handles it.

00:44:29.260 --> 00:44:29.420
Right.

00:44:29.420 --> 00:44:32.500
You give it a Python script, which is really interesting, right?

00:44:32.500 --> 00:44:35.800
You create a class and you say, here's a typical user of this type.

00:44:35.800 --> 00:44:39.100
And it does different things like it'll call the index page.

00:44:39.100 --> 00:44:40.160
It'll call the about page.

00:44:40.160 --> 00:44:41.320
It'll go do a search.

00:44:41.760 --> 00:44:46.800
And you can say things like, there's going to be a certain amount of delay between pages.

00:44:46.800 --> 00:44:49.000
And then you can, as you said, scale that up.

00:44:49.000 --> 00:44:57.560
Say like, I want 2,700 regular users and 50 admin users and let them go crawl around on the site and do what they do.

00:44:57.560 --> 00:44:57.800
Right.

00:44:58.120 --> 00:44:58.320
Yeah.

00:44:58.320 --> 00:44:59.500
So it's super cool.

00:44:59.500 --> 00:45:02.400
And the, like the real, a real time dashboard is neat.

00:45:02.400 --> 00:45:12.640
So if you want to know about your performance, you could use this, but you know, how do you all think about performance for the apps, either you're running or delivering, you know, what's fast enough?

00:45:12.640 --> 00:45:16.940
What's just wasting your time, getting that last millisecond of response time?

00:45:16.940 --> 00:45:23.820
So I'm going to make this the bold statement that for the vast majority of developers, Python is fast enough.

00:45:23.820 --> 00:45:25.100
Oh yeah, absolutely.

00:45:25.100 --> 00:45:27.460
It is fast enough to saturate a database.

00:45:27.460 --> 00:45:30.440
And once your database is saturated, you have different problems.

00:45:30.440 --> 00:45:34.580
Like there's definitely things for what it is, where it is a problem.

00:45:34.580 --> 00:45:37.580
Like you cannot saturate an LDAP server, for example.

00:45:37.580 --> 00:45:40.800
I know that because I tried and it's one of our Go services.

00:45:40.800 --> 00:45:43.680
It would be nice if Python could be faster.

00:45:43.680 --> 00:45:47.620
And I'm very excited about Guido's performance task force at Microsoft.

00:45:47.620 --> 00:45:48.240
It's great.

00:45:48.240 --> 00:45:48.520
Yeah.

00:45:48.520 --> 00:45:58.300
I feel like the indifference that the no-jill movement has been shown kind of shows that nobody really thinks that intensively about it anymore.

00:45:58.300 --> 00:46:00.980
Like for most people, it's fine.

00:46:00.980 --> 00:46:01.780
It's not good.

00:46:01.780 --> 00:46:02.300
It's not great.

00:46:02.300 --> 00:46:03.240
It's fine.

00:46:03.240 --> 00:46:05.380
And I mean, Instagram is printing money with it.

00:46:05.380 --> 00:46:08.160
So they absolutely are.

00:46:08.160 --> 00:46:08.880
Yeah.

00:46:09.020 --> 00:46:14.860
You know, you can go and put your site into PageSpeed Insights and see what you get.

00:46:14.860 --> 00:46:24.280
And I think once you get down to several millisecond response time per page, it just, you know, there's not a whole lot you can do to make it faster.

00:46:24.280 --> 00:46:27.840
There's not a lot of benefit to doing that work, right?

00:46:27.840 --> 00:46:31.120
It's mostly tuning a database with the right indexes and caching.

00:46:31.120 --> 00:46:32.160
That's the two things.

00:46:32.160 --> 00:46:32.680
Yeah.

00:46:32.680 --> 00:46:35.300
And everything else is like the last 10% or something.

00:46:35.300 --> 00:46:35.940
Yeah, absolutely.

00:46:36.120 --> 00:46:39.840
So what I wanted to definitely emphasize here and hear your thoughts.

00:46:39.840 --> 00:46:45.320
I feel like there's so many times I go to some page and it doesn't necessarily, some site doesn't necessarily have to be Python.

00:46:45.320 --> 00:46:47.960
It's just some database backed page.

00:46:47.960 --> 00:46:51.740
And you go there and it's sitting on what seems like one of their primary pages.

00:46:51.740 --> 00:46:55.460
And it's like five seconds until it responds or several seconds even.

00:46:55.460 --> 00:46:57.600
It's like, what is this thing doing?

00:46:58.120 --> 00:47:01.300
And I just, I know they don't have indexes in their database.

00:47:01.300 --> 00:47:02.700
They just, they can't.

00:47:02.700 --> 00:47:09.880
So, you know, I just want to put out a plea for please use like some database profiling feature.

00:47:09.880 --> 00:47:12.620
Please look at your queries and please put an index.

00:47:12.620 --> 00:47:14.080
It's so incredibly easy.

00:47:14.080 --> 00:47:16.400
Like any of you all want to rant on that with me?

00:47:16.400 --> 00:47:19.260
I need to bring up one tool before we talk about this.

00:47:19.260 --> 00:47:21.020
And it's called PG mustard.

00:47:21.020 --> 00:47:21.560
Okay.

00:47:21.560 --> 00:47:25.380
I've never been able to fully understand an explained statement.

00:47:25.380 --> 00:47:28.280
Like I learned it many times and then I forgot it again.

00:47:28.280 --> 00:47:30.140
And PG mustard is amazing.

00:47:30.140 --> 00:47:32.160
You just take an explain.

00:47:32.160 --> 00:47:36.460
You copy paste it into a web app and it tells you exactly what's going wrong.

00:47:36.460 --> 00:47:37.160
Oh, nice.

00:47:37.300 --> 00:47:40.340
And I've shaved, like I had like one query, which is like very big.

00:47:40.340 --> 00:47:41.480
It's from a financial system.

00:47:41.480 --> 00:47:46.620
And I've, I think like 66% of the query at runtime, I was able to shave off.

00:47:46.620 --> 00:47:46.880
Yeah.

00:47:46.880 --> 00:47:49.580
Just because there was an index, but it was set up wrong.

00:47:49.580 --> 00:47:50.820
It was amazing.

00:47:50.820 --> 00:47:51.100
Yeah.

00:47:51.100 --> 00:47:53.180
So here's a really beautiful visual thing.

00:47:53.180 --> 00:47:55.940
And it's also a bit of like a profiler.

00:47:55.940 --> 00:48:00.400
So it says on this part of the query statement, you spent 0.4% of the time.

00:48:00.400 --> 00:48:03.080
And this part you spent 58% of the time, right?

00:48:03.080 --> 00:48:04.360
Here's an index scan.

00:48:04.360 --> 00:48:05.260
That's the best part.

00:48:05.500 --> 00:48:10.240
It doesn't just tell you what's going wrong, but do this, make this index.

00:48:10.240 --> 00:48:11.540
Things like that.

00:48:11.540 --> 00:48:11.840
Okay.

00:48:11.840 --> 00:48:13.360
I've never seen this before.

00:48:13.360 --> 00:48:14.040
This is fantastic.

00:48:14.040 --> 00:48:15.840
Emily, thoughts on indexes?

00:48:15.840 --> 00:48:16.920
Join my plea.

00:48:16.920 --> 00:48:18.620
Yeah.

00:48:18.620 --> 00:48:21.220
So I think that that piece of it's definitely very important.

00:48:21.220 --> 00:48:25.020
Like making sure that you're doing those sort of basic things to get you to that, you

00:48:25.020 --> 00:48:27.560
know, 80 to 90% performance potential.

00:48:27.560 --> 00:48:33.700
But I would also argue that these days in a vast majority of applications, you're going

00:48:33.700 --> 00:48:35.800
to have a decoupled front end from your back end.

00:48:35.800 --> 00:48:41.540
And I would argue that making sure that you're implementing best practices for user experience

00:48:41.540 --> 00:48:46.760
on your front end is going to give you so much more payoff than trying to optimize that

00:48:46.760 --> 00:48:49.500
API call by, you know, a few milliseconds.

00:48:49.960 --> 00:48:54.180
So previously I had pulled up the PageSpeed Insights before, right?

00:48:54.180 --> 00:48:58.200
And at PageSpeed.web.dev from Google, I believe.

00:48:58.200 --> 00:48:58.780
Yeah.

00:48:59.220 --> 00:49:04.040
And what's really interesting about that is if you go and put your site into there,

00:49:04.040 --> 00:49:06.480
it doesn't feel fantastic to do it, by the way.

00:49:06.480 --> 00:49:11.260
So if you go and you put your site in here, you at some point might get a good number.

00:49:11.260 --> 00:49:14.940
I'm getting 100 out of 100 on the TalkByThan training site right now.

00:49:14.940 --> 00:49:19.400
But that's because I spent three days addressing every little thing.

00:49:19.400 --> 00:49:21.320
Like this image is not sized right.

00:49:21.320 --> 00:49:23.980
This JavaScript is not bundled with that JavaScript.

00:49:24.380 --> 00:49:26.820
This element is being resized by the browser.

00:49:26.820 --> 00:49:29.560
You should make it the same size by default.

00:49:29.560 --> 00:49:31.260
And just all of these little things.

00:49:31.260 --> 00:49:36.640
And it wasn't even about the server response time, which was always pretty low.

00:49:36.640 --> 00:49:38.760
It's about all the other stuff.

00:49:38.760 --> 00:49:43.960
It's like, how does it feel to the user to get to the page rather than what is the, you

00:49:43.960 --> 00:49:46.720
know, HTML out of the server response time?

00:49:46.720 --> 00:49:48.120
And that's what you're talking about, right?

00:49:48.120 --> 00:49:48.920
That kind of stuff?

00:49:48.920 --> 00:49:49.100
Yep.

00:49:49.100 --> 00:49:49.900
Yeah.

00:49:49.900 --> 00:49:50.400
Yeah, definitely.

00:49:50.400 --> 00:49:50.780
Awesome.

00:49:50.780 --> 00:49:51.420
All right.

00:49:51.420 --> 00:49:54.620
So I have so many feelings about this.

00:49:54.620 --> 00:49:57.880
This is probably the number two topic for me behind packaging.

00:49:57.880 --> 00:49:58.240
Okay.

00:49:58.240 --> 00:50:02.280
Those of you not listening on the live stream, if this next part sounds choppy, it's because

00:50:02.280 --> 00:50:05.260
I talked for an hour and a half and Michael had to edit it down.

00:50:05.260 --> 00:50:07.840
We just cut him off.

00:50:07.840 --> 00:50:09.960
We just had to cut him off when he lost his voice.

00:50:10.160 --> 00:50:15.720
But seriously, the thing that I think is most important is the, quite often the parts of

00:50:15.720 --> 00:50:22.200
your application that end up being the performance issues are once you're past this step.

00:50:22.200 --> 00:50:26.640
Now there are lots of like, you all have been mostly talking about websites, except for Hennek

00:50:26.640 --> 00:50:28.380
mentioning LDAP, which is an interesting.

00:50:28.380 --> 00:50:30.980
I've never really thought about scaling that one.

00:50:30.980 --> 00:50:31.140
No.

00:50:31.280 --> 00:50:33.520
I have also scaled LDAP.

00:50:33.520 --> 00:50:38.640
So the two major applications that I've dealt with performance issues on are an internal

00:50:38.640 --> 00:50:42.600
calendaring service that I maintained at a large company that you can figure out which

00:50:42.600 --> 00:50:48.440
one it is by reading my Wikipedia page and pilot.com's internal bookkeeping automations.

00:50:48.440 --> 00:50:54.560
And in both of those, so number one, the calendar service was an API with no front end.

00:50:54.560 --> 00:50:58.740
The front end was maintained by the client teams that were not even doing web stuff.

00:50:59.280 --> 00:51:05.500
And the way that we had to performance test that involved standing up our own custom load

00:51:05.500 --> 00:51:10.740
generation tool and running it as kind of a qualification process for our deployments.

00:51:10.740 --> 00:51:15.740
And the reason that I bring up that one is it's interesting because we had to figure out

00:51:15.740 --> 00:51:17.920
what our actual interesting load patterns were.

00:51:17.920 --> 00:51:22.980
We couldn't use any of these like standard HTTP load generation things because we needed very

00:51:22.980 --> 00:51:23.980
specific data.

00:51:23.980 --> 00:51:27.760
And that often ends up being the problem that you're facing.

00:51:27.760 --> 00:51:32.720
We, on that service in particular, we had performance problems that arose because we

00:51:32.720 --> 00:51:34.300
added too many indexes.

00:51:34.300 --> 00:51:37.420
So we were having problems on the right side of the equation.

00:51:37.420 --> 00:51:42.680
Every time you add an index, you're optimizing read at the expense of write.

00:51:42.680 --> 00:51:47.840
And usually it's like a lot of read performance for a little write performance, but eventually

00:51:47.840 --> 00:51:48.720
it does add up.

00:51:48.720 --> 00:51:49.260
It does.

00:51:49.260 --> 00:51:49.440
Yeah.

00:51:49.560 --> 00:51:54.300
Kivo out in the audience says, yeah, you've got to be careful adding too many indexes as

00:51:54.300 --> 00:51:54.720
well.

00:51:54.720 --> 00:51:56.840
There's two parts to that, right?

00:51:56.840 --> 00:52:01.480
One part is when you write something, the indexes have to be computed for that thing that's going

00:52:01.480 --> 00:52:01.740
in.

00:52:01.740 --> 00:52:04.680
And the other is more indexes mean more stuff in memory.

00:52:04.680 --> 00:52:09.880
And so like another really important aspect is do your, does the totality of your indexes

00:52:09.880 --> 00:52:12.240
reside in memory or does it have to get paged out?

00:52:12.240 --> 00:52:12.760
Right.

00:52:12.780 --> 00:52:14.340
And so you would, you would hit that problem.

00:52:14.340 --> 00:52:17.500
Both of those problems you would run into by having too many indexes.

00:52:17.500 --> 00:52:17.800
Right.

00:52:17.800 --> 00:52:23.400
And you really need to, so you need to be ready to measure things because you don't necessarily

00:52:23.400 --> 00:52:24.000
know.

00:52:24.000 --> 00:52:28.300
Like, I mean, this is that, you know, chestnut about not really, you know, premature optimization,

00:52:28.300 --> 00:52:30.860
not knowing what the hotspots are until you run them.

00:52:30.860 --> 00:52:32.580
But that also means, so there's two things about that.

00:52:32.580 --> 00:52:35.000
One, the tools for doing this are not great.

00:52:35.000 --> 00:52:41.020
Like the one that I really wish were good and just isn't is SpeedCenter, which Twisted and

00:52:41.020 --> 00:52:43.700
PyPy used to monitor their performance over time.

00:52:43.700 --> 00:52:46.160
Like what the performance of each revision of the code is.

00:52:46.160 --> 00:52:51.880
And there's nothing like, you know, GitHub Actions or Travis CI, there's no sort of leader

00:52:51.880 --> 00:52:56.960
in that field that will just tell you like, hey, your performance regressed by 10% on this

00:52:56.960 --> 00:52:57.260
commit.

00:52:57.260 --> 00:53:00.420
And that is the tool which I desperately want to exist.

00:53:00.420 --> 00:53:03.560
And so most of the things that I do are trying to approximate that.

00:53:03.560 --> 00:53:07.900
Part of that is making sure you have metrics in production that are telling you so that you

00:53:07.900 --> 00:53:09.180
notice when things are slow.

00:53:09.180 --> 00:53:14.400
You don't want to be having users telling you, or even really like if you're getting the bad

00:53:14.400 --> 00:53:18.840
performance metrics out of your load testing tool, and that's a surprise to you, that means

00:53:18.840 --> 00:53:23.200
you're probably not instrumenting enough in prod to know like, oh, users are seeing some

00:53:23.200 --> 00:53:23.840
slowness here.

00:53:23.840 --> 00:53:26.860
Because you're also going to get things where like your database is doing great.

00:53:26.860 --> 00:53:30.420
Everything seems like it's super fast, but your queries are actually really slow.

00:53:30.420 --> 00:53:31.640
They're just all running in memory.

00:53:31.640 --> 00:53:34.660
And then you hit the cliff where suddenly you're hitting the disk.

00:53:34.660 --> 00:53:36.640
And now everything's much, much slower.

00:53:36.640 --> 00:53:38.080
And none of your code changed.

00:53:38.080 --> 00:53:40.080
And your data only grew by like 10%.

00:53:40.080 --> 00:53:45.440
And being able to spot stuff like that means you have to be looking at perf in prod.

00:53:45.440 --> 00:53:48.280
You can't do synthetic tests for everything.

00:53:48.280 --> 00:53:53.540
And particularly if you have a large site with a lot of users, it's very easy to miss if your

00:53:53.540 --> 00:53:56.700
95th percentile is falling off a cliff, right?

00:53:56.700 --> 00:54:02.080
Like you have to be looking at your core tiles and like all of these different things, not just

00:54:02.080 --> 00:54:03.280
like average performance.

00:54:03.280 --> 00:54:08.740
And the second part of that is the custom data generation for your synthetic tests.

00:54:08.740 --> 00:54:12.980
So for example, on the calendar service had those issues that I just mentioned.

00:54:12.980 --> 00:54:18.240
And pilot service had this issue where most of the performance stuff was not the database.

00:54:18.240 --> 00:54:23.220
It was talking to APIs to pull in financial transactions and analyze them.

00:54:23.220 --> 00:54:28.500
And it was those APIs being slow, us being silly and not talking to those APIs in parallel,

00:54:29.200 --> 00:54:33.820
data volumes just being huge, like thousands and thousands and thousands of transactions

00:54:33.820 --> 00:54:35.220
in a single call.

00:54:35.220 --> 00:54:38.040
And that you have to know that that's going to happen.

00:54:38.040 --> 00:54:44.780
And you have to be able to on demand add to your test suite or your performance test suite

00:54:44.780 --> 00:54:46.120
new types of data.

00:54:46.120 --> 00:54:51.180
And that performance tool with where you like write Python code looks like a great way to

00:54:51.180 --> 00:54:51.500
do that.

00:54:51.500 --> 00:54:52.920
I actually had never used that one.

00:54:52.920 --> 00:54:53.900
This thing is glorious.

00:54:53.900 --> 00:54:54.300
Yeah.

00:54:54.300 --> 00:54:55.000
What was it?

00:54:55.000 --> 00:54:55.740
Locust.io?

00:54:55.740 --> 00:54:56.040
Yeah.

00:54:56.200 --> 00:54:56.560
Yeah.

00:54:56.560 --> 00:55:01.180
And it has the dynamic sort of graphs and dashboards show you sort of as it ramps up

00:55:01.180 --> 00:55:01.940
and as you change it.

00:55:01.940 --> 00:55:06.320
And it's, I think that might give you like the right, because you basically structure with

00:55:06.320 --> 00:55:09.120
Python how it hammers on the server, which is pretty neat.

00:55:09.120 --> 00:55:09.360
Yeah.

00:55:09.360 --> 00:55:12.640
And the one thing I'd say about that was you need to do that.

00:55:12.640 --> 00:55:14.760
You will need to write custom stuff.

00:55:14.760 --> 00:55:16.820
Don't just assume you can like add a couple indexes.

00:55:16.820 --> 00:55:21.220
Like you should just add a couple indexes at first if performance is not your primary concern.

00:55:21.820 --> 00:55:26.100
But having done that, you have to know you're going to need to think about perf and like

00:55:26.100 --> 00:55:28.280
write code to monitor it.

00:55:28.280 --> 00:55:28.640
Absolutely.

00:55:28.640 --> 00:55:29.460
All right.

00:55:29.460 --> 00:55:33.920
I think we're just about out of time, although we have barely scratched the surface of all

00:55:33.920 --> 00:55:35.020
the stuff we could talk about.

00:55:35.020 --> 00:55:36.780
Let's close out with this question.

00:55:36.780 --> 00:55:41.320
Do any of you have, sounds like maybe you've thought about this.

00:55:41.320 --> 00:55:47.780
You have CI performance checks or failures or anything like that, right?

00:55:47.780 --> 00:55:49.260
Like we have run our tests.

00:55:49.260 --> 00:55:51.620
The build doesn't pass if the tests don't pass.

00:55:51.620 --> 00:55:53.140
But do you have something like that for performance?

00:55:53.140 --> 00:55:54.620
That's the thing I want to exist.

00:55:54.620 --> 00:55:56.300
And I've never managed.

00:55:56.300 --> 00:56:00.780
I've done things that approximate it within tests, but never gotten it really.

00:56:00.780 --> 00:56:02.100
I have nothing like that either.

00:56:02.100 --> 00:56:02.740
Emily?

00:56:02.740 --> 00:56:08.360
No, I mean, I think the closest approximation that we get is we focus a lot on like Cypress

00:56:08.360 --> 00:56:09.300
tests for front end.

00:56:09.300 --> 00:56:12.760
So actually the user going through and working with the application.

00:56:12.760 --> 00:56:17.860
And I think the closest thing that we could get at this point is just setting our max time

00:56:17.860 --> 00:56:23.680
out in our like HTTP service, bumping that down and saying like, if anything's taking over

00:56:23.680 --> 00:56:26.740
10 seconds to respond when we run the tests, then it should fail.

00:56:26.740 --> 00:56:31.040
But no, I don't think we don't do any like regular performance testing.

00:56:31.040 --> 00:56:31.380
Yeah.

00:56:31.380 --> 00:56:32.040
Hynek?

00:56:32.040 --> 00:56:32.920
We do not know.

00:56:32.920 --> 00:56:33.240
Yeah.

00:56:33.240 --> 00:56:36.000
But we all kind of are like, that was kind of nice to have, I think.

00:56:36.000 --> 00:56:36.560
But yeah.

00:56:36.920 --> 00:56:38.360
There's a lot to set it up though, right?

00:56:38.360 --> 00:56:40.760
You've got to have enough data in the database for it to be meaningful.

00:56:40.760 --> 00:56:42.200
And that's tricky to do in CI.

00:56:42.200 --> 00:56:43.600
And it would be cool though.

00:56:43.600 --> 00:56:48.100
I think Glowth makes a really good point that like the load testing and the manual testing

00:56:48.100 --> 00:56:51.920
of performance is great, especially when it's a prerequisite to launch.

00:56:51.920 --> 00:56:55.520
But there's no way that you're going to be able to replicate anything in production.

00:56:55.520 --> 00:56:59.260
And the best thing that you can do is monitor prod as closely as you can.

00:56:59.260 --> 00:57:00.180
Yeah, absolutely.

00:57:00.180 --> 00:57:01.560
Some of that real-time monitoring.

00:57:01.560 --> 00:57:02.160
Fantastic.

00:57:02.160 --> 00:57:02.860
All right.

00:57:02.860 --> 00:57:04.580
Well, thank you all for being here.

00:57:04.580 --> 00:57:06.480
This has been super interesting to chat about.

00:57:06.680 --> 00:57:10.280
Before you got out of here, let me, I think I'll just put it down to one of the final

00:57:10.280 --> 00:57:12.540
two questions so that we don't take too long.

00:57:12.540 --> 00:57:15.340
But you're going to write some Python code, what editor to use?

00:57:15.340 --> 00:57:15.860
Let's go.

00:57:15.860 --> 00:57:17.300
Blockwise, Hynek, how about you?

00:57:17.300 --> 00:57:18.540
What are you writing code with these days?

00:57:18.540 --> 00:57:20.520
Well, I have a long story of editors.

00:57:20.520 --> 00:57:22.580
I've used almost all of them at some point.

00:57:22.580 --> 00:57:26.640
I usually stopped using because I got crippled, like Emix Pinky.

00:57:26.640 --> 00:57:31.400
Nowadays, I usually use either Vim in a console or VS Code.

00:57:31.400 --> 00:57:31.660
Okay.

00:57:31.660 --> 00:57:32.160
Right on.

00:57:32.280 --> 00:57:32.460
Cliff?

00:57:32.460 --> 00:57:36.740
There's an implicit thing in this question where it sounds like you're recommending the

00:57:36.740 --> 00:57:37.540
thing that you use.

00:57:37.540 --> 00:57:39.360
So I want to be clear that I'm not doing that.

00:57:39.360 --> 00:57:41.960
Don't do as I do.

00:57:41.960 --> 00:57:42.620
Yeah.

00:57:42.620 --> 00:57:48.660
I use Emix, but with about 10 megabytes of custom Elisp, which I'm never going to share

00:57:48.660 --> 00:57:49.200
with anyone.

00:57:49.200 --> 00:57:49.960
Fantastic.

00:57:49.960 --> 00:57:50.960
Because you shouldn't use it.

00:57:50.960 --> 00:57:51.680
Just use VS Code.

00:57:51.680 --> 00:57:52.440
Awesome.

00:57:52.440 --> 00:57:52.860
Emily?

00:57:52.860 --> 00:57:53.500
VS Code.

00:57:53.500 --> 00:57:54.180
All day.

00:57:54.180 --> 00:57:54.460
Okay.

00:57:54.620 --> 00:57:59.880
Big thanks to Brett Cannon, who sat me down at a Python and forced me to use it because

00:57:59.880 --> 00:58:02.420
the first time I used it, I hated it and I went back to Sublime.

00:58:02.420 --> 00:58:05.080
But yeah, I don't think there's anything that competes these days.

00:58:05.080 --> 00:58:05.360
Yeah.

00:58:05.360 --> 00:58:06.860
He's such a good ambassador for that.

00:58:06.860 --> 00:58:08.560
So it's good to have him working on it.

00:58:08.560 --> 00:58:09.060
All right.

00:58:09.060 --> 00:58:10.600
Thank you all for being on the podcast.

00:58:10.600 --> 00:58:15.220
It's been great to chat about the stuff and really insightful to get your experience.

00:58:15.220 --> 00:58:16.200
Thanks so much for having us.

00:58:16.200 --> 00:58:16.360
Awesome.

00:58:16.360 --> 00:58:17.000
Thanks for having us.

00:58:17.000 --> 00:58:17.100
Yeah.

00:58:17.100 --> 00:58:17.440
You bet.

00:58:17.440 --> 00:58:17.600
Bye.

00:58:17.600 --> 00:58:17.940
Bye.

00:58:17.940 --> 00:58:17.980
Bye.

00:58:19.800 --> 00:58:22.460
This has been another episode of Talk Python To Me.

00:58:22.460 --> 00:58:24.260
Thank you to our sponsors.

00:58:24.260 --> 00:58:25.880
Be sure to check out what they're offering.

00:58:25.880 --> 00:58:27.300
It really helps support the show.

00:58:27.300 --> 00:58:32.180
Add high-performance, multi-party video calls to any app or website with SignalWire.

00:58:32.180 --> 00:58:38.280
Visit talkpython.fm/SignalWire and mention that you came from Talk Python To Me to get started

00:58:38.280 --> 00:58:39.420
and grab those free credits.

00:58:39.420 --> 00:58:46.480
Tonic.ai creates quality test data that does not contain personally identifiable information.

00:58:47.140 --> 00:58:51.440
Your generated data sets are safe to share with developers, UA, and data scientists.

00:58:51.440 --> 00:58:56.620
Most importantly, they behave like production because they're made from production data.

00:58:56.620 --> 00:59:00.720
Check them out at talkpython.fm/tonic.

00:59:00.720 --> 00:59:02.780
Want to level up your Python?

00:59:02.780 --> 00:59:06.820
We have one of the largest catalogs of Python video courses over at Talk Python.

00:59:06.820 --> 00:59:12.000
Our content ranges from true beginners to deeply advanced topics like memory and async.

00:59:12.000 --> 00:59:14.660
And best of all, there's not a subscription in sight.

00:59:15.060 --> 00:59:17.580
Check it out for yourself at training.talkpython.fm.

00:59:17.580 --> 00:59:19.480
Be sure to subscribe to the show.

00:59:19.480 --> 00:59:22.260
Open your favorite podcast app and search for Python.

00:59:22.260 --> 00:59:23.560
We should be right at the top.

00:59:23.560 --> 00:59:28.740
You can also find the iTunes feed at /itunes, the Google Play feed at /play,

00:59:28.740 --> 00:59:32.940
and the direct RSS feed at /rss on talkpython.fm.

00:59:32.940 --> 00:59:36.360
We're live streaming most of our recordings these days.

00:59:36.360 --> 00:59:39.780
If you want to be part of the show and have your comments featured on the air,

00:59:40.080 --> 00:59:44.200
be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

00:59:44.200 --> 00:59:46.040
This is your host, Michael Kennedy.

00:59:46.040 --> 00:59:47.340
Thanks so much for listening.

00:59:47.340 --> 00:59:48.500
I really appreciate it.

00:59:48.500 --> 00:59:50.400
Now get out there and write some Python code.

00:59:50.400 --> 00:59:51.100
Bye.

00:59:51.100 --> 00:59:51.100
Bye.

00:59:51.100 --> 00:59:51.220
Bye.

00:59:51.220 --> 00:59:51.840
Bye.

00:59:51.840 --> 00:59:52.220
Bye.

00:59:52.220 --> 00:59:53.220
Bye.

00:59:53.220 --> 00:59:54.220
Bye.

00:59:54.220 --> 00:59:55.220
Bye.

00:59:55.220 --> 00:59:55.220
Bye.

00:59:55.220 --> 00:59:56.220
Bye.

00:59:56.220 --> 00:59:57.220
Bye.

00:59:57.220 --> 00:59:57.220
Bye.

00:59:57.220 --> 00:59:57.220
Bye.

00:59:57.220 --> 00:59:57.220
Bye.

00:59:57.220 --> 00:59:57.220
Bye.

00:59:57.220 --> 00:59:57.220
Bye.

00:59:57.220 --> 00:59:58.220
Bye.

00:59:58.220 --> 00:59:59.220
Bye.

00:59:59.220 --> 00:59:59.220
Bye.

00:59:59.220 --> 00:59:59.220
Bye.

00:59:59.220 --> 01:00:00.220
Bye.

01:00:00.220 --> 01:00:01.220
Bye.

01:00:01.220 --> 01:00:01.220
Bye.

01:00:01.220 --> 01:00:01.220
Bye.

01:00:01.220 --> 01:00:01.220
Bye.

01:00:01.220 --> 01:00:02.220
Bye.

01:00:02.220 --> 01:00:02.220
Bye.

01:00:02.220 --> 01:00:03.220
Bye.

01:00:03.220 --> 01:00:03.220
Bye.

01:00:03.220 --> 01:00:04.220
Bye.

01:00:04.220 --> 01:00:05.220
Bye.

01:00:05.220 --> 01:00:05.220
Bye.

01:00:05.220 --> 01:00:05.220
Bye.

01:00:05.220 --> 01:00:06.220
Bye.

01:00:06.220 --> 01:00:07.220
Bye.

01:00:07.220 --> 01:00:07.720
you

01:00:07.720 --> 01:00:08.220
you

01:00:08.220 --> 01:00:08.720
you

01:00:08.720 --> 01:00:10.720
Thank you.

01:00:10.720 --> 01:00:40.700
Thank you.

