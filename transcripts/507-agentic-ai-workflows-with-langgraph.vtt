WEBVTT

00:00:00.020 --> 00:00:26.660
If you want to leverage the power of LLMs in your Python apps, you would be wise to consider an Identic framework. Identic empowers the LLMs to use tools and take further actions based on what it's learned at that point. And frameworks provide all the necessary building blocks to weave these into your apps with features like long-term memory and durable resumability. I'm excited to have Sydney Runkle back on the podcast to dive into building Python apps with LangChain and LangGraph.

00:00:27.320 --> 00:00:32.900
This is Talk Python To Me, episode 507, recorded Tuesday, May 6th, 2025.

00:00:34.180 --> 00:00:34.560
Are

00:00:34.560 --> 00:00:36.020
you ready for your host, please?

00:00:36.800 --> 00:00:39.640
You're listening to Michael Kennedy on Talk Python To Me.

00:00:40.260 --> 00:00:43.360
Live from Portland, Oregon, and this segment was made with Python.

00:00:46.620 --> 00:00:49.480
Welcome to Talk Python To Me, a weekly podcast on Python.

00:00:50.080 --> 00:00:51.720
This is your host, Michael Kennedy.

00:00:52.100 --> 00:01:04.980
Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython, both accounts over at Fostadon.org and keep up with the show and listen to over nine years of episodes at talkpython.fm.

00:01:05.580 --> 00:01:09.440
If you want to be part of our live episodes, you can find the live streams over on YouTube.

00:01:09.940 --> 00:01:15.720
Subscribe to our YouTube channel over at talkpython.fm/youtube and get notified about upcoming shows.

00:01:16.140 --> 00:01:19.820
This episode is sponsored by Posit Connect from the makers of Shiny.

00:01:20.300 --> 00:01:24.160
Publish, share, and deploy all of your data projects that you're creating using Python.

00:01:24.820 --> 00:01:30.860
Streamlit, Dash, Shiny, Bokeh, FastAPI, Flask, Quarto, Reports, Dashboards, and APIs.

00:01:31.760 --> 00:02:18.480
Posit Connect supports all of them. Try Posit Connect for free by going to talkpython.fm/posit, B-O-S-I-T. And it's brought to you by Auth0. Auth0 is an easy to implement adaptable authentication and authorization platform. Think easy user logins, social sign-on, multi-factor authentication and robust role-based access control. With over 30 SDKs and quick starts, Auth0 scales with your product at every stage. Get 25,000 monthly active users for free at talkpython.fm/auth0. Hey all, over at Talk Python we have an exciting new course, Polars for Power Users. If you're into data science or you're just looking for a better way to automate your world than tools like Excel and Google Sheets, this course is for you.

00:02:19.240 --> 00:02:25.560
And if you already do data science using older tools like Pandas and NumPy, maybe it's time to add Polars to your toolkit.

00:02:26.260 --> 00:02:41.880
Whether you're wrangling CSVs, taming messy Excel files, or joining multi-gigabyte datasets, Polars for Power Users shows you exactly how to translate everyday data tasks into clean Pythonic code that runs 10 to 100 times faster than traditional approaches.

00:02:42.460 --> 00:02:57.280
Through a pragmatic mix, short lectures, live REPL sessions, and a capstone case study, You'll master Polars fluent syntax, lazy evaluation engine, and productivity-boosting features such as pivot tables, joins, and Excel-style data accessors.

00:02:57.800 --> 00:03:01.560
This excellent and very entertaining course was created by Christopher Trudeau.

00:03:01.980 --> 00:03:06.160
His deep experience and connections in the Python world are evident right from the start.

00:03:06.520 --> 00:03:08.120
Plus, this course doesn't waste your time.

00:03:08.380 --> 00:03:10.760
You'll be up and running with Polars in under three hours.

00:03:11.260 --> 00:03:14.160
It's currently available for just $29 or less.

00:03:14.960 --> 00:03:20.280
Visit talkpython.fm and click Courses in the nav bar or just click the link in your podcast player's show notes.

00:03:20.600 --> 00:03:23.700
Thanks to everyone who's taken one of our courses. It really helps support the show.

00:03:24.440 --> 00:03:25.620
Now, let's get on to that interview.

00:03:26.420 --> 00:03:29.220
Sydney, welcome back to Talk Python To Me. Fantastic to have you here.

00:03:29.420 --> 00:03:31.100
Thanks. Super excited to be back.

00:03:31.420 --> 00:03:32.700
I'm excited to have you back.

00:03:33.020 --> 00:03:34.660
We always get to talk about cool things.

00:03:35.520 --> 00:03:38.300
And for a long time, that's been Pydantic.

00:03:38.760 --> 00:03:42.160
And the last time you were on, we talked about Pydantic performance tips.

00:03:42.300 --> 00:03:46.140
and now it's the topic du jour really.

00:03:46.880 --> 00:04:02.880
Building with LLMs, which I think is really, really different than saying I use an LLM to build my code or I use LLMs to answer questions or kind of an alternative search, but actually using it as a library style in your code, right?

00:04:03.140 --> 00:04:03.740
Yeah, definitely.

00:04:04.250 --> 00:04:06.920
I think you can kind of turn things inside out, right?

00:04:07.240 --> 00:04:14.300
And like thinking about all the things that you can build with LLMs, specifically with LangRaph, And just what we're going to chat about today is really exciting.

00:04:14.480 --> 00:04:14.600
Yeah,

00:04:14.700 --> 00:04:22.320
it is also tricky because as everyone knows who's played with them, LLMs are weirdly not deterministic.

00:04:22.900 --> 00:04:26.080
And so programs typically like determinism.

00:04:26.320 --> 00:04:29.020
They like, if I call this function, it's going to do a thing.

00:04:29.220 --> 00:04:33.180
If I don't call the function, it's not going to do the thing or whatever, right?

00:04:33.760 --> 00:04:35.320
So that's a big challenge.

00:04:35.500 --> 00:04:38.920
And I think that's part of what your old toolchain is meant to do.

00:04:39.520 --> 00:04:47.200
Now, just because you've been on the show a couple of times, there's probably still people out there who don't know who you are, and your role has changed a little bit.

00:04:47.500 --> 00:04:51.500
How about a quick introduction into yourself again and what you're doing these days?

00:04:51.560 --> 00:04:54.100
I know you made a career transition recently.

00:04:54.240 --> 00:04:54.320
My

00:04:54.320 --> 00:04:55.420
name is Sydney. Nice to meet folks.

00:04:56.380 --> 00:04:58.580
I am a passionate open source developer.

00:04:59.040 --> 00:05:05.060
And so on previous episodes, I was on talking about Pydantic and doing kind of open source maintenance there.

00:05:05.500 --> 00:05:10.320
So Pydantic is both an open source tool, and now there's a company built around that open source tool.

00:05:10.720 --> 00:05:17.280
And I recently switched over to LangChain, excited to be working with an open source team in person in Boston.

00:05:18.020 --> 00:05:22.700
So that's quite exciting. And I'm mostly working on maintaining LangGraph now.

00:05:22.940 --> 00:05:23.020
So

00:05:23.020 --> 00:05:23.540
many folks

00:05:23.540 --> 00:05:26.840
are probably familiar with LangChain, the open source package.

00:05:26.980 --> 00:05:28.640
We'll probably get into that a little bit more soon.

00:05:29.220 --> 00:05:32.580
But LangGraph is kind of our newer agent orchestration framework.

00:05:33.080 --> 00:05:47.160
Other than that, yeah, I love just engaging with folks in the open source community, getting excited about building cooler AI applications now. And I don't know, in my free time, I'm quite the podcast addict. So it's always nice to listen to your other podcasts.

00:05:47.580 --> 00:05:56.500
Oh, thank you. Well, I know you worked on some cool projects and this certainly is another one of them. I didn't realize they were based in Boston. That's cool.

00:05:56.720 --> 00:05:56.820
The

00:05:56.820 --> 00:06:07.880
main office is actually in San Francisco, but the company's growing pretty fast. So we now have the Boston office as of last week. So got my nice little call booth background here and then a New York office as well.

00:06:08.000 --> 00:06:12.460
Is it off that 128 highway there where a lot of the tech companies are? I'm

00:06:12.460 --> 00:06:16.240
actually not super familiar with San Francisco geography,

00:06:16.540 --> 00:06:19.180
but... No, no, I'm taking Boston up by like Woburn. Oh,

00:06:19.270 --> 00:06:21.840
oh, gotcha. Yeah, we're actually in Harvard Square. Okay.

00:06:22.300 --> 00:06:24.360
Oh, very cool. Yeah, proper Boston.

00:06:24.780 --> 00:06:24.880
Yes.

00:06:25.280 --> 00:07:11.340
Not Boston suburbs where a lot of those companies are out there. Yeah, really cool. So let's just start by talking about AI a little bit. I know some people are AI accelerators. People believe that AI should be just pushed as fast as it can be. And I suspect a lot of folks like that can be found on a playing chain. Also, people can be very hesitant to it, right? I know some people are like, I'm not using it. I won't use it. I'm not sending my data to the big cloud companies or whatever to train their models to take my job or however people perceive this. I'm certainly pretty, I see a ton of value in it. I think it's really quite incredible. But you know, first, what are your thoughts on using LLMs in general? And then two, what is an agent?

00:07:11.600 --> 00:07:42.420
I definitely fall more on the accelerator side of things, but with guardrails and constraints and rules and boundaries, which is sort of what we're trying to do with LandGraph, I think it can be really helpful to use LLMs as a development tool. So I personally do that when I'm writing code. I also think it's really helpful to brainstorm with the help of LLMs. And yeah, so I think they're increasingly valuable. And as long as we're being careful about usage, then it's great to kind of lean into their strengths.

00:07:42.760 --> 00:07:56.740
Yeah. Reuven Lerner, I just had him on the show a couple episodes ago, and he says he uses the reverse Socratic method with LLMs where he writes the code, then he asks it to, he sort of interrogates it like, well, what's wrong with my code?

00:07:56.880 --> 00:07:57.560
What could be better?

00:07:57.720 --> 00:07:58.540
When would I use it?

00:07:58.540 --> 00:07:59.480
And what am I overlooking?

00:08:00.099 --> 00:08:02.540
And it's, there's so many uses, it's wild.

00:08:02.740 --> 00:08:03.700
Yeah, it's crazy.

00:08:03.840 --> 00:08:08.800
I think sometimes I'll talk to my parents and be like, oh, I use ChatGPT to help me come up with a recipe.

00:08:08.880 --> 00:08:09.780
And they're like, you did what?

00:08:10.000 --> 00:08:11.220
Like, I didn't even know you could do that.

00:08:11.979 --> 00:08:15.800
Whereas folks at work are having, interacting with AI on the hour.

00:08:16.020 --> 00:08:54.140
So yeah, so I'll talk a little bit more about what are agents? Agents are a little bit hard to define. I think it's kind of an overloaded term, but many people are kind of leading into that as the future of AI from a developer perspective. So I think about agents as tools that can reason and make decisions for you, specifically LLMs that are powered with things like tool calling and memory and context and things like that. So like chat based model for an LLM to how can you use an LLM in an application.

00:08:54.520 --> 00:09:07.660
Okay. So it can call tools like you could give it a request and maybe it's going to go and call, do like web searches or read web pages or you can tell it, go read the docs and then do this thing or something along those lines.

00:09:07.960 --> 00:09:35.700
LLMs are trained on older data. And so if you're not enabling your LLMs to use kind of real world data and information, then their utility is limited. But agents have the power to use tools, which are basically like API calls, or it can even be more simple than that, like running some code in order to incorporate your like real time relevant data. And it can also kind of make decisions about like, which tools do I want to use and that sort of thing.

00:09:35.760 --> 00:09:52.360
So interesting. I guess it's related to deep research a little bit, right? Like a lot of the deep research thing seem to be kind of using tools and using modern web and other stuff that they can find rather than just whatever the model was when it was constructed? Definitely.

00:09:52.650 --> 00:10:10.760
I think about like perplexity is kind of gaining in popularity and what's really good at you can ask it a question and then it does real-time research against like looking things up on the internet and then provides you back with citations which is a really exciting feature I think for folks who are like LLMs can just make anything up.

00:10:10.840 --> 00:10:13.440
Like, how do I know this is being pulled from reasonable sources?

00:10:13.620 --> 00:10:15.800
And that's kind of an exciting jump in that space.

00:10:16.080 --> 00:10:16.420
It is.

00:10:16.600 --> 00:10:26.960
And I think it also goes a little ways, not that far, but a little ways to fill that gap where people said these search engines, they just take our data and they give us no credit.

00:10:27.480 --> 00:10:27.580
Right.

00:10:27.800 --> 00:10:27.920
Yeah.

00:10:28.080 --> 00:10:29.660
Which I'm sympathetic to.

00:10:30.060 --> 00:10:36.060
However, I do think it says, here's the four places I found and here's the video you should watch if you want to go deeper.

00:10:36.180 --> 00:10:37.780
I think that's actually super valuable.

00:10:38.340 --> 00:10:41.100
I just asked perplexity a historical question.

00:10:41.820 --> 00:10:46.780
Like, what is the age of these people who did this thing hundreds of years ago?

00:10:47.160 --> 00:10:51.620
And it said in their work, and it wrote Python code, and then it gave me the answer.

00:10:51.800 --> 00:10:58.120
I think it was doing pandas type stuff, but it was sitting there thinking, thinking, and it shows you what it's doing.

00:10:58.180 --> 00:11:00.960
And it just says, writing Python, and then boom, out comes the answer.

00:11:01.040 --> 00:11:02.020
I'm like, okay, that's weird.

00:11:02.220 --> 00:11:02.660
Oh, man.

00:11:02.920 --> 00:11:03.300
Was it correct?

00:11:03.680 --> 00:11:04.660
Yeah, it was correct, I

00:11:04.660 --> 00:11:04.760
think.

00:11:04.800 --> 00:11:05.000
Wow.

00:11:05.760 --> 00:11:06.040
Wild.

00:11:06.500 --> 00:11:11.200
But that's kind of the stuff we're talking about a little bit, right, in this agentic sense.

00:11:11.270 --> 00:11:13.060
It's not just like, well, I went to my database.

00:11:13.230 --> 00:11:14.200
These are answers I have.

00:11:14.230 --> 00:11:20.480
But it's like, well, if I can make this web query and I could write some pandas, then I could format it the way you asked for it, right?

00:11:20.740 --> 00:11:23.820
Yeah, there's definitely a lot more kind of reasoning and thinking.

00:11:24.420 --> 00:11:27.680
It is a bit of a black box right behind, like, what is the LLM doing?

00:11:27.730 --> 00:11:31.980
And so you've got a bit of transparency there with the, like, writing Python code clue.

00:11:32.110 --> 00:11:33.140
I wish there was a view source.

00:11:33.600 --> 00:11:34.140
You know what I mean?

00:11:34.320 --> 00:11:34.440
One

00:11:34.440 --> 00:12:00.620
of the things that we're trying to do at LinkChain right now with LandGraph development is to kind of bridge the gap between like, what are the parts of your system that you really need to like understand and kind of be guaranteed are reliable and will always run in the same way versus like, where can you afford to introduce non-determinism in favor of like a bit more maybe agency or utility, but also introducing risk like, who knows what your LLM was doing in the research phase.

00:12:00.840 --> 00:12:00.920
It's

00:12:00.920 --> 00:12:44.820
just so powerful, but it's also so, scary as a developer to think, I can't debug this. I can't prove that it's going to do the right thing all the time. Right. And I guess it really depends, right? Like Siri, for example, on the iPhone, sorry if that made anybody's phones go off. It's so bad. It's just so weirdly not capable of doing stuff. And you know that if that were backed by an LLM instead, sure, it might make mistakes periodically, but as a net, it would be so much better that it's probably worth it. You know what I mean, on the other hand, you wouldn't want to say, I'm going to create an agentic workflow to determine if you get a mortgage or things like that would be a little bit too much, I think, right?

00:12:44.900 --> 00:12:47.400
It would deny people for weird, random reasons.

00:12:47.940 --> 00:12:48.080
Right.

00:12:48.320 --> 00:12:55.060
You definitely have to consider the stakes and risk and value of a decision that you're making, right?

00:12:55.220 --> 00:13:01.180
And like, I think generally I wouldn't empower LLMs to make like the most important decisions in my life on a day-to-day basis.

00:13:01.800 --> 00:13:02.180
Like

00:13:02.180 --> 00:13:08.260
you said, I do find myself like laughing at series responses more than actually like valuing the response.

00:13:08.920 --> 00:13:09.660
It's incredible.

00:13:09.820 --> 00:13:13.760
It's setting timers and telling you what the temperature is.

00:13:13.840 --> 00:13:16.320
But boy, you're very off very much.

00:13:16.480 --> 00:13:17.480
No, no, it doesn't.

00:13:17.490 --> 00:13:18.380
It just doesn't do it.

00:13:18.840 --> 00:13:19.040
Okay.

00:13:19.540 --> 00:13:24.140
So when would you use, I mean, I talked about examples you wouldn't use.

00:13:24.200 --> 00:13:27.020
Give us examples of when you might really embrace these things.

00:13:27.240 --> 00:13:27.480
We've

00:13:27.480 --> 00:13:30.100
talked about the non-determinism of LLM behavior, right?

00:13:30.420 --> 00:13:34.020
But also kind of the power of an agent is in making decisions for you.

00:13:34.200 --> 00:13:45.640
And if you can give your large language model the appropriate context and descriptions of tools so that it can do a good job making decisions for you, then that can save you a lot of time.

00:13:46.020 --> 00:13:51.740
So one classic example I'll probably touch on a couple times today is you might build an email agent.

00:13:52.280 --> 00:13:56.500
And so the value of that email agent, right, is that it can categorize emails for you.

00:13:56.920 --> 00:14:18.960
Maybe some things are spam, some things need immediate response, some things can even be automatically responded to, and then said agent could learn from your preferences over time. So maybe before an automatic reply is sent out, you, the user, get to approve that reply, but you didn't have to do all the work of drafting and that sort of thing.

00:14:19.280 --> 00:14:44.200
And so I think the greatest value that I see in developing workflows with agents is if you can come up with systems that we call like ambient workflows, basically, instead of responding to some user prompt, like, hey, what's the weather? They're running to in response to external triggers. So in this example, like the email that you received. And so basically, you're trying to create a system that does work and automates work for you.

00:14:44.380 --> 00:14:59.740
Yeah, that's cool. It could determine if it's, is this an emergency? Is this something, a tech support type thing? Is it an accounting thing? And then it could just maybe do a little bit of work through your docs, give a response and then forward it to the right person or something like that, right?

00:14:59.780 --> 00:15:00.080
I

00:15:00.080 --> 00:15:05.640
think we think about like chatbots on like websites that help users as another common example.

00:15:06.180 --> 00:15:15.740
And when you're talking with a chatbot, at what point does it decide like, you know what, we need to escalate this and like actually provide them with a phone number to call with a real person or that sort of thing.

00:15:15.860 --> 00:15:15.980
I

00:15:15.980 --> 00:15:22.900
definitely see this as being one of the more powerful, more common ways that allow them to actually interact with people.

00:15:23.340 --> 00:15:25.340
It's kind of weird that it's a chat.

00:15:25.740 --> 00:15:26.080
You know what I mean?

00:15:26.240 --> 00:15:35.020
It is pretty useful, but it's also just weird that it's like, okay, we're going to have this chat with this thing, whereas it could do so much more.

00:15:35.360 --> 00:15:38.200
Yeah. You mean with the chatbot example?

00:15:38.460 --> 00:15:38.620
Yeah,

00:15:38.760 --> 00:15:42.740
exactly. Or even ChatGPT itself or something like that, right?

00:15:42.920 --> 00:15:54.840
Yeah. I think that's kind of why we're so excited about the ambient or background system pattern is a chat is super helpful if I'm like, I have these five ingredients in my fridge. What should I make for dinner?

00:15:55.360 --> 00:15:57.180
That's like a very simplistic pattern.

00:15:57.640 --> 00:16:02.380
And we think you should kind of build a system that is only as simple as you need.

00:16:02.620 --> 00:16:08.160
But then I think the real value comes when you're responding to those external like real-time triggers.

00:16:08.540 --> 00:16:11.400
Can one of them be like a cron job?

00:16:11.640 --> 00:16:18.960
Like every hour, check this stock price or every hour, evaluate the sentiment of this community and then do something.

00:16:19.220 --> 00:16:19.640
Yeah, definitely.

00:16:20.220 --> 00:16:27.300
I think as we get more into discussing how you can build workflows with LandGraph, it'll become pretty evident how you can schedule things.

00:16:27.400 --> 00:16:29.580
And that's a very predictable pattern, right?

00:16:29.940 --> 00:16:30.060
Maybe

00:16:30.060 --> 00:16:33.300
as useful for you, I can imagine for the case where you're running something on a schedule.

00:16:33.680 --> 00:16:36.120
The end result might be like, okay, you've analyzed sentiment.

00:16:36.480 --> 00:16:40.320
Do I need to send an alert to my user or things like that?

00:16:41.600 --> 00:16:44.380
This portion of Talk Python To Me is brought to you by the folks at Posit.

00:16:44.800 --> 00:16:47.700
Posit has made a huge investment in the Python community lately.

00:16:48.360 --> 00:16:53.500
Known originally for RStudio, they've been building out a suite of tools and services for Team Python.

00:16:54.220 --> 00:16:57.480
Over the past few years, we've all learned some pretty scary terms.

00:16:58.380 --> 00:17:02.420
Hypersquatting, supply chain attack, obfuscated code, and more.

00:17:03.040 --> 00:17:13.280
These all orbit around the idea that when you install Python packages, you're effectively running arbitrary code off the internet on your dev machine, and usually even on your servers.

00:17:14.040 --> 00:17:15.260
But thought alone makes me shudder.

00:17:15.660 --> 00:17:19.439
And this doesn't even touch the reproducibility issues surrounding external packages.

00:17:20.240 --> 00:17:21.459
But there are tools to help.

00:17:22.000 --> 00:17:24.900
Posit Package Manager can solve both problems for you.

00:17:25.760 --> 00:17:29.020
Think of Posit Package Manager as your personal package concierge.

00:17:29.260 --> 00:17:33.760
You use it to build your own package repositories within your firewall that keep your project safe.

00:17:33.920 --> 00:17:39.020
You can upload your own internal packages to share or import packages directly from PyPI.

00:17:39.680 --> 00:17:45.040
Your team members can install from these repos in normal ways using tools like pip, Poetry, and uv.

00:17:45.700 --> 00:18:19.020
Posit Package Manager can help you manage updates ensuring you're using the latest, most secure versions of your packages but it also takes point-in-time snapshots of your repos which you can use to rerun your code reproducibly in the future Posit Package Manager reports on packages with known CVEs and other vulnerabilities so you can keep ahead of threats and if you need the highest level of security you can even run Posit Package Manager in air-gapped environments If you work on our data science team where security matters, you owe it to you and your org to check out Posit Package Manager.

00:18:19.780 --> 00:18:25.480
Visit talkpython.fm/ppm today and get a three-month free trial to see if it's a good fit.

00:18:25.720 --> 00:18:27.900
That's talkpython.fm/ppm.

00:18:28.440 --> 00:18:30.160
The link is in your podcast player's show notes.

00:18:30.620 --> 00:18:32.160
Thank you to Posit for supporting the show.

00:18:33.480 --> 00:18:36.660
Well, let's talk a bit about, I guess, Langchain first.

00:18:37.500 --> 00:18:39.380
So there's a lot of things going on here, right?

00:18:39.460 --> 00:18:41.000
There's Langchain, the company.

00:18:41.820 --> 00:18:44.340
There's LangChain, the open source project.

00:18:45.040 --> 00:18:46.200
There's LangGraph.

00:18:46.660 --> 00:18:47.480
Tell us about all these things.

00:18:47.700 --> 00:18:49.220
Yeah, so happy to give kind of an overview.

00:18:49.560 --> 00:19:04.020
So LangChain, the open source tool, is basically a tool for developers to use LLMs and really increase the value of LLMs or kind of augment the power of LLMs with your data.

00:19:04.340 --> 00:19:09.600
And so some of the things that we've built in LangChain include abstractions around chat models.

00:19:10.120 --> 00:19:22.860
So you can decide you're going to use the 4.1 model and you want to use some different settings, like maybe set the temperature to zero or only send some maximum number of tokens or things like that.

00:19:23.090 --> 00:19:29.800
And then specifically, a lot of the value from LinkChain, the open source tool, comes from introducing this model agnostic interface.

00:19:30.470 --> 00:19:39.620
So if I want to chain a bunch of model responses together, it's kind of hard to do that if I am separately querying the OpenAI API.

00:19:40.060 --> 00:19:43.500
and then the Anthropic API, and then trying to work with Gemini as well.

00:19:43.540 --> 00:19:46.760
There are kind of different response formats and features, et cetera.

00:19:47.060 --> 00:19:53.680
And so Langchain makes it really easy for developers to use all of those different model providers in a single application.

00:19:53.940 --> 00:19:54.440
That's interesting.

00:19:54.820 --> 00:19:59.360
So if I wanted, I could bring my own LLM?

00:19:59.680 --> 00:20:00.140
Is that possible?

00:20:00.360 --> 00:20:00.660
Yes.

00:20:01.000 --> 00:20:06.460
Right, like I could use Olama or LM Studio or one of those things rather than the full cloud version?

00:20:06.640 --> 00:20:06.800
Definitely,

00:20:07.040 --> 00:20:10.280
which is nice just from an extensibility point of view.

00:20:10.440 --> 00:20:16.680
And then if you want to use that LLM and then start to use some other more commercially available ones as well, you can combine those.

00:20:17.220 --> 00:20:21.240
And so the name Langchain comes together when we think about chains of systems.

00:20:21.500 --> 00:20:24.100
One common example that we use is RAG, right?

00:20:24.300 --> 00:20:34.800
So you have your vector store, some user asks a question and you fetch the relevant data from the vector store and then you kind of feed that into the LLM.

00:20:35.140 --> 00:20:38.700
And so that was a very common pattern like folks, LangChain users would implement.

00:20:39.080 --> 00:20:39.600
Yeah, very cool.

00:20:40.020 --> 00:20:41.020
And then LangGraph.

00:20:41.880 --> 00:20:47.000
LangGraph is more about building workflows out of this, right?

00:20:47.220 --> 00:20:52.740
LangGraph is kind of the evolution of LangChain and it has two main things that we focus on.

00:20:52.820 --> 00:20:56.480
So it's got a low-level system for building graphs.

00:20:56.780 --> 00:20:58.980
So you can think of that, the very like computer science.

00:20:59.660 --> 00:21:02.400
Folks will be excited to hear the terms like nodes and edges, right?

00:21:02.480 --> 00:21:04.660
You're building this low-level system.

00:21:05.300 --> 00:21:10.000
You can control the flow of data or the flow of your application.

00:21:10.480 --> 00:21:11.640
You can have conditional branches.

00:21:11.820 --> 00:21:15.900
You can run nodes in parallel, all of those kind of exciting graph features.

00:21:16.500 --> 00:21:23.860
And then we also have, and you can see here, what we call pre-builds, which help you build agents on top of that graph framework.

00:21:24.380 --> 00:21:34.240
So the promise of LaneGraph is that it's extensible because we have those low-level building blocks, but then also very easy to get started with an agent

00:21:34.240 --> 00:21:34.800
application.

00:21:35.100 --> 00:21:41.480
Yeah, like one of the examples you have, the GitHub page is create React agent from the prebuilt graphs.

00:21:41.940 --> 00:21:47.200
Is that React as in JavaScript's React framework or just a reactive thing?

00:21:47.380 --> 00:21:47.740
Good question.

00:21:47.880 --> 00:21:52.220
So that's React is kind of a common term in the agent building space.

00:21:52.240 --> 00:21:54.980
So it's reasoning and action agent.

00:21:55.320 --> 00:21:55.620
I see.

00:21:55.980 --> 00:21:57.720
So it has nothing to do with JavaScript necessarily.

00:21:58.180 --> 00:21:58.380
No,

00:21:58.380 --> 00:21:58.800
it does not.

00:21:58.980 --> 00:22:05.040
We actually will be probably changing this name soon to just create agent to be a little bit more clear for folks.

00:22:05.300 --> 00:22:05.980
Yeah, super cool.

00:22:06.220 --> 00:22:11.720
So I guess give us a sense of maybe, well, I want to, let me circle back to some stuff you talked about that I want to talk about.

00:22:11.820 --> 00:22:14.800
Why not just call OpenAI or something like that?

00:22:14.800 --> 00:22:15.660
It's API directly.

00:22:16.340 --> 00:22:20.700
But you mentioned tokens and, oh gosh, what was the other thing?

00:22:20.820 --> 00:22:21.840
I think I mentioned temperature.

00:22:22.080 --> 00:22:22.500
Temperature,

00:22:22.620 --> 00:22:23.240
that was the one.

00:22:23.380 --> 00:22:23.840
Yes, thank you.

00:22:24.340 --> 00:22:24.680
Temperature.

00:22:24.890 --> 00:22:30.980
So LLMs can have a certain amount of creativity is maybe the right word, but not really.

00:22:31.840 --> 00:22:57.320
a certain amount of flexibility in, I guess, almost randomness in the amount of the way that it works its way through finding the answers. And the higher the temperature, the more you let it be creative and wonder, whereas the lower the temperature, you're like, no, just give me the closest answer with the least amount of variability. I imagine that that is really an important consideration in building these types of things because it probably significantly influences how deterministic a thing is.

00:22:57.420 --> 00:23:29.380
Yeah, I think a great way to think about it is if your temperature is low, let's say we're going on a scale of like zero to one. So if your temperature is like zero to 0.3, it's more deterministic. We can't say like guaranteed to be fully deterministic. But I think when you're building a system that's going to be deployed for users or basically a production ready system, you're certainly going to lean on the side of lower temperature. But, you know, for more creative applications, it's nice to have that setting be pretty configurable just so you can really see what kind of ideas your LLM can help you with.

00:23:29.480 --> 00:23:29.840
Right, right.

00:23:30.000 --> 00:23:34.480
I want to, you helped me think creatively about doing this thing and what are my options, right?

00:23:34.600 --> 00:23:41.720
Like that kind of thing, you probably don't want to set temperature to zero, but when you're building workflows, I imagine it's usually more tending lower.

00:23:42.140 --> 00:23:42.280
Definitely.

00:23:42.760 --> 00:23:56.420
Sort of a like silly example of maybe when you would set the temperature to higher is I was doing some shopping this weekend and stumbled upon a listing on Facebook Marketplace and the listing had a riddle as the description.

00:23:56.580 --> 00:24:00.420
And it was like, if you solve this riddle, you can have this bedside table for free.

00:24:00.860 --> 00:24:05.680
And so plugging the riddle into ChatGPT with a higher temperature might be more valuable there.

00:24:05.920 --> 00:24:08.940
Obviously more of a like toy example, but kind of shows the utility there.

00:24:09.040 --> 00:24:09.120
I've

00:24:09.120 --> 00:24:10.800
been doing a bunch of stuff on Facebook Marketplace.

00:24:10.940 --> 00:24:12.560
That is in a crazy place.

00:24:13.500 --> 00:24:14.520
It's the new Craigslist.

00:24:14.700 --> 00:24:15.160
It's really weird.

00:24:15.420 --> 00:24:15.520
Indeed.

00:24:15.720 --> 00:24:17.080
Yeah, that's a wild, wild example.

00:24:17.580 --> 00:24:22.180
And the other one that I think is probably important to define would be just tokens.

00:24:22.360 --> 00:24:27.480
because a lot of the models, especially the local models, I don't want to send my data off.

00:24:27.480 --> 00:24:28.460
I want to run OLAMA.

00:24:28.510 --> 00:24:29.840
I want to run LMStudio.

00:24:30.300 --> 00:24:32.280
Those are quite limited, right?

00:24:32.440 --> 00:24:36.600
Like 8,000 tokens versus half a million for some of the cloud models, right?

00:24:36.860 --> 00:24:39.640
So that can be really a significant consideration as well.

00:24:39.980 --> 00:24:40.260
What are they?

00:24:41.000 --> 00:24:43.240
They're not exactly words, but they're kind of words, but they're not words.

00:24:43.380 --> 00:24:49.060
It's basically just a way to quantify the amount of data that you're sending to your LLM.

00:24:49.440 --> 00:24:56.380
And so they also can be used to quantify the amount of data or text that your LLM is sending out as well.

00:24:56.680 --> 00:24:59.300
So you'll have like input tokens and output tokens.

00:25:00.120 --> 00:25:02.560
Generally, you can kind of equate them to characters.

00:25:02.900 --> 00:25:08.420
I think there's a little bit of like nuance there, but I think the simple explanation, that's good enough.

00:25:08.880 --> 00:25:17.520
And the value of such a wide, what we call like context window, is that the more context and direction your LLM has, the better it's going to perform.

00:25:18.040 --> 00:25:26.880
And so if you have those bigger windows, you can write more verbose instructions and, yeah, just write more clear guidelines for your LLM to follow.

00:25:27.270 --> 00:25:36.460
You can also put more relevant context for if your, you know, system is answering questions, maybe you need to, like, yeah, provide just relevant context there.

00:25:36.840 --> 00:25:42.200
And we'll talk a little bit more down the line about the value of, like, memory and long-term storage in agentic applications.

00:25:42.980 --> 00:25:47.920
And so we'll often like feed back in some of the important things logged in memory.

00:25:47.980 --> 00:25:51.680
And so if you have that bigger context window, you can send in more of that data as well.

00:25:51.900 --> 00:25:57.640
So that might be like user preferences or previous responses or previous messages, that sort of thing.

00:25:57.740 --> 00:26:11.820
Yeah, and if you run out of that space, then it's kind of as if the first part of the conversation, which might include important instructions like we are working on this project, here is our goal, would just kind of either drop off or it would be a runtime error or something.

00:26:12.080 --> 00:26:13.660
So yeah, it super matters.

00:26:14.000 --> 00:26:18.920
I guess one other important note around tokens is that's often how you're charged for LLM use.

00:26:19.220 --> 00:26:20.260
Like if you are using more.

00:26:20.340 --> 00:26:21.000
Yes, exactly.

00:26:21.540 --> 00:26:21.640
More

00:26:21.640 --> 00:26:24.660
tokens on input and more tokens on output is higher cost.

00:26:24.860 --> 00:26:28.700
But that higher cost is often worth the significant performance improvement.

00:26:29.020 --> 00:26:32.220
Yeah, so that's for the cloud usage-based ones.

00:26:32.620 --> 00:26:34.240
Certainly that's what you get charged.

00:26:34.660 --> 00:26:36.360
And that's some of the appeal of the local ones.

00:26:36.480 --> 00:26:40.140
But the local ones, they can't answer the questions as well.

00:26:40.140 --> 00:26:41.180
They don't have as much data.

00:26:41.800 --> 00:26:44.900
Are there local models that do agentic stuff?

00:26:45.010 --> 00:26:52.280
Or would I do something like lane graph talking to, I don't know, Mistral or something running, like a 7 billion parameter Mistral running locally?

00:26:52.760 --> 00:26:58.400
Would I use lane graph to get that agentic behavior and things like that?

00:26:58.560 --> 00:27:04.940
One of the main features that we think about when we think about agentic behavior is that like tool calling ability.

00:27:05.560 --> 00:27:10.900
And so I think if a model doesn't have support for that, then its utility is more limited.

00:27:11.180 --> 00:27:20.460
But that being said, as long as the model can reason and provide guidance, then you can certainly kind of build an agentic application around it with LaneGraph.

00:27:20.680 --> 00:27:21.480
Yeah, very neat.

00:27:21.860 --> 00:27:29.760
All right, so my other question before we got distracted on this terminology thing, it was, why not just call the LLM directly?

00:27:31.360 --> 00:27:32.120
What's wrong with that?

00:27:32.440 --> 00:27:35.280
Which I can think of a couple of things, but, you know, lay it out for us.

00:27:35.440 --> 00:27:41.320
The most important thing to think about is controllability and reliability.

00:27:41.940 --> 00:27:47.320
So one of the main kind of hallmark features of LaneGraph is this human-in-the-loop support.

00:27:47.960 --> 00:27:52.540
So you can send a request to your LLM, and then it's going to return a response.

00:27:52.740 --> 00:28:04.140
And maybe before you do anything with that response, you probably want to check that maybe the tool call that it made is okay, or whatever expensive or risky operation that is about to be run.

00:28:04.360 --> 00:28:07.900
you want to confirm that or maybe edit the parameters or that sort of thing.

00:28:08.200 --> 00:28:13.180
And so I think for very simple chat-based applications, just calling the LLM is okay.

00:28:13.520 --> 00:28:19.200
You can use LangChain to specify some of those model setting parameters easily or use different LLMs.

00:28:19.660 --> 00:28:28.560
But anytime you want to build something with just more reliability, I think you definitely want more of a framework around it that provides those kind of guardrails.

00:28:28.760 --> 00:28:40.620
There's also things like persistence and durability, which if you've had a long, drawn-out conversation with these APIs, that can be a big deal, right?

00:28:40.820 --> 00:28:45.760
One of the nice things about Lingrath is that we offer kind of short-term and long-term persistence.

00:28:45.980 --> 00:28:55.140
So you can have persistence across just a conversation, so like kind of one thread of discussion or one application run, and then also across runs.

00:28:55.740 --> 00:28:56.200
So going back

00:28:56.200 --> 00:28:57.200
to the assistant

00:28:57.200 --> 00:29:08.300
example, you probably want your email assistant to learn from your long-term preferences, rather than just your like single human in the loop edit on one email that came in, for example.

00:29:08.480 --> 00:29:15.720
You might say, I know you flagged this to be forward over here, but when it looks like this, I need you to take that into account and it means something different.

00:29:16.070 --> 00:29:17.140
You want it to know that forever.

00:29:17.440 --> 00:29:17.600
Exactly.

00:29:17.940 --> 00:29:18.020
At

00:29:18.020 --> 00:29:19.700
the same time, people get freaked out.

00:29:19.810 --> 00:29:27.140
I think ChatGPT started identifying your name and addressing you by name without you saying what your name is, and it freaked people out.

00:29:27.300 --> 00:29:30.200
It's like, well, you've been giving it your secrets for six months.

00:29:30.720 --> 00:29:39.660
I think my equivalent of being a little freaked out by ChatGPT was I was asking it for some like birthday gift ideas for my partner.

00:29:39.770 --> 00:29:47.920
And it was like, oh, I think based on your previous commentary about your hobbies and things you like to do with your partner, then like, here's some ideas.

00:29:48.090 --> 00:29:48.840
And I was like, wait a minute.

00:29:48.890 --> 00:29:49.760
I didn't tell you that today.

00:29:50.240 --> 00:29:53.820
Like, I didn't mention that I like to run and go on hikes, but you know that.

00:29:53.940 --> 00:29:54.560
It's spionomic.

00:29:54.780 --> 00:29:56.820
No, wait, I told it this over and over and over.

00:29:56.940 --> 00:29:58.200
But yeah, it's very interesting.

00:29:58.390 --> 00:29:58.520
Yeah.

00:29:58.640 --> 00:29:59.140
Mostly useful.

00:29:59.640 --> 00:30:00.000
Somewhat weird.

00:30:00.600 --> 00:30:06.620
All right, let's maybe talk through, to give people a sense how this might work, let's maybe just talk through like a quick start example.

00:30:06.730 --> 00:30:12.260
I know talking about code is a bit troublesome on audio formats, but we'll just keep it like high level.

00:30:12.470 --> 00:30:15.360
But give us a sense of what is it like to write some code here.

00:30:15.460 --> 00:30:17.680
Maybe we could just talk through the quick start for line graph.

00:30:17.900 --> 00:30:18.040
Our

00:30:18.040 --> 00:30:21.800
quick start is now kind of focused around creating a simple agent.

00:30:22.480 --> 00:30:29.040
So you can use that create React agent method that we talked about to create a weather agent.

00:30:29.540 --> 00:30:33.380
And so what that looks like is you call the function with your preferred model.

00:30:33.650 --> 00:30:34.440
So in this case, we're

00:30:34.440 --> 00:30:34.700
using

00:30:34.700 --> 00:30:36.240
Cloud 3.7.

00:30:36.590 --> 00:30:43.320
And then we give it a get weather tool or kind of, yeah, it just calls a weather API, maybe with like city and state.

00:30:43.990 --> 00:30:46.500
And then you can provide it with a system prompt as well.

00:30:46.720 --> 00:30:55.200
So we give it some information about you're a helpful assistant that specializes in weather forecasting and you have access to this get weather tool.

00:30:55.420 --> 00:30:58.960
And we're just trying to provide it with helpful context to get it started.

00:30:59.280 --> 00:31:09.280
How much do you think that matters that you kind of set the stage, like you're a helpful weather assistant or you're an expert in this field before I ask you questions?

00:31:09.620 --> 00:31:09.760
For

00:31:09.760 --> 00:31:18.860
a really simple application, like just providing one weather tool that has a great description, I think it maybe doesn't matter quite as much.

00:31:19.400 --> 00:31:29.960
But I think as you start to build out those like customer facing chat bots or the email agent that we've been discussing, I think the prompt and guidance becomes really important.

00:31:30.280 --> 00:31:38.560
And I think updating that with those memory stores like we've talked about helps your system be the most useful that it could be for you.

00:31:38.640 --> 00:31:39.140
Yeah, that's true.

00:31:39.220 --> 00:31:45.800
Because the long term persistence probably serves a little bit of that role of like setting the context more deeply.

00:31:47.300 --> 00:31:50.360
This portion of Talk Python To Me is brought to you by Auth0.

00:31:50.900 --> 00:31:51.920
Do you struggle with authentication?

00:31:53.040 --> 00:31:59.460
Sure, you can start with usernames and passwords, but what about single sign-on, social auth, integration with AI agents?

00:32:00.080 --> 00:32:05.400
It can quickly become a major time sink, and rarely is authentication your core business.

00:32:06.060 --> 00:32:10.640
It's just table stakes that you've got to get right before you can move on to building your actual product.

00:32:11.240 --> 00:32:13.080
That's why you should consider Auth0.

00:32:13.580 --> 00:32:17.900
Auth0 is an easy-to-implement, adaptable authentication and authorization platform.

00:32:18.760 --> 00:32:25.120
Think easy user logins, social sign-on, multi-factor authentication, and robust role-based access control.

00:32:25.740 --> 00:32:32.160
With over 30 different SDKs and quick starts, Auth0 scales with your product at every stage.

00:32:32.860 --> 00:32:37.960
Auth0 lets you implement secure authentication and authorization for your preferred deployment environment.

00:32:38.400 --> 00:32:46.360
You can use all of your favorite tools and frameworks, whether it's Flask, Django, FastAPI, or something else, to manage user logins, roles, and permissions.

00:32:47.100 --> 00:32:52.020
Leave authentication to Auth0 so that you can start focusing on the features your users will love.

00:32:52.860 --> 00:32:57.320
Auth0's latest innovation, Auth4GenAI, which is now available in developer preview.

00:32:57.800 --> 00:33:08.680
Secure your agentic apps and integrate with the GenAI ecosystem using features like user authentication for AI agents, token vault, async authorization, and FGA for RAG.

00:33:09.060 --> 00:33:22.460
So if you're a Python developer or data scientist looking for an easy and powerful way to secure your applications, get started now with up to 25,000 monthly active users for free at talkpython.fm/auth0.

00:33:23.080 --> 00:33:25.260
That's talkpython.fm/auth0.

00:33:25.350 --> 00:33:27.100
The link is in your podcast player's show notes.

00:33:27.780 --> 00:33:29.760
Thank you to Auth0 for supporting the show.

00:33:30.900 --> 00:33:32.860
Just to give kind of an abbreviated example here.

00:33:33.260 --> 00:34:17.280
let's say my email assistant drafts me email response, and then I edit that and I make it more concise and maybe a little bit more friendly or something like that. My agent, I can build in logic so that it updates the long-term memory store with that kind of guidance. And then the next time that the agent is called with an email stimulus, the prompt now has like, keep things concise and a bit more friendly. And I didn't necessarily tell it that I wanted it to be shorter and more friendly. I just made that relevant edit and then it summarized that into the prompt. So all that being said, I think my analysis here would be that context and guidance is really, really important. For this toy example, I think it would be okay without.

00:34:17.480 --> 00:34:17.560
I

00:34:17.560 --> 00:34:34.200
do think one of the things that people, I would like to hear your thoughts on this as well and the guidance that you all give. But one of the things I think people who are new to LLMs I think they just, they kind of assume that it has too much knowledge or too much context.

00:34:34.520 --> 00:34:39.820
And you'll see like a one sentence, please analyze this document for me.

00:34:40.480 --> 00:34:47.960
Whereas if you gave it a two page write-up of the request you want, you might get dramatically better results.

00:34:48.440 --> 00:34:53.300
And I also think that that's where a lot of people say, oh, LLMs make mistakes and they do all these crazy things.

00:34:53.840 --> 00:34:55.679
It's like, well, you gave it so little information.

00:34:56.200 --> 00:34:57.360
What's it supposed to go on?

00:34:57.420 --> 00:35:06.740
And I guess in general, my comment is, I think people give way too short of prompts and background information to these things.

00:35:07.220 --> 00:35:07.760
What do you guys think?

00:35:07.860 --> 00:35:08.880
That's definitely true.

00:35:09.240 --> 00:35:11.640
I'll send a link here to OpenAI.

00:35:11.680 --> 00:35:14.620
I just released a GPT 4.1 prompting guide.

00:35:14.880 --> 00:35:20.680
And if you look at any of the guide, you'll see these huge context blocks.

00:35:21.340 --> 00:35:26.780
And so obviously, these guys really know how these models work and are going to give us the best guidance.

00:35:27.040 --> 00:35:28.880
Yeah, let me just give people listening a sense.

00:35:28.880 --> 00:35:32.740
It says, a sample prompt for a software engineering benchmark verified.

00:35:33.200 --> 00:35:35.500
So here's a prompt you might get.

00:35:35.800 --> 00:35:38.860
And it says, you'll be tasked to fix an issue in an open source repository.

00:35:39.720 --> 00:35:48.380
And then it just, it doesn't even word wrap, but it still goes on both horizontally and vertically, hundreds of lines.

00:35:48.840 --> 00:36:04.520
Another thing that I think is super interesting is the use of markdown when you talk to these things because it can it can convey a little bit of like formatting like oh this is a link or this is bolded so it means something different than if it's not bolded

00:36:04.520 --> 00:36:24.160
it's really interesting kind of trying to play the game of how can i optimize my context so that i'm optimizing the behavior of the llm i think we talk a lot in when we're talking about link graph about like the structure of your application but then really all that boils down to is like getting your getting your context right so that your agent can do the right thing.

00:36:24.300 --> 00:36:25.280
Yeah, well, thanks for the diversion.

00:36:25.460 --> 00:36:35.700
I just, I do think to be successful with this stuff, you've really got to have your, you're talking to LLM's game really strong because that's the atomic units of this, right?

00:36:35.860 --> 00:36:36.040
Definitely.

00:36:36.320 --> 00:36:36.440
Okay,

00:36:36.500 --> 00:36:37.160
back to the quick start.

00:36:37.500 --> 00:36:37.960
Give it a prompt.

00:36:38.680 --> 00:36:39.500
Maybe a long, good one.

00:36:39.620 --> 00:36:43.080
If you go on our docs, there should be like an agent section.

00:36:43.380 --> 00:36:50.500
And I think there's, and yeah, so if you go to lane graph and then up at the, let's see, oh yeah, the agents tab on the top.

00:36:50.680 --> 00:36:50.940
That do?

00:36:51.060 --> 00:36:51.160
like

00:36:51.160 --> 00:36:52.620
the top nav if you go to agents.

00:36:52.900 --> 00:36:52.960
Yeah.

00:36:53.320 --> 00:36:58.640
So we should have a like getting started with agents section that's maybe a little bit more relevant for users.

00:36:59.240 --> 00:37:05.780
So what that would look like, you have your create react agent and then you can invoke it with a set of messages.

00:37:05.880 --> 00:37:08.300
So maybe I want to say, what's the weather like in San Francisco?

00:37:08.920 --> 00:37:16.300
And then it returns a response and then you can invoke it again and say like, what about New York or other questions like that?

00:37:16.420 --> 00:37:18.320
And that's kind of the like out of the box.

00:37:18.620 --> 00:37:21.200
How do you build just 10 lines of code?

00:37:21.360 --> 00:37:25.180
You've got an agent working for you with context and tools, et cetera.

00:37:25.420 --> 00:37:25.740
Super neat.

00:37:25.910 --> 00:37:31.160
So when is your new Getting Started tutorial walkthrough thing coming out?

00:37:31.260 --> 00:37:34.600
We have it coming out, I believe, at the end of the week.

00:37:34.980 --> 00:37:36.940
We have our interrupt conference next week.

00:37:37.140 --> 00:37:39.800
So we want to have our docs kind of revamped for that.

00:37:40.080 --> 00:37:40.300
Interrupt.

00:37:40.430 --> 00:37:40.540
Okay.

00:37:40.650 --> 00:37:45.440
Is that like, because that's the call for human in the loop callback effectively, right?

00:37:45.560 --> 00:37:47.380
Is that the origin of that name?

00:37:47.560 --> 00:37:48.160
Yeah, it is.

00:37:48.300 --> 00:37:51.600
I think we're like really emphasizing kind of the value of that.

00:37:51.910 --> 00:37:54.040
I just sent you the link as well for that.

00:37:54.570 --> 00:37:55.820
It should be viewable publicly.

00:37:56.360 --> 00:37:56.760
It's kind of our

00:37:56.760 --> 00:37:57.680
old agent's guide.

00:37:57.860 --> 00:37:58.280
Happy

00:37:58.280 --> 00:38:00.760
to talk more about human in the loop, if that's helpful.

00:38:00.980 --> 00:38:04.200
Yeah, yeah, let's talk about that because that sounds pretty interesting, right?

00:38:05.020 --> 00:38:10.540
We've got chat, which is obviously human in the loop, although when you press deep research, less in the loop.

00:38:10.870 --> 00:38:16.640
Then you've got these Python programs, which maybe just completely run by themselves.

00:38:17.100 --> 00:38:19.440
But yeah, what is this human in the loop and how does that work?

00:38:19.700 --> 00:38:30.680
The human in the loop, we are trying to basically add human approval or edits for cases that are basically sensitive decisions that are being made.

00:38:31.140 --> 00:38:40.100
So very simple example, let's say you have a hotel booking agent and you get a request in that's like, I'd like to book a stay at whatever Hilton in Boston.

00:38:40.740 --> 00:39:11.000
You might want a user to just kind of approve that sensitive financial, like significant financial burden before the booking actually goes through. And so we make it really easy with this interrupt function anywhere in a tool or node to say interrupt. I want to get human input here that can review tool calls, validate outputs from an LLM, or like augment existing data or context so that you can make sure your LLM is doing the right thing.

00:39:11.080 --> 00:39:29.920
I see. And it's like, well, we found these things, what would you like me to focus on? Or is it okay to make this call to this API? Maybe it's a paid API, like we're accessing this expensive stock data. But I think if I knew this, I could answer your question. You're like, yeah, spend the dollar. Let's see. That kind of thing. Exactly.

00:39:30.280 --> 00:39:41.580
Or like with the hotel booking agent, let's say I'm like, I'd like to book the Hilton in Boston. And then it comes up with a proposal for booking not the right hotel.

00:39:41.640 --> 00:39:52.820
I can say like actually can you clarify with the user which hotel they'd like to book like maybe provide an address or something and then that like cycles back through and then maybe it gets the proper tool called the second time.

00:39:52.960 --> 00:40:04.180
They didn't mind driving so we put them on the opposite end with the maximum traffic. It only looks like at 10 miles but it actually turns out to be two hours to get from here to there when they need to be there. They don't want that right? We

00:40:04.180 --> 00:40:19.920
one kind of exciting other open source project we have is called Agent Inbox and so if you have a lot of interrupts going in your application the agent inbox you can hook up to that application and it basically provides you almost looking like this email inbox of all the interrupts for you to

00:40:20.340 --> 00:40:21.300
address so

00:40:21.300 --> 00:40:39.880
maybe if you had like a social media agent or something that was helping you with your posts and it interrupted right before the final post you can go through and like accept all those tool calls or edit posts right before they get sent out just making it easier to kind of deal with multiple runs, things like that.

00:40:39.980 --> 00:40:48.540
Yeah, I guess that makes a lot of sense because otherwise we're going to just have a bunch of programs that are executing but just stuck there waiting for people until they go and just start typing back to it.

00:40:48.600 --> 00:40:48.680
And

00:40:48.680 --> 00:40:52.640
how human in the loop ties into this persistence layer is sort of twofold.

00:40:53.140 --> 00:40:57.500
One, an application can be suspended for any indefinite amount of time.

00:40:57.900 --> 00:41:07.720
So the guarantee with our persistence is that you can just restart back where the interrupt was, whether that's a minute later, five minutes later, 10 days later, a year later, whatever that may be.

00:41:08.300 --> 00:41:16.220
And so just making sure that like you're guaranteed to get that human input before you resume, but then you have a kind of resumable state.

00:41:16.720 --> 00:41:19.900
The second thing is like we've talked about, you want to learn from that human input.

00:41:20.140 --> 00:41:25.780
And so you can kind of analyze that in a separate step and then store that in the long-term memory.

00:41:25.980 --> 00:41:29.500
Is that a coding thing that you do or is that just the way you set up the workflow?

00:41:29.960 --> 00:41:30.740
Yeah, great question.

00:41:30.980 --> 00:41:35.460
So we provide kind of support for the fundamentals in LandGraph.

00:41:35.740 --> 00:41:38.780
But we think kind of extensibility and customizability is pretty important.

00:41:38.960 --> 00:41:43.040
So we have like data structures for those short term and long term stores.

00:41:43.580 --> 00:41:46.660
But we also provide interfaces so that you could customize your store.

00:41:46.840 --> 00:41:51.280
And we have like in-memory stores or Postgres stores or things like that.

00:41:51.400 --> 00:41:54.540
But if you wanted to use some custom location, you could do that as well.

00:41:54.660 --> 00:41:55.500
Yeah, super interesting.

00:41:55.800 --> 00:42:03.760
So with this ability to suspend these conversations, does that depend upon the LLM keeping that information around?

00:42:03.920 --> 00:42:09.820
Like if you make a call to an API, do you get like a session ID sort of thing for that conversation?

00:42:10.270 --> 00:42:17.400
Or does it somehow store it, like you say, in Postgres or whatever, and then provide that back to the LLM to keep talking to it?

00:42:17.500 --> 00:42:19.320
Yeah, so it's the latter of those.

00:42:19.780 --> 00:42:25.360
You're kind of storing the conversation state and message history and any other relevant state to your application.

00:42:25.870 --> 00:42:30.620
And then once you get that signal from the user, you can pick all that up.

00:42:30.780 --> 00:42:32.520
It's kind of unique to the thread.

00:42:32.720 --> 00:42:34.960
We call it like the thread ID or the run ID.

00:42:35.400 --> 00:42:36.600
And yeah, start back from there.

00:42:36.920 --> 00:42:40.740
And then you would, if you're going to call the LLM again, just speed in that context.

00:42:41.040 --> 00:42:43.380
Yeah, because that's kind of what subsequent chats are, right?

00:42:44.020 --> 00:42:46.660
It's see everything again, plus the new information.

00:42:47.260 --> 00:42:48.800
Now answer that, right?

00:42:49.300 --> 00:42:49.980
It's a little weird.

00:42:50.340 --> 00:42:50.400
Yeah.

00:42:50.900 --> 00:43:00.040
So that's really helpful for things like local LLMs that might run or restart more frequently, unlike some of the cloud ones where they kind of keep that data.

00:43:00.180 --> 00:43:01.640
But I don't know how long they keep it for.

00:43:01.940 --> 00:43:11.560
Yeah, and a new kind of exciting feature in the persistence space generally is we're going to very soon introduce caching on the kind of node-by-node level.

00:43:11.900 --> 00:43:24.240
So you can imagine if you have a node, which is really just a function running some expensive operation, for simplicity, we can just say it's like a pre-processing step, not even involved with an LLM.

00:43:24.680 --> 00:43:41.780
you pass certain inputs to that node and you could enable some sort of cache policy there so that if you're starting up with the same inputs, you get to skip that step. And so really just trying to optimize for how can lots of different types of users use this same low-level system for a variety of agentic workflows.

00:43:42.120 --> 00:44:03.260
You have these nodes and graphs and then you put it together in this workflow and you've got this cool persistence of memory. Can some of those operations be just pure Python workflow type things rather than always LLM, right? Like can it be call this LLM, use this tool, call this LLM, now just make an entry in our database or just send an email or whatever?

00:44:03.600 --> 00:44:13.540
Yeah, definitely. I think that's the kind of value of that low level framework. Other what we call agent frameworks are really just focusing on the operations involved with LLMs.

00:44:13.800 --> 00:44:16.420
Right, or orchestrating multiple LLM calls more or less, yeah?

00:44:16.620 --> 00:44:26.080
The OpenAI Agents SDK, for example, helps you query OpenAI LLMs and orchestrate many agents together and maybe have guardrails.

00:44:26.240 --> 00:44:31.860
But there's no kind of built-in support for just those standard Python operations, functions, processing, et cetera.

00:44:32.240 --> 00:44:35.580
And so Node really just can contain any function.

00:44:35.960 --> 00:44:41.840
So calling some API or doing any other thing you might possibly want it to do in just pure Python.

00:44:42.280 --> 00:44:48.540
So we talked about using different LLMs like Anthropic or OpenAI or a local one that you might download.

00:44:49.080 --> 00:44:52.140
Can I mix and match LLMs within a single workflow?

00:44:52.600 --> 00:44:58.280
Can I say this part I want to run in a local model and that part I want to run on O3 Pro or whatever?

00:44:58.760 --> 00:44:59.940
I don't know if that's even out on the API.

00:45:00.080 --> 00:45:02.280
It's certainly not out in chat, but where's O3 Pro?

00:45:02.460 --> 00:45:02.600
Come on.

00:45:02.680 --> 00:45:04.940
And they have O4 and 4.0 as well.

00:45:05.660 --> 00:45:06.840
Oh my gosh, it's even.

00:45:07.780 --> 00:45:08.580
Yeah, absolutely.

00:45:08.840 --> 00:45:11.620
You can mix and match as your heart desires.

00:45:12.100 --> 00:45:22.540
So LingChain, that first open source tool that we have, makes it really easy to pass messages from one OM provider to another in that kind of model agnostic format.

00:45:23.000 --> 00:45:30.800
And so if you're using LingChain's model abstractions from one node to the next, it's quite easy to use.

00:45:30.860 --> 00:45:31.420
Yeah, that's really cool.

00:45:31.420 --> 00:45:40.200
And you could even do things, I imagine, such as I want to call a certain model that's really good at coding, maybe Cloud3.7 or something like that.

00:45:40.520 --> 00:45:46.560
But I want to call, now I need to call another one that's better at tool usage and is more creative now that I have the answer, right?

00:45:46.680 --> 00:45:53.360
A little bit like my weirdo perplexity thing went and wrote Python code to answer my history question, right?

00:45:53.440 --> 00:45:53.720
Totally.

00:45:53.950 --> 00:45:59.280
And it's really interesting to think about the pattern of using a model as a router.

00:45:59.800 --> 00:46:09.360
So you can imagine like a user asks a question and you give some model the context that's like, I have three model providers that I can use.

00:46:09.840 --> 00:46:11.800
Model provider A is really great at research.

00:46:12.200 --> 00:46:15.480
Model provider B is really good at solving coding problems, etc.

00:46:15.900 --> 00:46:27.160
And so first, the user question gets routed through that router node, and then that can send and basically delegate to one of the other nodes based on that model's reasoning decision.

00:46:27.580 --> 00:46:33.020
So that is kind of a cool multi-agent architecture that you could design with Langrass pretty easily.

00:46:33.520 --> 00:46:46.320
One other thing that I wanted to note when you were asking about running Python code in nodes, kind of LM or non-agentic patterns, is that we have this main graph API, which is what most folks use.

00:46:46.380 --> 00:46:53.300
But we've also introduced an imperative API that allows you to decorate functions with kind of two key things.

00:46:53.460 --> 00:46:58.740
One is an entry point decorator that's like kind of indicative of the start node in your graph.

00:46:59.100 --> 00:47:01.340
And then another one is a task decorator.

00:47:01.800 --> 00:47:12.740
And so if you already have kind of a workflow-like system or functions already written, you can easily turn that into a LNGraph workflow just with a couple of decorators, which is very nice.

00:47:12.880 --> 00:47:13.340
I like it.

00:47:13.520 --> 00:47:13.980
I like it a lot.

00:47:14.240 --> 00:47:20.740
Let's talk about just what if we look inside LNGraph, surely it's written in Rust, right?

00:47:20.900 --> 00:47:22.040
That has to be written in Rust.

00:47:22.900 --> 00:47:23.540
Still in Python.

00:47:23.900 --> 00:47:25.140
The library itself is written in Python.

00:47:25.640 --> 00:47:27.560
So it's easy to understand.

00:47:28.480 --> 00:47:32.300
Do you all have external contributors and take pull across and stuff like that?

00:47:32.540 --> 00:47:33.140
Yeah, we do.

00:47:33.420 --> 00:47:44.000
I think we see a bit more engagement on the LangChain side because LangChain itself is more involved with new features released by model providers or things like that.

00:47:44.080 --> 00:47:45.700
We see more activity there.

00:47:46.080 --> 00:47:48.300
And LangChain is where all of our integrations lie.

00:47:48.500 --> 00:47:52.060
So like the LangChain OpenAI package or LangChain Anthropic.

00:47:52.500 --> 00:47:54.680
So that's where most of the community engagement is.

00:47:54.820 --> 00:47:58.120
But, you know, like in the other good open source project, we've got open issues.

00:47:58.240 --> 00:48:02.340
and discussions and a Slack help channel for LandGraph users

00:48:02.340 --> 00:48:02.780
as well.

00:48:02.780 --> 00:48:03.520
Right, right, a community.

00:48:03.630 --> 00:48:04.040
Okay, cool.

00:48:04.480 --> 00:48:13.640
Well, I was going to say you had 174 contributors in LandGraph, but I thought that was pretty good in terms of number of people, but you've got 3,600 in LangChain.

00:48:13.670 --> 00:48:14.940
So yeah, that's pretty wild.

00:48:15.420 --> 00:48:17.720
Used by 237,000 projects.

00:48:18.180 --> 00:48:21.680
Yeah, it is really impressive kind of the scope of usage there.

00:48:21.810 --> 00:48:26.060
I think it's the biggest like developer community in the AI space.

00:48:26.740 --> 00:48:35.160
So it's really nice to have so many eager contributors and get to use that as kind of the foundation for the more model driven operations in LandGraph.

00:48:35.320 --> 00:48:41.840
I always like to use the Python web frameworks as measuring sticks or whatever for these things.

00:48:42.360 --> 00:48:45.460
So for example, if you look at search for Flask on GitHub, right?

00:48:46.160 --> 00:48:47.340
It's, where are you Flask?

00:48:47.440 --> 00:48:47.560
Come here.

00:48:47.860 --> 00:48:50.860
It's got 70,000 stars, which is pretty awesome.

00:48:50.920 --> 00:48:55.000
You guys have 107, like just to give people a sense of like how popular this is.

00:48:55.160 --> 00:49:00.160
And I also bring this up because there are people out there go, oh, I can't believe you're doing an AI topic.

00:49:00.500 --> 00:49:01.180
I'm so tired of AI.

00:49:01.280 --> 00:49:03.940
I can't take any more AI, which I get on one hand.

00:49:04.140 --> 00:49:08.180
Like on the other, this is how significant it is.

00:49:08.280 --> 00:49:12.580
It's like literally revolutionizing tech and non-tech before our eyes.

00:49:12.660 --> 00:49:13.500
It's like history in the making.

00:49:13.920 --> 00:49:15.680
It's like the invention of the web a little bit, I think.

00:49:15.960 --> 00:49:16.360
What are your thoughts?

00:49:16.600 --> 00:49:16.680
I

00:49:16.680 --> 00:49:23.120
understand kind of the things are so novel and I understand wanting to push back against such big change.

00:49:23.620 --> 00:49:32.720
But I think one of the reasons that I like LaneGraph as a tool so much is that it really lets you balance the AI when you need it and not when you don't.

00:49:33.260 --> 00:49:38.240
We talk a lot about kind of finding yourself on this workflow versus agents curve.

00:49:38.660 --> 00:49:46.060
The more like pure agent is much less predictable and pure workflow is predictable, but maybe not quite as useful.

00:49:46.580 --> 00:49:49.460
And I think it lets you pick like wherever you want to be on that curve.

00:49:50.040 --> 00:49:53.260
And so hopefully that's also relevant for listeners who are like, man, I'm not.

00:49:53.320 --> 00:49:57.360
super interested in AI applications, but like maybe I have this one

00:49:57.360 --> 00:49:58.360
huge workflow

00:49:58.360 --> 00:50:16.960
where one step could really use the help of an LLM. And so I think it's a nice like on-ramp into AI usage for developers and just the customizability is very valuable. But yeah, I mean, I understand like the general hesitation. I think it's kind of revolutionizing our industry and it's unpredictable.

00:50:17.310 --> 00:51:16.200
I agree. One other thing I'd like to throw out there is I think rightly so a lot of people are concerned about the environmental impacts of heavy AI usage. On the other hand, I think there's a lot of interesting things you can do with local models and the local models really, they really don't make any difference running them, not necessarily training them. Like for example, I was running a 7 billion parameter model for this Git project that I'm doing and this Git tool. And I think I have a little iStats menu thing for my my computer's energy usage right now it's using between 30 says 13 watts but it bumps around like 13 14 watts when the llm was running it jumped all the way up to 20 which is like one led light bulb for four seconds right so if people who are concerned about those things like you can run these models locally and still get a huge amount of benefit even if they you don't get like the full deep research, massive, massive type of stuff you get from the cloud.

00:51:16.300 --> 00:51:18.340
It's still really, really impactful, I think.

00:51:18.520 --> 00:51:19.060
Yeah, definitely.

00:51:19.450 --> 00:51:23.440
I think that like really leans into the idea of like build simple when you can.

00:51:23.760 --> 00:51:29.240
Like a lot of these systems can be relatively simple and context windows are widening.

00:51:29.400 --> 00:51:40.320
And I think if you can construct a solution that just meets your needs, but doesn't do anything like crazy outside of it, it's both going to be better environmentally and like more reliable of a system.

00:51:40.600 --> 00:51:51.100
Yeah, I was talking to Ines Montani about this whole topic, but mostly about the open source, or at least open weights models and things like that.

00:51:51.500 --> 00:51:59.140
And one of the things she really focused on was about task-specific models or models that are really focused on solving a certain problem.

00:51:59.300 --> 00:52:09.440
And it sounds to me like combining that idea with Lengraph and sort of bringing in multiple models to the model router, I think you called it, could be really cool.

00:52:09.620 --> 00:52:12.400
So instead of running huge models, I run five small ones.

00:52:12.880 --> 00:52:15.720
But in coordination, they actually become very powerful.

00:52:16.000 --> 00:52:21.300
One of the challenges of using LLMs is reliability if tasks are ambiguous.

00:52:22.160 --> 00:52:25.500
It's not going to do a good job if you don't tell it what you want it to do.

00:52:25.500 --> 00:52:35.620
And so, yeah, if you have these really specialized tools and you only route things to, or I say tools, what I really mean is models, to clarify when tools is also overloaded.

00:52:35.900 --> 00:52:47.140
But yeah, if you have very specialized models and you can kind of properly route queries and questions and tasks and demands to them, I think that's the best way to have a reliable system.

00:52:47.250 --> 00:52:48.280
Yeah, it's probably faster too.

00:52:48.460 --> 00:52:50.280
And lower environmental footprint.

00:52:50.670 --> 00:52:58.900
The one bonus, the one good thing I think here is that the negative environmental impact is directly correlated with cost.

00:52:59.130 --> 00:53:05.660
So people's incentives are aligned for trending towards less environmental impact.

00:53:05.820 --> 00:53:07.840
even if they don't care about it, right?

00:53:07.870 --> 00:53:08.680
They just want to save money.

00:53:09.080 --> 00:53:10.880
That's still going to get kind of the same output.

00:53:11.120 --> 00:53:11.280
I'm

00:53:11.280 --> 00:53:12.360
actually really excited.

00:53:12.520 --> 00:53:17.200
I won't be able to go to PyCon this year, but I'm excited to watch some of the talks after the fact.

00:53:17.580 --> 00:53:20.660
And I helped kind of review the talk submissions this year.

00:53:20.660 --> 00:53:31.460
And it seems like there's some really great ones focusing on environmental impacts of your Python code and how to reduce those and, you know, how to think about that on an individual versus like large scale.

00:53:31.940 --> 00:53:36.060
And I think it's really great that that's kind of driving more public conversation now.

00:53:36.140 --> 00:53:36.500
Yeah, yeah.

00:53:37.120 --> 00:53:38.760
Don't tell your LLM things at the end.

00:53:39.040 --> 00:53:39.220
Go on.

00:53:39.300 --> 00:53:39.620
Yeah.

00:53:42.360 --> 00:53:46.000
So let's wrap this up with maybe a bit of a showcase.

00:53:46.540 --> 00:53:55.740
Have you got some example apps or things you've seen build or things people are doing that you're like, there's really cool stuff you can do with a link graph and link chain or LLMs in general.

00:53:56.160 --> 00:53:58.220
What's some cool stuff you've seen out there people building?

00:53:58.440 --> 00:54:08.720
I mentioned the email assistant as a very simple one that I think is satisfying just in that the utility is really high and that's saving folks a lot of time.

00:54:09.160 --> 00:54:22.700
I think the social media agent is a cool idea if you're like, I don't want to spend a bunch of time working on social media engagement, but I do want to kind of keep up with the most important updates in my space and then also kind of contribute.

00:54:23.160 --> 00:54:31.940
I think hopefully I'll have some better answers in the short term, still kind of onboarding at BlaineChain and trying to better understand customer use cases.

00:54:32.420 --> 00:54:32.840
Very cool.

00:54:33.800 --> 00:54:52.620
One of the things that I think, just thinking out loud of the examples of things that'd be really amazing is you look at research labs and academia in general, especially the research heavy ones, how much those folks have to read long detailed papers and assess whether they even make any sense.

00:54:52.680 --> 00:55:28.080
you could set up some kind of hygienic workflow that just watches for new things being published in your field looks to summarize them, determines the impact and relevance and then brings you like a digest a weekly digest. Well here's what's happened in academia for you and do you have other questions that could be like your interrupt or your inbox sort of thing like well which of these would you like us to dive into right and like maybe really go in and analyze these further and I don't know. It seems like when you're really trying to summarize text or process it or correlate, I don't know. It's just, there's so many possibilities there.

00:55:28.240 --> 00:55:44.180
Yeah, definitely. And it's really cool to think about how extensible that is. You could kind of build this like general system for a research agent that's going to provide you with a summary. And then different users can be like, okay, here are the three topics that I'm most interested in, like provide me with the summary for those.

00:55:44.840 --> 00:55:52.620
Nice to basically think about like individual users specifying things that they want to narrow in on or like specialties that they want to emphasize.

00:55:52.920 --> 00:55:53.400
Super fun.

00:55:53.540 --> 00:55:53.760
All right.

00:55:54.480 --> 00:55:57.240
How about a last thing here, a roadmap?

00:55:57.940 --> 00:55:58.600
Where are things going?

00:55:59.160 --> 00:56:04.220
What should people maybe have been asking for that might be showing up they should be on the lookout for?

00:56:04.320 --> 00:56:07.740
One thing I mentioned a little bit earlier is that kind of node level cache.

00:56:08.050 --> 00:56:14.980
I think that'll just be a nice performance boost for folks, especially folks looking to really scale with their lane graph applications.

00:56:15.600 --> 00:56:35.580
The thing that we're focusing on the most now is kind of this happy path for those pre-built. So the like create agent functions or some other frameworks have classes. I think, for example, with human in the loop, I'm working on a feature right now that makes it really easy to use kind of default patterns for human in the loop with those pre-built agents.

00:56:35.800 --> 00:56:50.360
So maybe you have like tools A, B, and C, and for all of them, you want default logic such that when the tools run, a human has to accept the tool call before or when the tools are recommended by an LLM before the tool actually runs.

00:56:50.860 --> 00:57:04.620
So basically, just emphasizing really making it easier to get started with those like create agent pre-builts, because we think there's going to be increasing demand on the like, how do I in 10 lines of code get started with a really functional agent?

00:57:04.820 --> 00:57:20.860
Oh, this is very Pythonic, right? Like it has a lot of, you can put, you can do all the things with it if you want, but then you're down in the details and you're doing advanced programming type stuff, right? But what's the import library? Do the few things have magic happen, right?

00:57:21.100 --> 00:57:28.240
I think those like quick start guides are great if they're truly quick. And so I think just kind of shooting for that on our end.

00:57:28.380 --> 00:57:32.940
Yeah, that's cool. I would also, this is pure speculation, but maybe informed speculation.

00:57:33.190 --> 00:57:33.560
I don't know.

00:57:33.920 --> 00:57:46.000
If I were to guess, people coming to do web stuff in Python are pretty CS-like folks, maybe not pure, like they don't necessarily have CS degrees, but they've got that way of thinking.

00:57:46.190 --> 00:57:48.280
Like they know what a unit test is, even if they don't do them.

00:57:48.720 --> 00:57:51.320
They know about SQL, even if they use an ORM or whatever.

00:57:51.840 --> 00:58:03.240
Then we've got the data science folks where before the AI revolution, that was like, well, this is where people who are not really programmers sort of get their first step into Python, and then they kind of get hooked.

00:58:03.290 --> 00:58:08.000
But they start by doing these real simple things like the projects you're talking about or the features you're talking about.

00:58:08.310 --> 00:58:10.400
And then they realize, oh, well, I can do a little more.

00:58:10.520 --> 00:58:13.900
And then they sort of work their way backwards into becoming Python programmers.

00:58:14.380 --> 00:58:20.260
I would say that people coming to AI are maybe even less formal than the data science folks, right?

00:58:20.380 --> 00:58:22.900
It's a little even further along that spectrum there.

00:58:22.930 --> 00:58:29.240
And so really making those people feel welcome until they become a little more programmer-focused is probably really important.

00:58:29.540 --> 00:58:41.000
I recall you chatted on your kind of developer trends for 2025 forecast podcast about the value of being able to get started in Python if you've never written a single line of code and it being pretty easy.

00:58:41.180 --> 00:58:52.340
And then also folks like the or teams like the folks at Astral developing tools for really advanced users and people who really want to make Python a sophisticated system.

00:58:52.880 --> 00:59:07.820
And I think we basically with LangGraph want to provide that same benefit of you can get started with a couple lines of code and then also like you want to deploy tons of parallel instances, running parallel processes and streaming responses with all of these features.

00:59:08.060 --> 00:59:09.100
Like you can do that too.

00:59:09.260 --> 00:59:09.840
Yeah, that's awesome.

00:59:10.260 --> 00:59:13.680
Yeah, you start with one, grow to the other if that's the path you're on, right?

00:59:13.680 --> 00:59:13.960
I guess

00:59:13.960 --> 00:59:24.300
one other thing that I'll mention roadmap wise is both LangChain and LangGraph integrate with LangSmith, which is our commercial product.

00:59:24.920 --> 00:59:33.460
And we didn't talk a ton about observability and more of the prompt engineering or evaluation side of things that is often tied into agentic workflows.

00:59:33.800 --> 00:59:40.480
But yeah, so if you're building applications with LangGraph, using LangSmith for observability can be super valuable, especially when you're at scale.

00:59:40.640 --> 00:59:43.340
Yeah, monitoring for AI app performance, super cool.

00:59:43.720 --> 00:59:49.320
I guess maybe one more is LangChain Studio or LangStudio, the studio for debugging.

00:59:49.660 --> 00:59:56.200
So if you're developing your lane graph application, you can just run lane graph dev in your terminal.

00:59:56.430 --> 00:59:58.980
And then that pops up with kind of a local studio.

00:59:59.660 --> 01:00:01.680
You can like step through your graph.

01:00:02.120 --> 01:00:03.680
Yeah, that's a great screenshot there.

01:00:03.990 --> 01:00:11.160
Or yes, but that makes it really easy to debug things locally and just kind of understand the flow of your system in real time.

01:00:11.620 --> 01:00:17.520
And then we also offer a lane graph platform where you can do this kind of thing as well, but deployed locally.

01:00:17.800 --> 01:00:17.980
Awesome.

01:00:18.300 --> 01:00:19.300
Well, final call to action.

01:00:19.820 --> 01:00:22.280
People are interested in getting started with all these ideas.

01:00:22.500 --> 01:00:22.960
What do you tell them?

01:00:23.060 --> 01:00:27.660
I would say look forward to our new docs drop this week.

01:00:27.820 --> 01:00:33.360
I think it's going to really emphasize helping folks get started with those create agent pre-builds.

01:00:33.480 --> 01:00:36.820
Can they just go to linkchain.com and click on docs and go from there?

01:00:37.180 --> 01:00:37.300
Okay.

01:00:37.460 --> 01:00:37.620
Yeah.

01:00:37.700 --> 01:00:43.480
So we have our linkchain docs there, our link graph docs there, and it should be quite easy to get started.

01:00:43.740 --> 01:00:47.600
And obviously I mentioned our kind of community forums earlier, but we have a community help slack.

01:00:47.960 --> 01:00:51.640
open issues and discussions, et cetera. So feel free to ask questions there.

01:00:51.640 --> 01:00:58.260
And then this interrupt conference you talked about, is this going to be live streamed or recorded and videos available?

01:00:58.500 --> 01:00:58.780
Yes,

01:00:58.920 --> 01:01:19.420
we should have talk recordings and demo recordings available as well. One other resource that I should mention is that we are kind of trying to grow in the education space. And so we also offer a Langrath course, and we'll be releasing a new course next week, kind of focusing on those ambient agents.

01:01:20.040 --> 01:01:26.300
And that's a really great way to get started if you're maybe not quite ready to write code, but just kind of want to learn more about the LandGraph features.

01:01:26.960 --> 01:01:31.000
Or if you are ready to write code, it has some awesome resources and notebooks as well.

01:01:32.039 --> 01:01:33.220
Well, very interesting.

01:01:33.560 --> 01:01:37.040
And it's an exciting time to be working in code.

01:01:37.859 --> 01:01:38.920
It's changing fast.

01:01:38.990 --> 01:01:42.500
It's great to talk about these cool new systems that we get to use and explore.

01:01:42.680 --> 01:01:45.520
Yeah, well, looking forward to seeing what you all keep creating.

01:01:45.780 --> 01:01:46.960
Thanks for being back on the show, Sydney.

01:01:47.180 --> 01:01:48.320
Yeah, thank you so much for having me.

01:01:48.440 --> 01:01:48.940
Yeah, bye.

01:01:50.400 --> 01:01:52.780
This has been another episode of Talk Python To Me.

01:01:53.680 --> 01:01:54.520
Thank you to our sponsors.

01:01:54.970 --> 01:01:56.220
Be sure to check out what they're offering.

01:01:56.380 --> 01:01:57.640
It really helps support the show.

01:01:58.580 --> 01:02:02.240
This episode is sponsored by Posit Connect from the makers of Shiny.

01:02:02.720 --> 01:02:06.720
Publish, share, and deploy all of your data projects that you're creating using Python.

01:02:07.280 --> 01:02:13.280
Streamlit, Dash, Shiny, Bokeh, FastAPI, Flask, Quarto, Reports, Dashboards, and APIs.

01:02:14.220 --> 01:02:15.800
Posit Connect supports all of them.

01:02:16.140 --> 01:02:21.440
Try Posit Connect for free by going to talkpython.fm/posit, P-O-S-I-T.

01:02:22.560 --> 01:02:24.600
And it's brought to you by Auth0.

01:02:25.300 --> 01:02:29.560
Auth0 is an easy-to-implement, adaptable authentication and authorization platform.

01:02:30.280 --> 01:02:36.700
Think easy user logins, social sign-on, multi-factor authentication, and robust role-based access control.

01:02:37.180 --> 01:02:42.000
With over 30 SDKs and quick starts, Auth0 scales with your product at every stage.

01:02:42.780 --> 01:02:48.240
Get 25,000 monthly active users for free at talkpython.fm/auth zero.

01:02:49.340 --> 01:02:50.220
Want to level up your Python?

01:02:50.680 --> 01:02:54.320
We have one of the largest catalogs of Python video courses over at Talk Python.

01:02:54.800 --> 01:02:59.480
Our content ranges from true beginners to deeply advanced topics like memory and async.

01:02:59.920 --> 01:03:02.020
And best of all, there's not a subscription in sight.

01:03:02.500 --> 01:03:05.020
Check it out for yourself at training.talkpython.fm.

01:03:05.740 --> 01:03:09.900
Be sure to subscribe to the show, open your favorite podcast app, and search for Python.

01:03:10.360 --> 01:03:11.220
We should be right at the top.

01:03:11.740 --> 01:03:20.580
You can also find the iTunes feed at /itunes, the Google Play feed at /play, and the direct RSS feed at /rss on talkpython.fm.

01:03:21.240 --> 01:03:23.500
We're live streaming most of our recordings these days.

01:03:23.860 --> 01:03:31.340
If you want to be part of the show and have your comments featured on the air, be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

01:03:32.360 --> 01:03:33.480
This is your host, Michael Kennedy.

01:03:33.900 --> 01:03:34.740
Thanks so much for listening.

01:03:34.900 --> 01:03:35.880
I really appreciate it.

01:03:36.240 --> 01:03:37.820
Now get out there and write some Python code.

