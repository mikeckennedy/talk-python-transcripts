WEBVTT

00:00:00.000 --> 00:00:05.320
This episode dives into some of the most important data science libraries from the Python space

00:00:05.320 --> 00:00:08.280
with one of its pioneers, Wes McKinney.

00:00:08.280 --> 00:00:13.300
He's the creator or co-creator of the pandas Apache Arrow and Evus projects, as well as

00:00:13.300 --> 00:00:16.240
an entrepreneur in this space.

00:00:16.240 --> 00:00:21.680
This is Talk Python to Me, episode 462, recorded April 11th, 2024.

00:00:21.680 --> 00:00:25.000
Are you ready for your host, Darius?

00:00:25.000 --> 00:00:28.000
You're listening to Michael Kennedy on Talk Python to Me.

00:00:28.000 --> 00:00:33.000
Live from Portland, Oregon, and this segment was made with Python.

00:00:33.000 --> 00:00:38.640
Welcome to Talk Python to Me, a weekly podcast on Python.

00:00:38.640 --> 00:00:40.500
This is your host, Michael Kennedy.

00:00:40.500 --> 00:00:45.400
Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython,

00:00:45.400 --> 00:00:48.040
both on fosstodon.org.

00:00:48.040 --> 00:00:53.280
Keep up with the show and listen to over seven years of past episodes at talkpython.fm.

00:00:53.280 --> 00:00:56.760
We've started streaming most of our episodes live on YouTube.

00:00:56.760 --> 00:01:02.720
Subscribe to our YouTube channel over at talkpython.fm/youtube to get notified about upcoming shows and

00:01:02.720 --> 00:01:04.720
be part of that episode.

00:01:04.720 --> 00:01:07.480
This episode is sponsored by Neo4j.

00:01:07.480 --> 00:01:12.840
It's time to stop asking relational databases to do more than they were made for and simplify

00:01:12.840 --> 00:01:15.660
complex data models with graphs.

00:01:15.660 --> 00:01:21.200
Check out the sample FastAPI project and see what Neo4j, a native graph database, can do

00:01:21.200 --> 00:01:22.200
for you.

00:01:22.200 --> 00:01:24.200
Find out more at talkpython.fm/neo4j.

00:01:24.200 --> 00:01:32.760
And it's brought to you by Mailtrap, an email delivery platform that developers love.

00:01:32.760 --> 00:01:34.960
Try for free at mailtrap.io.

00:01:34.960 --> 00:01:37.880
Hey, Wes, welcome to Talk Python to Me.

00:01:37.880 --> 00:01:38.880
Thanks for having me.

00:01:38.880 --> 00:01:42.600
You know, honestly, I feel like it's been a long time coming having you on the show.

00:01:42.600 --> 00:01:46.080
You've had such a big impact in the Python space, especially the data science side of

00:01:46.080 --> 00:01:48.960
that space, and it's high time to have you on the show.

00:01:48.960 --> 00:01:49.960
So welcome.

00:01:49.960 --> 00:01:50.960
Good to have you.

00:01:50.960 --> 00:01:51.960
Yeah, it's great to be here.

00:01:51.960 --> 00:01:56.640
I've been heads down a lot the last, you know, last n years.

00:01:56.640 --> 00:02:01.600
And I actually haven't been, because I think a lot of my work has been more like data infrastructure

00:02:01.600 --> 00:02:05.560
and working at even a lower level than Python.

00:02:05.560 --> 00:02:10.680
So I haven't been as engaging as much directly with the Python community.

00:02:10.680 --> 00:02:15.200
But it's been great to kind of get back more involved and start catching up on all the

00:02:15.200 --> 00:02:17.320
things that people have been building.

00:02:17.320 --> 00:02:21.880
And being at Posit gives me the ability to, yeah, sort of have more exposure to what's

00:02:21.880 --> 00:02:24.920
going on and people that are using Python in the real world.

00:02:24.920 --> 00:02:27.080
There's a ton of stuff going on at Posit that's super interesting.

00:02:27.080 --> 00:02:28.440
And we'll talk about some of that.

00:02:28.440 --> 00:02:32.000
And, you know, it's sometimes it's just really fun to build, you know, and work with people

00:02:32.000 --> 00:02:33.000
building things.

00:02:33.000 --> 00:02:35.000
And I'm sure you're enjoying that aspect of it.

00:02:35.000 --> 00:02:36.000
For sure.

00:02:36.000 --> 00:02:37.000
Nice.

00:02:37.000 --> 00:02:41.480
Well, before we dive into pandas and all the things that you've been working on after that,

00:02:41.480 --> 00:02:45.080
you know, let's just hear a quick bit about yourself for folks who don't know you.

00:02:45.080 --> 00:02:46.080
Sure.

00:02:46.080 --> 00:02:47.080
Yeah.

00:02:47.080 --> 00:02:49.880
So I grew up in Akron, Ohio, mostly.

00:02:49.880 --> 00:02:55.760
And I got involved, started getting involved in Python development around 2007, 2008.

00:02:55.760 --> 00:03:00.120
And built, I was working in quant finance at the time, I started building a personal

00:03:00.120 --> 00:03:04.280
data analysis toolkit that turned into the pandas project.

00:03:04.280 --> 00:03:08.920
And then open source that in 2009, started getting involved in the Python community.

00:03:08.920 --> 00:03:14.220
And I spent several years like writing my book, Python for Data Analysis, and then working

00:03:14.220 --> 00:03:19.720
with the broader scientific Python, Python data science community to help enable Python

00:03:19.720 --> 00:03:24.280
to become a mainstream programming language for doing data analysis and data science.

00:03:24.280 --> 00:03:30.360
In the meantime, I've become an entrepreneur, I've started some companies and I've been

00:03:30.360 --> 00:03:36.400
working to innovate and improve the computing infrastructure that powers data science tools

00:03:36.400 --> 00:03:38.680
and libraries like pandas.

00:03:38.680 --> 00:03:44.200
So that's led to some other projects like Apache Arrow and IBIS and some other things.

00:03:44.200 --> 00:03:50.080
Recent years, I've been worked on a startup, Voltron Data, which is still very much going

00:03:50.080 --> 00:03:54.520
strong and has a big team and is off to the races.

00:03:54.520 --> 00:03:58.760
And I've had a long relationship with Posit, formerly RStudio.

00:03:58.760 --> 00:04:04.960
And they were my home for doing Arrow development from 2018 to 2020.

00:04:04.960 --> 00:04:08.920
They helped me incubate the startup that became Voltron Data.

00:04:08.920 --> 00:04:14.120
And so I've gone back to work full time there as a software architect to help them with

00:04:14.120 --> 00:04:19.720
their Python strategy to make sort of their data science platform a delight to use for

00:04:19.720 --> 00:04:21.160
the Python user base.

00:04:21.160 --> 00:04:22.600
I'm pretty impressed with what they're doing.

00:04:22.600 --> 00:04:28.160
I didn't realize the connection between Voltron and Posit, but I have had Joe Chung on the

00:04:28.160 --> 00:04:32.600
show before to talk about Shiny for Python.

00:04:32.600 --> 00:04:37.280
And I've seen him demo a few really interesting things, how it integrates to notebooks these

00:04:37.280 --> 00:04:40.360
days, some of the stuff that you all are doing.

00:04:40.360 --> 00:04:41.860
And yeah, it's just it's fascinating.

00:04:41.860 --> 00:04:45.560
Can you give people a quick elevator pitch on that while we're on that subject?

00:04:45.560 --> 00:04:47.680
On Shiny or on Posit in general?

00:04:47.680 --> 00:04:49.200
Yeah, whichever you feel like.

00:04:49.200 --> 00:04:54.760
Yeah, so Posit started out 2009 as RStudio.

00:04:54.760 --> 00:04:57.600
And so it didn't start out intending to be a company.

00:04:57.600 --> 00:05:03.540
JJ Allaire and Joe Chang built a new IDE, integrated development environment for R,

00:05:03.540 --> 00:05:06.240
because what was available at the time wasn't great.

00:05:06.240 --> 00:05:11.520
And so they made that into, I think, probably one of the best data science IDEs that's ever

00:05:11.520 --> 00:05:12.520
been built.

00:05:12.520 --> 00:05:13.760
It's really an amazing piece of tech.

00:05:13.760 --> 00:05:20.120
So it started becoming a company with customers and revenue in the 2013 timeframe.

00:05:20.120 --> 00:05:24.320
And they've built a whole suite of tools to support enterprise data science teams to make

00:05:24.320 --> 00:05:27.000
open source data science work in the real world.

00:05:27.000 --> 00:05:30.760
But the company itself, it's a certified B corporation, has no plans to go public or

00:05:30.760 --> 00:05:31.760
IPO.

00:05:31.760 --> 00:05:36.960
It is dedicated to the mission of open source software for data science and technical communication,

00:05:36.960 --> 00:05:42.240
and is basically building itself to be a hundred year company that has a revenue generating

00:05:42.240 --> 00:05:49.160
enterprise product side and an open source side so that the open source feeds the enterprise

00:05:49.160 --> 00:05:50.160
part of the business.

00:05:50.160 --> 00:05:53.920
The enterprise part of the business generates revenue to support the open source development.

00:05:53.920 --> 00:05:58.040
And the goal is to be able to sustainably support the mission of open source data science

00:05:58.040 --> 00:06:01.640
for, hopefully, the rest of our lives.

00:06:01.640 --> 00:06:02.840
And it's an amazing company.

00:06:02.840 --> 00:06:06.920
It's been one of the most successful companies that dedicates a large fraction of its engineering,

00:06:06.920 --> 00:06:09.480
time to open source software development.

00:06:09.480 --> 00:06:13.920
So it's very impressed with the company and JJ Allaire, its founder.

00:06:13.920 --> 00:06:21.800
And I'm excited to be helping it grow and become a sustainable long-term fixture in

00:06:21.800 --> 00:06:22.800
the ecosystem.

00:06:22.800 --> 00:06:23.800
Yes.

00:06:23.800 --> 00:06:25.920
Yeah, it's definitely doing cool stuff.

00:06:25.920 --> 00:06:27.520
Incentives are aligned well, right?

00:06:27.520 --> 00:06:31.560
It's not private equity or IPO.

00:06:31.560 --> 00:06:38.100
Many people know JJ Allaire created ColdFusion, which is like the original dynamic web development

00:06:38.100 --> 00:06:40.120
framework in the 1990s.

00:06:40.120 --> 00:06:45.120
And so he and his brother, Jeremy, and some others built Allaire Corp to commercialize

00:06:45.120 --> 00:06:46.120
ColdFusion.

00:06:46.120 --> 00:06:49.880
And they built a successful software business that was acquired by Macromedia, which was

00:06:49.880 --> 00:06:51.880
eventually acquired by Adobe.

00:06:51.880 --> 00:06:55.560
But they did go public as Allaire Corp during the dot-com bubble.

00:06:55.560 --> 00:06:59.100
And JJ went on to found a couple of other successful startups.

00:06:59.100 --> 00:07:05.080
And so he found himself in his late 30s 15 years ago, or around age 40, around the age

00:07:05.080 --> 00:07:11.000
I am now, having been very successful as an entrepreneur, no need to make money, and looking

00:07:11.000 --> 00:07:15.360
for a mission to spend the rest of his career on.

00:07:15.360 --> 00:07:21.320
And that identifying data science and statistical computing as an open source, in particular,

00:07:21.320 --> 00:07:25.780
making open source for data science work, was the mission that he aligned with and something

00:07:25.780 --> 00:07:28.580
that he had been interested in earlier in his career.

00:07:28.580 --> 00:07:30.380
But he had gotten busy with other things.

00:07:30.380 --> 00:07:34.760
So it's really refreshing to work with people who are really mission focused and focused

00:07:34.760 --> 00:07:41.000
on making impact in the world, creating great software, empowering people, increasing accessibility,

00:07:41.000 --> 00:07:45.000
and making most of it available for free on the internet, and not being so focused on

00:07:45.000 --> 00:07:50.700
empire building and producing great profits for venture investors and things like that.

00:07:50.700 --> 00:07:58.560
So I think the goal of the company is to provide an amazing home for top-tier software developers.

00:07:58.560 --> 00:08:05.620
To work on this software, to spend their careers, and to build families, and to be a happy and

00:08:05.620 --> 00:08:08.600
healthy culture for working on this type of software.

00:08:08.600 --> 00:08:09.600
That sounds excellent.

00:08:09.600 --> 00:08:10.600
Very cool.

00:08:10.600 --> 00:08:12.880
I didn't realize the history all the way back to ColdFusion.

00:08:12.880 --> 00:08:14.520
Speaking of history, let's jump in.

00:08:14.520 --> 00:08:20.000
Wes, there's a possibility that people out there listening don't know what Pandas is.

00:08:20.000 --> 00:08:24.120
You would think it's pretty ubiquitous, and I certainly would say that it is, especially

00:08:24.120 --> 00:08:25.480
in the data science space.

00:08:25.480 --> 00:08:29.920
I got a bunch of listeners who listen and they say really surprising things.

00:08:29.920 --> 00:08:33.840
They'll say stuff to me like, "Michael, I've been listening for six weeks now and I'm starting

00:08:33.840 --> 00:08:36.320
to understand some of the stuff you all are talking about."

00:08:36.320 --> 00:08:38.660
I'm like, "Why did you listen for six weeks?

00:08:38.660 --> 00:08:40.040
You didn't know what I was talking about.

00:08:40.040 --> 00:08:41.680
That's crazy."

00:08:41.680 --> 00:08:45.640
And a lot of people use it as language immersion to get into the Python space.

00:08:45.640 --> 00:08:49.800
So I'm sure there's plenty of people out there who are immersing themselves but are pretty

00:08:49.800 --> 00:08:50.800
new.

00:08:50.800 --> 00:08:54.120
So maybe for that crew, we could introduce what Pandas is to them.

00:08:54.120 --> 00:08:55.120
Absolutely.

00:08:55.120 --> 00:08:59.360
So this is the Data Manipulation and Analysis Toolkit for Python.

00:08:59.360 --> 00:09:04.160
So it's a Python library that you install that enables you to read data files.

00:09:04.160 --> 00:09:09.420
So read many different types of data files off of disk or off of remote storage, or read

00:09:09.420 --> 00:09:13.000
data out of a database or some other remote data storage system.

00:09:13.000 --> 00:09:14.460
This is tabular data.

00:09:14.460 --> 00:09:16.760
So it's structured data like with columns.

00:09:16.760 --> 00:09:20.640
You can think of it like a spreadsheet or some other tabular data set.

00:09:20.640 --> 00:09:27.240
And then it provides you with this DataFrame object, which is kind of pandas.DataFrame

00:09:27.240 --> 00:09:29.560
that is the main tabular data object.

00:09:29.560 --> 00:09:36.840
And it has a ton of methods for accessing, slicing, grabbing subsets of the data, applying

00:09:36.840 --> 00:09:42.320
functions on it that do filtering and subsetting and selection, as well as more analytical

00:09:42.320 --> 00:09:46.960
operations like things that you might do with a database system or SQL.

00:09:46.960 --> 00:09:53.280
So joins and lookups, as well as analytical functions like summary statistics, grouping

00:09:53.280 --> 00:09:57.040
by some key and producing summary statistics.

00:09:57.040 --> 00:10:03.860
So it's basically a Swiss Army knife for doing data manipulation, data cleaning, and supporting

00:10:03.860 --> 00:10:05.560
the data analysis workflow.

00:10:05.560 --> 00:10:09.960
But it doesn't actually include very much as far as actual statistics or models.

00:10:09.960 --> 00:10:16.000
Or if you're doing something with LLMs or linear regression, or some type of machine

00:10:16.000 --> 00:10:18.000
learning, you have to use another library.

00:10:18.000 --> 00:10:23.200
But pandas is the on-ramp for all of the data into your environment in Python.

00:10:23.200 --> 00:10:29.000
So when people are building some kind of application that touches data in Python, pandas is often

00:10:29.000 --> 00:10:34.880
like the initial on-ramp for how data gets into Python, where you clean up the data,

00:10:34.880 --> 00:10:41.760
you regularize it, you get it ready for analysis, and then you feed the clean data into the

00:10:41.760 --> 00:10:46.040
downstream statistical library or data analysis library that you're using.

00:10:46.040 --> 00:10:48.800
- That whole data wrangling side of things, right?

00:10:48.800 --> 00:10:50.480
- Yeah, that's right, that's right.

00:10:50.480 --> 00:10:56.100
And so, you know, in some history, Python had arrays like matrices and what we call

00:10:56.100 --> 00:11:02.400
tensors now, multi-dimensional arrays going back all the way to 1995, which is pretty

00:11:02.400 --> 00:11:04.760
early history for Python.

00:11:04.760 --> 00:11:09.480
Like the Python programming language has only been around since like 1990 or 1991, if my

00:11:09.480 --> 00:11:10.880
memory serves.

00:11:10.880 --> 00:11:19.720
But what became NumPy in 2005, 2006, started out as numeric in 1995, and it provided numerical

00:11:19.720 --> 00:11:24.800
computing, multi-dimensional arrays, matrices, the kind of stuff that you might do in MATLAB,

00:11:24.800 --> 00:11:30.640
but it was mainly focused on numerical computing and not with the type of business datasets

00:11:30.640 --> 00:11:36.160
that you find in database systems, which contain a lot of strings or dates or non-numeric data.

00:11:36.160 --> 00:11:41.440
And so my initial interest was I found Python to be a really productive programming language.

00:11:41.440 --> 00:11:45.240
I really liked writing code in it, writing simple scripts, like, you know, doing random

00:11:45.240 --> 00:11:47.520
things for my job.

00:11:47.520 --> 00:11:51.840
But then you had this numerical computing library, NumPy, which enabled you to work

00:11:51.840 --> 00:11:57.200
with large numeric arrays and large datasets with a single data type.

00:11:57.200 --> 00:12:00.680
But working with this more tabular type data, stuff that you would do in Excel or stuff

00:12:00.680 --> 00:12:05.200
that you do in a database, it wasn't very easy to do that with NumPy or it wasn't really

00:12:05.200 --> 00:12:06.200
designed for that.

00:12:06.200 --> 00:12:11.520
And so that's what led to building this, like, higher level library that deals with these

00:12:11.520 --> 00:12:16.760
tabular datasets in the Pandas library, which was originally focused on, you know, building

00:12:16.760 --> 00:12:19.360
really with a really close relationship with NumPy.

00:12:19.360 --> 00:12:24.320
So Pandas itself was like a thin layer on top of NumPy originally.

00:12:24.320 --> 00:12:27.320
This portion of Talk Python to Me is brought to you by Neo4j.

00:12:27.320 --> 00:12:32.560
I have told you about Neo4j, the native graph database on previous AdSpots.

00:12:32.560 --> 00:12:37.480
This time, I want to tell you about their relatively new podcast, Graph Stuff.

00:12:37.480 --> 00:12:41.240
If you care about graph databases and modeling with graphs, you should definitely give it

00:12:41.240 --> 00:12:42.400
a listen.

00:12:42.400 --> 00:12:47.160
On their season finale last year, they talked about the intersection of LLMs and knowledge

00:12:47.160 --> 00:12:48.160
graphs.

00:12:48.160 --> 00:12:50.320
Remember when ChatGPT launched?

00:12:50.320 --> 00:12:54.160
It felt like the LLM was a magical tool out of the toolbox.

00:12:54.160 --> 00:12:57.920
However, the more you use it, the more you realize that's not the case.

00:12:57.920 --> 00:13:03.000
The technology is brilliant, but it's prone to issues such as hallucinations.

00:13:03.000 --> 00:13:04.000
But there's hope.

00:13:04.000 --> 00:13:09.400
If you feed the LLM reliable current data, ground it in the right data and context, then

00:13:09.400 --> 00:13:12.960
it can make the right connections and give the right answers.

00:13:12.960 --> 00:13:18.680
On the episode, the team at Neo4j explores how to get the results by pairing LLMs with

00:13:18.680 --> 00:13:21.400
knowledge graphs and vector search.

00:13:21.400 --> 00:13:24.360
Check out their podcast episode on Graph Stuff.

00:13:24.360 --> 00:13:28.000
They share tips for retrieval methods, prompt engineering, and more.

00:13:28.000 --> 00:13:35.600
So just visit talkpython.fm/neo4j-graphstuff to listen to an episode.

00:13:35.600 --> 00:13:39.680
That's talkpython.fm/neo4j-graphstuff.

00:13:39.680 --> 00:13:42.440
The link is in your podcast players show notes.

00:13:42.440 --> 00:13:46.600
Thank you to Neo4j for supporting Talk Python to me.

00:13:46.600 --> 00:13:52.360
One thing I find interesting about Pandas is it's almost its own programming environment

00:13:52.360 --> 00:13:59.160
these days in the sense that, you know, traditional Python, we do a lot of loops, we do a lot

00:13:59.160 --> 00:14:05.720
of attribute dereferencing, function calling, and a lot of what happens in Pandas is more

00:14:05.720 --> 00:14:07.280
functional.

00:14:07.280 --> 00:14:09.160
It's more applied to us.

00:14:09.160 --> 00:14:11.800
It's almost like set operations, right?

00:14:11.800 --> 00:14:14.480
And a lot of vector operations and so on.

00:14:14.480 --> 00:14:17.280
Yeah, that was behavior that was inherited from NumPy.

00:14:17.280 --> 00:14:21.100
So NumPy is very array oriented, vector oriented.

00:14:21.100 --> 00:14:26.720
So you rather than write a for loop, you would write an array expression, which would operate

00:14:26.720 --> 00:14:32.920
on whole batches of data in a single function call, which is a lot faster because you can

00:14:32.920 --> 00:14:37.260
drop down into C code and get good performance that way.

00:14:37.260 --> 00:14:43.880
And so Pandas adopted the NumPy way of like the NumPy like array expression or vector

00:14:43.880 --> 00:14:44.880
operations.

00:14:44.880 --> 00:14:49.080
And I'm curious to hear that that's extended to the types of like non-numeric data operations

00:14:49.080 --> 00:14:53.400
that you can do in Pandas, like, you know, vectorized set lookups, where you can say

00:14:53.400 --> 00:14:57.240
like, you would say like, oh, like this, I have this array of strings, and I have this

00:14:57.240 --> 00:15:01.860
subset of strings, and I want to compute a Boolean array, which says whether or not each

00:15:01.860 --> 00:15:04.260
string is contained in this set of strings.

00:15:04.260 --> 00:15:06.800
And so in Pandas, that's the is in function.

00:15:06.800 --> 00:15:12.320
So you would say like, column A, like is in some set of substrings, and that produces

00:15:12.320 --> 00:15:17.020
that single function call produces a whole Boolean array that you can use for subsetting

00:15:17.020 --> 00:15:18.020
later on.

00:15:18.020 --> 00:15:20.580
Yeah, there's a ton of things that are really interesting in there.

00:15:20.580 --> 00:15:23.800
One of the challenges, maybe you could speak to this a little bit, then I want to come

00:15:23.800 --> 00:15:25.480
back to your performance comment.

00:15:25.480 --> 00:15:29.680
One of the challenges I think is that some of these operations are not super obvious

00:15:29.680 --> 00:15:32.640
that they exist or that they're discoverable, right?

00:15:32.640 --> 00:15:37.500
Like instead of just indexing into say a column, you can index on an expression that might

00:15:37.500 --> 00:15:40.920
filter out the columns or project them or things like that.

00:15:40.920 --> 00:15:45.960
How do you recommend people like kind of discover a little bigger breadth of what they can do?

00:15:45.960 --> 00:15:48.880
There's plenty of great books written about Pandas.

00:15:48.880 --> 00:15:51.160
So there's my book, Python for Data Analysis.

00:15:51.160 --> 00:15:55.240
I think Matt Harrison has written an excellent book, Effective Pandas.

00:15:55.240 --> 00:15:59.820
Pandas documentation, I think has provides really nitty gritty detail about how all the

00:15:59.820 --> 00:16:01.640
different things work.

00:16:01.640 --> 00:16:06.720
But when I was writing my this book, Python for Data Analysis, my goal was to, you know,

00:16:06.720 --> 00:16:12.200
create a primer, like a tutorial on how to solve data problems with Pandas.

00:16:12.200 --> 00:16:16.800
And so for that, you know, I had to introduce some basics of how NumPy works so people could

00:16:16.800 --> 00:16:21.840
understand array oriented computing, basics of Python, so you know enough Python to be

00:16:21.840 --> 00:16:25.520
able to understand what things that Pandas is doing.

00:16:25.520 --> 00:16:26.520
It builds incrementally.

00:16:26.520 --> 00:16:30.520
And so like, as you go through the book, the content gets more and more advanced.

00:16:30.520 --> 00:16:35.560
It introduces, you learn, you master initial set of techniques, and then you can start

00:16:35.560 --> 00:16:37.720
learning about more advanced techniques.

00:16:37.720 --> 00:16:41.160
So it's definitely a pedagogical resource.

00:16:41.160 --> 00:16:44.840
And it is now freely as you as you're showing there on the screen, it is freely available

00:16:44.840 --> 00:16:46.000
on the internet.

00:16:46.000 --> 00:16:52.440
So JJ Allaire helped me port the book to use Quarto, which is a new technical publishing

00:16:52.440 --> 00:16:57.080
system for writing books and blogs and website, you know, quarto.org.

00:16:57.080 --> 00:17:03.180
And yes, that's how I was able to publish my book on the internet, as you know, essentially,

00:17:03.180 --> 00:17:06.720
you can use Quarto to write books using Jupyter notebooks, which is cool.

00:17:06.720 --> 00:17:10.200
My book was written a long time ago in O'Reilly's DocBook XML.

00:17:10.200 --> 00:17:12.000
So not particularly fun to edit.

00:17:12.000 --> 00:17:18.680
But yeah, because Quarto is built on Pandoc, which is a sort of markup language transpilation

00:17:18.680 --> 00:17:19.680
system.

00:17:19.680 --> 00:17:23.420
So you can use Pandoc to convert from one, you know, you can to convert documents from

00:17:23.420 --> 00:17:25.320
one format to another.

00:17:25.320 --> 00:17:30.320
And so that's the kind of the root framework that Quarto is built on for, you know, generating

00:17:30.320 --> 00:17:33.920
starting with one document format and generating many different types of output formats.

00:17:33.920 --> 00:17:34.920
That's cool.

00:17:34.920 --> 00:17:38.760
I didn't realize your book was available just to read on the internet.

00:17:38.760 --> 00:17:40.680
If you navigate around.

00:17:40.680 --> 00:17:45.960
In the third edition, I was able to negotiate with O'Reilly and add a, you know, add an

00:17:45.960 --> 00:17:52.400
append and make an amendment to my very old book contract from 2011 to let me release

00:17:52.400 --> 00:17:54.800
the book for free on my website.

00:17:54.800 --> 00:17:58.960
So it's yeah, it's just available there at westmckinney.com/book.

00:17:58.960 --> 00:18:02.500
I find that like a lot of people really like the print book.

00:18:02.500 --> 00:18:06.720
And so I think that having the online book just available, like whenever you are somewhere

00:18:06.720 --> 00:18:08.280
and you want to look something up is great.

00:18:08.280 --> 00:18:09.560
Print books are hard to search.

00:18:09.560 --> 00:18:10.560
Yeah, that's true.

00:18:10.560 --> 00:18:11.560
That's true.

00:18:11.560 --> 00:18:12.560
Yeah.

00:18:12.560 --> 00:18:14.800
And like, if you go to the search bar, and if you go back to the book and just look at

00:18:14.800 --> 00:18:18.640
the search bar, you know, look at just search for like group by like, you know, all one

00:18:18.640 --> 00:18:21.860
word or you know, yeah, it's like, it comes up really fast.

00:18:21.860 --> 00:18:23.320
You can go to that section.

00:18:23.320 --> 00:18:25.080
And it's pretty cool.

00:18:25.080 --> 00:18:29.600
I thought that releasing the book for free online would would affect sales, but people

00:18:29.600 --> 00:18:33.040
just really like having paper books, it seems even in 2024.

00:18:33.040 --> 00:18:34.880
Yeah, even digital books are nice.

00:18:34.880 --> 00:18:38.120
You got them with you all the time you can I think it's about taking the notes, where

00:18:38.120 --> 00:18:39.120
do I put my highlights?

00:18:39.120 --> 00:18:40.520
And how do I remember it?

00:18:40.520 --> 00:18:41.520
And that's right.

00:18:41.520 --> 00:18:42.900
Yeah, yeah, stuff like that.

00:18:42.900 --> 00:18:45.520
This quarter thing looks super interesting.

00:18:45.520 --> 00:18:53.040
If you look at Pandoc, if people haven't looked at this before, the conversion matrix.

00:18:53.040 --> 00:18:57.160
I don't know how you would how would you describe this was busy and complete?

00:18:57.160 --> 00:18:58.160
What is this?

00:18:58.160 --> 00:18:59.160
This is crazy.

00:18:59.160 --> 00:19:00.160
It's very busy.

00:19:00.160 --> 00:19:05.200
Yeah, it's it's it can convert from looks like about, you know, 30 or 40 input formats,

00:19:05.200 --> 00:19:10.040
formats to, you know, 50 or 60 output formats, maybe maybe more than that, kind of just my

00:19:10.040 --> 00:19:11.040
just eyeballing it.

00:19:11.040 --> 00:19:12.840
But yeah, it's like a pretty, pretty impressive.

00:19:12.840 --> 00:19:16.840
And then if you took the combinatorial of like, how many different ways could you combine

00:19:16.840 --> 00:19:18.480
the 30 to the 50?

00:19:18.480 --> 00:19:19.480
It's kind of what it looks like.

00:19:19.480 --> 00:19:20.480
It's right.

00:19:20.480 --> 00:19:21.480
It's truly amazing.

00:19:21.480 --> 00:19:26.880
So you've got Markdown, you want to turn it into a PDF, or you've got a Doku wiki and

00:19:26.880 --> 00:19:29.240
you want to turn it into an EPUB or whatever, right?

00:19:29.240 --> 00:19:33.600
Or even like reveal JS, probably to PowerPoint, I would imagine.

00:19:33.600 --> 00:19:34.600
I don't know.

00:19:34.600 --> 00:19:35.600
Yeah.

00:19:36.600 --> 00:19:38.560
As history like backstory about about quarto.

00:19:38.560 --> 00:19:44.200
So you know, it helps to keep in mind that that JJ created cold fusion, which was this,

00:19:44.200 --> 00:19:48.280
you know, essentially publishing system, early publishing system for the internet, similar

00:19:48.280 --> 00:19:53.960
to CGI and, and PHP and another dynamic web publishing systems.

00:19:53.960 --> 00:19:59.580
And so at early on it at our studio, they created our Markdown, which is a basically

00:19:59.580 --> 00:20:05.560
a extensions to Markdown that allow you to have code cells written in, in our and then

00:20:05.560 --> 00:20:08.920
eventually they added support for some other languages where it's kind of like a Jupiter

00:20:08.920 --> 00:20:12.400
notebook in the sense that you could have some Markdown and some code and some plots

00:20:12.400 --> 00:20:17.640
and output, and you would run the arm Markdown render and it would it would, it would, you

00:20:17.640 --> 00:20:21.120
know, generate all the output and insert it into the document.

00:20:21.120 --> 00:20:24.800
And so you could use that to write blogs and websites and everything.

00:20:24.800 --> 00:20:29.160
But but our markdown was written in our and so that limited in a sense, like it made it

00:20:29.160 --> 00:20:32.680
harder to install because you would have to install our to use it.

00:20:32.680 --> 00:20:37.800
And also people, it had an association with our that perhaps was like, like unmerited.

00:20:37.800 --> 00:20:42.160
And so in the meantime, you know, with all everything that's happened with web technology,

00:20:42.160 --> 00:20:47.440
it's now very easy to put a complete JavaScript engine in a stall footprint, you know, on

00:20:47.440 --> 00:20:53.720
a machine with no dependencies, and to be able to run a system that is, you know, written

00:20:53.720 --> 00:20:55.920
in a system that's written in JavaScript.

00:20:55.920 --> 00:21:01.360
And so Cordo is completely language agnostic, it's written in TypeScript, and uses Pandoc

00:21:01.360 --> 00:21:03.320
as an as an underlying engine.

00:21:03.320 --> 00:21:05.360
And it's very easy to install.

00:21:05.360 --> 00:21:09.840
And so it addresses some of the portability and extensibility issues that were that were

00:21:09.840 --> 00:21:11.400
present in our Markdown.

00:21:11.400 --> 00:21:16.720
But but as a result, you know, I think our the the Posit team had a lot of just has more

00:21:16.720 --> 00:21:20.400
than a, you know, a decade, or if you include cold fusion, you know, more than 25 years

00:21:20.400 --> 00:21:25.640
of experience and in building really developer friendly technical publishing tools.

00:21:25.640 --> 00:21:30.440
And so I think that that it's not data science, but it's something that is an important part

00:21:30.440 --> 00:21:35.640
of the data science workflow, which is, how do you present your make your analysis and

00:21:35.640 --> 00:21:38.560
your work available for consumption in different formats.

00:21:38.560 --> 00:21:43.600
And so having this this system that can, you know, publish outputs in in many different

00:21:43.600 --> 00:21:46.200
places is super valuable.

00:21:46.200 --> 00:21:49.600
So a lot of people start out in Jupyter notebooks, but but there's many different, you know,

00:21:49.600 --> 00:21:51.520
many different possible input formats.

00:21:51.520 --> 00:21:56.160
And so to be able to, you know, use the same source to publish to a website or to a confluence

00:21:56.160 --> 00:21:59.120
page or to a PDF is like, super valuable.

00:21:59.120 --> 00:22:00.480
Yeah, it's super interesting.

00:22:00.480 --> 00:22:03.040
Okay, so then I got to explore some more.

00:22:03.040 --> 00:22:05.640
Let's go back to this for a minute.

00:22:05.640 --> 00:22:08.560
Just how about some kind words from the audience for you?

00:22:08.560 --> 00:22:11.120
Ravid says, Wes, your work has changed my life.

00:22:11.120 --> 00:22:12.600
It's very, very nice.

00:22:12.600 --> 00:22:13.880
I'm happy to hear it.

00:22:13.880 --> 00:22:16.680
But yeah, yeah, I'm more than happy to talk.

00:22:16.680 --> 00:22:18.240
Let's talk in depth about pandas.

00:22:18.240 --> 00:22:20.960
And I think history of the project is interesting.

00:22:20.960 --> 00:22:28.200
And I think also how the project has has developed in the intervening 1516 years is pretty interesting

00:22:28.200 --> 00:22:29.200
as well.

00:22:29.200 --> 00:22:30.840
Yeah, let's talk derivatives for a minute.

00:22:30.840 --> 00:22:34.160
So growth and speed of adoption and all those things.

00:22:34.160 --> 00:22:37.880
When you first started working on this, and you first put it out, did you foresee a world

00:22:37.880 --> 00:22:42.000
where this was so popular and so important?

00:22:42.000 --> 00:22:46.400
Did you think of Yeah, pretty soon black holes, I'm pretty sure I'll be part of that somehow.

00:22:46.400 --> 00:22:52.840
It was always the aspiration of making Python this mainstream language for statistical computing

00:22:52.840 --> 00:22:54.800
and data analysis.

00:22:54.800 --> 00:22:59.600
Like I didn't, it didn't occur to me that it would become this popular or that it would

00:22:59.600 --> 00:23:04.640
become like the one of the main tools that people use for working with data in a business

00:23:04.640 --> 00:23:08.560
in a business setting like that would have been if that was the aspiration, or if that

00:23:08.560 --> 00:23:12.160
was, you know, what I needed to achieve to be satisfied, that that would have been completely

00:23:12.160 --> 00:23:13.360
unreasonable.

00:23:13.360 --> 00:23:18.000
And I also don't think that in a certain sense, like, I don't know that that it's popularity,

00:23:18.000 --> 00:23:19.880
it is deserved, and it's not deserved.

00:23:19.880 --> 00:23:23.960
Like I think there's there's many other worthy efforts that have been created over the years

00:23:23.960 --> 00:23:28.720
that have been really great work that that others have have done in this this domain.

00:23:28.720 --> 00:23:33.720
And so the fact that that pandas caught on and became as popular as it is, I think it's

00:23:33.720 --> 00:23:35.800
a combination of timing.

00:23:35.800 --> 00:23:40.120
And you know, there was like a developer relations aspect that there was content available.

00:23:40.120 --> 00:23:43.560
And like I wrote my book, and that made it easier for people to learn how to use the

00:23:43.560 --> 00:23:44.560
project.

00:23:44.560 --> 00:23:49.040
But also like we we had a serendipitous open source developer community that that came

00:23:49.040 --> 00:23:55.160
together that allowed the project to grow and expand like really rapidly in the early

00:23:55.160 --> 00:23:56.360
2010s.

00:23:56.360 --> 00:24:01.000
And I definitely spent a lot of work like recruiting people to work on the project and

00:24:01.000 --> 00:24:04.480
encouraging you know, others to work on it, because sometimes people create open source

00:24:04.480 --> 00:24:08.280
projects, and then it's hard for hard for others to get involved and get a seat at the

00:24:08.280 --> 00:24:09.700
table, so to speak.

00:24:09.700 --> 00:24:16.080
But I was very keen to bring on others and to give them responsibility and you know,

00:24:16.080 --> 00:24:20.080
ultimately, you know, hand over the reins to the project to others.

00:24:20.080 --> 00:24:23.680
And that I've spoken a lot about that, you know, over the years, how important that is

00:24:23.680 --> 00:24:28.880
to you know, for open source project creators to to make room for others in, you know, steering

00:24:28.880 --> 00:24:32.000
and growing the project so that they can become owners of it as well.

00:24:32.000 --> 00:24:36.080
It's tough to make space and tough to bring on folks.

00:24:36.080 --> 00:24:38.080
Have you heard of the Djangonauts?

00:24:38.080 --> 00:24:39.080
The Djangonauts?

00:24:39.080 --> 00:24:40.480
I think it's Djangonauts dot space.

00:24:40.480 --> 00:24:42.000
They have an awesome domain.

00:24:42.000 --> 00:24:46.280
But it's basically like kind of like a boot camp, but it's for taking people who just

00:24:46.280 --> 00:24:50.600
like Django and turn them into actually contributors or core contributors.

00:24:50.600 --> 00:24:53.800
What's your onboarding story for people who do want to participate?

00:24:53.800 --> 00:24:58.280
I'm embarrassed to say that I'm not, I don't have a comprehensive view of like all of the

00:24:58.280 --> 00:25:02.840
different, you know, community outreach channels that the pandas project has done to help grow

00:25:02.840 --> 00:25:04.600
new contributors.

00:25:04.600 --> 00:25:12.100
So one of the core team members, Mark Garcia, has done an amazing job organizing documentation

00:25:12.100 --> 00:25:18.640
sprints and other like contributor sourcing events, essentially creating very friendly,

00:25:18.640 --> 00:25:22.520
accessible events where people who are interested in getting involved in pandas can meet each

00:25:22.520 --> 00:25:27.800
other and then assist each other in making their first pull request.

00:25:27.800 --> 00:25:31.600
And it could be something as simple as, you know, making a small improvement to the to

00:25:31.600 --> 00:25:34.520
the pandas documentation because it's such a large project.

00:25:34.520 --> 00:25:40.120
The documentation is like always something that could be better, you know, either adding

00:25:40.120 --> 00:25:44.880
more adding more examples or documenting things that aren't documented or making, yeah, just

00:25:44.880 --> 00:25:46.680
just making the documentation better.

00:25:46.680 --> 00:25:52.380
And so it's something that for new contributors is more accessible than working on the internals

00:25:52.380 --> 00:25:55.280
of like one of the algorithms or something.

00:25:55.280 --> 00:26:00.320
And, and, or like we working on some significant performance improvement might be a bit intimidating

00:26:00.320 --> 00:26:02.240
if you've never worked on the pandas code base.

00:26:02.240 --> 00:26:05.420
And it's a pretty large code base because it's been it's been worked on continuously

00:26:05.420 --> 00:26:08.000
for you know, for like going on 20 years.

00:26:08.000 --> 00:26:13.880
So it's, yeah, it can be takes a while to really get to a place where you can be productive.

00:26:13.880 --> 00:26:18.780
And that can be discouraging for new contributors, especially those who don't have a lot of open

00:26:18.780 --> 00:26:19.780
source experience.

00:26:19.780 --> 00:26:24.440
That's one of the ironies of the challenges of these big projects is they're just so finely

00:26:24.440 --> 00:26:25.520
polished.

00:26:25.520 --> 00:26:27.220
So many people are using them.

00:26:27.220 --> 00:26:29.240
Every edge case matters to somebody.

00:26:29.240 --> 00:26:30.240
Right.

00:26:30.240 --> 00:26:34.440
And so to become a contributor and make changes to that, it takes a while, I'm sure.

00:26:34.440 --> 00:26:35.440
Yeah, yeah.

00:26:35.440 --> 00:26:39.320
I mean, I think it's definitely a big thing that helped is allowing people to get paid

00:26:39.320 --> 00:26:46.520
to work on pandas or to be able to contribute to pandas as, as a part of their job description,

00:26:46.520 --> 00:26:50.280
like as you know, maybe part of their job is maintaining, maintaining pandas.

00:26:50.280 --> 00:26:55.400
So Anaconda, you know, was like, you know, one of the earliest companies who had engineers

00:26:55.400 --> 00:27:01.280
on staff, you know, like, you know, Brock Mendel, Tom Augsberger, Jeff Reback, who part

00:27:01.280 --> 00:27:03.760
of their job was maintaining and developing, developing pandas.

00:27:03.760 --> 00:27:09.000
And that was, that was huge because prior to that, the project was purely based on volunteers.

00:27:09.000 --> 00:27:13.880
Like I, I was a volunteer and everyone was working on the project as a, as a passion

00:27:13.880 --> 00:27:15.920
project in their, in their free time.

00:27:15.920 --> 00:27:21.440
And then Travis Oliphant, one of the founders, he and Peter Wang founded Anaconda.

00:27:21.440 --> 00:27:26.120
Travis spun out from Anaconda to create Quonsight and has continued to sponsor development in

00:27:26.120 --> 00:27:27.120
and around pandas.

00:27:27.120 --> 00:27:32.200
And that's enabled people like Mark to do these community building events and, and for

00:27:32.200 --> 00:27:35.240
it to not be, you know, something that's, you know, totally uncompensated.

00:27:35.240 --> 00:27:36.240
Yeah.

00:27:36.240 --> 00:27:37.920
That's a lot of, a lot of stuff going on.

00:27:37.920 --> 00:27:40.600
And I think the interest is awesome, right?

00:27:40.600 --> 00:27:45.200
I mean, if it was just a different level of problems, I feel like we could take on, you

00:27:45.200 --> 00:27:48.840
know, you know what, I got this entire week and someone that's my job is to make this

00:27:48.840 --> 00:27:53.440
work rather than I've, I've got two hours and can't really take on a huge project.

00:27:53.440 --> 00:27:56.440
And so I'll work on the smaller improvements or whatever.

00:27:56.440 --> 00:27:57.440
Yeah.

00:27:57.440 --> 00:28:01.280
Many people know, but I, I haven't been involved day to day in pandas since 2013.

00:28:01.280 --> 00:28:02.880
So that's, that's getting on.

00:28:02.880 --> 00:28:03.880
That's a lot of years.

00:28:03.880 --> 00:28:06.400
I still talk to the pandas contributors.

00:28:06.400 --> 00:28:10.640
We had a, we had a pandas meetup core, core developer meetup here in Nashville pre COVID.

00:28:10.640 --> 00:28:13.440
I think it was in 2019, maybe.

00:28:13.440 --> 00:28:18.480
So I'm still in active contact with the pandas developers, but it's been a different team

00:28:18.480 --> 00:28:20.640
of people leading the project.

00:28:20.640 --> 00:28:23.200
It's taken on a life of its own, which is, which is amazing.

00:28:23.200 --> 00:28:24.280
That's exactly, yeah.

00:28:24.280 --> 00:28:28.600
As a project creator, that's exactly what you want is to not be beholden to the project

00:28:28.600 --> 00:28:32.760
that you created and forced and, you know, kind of have to be, be responsible for it

00:28:32.760 --> 00:28:35.520
and take care of it for the rest of your life.

00:28:35.520 --> 00:28:39.000
But if you look at like a lot of the community, a lot of the most kind of intensive community

00:28:39.000 --> 00:28:43.640
development has happened since, like, since I moved on to work on, on other projects.

00:28:43.640 --> 00:28:49.120
And so now the project has, I don't know the exact count, but 10 thousands of contributors.

00:28:49.120 --> 00:28:53.440
And so, you know, to have thousands of different unique individuals contributing to an open

00:28:53.440 --> 00:28:55.840
source project is a, it's a big deal.

00:28:55.840 --> 00:29:00.880
So I think even, I don't know what it says on the bottom of GitHub, it says, you know,

00:29:00.880 --> 00:29:07.000
3,200 contributors, but that's maybe not even the full story because sometimes, you know,

00:29:07.000 --> 00:29:11.640
people, they don't have their email address associated with their GitHub profile and, you

00:29:11.640 --> 00:29:13.480
know, how GitHub counts contributors.

00:29:13.480 --> 00:29:16.280
I would say probably the true number is closer to 4,000.

00:29:16.280 --> 00:29:20.520
That's a testament, you know, to the, to the core team and all the outreach that they've

00:29:20.520 --> 00:29:25.620
done and work making, making the project accessible and easy to contribute to.

00:29:25.620 --> 00:29:29.680
Because if people, if you go and try to make a pull request to a project and there's many

00:29:29.680 --> 00:29:31.880
different ways that you can fail.

00:29:31.880 --> 00:29:36.920
So like either the project is technically like there's issues with the build system

00:29:36.920 --> 00:29:38.720
or the developer tooling.

00:29:38.720 --> 00:29:40.440
And so you struggle with the developer tooling.

00:29:40.440 --> 00:29:43.800
And so if you aren't working on it every day and every night, you can't make heads or tails

00:29:43.800 --> 00:29:45.640
of how the developer tools work.

00:29:45.640 --> 00:29:50.040
But then there's also like the level of accessibility of the core development team.

00:29:50.040 --> 00:29:54.600
Like if they don't, if they aren't there to support you in getting involved in the project

00:29:54.600 --> 00:29:59.760
and learning how it works and creating documentation about how to contribute and what's expected

00:29:59.760 --> 00:30:04.400
of you, that can also be, you know, a source of frustration where people churn out of the

00:30:04.400 --> 00:30:09.060
project, you know, because it's just, it's too hard to find their sea legs.

00:30:09.060 --> 00:30:14.600
And maybe also, you know, sometimes development teams are unfriendly or unhelpful or, you

00:30:14.600 --> 00:30:18.840
know, they make, they make others feel like they make others feel like they're annoyed

00:30:18.840 --> 00:30:20.960
with them or like they're wasting their time or something.

00:30:20.960 --> 00:30:24.840
It's like, I don't want to look at your, you know, this pull request and give you feedback

00:30:24.840 --> 00:30:28.200
because you know, I could do it more quickly by myself or something.

00:30:28.200 --> 00:30:30.760
Like sometimes you see that an open source projects.

00:30:30.760 --> 00:30:33.760
But they've created a very welcoming environment.

00:30:33.760 --> 00:30:37.080
And yeah, I think the contribution numbers speak for themselves.

00:30:37.080 --> 00:30:38.080
- They definitely do.

00:30:38.080 --> 00:30:41.760
Maybe the last thing before we move on to the other stuff you're working on, but the

00:30:41.760 --> 00:30:47.720
other interesting GitHub statistic here is the used by 1.6 million projects.

00:30:47.720 --> 00:30:50.320
That's, I don't know if I've ever seen it used by that high.

00:30:50.320 --> 00:30:52.400
There's probably some that are higher, but not many.

00:30:52.400 --> 00:30:54.160
- Yeah, it's a lot of projects.

00:30:54.160 --> 00:30:55.640
I think it's, it's interesting.

00:30:55.640 --> 00:30:59.920
I think like many projects, it's reached a point where it's, it's an essential and assumed

00:30:59.920 --> 00:31:03.240
part of the, of many people's toolkit.

00:31:03.240 --> 00:31:07.200
Like they, like the first thing that they write at the top of a file that they're working

00:31:07.200 --> 00:31:12.880
on is import pandas as PD or import NumPy as PD, you know, to create, I think in a sense,

00:31:12.880 --> 00:31:16.360
like I think one of the reasons why, you know, pandas has gotten so popular is that it is

00:31:16.360 --> 00:31:21.160
beneficial to the community, to the Python community to have fewer solutions, you know,

00:31:21.160 --> 00:31:25.160
kind of the Zen of Python, there should be one and preferably only one obvious, obvious

00:31:25.160 --> 00:31:26.800
way to do things.

00:31:26.800 --> 00:31:32.320
And so if there were 10 different pandas like projects, you know, that creates skill portability

00:31:32.320 --> 00:31:36.640
problems and it's just easier if everyone says, oh, we just, pandas is the thing that

00:31:36.640 --> 00:31:40.920
we use and you change jobs and you can take all your skills, like how to use pandas with

00:31:40.920 --> 00:31:41.920
you.

00:31:41.920 --> 00:31:45.120
And I think that's also one of the reasons why Python has become so successful in the

00:31:45.120 --> 00:31:50.680
business world is because you can teach somebody even without a lot of programming experience,

00:31:50.680 --> 00:31:56.740
how to use Python, how to use pandas and become productive doing basic work very, very quickly.

00:31:56.740 --> 00:32:02.680
And so one of the solutions I remember back in the early 2010s, there were a lot of articles

00:32:02.680 --> 00:32:06.340
and talks about how to address the data science shortage.

00:32:06.340 --> 00:32:16.620
And my belief and I gave a talk at Web Summit in Dublin in 2000, gosh, maybe 2017, I have

00:32:16.620 --> 00:32:17.900
to look exactly.

00:32:17.900 --> 00:32:21.180
But basically it was the data scientist shortage.

00:32:21.180 --> 00:32:25.780
And my thesis was always, we should make it easier to be a data scientist or like lower

00:32:25.780 --> 00:32:31.620
the bar for like what sort of skills you have to master before you can do productive work

00:32:31.620 --> 00:32:32.620
in a business setting.

00:32:32.620 --> 00:32:36.500
And so I think the fact that there is just pandas and that's like the one thing that

00:32:36.500 --> 00:32:41.860
people have to learn how to use is like their essential starting point for doing any data

00:32:41.860 --> 00:32:47.540
work has also led to this piling on of like people being motivated to make this one thing

00:32:47.540 --> 00:32:52.340
better because you make improvements to pandas and they benefit millions of projects and

00:32:52.340 --> 00:32:53.740
millions of people around the world.

00:32:53.740 --> 00:32:59.240
And that's, yeah, so it's like a steady snowballing effect.

00:32:59.240 --> 00:33:04.560
This portion of Talk Python to Me is brought to you by Mailtrap, an email delivery platform

00:33:04.560 --> 00:33:06.440
that developers love.

00:33:06.440 --> 00:33:13.480
An email sending solution with industry best analytics, SMTP and email API SDKs for major

00:33:13.480 --> 00:33:17.800
programming languages and 24/7 human support.

00:33:17.800 --> 00:33:22.400
Try for free at mailtrap.io.

00:33:22.400 --> 00:33:24.760
I think doing data science is getting easier.

00:33:24.760 --> 00:33:27.240
We've got a lot of interesting frameworks and tools.

00:33:27.240 --> 00:33:32.840
Shiny for Python, one of them, right, that makes it easier to share and run your code,

00:33:32.840 --> 00:33:33.840
you know?

00:33:33.840 --> 00:33:38.800
Yeah, Shiny for Python, Streamlit, Dash, like these different interactive data application

00:33:38.800 --> 00:33:39.800
publishing frameworks.

00:33:39.800 --> 00:33:45.360
So you can go from a few lines of pandas code, loading some data and doing some analysis

00:33:45.360 --> 00:33:51.040
and visualization to publishing that as an interactive website without having to know

00:33:51.040 --> 00:33:56.040
how to use any web development frameworks or Node.js or anything like that.

00:33:56.040 --> 00:34:01.680
And so to be able to get up and running and build a working interactive web application

00:34:01.680 --> 00:34:07.640
that's powered by Python is, yeah, it's a game changer in terms of shortening end-to-end

00:34:07.640 --> 00:34:08.640
development life cycles.

00:34:08.640 --> 00:34:17.200
What do you think about JupyterLite and these PyOxidized and basically Jupyter in a browser

00:34:17.200 --> 00:34:19.920
type of things, WebAssembly and all that?

00:34:19.920 --> 00:34:24.760
Yeah, so I'm definitely very excited about it, been following WebAssembly in general.

00:34:24.760 --> 00:34:28.680
And so I guess some people listening will know about WebAssembly, but basically it's

00:34:28.680 --> 00:34:35.600
a portable machine code that can be compiled and executed within your browser in a sandbox

00:34:35.600 --> 00:34:36.600
environment.

00:34:36.600 --> 00:34:42.080
So it protects against security issues and allows, prevents like the person who wrote

00:34:42.080 --> 00:34:46.240
the WebAssembly code from doing something malicious on your machine, which is very important.

00:34:46.240 --> 00:34:49.440
Won't necessarily stop them from like, you know, mining cryptocurrency while you have

00:34:49.440 --> 00:34:50.440
the browser tab open.

00:34:50.440 --> 00:34:51.840
That's a whole separate problem.

00:34:51.840 --> 00:34:57.000
And it's enabled us to run the whole scientific Python stack, including Jupyter and NumPy

00:34:57.000 --> 00:35:03.000
and Pandas totally in the browser without having a client and server and needing to

00:35:03.000 --> 00:35:05.720
run a container someplace in the cloud.

00:35:05.720 --> 00:35:10.960
And so I think in terms of creating application deployment, so like being able to deploy an

00:35:10.960 --> 00:35:16.440
interactive data application like with Shiny, for example, without needing to have a server,

00:35:16.440 --> 00:35:17.560
that's actually pretty amazing.

00:35:17.560 --> 00:35:22.920
And so I think that simplifies, opens up new use cases, like new application architectures

00:35:22.920 --> 00:35:27.400
and make that makes things a lot easier for, because setting up and running a server creates

00:35:27.400 --> 00:35:29.640
brittleness, like it has cost.

00:35:29.640 --> 00:35:34.480
And so if the browser is doubling as your server process, like that's, I think that's

00:35:34.480 --> 00:35:35.480
really cool.

00:35:35.480 --> 00:35:41.800
You also have like other projects like DuckDB, which is a high performance, embeddable analytic

00:35:41.800 --> 00:35:43.060
SQL engine.

00:35:43.060 --> 00:35:49.080
And so now with DuckDB compiled to Wasm, you can get a high performance database running

00:35:49.080 --> 00:35:50.080
in your browser.

00:35:50.080 --> 00:35:55.320
And so you can get low latency, interactive queries and interactive dashboards.

00:35:55.320 --> 00:36:00.120
And so it's, yeah, there's WebAssembly has opened up this whole kind of new world of

00:36:00.120 --> 00:36:03.480
possibilities and it's transformative, I think.

00:36:03.480 --> 00:36:08.560
For Python in particular, you mentioned Pyodide, which is kind of a whole packaged stack.

00:36:08.560 --> 00:36:14.700
So it's like a framework for build and build and packaging and basically building an application

00:36:14.700 --> 00:36:16.140
and managing its dependencies.

00:36:16.140 --> 00:36:21.420
So you could create a WebAssembly version of your application to be deployed like this.

00:36:21.420 --> 00:36:26.260
But yeah, so I think one of the Pyodide, either the Pyodide main creator or maintainer went

00:36:26.260 --> 00:36:29.940
to Anaconda, they created PyScript, which made, which is another attempt to make it

00:36:29.940 --> 00:36:36.620
even easier to use Python to make it even easier to use Python to create web applications,

00:36:36.620 --> 00:36:37.620
interactive web applications.

00:36:37.620 --> 00:36:41.420
There's so many cool things here, like in the R community, they have WebR, which is

00:36:41.420 --> 00:36:46.860
similar to PyScript and Pyodide in some ways, like compiling the whole R stack to WebAssembly.

00:36:46.860 --> 00:36:50.860
There was just an article I saw on Hacker News where they worked on figuring out how

00:36:50.860 --> 00:36:57.260
to get, how to trick LLVM into compiling Fortran code, like legacy Fortran code to WebAssembly.

00:36:57.260 --> 00:37:00.780
Because when you're talking about all of this scientific computing stack, you need the linear

00:37:00.780 --> 00:37:06.020
algebra and all of the 40 years of Fortran code that have been built to support scientific

00:37:06.020 --> 00:37:07.020
applications.

00:37:07.020 --> 00:37:09.100
And you can compile to and run in the browser.

00:37:09.100 --> 00:37:12.220
So yeah, that's pretty wild to think of putting that in there, but very useful.

00:37:12.220 --> 00:37:16.820
I didn't realize that you could use DuckDB as a WebAssembly component.

00:37:16.820 --> 00:37:17.820
That's pretty cool.

00:37:17.820 --> 00:37:21.700
Yeah, there's a company, I'm not an investor or plugging them or anything, but it's called

00:37:21.700 --> 00:37:23.140
evidence.dev.

00:37:23.140 --> 00:37:28.060
It's like a whole like business intelligence, open source business intelligence application

00:37:28.060 --> 00:37:30.340
that's powered by, powered by DuckDB.

00:37:30.340 --> 00:37:34.860
And so if you have data that fits in the browser, you know, to have a whole like interactive

00:37:34.860 --> 00:37:39.220
dashboard or to be able to do business intelligence, like fully, like fully in the browser with

00:37:39.220 --> 00:37:41.740
no need of a, no need of a server.

00:37:41.740 --> 00:37:43.820
It's yeah, it's, it's very, very cool.

00:37:43.820 --> 00:37:48.980
So I've been following DuckDB since the, you know, since the early days and you know, my

00:37:48.980 --> 00:37:54.340
company Voltron Data, like we became members of the DuckDB foundation and built an actively

00:37:54.340 --> 00:37:56.660
built a relationship with, with DuckDB labs.

00:37:56.660 --> 00:38:02.340
So we could help accelerate progress in this space because I think the impact, the impact

00:38:02.340 --> 00:38:07.340
is so is so immense and we just, it's hard to predict like what you know, what people

00:38:07.340 --> 00:38:09.700
are going to build, build with all this stuff.

00:38:09.700 --> 00:38:13.900
And so that was all, you know, with, I guess going back, you know, 15 years ago to Python,

00:38:13.900 --> 00:38:19.620
like one of the reasons I became so passionate about building stuff for Python was about

00:38:19.620 --> 00:38:24.380
in I think the way that Peter Wang puts that, it puts it as, you know, giving people superpowers.

00:38:24.380 --> 00:38:29.980
So we want to enable people to build things with much less code and much less time.

00:38:29.980 --> 00:38:34.740
And so by making it things that much more accessible, that much easier to do, like the

00:38:34.740 --> 00:38:37.940
mantra in pandas was like, how do we make things one line of code?

00:38:37.940 --> 00:38:39.380
Or like this, that must be easy.

00:38:39.380 --> 00:38:41.300
It's like one line of code, one line of code.

00:38:41.300 --> 00:38:45.860
It must be like, like make this as terse and simple and easy to do as possible so that

00:38:45.860 --> 00:38:49.520
you can move on and focus on building the more interesting parts of your application

00:38:49.520 --> 00:38:54.740
rather than struggling with how to read a CSV file or you know, how to do whichever

00:38:54.740 --> 00:38:58.460
data munging technique that you need for your, for your data set.

00:38:58.460 --> 00:39:00.380
- That would be an interesting mental model for DuckDB.

00:39:00.380 --> 00:39:04.860
It's kind of an equivalent to SQLite, but more analytics database for folks, you know,

00:39:04.860 --> 00:39:07.060
in process and that kind of things, right?

00:39:07.060 --> 00:39:08.060
What do you think?

00:39:08.060 --> 00:39:09.860
- Yeah, so yeah, DuckDB is like SQLite.

00:39:09.860 --> 00:39:14.020
And in fact, it can run the whole SQLite test suite, I believe.

00:39:14.020 --> 00:39:17.700
So it's a full database, but it's for analytic processing.

00:39:17.700 --> 00:39:21.980
So it's optimized for analytic processing and as compared, you know, with SQLite, which

00:39:21.980 --> 00:39:23.420
is not for data processing.

00:39:23.420 --> 00:39:24.420
- Yeah, cool.

00:39:24.420 --> 00:39:29.540
All right, well, let's talk about some things that you're working on beyond pandas.

00:39:29.540 --> 00:39:31.260
You talked about Apache Arrow earlier.

00:39:31.260 --> 00:39:34.340
What are you doing with Arrow and how's it fit in your world?

00:39:34.340 --> 00:39:39.260
- The backstory there was, I don't know if you can hear the sirens in downtown Nashville,

00:39:39.260 --> 00:39:40.260
but.

00:39:40.260 --> 00:39:41.260
- No, actually, I don't hear the sirens.

00:39:41.260 --> 00:39:44.260
- It's good, the microphone filters it out, filters it out pretty well.

00:39:44.260 --> 00:39:47.180
- Yay for dynamic microphones, they're amazing.

00:39:47.180 --> 00:39:53.620
- Yeah, so in like around the mid, like the mid 2010s, 2015, I started working at Cloudera,

00:39:53.620 --> 00:39:58.860
like in the, which is a company that was like one of the pioneers in the big data ecosystem.

00:39:58.860 --> 00:40:03.740
And I had been spent several years working on five, five years, five, six years working

00:40:03.740 --> 00:40:04.740
on pandas.

00:40:04.740 --> 00:40:09.580
And so I'd gone through the experience of building pandas from top to bottom.

00:40:09.580 --> 00:40:15.140
And it was this full stack system that had had its own, you know, mini query engine,

00:40:15.140 --> 00:40:19.180
all of its own algorithms and data structures and all this stuff that we had to build from

00:40:19.180 --> 00:40:20.180
scratch.

00:40:20.180 --> 00:40:24.980
And I started thinking about, you know, what if it was possible to build some of the underlying

00:40:24.980 --> 00:40:31.420
computing technology, like data readers, like file readers, all the algorithms that power

00:40:31.420 --> 00:40:37.460
the core components of pandas, like group operations, aggregations, filtering, selection,

00:40:37.460 --> 00:40:38.460
all those things.

00:40:38.460 --> 00:40:43.300
Like what if it were possible to have a general purpose library that isn't specific to Python,

00:40:43.300 --> 00:40:48.180
isn't specific to pandas, but is really, really fast, really efficient, and has a large community

00:40:48.180 --> 00:40:52.880
building, building it so that you could take that code with you and use it to build many

00:40:52.880 --> 00:40:57.420
different types of libraries, not just data frame libraries, but also database engines

00:40:57.420 --> 00:41:00.640
and stream processing engines and all kinds of things.

00:41:00.640 --> 00:41:04.540
That was kind of what was in my mind when I started getting interested in what turned

00:41:04.540 --> 00:41:06.380
into turn into arrow.

00:41:06.380 --> 00:41:10.840
And one of the problems we realized we needed to solve this was like a group of other open

00:41:10.840 --> 00:41:15.820
source developers and me was that we needed to create a way to represent data that was

00:41:15.820 --> 00:41:19.180
not tied to a specific programming language.

00:41:19.180 --> 00:41:24.540
And that could be used for a very efficient interchange between components.

00:41:24.540 --> 00:41:29.380
And the idea is that you would have this immutable, this kind of constant data structure, which

00:41:29.380 --> 00:41:32.440
is like it's the same in every programming language.

00:41:32.440 --> 00:41:35.880
And then you can use that as the basis for writing all of your algorithms.

00:41:35.880 --> 00:41:41.020
So as long as it's arrow, you have these reusable algorithms that process arrow data.

00:41:41.020 --> 00:41:44.940
So we started with building the arrow format and standardizing it.

00:41:44.940 --> 00:41:50.420
And then we've built a whole ecosystem of components like library components and different

00:41:50.420 --> 00:41:54.560
programming languages for building applications that use the arrow format.

00:41:54.560 --> 00:42:00.420
So that includes not only tools for building and interacting with the data, but also file

00:42:00.420 --> 00:42:01.420
readers.

00:42:01.420 --> 00:42:07.180
So you can read CSV files and JSON data and parquet files, read data out of database systems,

00:42:07.180 --> 00:42:10.380
you know, wherever the data comes from, we want to have an efficient way to get it into

00:42:10.380 --> 00:42:11.820
the arrow format.

00:42:11.820 --> 00:42:18.120
And then we moved on to building data processing engines that are native to the arrow format,

00:42:18.120 --> 00:42:22.100
so that arrow goes in, the data is processed, arrow goes out.

00:42:22.100 --> 00:42:26.680
So DuckDB, for example, supports arrow as a preferred input format.

00:42:26.680 --> 00:42:32.600
And is DuckDB is more or less arrow like in its internals, it has kind of arrow format

00:42:32.600 --> 00:42:37.680
plus a number of extensions that are DuckDB specific for better performance within the

00:42:37.680 --> 00:42:39.900
context of DuckDB.

00:42:39.900 --> 00:42:43.860
So in numerous communities, so there's the Rust community, which has built Data Fusion,

00:42:43.860 --> 00:42:48.020
which is an execution engine for arrow, SQL engine for arrow.

00:42:48.020 --> 00:42:51.300
And so yeah, we've kind of like looked at the different layers of the stack, like data

00:42:51.300 --> 00:42:55.100
access, computing, data transport, everything under the sun.

00:42:55.100 --> 00:42:59.300
And then we've built libraries that are across many different programming languages so that

00:42:59.300 --> 00:43:03.220
are you can pick and choose the pieces that you need to build your build your system.

00:43:03.220 --> 00:43:07.860
And the goal ultimately, was that we in the future, which is now, we don't want people

00:43:07.860 --> 00:43:11.900
to have to reinvent the wheel whenever they're building something like pandas that they could

00:43:11.900 --> 00:43:17.060
just pick up these off the shelf components, they can design the developer experience,

00:43:17.060 --> 00:43:21.460
the user experience that they want to create, and they can get build, you know, so if you

00:43:21.460 --> 00:43:26.940
were building pandas, now you could build a pandas like library based on the arrow components

00:43:26.940 --> 00:43:31.380
in much less time, and it would be fast and efficient and interoperable with the whole

00:43:31.380 --> 00:43:33.940
ecosystem of other projects that use arrow.

00:43:33.940 --> 00:43:34.940
It's very cool.

00:43:34.940 --> 00:43:39.080
It's I mean, it was really ambitious, in some ways, obvious to people, they would they would

00:43:39.080 --> 00:43:43.580
hear about arrow and they say that sounds obvious, like, clearly, we should have a universal

00:43:43.580 --> 00:43:47.820
way of transporting data between systems and processing it in memory.

00:43:47.820 --> 00:43:49.100
Why hasn't this been done in the past?

00:43:49.100 --> 00:43:54.620
And it turns out that, as is true with many open source software problems that many of

00:43:54.620 --> 00:43:58.480
these problems are, the social problems are harder than the technical problems.

00:43:58.480 --> 00:44:03.720
And so if you can solve the people coordination and consensus problems, solving the technical

00:44:03.720 --> 00:44:06.220
issues is much, much easier by comparison.

00:44:06.220 --> 00:44:10.700
So I think we were lucky in that we found like the right group of people, the right

00:44:10.700 --> 00:44:15.880
personalities where we were able to, as soon as I met, I met Jacques Nadeau, who had been

00:44:15.880 --> 00:44:19.500
at MapR and we was working on his startup Dremio.

00:44:19.500 --> 00:44:23.580
Like I knew instantly when I met Jacques Nadeau, I was like, I can work he's like, he's like

00:44:23.580 --> 00:44:25.860
him like he's gonna help me make this happen.

00:44:25.860 --> 00:44:29.220
And I met Julien Ledem, who had also co created Parquet.

00:44:29.220 --> 00:44:33.540
I was like, yes, like, we were gonna make like I found the right people like we're we're

00:44:33.540 --> 00:44:34.900
gonna make this happen.

00:44:34.900 --> 00:44:38.940
It's been a labor of love and much, much work and stress and everything.

00:44:38.940 --> 00:44:43.140
But I've been working on things circling, you know, with arrows, the sun, you know,

00:44:43.140 --> 00:44:48.060
I've been building kind of satellites and moons and planets circling the arrow sun over

00:44:48.060 --> 00:44:49.260
the last eight years or so.

00:44:49.260 --> 00:44:50.660
And that's kept me pretty busy.

00:44:50.660 --> 00:44:53.700
Yeah, it's only getting more exciting and interesting.

00:44:53.700 --> 00:44:59.540
Over here, it says it uses efficient analytic operations on modern hardware like CPUs and

00:44:59.540 --> 00:45:00.540
GPUs.

00:45:00.540 --> 00:45:05.060
One of the big challenges of Python has been the gill also one of its big benefits, but

00:45:05.060 --> 00:45:09.500
one of its challenges when you get to multi core computational stuff has been the gill.

00:45:09.500 --> 00:45:10.500
What's the story here?

00:45:10.500 --> 00:45:11.500
Yeah.

00:45:11.500 --> 00:45:17.700
So in Arrowland, when we're talking about analytic efficiency, it mainly has to do with

00:45:17.700 --> 00:45:24.160
the like underlying like how the how modern CPU works or how a GPU works.

00:45:24.160 --> 00:45:29.820
And so when the data is arranged in column oriented format, that enables the data to

00:45:29.820 --> 00:45:34.640
be moved efficiently through the CPU cache pipelines.

00:45:34.640 --> 00:45:39.480
So the data is made made available efficiently to the to the CPU cores.

00:45:39.480 --> 00:45:45.480
And so we spent a lot of energy in Arrow making decisions firstly, to enable very cache of

00:45:45.480 --> 00:45:49.960
like CPU cache or GPU cache efficient analytics on the data.

00:45:49.960 --> 00:45:53.900
So we were kind of always when we were deciding we would break ties and make decisions based

00:45:53.900 --> 00:45:57.020
on like what's going to be more efficient for the for the computer chip.

00:45:57.020 --> 00:46:02.760
The other thing is that modern and this is true with GPUs, which have a different parallelism

00:46:02.760 --> 00:46:08.220
model than or different kind of multi core parallelism model than CPUs.

00:46:08.220 --> 00:46:13.500
But in CPUs, they've focused on adding what are called single instruction, multiple data

00:46:13.500 --> 00:46:20.060
intrinsic, like built in operations in the processor, where you know, now you can process

00:46:20.060 --> 00:46:25.300
up to 512 bytes of data in a single CPU instruction.

00:46:25.300 --> 00:46:31.300
And so that's like my brain's doing the math right, like 1632 bit floats, or, you know,

00:46:31.300 --> 00:46:34.540
eight 64 bit integers in a single CPU cycle.

00:46:34.540 --> 00:46:35.540
There's like intrinsic operations.

00:46:35.540 --> 00:46:41.180
So multiply this number by that one, multiply that number to these eight things all at once,

00:46:41.180 --> 00:46:42.180
something like that.

00:46:42.180 --> 00:46:43.180
That's right.

00:46:43.180 --> 00:46:44.180
Yeah.

00:46:44.180 --> 00:46:48.220
Or you might say like, oh, I have a bit mask and I want to select I want to gather like

00:46:48.220 --> 00:46:52.860
the one bits that are set in this bit mask from this array of integers.

00:46:52.860 --> 00:46:58.340
And so there's like a gather instruction, which allows you to select a subset sort of

00:46:58.340 --> 00:47:01.340
SIMD vector of integers, you know, using a bit mask.

00:47:01.340 --> 00:47:05.460
And so that turns out to be like a pretty critical operation in certain data analytic

00:47:05.460 --> 00:47:06.460
workloads.

00:47:06.460 --> 00:47:11.060
So yeah, we were really, we wanted to have a data format that was essentially, you know,

00:47:11.060 --> 00:47:16.500
future proofed in the sense that it's, it's ideal for the coming wave, like current current

00:47:16.500 --> 00:47:21.380
generation of CPUs, but also given that a lot of processing is moving to GPUs and to

00:47:21.380 --> 00:47:27.980
FPGAs and to custom silicon, like we wanted Arrow to be usable there as well.

00:47:27.980 --> 00:47:32.820
And it's Arrow has been successfully, you know, used as the foundation of GPU computing

00:47:32.820 --> 00:47:33.820
libraries.

00:47:33.820 --> 00:47:38.580
Like we kind of at Voltron Data, we built, we've built a whole accelerator native GPU

00:47:38.580 --> 00:47:42.700
native, you know, scalable execution engine that's, that's Arrow based.

00:47:42.700 --> 00:47:46.100
And so I think the fact that we, that was our aspiration and we've been able to prove

00:47:46.100 --> 00:47:50.540
that out in real world workloads and show the kinds of efficiency gains that you can

00:47:50.540 --> 00:47:56.260
get with using modern computing hardware correctly, or at least as well as it's intended to be

00:47:56.260 --> 00:47:57.260
used.

00:47:57.260 --> 00:48:01.620
That's a big deal in terms of like making applications faster, reducing the carbon footprint

00:48:01.620 --> 00:48:03.980
of large scale data workloads, things like that.

00:48:03.980 --> 00:48:04.980
Yeah.

00:48:04.980 --> 00:48:05.980
Amazing.

00:48:05.980 --> 00:48:06.980
All right.

00:48:06.980 --> 00:48:07.980
Let's see what else have I got on deck here to talk to you about.

00:48:07.980 --> 00:48:11.620
Do Ibis, want to talk about Ibis or which one you want to, we got a little time left.

00:48:11.620 --> 00:48:13.300
We got a couple of things to cover.

00:48:13.300 --> 00:48:14.300
Yeah.

00:48:14.300 --> 00:48:15.300
Let's, we can talk about Ibis.

00:48:15.300 --> 00:48:16.300
Yeah.

00:48:16.300 --> 00:48:17.300
We could, we could probably spend another hour talking.

00:48:17.300 --> 00:48:18.300
Yes.

00:48:18.300 --> 00:48:19.300
Easy.

00:48:19.300 --> 00:48:24.420
So one of the more interesting areas in recent years has been new DataFrame libraries and

00:48:24.420 --> 00:48:29.740
DataFrame APIs that transpile or compile to different execute on different backends.

00:48:29.740 --> 00:48:33.860
And so around the time that I was helping start Arrow, I created this project called

00:48:33.860 --> 00:48:40.620
Ibis, which is basically a portable DataFrame API that knows how to generate SQL queries

00:48:40.620 --> 00:48:46.980
and compile to pandas and pollers and different DataFrame, DataFrame backends.

00:48:46.980 --> 00:48:53.140
And the goal is to provide a really productive DataFrame API that gives you portability across

00:48:53.140 --> 00:48:57.860
different execution backends with the goal of enabling what we call the multi-engine

00:48:57.860 --> 00:48:58.860
data stack.

00:48:58.860 --> 00:49:03.180
So you aren't stuck with using one particular system because all of the code that you've

00:49:03.180 --> 00:49:05.500
written is specialized to that system.

00:49:05.500 --> 00:49:10.200
You have this tool, which, so maybe you could work with, you know, DuckDB on your laptop

00:49:10.200 --> 00:49:13.660
or pandas or pollers with Ibis on your laptop.

00:49:13.660 --> 00:49:17.780
But if you have, if you need to run that workload someplace else, maybe with, you know, ClickHouse

00:49:17.780 --> 00:49:22.780
or BigQuery, or maybe it's a large big data workload that's too big to fit on your laptop

00:49:22.780 --> 00:49:28.260
and you need to use Spark SQL or something that you can just ask Ibis, say, "Hey, I want

00:49:28.260 --> 00:49:31.140
to do the same thing on this larger dataset over here."

00:49:31.140 --> 00:49:35.940
And it has all the logic to generate the correct query representation and run that workload

00:49:35.940 --> 00:49:36.940
for you.

00:49:36.940 --> 00:49:37.940
So it's super useful.

00:49:37.940 --> 00:49:42.140
But there's a whole wave of like, you know, work right now to help enable people to work

00:49:42.140 --> 00:49:47.500
in a pandas-like way, but get work with big data or, you know, get better performance

00:49:47.500 --> 00:49:51.840
than pandas because pandas is a Swiss army knife, but isn't a chainsaw.

00:49:51.840 --> 00:49:55.380
So if you were rebuilding pandas from scratch, it would end up a lot.

00:49:55.380 --> 00:49:59.720
There's areas of the projects that are, you know, more bloated or have performance overhead

00:49:59.720 --> 00:50:01.160
that's hard to get rid of.

00:50:01.160 --> 00:50:06.340
And so that's why you have Richie Fink started the Pollers project, which is kind of a reimagining

00:50:06.340 --> 00:50:11.660
of pandas, pandas data frames written in Rust and exposed in Python.

00:50:11.660 --> 00:50:15.520
And Pollers, of course, is built on Apache Arrow at its core.

00:50:15.520 --> 00:50:20.180
So building an Arrow native data frame library in Rust and, you know, all the benefits that

00:50:20.180 --> 00:50:25.100
come with building Python extensions in Rust, you know, you avoid the gill and you can manage

00:50:25.100 --> 00:50:28.020
the multithreading in a systems language, all that fun stuff.

00:50:28.020 --> 00:50:29.020
Yeah.

00:50:29.020 --> 00:50:32.440
When you're talking about Arrow and supporting different ways of using it and things being

00:50:32.440 --> 00:50:35.060
built on it, certainly Pollers came to mind for me.

00:50:35.060 --> 00:50:39.340
You know, when you talk about Ibis, I think it's interesting that a lot of these data

00:50:39.340 --> 00:50:46.100
frame libraries, they try to base their API to be pandas-like, but not identical, potentially,

00:50:46.100 --> 00:50:49.060
you know, thinking of Dask and others.

00:50:49.060 --> 00:50:54.980
But this Ibis sort of has the ability to configure it and extend it and make it different, kind

00:50:54.980 --> 00:50:57.940
of like, for example, Dask, which is one of the backends here.

00:50:57.940 --> 00:50:59.460
But the API doesn't change, right?

00:50:59.460 --> 00:51:01.420
It just, it talks to the different backends.

00:51:01.420 --> 00:51:02.420
Yeah.

00:51:02.420 --> 00:51:03.420
There's different schools of thought on this.

00:51:03.420 --> 00:51:07.980
So there's another project called Modin, which is similar to Ibis in many ways, in the sense

00:51:07.980 --> 00:51:12.580
of like transpilation and sort of dynamically supporting different backends, but sought

00:51:12.580 --> 00:51:19.980
to closely emulate the exact details of like the API call, the function name, the function

00:51:19.980 --> 00:51:25.060
arguments must be exactly the same as pandas, with the goal of being a drop-in replacement

00:51:25.060 --> 00:51:26.500
for people's pandas code.

00:51:26.500 --> 00:51:29.540
And that's one approach, kind of the pandas emulation route.

00:51:29.540 --> 00:51:35.300
And there's a library called Koalas for Spark, which is like a PySpark emulation layer for

00:51:35.300 --> 00:51:36.300
the pandas API.

00:51:36.300 --> 00:51:41.740
And then there's other projects like Polars and Ibis, Dask DataFrame, that take like design

00:51:41.740 --> 00:51:47.300
cues from pandas in the sense of like the general way in which the API works, but has

00:51:47.300 --> 00:51:52.260
made meaningful departures in the interest of doing things better in many ways than pandas

00:51:52.260 --> 00:51:57.140
did in certain parts of the API, and making things simpler, and not being beholden to

00:51:57.140 --> 00:51:59.980
decisions that were made in pandas, you know, 15 years ago.

00:51:59.980 --> 00:52:04.580
Not to say there's anything bad about the pandas API, but like with any API, it's large,

00:52:04.580 --> 00:52:10.060
like it's very large as evidenced by, you know, the 2000 pages of documentation.

00:52:10.060 --> 00:52:14.980
And so I understand the desire to make things simpler, but also refining certain things,

00:52:14.980 --> 00:52:18.900
making certain types of workloads easier to express.

00:52:18.900 --> 00:52:21.740
And so Polars, for example, is very expression based.

00:52:21.740 --> 00:52:26.580
And so everything is column expressions and is lazy, and not eagerly computed, whereas

00:52:26.580 --> 00:52:33.020
pandas is eager execution, just like NumPy is, which is how pandas became eagerly executed

00:52:33.020 --> 00:52:34.260
in the first place.

00:52:34.260 --> 00:52:40.520
And so I think the mantra with Polars was, we don't want to support the eager execution

00:52:40.520 --> 00:52:44.540
by default that pandas provides, we want to be able to build expressions so that we can

00:52:44.540 --> 00:52:50.100
do query optimization, and take inefficient code and under the hood, rewrite it to be

00:52:50.100 --> 00:52:53.480
more efficient, which is, you know, what you can do with a query optimizer.

00:52:53.480 --> 00:52:57.800
And so ultimately, like, that matters a lot when you're executing code remotely, or in

00:52:57.800 --> 00:53:03.300
like a big data system, you want to have the freedom to be able to take like a lazy analytic

00:53:03.300 --> 00:53:08.100
expression and rewrite it based on it might be like you need to seriously rewrite the

00:53:08.100 --> 00:53:13.120
expression in the case of like, Dask, for example, like Dask has to do planning across

00:53:13.120 --> 00:53:14.720
a distributed cluster.

00:53:14.720 --> 00:53:20.240
And so, you know, Dask DataFrame is very pandas like, but it also includes some explicit details

00:53:20.240 --> 00:53:25.080
of being able to control how the data is partitioned and being able to have some knobs to turn

00:53:25.080 --> 00:53:29.760
in terms of like, having more control over what's happening on a distributed cluster.

00:53:29.760 --> 00:53:32.920
And I think the goal there is like to give the developer more control as opposed to like

00:53:32.920 --> 00:53:37.200
trying to be intelligent, you know, make all of the decisions on behalf of the developer.

00:53:37.200 --> 00:53:41.400
So you know, if you know about how you know, know a lot about your data set, then you can

00:53:41.400 --> 00:53:45.880
make more you can make a, you know, decisions about how to how to schedule and execute it.

00:53:45.880 --> 00:53:49.820
Of course, that Dask is building query, you know, query optimization to start making more

00:53:49.820 --> 00:53:51.840
of those decisions on behalf of the user.

00:53:51.840 --> 00:53:56.460
But you know, Dask has become very popular and impactful and making distributed computing

00:53:56.460 --> 00:53:57.840
easier in Python.

00:53:57.840 --> 00:54:01.200
So they've gotten, I think, got a long way without turning into a database.

00:54:01.200 --> 00:54:05.600
And I think Dask never aspired to be a to be a database engine, which is a lot of distributed

00:54:05.600 --> 00:54:09.040
computing is, you know, not database like it's could be distributed array computing

00:54:09.040 --> 00:54:14.160
or distributed model training, and just being able to easily run distributed Python functions

00:54:14.160 --> 00:54:16.800
on a cluster do distributed computing that way.

00:54:16.800 --> 00:54:21.160
It was amazing, like how many people were using PySpark in the early days, just for

00:54:21.160 --> 00:54:25.840
the convenience of being able to run Python functions in parallel on a cluster.

00:54:25.840 --> 00:54:27.960
Yeah, and that's pretty interesting.

00:54:27.960 --> 00:54:30.120
Not exactly what it's designed for, right?

00:54:30.120 --> 00:54:35.680
Dask, you know, you probably come across situations where you do a sequence of operations, they're

00:54:35.680 --> 00:54:39.840
kind of commutative in the end and practice, but from a computational perspective, like

00:54:39.840 --> 00:54:44.200
how do I distribute this amongst different servers, maybe one order matters a lot more

00:54:44.200 --> 00:54:47.000
than the other outperformance, you know?

00:54:47.000 --> 00:54:48.000
Yeah, yeah.

00:54:48.000 --> 00:54:49.000
Interesting.

00:54:49.000 --> 00:54:50.000
All right.

00:54:50.000 --> 00:54:51.000
One final thing.

00:54:51.000 --> 00:54:52.000
SQLGLOT.

00:54:52.000 --> 00:54:53.000
Yeah.

00:54:53.000 --> 00:54:54.720
So SQLGLOT project started by Toby Mao.

00:54:54.720 --> 00:55:00.960
So he's a Netflix alum and really, really talented, talented developer who's created

00:55:00.960 --> 00:55:08.700
this SQL query transpilation framework library for Python and kind of underlying core library.

00:55:08.700 --> 00:55:12.800
And so the problem that's being solved there is that SQL, despite being a quote unquote

00:55:12.800 --> 00:55:17.120
standard is not at all standardized across different database systems.

00:55:17.120 --> 00:55:21.800
And so if you want to take your SQL queries written for one engine and use them someplace

00:55:21.800 --> 00:55:25.860
else, without something like SQLGLOT, you would have to manually rewrite and make sure

00:55:25.860 --> 00:55:30.000
you get the typecasting and coalescing rules correct.

00:55:30.000 --> 00:55:36.160
And so SQLGLOT understands the intricacies and the quirks of every database dialect,

00:55:36.160 --> 00:55:41.000
SQL dialect, and knows how to correctly translate from one dialect to another.

00:55:41.000 --> 00:55:47.080
And so IBIS now uses SQLGLOT as its underlying engine for query transpilation and generating

00:55:47.080 --> 00:55:48.080
SQL outputs.

00:55:48.080 --> 00:55:54.440
So originally, IBIS had its own kind of bad version of SQLGLOT, kind of a query transpilation,

00:55:54.440 --> 00:56:00.960
like SQL transpilation that was powered by, I think, powered by SQLAlchemy and a bunch

00:56:00.960 --> 00:56:01.960
of custom code.

00:56:01.960 --> 00:56:06.200
And so I think they've been able to delete a lot of code in IBIS by moving to SQLGLOT.

00:56:06.200 --> 00:56:11.780
And I know that, you know, SQLGLOT is also being used to power kind of a new, yeah, being

00:56:11.780 --> 00:56:16.120
used in people building new products that are Python powered and things like that.

00:56:16.120 --> 00:56:23.000
So Toby, like his company, Tobiko Data, they're building a product called SQL Mesh that's

00:56:23.000 --> 00:56:24.320
powered by SQLGLOT.

00:56:24.320 --> 00:56:28.040
So very cool project and maybe a bit in the weeds, but if you've ever needed to convert

00:56:28.040 --> 00:56:32.520
a SQL query from one dialect to another, it's, yeah, SQLGLOT is here to save the day.

00:56:32.520 --> 00:56:38.000
I would say, you know, even simple things is how do you specify a parameter variable,

00:56:38.000 --> 00:56:39.960
you know, for parameterized query, right?

00:56:39.960 --> 00:56:45.840
And Microsoft SQL Server, it's like at the parameter name and Oracle, it's like question

00:56:45.840 --> 00:56:49.680
mark or SQL, I think it's also quite, you know, just that, even those simple things.

00:56:49.680 --> 00:56:50.680
It's a pain.

00:56:50.680 --> 00:56:53.480
And without it, you end up with little Bobby tables, which is also not good.

00:56:53.480 --> 00:56:54.680
So that's true.

00:56:54.680 --> 00:56:55.680
That's true.

00:56:55.680 --> 00:56:57.560
Nobody wants to talk to him.

00:56:57.560 --> 00:56:58.560
Yeah, this is really cool.

00:56:58.560 --> 00:57:02.000
SQLGLOT, like polyglot, but all the languages of SQL.

00:57:02.000 --> 00:57:03.000
Nice.

00:57:03.000 --> 00:57:09.640
And you do things like you can say, read DuckDB and write to Hive or read DuckDB and then

00:57:09.640 --> 00:57:11.200
write to Spark or whatever.

00:57:11.200 --> 00:57:12.200
It's pretty cool.

00:57:12.200 --> 00:57:17.640
All right, Wes, I think we're getting short on time, but you know, I know everybody appreciated

00:57:17.640 --> 00:57:20.940
hearing from you and hearing what you're up to these days.

00:57:20.940 --> 00:57:22.840
Anything you want to add before we wrap up?

00:57:22.840 --> 00:57:23.840
I don't think so.

00:57:23.840 --> 00:57:24.840
Yeah.

00:57:24.840 --> 00:57:30.040
I enjoyed the conversation and yeah, there's a lot of stuff going on and still plenty of

00:57:30.040 --> 00:57:32.040
things to get excited about.

00:57:32.040 --> 00:57:36.560
So I think often people feel like all the exciting problems in the Python ecosystem

00:57:36.560 --> 00:57:39.640
have been solved, but there's still a lot to do.

00:57:39.640 --> 00:57:45.160
And yeah, we've made a lot of progress in the last 15 plus years, but in some ways it

00:57:45.160 --> 00:57:47.200
feels like we're just getting started.

00:57:47.200 --> 00:57:49.880
So we are just excited to see where things go next.

00:57:49.880 --> 00:57:50.880
Yeah.

00:57:50.880 --> 00:57:53.320
Every time I think, oh, all the problems are solved, then you discover all these new things

00:57:53.320 --> 00:57:55.760
that are so creative and you're like, oh, well, that was a big problem.

00:57:55.760 --> 00:57:57.400
I didn't even know it was a problem.

00:57:57.400 --> 00:57:58.400
It's great.

00:57:58.400 --> 00:57:59.400
All right.

00:57:59.400 --> 00:58:02.040
Well, thanks for being here and taking the time and keep us updated on what you're up

00:58:02.040 --> 00:58:03.040
to.

00:58:03.040 --> 00:58:04.040
All right.

00:58:04.040 --> 00:58:05.040
Thanks for joining us.

00:58:05.040 --> 00:58:06.040
Bye-bye.

00:58:07.040 --> 00:58:08.840
This has been another episode of Talk Python to Me.

00:58:08.840 --> 00:58:10.760
Thank you to our sponsors.

00:58:10.760 --> 00:58:12.120
Be sure to check out what they're offering.

00:58:12.120 --> 00:58:14.320
It really helps support the show.

00:58:14.320 --> 00:58:19.640
It's time to stop asking relational databases to do more than they were made for and simplify

00:58:19.640 --> 00:58:22.480
complex data models with graphs.

00:58:22.480 --> 00:58:28.020
Check out the sample FastAPI project and see what Neo4j, a native graph database, can do

00:58:28.020 --> 00:58:29.020
for you.

00:58:29.020 --> 00:58:33.640
You can find out more at talkpython.fm/neo4j.

00:58:33.640 --> 00:58:37.520
Mailtrap, an email delivery platform that developers love.

00:58:37.520 --> 00:58:39.640
Try for free at mailtrap.io.

00:58:39.640 --> 00:58:42.280
Want to level up your Python?

00:58:42.280 --> 00:58:46.380
We have one of the largest catalogs of Python video courses over at Talk Python.

00:58:46.380 --> 00:58:51.480
Our content ranges from true beginners to deeply advanced topics like memory and async.

00:58:51.480 --> 00:58:54.160
And best of all, there's not a subscription in sight.

00:58:54.160 --> 00:58:57.320
Check it out for yourself at training.talkpython.fm.

00:58:57.320 --> 00:58:58.880
Be sure to subscribe to the show.

00:58:58.880 --> 00:59:01.960
Open your favorite podcast app and search for Python.

00:59:01.960 --> 00:59:03.320
We should be right at the top.

00:59:03.320 --> 00:59:08.880
You can also find the iTunes feed at /iTunes, the Google Play feed at /play, and the Direct

00:59:08.880 --> 00:59:12.920
RSS feed at /rss on talkpython.fm.

00:59:12.920 --> 00:59:15.480
We're live streaming most of our recordings these days.

00:59:15.480 --> 00:59:19.060
If you want to be part of the show and have your comments featured on the air, be sure

00:59:19.060 --> 00:59:24.000
to subscribe to our YouTube channel at talkpython.fm/youtube.

00:59:24.000 --> 00:59:25.520
This is your host, Michael Kennedy.

00:59:25.520 --> 00:59:26.520
Thanks so much for listening.

00:59:26.520 --> 00:59:27.760
I really appreciate it.

00:59:27.760 --> 00:59:29.440
Now get out there and write some Python code.

00:59:29.440 --> 00:59:32.800
[MUSIC PLAYING]

00:59:32.800 --> 00:59:50.160
[END PLAYBACK]

