WEBVTT

00:00:00.001 --> 00:00:04.880
This episode dives into some of the most important data science libraries from the Python space

00:00:04.880 --> 00:00:07.660
with one of its pioneers, Wes McKinney.

00:00:07.660 --> 00:00:12.500
He's the creator or co-creator of the Pandas, Apache Arrow, and Evis projects,

00:00:12.500 --> 00:00:14.940
as well as an entrepreneur in this space.

00:00:14.940 --> 00:00:21.400
This is Talk Python To Me, episode 462, recorded April 11th, 2024.

00:00:21.400 --> 00:00:23.440
Are you ready for your host?

00:00:23.440 --> 00:00:27.740
You're listening to Michael Kennedy on Talk Python To Me.

00:00:27.740 --> 00:00:31.500
Live from Portland, Oregon, and this segment was made with Python.

00:00:31.500 --> 00:00:38.160
Welcome to Talk Python To Me, a weekly podcast on Python.

00:00:38.160 --> 00:00:39.900
This is your host, Michael Kennedy.

00:00:39.900 --> 00:00:45.020
Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython,

00:00:45.020 --> 00:00:47.400
both on fosstodon.org.

00:00:47.400 --> 00:00:52.480
Keep up with the show and listen to over seven years of past episodes at talkpython.fm.

00:00:52.860 --> 00:00:56.280
We've started streaming most of our episodes live on YouTube.

00:00:56.280 --> 00:01:03.820
Subscribe to our YouTube channel over at talkpython.fm/youtube to get notified about upcoming shows and be part of that episode.

00:01:03.820 --> 00:01:06.640
This episode is sponsored by Neo4j.

00:01:07.020 --> 00:01:14.820
It's time to stop asking relational databases to do more than they were made for and simplify complex data models with graphs.

00:01:14.820 --> 00:01:21.660
Check out the sample FastAPI project and see what Neo4j, a native graph database, can do for you.

00:01:21.660 --> 00:01:26.080
Find out more at talkpython.fm/Neo4j.

00:01:26.860 --> 00:01:32.200
And it's brought to you by Mailtrap, an email delivery platform that developers love.

00:01:32.200 --> 00:01:35.420
Try for free at mailtrap.io.

00:01:35.420 --> 00:01:36.120
Hey, Wes.

00:01:36.120 --> 00:01:37.540
Welcome to Talk Python To Me.

00:01:37.540 --> 00:01:38.320
Thanks for having me.

00:01:38.320 --> 00:01:42.440
You know, honestly, I feel like it's been a long time coming having you on the show.

00:01:42.440 --> 00:01:46.680
You've had such a big impact in the Python space, especially the data science side of that space.

00:01:46.680 --> 00:01:48.820
And it's high time to have you on the show.

00:01:48.820 --> 00:01:49.280
So welcome.

00:01:49.280 --> 00:01:49.980
Good to have you.

00:01:49.980 --> 00:01:51.320
Yeah, it's great to be here.

00:01:51.320 --> 00:01:56.260
I've had my been heads down a lot the last, you know, and last N years.

00:01:56.260 --> 00:02:05.240
And, you know, I actually haven't been because I think a lot of my work has been more like data infrastructure and and work working at even a lower level than than Python.

00:02:05.240 --> 00:02:10.180
So I haven't been I haven't been as engaging as much directly with the with the Python community.

00:02:10.180 --> 00:02:16.260
But it's it's been great to kind of get back more involved and start catching up on all the things that people have been building.

00:02:16.260 --> 00:02:24.560
And and being at Posit gives me the ability to, yeah, sort of have more exposure to what's going on and people that are using Python in the real world.

00:02:24.560 --> 00:02:27.040
There's a ton of stuff going on at Posit that's super interesting.

00:02:27.040 --> 00:02:28.300
And we'll talk about some of that.

00:02:28.300 --> 00:02:32.780
And, you know, it's sometimes it's just really fun to build, you know, and work with people building things.

00:02:32.780 --> 00:02:34.900
And I'm sure you're enjoying that aspect of it.

00:02:34.900 --> 00:02:35.240
For sure.

00:02:35.240 --> 00:02:35.580
Nice.

00:02:35.580 --> 00:02:44.720
Well, before we dive into Pandas and all the things that you've been working on after that, you know, let's just hear a quick bit about yourself for folks who don't know you.

00:02:44.720 --> 00:02:45.600
Sure. Yeah.

00:02:45.600 --> 00:02:46.840
My name is Wes McKinney.

00:02:46.840 --> 00:02:49.240
I grew up in Akron, Ohio, mostly.

00:02:49.240 --> 00:02:55.260
And I got involved, started getting involved in Python development around 2007, 2008.

00:02:55.260 --> 00:02:58.300
And built I was working in quant finance at the time.

00:02:58.300 --> 00:03:03.760
I started building a personal data analysis toolkit that turned into the Pandas project.

00:03:03.760 --> 00:03:08.140
And then open source that in 2009, started getting involved in the Python community.

00:03:08.140 --> 00:03:23.840
And, you know, I spent several years like writing my book, Python for data analysis, and then working with the broader scientific Python, Python data science community to help enable Python to become a mainstream programming language for doing data analysis and data science.

00:03:23.840 --> 00:03:26.460
In the meantime, I've become an entrepreneur.

00:03:26.460 --> 00:03:38.300
I've started some companies and I've been working to innovate and improve the computing infrastructure that powers data science tools and libraries like Pandas.

00:03:38.420 --> 00:03:43.720
So that's led to some other projects like Apache Arrow and Ibis and some other things.

00:03:43.720 --> 00:03:53.940
Since recent years, I've been working on a startup, Voltron data, which is still very much going strong and has a big team and is off to the races.

00:03:54.440 --> 00:03:58.160
And I've had a long relationship with Posit, formerly RStudio.

00:03:58.160 --> 00:04:04.520
And they were, you know, my home for doing aero development from 2018 to 2020.

00:04:04.520 --> 00:04:08.200
They helped me incubate the startup that became Voltron data.

00:04:08.200 --> 00:04:21.040
And so I've gone back to work full time there as a software architect to help them with their Python strategy to make sort of their data science platform a delight to use for the Python user base.

00:04:21.040 --> 00:04:22.460
I'm pretty impressed with what they're doing.

00:04:22.660 --> 00:04:31.360
I didn't realize the connection between Voltron and Posit, but I have had Joe Chung on the show before to talk about Shiny for Python.

00:04:31.360 --> 00:04:37.780
And I've seen him demo a few really interesting things, how it integrates to notebooks these days.

00:04:37.780 --> 00:04:40.040
Some of the stuff that y'all are doing.

00:04:40.040 --> 00:04:41.580
And yeah, it's just it's fascinating.

00:04:41.580 --> 00:04:45.360
Maybe give people a quick elevator pitch on that while we're on that subject.

00:04:45.360 --> 00:04:47.400
On Shiny or on Posit in general?

00:04:47.400 --> 00:04:48.780
Yeah, whichever you feel like.

00:04:48.780 --> 00:04:49.580
Yeah.

00:04:49.580 --> 00:04:54.080
So Posit started out 2009 as RStudio.

00:04:54.080 --> 00:04:57.260
And so it didn't start out intending to be a company.

00:04:57.260 --> 00:05:05.880
JJ Lair and Joe Chang built a new IDE, Integrated Development Environment, for R, because what was available at the time wasn't great.

00:05:06.240 --> 00:05:11.960
And so they made that into, you know, I think probably one of the best data science IDs that's ever been built.

00:05:11.960 --> 00:05:13.640
It's really an amazing piece of tech.

00:05:13.640 --> 00:05:19.360
So I started becoming a company with customers and revenue in the 2013 timeframe.

00:05:20.020 --> 00:05:26.620
And they've built a whole suite of tools to support enterprise data science teams to make open source data science work in the real world.

00:05:26.620 --> 00:05:31.120
But the company itself, it's a certified B Corporation, has no plans to go public or IPO.

00:05:31.760 --> 00:05:49.880
It is dedicated to the mission of open source software for data science and technical communication and is basically building itself to be a hundred year company that has a revenue generating enterprise product side and an open source side so that we the open source feeds the kind of enterprise enterprise part of the business.

00:05:49.880 --> 00:05:53.460
It's the enterprise part of the business generates revenue to support the open source development.

00:05:53.460 --> 00:06:01.300
And the goal is to be able to sustainably support the mission of open source data science, you know, for, you know, hopefully the rest of our hopefully the rest of our lives.

00:06:01.300 --> 00:06:02.720
And it's an amazing company.

00:06:02.720 --> 00:06:09.240
It's been one of the most successful companies that dedicates a large fraction of its engineering time to open source software development.

00:06:09.240 --> 00:06:13.620
So it's very impressed with the company and, you know, JJ Lair, its founder.

00:06:13.880 --> 00:06:22.600
And, you know, I'm excited to be, you know, helping it, helping it grow and, and become a sustainable long-term fixture in the, in the ecosystem.

00:06:22.600 --> 00:06:23.140
Yes.

00:06:23.140 --> 00:06:23.700
Yeah.

00:06:23.700 --> 00:06:25.100
It's definitely doing cool stuff.

00:06:25.100 --> 00:06:27.380
Incentives are aligned well, right?

00:06:27.380 --> 00:06:29.960
It's not private equity or IPO.

00:06:29.960 --> 00:06:30.360
Yeah.

00:06:30.360 --> 00:06:39.720
Many people know JJ Lair created cold fusion, which is like the original dynamic web development framework in the 1990s.

00:06:39.720 --> 00:06:45.700
And so, so he and his brother, he and his brother, Jeremy and some others built a layer corp to commercialize cold fusion.

00:06:45.700 --> 00:06:51.480
And they built a successful software business that was acquired by macro media, which was eventually acquired by Adobe.

00:06:51.480 --> 00:06:54.020
But they did go public as a layer corp.

00:06:54.020 --> 00:06:58.700
And during the.com bubble, then JJ went on to found a couple of other successful startups.

00:06:58.700 --> 00:07:14.720
And so he found himself in his late thirties in the 15 years ago or around, around age 40 is around, around the age I am now having been very successful as an entrepreneur, like no need to, to make money and looking for like a mission to spend the rest of his career on.

00:07:14.720 --> 00:07:30.200
And that identifying data science and statistical computing as an open source in particular, like making open source for data science work, you know, was the mission that he aligned with and something that he had been interested in earlier in his career, but he had gotten busy with other things.

00:07:30.200 --> 00:07:50.540
So it's really refreshing to work with people who are really mission focused and focused on making impact in the world, creating great software, empowering people, increasing accessibility and making most of it available for free on the internet and not being so focused on empire building and producing great profits for venture investors and things like that.

00:07:50.540 --> 00:08:08.360
So I think, yeah, I think the, I think the goal of the company and, is to, you know, provide like a, you know, an amazing home for top tier software developers to work on this software, to, you know, spend their careers and to, to build families and, to be a happy, happy and healthy culture for working on this type of software.

00:08:08.360 --> 00:08:09.160
That sounds excellent.

00:08:09.160 --> 00:08:09.740
Very cool.

00:08:09.740 --> 00:08:11.980
I didn't realize the history all the way back to cold fusion.

00:08:11.980 --> 00:08:14.560
Speaking of history, let's jump in.

00:08:14.560 --> 00:08:19.560
There's a, Wes, there's a possibility that people out there listening don't know what pandas is.

00:08:19.560 --> 00:08:21.820
You would think it's pretty ubiquitous.

00:08:21.820 --> 00:08:29.740
And I certainly would say that it is, especially in the data science space, but I got a bunch of listeners who listen and they say really surprising things.

00:08:29.740 --> 00:08:36.120
They'll say stuff to me like, Michael, I've been listening for six weeks now and I'm starting to understand some of the stuff y'all are talking about.

00:08:36.120 --> 00:08:37.900
I'm like, why did you list for six weeks?

00:08:37.900 --> 00:08:39.700
You didn't know what I was talking about.

00:08:39.700 --> 00:08:40.440
Like, that's crazy.

00:08:40.440 --> 00:08:45.340
And a lot of people use it as like language immersion to get into the Python space.

00:08:45.340 --> 00:08:49.980
So I'm sure there's plenty of people out there who are, you know, immersing themselves, but are pretty new.

00:08:49.980 --> 00:08:53.400
So maybe for that crew, we could introduce what pandas is to them.

00:08:53.400 --> 00:08:53.860
Absolutely.

00:08:53.860 --> 00:08:59.040
It's a data manipulation and analysis toolkit for Python.

00:08:59.320 --> 00:09:03.940
So it's a Python library that you install that enables you to read data files.

00:09:03.940 --> 00:09:12.580
So read many different types of data files off of disk or off of remote storage or read data out of a database or some other remote data storage system.

00:09:12.580 --> 00:09:14.240
This is tabular data.

00:09:14.240 --> 00:09:16.620
So it's structured data like with columns.

00:09:16.620 --> 00:09:20.240
You can think of it like a spreadsheet or some other tabular data set.

00:09:20.240 --> 00:09:26.740
And then it provides you with this data frame object, which is kind of pandas dot data frame.

00:09:26.740 --> 00:09:29.240
That is the main tabular data object.

00:09:29.240 --> 00:09:46.480
And it has a ton of methods for accessing, slicing, grabbing subsets of the data, applying functions on it that do filtering and subsetting and selection, as well as like more analytical operations, like things that you might do with a database system or SQL.

00:09:46.480 --> 00:09:56.380
So joins and lookups, as well as analytical functions, like summary statistics, you know, grouping by some key and producing summary statistics.

00:09:56.380 --> 00:10:05.100
So it's basically a Swiss army knife for doing data manipulation, data cleaning and supporting the data analysis workflow.

00:10:05.100 --> 00:10:17.780
But it doesn't actually include very much as far as actual statistics or models or, you know, if you're doing something with LLMs or linear regression or some type of machine learning, you have to use another library.

00:10:17.780 --> 00:10:22.920
But pandas is the on ramp for all of the data into your environment in Python.

00:10:22.920 --> 00:10:45.820
So when people are building some kind of application that touches data in Python, pandas is often like the initial like on ramp for how data gets into Python, where you clean up the data, you regularize, regularize it, you get it ready for analysis, and then you feed the clean data into the downstream, you know, statistical library or data analysis library that you're using.

00:10:45.820 --> 00:10:48.460
That whole data wrinkling side of things, right?

00:10:48.460 --> 00:10:49.580
Yeah, that's right.

00:10:49.580 --> 00:10:50.040
That's right.

00:10:50.040 --> 00:11:10.280
And so, you know, in some history, Python had arrays, like matrices and what we call tensors now, multidimensional arrays going back all the way to 1995, which is pretty, pretty early history for, for Python, like the Python programming language has only been around since like 1990 or 1991, if my memory serves.

00:11:10.280 --> 00:11:17.300
But the numpy, what became numpy in 2005, 2006 started out as numeric in 1995.

00:11:17.300 --> 00:11:24.640
And it provided numerical computing, multidimensional arrays, matrices, the kind of stuff that you might do might do in MATLAB.

00:11:24.640 --> 00:11:35.600
But it was mainly focused on numerical computing, and not with the type of business data sets that you find in database systems, which contain a lot of strings or dates or non numeric data.

00:11:35.600 --> 00:11:47.240
And so my initial interest was I found Python to be a really productive programming language, I really liked writing code in it, writing simple scripts, like, you know, doing random things, you know, for my for my job.

00:11:47.240 --> 00:11:56.880
But then you had this numerical computing library, NumPy, which enabled you to work with large numeric arrays and large, you know, data sets with a single data type.

00:11:56.880 --> 00:12:06.040
But working with this more tabular type data stuff that you would do in Excel or stuff that you do in a database, it wasn't very easy to do that with with NumPy or it wasn't really designed for that.

00:12:06.040 --> 00:12:19.140
And so that's what led to building this like higher level library that deals with these tabular data sets in the pandas library, which was originally focused on, you know, building really with a really close relationship with NumPy.

00:12:19.140 --> 00:12:22.660
So pandas itself was like a thin layer on top of NumPy originally.

00:12:23.940 --> 00:12:26.760
This portion of Talk Python To Me is brought to you by Neo4j.

00:12:26.760 --> 00:12:31.820
I have told you about Neo4j, the native graph database on previous ad spots.

00:12:31.820 --> 00:12:36.520
This time, I want to tell you about their relatively new podcast, Graph Stuff.

00:12:36.520 --> 00:12:41.560
If you care about graph databases and modeling with graphs, you should definitely give it a listen.

00:12:41.560 --> 00:12:47.240
On their season finale last year, they talked about the intersection of LLMs and knowledge graphs.

00:12:47.240 --> 00:12:49.680
Remember when ChatGPT launched?

00:12:49.680 --> 00:12:53.680
It felt like the LLM was a magical tool out of the toolbox.

00:12:53.680 --> 00:12:57.720
However, the more you use it, the more you realize that's not the case.

00:12:57.720 --> 00:13:01.980
The technology is brilliant, but it's prone to issues such as hallucinations.

00:13:01.980 --> 00:13:03.400
But there's hope.

00:13:03.400 --> 00:13:12.140
If you feed the LLM reliable current data, grounded in the right data and context, then it can make the right connections and give the right answers.

00:13:12.380 --> 00:13:20.700
On the episode, the team at Neo4j explores how to get the results by pairing LLMs with knowledge graphs and vector search.

00:13:20.700 --> 00:13:23.620
Check out their podcast episode on Graph Stuff.

00:13:23.620 --> 00:13:27.400
They share tips for retrieval methods, prompt engineering, and more.

00:13:27.900 --> 00:13:34.820
So just visit talkpython.fm/neo4j dash graph stuff to listen to an episode.

00:13:34.820 --> 00:13:39.380
That's talkpython.fm/neo4j dash graph stuff.

00:13:39.380 --> 00:13:41.700
The link is in your podcast player's show notes.

00:13:41.700 --> 00:13:45.000
Thank you to Neo4j for supporting Talk Python To Me.

00:13:46.080 --> 00:13:58.000
One thing I find interesting about pandas is it's almost its own programming environment these days in the sense that, you know, traditional Python, we do a lot of loops.

00:13:58.000 --> 00:14:02.760
We do a lot of attribute dereferencing, function calling.

00:14:02.760 --> 00:14:06.880
And a lot of what happens in pandas is more functional.

00:14:06.880 --> 00:14:09.020
It's more applied to us.

00:14:09.020 --> 00:14:11.100
It's almost like set operations, right?

00:14:11.460 --> 00:14:14.300
Yeah, lots of vector operations and so on.

00:14:14.300 --> 00:14:16.780
Yeah, that was behavior that was inherited from NumPy.

00:14:16.780 --> 00:14:20.600
So NumPy is very array-oriented, vector-oriented.

00:14:20.600 --> 00:14:36.820
So you, rather than write a for loop, you would write an array expression, which would operate on whole batches of data in a single function call, which is a lot faster because you can drop down into C code and get good performance that way.

00:14:36.820 --> 00:14:43.860
And so pandas adopted the, you know, the NumPy way of like the NumPy-like array expression or vector operations.

00:14:43.860 --> 00:14:58.080
But it's true that that's extended to the types of like non-numeric data operations that you can do in pandas, like, you know, vectorized set lookups where you can say like, you would say like, oh, like this, I have this array of strings and I have this subset of strings.

00:14:58.080 --> 00:15:04.080
And I want to compute a Boolean array, which says whether or not each string is contained in this set of strings.

00:15:04.080 --> 00:15:06.220
And so in pandas, that's the isin function.

00:15:06.220 --> 00:15:17.520
So you would say like column A, like isin some set of substrings and that produces that single function call produces a whole Boolean array that you can use for subsetting later on.

00:15:17.680 --> 00:15:20.300
Yeah, there's a ton of things that are really interesting in there.

00:15:20.300 --> 00:15:24.900
One of the challenges, maybe you could speak to this a little bit, then I want to come back to your performance comment.

00:15:24.900 --> 00:15:32.460
One of the challenges, I think, is that some of these operations are not super obvious that they exist or that they're discoverable, right?

00:15:32.460 --> 00:15:40.440
Like instead of just indexing into, say, a column, you can index on an expression that might filter out the columns or project them or things like that.

00:15:40.440 --> 00:15:45.660
How do you recommend people kind of discover a little bigger breadth of what they can do?

00:15:45.660 --> 00:15:48.660
There's plenty of great books written about pandas.

00:15:48.660 --> 00:15:51.080
So there's my book, Python for Data Analysis.

00:15:51.080 --> 00:15:54.780
I think Matt Harrison has written an excellent book, Effective Pandas.

00:15:54.780 --> 00:16:01.320
The pandas documentation, I think, provides really nitty gritty detail about how all the different things work.

00:16:01.320 --> 00:16:11.820
But when I was writing this book, Python for Data Analysis, my goal was to provide a primer, like a tutorial on how to solve data problems with pandas.

00:16:11.820 --> 00:16:25.080
And so for that, I had to introduce some basics of how NumPy works so people could understand array-oriented computing, basics of Python, so you know enough Python to be able to understand what things that pandas is doing.

00:16:25.080 --> 00:16:30.380
It builds incrementally, and so like as you go through the book, the content gets more and more advanced.

00:16:30.380 --> 00:16:37.160
It introduces, you learn, you master an initial set of techniques, and then you can start learning about more advanced techniques.

00:16:37.160 --> 00:16:40.420
So it's definitely a pedagogical resource.

00:16:40.420 --> 00:16:45.380
And it is now freely, as you're showing there on the screen, it is freely available on the internet.

00:16:45.920 --> 00:16:56.500
So JJ Allaire helped me port the book to use Quarto, which is a new technical publishing system for writing books and blogs and website, you know, Quarto.org.

00:16:56.500 --> 00:17:06.320
And yeah, so that's how I was able to publish my book on the internet, as, you know, essentially, you can use Quarto to write books using Jupyter Notebooks, which is cool.

00:17:06.320 --> 00:17:11.640
My book was written a long time ago in O'Reilly's Docbook XML, so not particularly fun to edit.

00:17:11.640 --> 00:17:19.020
But yeah, because Quarto is built on Pandoc, which is a sort of markup language transpilation system.

00:17:19.020 --> 00:17:24.760
So you can use Pandoc to convert from one, you know, you can to convert documents from one format to another.

00:17:24.760 --> 00:17:34.260
And so that's the kind of the root framework that Quarto is built on for, you know, generating, starting with one document format and generating many different types of output formats.

00:17:34.260 --> 00:17:38.400
That's cool. I didn't realize your book was available just to read on the internet.

00:17:38.400 --> 00:17:39.360
Yeah.

00:17:39.360 --> 00:17:40.480
Navigate around.

00:17:40.480 --> 00:17:54.600
In the third edition, I was able to negotiate with O'Reilly and add a, you know, add an append and make an amendment to my very old book contract from 2011 to let me release the book for free on my website.

00:17:54.600 --> 00:17:58.640
So it's, yeah, it's just available there at westmckinney.com slash book.

00:17:58.920 --> 00:18:02.020
I find that like a lot of people really like the print book.

00:18:02.020 --> 00:18:08.280
And so I think that having the online book just available, like whenever you are somewhere and you want to look something up is great.

00:18:08.280 --> 00:18:09.340
Print books are hard to search.

00:18:09.340 --> 00:18:11.140
Yeah, that's true. That's true. Yeah.

00:18:11.200 --> 00:18:21.720
And like, if you go to the search bar and if you go back to the book and just look at the search bar, you know, look at just search for like group by like, you know, all one word or, you know, yeah, it's like it comes up really fast.

00:18:21.720 --> 00:18:24.820
You can go to that section and it's pretty cool.

00:18:25.120 --> 00:18:31.040
I thought that releasing the book for free online would would affect sales, but no, people just really like having paper books.

00:18:31.040 --> 00:18:32.660
It seems even in 2024.

00:18:32.660 --> 00:18:36.020
Yeah. Even digital books are nice. You got them with you all the time.

00:18:36.020 --> 00:18:37.860
You can I think it's about taking the notes.

00:18:37.860 --> 00:18:40.320
Where do I put my highlights and how do I remember it?

00:18:40.320 --> 00:18:41.000
And that's right.

00:18:41.000 --> 00:18:42.440
Yeah. Yeah. Stuff like that.

00:18:42.760 --> 00:18:45.000
This quarter thing looks super interesting.

00:18:45.000 --> 00:18:55.280
If you look at Pandoc, if people haven't looked at this before, the conversion matrix, I don't know how you would, how would you describe this, Wes?

00:18:55.280 --> 00:18:58.300
Busy? Complete? What is this? This is crazy.

00:18:58.300 --> 00:18:59.600
It's very busy. Yeah.

00:18:59.600 --> 00:19:09.240
It can convert from looks like about, you know, 30 or 40 input formats to, you know, 50 or 60 output formats, maybe, maybe more than that.

00:19:09.500 --> 00:19:12.760
Kind of like just my just eyeballing it. But yeah, it's like a pretty, pretty impressive.

00:19:12.760 --> 00:19:17.880
And then if you took the combinatorial of like how many different ways could you combine the 30 to the 50?

00:19:17.880 --> 00:19:19.640
It's kind of what it looks like. It's right.

00:19:19.640 --> 00:19:28.860
It's truly amazing. So if you've got Markdown, you want to turn it into a PDF or you've got a, a Doku wiki and you want to turn it into an EPUB or whatever.

00:19:28.860 --> 00:19:34.000
Right. Or even like reveal JS probably to PowerPoint, I would imagine. I don't know.

00:19:34.000 --> 00:19:34.220
Yeah.

00:19:34.220 --> 00:19:34.760
Yeah.

00:19:34.760 --> 00:19:37.880
As history, like backstory about, about Quarto.

00:19:38.260 --> 00:19:53.440
So, you know, it helps to keep in mind that, that JJ created ColdFusion, which was this, you know, essentially publishing system, early publishing system for the internet, similar to CGI and, and PHP and, and other dynamic web publishing systems.

00:19:53.440 --> 00:20:12.900
And so at early on at RStudio, they created R Markdown, which is a, basically a extensions to Markdown that allow you to have code cells written in, in R. And then eventually they added support for some other languages where it's kind of like a Jupyter notebook in the sense that you could have some Markdown and some code and some plots and output.

00:20:13.060 --> 00:20:20.440
And you would run the R Markdown renderer and it would, it would, it would, you know, generate all the output and insert it into the document.

00:20:20.440 --> 00:20:24.200
And so you could use that to write blogs and websites and everything.

00:20:24.780 --> 00:20:32.340
But, but R Markdown was written in R. And so that limited, in a sense, like it made it harder to install because you would have to install R to use it.

00:20:32.340 --> 00:20:37.260
And also people, it had an association with R that perhaps was like, like unmerited.

00:20:37.260 --> 00:20:55.660
And so in the meantime, you know, with, with all, all, everything that's happened with web technology, it's now very easy to put a complete JavaScript engine in a stall footprint, you know, on a machine with no dependencies and to be able to run a system that is, you know, written in a system that's written in JavaScript.

00:20:55.660 --> 00:21:02.700
And so Quarto is completely language agnostic. It's written in TypeScript and it uses Pandoc as an, as an underlying engine.

00:21:02.700 --> 00:21:11.100
And it's very easy to install. And so it addresses some of the portability and extensibility issues that were, that were present in R Markdown.

00:21:11.100 --> 00:21:25.440
But, but as a result, you know, I think our, the Posit team had a lot of just has more than a, you know, a decade, or if you include ColdFusion, you know, more than 25 years of experience in, in building really developer friendly technical publishing tools.

00:21:25.440 --> 00:21:37.880
And so I think that, that it's not data science, but it's something that is an important part of the data science workflow, which is how do you present your, make your analysis and your work available for consumption in different formats.

00:21:38.100 --> 00:21:45.680
And so having this, this system that can, you know, publish outputs in, in many different places is, is super valuable.

00:21:45.680 --> 00:21:51.060
So a lot of people start out in Jupyter notebooks, but, but there's many different, you know, many different possible input formats.

00:21:51.140 --> 00:21:59.000
And so to be able to, you know, use the same source to publish to a website or to a Confluence page or to a PDF is like, yeah, it's super valuable.

00:21:59.000 --> 00:22:00.920
Yeah. It's super interesting. Okay.

00:22:00.920 --> 00:22:04.880
So then I got to explore some more. All right, let's go back to Pandas for a minute.

00:22:05.240 --> 00:22:08.260
First, how about some kind words from the audience for you?

00:22:08.260 --> 00:22:12.460
Ravid says, Wes, your work has changed my life. It's very, very nice.

00:22:12.460 --> 00:22:17.980
I'm happy to hear it. But yeah, yeah, I'm more than happy to talk, let's talk in depth about Pandas.

00:22:17.980 --> 00:22:20.500
And I think history of the project is, is interesting.

00:22:20.500 --> 00:22:28.520
And I think also how the project has, has developed in the intervening 15, 16 years is, is pretty interesting as well.

00:22:28.520 --> 00:22:30.440
Yeah. Let's talk derivatives for a minute.

00:22:30.440 --> 00:22:33.680
So growth and speed of adoption and all those things.

00:22:33.680 --> 00:22:41.320
When you first started working on this and you first put it out, did you foresee a world where this was so popular and so important?

00:22:41.320 --> 00:22:45.660
Did you think of, yeah, pretty soon black holes. I'm pretty sure I'll be part of that somehow.

00:22:45.660 --> 00:22:54.140
It was always the aspiration of making Python this mainstream language for statistical computing and data analysis.

00:22:54.140 --> 00:23:05.520
Like I didn't, it didn't occur to me that it would become this popular or that it would become like the, one of the main tools that people use for working with data in a business, in a business setting.

00:23:05.520 --> 00:23:12.560
Like that would have been, if that was the aspiration or if that was, you know, what I needed to achieve to be satisfied, that that would have been completely unreasonable.

00:23:13.220 --> 00:23:19.700
And I also don't think that in a certain sense, like I don't know that, that it's popularity, it is deserved and it's not deserved.

00:23:19.700 --> 00:23:28.240
Like I think there's, there's many other worthy efforts that have been created over the years that have been really great work that, that others have, have done in this, in this domain.

00:23:28.400 --> 00:23:34.940
And so the fact that, that pandas caught on and became as popular as it is, I think it's a combination of timing.

00:23:34.940 --> 00:23:39.700
And, you know, there was like a developer relations aspect that there was content available.

00:23:39.700 --> 00:23:43.580
And like I wrote my book and that made it easier for people to learn how to use the project.

00:23:43.580 --> 00:23:55.820
But also like we, we had a serendipitous open source developer community that, that came together that allowed the project to grow and expand like really rapidly in the early 2010s.

00:23:56.220 --> 00:24:03.060
And I definitely spent a lot of work like recruiting people to work on the project and encouraging, you know, others to work on it.

00:24:03.060 --> 00:24:09.180
Because sometimes people create open source projects and then it's hard for, hard for others to get involved and get a seat at the table, so to speak.

00:24:09.180 --> 00:24:19.500
But I was very keen to bring on others and to give them responsibility and, you know, ultimately, you know, hand over the reins to the project to others.

00:24:19.500 --> 00:24:32.000
And I've spoken a lot about that, you know, over the years, how important that is to, you know, for open source project creators to, to make room for others in, you know, steering and growing the project so that they can become owners of it as well.

00:24:32.000 --> 00:24:35.500
It's tough to make space and tough to, to bring on folks.

00:24:35.700 --> 00:24:37.420
Have you heard of the Django Nauts?

00:24:37.420 --> 00:24:40.380
The Django Nauts, I think it's Django Nauts dot space.

00:24:40.380 --> 00:24:49.940
They have an awesome domain, but it's basically like kind of like a bootcamp, but it's for taking people who just like Django and turn them into actually contributors or core contributors.

00:24:49.940 --> 00:24:53.600
What's your onboarding story for people who do want to participate?

00:24:53.600 --> 00:25:03.840
I'm embarrassed to say that I'm not, I don't have a comprehensive view of like all of the different, you know, community outreach channels that the Pandas project has done to help grow new contributors.

00:25:04.140 --> 00:25:27.520
So one of the core team members, Mark Garcia has done an amazing job organizing documentation sprints and other like contributor sourcing events, essentially creating very friendly, accessible events where people who are interested in getting involved in Pandas can meet each other and then assist each other in making their first pull request.

00:25:27.520 --> 00:25:34.120
And it could be something as simple as, you know, making a small improvement to the, to the Pandas documentation because it's such a large project.

00:25:34.120 --> 00:25:46.100
The documentation is like, you know, either adding more, adding more examples or documenting things that aren't documented or making, yeah, just, just making the documentation better.

00:25:46.640 --> 00:25:54.880
And so it's something that for new contributors is, is more accessible than working on the internals of like one of the algorithms or something.

00:25:54.880 --> 00:26:02.020
And, and, or like we working on some significant performance improvement might be a bit intimidating if you've never worked on the Pandas code base.

00:26:02.160 --> 00:26:07.820
And it's a pretty large code base because it's been, it's been worked on continuously for, you know, for like going on 20 years.

00:26:07.820 --> 00:26:19.640
So it's, yeah, it can be, takes a while to really get to a place where you can be productive and that can be discouraging for new contributors, especially those who don't have a lot of open source experience.

00:26:19.640 --> 00:26:24.960
That's one of the ironies of challenges of these big projects is they're just so finely polished.

00:26:24.960 --> 00:26:26.660
So many people are using them.

00:26:26.660 --> 00:26:29.680
Every edge case matters to somebody, right?

00:26:29.680 --> 00:26:33.900
And so to become a contributor and make changes to that, it takes a while, I'm sure.

00:26:33.900 --> 00:26:34.720
Yeah. Yeah.

00:26:34.720 --> 00:26:46.160
I mean, I think it's definitely a big thing that helped is allowing people to get paid to work on Pandas or to be able to contribute to Pandas as, as a part of their job description.

00:26:46.480 --> 00:26:50.140
Like as, you know, maybe part of their job is maintaining, maintaining Pandas.

00:26:50.140 --> 00:27:03.860
So Anaconda, you know, was like, you know, one of the earliest companies who had engineers on staff, you know, like, you know, Brock Mendel, you know, Tom Augsberger, Jeff Reback, who part of their job was maintaining and developing, developing Pandas.

00:27:03.860 --> 00:27:08.700
And that was, that was huge because prior to that, the project was purely based on volunteers.

00:27:08.700 --> 00:27:15.440
Like I was a volunteer and everyone was working on the project as a, as a passion project in their, in their free time.

00:27:15.440 --> 00:27:20.920
And then, Travis Oliphant, one of the founders, he and Peter Wang founded Anaconda.

00:27:20.920 --> 00:27:26.840
Travis spun out from Anaconda to create Quonsight and has continued to sponsor development in and around Pandas.

00:27:26.840 --> 00:27:35.380
And that's enabled people like Mark to do these community building events and, and for it to not be, you know, something that's, you know, totally uncompensated.

00:27:35.380 --> 00:27:37.300
Yeah, that's a lot of, a lot of stuff going on.

00:27:37.300 --> 00:27:40.560
And I think the commercial interest is awesome, right?

00:27:40.560 --> 00:27:53.200
I mean, if there's just a different level of problems, I feel like we could take on, you know, you know what, I got this entire week and someone, that's my job is to make this work rather than I've, I've got two hours and can't really take on a huge project.

00:27:53.200 --> 00:27:56.080
And so I'll work on the smaller improvements or whatever.

00:27:56.080 --> 00:27:56.340
Yeah.

00:27:56.340 --> 00:28:00.760
Many people know, but I haven't been involved day to day in Pandas since 2013.

00:28:01.080 --> 00:28:02.560
So that's, that's getting on.

00:28:02.560 --> 00:28:03.500
That's a lot of years.

00:28:03.500 --> 00:28:05.640
You know, I still talk to the Pandas contributors.

00:28:05.640 --> 00:28:10.800
we had a, we had a Pandas meetup core, core developer meetup here in Nashville pre COVID.

00:28:10.800 --> 00:28:12.840
I think it was in 2019 maybe.

00:28:12.840 --> 00:28:20.400
So, you know, I'm still in active contact with the Pandas developers, but, it's been a different team of people leading the project.

00:28:20.400 --> 00:28:22.940
It's taken on a life of its own, which is, which is amazing.

00:28:22.940 --> 00:28:23.620
That's exactly.

00:28:23.620 --> 00:28:24.140
Yeah.

00:28:24.140 --> 00:28:31.020
As a project creator, that's exactly what you want is to not be beholden to the project that you created and forced and, you know, kind of.

00:28:31.140 --> 00:28:35.040
Have to be, be responsible for it and take care of it for the rest of your life.

00:28:35.040 --> 00:28:43.480
But if you look at like a lot of the community, a lot of the most kind of intensive community development has happened since, like, since I moved on to work on, on other projects.

00:28:43.480 --> 00:28:48.840
And so now the project is, I don't know the exact count, but it's had thousands of contributors.

00:28:48.840 --> 00:28:55.480
And so, you know, to have thousands of different unique individuals contributing to an open source project is, it's a big deal.

00:28:55.480 --> 00:29:00.560
So I think even, I don't know what it says on the bottom of, on the bottom of GitHub, it says, you know, 30,

00:29:01.000 --> 00:29:12.940
30, 200 contributors, but that's maybe not even the full story because sometimes, you know, people, they don't have their email address associated with their GitHub profile and, you know, how GitHub counts contributors.

00:29:12.940 --> 00:29:15.940
You know, I would say probably the true number is closer to 4,000.

00:29:15.940 --> 00:29:25.120
That's a testament, you know, to the, to the core team and all the outreach that they've done and work making, making the project accessible and easy to contribute to.

00:29:25.260 --> 00:29:31.660
Because if people, if you go and try to make a pull request to a project and there's many different ways that, that you can fail.

00:29:31.660 --> 00:29:38.220
So like either the project is technically like there's issues with the build system or the developer tooling.

00:29:38.680 --> 00:29:40.340
And so you struggle with the developer tooling.

00:29:40.340 --> 00:29:45.280
And so if you aren't working on it every day and every night, you can't make heads or tails of how the developer tools work.

00:29:45.280 --> 00:29:49.900
But then there's also like the level of accessibility of the core development team.

00:29:50.000 --> 00:30:08.560
Like if they don't, if they aren't there to support you in getting involved in the project and learning how it works and creating documentation about how to contribute and what's expected of you, that can also be, you know, a source of frustration where people churn out of the project, you know, because it's just, it's too hard to find their sea legs.

00:30:08.560 --> 00:30:20.800
And maybe also, you know, sometimes development teams are unfriendly or unhelpful or, you know, they make, they make others feel like they make others feel like they're annoyed with them or like they're wasting their time or something.

00:30:20.800 --> 00:30:27.980
It's like, I don't want to look at your, you know, this pull request and give you feedback because, you know, I could do it more quickly by myself or something.

00:30:27.980 --> 00:30:29.800
Like sometimes you see that at open source projects.

00:30:29.800 --> 00:30:30.460
Yeah.

00:30:30.460 --> 00:30:32.960
But they've created a very welcoming environment.

00:30:32.960 --> 00:30:37.040
And yeah, I think the contribution numbers speak for themselves.

00:30:37.040 --> 00:30:37.820
They definitely do.

00:30:38.140 --> 00:30:47.360
Maybe the last thing before we move on to the other stuff you're working on, but the other interesting GitHub statistic here is the used by 1.6 million projects.

00:30:47.360 --> 00:30:50.120
That's, I don't know if I've ever seen it used by that high.

00:30:50.120 --> 00:30:52.220
There's probably some that are higher, but not many.

00:30:52.220 --> 00:30:52.540
Yeah.

00:30:52.540 --> 00:30:53.700
It's a lot of projects.

00:30:53.700 --> 00:30:55.480
I think it's, it's interesting.

00:30:55.480 --> 00:31:02.660
I think like many projects, it's reached a point where it's, it's an essential and assumed part of the, of many people's toolkit.

00:31:02.660 --> 00:31:07.720
Like they, like the first thing that they write at the top of a file that they're working on is important.

00:31:07.720 --> 00:31:20.660
Import pandas as PD or import numpy as PD, you know, to create, I think in a sense, like, I think one of the reasons why, you know, pandas has gotten so popular is that it is beneficial to the community, to the Python community to have fewer solutions.

00:31:20.800 --> 00:31:21.800
Kind of the zen of Python.

00:31:21.800 --> 00:31:26.440
There should be one and preferably only one obvious, obvious way to do things.

00:31:26.820 --> 00:31:32.780
And so if there were 10 different pandas like projects, you know, that creates skill portability problems.

00:31:32.780 --> 00:31:41.060
And it's just easier if everyone says, oh, we just pandas is the thing that we use and you change jobs and you can take all your skills, like how to use pandas with you.

00:31:41.060 --> 00:31:56.120
And I think that's also one of the reasons why Python has become so successful in the business world is because you can teach somebody even without a lot of programming experience, how to use Python, how to use pandas and become productive doing basic work very, very quickly.

00:31:56.120 --> 00:32:05.380
And so one of the solutions I remember back in the early 2010s, there were a lot of articles and talks about how to address the data science shortage.

00:32:06.120 --> 00:32:12.800
And my belief and I gave a I gave a I gave a talk at Web Summit in Dublin in 2000.

00:32:12.800 --> 00:32:16.320
Gosh, maybe 2017, 2000.

00:32:16.320 --> 00:32:17.300
I have to look exactly.

00:32:17.300 --> 00:32:20.540
But basically, it was the data scientist shortage.

00:32:20.540 --> 00:32:32.460
And my thesis was always it's we should make it easier to be a data scientist or like lower the bar for like what sort of skills you have to master before you can you can do productive work in a business setting.

00:32:32.460 --> 00:32:53.600
And so I think the fact that that there is just pandas and that's like the one thing that people have to have to learn how to how to use is like their essential like starting point for doing any data work has also led to this piling on of like people being motivated to make this one thing better because it you know, you make improvements to pandas and they benefit millions of projects and millions of people around the world.

00:32:53.600 --> 00:32:54.560
And that's yeah.

00:32:54.560 --> 00:32:57.220
So it's like a, you know, steady snowballing effect.

00:32:58.840 --> 00:33:05.720
This portion of Talk Python To Me is brought to you by Mailtrap, an email delivery platform that developers love.

00:33:05.720 --> 00:33:17.160
An email sending solution with industry best analytics, SMTP and email API SDKs for major programming languages and 24 seven human support.

00:33:17.160 --> 00:33:20.120
Try for free at mailtrap.io.

00:33:22.000 --> 00:33:24.380
I think doing data science is getting easier.

00:33:24.380 --> 00:33:27.240
We've got a lot of interesting frameworks and tools.

00:33:27.240 --> 00:33:27.840
Yeah.

00:33:27.840 --> 00:33:29.200
For Python, one of them, right?

00:33:29.200 --> 00:33:33.060
That makes it easier to share and run, run your code, you know?

00:33:33.060 --> 00:33:33.240
Yeah.

00:33:33.240 --> 00:33:39.700
Shiny, Shiny for Python, Streamlit, you know, Dash, like these different interactive data application publishing frameworks.

00:33:39.700 --> 00:33:55.840
So you can go from, you know, a few lines of pandas code, loading some data and doing some analysis and visualization to publishing that as an interactive website without having to know how to use any web development frameworks or Node.js or anything like that.

00:33:55.840 --> 00:34:08.580
And so to be able to, you know, get up and running and build a working, you know, interactive web application that's powered by Python is, yeah, it's a game changer in terms of, you know, shortening end-to-end development life cycles.

00:34:08.580 --> 00:34:18.340
What do you think about Jupyter Lite and these PyOxidide and basically Jupyter, Jupyter in a browser type of things?

00:34:18.340 --> 00:34:18.740
Yeah.

00:34:18.740 --> 00:34:19.740
WebAssembly and all that?

00:34:19.740 --> 00:34:20.080
Yeah.

00:34:20.080 --> 00:34:22.520
So definitely very excited about it.

00:34:22.520 --> 00:34:24.560
I've been following WebAssembly in general.

00:34:24.560 --> 00:34:35.960
And so I guess some people listening will know about WebAssembly, but basically it's a portable machine code that can be compiled and executed within your browser in a sandbox environment.

00:34:35.960 --> 00:34:45.780
So it protects against security issues and allows, prevents, like the person who wrote the WebAssembly code from doing something malicious on your machine, which is very important.

00:34:45.780 --> 00:34:50.220
Won't necessarily stop them from like, you know, mining cryptocurrency while you have the browser tab open.

00:34:50.220 --> 00:34:51.340
That's a whole separate problem.

00:34:51.340 --> 00:35:05.120
But it's enabled us to, you know, run the whole scientific Python stack, including Jupyter and NumPy and Pandas totally in the browser without, you know, having a client and server and needing to, you know, run a container someplace in the cloud.

00:35:05.120 --> 00:35:17.220
And so I think in terms of creating application deployment, so like being able to deploy an interactive data application, like with Shiny, for example, without needing to have a server, that's actually pretty amazing.

00:35:17.220 --> 00:35:29.180
And so I think that, you know, simplifies, opens up new use cases, like new application architectures and makes things a lot easier for, because setting up and running a server creates brittleness, like it has cost.

00:35:29.180 --> 00:35:35.000
And so if the browser is doubling as your server process, like that's, I think that's really cool.

00:35:35.000 --> 00:35:42.560
You also have like other projects like DuckDB, which is a high performance, embeddable analytic SQL engine.

00:35:43.020 --> 00:35:49.680
And so, you know, now with DuckDB compiled to WASM, you can get a, you know, high performance database running in your browser.

00:35:49.680 --> 00:35:54.560
And so you can get low latency interactive queries and interactive dashboards.

00:35:54.560 --> 00:36:00.680
And so it's, yeah, there's, WebAssembly has opened up this whole kind of new world of possibilities.

00:36:01.200 --> 00:36:03.220
And it's transformative, I think.

00:36:03.220 --> 00:36:08.200
For Python in particular, you mentioned Pyodide, which is kind of a whole package stack.

00:36:08.200 --> 00:36:16.120
So it's like a framework for build and build and packaging and, you know, basically building an application and managing its dependencies.

00:36:16.120 --> 00:36:21.000
So you could create a WebAssembly version of your application to be deployed like this.

00:36:21.340 --> 00:36:26.840
But yeah, so I think one of the Pyodide, either the Pyodide main creator or maintainer went to Anaconda.

00:36:26.840 --> 00:36:37.320
They created PyScript, which made, which is another attempt to make it even easier to use Python to, to make it even easier to use Python to create web applications, interactive web applications.

00:36:37.320 --> 00:36:38.920
There's so many cool things here.

00:36:38.920 --> 00:36:46.560
Like in the R community, they have WebR, which is, you know, similar to PyScript and Pyodide in some ways, like compiling the whole R stack to WebAssembly.

00:36:46.560 --> 00:36:56.800
There was just an article I saw on Hacker News where they worked on, you know, figuring out how to get, how to trick LLVM into compiling Fortran code, like legacy Fortran code to WebAssembly.

00:36:56.800 --> 00:37:05.980
Because when you're talking about all of this scientific computing stack, you need the linear algebra and all of the 40 years of Fortran code that have been built to support scientific applications.

00:37:05.980 --> 00:37:08.660
Like you need all that to compile to and run in the browser.

00:37:08.660 --> 00:37:11.920
So yeah, that's pretty wild to think of putting that in there, but very useful.

00:37:11.920 --> 00:37:16.600
I didn't realize that you could use DuckDB as a WebAssembly component.

00:37:16.600 --> 00:37:17.180
That's pretty cool.

00:37:17.180 --> 00:37:22.700
Yeah, there's a company, I'm not an investor or plugging them or anything, but it's called evidence.dev.

00:37:22.700 --> 00:37:29.980
It's like a whole like business intelligence, open source business intelligence application that's powered by, powered by DuckDB.

00:37:29.980 --> 00:37:41.120
And so if you have data that fits in the browser, you know, to have a whole like interactive dashboard or be able to do business intelligence, like fully, like fully in the browser with no need of a, no need of a server.

00:37:41.120 --> 00:37:43.500
It's, yeah, it's, it's very, very cool.

00:37:43.820 --> 00:37:56.520
So I've been following DuckDB since the, you know, since the early days and, you know, my company Voltron Data, like we became members of the DuckDB Foundation and build, actively build a relationship with, with DuckDB Labs.

00:37:56.520 --> 00:38:04.180
So we could help accelerate progress in this space because I think the impact, the impact is so, is so immense.

00:38:04.180 --> 00:38:09.180
And we just, it's hard to predict like what, you know, what people are going to build, build with all this stuff.

00:38:09.180 --> 00:38:09.460
Yeah.

00:38:09.460 --> 00:38:24.020
And so that was all, you know, with, I guess, going back, you know, 15 years ago to Python, like one of the reasons I became so passionate about building stuff for Python was about in, I think the way that Peter Wang puts that, it puts it as, you know, giving people superpowers.

00:38:24.020 --> 00:38:29.340
So we want to enable people to build things with much less code and much less time.

00:38:29.860 --> 00:38:39.260
And so by making it things that much more accessible, that much easier to do, like the mantra in Pandas was like, how do we make things one line of code or like this, that must be easy.

00:38:39.260 --> 00:38:41.240
It's like one line of code, one line of code.

00:38:41.240 --> 00:38:58.200
It must be like, like make this as terse and simple and easy to do as possible so that you can move on and focus on building the more interesting parts of your application rather than struggling with how to read a CSV file or, you know, how to do whichever data munging technique that you need for your, for your data set.

00:38:58.420 --> 00:39:07.080
Maybe an interesting mental model for DuckDB is kind of an equivalent to SQLite, but more analytics database for folks, you know, in process and that kind of things, right?

00:39:07.080 --> 00:39:07.580
What do you think?

00:39:07.580 --> 00:39:07.780
Yeah.

00:39:07.780 --> 00:39:10.040
So yeah, DuckDB is like SQLite.

00:39:10.040 --> 00:39:13.880
And in fact, it can run the whole SQLite test suite, I believe.

00:39:13.880 --> 00:39:17.340
So it's a full database, but it's for analytic processing.

00:39:17.340 --> 00:39:19.600
So it's optimized for analytic processing.

00:39:19.600 --> 00:39:23.380
And as compared, you know, with SQLite, which is not for data processing.

00:39:23.380 --> 00:39:23.740
Yeah.

00:39:23.740 --> 00:39:24.140
Cool.

00:39:24.240 --> 00:39:24.420
All right.

00:39:24.420 --> 00:39:29.240
Well, let's talk about some things that you're working on beyond Pandas.

00:39:29.240 --> 00:39:31.640
You talked about Apache Arrow earlier.

00:39:31.640 --> 00:39:34.200
What are you doing with Arrow and how's it fit in your world?

00:39:34.200 --> 00:39:39.460
The backstory there was, I don't know if you can hear the sirens in downtown Nashville, but...

00:39:39.460 --> 00:39:40.640
No, actually, it's...

00:39:40.640 --> 00:39:41.080
It's good.

00:39:41.080 --> 00:39:41.420
It's filled.

00:39:41.420 --> 00:39:43.940
The microphone filters it out, filters it out pretty well.

00:39:43.940 --> 00:39:46.340
Yay for dynamic microphones.

00:39:46.340 --> 00:39:46.820
They're amazing.

00:39:46.980 --> 00:39:47.180
Yeah.

00:39:47.180 --> 00:39:53.820
So in like around the mid, like the mid 2010s, 2015, I started working at Cloudera, like in

00:39:53.820 --> 00:39:54.280
the...

00:39:54.280 --> 00:39:57.980
Which is a company that was like one of the pioneers in the big data ecosystem.

00:39:57.980 --> 00:40:03.940
And I had been spent several years working on five, five years, five, six years working on

00:40:03.940 --> 00:40:04.340
Pandas.

00:40:04.460 --> 00:40:08.840
And so I had gone through the experience of building Pandas from top to bottom.

00:40:08.840 --> 00:40:15.300
And it was this full stack system that had its own, you know, mini query engine, all of

00:40:15.300 --> 00:40:19.640
its own algorithms and data structures and all the stuff that we had to build from scratch.

00:40:19.640 --> 00:40:24.440
And I started thinking about, you know, what if it was possible to build some of the underlying

00:40:24.440 --> 00:40:30.900
computing technology, like data readers, like file readers, all the algorithms that power

00:40:30.900 --> 00:40:37.740
the core components of Pandas, like group operations, aggregations, filtering, selection, all those

00:40:37.740 --> 00:40:38.140
things.

00:40:38.140 --> 00:40:43.160
Like what if it were possible to have a general purpose library that isn't specific to Python,

00:40:43.160 --> 00:40:46.620
isn't specific to Pandas, but is really, really fast, really efficient.

00:40:46.620 --> 00:40:51.820
It has a large community building, building it so that you could take that code with you

00:40:51.820 --> 00:40:55.700
and use it to build many different types of libraries, not just data frame libraries, but

00:40:55.700 --> 00:41:00.220
also database engines and stream processing engines and all kinds of things.

00:41:00.560 --> 00:41:04.960
That was kind of what was in my mind when I started getting interested in what turned into

00:41:04.960 --> 00:41:05.680
Arrow.

00:41:05.680 --> 00:41:10.860
And one of the problems we realized we needed to solve, this was like a group of other open

00:41:10.860 --> 00:41:16.160
source developers and me, was that we needed to create a way to represent data that was not

00:41:16.160 --> 00:41:18.620
tied to a specific programming language.

00:41:18.620 --> 00:41:23.560
And that could be used for a very efficient interchange between components.

00:41:23.560 --> 00:41:29.480
And the idea is that you would have this immutable, this kind of constant data structure, which is

00:41:29.480 --> 00:41:31.820
like it's the same in every programming language.

00:41:31.820 --> 00:41:35.440
And then you can use that as the basis for writing all of your algorithms.

00:41:35.440 --> 00:41:40.520
So as long as it's Arrow, you have these reusable algorithms that process Arrow data.

00:41:40.520 --> 00:41:44.580
So we started with building the Arrow format and standardizing it.

00:41:44.680 --> 00:41:50.780
And then we've built a whole ecosystem of components like library components and different programming

00:41:50.780 --> 00:41:54.340
languages for building applications that use the Arrow format.

00:41:54.340 --> 00:42:00.480
So that includes not only tools for building and interacting with the data, but also file

00:42:00.480 --> 00:42:00.840
readers.

00:42:00.840 --> 00:42:06.780
So you can read CSV files and JSON data and Parquet files, read data out of database systems.

00:42:07.200 --> 00:42:11.080
Wherever the data comes from, we want to have an efficient way to get it into the Arrow format.

00:42:11.080 --> 00:42:18.160
And then we moved on to building data processing engines that are native to the Arrow format so

00:42:18.160 --> 00:42:21.700
that Arrow goes in, the data is processed, Arrow goes out.

00:42:21.700 --> 00:42:26.080
So DuckDB, for example, supports Arrow as a preferred input format.

00:42:26.080 --> 00:42:30.260
And DuckDB is more or less Arrow-like in its internals.

00:42:30.260 --> 00:42:36.260
It has kind of Arrow format plus a number of extensions that are DuckDB specific for better

00:42:36.260 --> 00:42:39.260
performance within the context of DuckDB.

00:42:39.260 --> 00:42:44.000
So in numerous communities, so there's the Rust community, which has built Data Fusion, which

00:42:44.000 --> 00:42:47.540
is an execution engine for Arrow, SQL engine for Arrow.

00:42:47.540 --> 00:42:51.300
And so, yeah, we've kind of like looked at the different layers of the stack, like data

00:42:51.300 --> 00:42:55.120
access, computing, data transport, everything under the sun.

00:42:55.120 --> 00:42:59.320
And then we've built libraries that are across many different programming languages so that

00:42:59.320 --> 00:43:02.780
are, you can pick and choose the pieces that you need to build your system.

00:43:02.780 --> 00:43:08.040
And the goal ultimately was that we, in the future, which is now, we don't want people to

00:43:08.040 --> 00:43:12.080
have to reinvent the wheel whenever they're building something like Pandas, that they could just

00:43:12.080 --> 00:43:13.760
pick up these off-the-shelf components.

00:43:13.920 --> 00:43:18.940
They can design the developer experience, the user experience that they want to create,

00:43:18.940 --> 00:43:20.280
and they can get built.

00:43:20.280 --> 00:43:25.620
You know, so if you were building Pandas now, you could build a Pandas-like library based on

00:43:25.620 --> 00:43:27.900
the Arrow components in much less time.

00:43:27.900 --> 00:43:32.920
And it would be fast and efficient and interoperable with the whole ecosystem of other projects that

00:43:32.920 --> 00:43:33.340
use Arrow.

00:43:33.340 --> 00:43:34.340
It's very cool.

00:43:34.340 --> 00:43:38.380
It's, I mean, it was really ambitious in some ways, obvious to people.

00:43:38.380 --> 00:43:40.920
They would hear about Arrow and they say, that sounds obvious.

00:43:40.920 --> 00:43:46.780
Like, clearly we should have a universal way of transporting data between systems and processing

00:43:46.780 --> 00:43:47.360
it in memory.

00:43:47.360 --> 00:43:49.160
Why hasn't this been done in the past?

00:43:49.160 --> 00:43:54.740
And it turns out that, as is true with many open source software problems, that many of these

00:43:54.740 --> 00:43:58.300
problems are, the social problems are harder than the technical problems.

00:43:58.300 --> 00:44:03.660
And so if you can solve the people coordination and consensus problems, solving the technical

00:44:03.660 --> 00:44:05.900
issues is much, much easier by comparison.

00:44:05.900 --> 00:44:11.400
So I think we were lucky in that we found like the right group of people, the right personalities

00:44:11.400 --> 00:44:16.820
where we were able to, as soon as I met, I met Jacques Nadeau, who had been at MapR and

00:44:16.820 --> 00:44:19.000
we was working on his startup, Dremio.

00:44:19.000 --> 00:44:22.480
Like I knew instantly when I met Jacques Nadeau, I was like, I can work.

00:44:22.480 --> 00:44:23.920
He's like, it's like him.

00:44:23.920 --> 00:44:25.840
Like, he's going to help me make this happen.

00:44:25.840 --> 00:44:31.080
And when I met Julien Ledem, who had also co-created Parquet, I was like, yes, like we are

00:44:31.080 --> 00:44:32.820
going to make, like, I found the right people.

00:44:32.940 --> 00:44:34.400
Like we are, we are going to make this happen.

00:44:34.400 --> 00:44:38.480
It's been a labor of love and much, much work and stress and everything.

00:44:38.480 --> 00:44:43.300
But I've been working on things circling, you know, with Arrow as the sun, you know, I've

00:44:43.300 --> 00:44:48.340
been building kind of satellites and moons and planets circling the Arrow sun over the last

00:44:48.340 --> 00:44:49.120
eight years or so.

00:44:49.120 --> 00:44:50.520
And that's kept me pretty busy.

00:44:50.520 --> 00:44:50.860
Yeah.

00:44:50.860 --> 00:44:52.980
It's only getting more exciting and interesting.

00:44:52.980 --> 00:44:59.140
Over here, it says it uses efficient analytic operations on modern hardware like CPUs and

00:44:59.140 --> 00:44:59.640
GPUs.

00:44:59.980 --> 00:45:03.580
One of the big challenges of Python has been the GIL.

00:45:03.580 --> 00:45:08.040
Also one of its big benefits, but one of its challenges when you get to multi-core computational

00:45:08.040 --> 00:45:09.020
stuff has been the GIL.

00:45:09.020 --> 00:45:10.140
What's the story here?

00:45:10.140 --> 00:45:10.540
Yeah.

00:45:10.540 --> 00:45:18.840
So in Arrowland, when we're talking about analytic efficiency, it mainly has to do with the underlying,

00:45:18.840 --> 00:45:23.640
like how the, how a modern CPU works or how a GPU works.

00:45:24.060 --> 00:45:30.100
And so when the data is arranged in column oriented format that enables the data to be

00:45:30.100 --> 00:45:34.200
moved efficiently through the CPU cache pipelines.

00:45:34.200 --> 00:45:38.860
So the data is made, made available efficiently to the, to the CPU cores.

00:45:39.100 --> 00:45:45.680
And so we spent a lot of energy in Arrow making decisions firstly to enable very cache of like

00:45:45.680 --> 00:45:49.460
CPU cache or GPU cache efficient analytics on the data.

00:45:49.460 --> 00:45:53.860
So we were kind of always, when we were deciding we would break ties and make decisions based

00:45:53.860 --> 00:45:56.700
on like what's going to be more efficient for the, for the computer chip.

00:45:56.700 --> 00:46:02.560
The other thing is that modern, and this is true with GPUs, which have a different parallelism

00:46:02.560 --> 00:46:08.080
model than, or different kind of multi-core parallelism model than CPUs.

00:46:08.080 --> 00:46:14.400
But in CPUs, they've focused on adding what are called single instruction, multiple data intrinsic,

00:46:14.400 --> 00:46:21.760
like a built-in operations in the processor where, you know, now you can process up to 512

00:46:21.760 --> 00:46:24.700
bytes of data in a single CPU instruction.

00:46:24.700 --> 00:46:28.320
And so that's like, my brain's doing the math, right?

00:46:28.320 --> 00:46:34.300
Like 16 32-bit floats or, you know, eight 64-bit integers in a single CPU cycle.

00:46:34.300 --> 00:46:35.740
There's like intrinsic operations.

00:46:35.740 --> 00:46:41.020
So multiply this number by that one, multiply that number to these eight things all at once,

00:46:41.020 --> 00:46:41.560
something like that.

00:46:41.560 --> 00:46:42.100
That's right.

00:46:42.100 --> 00:46:42.400
Yeah.

00:46:42.460 --> 00:46:47.480
Or you might say like, oh, I have a bit mask and I want to select, I want to gather

00:46:47.480 --> 00:46:52.560
like the one bits that are set in this bit mask from this array of integers.

00:46:52.560 --> 00:46:59.060
And so there's like a gather instruction, which allows you to select a subset sort of SIMD vector

00:46:59.060 --> 00:47:01.120
of integers, you know, using a bit mask.

00:47:01.120 --> 00:47:05.820
And so that turns out to be like a pretty critical operation in certain data analytic workloads.

00:47:05.820 --> 00:47:10.780
So yeah, we were really, we wanted to have a data format that was essentially, you know,

00:47:10.840 --> 00:47:16.480
future-proofed in the sense that it's, it's ideal for the coming wave, like current, current

00:47:16.480 --> 00:47:22.300
generation of CPUs, but also given that a lot of processing is moving to GPUs and to FPGAs

00:47:22.300 --> 00:47:27.400
and, and to custom silicon, like we wanted Arrow to be usable there as well.

00:47:27.400 --> 00:47:33.200
And it's, Arrow has been successfully, you know, used as the foundation of GPU computing libraries.

00:47:33.200 --> 00:47:38.860
Like we kind of at Voltron Data, we built, we've built a whole accelerator native GPU native,

00:47:39.080 --> 00:47:42.240
you know, scalable execution engine that's, that's Arrow based.

00:47:42.240 --> 00:47:46.500
And so I think the fact that we, that was our aspiration and we've been able to prove that out

00:47:46.500 --> 00:47:51.740
in, in real world workloads and show the kinds of efficiency gains that you can get with using

00:47:51.740 --> 00:47:56.320
modern computing hardware correctly, or at least as well as it's intended to be used.

00:47:56.320 --> 00:47:59.720
That's a big deal in terms of like making applications faster,

00:47:59.720 --> 00:48:03.900
reducing the carbon footprint of large scale data workloads, things like that.

00:48:04.060 --> 00:48:07.340
Yeah. Amazing. All right. Let's see. What else have I got on deck here to talk

00:48:07.340 --> 00:48:11.800
to you about? You want to talk IBIS or which one do you want to, we got a little time left. We got

00:48:11.800 --> 00:48:13.060
a couple of things to cover.

00:48:13.060 --> 00:48:17.100
Yeah. Let's, we can talk about IBIS. Yeah. We could, we could probably spend another hour talking.

00:48:17.100 --> 00:48:17.840
Yes. Easy.

00:48:18.620 --> 00:48:23.760
I think one of the more interesting areas in recent years has been new data frame libraries

00:48:23.760 --> 00:48:30.040
and data frame APIs that transpile or compile to different execute on different backends. And so

00:48:30.040 --> 00:48:35.740
around the time that I was helping start Arrow, I created this project called IBIS, which is basically

00:48:35.740 --> 00:48:43.900
a portable data frame API that knows how to generate SQL queries and compile to pandas and polars and

00:48:43.900 --> 00:48:50.580
different data frame, data frame backends. And the goal is to provide a really productive data frame

00:48:50.580 --> 00:48:56.320
API that gives you portability across different execution backends with the goal of enabling what

00:48:56.320 --> 00:49:02.180
we call the multi-engine data stack. So you aren't stuck with using one particular system because all of

00:49:02.180 --> 00:49:06.440
the code that you've written is specialized to that system. You have this tool, which,

00:49:06.440 --> 00:49:12.740
so maybe you could work with, you know, DuckDB on your laptop or pandas or polars with IBIS on your

00:49:12.740 --> 00:49:17.200
laptop. But if you have, if you need to run that workload someplace else, maybe with, you know,

00:49:17.200 --> 00:49:22.560
ClickHouse or BigQuery, or maybe it's a large big data workload that's too big to fit on your laptop

00:49:22.560 --> 00:49:28.760
and you need to use Spark SQL or something that you can just ask IBIS, say, Hey, I want to do the same

00:49:28.760 --> 00:49:34.420
thing on this larger data set over here. And it has all the logic to generate the correct query

00:49:34.420 --> 00:49:38.920
representation and run that workload for you. So it's super useful, but there's a whole wave of like,

00:49:38.920 --> 00:49:44.780
you know, work right now to help enable people to work in a pandas like way, but get work with big

00:49:44.780 --> 00:49:50.740
data or, you know, get better performance than pandas because pandas is a Swiss army knife, but is,

00:49:50.740 --> 00:49:54.880
isn't a chainsaw. So it, if you were rebuilding pandas from scratch, it would end up a lot.

00:49:55.200 --> 00:50:00.320
There's areas of the project that are more bloated or have performance overhead. That's hard to get

00:50:00.320 --> 00:50:05.720
rid of. And so that's why you have Richie Fink started the Polars project, which is kind of a

00:50:05.720 --> 00:50:12.240
re-imagining of, of pandas, pandas dataframes written in Rust and exposed in Python. And Polars,

00:50:12.240 --> 00:50:18.880
of course, is built on Apache Arrow at its core. So building an Arrow native dataframe library in Rust and,

00:50:18.880 --> 00:50:23.380
you know, all the benefits that come with building Python extensions in Rust, you know,

00:50:23.380 --> 00:50:27.180
you avoid the GIL and you can manage the multi-threading in a systems language, all that,

00:50:27.180 --> 00:50:28.000
all that fun stuff.

00:50:28.000 --> 00:50:32.660
Yeah. When you're talking about Arrow and supporting different ways of using it and things being built

00:50:32.660 --> 00:50:36.800
on it, it's certainly Polars came to mind for me. You know, when you talk about IBIS, I think it's

00:50:36.800 --> 00:50:43.460
interesting that a lot of these dataframe libraries, they try to base their API to be

00:50:43.460 --> 00:50:48.040
pandas-like, but not identical potentially, you know, thinking of Dask and others.

00:50:48.500 --> 00:50:54.380
But this IBIS sort of has the ability to configure it and extend it and make it different,

00:50:54.380 --> 00:50:57.360
kind of like, for example, Dask, which is one of the backends here.

00:50:57.360 --> 00:50:57.660
Yeah.

00:50:57.660 --> 00:51:01.140
But the API doesn't change, right? It just, it talks to the different backends.

00:51:01.140 --> 00:51:05.300
Yeah. There's different schools of thought on this. So there's another project called Moden,

00:51:05.300 --> 00:51:10.660
which is similar to IBIS in many ways in the sense of like transpilation and sort of dynamically

00:51:10.660 --> 00:51:17.780
supporting different backends, but sought to closely emulate exact details of like the API call,

00:51:17.780 --> 00:51:23.520
the function name, the function arguments must be exactly the same as pandas to with the goal of

00:51:23.520 --> 00:51:28.100
being a drop-in replacement for people's pandas code. And that's one approach, kind of the pandas

00:51:28.100 --> 00:51:33.920
emulation route. And there's a library called Koalas for Spark, which is like a PySpark

00:51:33.920 --> 00:51:38.880
emulation layer for the pandas API. And then there's other projects like Polars and IBIS,

00:51:38.880 --> 00:51:45.260
Das DataFrame that take like design cues from pandas in the sense of like the general way in

00:51:45.260 --> 00:51:50.000
which the API works, but has made meaningful departures in the interest of doing things

00:51:50.000 --> 00:51:56.080
better in many ways than pandas did in certain parts of the API and making things simpler and not

00:51:56.080 --> 00:52:00.000
being beholden to decisions that were made in pandas, you know, 15 years ago.

00:52:00.200 --> 00:52:04.320
Not to say there's anything bad about the pandas API, but like with any API, it's large,

00:52:04.320 --> 00:52:09.180
like it's, it's very large as evidenced by, you know, the 2000 page pages of documentation.

00:52:09.180 --> 00:52:14.740
And so I understand the desire to make things simpler while also refining certain things,

00:52:14.740 --> 00:52:19.800
making certain types of workloads easier to easier to express. And so Polars, for example,

00:52:19.800 --> 00:52:25.820
is very expression based. And so everything is column expressions and is lazy and not eagerly

00:52:25.820 --> 00:52:32.540
computed. Whereas pandas is eager execution, just like NumPy is, which is how pandas became eagerly

00:52:32.540 --> 00:52:39.080
executed in the first place. And so I think the mantra with Polars was we don't want to support the

00:52:39.080 --> 00:52:44.520
eager execution by default that pandas provides. We want to be able to build expressions so that we can

00:52:44.520 --> 00:52:50.680
do query optimization and take inefficient code and under the hood, rewrite it to be more efficient,

00:52:50.680 --> 00:52:52.980
which is, you know, what you can do with a query optimizer.

00:52:52.980 --> 00:52:58.180
And so ultimately like that matters a lot when you're executing code remotely or in like a big

00:52:58.180 --> 00:53:04.000
data system, you want to have the freedom to be able to take like a lazy analytic expression and

00:53:04.000 --> 00:53:09.340
rewrite it based on, it might be like, you need to seriously rewrite the expression in the case of

00:53:09.340 --> 00:53:15.560
like Dask, for example, like Dask has to do planning across a distributed cluster. And so, you know,

00:53:15.560 --> 00:53:21.060
Dask data frame is very pandas like, but it also includes some explicit details of being able to

00:53:21.060 --> 00:53:25.940
control how the data is partitioned and being able to have some knobs to turn in terms of like

00:53:25.940 --> 00:53:30.560
having more control over what's happening on a distributed cluster. And I think the goal there

00:53:30.560 --> 00:53:34.340
is like to give the developer more control as opposed to like trying to be intelligent, you know,

00:53:34.340 --> 00:53:39.000
make all of the decisions on behalf of the developer. So, you know, if you know about how,

00:53:39.000 --> 00:53:43.300
you know, know a lot about your data set, then you can make more, you can make, you know,

00:53:43.300 --> 00:53:47.960
decisions about how to schedule and execute it. Of course, Dask is building, you know,

00:53:48.020 --> 00:53:51.900
query optimization to start making more of those decisions on behalf of the user. But,

00:53:51.900 --> 00:53:57.260
you know, Dask has become very popular and impactful and making distributed computing easier in Python.

00:53:57.260 --> 00:54:01.760
So they've gotten, I think, gotten a long way without turning into a database. And I think Dask

00:54:01.760 --> 00:54:06.540
never aspired to be a, to be a database engine, which is a lot of distributed computing is, you know,

00:54:06.540 --> 00:54:10.880
not database like it's could be distributed array computing or distributed model training and

00:54:10.880 --> 00:54:16.200
just being able to easily run distributed Python functions on a cluster, do distributed computing

00:54:16.200 --> 00:54:21.280
that way. It was amazing. Like how many people were using PySpark in the early days, just for the

00:54:21.280 --> 00:54:25.720
convenience of being able to run Python functions in parallel on a cluster.

00:54:25.720 --> 00:54:29.540
Yeah. And that's pretty interesting. Not exactly what it's designed for.

00:54:29.540 --> 00:54:29.820
Right.

00:54:29.820 --> 00:54:36.020
You know, you probably come across situations where you do a sequence of operations. They're kind of

00:54:36.020 --> 00:54:40.580
commutative in the end and practice, but from a computational perspective, like how do I distribute

00:54:40.580 --> 00:54:46.180
this amongst different servers? Maybe one order matters a lot more than the other performance,

00:54:46.180 --> 00:54:46.720
you know?

00:54:46.720 --> 00:54:46.920
Yeah.

00:54:46.920 --> 00:54:50.440
Yeah. Interesting. All right. One final thing. SQLglot.

00:54:50.440 --> 00:54:58.180
Yeah. So SQLglot project started by Toby Mao. So he's a Netflix alum and, you know, really,

00:54:58.180 --> 00:55:05.380
really talented, talented developer who's created this SQL query transpilation framework library for Python.

00:55:05.380 --> 00:55:10.840
And, you know, kind of underlying core library. And so the problem that's being solved there is that

00:55:10.840 --> 00:55:16.020
SQL, despite being a quote unquote standard, is not at all standardized across different database

00:55:16.020 --> 00:55:21.940
systems. And so if you want to take your SQL queries written for one engine and use them someplace else,

00:55:21.940 --> 00:55:26.600
without something like SQLglot, you would have to manually rewrite and make sure you get the

00:55:26.600 --> 00:55:34.060
typecasting and coalescing rules correct. And so SQLglot has, understands the intricacies and the

00:55:34.060 --> 00:55:40.080
quirks of every database dialect, SQL dialect, and knows how to correctly translate from one dialect

00:55:40.080 --> 00:55:46.660
to another. And so IBIS now uses SQLglot as its underlying engine for query transpilation and

00:55:46.660 --> 00:55:53.640
generating SQL outputs. So originally IBIS had its own kind of bad version of SQLglot, kind of a query

00:55:53.640 --> 00:56:00.560
transpile, like SQL transpilation that was powered by, I think, powered by SQLAlchemy and some,

00:56:00.700 --> 00:56:05.080
a bunch of custom code. And so I think they've been able to delete a lot of code in IBIS by moving to

00:56:05.080 --> 00:56:11.020
SQLglot. And I know that, you know, SQLglot is also, you know, being used to power kind of a new,

00:56:11.020 --> 00:56:15.920
yeah, being used in people building new products that are Python powered and things like that.

00:56:15.920 --> 00:56:21.620
So, and Toby, like his, his company, Tobiko data, they, yeah, they're, they're building a product

00:56:21.620 --> 00:56:26.780
called SQL mesh that's powered by SQLglot. So very cool project and maybe a bit in the weeds,

00:56:26.780 --> 00:56:30.960
but if you've ever needed to convert a SQL query from one dialect to another, it's a, yeah,

00:56:30.960 --> 00:56:32.380
SQLglot is here to save the day.

00:56:32.380 --> 00:56:37.560
I would say, you know, even simple things is how do you specify a parameter variable,

00:56:37.560 --> 00:56:42.880
you know, for a parameterized query, right? And Microsoft SQL server, it's like at the very,

00:56:42.880 --> 00:56:47.300
the, the parameter name and Oracle, it's like question mark or SQLite. I think it's also,

00:56:47.300 --> 00:56:49.400
you know, just that, even those simple things.

00:56:49.400 --> 00:56:50.280
It's a pain, yeah.

00:56:50.280 --> 00:56:53.580
And without it, you end up with little Bobby tables, which is also not good. So.

00:56:53.580 --> 00:56:54.960
That's true. That's true.

00:56:54.960 --> 00:56:59.800
Nobody wants to talk to him. Yeah, this is really cool. SQLglot, like polyglot,

00:56:59.800 --> 00:57:05.140
but all the languages of SQL. Nice. And you do things like, you can say,

00:57:05.140 --> 00:57:10.620
read DuckDB and write to Hive or read DuckDB and then write to Spark or,

00:57:10.620 --> 00:57:14.600
or whatever. It's pretty cool. Yeah. All right, Wes, I think we're getting short on time,

00:57:14.600 --> 00:57:20.140
but you know, I know everybody appreciated hearing from you and hearing what you're up to these days.

00:57:20.140 --> 00:57:22.780
Anything you want to add before we wrap up?

00:57:22.780 --> 00:57:29.260
I don't think so. Yeah. I enjoyed the conversation and yeah, there's a lot of stuff going on and

00:57:29.260 --> 00:57:34.560
still plenty of things to get, get excited about. So I think often people feel like all the exciting

00:57:34.560 --> 00:57:40.900
problems in the Python ecosystem have been solved, but there's still a lot to do. And yeah, we've made

00:57:40.900 --> 00:57:46.960
a lot of progress in the last 15 plus years, but in some ways feels like we're just getting started.

00:57:46.960 --> 00:57:49.760
So we are just excited to see where things go next.

00:57:49.760 --> 00:57:53.360
Yeah. Every time I think all the problems are solved, then you discover all these new things

00:57:53.360 --> 00:57:56.900
that are so creative and you're like, oh, well, that was a big problem. I didn't even know it was a

00:57:56.900 --> 00:58:01.920
problem. It's great. All right. Thank you for being here and taking the time and keep us updated on what

00:58:01.920 --> 00:58:04.440
you're up to. All right. Thanks for joining us. Bye-bye. Bye-bye.

00:58:04.440 --> 00:58:11.300
This has been another episode of Talk Python To Me. Thank you to our sponsors. Be sure to check

00:58:11.300 --> 00:58:16.200
out what they're offering. It really helps support the show. It's time to stop asking relational

00:58:16.200 --> 00:58:21.640
databases to do more than they were made for and simplify complex data models with graphs.

00:58:21.640 --> 00:58:28.460
Check out the sample FastAPI project and see what Neo4j, a native graph database, can do for you.

00:58:28.460 --> 00:58:36.960
Find out more at talkpython.fm/Neo4j. Mailtrap, an email delivery platform that developers love.

00:58:36.960 --> 00:58:44.400
Try for free at mailtrap.io. Want to level up your Python? We have one of the largest catalogs of Python

00:58:44.400 --> 00:58:49.880
video courses over at Talk Python. Our content ranges from true beginners to deeply advanced topics like

00:58:49.880 --> 00:58:54.800
memory and async. And best of all, there's not a subscription in sight. Check it out for yourself

00:58:54.800 --> 00:59:00.980
at training.talkpython.fm. Be sure to subscribe to the show. Open your favorite podcast app and search

00:59:00.980 --> 00:59:05.720
for Python. We should be right at the top. You can also find the iTunes feed at /itunes,

00:59:05.720 --> 00:59:12.120
the Google Play feed at /play, and the direct RSS feed at /rss on talkpython.fm.

00:59:12.120 --> 00:59:16.800
We're live streaming most of our recordings these days. If you want to be part of the show and have

00:59:16.800 --> 00:59:21.980
your comments featured on the air, be sure to subscribe to our YouTube channel at talkpython.fm

00:59:21.980 --> 00:59:27.200
slash YouTube. This is your host, Michael Kennedy. Thanks so much for listening. I really appreciate

00:59:27.200 --> 00:59:29.360
it. Now get out there and write some Python code.

00:59:29.360 --> 00:59:59.340
Thank you.

