WEBVTT

00:00:00.001 --> 00:00:05.880
In this special episode, you'll meet DJ Patil, the current Chief Data Scientist of the United States.

00:00:05.880 --> 00:00:09.440
You'll hear his thoughts on data at the level of the United States government,

00:00:09.440 --> 00:00:12.300
and look back on his term over the past few years.

00:00:12.300 --> 00:00:15.480
This is Talk Python to Me, Episode 89.

00:00:15.480 --> 00:00:43.600
Welcome to Talk Python to Me, a weekly podcast on Python,

00:00:43.600 --> 00:00:46.680
the language, the libraries, the ecosystem, and the personalities.

00:00:46.680 --> 00:00:50.780
This is your host, Michael Kennedy. Follow me on Twitter, where I'm @mkennedy.

00:00:50.780 --> 00:00:54.660
Keep up with the show and listen to past episodes at talkpython.fm,

00:00:54.660 --> 00:00:57.200
and follow the show on Twitter via at Talk Python.

00:00:57.200 --> 00:01:02.120
This episode has been sponsored by Rollbar and GoCD.

00:01:02.120 --> 00:01:07.820
Thank them both for supporting the podcast by checking out what they're offering during their segments.

00:01:07.820 --> 00:01:11.260
Hey, Jonathan. Welcome back to Talk Python.

00:01:11.640 --> 00:01:13.920
Hey, man. Thanks so much for having me. It's great to be back.

00:01:13.920 --> 00:01:16.760
Yeah, absolutely. You're actually here twice this month.

00:01:16.760 --> 00:01:21.580
So later this month, we're going to do our top 10 data science stories of 2016, and that's super fun.

00:01:21.580 --> 00:01:24.080
We already recorded it, and I'm looking forward to sharing it with everyone.

00:01:24.080 --> 00:01:27.160
But we actually have a really special opportunity here.

00:01:27.160 --> 00:01:30.800
And you talked to me a few weeks ago and said, hey, look, I have this great opportunity.

00:01:30.800 --> 00:01:35.580
Could I do a co-host show on Talk Python?

00:01:35.780 --> 00:01:38.460
And I'll just let you tell everyone what it is because it's really great.

00:01:38.460 --> 00:01:40.700
You told me what it was. I'm like, yes, you should do this.

00:01:40.700 --> 00:01:41.940
What do you got on store for us?

00:01:41.940 --> 00:01:43.240
Yeah, yeah, absolutely.

00:01:43.240 --> 00:01:49.680
So thank you, by the way, and thanks to the audience for letting me elbow into this week's episode.

00:01:50.160 --> 00:01:51.640
But basically, we got invited.

00:01:51.640 --> 00:01:58.840
So I co-host a podcast called Partially Derivative, which is a kind of a nerdy data science and data podcast.

00:01:58.840 --> 00:02:05.760
And we got invited to the White House to go interview the U.S. chief data scientist, DJ Patil,

00:02:05.760 --> 00:02:11.140
who's part of the Office of Science and Technology Policy, which is where the CTO, Megan Smith, works.

00:02:11.560 --> 00:02:19.280
And so all of the cool stuff that government has been doing during the Obama administration to get technology and data and data science into government,

00:02:19.280 --> 00:02:22.980
we just kind of wanted to talk to him about it and say, like, how's it been going?

00:02:22.980 --> 00:02:27.740
You know, basically, like, do DJ's exit interview as the U.S. chief data scientist.

00:02:27.740 --> 00:02:29.900
And it was awesome.

00:02:29.900 --> 00:02:31.580
I mean, we got to go to the actual White House.

00:02:32.580 --> 00:02:39.680
And we just wanted to share what we learned from DJ and the conversation that we had with as many tech people as possible.

00:02:39.680 --> 00:02:45.140
And so I thought, well, I know a guy who gets to talk to a lot of really fantastic tech people every week.

00:02:45.140 --> 00:02:46.720
And so I'm really glad this worked out.

00:02:46.720 --> 00:02:48.180
Yeah, I'm really glad as well.

00:02:48.180 --> 00:02:55.160
And, you know, doing a show live from the White House is pretty amazing for a podcast, I think, especially for a tech podcast.

00:02:55.160 --> 00:02:58.860
DJ is the first chief data scientist in the United States, right?

00:02:58.860 --> 00:02:59.620
That's right.

00:02:59.620 --> 00:03:03.140
He's the first person to ever hold that position, which is which is very cool.

00:03:03.140 --> 00:03:06.420
I think he really understands the significance of that of that position.

00:03:06.420 --> 00:03:08.420
So it was really fun to hear him reflect on it.

00:03:08.420 --> 00:03:11.220
Well, I really enjoyed the interview that you did with him.

00:03:11.220 --> 00:03:13.700
And so without further ado, take it to the White House.

00:03:13.700 --> 00:03:14.340
All right.

00:03:14.340 --> 00:03:15.420
Let's go to the White House.

00:03:15.420 --> 00:03:16.140
All right.

00:03:16.140 --> 00:03:23.420
So I am here in the actual White House with the actual chief data scientist of the United States.

00:03:23.420 --> 00:03:25.100
DJ, thank you so much for being on the show.

00:03:25.100 --> 00:03:25.700
My pleasure.

00:03:25.700 --> 00:03:26.400
Awesome.

00:03:26.400 --> 00:03:26.720
All right.

00:03:26.720 --> 00:03:28.220
So we should probably start first things first.

00:03:28.340 --> 00:03:31.460
I think most of the audience will be familiar with who you are and your work.

00:03:31.460 --> 00:03:35.960
But just in case they're not, what does the chief data scientist of the United States do?

00:03:35.960 --> 00:03:36.840
Like, what's your gig?

00:03:36.840 --> 00:03:41.780
Well, the simplest way to put it is actually the mission the president gave us.

00:03:42.180 --> 00:03:50.560
And it's something that's sort of phenomenal in itself is that how does a constitutional law professor get so focused on data and technology?

00:03:50.560 --> 00:03:58.880
And the mission he gave us is to responsibly unleash the power of data to benefit all Americans in return on America's investment in data.

00:03:59.240 --> 00:04:00.740
And the components that I think are critical.

00:04:00.740 --> 00:04:13.060
And the components that I think are critical there, responsibly, something that we've, we've, in all of, I mean, both you and Chris and NVIDIA have talked about extensively, is what does responsibly mean with respect to data algorithms technology?

00:04:13.300 --> 00:04:16.400
And then what does it mean to benefit all Americans?

00:04:16.400 --> 00:04:26.480
You know, just because we have technology, you know, and then people have access to certain systems and solutions doesn't mean everyone in the country does.

00:04:26.640 --> 00:04:28.080
So how do you make that happen?

00:04:28.080 --> 00:04:34.120
And if our belief in theory of the case is that if we do it here, everybody around the world will benefit as a result.

00:04:34.120 --> 00:04:34.760
Yeah.

00:04:34.760 --> 00:04:42.820
I mean, fortunately, that's a nice, small, manageable mandate that, you know, has no serious, long, broad reaching implications or consequences.

00:04:42.820 --> 00:04:47.920
But I mean, so you've, and, but you've been a part of this from the beginning, obviously, you're the first chief data scientist.

00:04:47.920 --> 00:04:55.520
So how, how have you, how have those, how's that mission that the president gave you, how's that been playing out over your time here?

00:04:55.840 --> 00:05:00.120
Well, I actually think that the first chief data scientist was really Washington.

00:05:00.120 --> 00:05:02.080
He's a cartographer.

00:05:02.080 --> 00:05:03.480
That's true.

00:05:03.480 --> 00:05:04.020
That's true.

00:05:04.020 --> 00:05:22.240
And if you look through the arc of history, you know, even Lincoln did basically Euclid's principles of mathematics from first, like we've had a lot of presidents who've been deeply, deeply mathematical or analytical in their, their, just the way they operate, the way they think.

00:05:22.920 --> 00:05:29.560
I think what's true in the case specifically for this president, when you walk into the oval, you don't see dishware.

00:05:29.560 --> 00:05:42.200
You don't see, you know, just kind of like kind of little cotch, tchotchkes in the, in the wall that are that he actually has the submissions of the real original patents for things like the telegraph and the gear cutter.

00:05:42.420 --> 00:05:56.660
And the reason for that is if you think through the arc of our entire history as a country from the founding of the institution, what's really there is, is that in every case data and technology has been a force multiplier.

00:05:56.840 --> 00:06:01.140
It has really transformed our ability to move as a society.

00:06:01.140 --> 00:06:04.980
And we're seeing that next wave of transformation take place right now.

00:06:05.700 --> 00:06:20.580
So what does that look like over this arc of just this time period, just in this time period alone, just in the last couple of years, we have major movements on precision medicine, the idea of creating tailored treatments.

00:06:20.580 --> 00:06:28.580
And as, as, as for health purposes, you have the affordable care act that kind of doubles down in a way that people don't always realize.

00:06:29.020 --> 00:06:34.520
One of those components is that you can't be denied coverage because of a preexisting condition.

00:06:34.520 --> 00:06:38.340
When you get to the genome, every one of us has a preexisting condition.

00:06:38.340 --> 00:06:40.000
It's called being human.

00:06:40.000 --> 00:06:43.560
So, so there's kind of like these fundamental kind of things that are entwined.

00:06:43.560 --> 00:06:44.740
Cancer moonshot.

00:06:44.740 --> 00:06:54.980
All the aspects of cancer are fundamentally based on being able to move data, collect it, store it, use it responsibly, and then act on it extremely fast, get you to the right treatment.

00:06:54.980 --> 00:06:58.220
So right cares, smarter, faster, better, those things.

00:06:58.220 --> 00:07:00.320
And criminal justice.

00:07:00.320 --> 00:07:11.660
We have the data driven justice program on the police data initiative, both working on different sides, one working to create transparency for police departments with citizens, the other to find early intervention techniques.

00:07:11.660 --> 00:07:17.520
All of these is just a few across every single aspect that we have.

00:07:17.520 --> 00:07:28.200
There is data at almost every level of conversation, whether that's how do you think about getting kids into school, whether that's national security.

00:07:28.640 --> 00:07:30.060
That's a weather forecast.

00:07:30.060 --> 00:07:31.580
It's in every single thing.

00:07:32.040 --> 00:07:39.500
Every single thing is as it's supposed to be, as it is supposed to be with DNA, which just basically in lies that everyone that's listening out there, we have good job security.

00:07:39.500 --> 00:07:41.620
That's true.

00:07:41.620 --> 00:07:42.040
That's true.

00:07:42.040 --> 00:07:43.800
It's a good time to be a data person.

00:07:44.680 --> 00:07:46.820
And you touched on something interesting just there.

00:07:46.820 --> 00:07:53.880
I mean, there's these sort of specific programs and initiatives that you talked about, the precision medicine and kind of data driven policing.

00:07:54.120 --> 00:07:58.920
But then you've also you also talked about this kind of general capacity building inside government.

00:07:58.920 --> 00:08:06.560
I mean, this administration has used data or kind of embedded data into different agencies like no other administration before.

00:08:06.560 --> 00:08:16.420
And can you talk a little bit about the play between those two things, both kind of building capacity, looking at big issues like government transparency and accountability and how data informs that.

00:08:16.420 --> 00:08:20.080
But then also and then we can talk about some of these specific examples as well.

00:08:20.140 --> 00:08:25.720
But I'd be interested in how you this kind of this general idea about growing the data capacity of government.

00:08:25.720 --> 00:08:28.260
And that's been a real I think a real revolution here in D.C.

00:08:28.260 --> 00:08:28.460
Right.

00:08:28.460 --> 00:08:38.040
Well, it's actually one of the more fascinating things is that, you know, there's this narrative going around that Silicon Valley has got to come to D.C. to save it.

00:08:38.040 --> 00:08:42.880
But people forget, though, like where is all the investment in data originally come from?

00:08:42.880 --> 00:08:48.760
You know, it's the government, whether it's census, whether or any of the other type of things.

00:08:49.340 --> 00:08:53.560
DoD has funded some of the greatest advancements in data.

00:08:53.560 --> 00:08:56.900
So as the Department of Energy, so has NIH.

00:08:56.900 --> 00:09:09.920
All these programs, whether it's CERN and big, large scale atomic, you know, understanding of forces of nature to just the operational aspects of health care.

00:09:09.920 --> 00:09:11.040
It isn't everything.

00:09:11.040 --> 00:09:18.600
The part that I think is unique and why there's a data site, like why do we need a chief data scientist when we have?

00:09:19.040 --> 00:09:21.660
We're economists and statistician.

00:09:21.660 --> 00:09:23.220
There's a chief statistician, by the way.

00:09:23.260 --> 00:09:28.620
There is basically a chief economist, Jason Furman, who's the head of Council Economic Advisors.

00:09:28.620 --> 00:09:30.560
So why do we need a chief data scientist?

00:09:30.560 --> 00:09:37.160
It's interesting that the statistician and economists don't always talk to each other.

00:09:37.160 --> 00:09:39.420
And it's not a Bayesian thing.

00:09:39.420 --> 00:09:43.480
It's like we're in different verticals or different silos.

00:09:43.480 --> 00:09:49.400
And then the aspect is an increasing amount of data is happening from outside the federal government.

00:09:49.400 --> 00:09:50.520
There's more.

00:09:50.520 --> 00:09:52.120
So how do you bring that data together?

00:09:52.120 --> 00:09:55.920
How do you really rethink the way data is being used?

00:09:56.600 --> 00:09:59.980
That's the component that the role is there.

00:09:59.980 --> 00:10:12.960
And that's the shift that we're seeing is that people who are coming into this, these, these new roles in the federal government have much more of that data science ethos, which say, well, there's lots of ways we can be clever to solve this problem.

00:10:12.960 --> 00:10:14.820
I may not actually have the data.

00:10:15.040 --> 00:10:16.540
I know where to go get the data.

00:10:16.540 --> 00:10:19.320
Oh, by the way, the data is incredibly messy and not in the right format.

00:10:19.320 --> 00:10:21.460
So I'm going to figure out how to get it together.

00:10:21.460 --> 00:10:23.100
We're going to try a hypothesis.

00:10:23.100 --> 00:10:23.780
We're going to test.

00:10:23.780 --> 00:10:24.360
We're going to iterate.

00:10:24.360 --> 00:10:26.000
We're going to try lots of different things.

00:10:26.000 --> 00:10:34.720
Most of what we do at the end of the day is literally giving them freedom of space to do their job because they have the ideas.

00:10:34.720 --> 00:10:35.440
They know what to do.

00:10:35.440 --> 00:10:36.640
We just got to give them a runway.

00:10:36.640 --> 00:10:37.320
Yeah.

00:10:37.320 --> 00:10:45.020
And I think you're in a unique position to recognize that kind of that technology professional mentality, right?

00:10:45.020 --> 00:10:46.480
Like you come out of Silicon Valley.

00:10:46.480 --> 00:10:49.000
Well, I know that you worked in government before.

00:10:49.000 --> 00:10:57.860
You've been in Silicon Valley and now you're back in government again, kind of trying to bridge that gap between these two communities that sometimes can be a little bit distant.

00:10:57.860 --> 00:11:01.940
Like even the idea that Silicon Valley needs to come in and save government.

00:11:01.940 --> 00:11:05.600
I know sometimes people bristle at that here in D.C.

00:11:05.600 --> 00:11:12.100
And then I know at the same time, people in the tech community kind of bristle at the way that government does things because they don't really understand it.

00:11:12.240 --> 00:11:18.180
So how do you see that relationship getting stronger?

00:11:18.180 --> 00:11:19.580
How are you bridging that gap?

00:11:19.580 --> 00:11:29.200
Because it really sounds like that's a big part of what you're describing here is data inside government, data outside government, kind of adopting that mentality and bringing that capacity to the work that government's already doing.

00:11:29.620 --> 00:11:35.340
But at the same time, not forgetting that the foundation of a lot of these technologies and data driven approaches came out of government in the first place.

00:11:35.340 --> 00:11:37.800
So how are you kind of making that marriage stronger?

00:11:37.800 --> 00:11:38.160
Yeah.

00:11:38.320 --> 00:11:47.080
So the easiest way to think about this is think through all the different podcasts that you've done where you said, wow, we need to work on this.

00:11:47.080 --> 00:11:50.820
Or you call it a story and you're like, geez, how did that happen?

00:11:50.820 --> 00:11:51.620
That's crazy.

00:11:51.620 --> 00:11:53.660
Like there's so many different problems.

00:11:53.660 --> 00:11:56.460
The biggest issue is we don't know how to help.

00:11:57.280 --> 00:11:58.740
Like we say, I know how to do that.

00:11:58.740 --> 00:12:00.840
We know how to raise our hand and say, I can help on that.

00:12:00.840 --> 00:12:02.860
But there's no door.

00:12:02.860 --> 00:12:04.640
There actually often is a door.

00:12:04.640 --> 00:12:09.860
It's just not well marked or it's hidden or obfuscated in layers of bureaucracy.

00:12:10.360 --> 00:12:14.320
So what we really tried to do is try to figure out how to show everyone the door.

00:12:14.320 --> 00:12:23.660
The other part that's there is really just helping people exchange, like create a common language.

00:12:23.660 --> 00:12:26.960
And one of the most powerful common languages is data.

00:12:26.960 --> 00:12:31.760
At the end of the day, you're able to talk about things, share things and kind of see the difference approaches.

00:12:32.180 --> 00:12:36.920
When we don't use data as a weapon against each other, we're actually using to have a conversation.

00:12:36.920 --> 00:12:41.460
It changes the tenor of the whole nature of how we're actually in discussion.

00:12:41.460 --> 00:12:42.460
We make it a discussion.

00:12:42.460 --> 00:12:52.620
So the final part there, I think that's most important is if we say that this is the mission, the Secretary of Defense has a great way of saying it.

00:12:52.620 --> 00:12:56.380
It's like there's nothing greater than waking up knowing you're part of something bigger.

00:12:56.380 --> 00:12:59.200
And when you're part of a mission, it's awesome.

00:12:59.200 --> 00:13:00.740
I mean, you've had a chance to experience it.

00:13:00.800 --> 00:13:03.160
Others have like who people have worked around these problems.

00:13:03.160 --> 00:13:08.780
And the part there that that is unbelievable is just.

00:13:08.780 --> 00:13:15.940
When you get a chance to do it and that's it's just giving people doors.

00:13:15.940 --> 00:13:16.840
Yeah.

00:13:16.840 --> 00:13:27.920
And actually, that's a good point to bring up, because I know this has been a big a big part of what you've been doing here is inviting more technology professionals kind of in data science professionals into the government.

00:13:27.920 --> 00:13:33.780
And I know that actually speaking of the Secretary of Defense, you were recently acknowledged for a pretty significant award.

00:13:33.780 --> 00:13:39.820
I think the highest award that a civilian can can receive from the from the DOD.

00:13:39.820 --> 00:13:40.300
Is that right?

00:13:40.300 --> 00:13:41.200
That's right.

00:13:41.200 --> 00:13:42.880
I know you probably want to talk about it too much.

00:13:44.480 --> 00:13:45.260
Where's your medal?

00:13:45.260 --> 00:13:47.140
I know.

00:13:47.140 --> 00:13:48.860
I can't.

00:13:48.860 --> 00:13:51.400
I can coin check you, but I can't.

00:13:51.400 --> 00:13:52.060
Exactly.

00:13:52.060 --> 00:13:52.820
I can't help check you.

00:13:52.820 --> 00:13:53.360
You can't.

00:13:53.360 --> 00:13:53.460
You can't.

00:13:53.460 --> 00:13:57.120
That's what I'm about.

00:13:57.120 --> 00:13:57.740
You'll probably lose at that.

00:13:57.740 --> 00:13:57.760
You'll probably lose at that.

00:13:57.760 --> 00:13:57.760
You'll probably lose at that.

00:13:57.760 --> 00:13:57.760
You'll probably lose at that.

00:13:57.760 --> 00:13:57.760
You'll probably lose at that.

00:13:57.760 --> 00:13:57.780
You'll probably lose at that.

00:13:57.780 --> 00:13:57.780
You'll probably lose at that.

00:13:57.780 --> 00:13:57.780
You'll probably lose at that.

00:13:57.780 --> 00:13:57.780
You'll probably lose at that.

00:13:57.780 --> 00:13:57.780
You'll probably lose at that.

00:13:57.780 --> 00:13:57.780
You'll probably lose at that.

00:13:57.780 --> 00:13:57.780
You'll probably lose at that.

00:13:57.780 --> 00:13:57.780
You'll probably lose at that.

00:13:57.780 --> 00:13:57.780
You'll probably lose at that.

00:13:57.780 --> 00:13:57.780
You'll probably lose at that.

00:13:57.780 --> 00:13:57.780
You'll probably lose at that.

00:13:57.780 --> 00:13:58.780
You'll probably lose at that.

00:13:58.780 --> 00:13:59.780
You'll probably lose at that.

00:13:59.780 --> 00:14:00.780
You'll probably lose at that.

00:14:00.780 --> 00:14:02.780
But it is true that I.

00:14:02.780 --> 00:14:12.100
It was an incredible honor to be able to receive an award, even a medal, which is kind of a weird, surreal thing.

00:14:12.100 --> 00:14:15.660
But the number one thing that I.

00:14:15.660 --> 00:14:19.000
You don't come to this job trying to win medals.

00:14:19.000 --> 00:14:23.060
What you're trying to do is find an avenue to add value.

00:14:23.060 --> 00:14:35.480
And the number one thing I tell the data scientists all the time is data scientists are you should be in a role where you are ridiculously overwhelmingly impactful.

00:14:35.480 --> 00:14:36.320
If you're not.

00:14:36.320 --> 00:14:42.620
Figure out how to be ridiculously impactful or go find another place where you can be.

00:14:43.500 --> 00:14:57.900
Because the ability to singularly be a force multiplier, like not like a 1x, a 2x, a 3x, like 10x, 100x force multiplier on any problem is never been more true.

00:14:57.900 --> 00:15:01.580
We got easy ability to have access to technology.

00:15:01.580 --> 00:15:03.520
Data ubiquity is there.

00:15:03.520 --> 00:15:18.240
And more than anything else, the ability to have a combination of some data with domain expertise allows a different type of integration that we just have not seen in recent times.

00:15:18.240 --> 00:15:19.080
Yeah.

00:15:19.080 --> 00:15:20.060
And so.

00:15:20.060 --> 00:15:32.500
But it sounds like that's already some of these programs just to give people examples for the as they're kind of thinking about their their new career in government after they hear this and and how excited and inspired they'll be to, you know, to come join the digital service.

00:15:32.500 --> 00:15:33.440
Like what's the.

00:15:33.440 --> 00:15:35.820
I mean, we mentioned precision medicine.

00:15:35.820 --> 00:15:36.740
Maybe let's focus on that.

00:15:36.740 --> 00:15:39.640
What what what did that look like from a data scientist perspective?

00:15:39.640 --> 00:15:42.900
So the highest level for precision medicine.

00:15:42.900 --> 00:15:55.140
So precision medicine first going to get over at least one million Americans to contribute their data and donate it to the National Institutes of Health so that researchers can work with it.

00:15:55.140 --> 00:15:56.440
Who are those researchers?

00:15:56.440 --> 00:16:00.520
So I'd love to see there just being generic data scientists who are training on it.

00:16:00.520 --> 00:16:02.480
Now, we have this kid, Nathan Hahn.

00:16:02.480 --> 00:16:05.280
He came to the White House Science Fair a couple of years back.

00:16:05.280 --> 00:16:12.040
And here's a kid literally 16, 17 years old, and he's just interested in machine learning algorithms.

00:16:12.040 --> 00:16:13.200
So he goes to D.B.

00:16:13.200 --> 00:16:17.620
Gap and he starts to play with some data and he's like, look, let me see what there is with cancer sites and all this stuff.

00:16:17.620 --> 00:16:20.620
His algorithms on machine learning are up there with the best.

00:16:20.620 --> 00:16:24.300
You know, we have stories of that all the time.

00:16:24.300 --> 00:16:26.800
We have this woman out of out of Kentucky.

00:16:26.800 --> 00:16:28.340
She's working on renal failure.

00:16:28.340 --> 00:16:29.800
She's building an artificial kidney.

00:16:30.060 --> 00:16:31.020
It's so good.

00:16:31.020 --> 00:16:31.060
It's so good.

00:16:31.060 --> 00:16:33.240
University of Kentucky gave her a whole lab.

00:16:33.240 --> 00:16:34.920
She's 17 this year.

00:16:35.440 --> 00:16:45.240
Like you have this amazing, like America, more than any place in the world, has an unbelievable arsenal of talent.

00:16:46.040 --> 00:16:46.980
It's everywhere.

00:16:46.980 --> 00:16:49.800
What we have to do is connect it with the problem.

00:16:50.160 --> 00:16:56.660
So if we're able to just open up a little bit of that data through the precision medicine initiative and say, hey, come in and work with it.

00:16:56.660 --> 00:16:57.740
What might we find?

00:16:57.740 --> 00:17:00.500
We talk about finding signatures of Vioxx.

00:17:00.500 --> 00:17:01.980
Why did somebody have to look?

00:17:01.980 --> 00:17:04.200
Why didn't that signal just emerge?

00:17:04.200 --> 00:17:08.960
What happens if you start to apply the machine learning algorithm, the feature set selection, things just interesting happen?

00:17:08.960 --> 00:17:12.980
As you get into that problem, you start to work with more and more sensitive data.

00:17:13.280 --> 00:17:20.280
You have to get vetted and you have to work on the data and maybe eventually the data is even air gapped or, you know, in some type of sandbox environment.

00:17:20.280 --> 00:17:24.900
We're working through all those security aspects as it develops.

00:17:24.900 --> 00:17:28.380
But I would love to see people do that and find a force multiplier change.

00:17:28.380 --> 00:17:34.220
Today, by the end of today, 100 people will die on our highways and roads.

00:17:34.940 --> 00:17:36.280
And that number is going up.

00:17:36.280 --> 00:17:37.300
Why?

00:17:37.300 --> 00:17:39.700
Better fuel efficiency?

00:17:39.700 --> 00:17:43.480
Is it cars are driving faster?

00:17:43.480 --> 00:17:45.240
Is it distracted driving?

00:17:45.240 --> 00:17:45.960
We don't know.

00:17:45.960 --> 00:17:48.260
But what we do know is that's a pristine data set.

00:17:48.260 --> 00:17:53.620
And what happens if we just say, hey, America, why don't you solve it?

00:17:53.740 --> 00:18:02.400
When you bring the full force of the United States of America to a problem, you will make that problem break.

00:18:02.400 --> 00:18:04.960
Like you will find a solution to break it.

00:18:04.960 --> 00:18:09.540
Like it is unbelievable how amazing we are as a country when we decide to do something.

00:18:09.540 --> 00:18:10.620
Yeah.

00:18:10.620 --> 00:18:12.940
And I think that you've mentioned two examples.

00:18:12.940 --> 00:18:17.140
I know you've talked about kind of criminal justice and open data and policing.

00:18:17.140 --> 00:18:28.620
It sounds like there's just so there's nothing but opportunities for people who are in the public and just want to engage even kind of prior to thinking about maybe coming and joining the digital service or being a data scientist inside government.

00:18:28.620 --> 00:18:37.760
How can people get involved in these various kind of open data initiatives that are part of tackling these really big society level problems?

00:18:37.760 --> 00:18:38.160
Yeah.

00:18:38.160 --> 00:18:40.680
So there's a whole lot of great ways to get involved.

00:18:40.680 --> 00:18:43.700
And a lot of times people think, oh, you've got to come to the White House to serve.

00:18:43.700 --> 00:18:46.340
No, you can serve in your local neighborhood.

00:18:46.520 --> 00:18:48.820
You know, there's there's a police department.

00:18:48.820 --> 00:18:50.440
There's an education department.

00:18:50.440 --> 00:18:58.220
There's somebody in the local city or your town or call your community, however you want to define it.

00:18:58.220 --> 00:18:58.900
County.

00:18:58.900 --> 00:19:01.520
All of them need help.

00:19:01.520 --> 00:19:06.620
And your ability to look and play with some data and get involved is really powerful.

00:19:06.620 --> 00:19:09.660
That could be through something like Code for America.

00:19:09.660 --> 00:19:16.960
It could also be through something around a cancer program or some other type of area where that just needs this skill sets.

00:19:16.960 --> 00:19:19.260
It could be education, looking at some of that data.

00:19:19.260 --> 00:19:23.840
You can there's all these competitions around what I say competitions.

00:19:23.960 --> 00:19:26.340
I just want to do something like that.

00:19:26.340 --> 00:19:28.120
Because nobody actually knows the answer.

00:19:28.120 --> 00:19:28.800
Get some ideas.

00:19:28.800 --> 00:19:29.940
Jump in there.

00:19:29.940 --> 00:19:33.600
If you want to get to the full federal level, there are the U.S.

00:19:33.600 --> 00:19:41.440
Digital Service and the different digital services in the Department of Defense or VA, Veteran Affairs, or even Department of Transportation.

00:19:42.040 --> 00:19:45.040
All those places are open and waiting for help.

00:19:45.040 --> 00:19:50.700
But the bigger thing is if you want to see a change happen to a community, you've got to jump in.

00:19:50.700 --> 00:19:51.760
Yeah.

00:19:51.760 --> 00:19:54.940
And so I think, OK, so people can make changes on a very micro level.

00:19:54.940 --> 00:19:56.700
People could, of course, get involved.

00:19:56.700 --> 00:20:00.700
The digital services will continue into the next administration.

00:20:01.280 --> 00:20:14.560
And then how do you think the – and kind of at a macro level, I know that your organization has been really out in front about data and ethics like we touched on.

00:20:14.560 --> 00:20:20.400
And I think that this is something that – it's a very challenging problem because on the one hand, it's a policy problem.

00:20:20.400 --> 00:20:22.120
So it does seem to come from government.

00:20:23.240 --> 00:20:31.560
It's really different to most technology innovation or, you know, maybe not if you have kind of a DARPA perspective, but different to most technology innovation.

00:20:31.560 --> 00:20:33.260
This came government first.

00:20:33.260 --> 00:20:37.600
This wasn't something that came out of industry that was a practice that was later adopted inside government.

00:20:37.600 --> 00:20:44.320
This was really the Office of Science and Technology Policy out in front, really the White House leading the charge on this.

00:20:44.320 --> 00:20:50.900
And I'm just – I'm curious, A, kind of how you see that permeating across data sciences and industry.

00:20:51.460 --> 00:20:54.960
And then, B, how you see that being carried forward into the future.

00:20:54.960 --> 00:21:01.360
Like what are the implications for this idea about ethics and data science and artificial intelligence and machine learning?

00:21:01.360 --> 00:21:03.140
Like what does that mean for us?

00:21:03.140 --> 00:21:03.500
Yeah.

00:21:03.500 --> 00:21:07.500
So the person who's led from the front of this is the president.

00:21:07.500 --> 00:21:10.160
This has been a forefront issue for the president.

00:21:10.160 --> 00:21:13.400
And it's – where does it really stem from?

00:21:13.400 --> 00:21:15.300
Is that force multiplier?

00:21:15.300 --> 00:21:18.300
Is the force multiplier good or is it harmful?

00:21:18.300 --> 00:21:19.940
Does it – what are the edge cases?

00:21:19.940 --> 00:21:27.680
And one of the things that I have taken away personally from this job is when you're building a company, as you guys are, you always get to say, well, that's an edge case.

00:21:27.680 --> 00:21:30.940
When you're here, those edge cases have names.

00:21:30.940 --> 00:21:36.200
They're names like Sally, Giselle, Juan, Ricardo, whatever.

00:21:36.200 --> 00:21:37.280
They all have names.

00:21:37.280 --> 00:21:39.740
And what's the impact for them?

00:21:40.440 --> 00:21:44.720
So when data is being used, how is it being used and what are the implications?

00:21:44.720 --> 00:21:51.060
So the very first big data report that John Podesta led, that emphasized the need for thinking along this direction.

00:21:51.060 --> 00:21:54.300
We've had since then three other data reports.

00:21:54.300 --> 00:21:56.140
And the latest one was an AI report.

00:21:56.140 --> 00:22:00.120
All of them, every single time we go out and talk to people, this is what's actually on their mind.

00:22:00.340 --> 00:22:07.140
It's not about the sentient being that's going to emerge and figure out how to become the robo-pocalypse.

00:22:07.140 --> 00:22:11.900
It's actually, well, who's getting harmed and what's there?

00:22:11.900 --> 00:22:15.420
And how do I know that I can trust this for my kids or my kids' kids?

00:22:16.380 --> 00:22:22.700
The place where this gets impactful, and you guys have talked a lot about this, is the black box of algorithms.

00:22:22.700 --> 00:22:25.600
The ability to know, is this data okay or not?

00:22:25.600 --> 00:22:33.760
There was a great aspect the other day where somebody pointed out, well, if we got self-driving cars and we got good data, bad data coming in,

00:22:33.760 --> 00:22:39.460
what are the implications when an algorithm can't recognize even African-American faces?

00:22:40.700 --> 00:22:46.960
Does that mean self-driving cars have a decision disparity when this comes to race?

00:22:46.960 --> 00:22:48.540
And is that a data science?

00:22:48.540 --> 00:22:49.940
Whose problem is that?

00:22:49.940 --> 00:22:55.520
Do we just be able to say, oh, sorry, that didn't have good training data?

00:22:55.520 --> 00:23:00.700
That's not acceptable when you're putting something out there that may harm the public.

00:23:00.700 --> 00:23:10.200
You wouldn't want a drug maker suddenly saying, oops, sorry, we didn't take into account the fact that hipster data scientists

00:23:10.200 --> 00:23:14.260
with beards living in Austin aren't in the training data set.

00:23:14.260 --> 00:23:17.020
Present company excluded.

00:23:17.020 --> 00:23:18.140
Sure, sure, sure.

00:23:18.140 --> 00:23:36.260
This portion of Talk Python to Me has been brought to you by Rollbar.

00:23:36.620 --> 00:23:47.980
One of the frustrating things about being a developer is dealing with errors, relying on users to report errors, digging through log files, trying to debug issues, or a million alerts just flooding your inbox and ruining your day.

00:23:47.980 --> 00:23:55.200
With Rollbar's full stack error monitoring, you'll get the context, insights, and control that you need to find and fix bugs faster.

00:23:55.740 --> 00:23:56.880
It's easy to install.

00:23:56.880 --> 00:24:01.160
You can start tracking production errors and deployments in eight minutes or even less.

00:24:01.160 --> 00:24:10.520
Rollbar works with all the major languages and frameworks, including the Python ones such as Django, Flask, Pyramid, as well as Ruby, JavaScript, Node, iOS, and Android.

00:24:10.880 --> 00:24:19.720
You can integrate Rollbar into your existing workflow, send error alerts to Slack or HipChat, or even automatically create issues in Jira, Pivotal Tracker, and a whole bunch more.

00:24:19.720 --> 00:24:23.100
Rollbar has put together a special offer for Talk Python to Me listeners.

00:24:23.100 --> 00:24:28.780
Visit rollbar.com slash Talk Python to Me, sign up, and get the bootstrap plan free for 90 days.

00:24:29.220 --> 00:24:31.820
That's 300,000 errors tracked all for free.

00:24:31.820 --> 00:24:35.540
But hey, just between you and me, I really hope you don't encounter that many errors.

00:24:35.540 --> 00:24:41.560
Loved by developers at awesome companies like Heroku, Twilio, Kayak, Instacart, Zendesk, Twitch, and more.

00:24:41.560 --> 00:24:43.160
Give Rollbar a try today.

00:24:43.160 --> 00:24:45.760
Go to rollbar.com slash Talk Python to Me.

00:24:45.760 --> 00:24:57.360
Zach Kohani did this great research project, sending cardiac death syndrome.

00:24:58.260 --> 00:25:06.340
Turns out African-American males have been being given too high a false positive reading on this thing.

00:25:06.340 --> 00:25:07.540
Why is that the case?

00:25:07.540 --> 00:25:12.980
Turns out genetics and genomics are substantially more complicated, we thought.

00:25:12.980 --> 00:25:17.460
But also there's not enough healthy African-American males in those clinical trials.

00:25:17.460 --> 00:25:28.120
But if you look even more, there's not even really women in a lot of clinical trials, let alone talk about ethnicity of women in trials.

00:25:28.120 --> 00:25:33.020
So as we're working with data, you kind of go back to the source and say, well, what bias is there?

00:25:33.020 --> 00:25:34.720
And all of these other things.

00:25:34.720 --> 00:25:47.040
And then the other one that I think we have to confront and everyone has to start really asking these questions is what does it mean when somebody just slaps a label on something that says data science verified?

00:25:47.740 --> 00:25:50.180
Just because it's great for sales and marketing.

00:25:50.180 --> 00:25:57.180
And as data scientists, all of us need to take a very serious look at that and say, does that meet our bar?

00:25:57.640 --> 00:26:02.720
Because those people, when they sell something, they're representing it on behalf of the community.

00:26:02.720 --> 00:26:05.480
And as a community, I'd like to think that we're better than that.

00:26:06.480 --> 00:26:06.580
Yeah.

00:26:06.580 --> 00:26:27.740
So I mean, I think there's this idea that, of course, it really speaks to the need for data scientists and almost like in policymaking positions or near policymakers to be able to inform at a high level when we should be or what steps should we be taking to verify that the data that we're using to make these decisions is not biased or that our processes aren't biased.

00:26:27.860 --> 00:26:31.220
But then at the same time, you're talking about this individual responsibility.

00:26:31.220 --> 00:26:34.380
And where do you feel like that's going to come from?

00:26:34.380 --> 00:26:45.440
I mean, is it situations like this where those of us in the community who have sort of seen this in action can be advocating for it or those who are listening to this podcast can accept it and do some thinking?

00:26:45.440 --> 00:26:55.200
But maybe beyond that, as a kind of a practice or as an industry, how do we – is that something we can institute?

00:26:55.200 --> 00:26:56.220
Like, what do you think?

00:26:56.220 --> 00:26:59.400
Are there changing policies, changing education, certifying?

00:26:59.400 --> 00:27:00.280
I mean, what do you think?

00:27:00.280 --> 00:27:02.700
Well, if we're not careful, we're going to get regulation.

00:27:02.700 --> 00:27:06.380
And the regulation will come in usually in the form of legislation.

00:27:06.380 --> 00:27:09.080
And that has good and bad effects.

00:27:09.080 --> 00:27:17.020
It's very tough to get that kind of legislation correct when it's a very fast-changing technical landscape.

00:27:17.020 --> 00:27:18.720
Extremely hard to do.

00:27:19.080 --> 00:27:31.640
So what I would like to see, and I think the place where we first are and that we have called for from the White House is that every training program, data science, economics, computer science, whatever, you've got to be trained in two things.

00:27:31.640 --> 00:27:33.920
As not electives, but as two core principles.

00:27:33.920 --> 00:27:36.000
What does ethics look like?

00:27:36.000 --> 00:27:37.320
And what does security look like?

00:27:37.820 --> 00:27:43.060
Because if you're learning about databases and you don't know what overflow is, that's crazy in this day and age.

00:27:43.060 --> 00:27:54.620
In the same way, if you don't know about training bias and with the ethical implications, as you're building something, that can't be a slap-on additive elective kind of thing.

00:27:54.620 --> 00:27:56.000
It's got to be intrinsic to every course.

00:27:56.000 --> 00:28:02.780
So if you don't have that in the training program you're in right now, you need to demand it because you're getting a subpar training course.

00:28:02.780 --> 00:28:04.840
And it's not going to prepare for the real world.

00:28:04.840 --> 00:28:10.440
The other aspect is as we're interviewing people, as we're talking to people, we should all ask an ethics question.

00:28:10.440 --> 00:28:17.080
Methods question could be just very simply, we're going to pretend we're doing an interview here and say, Jonathan, thanks for coming in.

00:28:17.080 --> 00:28:22.480
You happen to be building an algorithm and we're really focused on this because we're building job matching.

00:28:22.480 --> 00:28:24.920
And we're not supposed to use race.

00:28:25.440 --> 00:28:39.320
You got this amazing data set and because you're the all-star data scientist and you have a podcast, you just happen to look at all the data and say, hey, I think I found ability to bypass race or create a proxy of race.

00:28:39.320 --> 00:28:40.640
What do you do?

00:28:40.640 --> 00:28:43.160
Yeah, these are interesting questions, right?

00:28:43.160 --> 00:28:49.880
Because I think it's the kind of thing that as you could imagine, I think sometimes as technical people, we get really focused on the solution to the problem.

00:28:49.880 --> 00:28:51.960
And we put that ahead of everything else.

00:28:51.960 --> 00:28:54.640
And we're not really thinking about the implications of our work.

00:28:54.880 --> 00:28:56.720
And we just go, hey, I found the solution.

00:28:56.720 --> 00:28:58.200
I solved the Rubik's Cube.

00:28:58.200 --> 00:28:58.940
I'm done.

00:28:58.940 --> 00:28:59.560
That's right.

00:28:59.560 --> 00:29:01.380
But I think we don't say this all.

00:29:01.380 --> 00:29:02.300
We don't have a conversation.

00:29:02.300 --> 00:29:02.880
Right.

00:29:03.020 --> 00:29:09.600
Like all the people that are in the cadre of data scientists who have recognized this problem.

00:29:09.600 --> 00:29:18.740
What's the one commonality that we all have other than generally going to bars and having beer talking about these problems is we talk about the problems.

00:29:18.740 --> 00:29:20.340
We talk about these.

00:29:20.340 --> 00:29:22.760
We don't just talk about, oh, how did you do that?

00:29:23.300 --> 00:29:27.480
We always kind of look back and we're like, well, is that what we should do?

00:29:27.480 --> 00:29:28.760
What about this?

00:29:28.760 --> 00:29:30.720
What about these other implications?

00:29:30.720 --> 00:29:32.220
We care about the edge cases.

00:29:32.220 --> 00:29:36.120
We care about the longer term implications of what we're building.

00:29:36.580 --> 00:29:45.840
When you create something with data or an algorithm, that's equally important as if you were creating an artwork, you're creating a bridge or building a building.

00:29:45.840 --> 00:29:47.620
We have to take that responsibility.

00:29:48.500 --> 00:29:51.200
And it's not that because the responsibility will be imposed on us.

00:29:51.200 --> 00:29:59.460
It's because if we are going to be a massive force multiplier in the world, you have to accept the responsibility that comes with that.

00:29:59.460 --> 00:30:00.280
Yeah.

00:30:00.280 --> 00:30:08.320
Well, because we're kind of we're at the we're at the stage where it's a little bit the Wild West right now in terms of how data and data science works.

00:30:08.320 --> 00:30:17.260
And we're, I think, still in that window before there's any there aren't really sort of set mechanisms for how people collaborate in teams.

00:30:17.420 --> 00:30:25.120
There aren't set mechanisms for how data science models are deployed or or checked or QA and kind of all the stuff you have in other technical disciplines.

00:30:25.120 --> 00:30:26.480
It doesn't exist here yet.

00:30:26.480 --> 00:30:38.200
And so I think ultimately we get to decide this generation of data scientists get to decide how we would like history to look back or how we would like our industry to evolve going forward.

00:30:38.200 --> 00:30:41.840
And even more so, I think history will judge us.

00:30:42.280 --> 00:30:57.740
It will either judge us kindly or harshly depending on what comes out of these products and the implications of how not only our society uses these products, but other societies that could even be repressive use our products.

00:30:57.740 --> 00:30:59.500
And what are the implications of that?

00:30:59.980 --> 00:31:03.520
All of that falls on us for the implications of these things.

00:31:03.520 --> 00:31:10.620
And we just have to get ready to drive that world because the the time is now for us to do that.

00:31:10.620 --> 00:31:19.440
But let's actually, you know, one of the things I think given your audience, I'd actually be really curious to hear what the audience has to say on this.

00:31:19.440 --> 00:31:27.720
I think one of the things that we haven't heard sufficiently because it's just the data science community is early is what is the data science community think?

00:31:27.720 --> 00:31:32.400
What would be the most powerful mechanisms to move the needle on this?

00:31:32.400 --> 00:31:37.100
Should we have more of the institutional review boards and very fixated regimented process?

00:31:37.100 --> 00:31:38.760
Should we have a very laissez faire?

00:31:38.760 --> 00:31:40.320
Where do we stand?

00:31:40.320 --> 00:31:45.920
That's you know, that's one that I think the community has an incredible opportunity to stand up and say this is where we believe.

00:31:45.920 --> 00:31:58.660
And I have no idea what the the broad consensus of data scientists are thinking, except for the ones that we've had a lot of interactions with our our very traditional White House processes.

00:31:58.660 --> 00:32:00.640
And that makes sense.

00:32:00.640 --> 00:32:05.900
And I think that there I would wonder how much people are even thinking about it.

00:32:05.900 --> 00:32:12.300
It does seem like they're good because in other industries, there are there are roadmaps in the legal profession, the medical profession.

00:32:12.300 --> 00:32:24.120
Like these are also kind of knowledge based, highly technical professions that have established some concept of ethics and mechanisms for either encouraging or forcing people to adhere to whatever the community standards are.

00:32:24.120 --> 00:32:27.140
But but as of yet, we don't know.

00:32:27.140 --> 00:32:30.120
Is that the appropriate way to think about it for data science?

00:32:30.120 --> 00:32:31.640
I don't know.

00:32:31.640 --> 00:32:46.060
Well, the ethics, if you look at bioethics, biomedical ethics or physicians, any of the physicians who are at the very early front end of that space in terms of ethics getting implemented across that they're still practicing.

00:32:46.060 --> 00:32:48.700
That's how recent these things are.

00:32:49.140 --> 00:32:52.300
So for us to get ahead of that, that's going to be critical.

00:32:52.300 --> 00:33:00.420
And we're going to have an equal opportunity to drive more of medicine in this way through data driven approaches than people even recognize.

00:33:00.420 --> 00:33:04.260
So the analogies could be even more similar than we appreciate.

00:33:04.260 --> 00:33:05.100
And same with law.

00:33:06.100 --> 00:33:16.460
So speaking of the kind of implications for the broader community, now that, you know, now that this administration is coming to a close and obviously you're very passionate about where the community should be going.

00:33:16.460 --> 00:33:19.800
I feel like I have to ask, what are you doing after this, DJ?

00:33:19.800 --> 00:33:20.760
How are you going to be?

00:33:20.760 --> 00:33:24.300
How's your leadership role in the data science community going to transition?

00:33:24.300 --> 00:33:24.640
Right.

00:33:24.640 --> 00:33:27.680
Well, as much as you can talk about it, as much as I can talk about.

00:33:27.820 --> 00:33:31.860
So I feel like I should insert somewhere a Chris Albin is my cousin joke.

00:33:31.860 --> 00:33:37.200
I'll visit my own long lost relatives out on the border.

00:33:37.200 --> 00:33:38.200
He'll appreciate that.

00:33:38.200 --> 00:33:39.580
Having not been able to be here in person.

00:33:39.580 --> 00:33:40.280
That's right.

00:33:40.280 --> 00:33:42.920
It'll at least give him a laugh.

00:33:42.920 --> 00:33:46.060
Chris, we miss you.

00:33:46.060 --> 00:33:47.960
Most of the time.

00:33:47.960 --> 00:33:48.780
Most of the time.

00:33:48.780 --> 00:33:49.460
Most of the time.

00:33:49.460 --> 00:33:49.720
You know.

00:33:49.720 --> 00:33:54.480
But for me, the biggest thing will be to take a nap.

00:33:56.360 --> 00:33:57.760
I'll be taking off the tie.

00:33:57.760 --> 00:34:06.000
And the thing that I'm excited is that the biggest thing is the wave of data scientists that are in the federal structure.

00:34:06.000 --> 00:34:08.440
A lot of people are questioning what's going to happen.

00:34:08.440 --> 00:34:09.220
Is this going to collapse?

00:34:09.220 --> 00:34:14.960
The federal, the civil servants who are going to just carry this forward.

00:34:14.960 --> 00:34:25.060
There are chief data scientists or chief data officers or some type of analytics leader, data leader in more than 24 of the federal agencies.

00:34:25.260 --> 00:34:26.980
And they're going to carry on the mission.

00:34:26.980 --> 00:34:30.040
So I feel very good about the progress we've made.

00:34:30.040 --> 00:34:36.740
That doesn't mean like at all to say the mission is done and the community is going to have to continue to kind of continue to champion this.

00:34:36.740 --> 00:34:43.480
For me personally, I'm a big believer that there's a big difference between experience and wisdom.

00:34:43.900 --> 00:34:47.560
And you go from experience to wisdom through reflection.

00:34:47.560 --> 00:34:50.020
Reflection is sitting down thinking.

00:34:50.020 --> 00:34:56.440
It's talking to people, having these kind of conversations, writing, all those different things.

00:34:56.440 --> 00:35:04.380
And so I'll go through some period of reflection to try to distill as much as I can from this really very, very unique experience.

00:35:04.820 --> 00:35:08.260
And then I think I'm really going to be excited to get back to building.

00:35:08.260 --> 00:35:12.940
I think there's in the powerful thing about all data scientists is we're makers at heart.

00:35:13.800 --> 00:35:28.020
And if we've taken anything that's theirs, your ability to be creative and create something novel and do something that's unique that nobody saw, like your work on ISIS, you know, using Twitter data.

00:35:28.520 --> 00:35:34.640
Those type of things you just see, like people seeing the world through a lens that people hadn't seen before.

00:35:34.640 --> 00:35:38.220
It's kind of like when you see a photograph and you're like, wow, I never saw the world that way.

00:35:38.220 --> 00:35:40.640
We have that unique ability to do that with data.

00:35:40.640 --> 00:35:45.440
How does that resonate as a true product or something that somebody else can do?

00:35:45.440 --> 00:35:48.900
When we're building, we're learning in different ways.

00:35:48.900 --> 00:35:52.420
And I'm really excited to get back to a different form of building.

00:35:52.420 --> 00:35:54.360
This one has been largely policy-based.

00:35:55.000 --> 00:36:02.760
And I'll be looking forward to getting my hands dirty and hands back on keys and trying to figure out what that looks like in some form or another.

00:36:02.760 --> 00:36:03.620
All right.

00:36:03.620 --> 00:36:04.000
All right.

00:36:04.000 --> 00:36:04.300
All right.

00:36:04.300 --> 00:36:16.780
Well, we look forward to some writing, perhaps, to some reflection, and then ultimately to spitballing on how to build the right model for whatever it is, whatever problem you'd like to build to try and solve.

00:36:16.780 --> 00:36:18.540
So that's cool.

00:36:18.540 --> 00:36:21.480
That sounds like it'll be – and a well-deserved rest after –

00:36:21.480 --> 00:36:28.540
I mean, I know around here at the White House, I was joking just last night that it's not uncommon to find a fair number of workaholics around here.

00:36:28.540 --> 00:36:31.860
You guys all push pretty hard because everything is important at this level.

00:36:31.860 --> 00:36:32.940
There's always a crisis.

00:36:32.940 --> 00:36:35.520
And there's always an opportunity to do more.

00:36:35.520 --> 00:36:36.080
Yeah.

00:36:36.080 --> 00:36:40.420
So when you're balancing those, you can't let the urgent get in the way of the important.

00:36:40.420 --> 00:36:43.020
And there's this fundamental thing.

00:36:43.020 --> 00:36:44.680
It's actually – there's a great one.

00:36:44.760 --> 00:36:47.020
So I'll just share this kind of card with you.

00:36:47.020 --> 00:36:50.020
There's this – the president gives us these cards.

00:36:50.020 --> 00:36:54.720
And this one says, everything we do needs to be infused with a sense of possibility.

00:36:54.720 --> 00:36:56.040
We are not scared of the future.

00:36:56.040 --> 00:36:59.440
Everything we do needs to be infused with a sense of possibility.

00:36:59.440 --> 00:37:00.700
We are not scared of the future.

00:37:01.480 --> 00:37:07.280
So there's also the other analogy, Quarto, that he gives us, which is remarkable things happened in the last quarter.

00:37:07.280 --> 00:37:16.100
And for all the sports people, you don't need to – you can imagine your favorite game where you've seen that.

00:37:16.100 --> 00:37:18.340
So don't think that we're done.

00:37:19.440 --> 00:37:19.620
Yeah.

00:37:19.620 --> 00:37:19.960
Okay.

00:37:19.960 --> 00:37:23.940
So there's more to come here in the last quarter.

00:37:23.940 --> 00:37:27.020
But then also even going further past that.

00:37:27.020 --> 00:37:35.000
I mean, you mentioned something just a moment ago about the way that kind of data and analytics has been really embedded in government.

00:37:35.000 --> 00:37:36.280
And so the mission continues.

00:37:36.280 --> 00:37:42.200
And there's – I'm not – you know, again, I know that there's kind of a lot going on right now.

00:37:42.200 --> 00:37:44.640
And so there's maybe not a ton to say.

00:37:44.820 --> 00:37:48.880
But I bet a lot of people listening are thinking to themselves, well, what happens in the next administration?

00:37:48.880 --> 00:37:51.940
It's an administration from the other party.

00:37:51.940 --> 00:37:52.900
So it's a big transition.

00:37:52.900 --> 00:37:59.920
But then at the same time, I know, you know, from a technology perspective, that this is a largely bipartisan issue.

00:37:59.920 --> 00:38:04.360
I think that you actually served in George W. Bush's administration, if that's right.

00:38:04.360 --> 00:38:08.900
So, you know, you've seen this from the perspective of a civil servant.

00:38:08.900 --> 00:38:10.200
I don't know.

00:38:10.200 --> 00:38:14.260
What happens in the change as we move from one administration to the next?

00:38:14.260 --> 00:38:17.600
And what are the implications for people who might be thinking about getting involved?

00:38:17.600 --> 00:38:22.420
Well, the biggest thing to think of as an administration is it's a baton.

00:38:22.420 --> 00:38:23.740
It's like a baton race.

00:38:23.740 --> 00:38:30.440
And so your job is to hand the baton off to the next team while they are sprinting equally as fast as you are.

00:38:30.440 --> 00:38:36.740
No baton drops are acceptable because that's national security.

00:38:36.740 --> 00:38:38.100
That's people getting hurt.

00:38:38.100 --> 00:38:42.940
That's people – you know, there's a lot of services that people critically depend on.

00:38:42.940 --> 00:38:45.200
So we have to make sure the baton is well passed.

00:38:45.440 --> 00:38:57.460
So – and that is what the president has really emphasized is that just as the Bush administration transitioned to the Obama presidency, that was such a clean handoff.

00:38:57.460 --> 00:38:58.800
We have to do that as well.

00:38:58.800 --> 00:39:01.000
And we're all team America.

00:39:01.000 --> 00:39:02.500
We're team USA here.

00:39:02.920 --> 00:39:04.420
So we have to do that.

00:39:04.420 --> 00:39:07.420
And you never want to bet against the country.

00:39:07.420 --> 00:39:09.060
That's not who we are.

00:39:09.060 --> 00:39:20.740
The other part there that's the case is these problems – cancer doesn't care what religion you are, what political party you are, what socioeconomic class you are.

00:39:20.740 --> 00:39:22.700
It is a problem of species.

00:39:23.500 --> 00:39:27.220
Zika, Ebola, these are problems of a species.

00:39:27.220 --> 00:39:28.720
Climate change is as well.

00:39:28.720 --> 00:39:30.080
Obviously debated.

00:39:30.080 --> 00:39:32.140
The science is clear.

00:39:32.140 --> 00:39:40.180
And I'm sure as more and more people actually take a look at it as they start to shift to thinking about these things, more people will say that is obvious.

00:39:41.180 --> 00:39:49.560
When the people start looking at these other sets of problems around criminal justice at the local level, these are not federal problems inherently.

00:39:49.560 --> 00:39:50.260
They're local.

00:39:50.260 --> 00:40:02.720
And if you look at Governor Bevin, who is the governor of Kentucky, he knows that there's a giant gaping budget gap that is being caused by the local criminal justice system.

00:40:02.720 --> 00:40:08.920
And for those that are out there that don't know, we're talking like $20 billion out of the U.S.

00:40:08.920 --> 00:40:12.760
That's like basically how much we're paying for jails.

00:40:12.760 --> 00:40:14.320
And who are we paying for?

00:40:14.320 --> 00:40:24.220
There are more like – there's more than 10 million people, like basically 11 million people going through our 1,300 jails.

00:40:24.220 --> 00:40:26.800
And it's crazy.

00:40:26.800 --> 00:40:28.460
Like you think about those numbers.

00:40:28.460 --> 00:40:30.840
95% of them never will go to prison.

00:40:30.840 --> 00:40:31.940
These are local jails.

00:40:32.320 --> 00:40:34.560
And they're staying there for average 23 days.

00:40:34.560 --> 00:40:40.060
So 11.3 million people going through 3,100 jails.

00:40:40.060 --> 00:40:41.520
I said my number slightly differently there.

00:40:41.520 --> 00:40:45.460
But 11.3 million through 3,100 jails.

00:40:45.460 --> 00:40:47.980
That's insanity.

00:40:47.980 --> 00:40:50.480
And who are those people?

00:40:50.480 --> 00:40:52.880
You look at some of our jail facilities.

00:40:52.880 --> 00:40:58.480
Cook County Jail, 92 acres of a single-site jail in Illinois.

00:40:58.480 --> 00:41:00.740
That has one-third mentally ill.

00:41:01.540 --> 00:41:05.520
We were just in Las Vegas the other day, and they were talking about assaults on officers.

00:41:05.520 --> 00:41:09.460
And when they thought about it, they all thought, oh, it's gang-related violence.

00:41:09.460 --> 00:41:11.820
When they actually looked at the data, it's all mental illness.

00:41:12.460 --> 00:41:15.320
So why are we sending mentally ill to jail?

00:41:15.320 --> 00:41:17.260
Why not train officers?

00:41:17.260 --> 00:41:18.480
Miami-Dade, Florida did this.

00:41:18.480 --> 00:41:20.520
They trained officers in intervention.

00:41:20.520 --> 00:41:21.820
And what happens?

00:41:21.820 --> 00:41:23.040
Oh, gee.

00:41:23.040 --> 00:41:29.820
It turns out that if you spend a million bucks to train officers and dispatch and crisis intervention,

00:41:29.820 --> 00:41:33.520
you can save more than $10 million in the jails.

00:41:34.520 --> 00:41:38.120
And you can close the jail, which is the more important measure.

00:41:38.120 --> 00:41:41.280
Now, think about that with respect to the opioid crisis.

00:41:42.440 --> 00:41:47.180
So these ideas of what it means to use data in these clever ways.

00:41:47.180 --> 00:41:47.960
And how do you do that?

00:41:48.420 --> 00:41:53.760
Take your data from your portion of your criminal justice system.

00:41:53.760 --> 00:41:55.540
Move it over to the health care system.

00:41:55.540 --> 00:41:57.200
Not super sophisticated.

00:41:57.200 --> 00:42:00.540
Just move it over and look at who are the people that are constantly cycling.

00:42:00.540 --> 00:42:03.120
How many jail days are they going through?

00:42:03.120 --> 00:42:04.240
How many dollars are they doing?

00:42:04.240 --> 00:42:08.160
And then ask, next time the police see them, why don't they take them instead of jail,

00:42:08.160 --> 00:42:10.300
put them into this other treatment plan?

00:42:10.300 --> 00:42:11.600
Let's take them directly to treatment.

00:42:12.400 --> 00:42:13.400
Those are the ideas.

00:42:13.400 --> 00:42:33.080
This portion of Talk Python to Me is brought to you by GoCD from ThoughtWorks.

00:42:33.080 --> 00:42:37.740
GoCD is the on-premise, open-source, continuous delivery server.

00:42:38.220 --> 00:42:44.540
With GoCD's comprehensive pipelining model, you can model complex workflows for multiple teams with ease.

00:42:44.540 --> 00:42:50.360
And GoCD's value stream map lets you track changes from commit to deployment at a glance.

00:42:50.360 --> 00:42:55.360
GoCD's real power is in the visibility it provides over your end-to-end workflow.

00:42:55.360 --> 00:43:00.320
You get complete control of and visibility into your deployments across multiple teams.

00:43:00.320 --> 00:43:04.840
Say goodbye to release day panic and hello to consistent, predictable deliveries.

00:43:05.480 --> 00:43:09.720
Commercial support and enterprise add-ons, including disaster recovery, are available.

00:43:09.720 --> 00:43:15.680
To learn more about GoCD, visit talkpython.fm/gocd for a free download.

00:43:15.680 --> 00:43:18.980
That's talkpython.fm/gocd.

00:43:18.980 --> 00:43:20.060
Check them out.

00:43:20.060 --> 00:43:21.100
It helps support the show.

00:43:28.680 --> 00:43:30.360
Miami-Dade did that one year alone.

00:43:30.360 --> 00:43:31.900
They're making those crazy savings costs.

00:43:31.900 --> 00:43:33.460
How do you make that happen at scale?

00:43:33.460 --> 00:43:35.660
That's where the data science comes in.

00:43:35.660 --> 00:43:37.440
Because Miami can do it.

00:43:37.440 --> 00:43:39.880
How about Florida and other portions of Florida?

00:43:39.880 --> 00:43:41.580
How about, you know, Louisville?

00:43:41.580 --> 00:43:43.560
How about, you know, Boston?

00:43:43.560 --> 00:43:46.320
How about somewhere in, you know, Seattle?

00:43:46.320 --> 00:43:50.520
When that common platform is there, that's where we're going to see that change.

00:43:51.320 --> 00:43:58.940
So it sounds like at an individual level that there's opportunities to find important applications

00:43:58.940 --> 00:44:01.000
for data like this that really start bottom up.

00:44:01.000 --> 00:44:05.180
Like they really do start people getting engaged and being active in their communities and working

00:44:05.180 --> 00:44:06.880
on data that's local to their communities.

00:44:07.080 --> 00:44:13.800
And then when interesting solutions are discovered, then that's where at a federal level, you can

00:44:13.800 --> 00:44:18.680
be thinking, oh, how can we now be another force multiplier to take that kind of solution and

00:44:18.680 --> 00:44:23.100
see where else it might be applied or bring people together or orchestrate policy so that

00:44:23.100 --> 00:44:25.480
it enables this at some kind of national scale?

00:44:25.480 --> 00:44:27.260
Well, we think of it as scout and scale.

00:44:27.260 --> 00:44:27.780
Okay.

00:44:27.780 --> 00:44:33.080
So if somebody's doing it great over here, we say, hey, everybody else, look at this.

00:44:33.080 --> 00:44:38.120
And the White House gives you an incredible bully pulpit to say, hey, here's how we can

00:44:38.120 --> 00:44:38.680
scale that.

00:44:38.680 --> 00:44:44.800
But data-driven justice and police data initiative, these two kind of programs in this space, they're

00:44:44.800 --> 00:44:49.320
what they don't really have a White House, like no data is coming to the federal government.

00:44:49.320 --> 00:44:53.740
They're all local and they're trying to say, hey, here's what works for us.

00:44:53.740 --> 00:44:55.140
Here's what works for us.

00:44:55.140 --> 00:44:56.840
You know, we talk about A-B experiments.

00:44:56.840 --> 00:45:01.780
We forget in healthcare, each one of us is an A-B experiment of life.

00:45:02.040 --> 00:45:06.060
The question is of what was the original questions?

00:45:06.060 --> 00:45:07.000
What were those hypotheses?

00:45:07.000 --> 00:45:08.360
Was it local environment?

00:45:08.360 --> 00:45:09.640
Was it genetics?

00:45:09.640 --> 00:45:10.600
All these other things.

00:45:10.600 --> 00:45:16.140
Same way, each city, each township, each community is an A-B experiment across the country.

00:45:16.140 --> 00:45:20.220
But when we use big data techniques, we're able to abstract and say, hey, here are the

00:45:20.220 --> 00:45:23.360
common features that we believe lead to this.

00:45:23.360 --> 00:45:26.780
That creates a hypothesis that then we can test with policy.

00:45:28.040 --> 00:45:28.360
Okay.

00:45:28.360 --> 00:45:28.480
Okay.

00:45:28.480 --> 00:45:28.820
Okay.

00:45:28.820 --> 00:45:34.340
So, and then, so you can, you can basically test this hypothesis in multiple places and

00:45:34.340 --> 00:45:38.680
then, and then see whether or not the features that we assume actually lead to a reduction

00:45:38.680 --> 00:45:44.340
in the cost of, say, local incarceration are actually the features that are common from

00:45:44.340 --> 00:45:49.080
city to city with, you know, when we take away everything else that might be, might be

00:45:49.080 --> 00:45:49.960
relevant to the problem.

00:45:49.960 --> 00:45:52.280
Like, like location, for example, geographical location.

00:45:52.420 --> 00:45:53.480
That's absolutely correct.

00:45:53.480 --> 00:45:57.660
And one of the things that you start to see as you start asking these questions is you

00:45:57.660 --> 00:46:03.200
realize that other people just haven't had time to ask a question or technical expertise

00:46:03.200 --> 00:46:07.780
at helping them to ask the question or legacy systems that prevent them from asking the question.

00:46:07.780 --> 00:46:13.620
So in the case of an saying, hey, your officer assaults when your officer has been assaulted,

00:46:13.620 --> 00:46:16.440
who's, why is that happening?

00:46:16.440 --> 00:46:20.580
They say, huh, you know, we had a thing, but we've actually never checked.

00:46:20.800 --> 00:46:24.560
Now, is it the police officer's fault or the police department's fault that they haven't

00:46:24.560 --> 00:46:25.320
had time to do that?

00:46:25.320 --> 00:46:29.500
These officers are so massively overloaded.

00:46:29.500 --> 00:46:32.720
It's unbelievable because we're asking them to do more and more and more.

00:46:32.720 --> 00:46:37.220
And just to kind of go to another example here, because this is one that I think is important.

00:46:37.220 --> 00:46:42.240
It's like, we talk about police officers and we forget what is data doing for the officer.

00:46:43.080 --> 00:46:52.640
And the team at University of Chicago did a really cool set of research projects with a town down in the south.

00:46:52.640 --> 00:46:58.740
And what they did was they looked at the data and they sort of said, what is causing officers to use excessive force?

00:46:58.740 --> 00:46:59.520
What are those features?

00:46:59.520 --> 00:47:06.120
And so right away, it's a signal and noise problem because there's a very small number of officers that are actually using excessive force.

00:47:06.620 --> 00:47:10.120
So then you've got to kind of separate that out and you start looking at that data.

00:47:10.120 --> 00:47:12.380
And the first set of features is super obvious.

00:47:12.380 --> 00:47:16.460
You have a history of traffic accidents and, you know, the usual kind of things.

00:47:16.460 --> 00:47:19.560
Then suddenly a couple of features emerge in the middle.

00:47:19.560 --> 00:47:24.960
Number one, oh, look, you responded to multiple suicide calls.

00:47:24.960 --> 00:47:28.720
Oh, you responded to domestic violence where children were present.

00:47:29.400 --> 00:47:32.000
So what's a good data scientist doing this situation?

00:47:32.000 --> 00:47:33.840
They don't just try to extrapolate.

00:47:33.840 --> 00:47:38.060
They go talk to the officers and they go follow along with the officers.

00:47:38.060 --> 00:47:39.420
So what happens in a suicide?

00:47:39.420 --> 00:47:41.740
Suicides are physically messy.

00:47:41.740 --> 00:47:45.000
It's a lot of it's just a gory situation.

00:47:45.000 --> 00:47:46.320
It's very uncomfortable.

00:47:46.320 --> 00:47:52.900
But also there are human emotions that are highly supercharged in their families.

00:47:52.900 --> 00:47:55.120
Everything is it's just a high emotional thing.

00:47:55.120 --> 00:47:58.360
Same thing with domestic violence, especially when a child is present.

00:47:58.860 --> 00:48:00.060
So what's happening?

00:48:00.060 --> 00:48:04.180
Dispatch says at the end of that says, you're done.

00:48:04.180 --> 00:48:05.620
We'll get back on the beat.

00:48:05.620 --> 00:48:10.420
So now you pull some kid over with a broken taillight and they're flipping with you.

00:48:10.420 --> 00:48:13.160
You just came off this highly emotionally charged thing.

00:48:13.160 --> 00:48:16.860
None of us are good enough to go from that context shift.

00:48:16.860 --> 00:48:23.560
So why is the dispatch system not thinking about this ahead or anticipating this?

00:48:23.560 --> 00:48:24.780
Because the data is obvious.

00:48:24.780 --> 00:48:27.200
Give the officer some time to decompress.

00:48:27.780 --> 00:48:31.000
Give them some chance to become back to being normal and human.

00:48:31.000 --> 00:48:39.600
That's a failure of data rather than an opportunity of where data is being used in the way it could be to help the officers.

00:48:40.600 --> 00:48:43.840
And it sounds like these are things that I mean, given.

00:48:43.840 --> 00:48:47.240
And of course, you know, things are always obvious after the fact.

00:48:47.240 --> 00:48:54.960
But but that sounds like a relatively straightforward discovery that somebody made once it was once somebody looked at it.

00:48:54.960 --> 00:48:56.520
Playing just with a little bit of it.

00:48:56.520 --> 00:48:58.840
We're talking just a few months of effort.

00:48:58.840 --> 00:49:02.680
We're not talking like, you know, some whiz bag thing.

00:49:02.740 --> 00:49:05.920
Even danger of injustice, this idea of moving the data around from one system to another.

00:49:05.920 --> 00:49:09.540
The first portion of this, we're talking like passing spreadsheets.

00:49:09.540 --> 00:49:14.400
You know, we're not talking like crazy super infrastructure.

00:49:14.400 --> 00:49:18.440
This basic level stuff gets you very close to the problem.

00:49:18.440 --> 00:49:23.640
When you start working with the people, you will see a very different angle of the problem.

00:49:24.180 --> 00:49:26.720
Yeah. And so it sounds like it's it's kind of right there.

00:49:26.720 --> 00:49:28.120
It's right in front of us.

00:49:28.120 --> 00:49:36.860
And there are problems to be solved, like real human lives in the balance problems that we can be out.

00:49:36.860 --> 00:49:38.160
We can go out and be solving.

00:49:38.160 --> 00:49:38.800
Yeah.

00:49:38.800 --> 00:49:40.500
Imagine you did this.

00:49:40.500 --> 00:49:43.220
We always talk about a data set.

00:49:43.220 --> 00:49:46.340
We don't talk about the people behind the data set.

00:49:46.340 --> 00:49:50.600
And the thing that I have taken away more than this job, anything is people are greater than data.

00:49:50.600 --> 00:49:52.480
We all know that intrinsically.

00:49:52.480 --> 00:49:59.080
But if you remember and you have the people that you have in your mind when you're working on this, you'll have a different approach.

00:49:59.080 --> 00:50:00.120
All right.

00:50:00.120 --> 00:50:02.240
Well, I think that that's actually a fantastic place to end it.

00:50:02.240 --> 00:50:04.560
That's a that's a it's a nice reminder for the audience.

00:50:04.560 --> 00:50:06.300
And thank you so much, DJ.

00:50:06.300 --> 00:50:07.760
This has been a really fantastic interview.

00:50:07.760 --> 00:50:09.800
We really appreciate you coming on the show.

00:50:09.800 --> 00:50:15.440
And thanks for everything that you and your team have have done for both the data science industry and for the country.

00:50:15.440 --> 00:50:16.560
Yeah. Thank you, guys.

00:50:16.560 --> 00:50:20.700
Thanks for it's been fun and looking forward to seeing what the community can do.

00:50:20.700 --> 00:50:21.880
I'm really excited for everything.

00:50:21.880 --> 00:50:26.420
This has been another episode of Talk Python to Me.

00:50:26.420 --> 00:50:29.680
Today's guest was DJ Patel.

00:50:29.680 --> 00:50:33.120
And this episode was guest hosted by Jonathan Morgan.

00:50:33.120 --> 00:50:36.200
Thank you both for bringing us an excellent conversation.

00:50:36.760 --> 00:50:40.640
And thank you to Rollbar and GoCD for sponsoring this episode.

00:50:40.640 --> 00:50:43.340
Rollbar takes the pain out of errors.

00:50:43.340 --> 00:50:50.460
They give you the context and insight you need to quickly locate errors that might have otherwise gone unnoticed until your users complained to you, of course.

00:50:50.920 --> 00:50:54.680
As Talk Python to Me listeners, you can track a ridiculous number of errors for free.

00:50:54.680 --> 00:50:58.060
Just go to Rollbar dot com slash Talk Python To Me to get started.

00:50:58.060 --> 00:51:03.020
GoCD is the on premise open source continuous delivery server.

00:51:03.020 --> 00:51:07.160
Want to improve your deployment workflow, but keep your code and builds in house.

00:51:07.340 --> 00:51:13.500
Check out GoCD at Talk Python dot FM slash G O C D and take control over your process.

00:51:13.500 --> 00:51:16.240
Are you or a colleague trying to learn Python?

00:51:16.240 --> 00:51:20.900
Have you tried books and videos that just left you bored by covering topics point by point?

00:51:20.900 --> 00:51:29.480
Well, check out my online course Python Jumpstart by building 10 apps at Talk Python dot FM slash course to experience a more engaging way to learn Python.

00:51:29.880 --> 00:51:36.860
And if you're looking for something a little more advanced, try my write Pythonic code course at Talk Python dot FM slash Pythonic.

00:51:36.860 --> 00:51:39.580
Be sure to subscribe to the show.

00:51:39.580 --> 00:51:41.780
Open your favorite podcatcher and search for Python.

00:51:41.780 --> 00:51:43.020
We should be right at the top.

00:51:43.020 --> 00:51:52.300
You can also find the iTunes feed at /itunes, Google Play feed at /play and direct RSS feed at /rss on Talk Python dot FM.

00:51:52.300 --> 00:51:57.420
Our theme music is Developers, Developers, Developers by Corey Smith, who goes by Smix.

00:51:57.560 --> 00:52:04.120
Corey just recently started selling his tracks on iTunes, so I recommend you check it out at Talk Python dot FM slash music.

00:52:04.120 --> 00:52:09.460
You can browse his tracks he has for sale on iTunes and listen to the full length version of the theme song.

00:52:09.460 --> 00:52:11.540
This is your host, Michael Kennedy.

00:52:11.540 --> 00:52:12.820
Thanks so much for listening.

00:52:12.820 --> 00:52:14.020
I really appreciate it.

00:52:14.020 --> 00:52:16.160
Smix, let's get out of here.

00:52:16.160 --> 00:52:20.420
Stating with my voice, there's no norm that I can feel within.

00:52:20.420 --> 00:52:23.260
Haven't been sleeping, I've been using lots of rest.

00:52:23.260 --> 00:52:26.080
I'll pass the mic back to who rocked it best.

00:52:26.080 --> 00:52:27.900
First developers.

00:52:27.900 --> 00:52:28.640
First developers.

00:52:28.640 --> 00:52:29.440
First developers.

00:52:29.440 --> 00:52:37.440
Developers einfach florist like God bless you and be Remembering a bit?

00:52:37.440 --> 00:52:37.940
Bye.

00:52:37.940 --> 00:52:38.440
.

00:52:38.440 --> 00:53:08.420
Thank you.

