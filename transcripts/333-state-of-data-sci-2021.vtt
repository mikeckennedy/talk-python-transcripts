WEBVTT

00:00:00.001 --> 00:00:05.100
We know that Python and data science are growing in lockstep together, but exactly what's happening

00:00:05.100 --> 00:00:10.240
in the data science space in 2021? Stan Siebert from Anaconda is here to give us a report on what

00:00:10.240 --> 00:00:16.380
they found with their latest State of Data Science in 2021 survey. This is Talk Python to Me,

00:00:16.380 --> 00:00:19.940
episode 333, recorded August 9th, 2021.

00:00:19.940 --> 00:00:37.720
Welcome to Talk Python to Me, a weekly podcast on Python. This is your host, Michael Kennedy.

00:00:37.720 --> 00:00:42.360
Follow me on Twitter where I'm @mkennedy and keep up with the show and listen to past episodes

00:00:42.360 --> 00:00:49.040
at talkpython.fm and follow the show on Twitter via at talkpython. We've started streaming most of our

00:00:49.040 --> 00:00:54.880
episodes live on YouTube. Subscribe to our YouTube channel over at talkpython.fm/youtube to get

00:00:54.880 --> 00:01:01.760
notified about upcoming shows and be part of that episode. This episode is brought to you by Shortcut,

00:01:01.760 --> 00:01:07.880
formerly known as clubhouse.io, and masterworks.io. And the transcripts are brought to you by Assembly

00:01:07.880 --> 00:01:11.960
AI. Please check out what they're offering during their segments. It really helps support the show.

00:01:11.960 --> 00:01:15.280
Stan, welcome to Talk Python to Me.

00:01:15.280 --> 00:01:16.520
Hey, nice to be here.

00:01:16.880 --> 00:01:21.140
Yeah, it's great to have you here. I'm super excited to talk about data science things,

00:01:21.140 --> 00:01:27.460
Anaconda things, and we'll even squeeze a little one of my favorites, the Apple M1 stuff mixed in

00:01:27.460 --> 00:01:30.440
with data science. So it should be a fun conversation.

00:01:30.440 --> 00:01:32.260
I'm also very excited about the M1.

00:01:32.260 --> 00:01:37.260
Nice. Yeah, we can geek out about that a little bit. That'll be fun. But before we get there,

00:01:37.260 --> 00:01:40.020
let's just start with your story. How'd you get into programming in Python?

00:01:40.020 --> 00:01:44.720
Yeah, programming started as a kid, you know, dating myself here. I learned to program

00:01:44.720 --> 00:01:51.660
basic on the Osborne 1, a suitcase of a computer that we happened to have as a kid. And then

00:01:51.660 --> 00:01:57.620
eventually picked up C and stuff like that. Didn't learn Python until college, mostly because I was

00:01:57.620 --> 00:02:03.040
frustrated with Perl. I just found that Perl just never fit in my brain right. And so I was like,

00:02:03.040 --> 00:02:08.800
well, what other scripting languages are there? And I found Python. And that was a huge game changer.

00:02:08.800 --> 00:02:14.100
I didn't really use it professionally or like super seriously until grad school when I had a summer

00:02:14.100 --> 00:02:19.500
research job. And I realized that this new thing called NumPy could help me get do my analysis.

00:02:19.500 --> 00:02:25.380
And so that was when I really started to pick up Python seriously. And now here I am, basically.

00:02:25.380 --> 00:02:26.660
Yeah, what were you studying in grad school?

00:02:26.660 --> 00:02:33.000
I was doing physics. So I did particle physics and used Python quite extensively, actually,

00:02:33.080 --> 00:02:39.340
throughout my research. And C++, unfortunately, for better or worse. So yeah, but that's how I end.

00:02:39.340 --> 00:02:45.540
I always ended up kind of being the software person on experiments. So when I was leaving academia,

00:02:45.540 --> 00:02:49.080
going into software engineering kind of was a logical step for me.

00:02:49.080 --> 00:02:56.460
I was studying math in grad school and did a lot of programming as well. And I sort of trended more and

00:02:56.460 --> 00:03:01.580
more towards the computer side and decided that that was the path as well. But it's cool. A lot of the

00:03:01.580 --> 00:03:06.700
sort of logical thinking, problem solving you learn in physics or math or whatever, they translate

00:03:06.700 --> 00:03:08.440
pretty well to programming.

00:03:08.440 --> 00:03:14.260
Yeah, yeah. And definitely, you know, working on large experiments, a lot of the sort of soft skills

00:03:14.260 --> 00:03:18.360
of software engineering, things like how do you coordinate with people? How do you design software

00:03:18.360 --> 00:03:22.560
for multiple people to use? That sort of thing. I actually, I inadvertently was learning how to be

00:03:22.560 --> 00:03:28.080
a software manager as a physicist and then only realized it later when I went into industry.

00:03:29.700 --> 00:03:31.780
And how about now? You're over at Anaconda, right?

00:03:31.780 --> 00:03:37.060
Yeah. So, you know, maybe I'm doing the same thing. So now I'm both a developer and a manager

00:03:37.060 --> 00:03:38.400
at Anaconda.

00:03:38.400 --> 00:03:45.260
It's a direct path from like PhD physics, particle physics to programming to data science at Anaconda.

00:03:45.260 --> 00:03:45.800
Is that how it goes?

00:03:45.800 --> 00:03:51.640
Yeah. I mean, we employ a surprising number of scientists who are now software engineers.

00:03:51.640 --> 00:03:57.400
And so I manage the team that does a lot of the open source at Anaconda. So we work on stuff like

00:03:57.400 --> 00:04:04.720
Numba and Dask and various projects like that. Just recently hired the piston developers to broaden our

00:04:04.720 --> 00:04:11.640
scope into more Python JIT optimization kind of stuff. So yeah, so I'm doing a mix of actual development on

00:04:11.640 --> 00:04:15.940
some projects as well as just managing strategy, the usual kind of stuff.

00:04:16.060 --> 00:04:20.460
Well, I suspect most people out there know what Anaconda is, but I have listeners who come from all

00:04:20.460 --> 00:04:25.980
over, you know, what is Anaconda? It's kind of like a Python you download, but it's also,

00:04:25.980 --> 00:04:28.720
it has its own special advantages, right?

00:04:28.720 --> 00:04:35.600
Yeah. I mean, where we came out of and still is our main focus is how to get Python and just

00:04:35.600 --> 00:04:39.460
broader data science tools. One of the interesting things about data science is it's not just Python.

00:04:40.180 --> 00:04:44.540
Most of people are going to have to combine Python and maybe they don't realize it, but with Fortran

00:04:44.540 --> 00:04:49.480
and C++ and all the things that underpin all of these amazing libraries. And so a lot of what we do is

00:04:49.480 --> 00:04:54.640
try to get Python into the hands of data scientists is, you know, get them the latest things and make it

00:04:54.640 --> 00:04:58.820
easy for them to install on whatever platform they're on. Windows, Mac, Linux, that sort of thing.

00:04:58.820 --> 00:05:05.540
So the, you know, Anaconda has a, you know, a free, call it individual edition. It's basically a

00:05:05.540 --> 00:05:10.700
package distribution and installer that lets you get started. And then you can, there are thousands

00:05:10.700 --> 00:05:14.920
of Conda packages, Conda's packaging system. There are thousands of Conda packages that you can install

00:05:14.920 --> 00:05:19.740
where, you know, we, or, you know, the broader community have done a lot of the hard work to

00:05:19.740 --> 00:05:24.320
make sure all of those compiled packages are built to run on your system.

00:05:24.320 --> 00:05:29.820
That's one of the real big challenges of the data science stuff is getting it compiled for your

00:05:29.820 --> 00:05:34.780
system. Because if I use requests, it's, you know, pip install requests. I probably,

00:05:34.780 --> 00:05:39.700
maybe it runs a setup high. Maybe it just comes down as a wheel. I don't know, but it's just pure

00:05:39.700 --> 00:05:44.340
Python and there's not a whole lot of magic. If I'm really getting there far out there, maybe I'm

00:05:44.340 --> 00:05:49.940
using SQLAlchemy and it has some C optimizations. It will try to compile. And if it doesn't, well,

00:05:49.940 --> 00:05:55.520
it'll run some slower Python version probably. But in the data science world, you've got really heavy

00:05:55.520 --> 00:05:59.980
dependencies, right? Like, as you said, stuff that requires a Fortran compiler on your computer.

00:05:59.980 --> 00:06:05.260
I don't know if I have a Fortran compiler on my Mac. I'm pretty sure I don't. Maybe it's in there.

00:06:05.260 --> 00:06:12.840
Probably not. Right. And as maybe C++, probably have a C++ compiler, but maybe not the right one.

00:06:12.840 --> 00:06:18.440
Maybe not the right version. Maybe my path is not set up right. And plus it's slow, right? All of these

00:06:18.440 --> 00:06:25.100
things are a challenge. So Anaconda tries to basically be, let's rebuild that stuff with a tool chain that

00:06:25.100 --> 00:06:29.460
we know will work and then deliver you the final binaries, right? The challenge with that for a lot

00:06:29.460 --> 00:06:34.980
of tooling is it's downloaded and installed to different machines with different architectures,

00:06:34.980 --> 00:06:40.280
right? So you've gone and built stuff for macOS, you built stuff for Linux, you built stuff for

00:06:40.280 --> 00:06:42.240
Windows and whatnot. Is that right?

00:06:42.240 --> 00:06:50.760
Yeah. Yeah. Building software is non-trivial and no matter how much a developer tries to automate it so

00:06:50.760 --> 00:06:56.920
that things just work, it helps to have someone do a little bit of quality control and a little bit of

00:06:56.920 --> 00:07:02.060
just deciding how to set all the switches to make sure that you get a thing that works so that you

00:07:02.060 --> 00:07:10.520
can just get going quickly. Early on, I remember in the sort of 2014, 2015 era, Anaconda was extremely

00:07:10.520 --> 00:07:14.360
popular with Windows users who did not have a lot of good options for how to get this stuff.

00:07:14.360 --> 00:07:14.660
Right.

00:07:15.020 --> 00:07:19.320
Like with Linux, you could kind of get it together and get it going. If you were motivated on Windows,

00:07:19.320 --> 00:07:26.000
it was often just like a very much, I don't know what to do. And so this making it sort of one-stop

00:07:26.000 --> 00:07:29.700
shopping for all of these packages. And then another thing we wanted to do is make sure that there was a

00:07:29.700 --> 00:07:35.260
whole community of package building around it. It wasn't just us. So things like Condo Forge is a

00:07:35.260 --> 00:07:42.280
community of package builders that we are part of and hugely support. Because there's a long tail,

00:07:42.280 --> 00:07:45.560
there's always going to be stuff that is going to be, you know, we're never going to get around to

00:07:45.560 --> 00:07:45.940
packaging.

00:07:45.940 --> 00:07:52.720
Right. There's important stuff that you're like, this is essential. So NumPy, Matplotlib, and so on.

00:07:52.720 --> 00:07:57.860
Like you all take control of making sure that that one gets out. But there's some, you know,

00:07:57.860 --> 00:08:03.060
biology library that people don't know about that you're not in charge of. And that's what the

00:08:03.060 --> 00:08:10.020
Condo Forge plus Condo is, is like, sort of like pip and PyPI, but also in a slightly more structured way.

00:08:10.020 --> 00:08:14.680
Yeah. Yeah. And that was why, you know, Condo was built to help make it so that it is possible for

00:08:14.680 --> 00:08:18.880
this community to grow up, for people to package things that aren't Python at all that you might

00:08:18.880 --> 00:08:24.060
need, all kinds of stuff like that. And yeah, they, you know, there's always going to be, you know,

00:08:24.060 --> 00:08:28.360
in your specific scientific discipline. I mean, so for example, Bioconda is a really interesting

00:08:28.360 --> 00:08:34.820
distribution of packages built by the bioinformatics community built on Condo, but they have all of the

00:08:34.820 --> 00:08:37.420
packages that they care about. And many of which I've never heard of,

00:08:37.880 --> 00:08:41.500
aren't in common use, but are really important to that scientific discipline.

00:08:41.500 --> 00:08:47.760
Out in the live stream, we have a question from Neil Heather. Hey Neil, I mentioned Linux, Windows,

00:08:47.760 --> 00:08:53.260
macOS. Neil asked, does Anaconda work on Raspberry Pi OS as in ARM64?

00:08:53.260 --> 00:09:01.220
Yeah. So the answer to that is Anaconda, not yet. Condo Forge does have a set of community built

00:09:01.220 --> 00:09:08.120
packages for Raspberry Pi OS. The main challenge there is actually, we just a couple months ago

00:09:08.120 --> 00:09:14.880
announced ARM64 support, but it was aimed at the server ARM machines that are running ARM 8.2

00:09:14.880 --> 00:09:21.880
instruction set, which the Raspberry Pi is 8.0. And so the packages we built, which will work great on

00:09:21.880 --> 00:09:27.760
server ARM, are missing, are using some instructions that Raspberry Pis can't support. But Condo Forge,

00:09:27.760 --> 00:09:33.360
so if you go look up Condo Forge and Raspberry Pi, you'll find some instructions on how to install for

00:09:33.360 --> 00:09:33.580
that.

00:09:33.580 --> 00:09:40.700
ARM is interesting, right? So let's talk a little bit about that because I find that this whole Apple

00:09:40.700 --> 00:09:47.300
Silicon move, you know, they created their M1 processor and they said, you know what, we're dropping

00:09:47.300 --> 00:09:53.640
Intel, dropping x86, more importantly, and we're going to switch to basically iPad processors,

00:09:54.040 --> 00:09:59.480
slightly amped up iPad processors that turn out to be really, really fast, which is actually

00:09:59.480 --> 00:10:07.980
blew my mind and it was unexpected. But I think the success of Apple is actually going to encourage

00:10:07.980 --> 00:10:14.740
others to do this as well. And it's going to add, you know, more platforms that things like

00:10:14.740 --> 00:10:21.200
Anaconda, Condo Forge and stuff are going to have to support, right? So there's a cool article over here

00:10:21.200 --> 00:10:28.160
by you on Anaconda called A Python Data Scientist's Guide to the Apple Silicon Transition.

00:10:28.160 --> 00:10:33.960
Yeah, this was, you know, I've been, I'm a huge chip nerd, just due to background and thinking about

00:10:33.960 --> 00:10:40.040
optimization and performance. And so this came out of, you know, some experiments I was doing

00:10:40.040 --> 00:10:45.120
to just understand, I mean, we got some M1 Mac minis into our data center and started immediately

00:10:45.120 --> 00:10:49.720
playing with them. And I realized I, after some, you know, I should take the stuff I was,

00:10:49.720 --> 00:10:53.680
I was learning and finding and put it together in a document for other people because I couldn't find

00:10:53.680 --> 00:10:59.720
this information anywhere organized in a way that was, you know, for me as a Python developer,

00:10:59.720 --> 00:11:02.200
I was having a hard time putting it all together.

00:11:02.200 --> 00:11:07.020
Right. There was some anecdotal stuff about just like, yeah, this kind of works for me,

00:11:07.020 --> 00:11:09.900
or this is kind of fast or this kind of slow, but this is a little more,

00:11:10.340 --> 00:11:14.960
here's the whole picture and what the history is and where it's going and what it means and

00:11:14.960 --> 00:11:19.160
specifically focused on the Conda side of things. Right.

00:11:19.160 --> 00:11:23.440
Yeah. And even just the Python side, it's, I mean, it's, it's sort of an interesting

00:11:23.440 --> 00:11:27.020
problem of, you know, Python's an interpreted language. So you're like, well, I don't, I don't

00:11:27.020 --> 00:11:32.020
have any machine code to worry about. Right. But the interpreter of course is compiled. So you at

00:11:32.020 --> 00:11:37.020
least need that. And then many, many Python packages also contain compiled bits and you'll need those

00:11:37.020 --> 00:11:42.100
two. And, and this is, this is an interesting broad problem for the whole, the whole Python

00:11:42.100 --> 00:11:46.920
ecosystem to try and tackle because that's not too often a whole new platform kind of just appears,

00:11:46.920 --> 00:11:50.400
you know, making it a whole new architecture takes a while.

00:11:50.400 --> 00:11:56.060
It absolutely does. I think there's a lot of interesting benefits to come. I do want to point

00:11:56.060 --> 00:12:02.220
out for people listening. If you jump over to the PSF JetBrains Python developer survey,

00:12:02.420 --> 00:12:09.120
the most recent one from 2020, and you look around a bit, you'll see that while we don't run

00:12:09.120 --> 00:12:18.120
production stuff on macOS that much, 29% of the developers are using macOS to develop Python code.

00:12:18.120 --> 00:12:24.320
Right. So Apple's pledged that we're going to take a hundred percent of this and move it over

00:12:24.320 --> 00:12:31.440
to Silicon means almost a third of the people running Python in a couple of years will be under

00:12:31.440 --> 00:12:36.580
this environment. Right. And even if you have a windows or Linux machine and you don't care about

00:12:36.580 --> 00:12:42.540
macOS, you may be maintaining a package for people who do. Yeah. And that means Apple Silicon, right?

00:12:42.540 --> 00:12:46.720
Yeah. And there's, I mean, it's, it's interesting. There's a whole, I mean, just other stuff you take

00:12:46.720 --> 00:12:54.120
for granted you know, the availability of, of free continuous integration services that has been

00:12:54.120 --> 00:12:57.720
transformative for the open source community. I mean, it's really improved the software quality that all

00:12:57.720 --> 00:13:02.580
these open source projects can automatically run their tests and build packages every time there's a new

00:13:02.580 --> 00:13:10.260
change. However, it's something like this comes out. And until you get, you know, arm Macs into these services and

00:13:10.260 --> 00:13:14.620
if they're, you know, until they're freely available, a lot of the infrastructure of these open source

00:13:14.620 --> 00:13:19.800
projects, they don't have a way to test on an M1 Mac except manually if they happen to have one and they

00:13:19.800 --> 00:13:25.000
don't have a way to automate their build on an M1 Mac until that, until that sorts out. Yeah. And thinking

00:13:25.000 --> 00:13:32.340
about the workflow here, there there's two challenges that this presents. One is you want to do a get push

00:13:32.340 --> 00:13:38.620
production or get pushed to some branch or tag it. And that's going to trigger a CI build that might fork

00:13:38.620 --> 00:13:44.260
off to run a windows compile, a Linux compile, a Mac compile, generate some platform

00:13:44.260 --> 00:13:49.180
specific wheels with like Fortran compiled in there or whatever. And then you're going to ship that off.

00:13:49.180 --> 00:13:56.880
If that CI system doesn't have an Apple Silicon machine, it can't build for Apple Silicon, right?

00:13:56.880 --> 00:13:57.980
Yep. Yep.

00:13:57.980 --> 00:14:00.140
And there was a time.

00:14:00.140 --> 00:14:05.720
Yeah. Sorry. I mean, yeah. Well, where do you, you know, where do you get M1 in the cloud, right? As a

00:14:05.720 --> 00:14:13.900
normal, I know there's a few hosted places, but as a, like a normal GitHub or an Azure, it's not common to just go grab a bunch

00:14:13.900 --> 00:14:15.340
of those and pile them up. Right.

00:14:15.340 --> 00:14:19.760
Yeah. And it'll take time. I mean, eventually in the same way that, you know, I was thinking back to,

00:14:19.760 --> 00:14:25.280
you know, go back four or five years ago it was, there wasn't a whole lot of options for windows CI

00:14:25.280 --> 00:14:31.260
available. There were a couple of providers and, and then there was sort of a huge change and then

00:14:31.260 --> 00:14:36.360
pretty much everyone offered a windows option and they were faster and all of this stuff. And so I think,

00:14:36.360 --> 00:14:41.380
but that took time. And, and I think that's the thing is, is these, the hardware is in people's hands now,

00:14:41.380 --> 00:14:47.080
and it's just going to get more and more. And, and it's unclear how quickly we can catch up.

00:14:47.080 --> 00:14:49.900
That's going to be a challenge for all of us.

00:14:49.900 --> 00:14:54.100
It's absolutely going to be a challenge. It's, it's interesting. I hope that we, we get there soon.

00:14:54.100 --> 00:15:02.100
The other problem in this same workflow is I was actually just looking at some NumPy issues,

00:15:02.360 --> 00:15:07.600
specifically issue 18,143. I'm sure people have that right off the top of their head.

00:15:07.600 --> 00:15:15.300
The title is please provide universal two wheels for macOS. And there's a huge, long comp conversation

00:15:15.300 --> 00:15:22.680
about, I mean, this is like many, many lines of many, many messages in the thread. And one of the

00:15:22.680 --> 00:15:30.480
problems they brought up is like, look, we can find a way to compile the binary bits, the C++ bits for M1,

00:15:30.480 --> 00:15:36.940
but we can't test it. Like if we can't, we as developers cannot run this, this output, like it's,

00:15:36.940 --> 00:15:43.140
it's a little sketchy to just compile and ship it to the world. And to be fair, this is on January 9th of

00:15:43.140 --> 00:15:48.300
2021, when it was still hard, you know, these things were still shipping and still arriving there.

00:15:48.300 --> 00:15:51.160
It was not like you just go to the Apple store and pick one up.

00:15:51.160 --> 00:15:58.180
This portion of Talk Python to Me is brought to you by Shortcut, formerly known as clubhouse.io.

00:15:58.180 --> 00:16:03.440
Happy with your project management tool? Most tools are either too simple for a growing engineering team

00:16:03.440 --> 00:16:08.400
to manage everything, or way too complex for anyone to want to use them without constant prodding.

00:16:08.400 --> 00:16:12.780
Shortcut is different though, because it's worse. No, wait, no, I mean, it's better.

00:16:12.780 --> 00:16:18.560
Shortcut is project management built specifically for software teams. It's fast, intuitive, flexible,

00:16:18.760 --> 00:16:23.920
powerful, and many other nice, positive adjectives. Key features include team-based workflows.

00:16:23.920 --> 00:16:29.000
Individual teams can use default workflows or customize them to match the way they work.

00:16:29.000 --> 00:16:34.760
Org-wide goals and roadmaps. The work in these workflows is automatically tied into larger company

00:16:34.760 --> 00:16:40.720
goals. It takes one click to move from a roadmap to a team's work to individual updates and back.

00:16:40.720 --> 00:16:45.120
Type version control integration. Whether you use GitHub, GitLab, or Bitbucket,

00:16:45.260 --> 00:16:49.460
Clubhouse ties directly into them, so you can update progress from the command line.

00:16:49.460 --> 00:16:54.540
Keyboard-friendly interface. The rest of Shortcut is just as friendly as their power bar,

00:16:54.540 --> 00:16:59.400
allowing you to do virtually anything without touching your mouse. Throw that thing in the trash.

00:16:59.400 --> 00:17:05.580
Iteration planning. Set weekly priorities and let Shortcut run the schedule for you with accompanying

00:17:05.580 --> 00:17:11.940
burndown charts and other reporting. Give it a try over at talkpython.fm/shortcut.

00:17:11.940 --> 00:17:18.960
Again, that's talkpython.fm/shortcut. Choose shortcut because you shouldn't have to project manage

00:17:18.960 --> 00:17:20.160
your project management.

00:17:22.020 --> 00:17:29.660
Yeah, as an interesting example, CondoForge was able to get Condo packages for Apple Silicon out pretty

00:17:29.660 --> 00:17:34.440
quickly, but they did it with a clever sort of cross-compilation strategy where they were building

00:17:34.440 --> 00:17:44.160
on x86 Macs the ARM packages and pushing them out. But they had enough people manually testing that they

00:17:44.160 --> 00:17:48.520
had confidence in the process that it was okay. But that's very different than how they build other

00:17:48.520 --> 00:17:53.280
packages, which are built and tested immediately, automatically. And if they fail tests, they don't

00:17:53.280 --> 00:17:58.280
get uploaded. So that's, you know, it was, it was, it's a risk, but it helped get the software out in

00:17:58.280 --> 00:18:03.980
people's hands quicker. But yeah, long-term we need to get these machines onto all these CI systems so

00:18:03.980 --> 00:18:08.680
that we can use the same techniques we've built up over the years to ensure we have quality software.

00:18:08.680 --> 00:18:12.400
I think we'll get there, but it's just going to take some time, right?

00:18:12.740 --> 00:18:13.600
Yep. Yep. Yeah.

00:18:13.600 --> 00:18:19.240
Let's see. Neil on Livestream says, speaking of open source, Apple is rumored to be hiring experts

00:18:19.240 --> 00:18:25.320
in a risk. V or Fives have perhaps moved away from having to pay licensing fees to ARM. Yeah. I'm not

00:18:25.320 --> 00:18:26.200
sure about that, but.

00:18:26.200 --> 00:18:32.960
Yeah. I mean, it's a, what's interesting here is, is the, I mean, other, you know, chip architectures

00:18:32.960 --> 00:18:38.100
have been around for a long, long time, but until very recently, you know, average users

00:18:38.100 --> 00:18:43.040
didn't have to think about X86 versus ARM. ARM was for mobile phones and other, you know,

00:18:43.040 --> 00:18:45.300
never had to worry about power PC or anything like that.

00:18:45.300 --> 00:18:46.340
Not for real computers.

00:18:46.340 --> 00:18:46.880
Yeah.

00:18:46.880 --> 00:18:53.400
And so, but now once you, once, you know, going from one to two is a big step. Now the floodgates

00:18:53.400 --> 00:18:56.360
are open and now we're thinking about, well, what else is out there? I mean, you know, risk

00:18:56.360 --> 00:19:01.720
five, I'm not sure how you say, I think risk five is what you call it. Is, is an interesting

00:19:01.720 --> 00:19:07.980
thing. And has even, you know, being a completely open standard, you don't have to even pay licensing

00:19:07.980 --> 00:19:15.200
fees as mentioned. I don't know if Apple's going to make this transition again so quickly. But I,

00:19:15.200 --> 00:19:19.080
I can guarantee you that, you know, everyone probably somewhere in a basement is thinking

00:19:19.080 --> 00:19:24.760
about it, maybe doing some experiments. But yeah, chips move slowly, but it's interesting to think

00:19:24.760 --> 00:19:25.040
about.

00:19:25.040 --> 00:19:29.720
Yeah. That's not a thing you can change very frequently with drag developers along. I mean,

00:19:29.720 --> 00:19:34.480
we're talking about all the challenges, you know, that are just down the pipeline from

00:19:34.480 --> 00:19:34.720
that.

00:19:34.720 --> 00:19:35.180
Yeah.

00:19:35.180 --> 00:19:38.200
Very interesting. All right. Well, let's, let's just talk a few, a little bit about this.

00:19:38.200 --> 00:19:42.380
First, you're excited about these as a data scientist.

00:19:42.380 --> 00:19:46.960
Yeah. It's, it's there. I'm there really for sort of two reasons. I mean, one thing that's

00:19:46.960 --> 00:19:51.980
interesting is just the power efficiency. I always, there was a talk long ago from the chief

00:19:51.980 --> 00:19:55.280
scientist in NVIDIA, which really had an impression on me in which he, you know, paraphrasing roughly,

00:19:55.280 --> 00:20:02.240
basically said that because everything is now power constrained power efficiency equals performance

00:20:02.240 --> 00:20:06.460
in a way that is, you know, normally you just think, well, just put more power in there, but

00:20:06.460 --> 00:20:11.560
that heat has to go somewhere. So you, you, we long since hit that wall. And so now you just have to

00:20:11.560 --> 00:20:13.860
get more efficient to get more performance. Right.

00:20:13.960 --> 00:20:15.080
That's an interesting opportunity.

00:20:15.080 --> 00:20:19.100
You can get more, you can get like larger power supplies and larger computers. I have a

00:20:19.100 --> 00:20:24.240
gaming SIM computer and it is so loud. If you get it going full power, like if the windows are open,

00:20:24.240 --> 00:20:30.080
you can hear it outside the house. It's literally that loud. But at the same time, it's not just on

00:20:30.080 --> 00:20:36.580
your personal computer, you know, in the cloud and places like that, right. You, you pay not just,

00:20:36.580 --> 00:20:41.740
you know, how much performance you get. There's some sort of combination of how much energy does that

00:20:41.740 --> 00:20:47.380
particular processor take to run. And if it's one fifth, you might be able to buy more cloud compute

00:20:47.380 --> 00:20:48.580
per dollar.

00:20:48.580 --> 00:20:53.500
Yeah. Power and cooling is a huge part of a computer, you know, data center expenses.

00:20:53.500 --> 00:20:59.980
And even just, you know, it, you can only, you can put maybe, you know, one to 300 Watts into a CPU.

00:20:59.980 --> 00:21:04.580
You're not, you're not going to put, you know, multiple kilowatts in there or something. And so

00:21:04.580 --> 00:21:08.980
where, where is that? What else, what else can you do? And a lot of that is that, you know,

00:21:09.060 --> 00:21:13.200
Moore's law is driven a lot by just every time you shrink the process, you do get more power

00:21:13.200 --> 00:21:18.000
efficient. And, but now it's interesting to think about architectures that have been sort of thought

00:21:18.000 --> 00:21:23.700
of that, that arm has come in into its own in a extremely power constrained environment. And so now

00:21:23.700 --> 00:21:29.140
we're letting it loose on a laptop, which has way more power compared to a cell phone available.

00:21:29.700 --> 00:21:33.720
What could we do if we fed, you know, right into the socket in the wall?

00:21:33.720 --> 00:21:36.980
Yeah. And you know, what happens when I put it in the data center?

00:21:36.980 --> 00:21:38.100
Yeah.

00:21:38.100 --> 00:21:42.240
So that's, that's, I think arm in the data center is going to be really important.

00:21:42.240 --> 00:21:42.820
Yeah.

00:21:42.820 --> 00:21:43.360
Yeah.

00:21:43.360 --> 00:21:43.920
Yeah.

00:21:43.920 --> 00:21:49.300
I think it's, it's definitely, I'd always expected that to come before the desktop.

00:21:49.300 --> 00:21:55.240
To be honest, I was surprised as many people were by the, you know, suddenness of the Apple

00:21:55.240 --> 00:22:00.400
transition. cause I had assumed this maybe would happen much after we all got used to arm

00:22:00.400 --> 00:22:05.140
in the data center, where you're probably running Linux and it's easy to recompile compared

00:22:05.140 --> 00:22:06.900
to, you know, Mac and stuff like that.

00:22:06.900 --> 00:22:12.820
Yeah. That's what I thought as well. The payoff is so high, right? They spend so much energy

00:22:12.820 --> 00:22:18.400
on both direct electricity, as well as then cooling from the waste heat, from that energy

00:22:18.400 --> 00:22:24.780
that it's the payoff is just completely, completely clear. Right. All right. So let's see, a couple

00:22:24.780 --> 00:22:29.920
of things that you pointed out that make a big difference here is obviously arm versus x86,

00:22:29.920 --> 00:22:35.880
built in on chip GPU, the whole system as a system on a chip thing, rather than a bunch of pieces going

00:22:35.880 --> 00:22:41.940
through motherboard is pretty interesting. But I think the, maybe the most interesting one has to do

00:22:41.940 --> 00:22:46.540
with the acceleration, things like the Apple neural engine that's built in and whatnot.

00:22:46.540 --> 00:22:52.260
It sounds like the data science libraries in general are not targeting the built-in neural

00:22:52.260 --> 00:22:55.540
engines yet, but maybe, maybe they will in the future. I don't know.

00:22:55.540 --> 00:22:59.340
Yeah. It's a, it's something that we're going to have to figure out because, I mean, I think it

00:22:59.340 --> 00:23:02.860
was a bit of chicken the egg that, you know, until this happened, you didn't have this kind of

00:23:02.860 --> 00:23:07.880
hardware just sitting on people's desks. and you weren't going to, you know, run, data science

00:23:07.880 --> 00:23:12.480
stuff on your phone. So now that it's here now, the question is, okay, what can we do with it?

00:23:12.960 --> 00:23:16.900
I mean, right now, for example, you know, for the Apple neural engine, you can take advantage

00:23:16.900 --> 00:23:22.920
of it using something called Coromel tools, which actually did a webinar sometime back on,

00:23:22.920 --> 00:23:27.040
and, and, but that's like for basically you've trained a model and you want to run inference on it

00:23:27.040 --> 00:23:31.880
more efficiently and quickly. but that's, you know, that's it. There's a, there's an alpha,

00:23:31.880 --> 00:23:37.440
uh, release of TensorFlow. That's GPU accelerated. And it would take advantage of the, you know,

00:23:37.440 --> 00:23:41.940
on the M one, if you're, if you're running it there, but that's super early. and, and there's,

00:23:41.940 --> 00:23:45.500
uh, a lot more opportunities like that, but again, that will take time to adapt.

00:23:45.500 --> 00:23:53.700
It will. I suspect as there's bigger gains to be had, they'll probably more likely to be adopted.

00:23:53.700 --> 00:24:00.260
Right. So for example, I have my Mac mini here that I just completely love, but it, it's not that

00:24:00.260 --> 00:24:06.180
powerful say compared to like a GeForce video card or something like that. But if Apple announces

00:24:06.180 --> 00:24:14.520
something like a, a huge Apple pro Mac pro, with many, many, you know, 128 cores instead of 16 or

00:24:14.520 --> 00:24:18.560
whatever, right. Then all of a sudden in the neural engine, all of a sudden that neural engine becomes

00:24:18.560 --> 00:24:23.620
really interesting, right? And maybe it's worth going to the extra effort of writing specific code for it.

00:24:23.620 --> 00:24:27.240
Yeah. Yeah. Well, that's the other thing that's interesting about this is we've only seen one

00:24:27.240 --> 00:24:34.460
of these chips and it is by definition, the slowest one that will ever be made. And so, it's,

00:24:34.460 --> 00:24:37.760
it's, it's, we don't even know how, you know, what is it going to be like to scale up? I mean,

00:24:37.760 --> 00:24:42.760
one of those things that is, you know, you, if you're targeting that big desktop user, how are

00:24:42.760 --> 00:24:46.900
they going to scale this up? This, this all fit on one package. Can they still do that? Will they

00:24:46.900 --> 00:24:51.320
have to split out into multiple packages? there's a lot of engineering challenges that they

00:24:51.320 --> 00:24:56.100
have to solve and we're not sure how they're going to solve them yet out on the outside. So,

00:24:56.100 --> 00:25:00.080
we're going to have to, we have to see. It's going to be exciting to see that come along here.

00:25:00.080 --> 00:25:05.400
All right. So, let's touch on just a couple of things, getting Python packages for M1.

00:25:05.400 --> 00:25:11.280
What are some of the options there? Yeah. So, so the status still is roughly how I have in

00:25:11.280 --> 00:25:16.140
this article, which is basically you can use pip to install stuff if wheels have been built and a

00:25:16.140 --> 00:25:21.480
number of packages like NumPy have started to catch up and have, wheels that will run on the M1.

00:25:21.480 --> 00:25:27.780
another option which works surprisingly well is to just use an x86 Python packaging distribution.

00:25:27.780 --> 00:25:31.800
I think that's actually what I'm doing because it just runs over Rosetta 2.

00:25:31.800 --> 00:25:37.840
Yeah. And that, yeah, it just works. it is shocking. I mean, Rosetta 2 on average,

00:25:37.840 --> 00:25:44.340
I'm finding a sort of like a 20% speed hit, which for an entirely entire architecture switch is amazing.

00:25:44.340 --> 00:25:50.240
I've never seen that before. or you can use a condo forge has the, as I mentioned earlier,

00:25:50.240 --> 00:25:55.220
their, their sort of experimental, macOS arm, package distribution, which doesn't have

00:25:55.220 --> 00:26:00.200
everything, but has a lot of things, and is using them, you know, it is all built for arm.

00:26:00.200 --> 00:26:02.120
It's, there's no translation or anything going on there.

00:26:02.120 --> 00:26:09.020
Right. And on python.org, I believe the Python is that you, if you go and download, I believe it's,

00:26:09.020 --> 00:26:17.280
um, a universal binary now for sure. So that means it'll, it'll adapt and just run on arm or run on x86.

00:26:17.280 --> 00:26:23.520
You just get one binary. The, the numpy conversation was kind of around that as well,

00:26:23.520 --> 00:26:28.780
I believe. All right. you got, you did some, performance analysis on the performance cores

00:26:28.780 --> 00:26:33.600
versus efficiency cores. That was pretty interesting. And so that was pretty similar to hyper threading.

00:26:33.600 --> 00:26:38.820
If you want to run Linux or windows, you basically got to go with Docker or parallels. And then I guess

00:26:38.820 --> 00:26:43.520
maybe the last thing is like, let's wrap up this subtopic with like pros and cons for data scientists,

00:26:43.520 --> 00:26:47.900
people out there listening. They're like, ah, I can't take hearing about how cool the M1 is anymore.

00:26:47.900 --> 00:26:51.380
Maybe I'm going to have to get one of these. Like, should they like, what do you think as a data

00:26:51.380 --> 00:26:55.620
scientist? Yeah. As a data scientist, my takeaway from all the testing was you should be really excited

00:26:55.620 --> 00:27:00.740
about this, but I would wait unless you are doing what I would describe as a little bit of data science

00:27:00.740 --> 00:27:05.560
on the side and not a huge amount. mainly because, you know, these, the, what they've proven is

00:27:05.560 --> 00:27:10.300
the architecture has great performance and great battery life. The thing we still have to see is how are they

00:27:10.300 --> 00:27:14.420
going to get more Ram in there? How are they going to get more cores in there? and, and then also

00:27:14.420 --> 00:27:19.600
when is the rest of the ecosystem going to catch up on package support? so I, honestly, I, I'm,

00:27:19.600 --> 00:27:23.100
you know, if you're interested in sort of bleeding edge, knowing what's coming, I would totally jump in.

00:27:23.100 --> 00:27:27.480
if you want this for your day to day, I would probably still wait and see what comes out next.

00:27:27.480 --> 00:27:32.260
because I think a data scientist especially is going to want some of the, you know, more cores and

00:27:32.260 --> 00:27:36.700
more Ram, especially than what these machines offer. Right. There's always remote desktop or,

00:27:36.700 --> 00:27:41.160
or SSH or something like that. Right. If you've got an Intel machine sitting around,

00:27:41.160 --> 00:27:45.620
you can just connect over the network locally. Yeah. Yeah. Very cool. All right. Excellent.

00:27:45.620 --> 00:27:50.120
I just want to give a quick mention that Paul Everett from JetBrains and I did a Python developer

00:27:50.120 --> 00:27:56.100
explores Apple's M1 way, way back in December 11th of 2020, right. When this thing came out.

00:27:56.100 --> 00:28:01.920
so, people can check that. I'll put that in the show notes as well. All right. Let's talk about

00:28:01.920 --> 00:28:07.480
the state of data science, 2021. How'd you all find out about this? How do you know the state?

00:28:07.480 --> 00:28:12.540
Yeah. So, this is something we've been doing for a few years now. I mean, since we have

00:28:12.540 --> 00:28:17.120
a big data scientist audience, you know, a couple of years back, we decided, Hey, let's,

00:28:17.120 --> 00:28:22.520
let's ask them about what challenges they're seeing in their jobs, but, and then publish the results so

00:28:22.520 --> 00:28:26.980
that the whole industry can learn a little bit more about what are data scientists seeing in their day-to-day

00:28:26.980 --> 00:28:31.460
jobs that's, you know, going well, going poorly, where do they want to see improvements? What are

00:28:31.460 --> 00:28:36.360
they, what are they sort of, feeling and thinking? So you got a bunch of people to come

00:28:36.360 --> 00:28:44.500
fill out, the survey and give you some feedback and yeah, yeah, we, we, we, you know, 140 plus

00:28:44.500 --> 00:28:49.460
countries. So we have pretty good, reach across the world. and, and, you know, more than 4,200

00:28:49.460 --> 00:28:55.120
people took the survey. So it's, yeah, we got a lot of responses. It's always amazing to

00:28:55.120 --> 00:29:00.460
see. Yeah. Quick side thought here, I guess. So you've got in that survey, which I'll link to the

00:29:00.460 --> 00:29:06.040
PDF results in the show notes, you've got all the countries highlighted and obviously North America

00:29:06.040 --> 00:29:12.960
is basically completely lit up as like a popular place of results. So as Western Europe, Australia,

00:29:12.960 --> 00:29:19.480
and even Brazil, Africa is pretty, on the light on the side, what else can be done to get

00:29:19.480 --> 00:29:24.120
sort of more Python, more data science going in Africa? Do you think you have any thoughts on that?

00:29:24.120 --> 00:29:28.740
No, I don't. That's a good, that's an excellent question. I don't, that's actually might be a

00:29:28.740 --> 00:29:33.200
good question for a future survey to be honest is, is I can speculate, you know, I don't know if it's,

00:29:33.200 --> 00:29:38.760
um, you know, access to the computing or if it's bandwidth or, or if it's, you know,

00:29:39.360 --> 00:29:43.820
resources available in the local languages. I mean, there's all sorts of possibilities.

00:29:43.820 --> 00:29:47.940
One thing that is really nice about Python and data science is so much of the stuff is free,

00:29:47.940 --> 00:29:54.120
right? So it's, it's not like, oh, you got to pay, you know, some huge Oracle database license to use

00:29:54.120 --> 00:29:59.220
it or whatever. Right. So I mean, there's a real possibility of that. So yeah, I don't really know

00:29:59.220 --> 00:30:05.380
either, but, let's see, there's the standard stuff about like education level. I guess one of the

00:30:05.380 --> 00:30:09.780
areas maybe we could start on, it's just, you know, people who are doing data science,

00:30:09.780 --> 00:30:15.400
where, where do they live in the organization, right? Are they the CEO? Are they vice president?

00:30:15.400 --> 00:30:22.420
A good portion of them were, 50% is either senior folks or managers. That's kind of interesting,

00:30:22.420 --> 00:30:28.900
right? Yeah, I can see it sort of coming out of, data science as helping in decision-making

00:30:28.900 --> 00:30:34.420
and that sort of thing. And so I can, I can see it gravitating towards, the decision makers in an

00:30:34.420 --> 00:30:38.720
organization. and, and that sort of thing. I mean, one of the interesting things that,

00:30:38.720 --> 00:30:45.360
maybe as in a later, later one of the pages is, how spread out data science is across the

00:30:45.360 --> 00:30:51.240
different departments as well. that there was, you know, obviously it and R and D show up higher

00:30:51.240 --> 00:30:57.620
than the others. but you kind of see a long tail in all the departments. And, you know, my,

00:30:57.620 --> 00:31:02.940
my theory on that is I think we're seeing data science evolving into sort of a profession and a

00:31:02.940 --> 00:31:06.800
professional skill, if that makes sense. So in the same way that like every, you know,

00:31:06.800 --> 00:31:10.840
knowledge workers are always expected to do writing and to know how to write. Yeah.

00:31:10.840 --> 00:31:15.800
but we also hire professional technical writers. I think we're getting into a space where we'll

00:31:15.800 --> 00:31:21.240
have everyone will need to have some numerical literacy and data science skills, even while we

00:31:21.240 --> 00:31:26.700
also employ professional data scientists. Is it the new Excel? Like if I'm, if I'm a manager,

00:31:26.840 --> 00:31:31.560
I, and I don't know how to use Excel, people are going to go, what is wrong with you? Why are you,

00:31:31.560 --> 00:31:35.460
how did you get here? Right. You're going to have to know how to use a spreadsheet. I mean,

00:31:35.460 --> 00:31:39.960
it could be Google sheets or whatever, but something like that to, you know, pull in data,

00:31:39.960 --> 00:31:46.040
sum it up, put it in a graph and so on. And are you feel, are you seeing that more formal data science,

00:31:46.040 --> 00:31:49.300
you know, Jupyter type stuff is kind of edging in on that world.

00:31:49.300 --> 00:31:53.700
Yeah. It's, it's going to, again, I think we'll have to see sort of how the tools settle out.

00:31:53.700 --> 00:31:58.860
one thing I know for sure is that you'll have to at least become familiar with the concept so

00:31:58.860 --> 00:32:03.260
that even if the people doing the data science and reporting to you are using whatever their

00:32:03.260 --> 00:32:08.000
favorite tool set is at least understanding their workflow and how data, you know, goes through that

00:32:08.000 --> 00:32:13.680
life cycle and, you know, data cleaning and modeling and inference and all of those things,

00:32:13.680 --> 00:32:17.660
you'll have to understand that at least enough to interpret what, what is being told and ask the

00:32:17.660 --> 00:32:22.020
right questions about. Right. So if somebody comes to you and says, you asked me this question.

00:32:22.020 --> 00:32:26.960
So I put together a Jupyter notebook that's using PyTorch forecasting. Maybe you can do none of those,

00:32:26.960 --> 00:32:30.840
but you should kind of understand the realm of what that means. Something like that.

00:32:30.840 --> 00:32:34.460
Yes. Yes. You'll have to know at least what steps they had to go through to get to your,

00:32:34.460 --> 00:32:38.160
the answer. So you can ask good questions about, cause if you were a decision maker,

00:32:38.160 --> 00:32:41.780
you need to be able to kind of defend your decision, which means you're going to have to

00:32:41.780 --> 00:32:44.760
at least understand, you know, what went into the inputs into that decision.

00:32:45.180 --> 00:32:50.720
Well, we bought that company cause Jeff over in business analytics said it was a good idea.

00:32:50.720 --> 00:32:55.760
Turned out he, he didn't replace the, not a number section and that really broke it. So

00:32:55.760 --> 00:33:06.180
this portion of talk Python is brought to you by masterworks.io. You have an investment portfolio

00:33:06.180 --> 00:33:10.680
worth more than a hundred thousand dollars. Then this message is for you. There's a $6 trillion

00:33:10.680 --> 00:33:16.940
asset class. That's in almost every billionaire's portfolio. In fact, on average, they allocate more

00:33:16.940 --> 00:33:23.360
than 10% of their overall portfolios to it. It's outperformed the S and P gold and real estate by

00:33:23.360 --> 00:33:29.100
nearly twofold over the last 25 years. And no, it's not cryptocurrency, which many experts don't

00:33:29.100 --> 00:33:35.160
believe is a real asset class. We're talking about contemporary art. Thanks to a startup revolutionizing

00:33:35.160 --> 00:33:41.020
fine art investing, rather than shelling out $20 million to buy an entire Picasso painting yourself,

00:33:41.020 --> 00:33:45.640
you can now invest in a fraction of it. If you realize just how lucrative it can be,

00:33:45.640 --> 00:33:53.000
contemporary art pieces returned 14% on average per year between 1995 and 2020, beating the S and P by

00:33:53.000 --> 00:34:00.860
174%. Masterworks was founded by a serial tech entrepreneur and top 100 art collector. After he

00:34:00.860 --> 00:34:06.480
made millions on art investing personally, he set out to democratize the asset class for everyone,

00:34:06.480 --> 00:34:11.480
including you. Masterworks has been featured in places like the Wall Street Journal, the New York

00:34:11.480 --> 00:34:17.020
Times and Bloomberg. With more than 200,000 members, demand is exploding. But lucky for you,

00:34:17.200 --> 00:34:23.120
Masterworks has hooked me up with 23 passes to skip their extensive waitlist. Just head over to our

00:34:23.120 --> 00:34:29.620
link and secure your spot. Visit talkpython.fm/masterworks or just click the link in your podcast

00:34:29.620 --> 00:34:34.800
player's show notes. And be sure to check out their important disclosures at masterworks.io slash disclaimer.

00:34:37.060 --> 00:34:42.260
I guess one of the requisite topics we should talk about is probably COVID-19 because that was going

00:34:42.260 --> 00:34:46.900
to be over in a few weeks or months, but then it wasn't. So it's still ongoing. And one of the things

00:34:46.900 --> 00:34:53.020
that you all asked about and studied was basically did COVID-19 and more specifically sort of the shutdown

00:34:53.020 --> 00:35:00.900
as a result of it result in more data science, less data science, increased investment, not so much.

00:35:00.900 --> 00:35:02.180
What did you all find there?

00:35:02.180 --> 00:35:08.340
Yeah. So interestingly, I think we found that there was a sort of all different organizations

00:35:08.340 --> 00:35:15.860
had every possible answer. So, you know, the, the, the, about a third decreased investment,

00:35:15.860 --> 00:35:21.360
but a quarter increased investment and another quarter stayed the same. And so that's, you know,

00:35:21.360 --> 00:35:26.860
there wasn't one definitive answer that everyone had for that, which is, I think probably has a lot

00:35:26.860 --> 00:35:30.780
to do with where data science is at in their organization. I mean, on one hand, data

00:35:30.780 --> 00:35:36.840
science is an activity that, is easy to do remotely. you can, you know, there are a lot

00:35:36.840 --> 00:35:41.400
of jobs that you can't do remotely. Data science is one you could do remotely. So that, that part isn't

00:35:41.400 --> 00:35:46.840
an obstacle so much. but is a lot of it also is, has to do with risk. I mean, everyone, when they,

00:35:46.840 --> 00:35:51.440
when they face this was thinking in with their business hats on, what is the risk to my

00:35:51.440 --> 00:35:57.540
organization of an unknown economic impact of this pandemic? And so a lot of places might have

00:35:57.540 --> 00:36:03.140
viewed their data science as being, a risky still early kind of thing. And so let's pull back

00:36:03.140 --> 00:36:07.260
a little bit. Let's not spend that money. Is it optional? Okay. We cancel it for a while. We put

00:36:07.260 --> 00:36:11.760
it on hold. Yeah. Yeah. But, but clearly interesting for, for some organizations, it was so important.

00:36:11.760 --> 00:36:15.900
They put more money in. and so it, it, a lot of it had to do with just where you're at in the

00:36:15.900 --> 00:36:21.340
journey. I think industries, you found out where people were doing data science,

00:36:21.340 --> 00:36:26.920
obviously technology, right? Tech companies. I'm guessing this is like Airbnb, Netflix,

00:36:26.920 --> 00:36:31.360
those kinds of places. There's a lot of data science happening in those worlds. Academic was number two.

00:36:31.360 --> 00:36:38.480
Yeah. I mean, data science is a, is still a actively researched thing. I mean, as we, as you see,

00:36:38.480 --> 00:36:42.160
sometimes it's hard to keep up with all of the new advancements and changes and everything,

00:36:42.160 --> 00:36:47.600
not just in the software, but in techniques. And so academia is super busy on this. you know,

00:36:47.600 --> 00:36:52.940
banking is also a top one because, I kind of think of banking and finance as being some of the,

00:36:52.940 --> 00:36:58.880
you know, the original, you know, corporate data scientists in some ways. and so obviously

00:36:58.880 --> 00:37:03.020
there, it was interesting to see automotive actually score so highly. It's that's, that's the

00:37:03.020 --> 00:37:08.980
one that surprised me as well. Automotive is 6% and the highest industry was 10%. So yeah,

00:37:08.980 --> 00:37:12.500
that's really quite high. Yeah. I wonder how much of that is self-driving cars.

00:37:12.500 --> 00:37:17.020
You know, I don't know that. I mean, the other one is, you know, as we've heard with the chip

00:37:17.020 --> 00:37:22.340
shortages, supply chain logistics is an interesting use of data science to try and predict

00:37:22.340 --> 00:37:26.700
how much supply of all the things you're going to have, where and when, and how should you

00:37:26.700 --> 00:37:32.120
transport stuff. And I imagine car manufacturing is especially, challenging, especially now.

00:37:32.120 --> 00:37:36.620
Interesting. Yeah. They, they really shot themselves in the foot, didn't they? When they said,

00:37:36.620 --> 00:37:40.140
you know what, all these extra chips, people aren't going to need cars. They're not going

00:37:40.140 --> 00:37:44.120
to buy cars during this downturn. So let's cancel our order. We'll just pick it back up in six months.

00:37:44.120 --> 00:37:49.300
And six months later, there are no chips to be had. So, we have it. Yeah. I mean, GM,

00:37:49.300 --> 00:37:53.740
I think it's even shutting down a significant portion of their production in the U S because

00:37:53.740 --> 00:38:00.240
they're just out of chips, which is crazy. Antonio out in the live stream says he's doing

00:38:00.240 --> 00:38:04.680
data science with his team in the energy oil and gas industry. And we're not the only ones.

00:38:05.020 --> 00:38:09.100
Yeah. It's funny that doesn't appear in the list. we, we, we don't have energy, but they're,

00:38:09.100 --> 00:38:14.120
they're, you know, down to 2%. again, all of the percentages are low because there's so many

00:38:14.120 --> 00:38:17.740
industries and everyone was in all, it was all over the place, but yeah.

00:38:17.740 --> 00:38:22.280
Team size is interesting. I think one of the things that it's interesting here is what I think of

00:38:22.280 --> 00:38:29.460
software developers, they kind of cluster together in like development team groups, right? They've got

00:38:29.460 --> 00:38:35.360
the software development department, maybe in a company or a team building a piece of software or

00:38:35.360 --> 00:38:41.040
running a website. To me, data scientists feel like they might be more embedded within little groups.

00:38:41.040 --> 00:38:46.640
There might be a data scientist in the marketing department, a data scientist in the DevOps

00:38:46.640 --> 00:38:53.140
department and so on. is that maybe correct? Yeah. I think we've seen companies actually do both at

00:38:53.140 --> 00:38:56.420
the same time, even where sometimes they'll have, I mean, one of the things we have listed is a data

00:38:56.420 --> 00:39:01.620
science center of excellence. and, and what that ends up being is a, some sense, a group that

00:39:01.620 --> 00:39:05.400
is pathfinding for an organization. They're saying, okay, these are the best practices. These are the

00:39:05.400 --> 00:39:10.020
tools. This is what to do, figuring that out and then rolling it out to all the departments who have

00:39:10.020 --> 00:39:14.300
their embedded data scientists who can take advantage of that. cause I think it's valuable to have a

00:39:14.300 --> 00:39:18.080
data scientist embedded in the department because one of the most important things as a data scientist

00:39:18.080 --> 00:39:23.260
is your understanding of the data you're analyzing and your familiarity with it. that I would,

00:39:23.340 --> 00:39:28.740
I would really prefer the person analyzing, you know, car supply chains, understand what goes into

00:39:28.740 --> 00:39:33.400
that and also no data science as opposed to a data scientist for whom it's all numbers and they don't

00:39:33.400 --> 00:39:40.440
know. Right. If you could trade absolute expertise in Git versus really good understanding of the problem

00:39:40.440 --> 00:39:44.360
domain, you're probably better off going, you know what, just keep zipping it up and just really answer

00:39:44.360 --> 00:39:49.240
these questions. Well, I mean, you don't actually have to make that trade off, but I agree that domain

00:39:49.240 --> 00:39:55.340
knowledge is more important here. Yeah. So it had the highest, so think of the departments where

00:39:55.340 --> 00:40:01.880
data scientists live. It was pretty high than R and D and then this data center, center of excellence

00:40:01.880 --> 00:40:07.760
you spoke about, then ops finance, administration, marketing, human resources. It's really spread out,

00:40:07.760 --> 00:40:12.600
which is sort of what I was getting at before. Yeah. Yeah. So, so I think there are a lot of,

00:40:12.600 --> 00:40:17.000
uh, seeing a lot of organizations build their data science expertise, ground up department by

00:40:17.000 --> 00:40:22.020
department and then maybe we'll coalesce some of it into, you know, a single, single department

00:40:22.020 --> 00:40:26.440
at some point. Right. Maybe that department makes like the APIs for the rest of the sort of isolated

00:40:26.440 --> 00:40:31.220
folks and so on. one that was interesting is how do you spend your time? I mean, you think about

00:40:31.220 --> 00:40:37.120
these AI models or these plotly graphs and all these things that data scientists produce. Then there's the

00:40:37.120 --> 00:40:43.160
quote that data cleaning is not the grunge work. It is the work, right? And you sort of have this chart

00:40:43.160 --> 00:40:49.300
of like, how do you spend your time? And 22% is data preparation, 17% on top of that is data cleaning.

00:40:49.300 --> 00:40:54.740
And so, yeah, that's pretty significant portion of just getting ready to ask questions.

00:40:54.740 --> 00:40:59.380
Yeah. And that's, and that really, that that's the piece that requires that domain expertise to know

00:40:59.380 --> 00:41:04.340
what you're looking at, what's relevant, what problems it'll have. No data set is perfect and,

00:41:04.340 --> 00:41:10.140
and, no data set is perfect for all questions. And so, even if, you know,

00:41:10.180 --> 00:41:13.920
you can't ever clean the data just once, cause what you're doing is preparing it for the questions

00:41:13.920 --> 00:41:18.480
you're going to ask. And so you need someone who can, you know, understand what's going to happen

00:41:18.480 --> 00:41:22.840
there and do that. And that's what, that's really the expertise you want. Yeah. Cool. Another topic

00:41:22.840 --> 00:41:28.740
you asked about was, barriers to going to production. So, some pretty intense graphs,

00:41:28.740 --> 00:41:35.180
many, many options across many categories, but basically you asked, what are the roadblocks do you

00:41:35.180 --> 00:41:39.560
face when moving your models to a production environment? The, you know, intense graphs are really

00:41:39.560 --> 00:41:43.740
that everyone has a slightly different perception of this depending on what seat they're in.

00:41:43.740 --> 00:41:48.340
Are they, are they the analyst? Are they the data scientist? Are they the DevOps person? Everyone

00:41:48.340 --> 00:41:53.140
has a different answer for what the roadblocks are. right. And, and which is makes sense because

00:41:53.140 --> 00:41:57.460
you're going to see what is relevant to your job. when you, when you sum everyone up, you,

00:41:57.460 --> 00:42:04.040
you kind of sort of see this even split across it security. Honestly, what I found interesting was that

00:42:04.040 --> 00:42:09.880
there was both converting your model from Python or R into another language and also converting from

00:42:09.880 --> 00:42:17.500
another language into Python and R. Yeah, exactly. So one of the challenges that people had was just

00:42:17.500 --> 00:42:22.940
like you said, recoding models from Python or R into another language and then the exact reverse.

00:42:23.220 --> 00:42:27.560
And they were almost exactly tied. 24% of the people said, Oh, I got to convert these Python

00:42:27.560 --> 00:42:32.340
models to Java or whatever. The other people are like, he's got this Java model. I got to get into

00:42:32.340 --> 00:42:36.040
Python so I can put it in FastAPI on the web. Right. Something like that.

00:42:36.040 --> 00:42:40.520
Yeah. Anecdotally. I mean, I think the, the, the, you know, maybe we'll have to change the phrasing

00:42:40.520 --> 00:42:45.380
of this question in the future because putting Python and R together might have, conflated a

00:42:45.380 --> 00:42:50.520
couple of things potentially. cause so I just know anecdotal evidence. you know, we have

00:42:50.520 --> 00:42:54.880
talked to customers who their data scientists wrote everything in R, but they didn't want to put R in

00:42:54.880 --> 00:43:00.440
production and we're asking them to recode it into Python because Python was okay for production.

00:43:00.440 --> 00:43:04.340
but I've also had the conversation. People are like, we don't have our data modeling in Python

00:43:04.340 --> 00:43:10.620
and Python's not okay for production. Java is okay for production. and, and so it's, it's this weird

00:43:10.620 --> 00:43:15.880
problem of companies have built up how they do deployments on specific languages. And those aren't

00:43:15.880 --> 00:43:20.400
the languages that people are doing data science in all the time. Right. And I suspect in the Java

00:43:20.400 --> 00:43:26.300
one, it's just like, we have a bunch of Java APIs and apps running and those people that do that stuff,

00:43:26.300 --> 00:43:30.320
they run those apps and you're going to give us a model that's just going to fit into that world.

00:43:30.780 --> 00:43:34.960
But if you are already running Python for your web servers, just put it in production. It's,

00:43:34.960 --> 00:43:40.400
it's already right there, right? Yep. Yep. Yep. Yeah. Yeah. Quite interesting. Okay.

00:43:40.400 --> 00:43:45.840
let's see. I'll flip through here and find a couple more. one was interesting. It was about open

00:43:45.840 --> 00:43:50.620
source, enterprise adoption of open source. yeah, you may want to speak to the results there.

00:43:50.620 --> 00:43:54.500
Yeah. I wish we could have asked this question 10 years ago, cause I think it would have been

00:43:54.500 --> 00:43:59.060
fascinating to compare to now. Yeah. yeah. It's the trend that's super interesting. Yeah.

00:43:59.720 --> 00:44:03.540
The, you know, the, one of the surprising things for me was the outcome that said,

00:44:03.540 --> 00:44:09.680
uh, well, less surprising was 87% of organizations said that they allow the use of open source inside

00:44:09.680 --> 00:44:14.240
the organization. I think that's not too surprising. I mean, even just Linux is kind of like this sort

00:44:14.240 --> 00:44:19.360
of baseline. How is your organization functioning without Linux? Yeah. and then almost what

00:44:19.360 --> 00:44:24.640
programming language could you choose these days? That's not open source, right? You know, the,

00:44:25.020 --> 00:44:31.120
you've got Java, you've got.net, like especially.net was one that wasn't open source is pretty

00:44:31.120 --> 00:44:35.660
popular. Like too late. That's all open source and installed through package managers now. And then

00:44:35.660 --> 00:44:41.400
then the move to Python. And yeah, I mean, I can hardly think of a language or a place to run where

00:44:41.400 --> 00:44:46.000
you can't use some level of open source. Yeah. But the second question, which was,

00:44:46.000 --> 00:44:51.600
does your employer encourage you to contribute to open source? I was surprised to see 65% said,

00:44:51.700 --> 00:44:57.580
yes, that is a, a huge fraction and, is interesting because, that has not always

00:44:57.580 --> 00:45:02.580
been that high. I know that we have spoken again to, you know, people who have said, Hey, you know,

00:45:02.580 --> 00:45:07.500
my, I wish I could contribute, but my employer, we just don't have a policy for this or we don't have

00:45:07.500 --> 00:45:11.900
a way to do that. Yeah. I used to hear that a lot, right. That it's just, it's, it's too complicated.

00:45:11.900 --> 00:45:19.180
I might leak something out. yeah. Or bring in some GPL stuff and mess up our commercial product

00:45:19.180 --> 00:45:24.520
or whatever. Right. Yeah. So I don't know how all these companies have, have solved that internally,

00:45:24.520 --> 00:45:29.220
but I am excited to see, that there's now a huge potential base of open source contributors

00:45:29.220 --> 00:45:34.080
out there that, commercially that there wasn't before. I do think there's something about creating

00:45:34.080 --> 00:45:39.340
a culture for software developers and data scientists where they want to be. And people don't want to be

00:45:39.340 --> 00:45:44.700
in a place where they're forced to use just proprietary tools that are old and crusty, and they're not

00:45:44.700 --> 00:45:48.180
allowed to share their work or talk about their work. And, you know, there's people who would do

00:45:48.180 --> 00:45:52.480
that, but as a, I would love to be in that environment. Like that's not that feeling and,

00:45:52.480 --> 00:45:56.800
you know, talent's hard to come by. So you, you will probably create environments that attract

00:45:56.800 --> 00:46:02.100
the best developers and the best developers don't want to be locked in a basement told they can't share

00:46:02.100 --> 00:46:07.780
or contribute to anything. Yeah. Yeah. I definitely agree with that. Another thing that's hot these days,

00:46:07.780 --> 00:46:16.940
hot in the, as you don't want it, but it's a very hot potato style is, supply chain stuff and open

00:46:16.940 --> 00:46:21.860
source pipeline issues. Right. And the survey actually mentioned that one of the problems that

00:46:21.860 --> 00:46:26.280
people mentioned, one of the reasons that they don't want to use open source is they believed it

00:46:26.280 --> 00:46:33.360
was insecure because our $20 billion bank is now depending on, you know, this project from Sarah

00:46:33.360 --> 00:46:39.040
about padding numbers or whatever, right? Like if somebody takes over a thing, we're going to pip

00:46:39.040 --> 00:46:43.020
install a virus into the core trading engine. That's going to be bad, right? Like that's an extreme

00:46:43.020 --> 00:46:48.280
example, but you did ask about what people are doing to secure their, basically the code they're

00:46:48.280 --> 00:46:52.380
acquiring through open source. Yeah. And this is something, I mean, we're interested in just

00:46:52.380 --> 00:46:56.760
generally because there's a lot more focus on security and you see more reports about supply chain

00:46:56.760 --> 00:47:01.100
attacks on software. And so we're curious how different organizations are tackling the problem.

00:47:01.520 --> 00:47:06.800
uh, obviously the most unsurprisingly, the most, popular answer at 45% was they use a

00:47:06.800 --> 00:47:11.100
managed repository, which interpret to mean, basically it's kind of like you have a private

00:47:11.100 --> 00:47:15.500
mirror of the packages that are approved in your organization and everyone pulls from there,

00:47:15.500 --> 00:47:21.100
not from the internet directly. which is a, a, a smart approach because it gives you a natural

00:47:21.100 --> 00:47:26.500
sort of gating, thing that you can do where there is an, there is a review process to bring new

00:47:26.500 --> 00:47:31.840
software in there. and, and so there's a lot of, you know, things here. I mean,

00:47:31.840 --> 00:47:37.520
obviously even commercially, we sell a repository for condo packages, for precisely this reason,

00:47:37.520 --> 00:47:44.540
uh, because, customers want some governance and are more than happy to, pay us. Yeah.

00:47:44.540 --> 00:47:51.160
Team edition, is our on package, repository. and so this is a, this was an ask for customers,

00:47:51.160 --> 00:47:55.100
which is why we, we built this product, is they were like, Hey, we want your stuff, but we want

00:47:55.100 --> 00:48:00.440
it inside our firewall. We don't want to go directly to your public repo. You want to opt in to say,

00:48:00.440 --> 00:48:06.680
yes, we want the new numpy, not just, Oh, somebody randomly pushed them, pushed something out. And so

00:48:06.680 --> 00:48:11.040
we're going to just grab it and assume that it's good. Right. You can apply policies as well. That's

00:48:11.040 --> 00:48:16.400
common as a lot of places will say no GPL software for various reasons. or they might say, Oh,

00:48:16.400 --> 00:48:21.860
you know, if there are reported, you know, CVEs, these security, reports that, you know,

00:48:21.860 --> 00:48:27.960
go through NIST, they might say, I want no packages with a CVE more severe than some level.

00:48:27.960 --> 00:48:34.160
and those, the, every IT department wants some, some handles to control that kind of policy,

00:48:34.160 --> 00:48:39.140
decision-making. And so, yeah, so that's obviously that, that I think that's why that's the most popular

00:48:39.140 --> 00:48:43.980
option is it's the easiest, thing to get a handle on. It is. Yeah. You can set up a private

00:48:43.980 --> 00:48:49.900
PI PI server. Yep. Pretty straightforward. there's a cool article on testdriven.io,

00:48:49.900 --> 00:48:54.460
but yeah, the Conda and the Conda version that you all offer. That's pretty cool.

00:48:54.460 --> 00:49:02.280
45% as high. I didn't expect that many companies to have a private repository. It's good, but I don't,

00:49:02.280 --> 00:49:07.100
I just expected it to be, I don't know, lower. Yeah. I, although on the other side, you know,

00:49:07.100 --> 00:49:13.300
that means 55% of those were just downloading random stuff from the internet. So, so it's good. I think

00:49:13.300 --> 00:49:16.520
the message is getting out that you have to think about these things from a risk perspective.

00:49:16.520 --> 00:49:22.420
Another was 33% of the organizations do manual checks against a vulnerability database.

00:49:22.420 --> 00:49:28.480
Yeah. So this is, what I was describing earlier. The CVE databases are often a common,

00:49:28.480 --> 00:49:34.080
uh, vulnerability, manual checks. That's a lot of labor. so I, I, it'll be interesting to,

00:49:34.080 --> 00:49:39.660
um, see how many places move to automating that in some fashion in order to, the hard part there

00:49:39.660 --> 00:49:45.740
is those databases have, again, to data prep and data cleaning often to make use

00:49:45.740 --> 00:49:49.660
of those public databases. You need to do some amount of curation because there's a lot of stuff

00:49:49.660 --> 00:49:55.140
that ends up in there that's mistagged or unclear or whatever. and so a lot of this manual checking

00:49:55.140 --> 00:49:59.700
is probably also just doing that curation. One of the things that's nice. Yeah. One of the things that's

00:49:59.700 --> 00:50:05.220
nice is, GitHub will now do automatic PRs for security problems that it knows about at least.

00:50:05.220 --> 00:50:09.140
Yeah. Those, that kind of automation is going to be really, important, I think in the future,

00:50:09.140 --> 00:50:11.660
just because you can't manually go through all those things.

00:50:11.660 --> 00:50:17.420
What are you seeing around source control? You know, source code algorithms, these are

00:50:17.420 --> 00:50:22.860
really important and people want to keep them super secure, but if they put them on their own private

00:50:22.860 --> 00:50:28.380
source code repositories, they lose a lot of benefits like automatic vulnerability checking and stuff like

00:50:28.380 --> 00:50:34.860
that. What's the GitHub or GitLab versus other stuff, maybe enterprise GitHub. What's the trends there?

00:50:34.860 --> 00:50:39.180
The, the interesting thing there is, is yeah. you know, everyone is using source control at

00:50:39.180 --> 00:50:43.660
some point and they oftentimes they want it managed inside their firewall. And so yeah, things like

00:50:43.660 --> 00:50:48.460
GitHub enterprise and things and GitLab are pretty popular for that. a lot of times I think what

00:50:48.460 --> 00:50:53.900
a places will do is they'll use, some kind of the next item here, the 30% said they're

00:50:53.900 --> 00:50:58.700
using a vulnerability scanner. A lot of those vulnerability scanners you can use on your own internal source

00:50:58.700 --> 00:51:04.540
repositories. And so that way they're, they're not taking advantage of GitHub automatically doing that for them,

00:51:04.540 --> 00:51:08.220
but, they at least have some solution probably for looking for stuff.

00:51:08.220 --> 00:51:14.060
20% said they have no idea what they're doing. And then another 20% said we're not doing anything.

00:51:14.060 --> 00:51:22.940
Well, I'm sure of it. Let's maybe close out this overview of the survey results here by talking about

00:51:22.940 --> 00:51:29.900
Python, Python's popularity. Is it growing? Is it shrinking? Is everyone switching to Julia or have

00:51:29.900 --> 00:51:34.220
they all gone to go? What are they doing? Yeah. So I think, I think Python's

00:51:34.220 --> 00:51:39.420
advantage here is being a, pretty good at a lot of things. And so it ends up being a natural

00:51:39.420 --> 00:51:45.180
meeting point of people who are interested in, you know, web development and data science or system

00:51:45.180 --> 00:51:49.900
administration automation and all of that. So I think, I think Python still has some, some growth to go,

00:51:49.900 --> 00:51:53.740
but I mean, what's interesting is, is, you know, in our survey, the second, I would say the second

00:51:53.740 --> 00:51:58.460
most popular, was SQL, which has been around forever and is going nowhere.

00:51:58.460 --> 00:52:01.900
Those are often used. Yeah, exactly. And they're often used in parallel, right? Like,

00:52:01.900 --> 00:52:07.020
yeah, I'm going to do a SQL query and then run some Python code against the results, that type of thing.

00:52:07.020 --> 00:52:11.420
Yeah. Yeah, definitely. I, I'm a big believer in that there is no one language for everything and

00:52:11.420 --> 00:52:17.100
there never will be. but there is, you know, a lot of different options that people are

00:52:17.100 --> 00:52:21.580
looking to. I mean, go make sense for a lot of sort of network service kind of things. I mean,

00:52:21.580 --> 00:52:27.180
Kubernetes is built almost entirely out of go. but, I'm not sure if I'd want to do any data

00:52:27.180 --> 00:52:33.180
science and go at this point. and so it's going to always be a mix. It might not even be that

00:52:33.180 --> 00:52:38.540
you're doing one or the other. You might be doing both. Like for example, maybe you've written some core

00:52:38.540 --> 00:52:44.460
engine and rust, but then you wrap it in Python to program against it. Right. It could be both.

00:52:44.460 --> 00:52:49.260
I guess it could even be a more combination than that, but, yeah, the popularity of Python looks,

00:52:49.260 --> 00:52:55.500
looks strong. So it looks like it's still right up near the top. I mean, obviously the group that you

00:52:55.500 --> 00:53:00.620
pulled is somewhat self-selecting, right. But that's still a general trend outside of your space.

00:53:00.620 --> 00:53:03.500
Yeah. Yeah. This is definitely going to be skewed to Python because otherwise,

00:53:03.500 --> 00:53:08.060
why are you taking an anaconda survey? But, but still I think, yeah,

00:53:08.060 --> 00:53:10.780
it is definitely something you see broadly in the industry as well.

00:53:10.780 --> 00:53:13.900
Well, speaking of a different languages and stuff out in the live stream,

00:53:13.900 --> 00:53:19.980
Alexander Semenov says, just learned that I can use rust in JupyterLab with some help from Anaconda.

00:53:19.980 --> 00:53:21.420
My mind is blown. Good job.

00:53:21.420 --> 00:53:26.140
Yeah. That's the one thing I should mention about Python is one of the advantages is if you're using

00:53:26.140 --> 00:53:30.620
Python, you're probably benefiting from most of the languages on the stack, even if you're not writing

00:53:30.620 --> 00:53:35.580
them. And so the ability of Python to connect to anything is I think it's strength and why it

00:53:35.580 --> 00:53:42.540
continues to top these lists. Yeah, absolutely. And then Paul out there has a question about the

00:53:43.340 --> 00:53:47.020
commercial license. And I guess there was some changes to it. Can you maybe speak to that? I

00:53:47.020 --> 00:53:49.420
don't really track the details well enough to say much.

00:53:49.420 --> 00:53:57.180
Yeah. So what, what we did was, our, the Anaconda distribution packages have a,

00:53:57.180 --> 00:54:01.180
terms of service that says, if you are in an organization above a certain size, we want

00:54:01.180 --> 00:54:05.500
you to have a commercial license if you're using it in your business. I forgot the exact threshold,

00:54:05.500 --> 00:54:11.500
uh, where that's at. and, and the reason there was to help one support the development of

00:54:11.500 --> 00:54:15.900
those packages. And I should say, by the way, that terms of service does not apply to Condo Forge.

00:54:15.900 --> 00:54:20.140
Obviously those are community packages. but if you, if you want the assurances that

00:54:20.140 --> 00:54:23.500
Anaconda is providing on those packages and you are a company of a certain size,

00:54:23.500 --> 00:54:28.780
uh, we would like you to have a commercial license, that allows us to support you more directly.

00:54:28.780 --> 00:54:33.900
It allows us to fund, continued work on those packages. And that's, that's sort of, it was,

00:54:33.900 --> 00:54:39.580
it's a sustainability thing, I think. but it, it's, for most people, it's not an issue.

00:54:39.580 --> 00:54:42.220
cause they're either below that size or you're just using it individually.

00:54:42.220 --> 00:54:44.300
Do you know what that size is? What that cutoff is?

00:54:44.300 --> 00:54:47.340
I do not recall off the top of my head. And so I'm afraid to quote a number.

00:54:47.340 --> 00:54:52.140
Yeah. Yeah. Sure. No, no worries. Cool. All right. Well, thanks for giving us that. I mean,

00:54:52.140 --> 00:54:58.380
it seems fair that large companies benefiting from your tools contribute back. I think that statement

00:54:58.380 --> 00:55:04.460
should be applied to open source in general. If, if your company is built on Python, you should give back

00:55:04.460 --> 00:55:08.780
to the Python space. If your company is built on Java, it's Oracle. I don't know if they need help,

00:55:08.780 --> 00:55:13.180
but you know, in general, if you're built on top of something, there's a lot of support you can give

00:55:13.180 --> 00:55:17.820
back. Right. It's, it's kind of insane to me that, you know, banks that are worth many, many billions

00:55:17.820 --> 00:55:24.860
of dollars do very little in terms of like directly supporting the people who they're built upon. Right.

00:55:24.860 --> 00:55:31.340
they hire a pay for a couple of people building the core libraries. Like if you're using Flask,

00:55:31.340 --> 00:55:34.940
right. Support the Flask, pallets organization, something like that.

00:55:34.940 --> 00:55:38.780
Yeah. And then we in turn, you know, take that licensing money and some fraction of it goes to

00:55:38.780 --> 00:55:43.100
num focus for, the broader sort of data science open source community. In addition to,

00:55:43.100 --> 00:55:45.580
you know, us directly funding some open source projects as well.

00:55:45.580 --> 00:55:50.060
All right. Well, we're about out of time, Stan, but let's talk real quickly about Piston because

00:55:50.060 --> 00:55:59.100
Piston is not, rewriting Python and rust. It's not replacing it with Cython or just moving to go.

00:55:59.100 --> 00:56:01.900
It's, it's about making core Python faster, right?

00:56:01.900 --> 00:56:06.620
Yeah, this is something, I mean, we've been thinking about, performance in Python for a

00:56:06.620 --> 00:56:12.380
long time. one of the early projects that, you know, Anaconda created is called number. It's a

00:56:12.380 --> 00:56:18.380
Python compiler. It's focused on numerical use cases and it really is, does its best job, in

00:56:18.380 --> 00:56:23.820
dealing with that kind of numerical loop heavy code. but it's not a, it's not going to optimize your

00:56:23.820 --> 00:56:28.620
entire program, but optimize a specific functions. And so number has is very good at a very specific

00:56:28.620 --> 00:56:33.100
thing. And so we've been thinking for a long time about how we could broaden our impact. And so when

00:56:33.100 --> 00:56:38.780
I saw that, Piston, which I, you know, among many pilots on compiler projects had reemerged in

00:56:38.780 --> 00:56:45.100
2020, with a new version written from scratch, based on Python 3.8, as a just in time

00:56:45.100 --> 00:56:51.100
compiler in the interpreter. So it's designed to optimize any Python program. it can't necessarily do any

00:56:51.100 --> 00:56:56.620
given thing, as fast as number might be for a specific, you know, numerical algorithm, but the

00:56:56.620 --> 00:57:01.500
breadth is, is really what, is interesting to us. and so I saw this project had emerged,

00:57:01.500 --> 00:57:05.660
Piston 2.0 kind of came on the scene. I started looking more closely at it and we started talking

00:57:05.660 --> 00:57:10.220
with them. and we realized that there's a lot that I think the Piston Anaconda could do

00:57:10.220 --> 00:57:15.980
together. And so, we, have hired the Piston team on, to our open source group.

00:57:15.980 --> 00:57:20.300
So they are funded to work on Piston the same way we fund open source developers to work on other

00:57:20.300 --> 00:57:25.740
projects. and so we're really, but the benefit that, there's other, help we can

00:57:25.740 --> 00:57:29.660
give and resources and infrastructure that we can offer this project. And so we're really excited to

00:57:29.660 --> 00:57:33.660
see where this is going to go from here. Yeah. I'm excited as well. All these different things that

00:57:33.660 --> 00:57:39.820
people are doing to make Python faster for everyone, not just, well, let's try to recompile this loop,

00:57:39.820 --> 00:57:44.780
but just you run Python and it just goes better. I think that's pretty exciting. You know, we've got

00:57:44.780 --> 00:57:51.980
the cinder projects from Facebook. Yeah. This is a really good year for Python, optimization projects.

00:57:51.980 --> 00:57:58.140
I should be careful about typing that into a search engine, but, but the cinder project is,

00:57:58.140 --> 00:58:03.420
is not something that's publicly available really. it's not like a supported improvement,

00:58:03.420 --> 00:58:07.580
but it's a, here's what they did at Instagram. There's a bunch of speed ups. Maybe you all can

00:58:07.580 --> 00:58:12.060
bring some of that back into regular Python, but yeah, it's, there's a lot of these types of ideas.

00:58:12.060 --> 00:58:14.780
And yeah, awesome. Looking forward to see what you'll do with this.

00:58:14.780 --> 00:58:20.300
And, you know, the cPython core developers, have even announced that they're going to,

00:58:20.300 --> 00:58:25.340
you know, undertaking a new effort to speed up cPython. and so we will, we're looking to collaborate

00:58:25.340 --> 00:58:29.820
with them. they, they're going to have to, you know, figure out how, what they can do within

00:58:29.820 --> 00:58:35.420
the confines of cPython, because you are the Python interpreter for the world.

00:58:35.420 --> 00:58:35.980
Yeah.

00:58:35.980 --> 00:58:39.340
And so you need, you need to be careful, but there's a lot they're going to do. And we're

00:58:39.340 --> 00:58:43.900
going to try and share ideas as much as we can. because these are both open source projects.

00:58:43.900 --> 00:58:48.540
Right. A lot of the challenges have been in compatibility, right? Like, oh, we could do this,

00:58:48.540 --> 00:58:54.380
but then C extensions don't work. And those are also important for performance in, in big ways and

00:58:54.380 --> 00:58:58.620
other stuff, but yeah, so they do have to be careful, but that's great. All right. Final comment,

00:58:58.620 --> 00:59:04.780
real quick follow-up from Paul. I'd like my company to do more open source, more to do more to support

00:59:04.780 --> 00:59:10.060
open source. Any advice on promoting that? Yeah. I think the best, first place to start

00:59:10.060 --> 00:59:15.580
is identifying what open source does your company absolutely rely on. and especially if you can

00:59:15.580 --> 00:59:19.100
find an open source project that you absolutely rely on, that doesn't seem to be getting a lot

00:59:19.100 --> 00:59:24.300
of support, and then go look at those projects and see what are they, you know, do they have an

00:59:24.300 --> 00:59:30.060
established way to donate funds? do they have, you know, other needs? that's something I

00:59:30.060 --> 00:59:34.780
think that is easier to sell as you say, look, our organization absolutely depends on X, whatever

00:59:34.780 --> 00:59:39.500
this is, as opposed to picking a project at random. it's easier to show a specific business

00:59:39.500 --> 00:59:43.500
speed. Yeah. Yeah, for sure. You can say, look, this is the core thing that we do and it's built

00:59:43.500 --> 00:59:47.100
on this rather than, oh, here's some projects I ran across. We should give some of our money away.

00:59:47.100 --> 00:59:52.540
Yeah. That's a hard, harder sell to, stockholders, I guess. All right. Well,

00:59:52.540 --> 00:59:56.300
Stan, this has been really fun. Let me ask you the final two questions before we get out of here.

00:59:56.300 --> 00:59:59.980
if you're going to write some Python code, what editor do you use?

00:59:59.980 --> 01:00:05.820
So if I'm on, if I'm on a terminal, it's Emacs. if I have an actual GUI desktop,

01:00:05.820 --> 01:00:10.780
I'm usually using VS Code these days. And then notable PI PI package or conda package that you're like,

01:00:10.780 --> 01:00:13.420
oh, this thing is awesome. People should know about whatever.

01:00:13.420 --> 01:00:18.300
Yeah. you know, wearing my, my GPU fan hat. I think a lot more people should know about

01:00:18.300 --> 01:00:24.220
CUPI. C U P Y it's a, Python package. That's basically if you took NumPy, but made it run on the

01:00:24.220 --> 01:00:30.380
GPU. it's a, the easiest way I can think of to get started in GPU computing, because it just uses

01:00:30.380 --> 01:00:35.500
NumPy calls that you're familiar with. so I would highly recommend if you are at all curious about

01:00:35.500 --> 01:00:40.940
GPU computing, go check out Coupy. So over there on that computer I have over there, it has a G force,

01:00:40.940 --> 01:00:47.820
but on this one, it obviously doesn't have Nvidia on my Mac. does that work? Cuda cores,

01:00:47.820 --> 01:00:54.460
the CU part of that is for the Nvidia bits, right? What's my GPU story. If I don't have Nvidia on my

01:00:54.460 --> 01:01:00.700
machine, not as clear. yeah, there, the, you know, CUDA has kind of come to dominate the space,

01:01:00.700 --> 01:01:06.060
um, being sort of, first out of the gate, the, there's a lot more Python projects for CUDA.

01:01:06.060 --> 01:01:12.220
I'm, there are not, really clear choices, I think for AMD or for like, you know, built in GPUs,

01:01:12.220 --> 01:01:17.100
uh, at this point. although I've definitely watched the space, you know, Intel is coming

01:01:17.100 --> 01:01:22.140
out with their own GPUs, sort of this year and starting next year. and they have been

01:01:22.140 --> 01:01:26.940
collaborating with various open source projects, including the number project, to build Python

01:01:26.940 --> 01:01:33.820
tools to run on Intel GPUs, both embedded and discrete. So, yeah. Okay. So this may change

01:01:33.820 --> 01:01:37.500
in the future. It'll be interesting to see. Final call to action. People are excited about,

01:01:37.500 --> 01:01:41.500
you know, digging more into these results and learning more about the state of the industry.

01:01:41.500 --> 01:01:44.700
What do they do? go search for a state of data science,

01:01:44.700 --> 01:01:49.500
Anaconda, and you'll find the results of the survey. I would, there's a lot of detail in there. So I would

01:01:49.500 --> 01:01:53.420
definitely go through and take a look at all of the charts and things. Cause there's a,

01:01:53.420 --> 01:01:58.060
there's all kinds of topics covered in there. Yeah. I think it's 46 pages or something. And we

01:01:58.060 --> 01:02:02.460
just covered some of the highlights. So absolutely. All right, Stan. Well, thank you for being here.

01:02:02.460 --> 01:02:05.100
It's been great to chat with you. Thanks. It's been great. You bet.

01:02:05.100 --> 01:02:11.500
This has been another episode of talk Python to me. Our guest on this episode was Stan Siebert,

01:02:11.500 --> 01:02:16.460
and it's been brought to you by shortcut masterworks.io and the transcripts were brought to you by

01:02:16.460 --> 01:02:23.100
assembly AI. Choose shortcut, formerly clubhouse IO for tracking all of your projects work because you

01:02:23.100 --> 01:02:29.500
shouldn't have to project manage your project management. Visit talkpython.fm shortcut. Make

01:02:29.500 --> 01:02:35.420
contemporary art your investment portfolio's unfair advantage. With masterworks, you can invest in

01:02:35.420 --> 01:02:41.900
fractional works of fine art. Visit talkpython.fm/masterworks. Do you need a great automatic

01:02:41.900 --> 01:02:47.420
speech to text API? Get human level accuracy in just a few lines of code. Visit talkpython.fm slash

01:02:47.420 --> 01:02:53.580
assembly AI. Want to level up your Python? We have one of the largest catalogs of Python video courses over

01:02:53.580 --> 01:02:58.700
at talkpython. Our content ranges from true beginners to deeply advanced topics like memory

01:02:58.700 --> 01:03:05.340
and async. And best of all, there's not a subscription in sight. Check it out for yourself at training.talkpython.fm.

01:03:05.340 --> 01:03:10.380
Be sure to subscribe to the show, open your favorite podcast app, and search for Python. We should be

01:03:10.380 --> 01:03:16.060
right at the top. You can also find the iTunes feed at /itunes, the Google Play feed at /play,

01:03:16.060 --> 01:03:23.500
and the direct RSS feed at /rss on talkpython.fm. We're live streaming most of our recordings these

01:03:23.500 --> 01:03:28.220
days. If you want to be part of the show and have your comments featured on the air, be sure to subscribe

01:03:28.220 --> 01:03:34.460
to our YouTube channel at talkpython.fm/youtube. This is your host, Michael Kennedy. Thanks so much for

01:03:34.460 --> 01:03:50.380
listening. I really appreciate it. Now get out there and write some Python code.

01:03:50.380 --> 01:03:58.700
I really appreciate it. Now get out there and write some Python code. And I'll see you next time.

