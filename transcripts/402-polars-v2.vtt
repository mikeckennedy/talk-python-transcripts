WEBVTT

00:00:00.001 --> 00:00:04.740
When you think about processing tabular data in Python, what library comes to mind?

00:00:04.740 --> 00:00:05.940
Pandas, I'd guess.

00:00:05.940 --> 00:00:10.720
But there are other libraries out there, and Polar's is one of the more exciting new ones.

00:00:10.720 --> 00:00:17.080
It's built in Rust, embraces parallelism, and can be 10 to 20 times faster than Pandas out of the box.

00:00:17.080 --> 00:00:23.060
We have Polar's creator, Richie Vink, here to give us a look at this exciting new data frame library.

00:00:23.060 --> 00:00:29.500
This is Talk Python to Me, episode 402, recorded January 29, 2023.

00:00:29.500 --> 00:00:46.580
Welcome to Talk Python to Me, a weekly podcast on Python.

00:00:46.580 --> 00:00:48.320
This is your host, Michael Kennedy.

00:00:48.320 --> 00:00:55.800
Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython, both on fosstodon.org.

00:00:55.800 --> 00:00:58.400
Be careful with impersonating accounts on other instances.

00:00:58.400 --> 00:00:59.360
There are many.

00:00:59.820 --> 00:01:04.420
Keep up with the show and listen to over seven years of past episodes at talkpython.fm.

00:01:04.420 --> 00:01:08.420
We've started streaming most of our episodes live on YouTube.

00:01:08.420 --> 00:01:16.000
Subscribe to our YouTube channel over at talkpython.fm/youtube to get notified about upcoming shows and be part of that episode.

00:01:17.140 --> 00:01:19.260
This episode is brought to you by TypePy.

00:01:19.260 --> 00:01:26.980
TypePy is here to take on the challenge of rapidly transforming a bare algorithm in Python into a full-fledged decision support system for end users.

00:01:26.980 --> 00:01:31.620
Check them out at talkpython.fm/taipy, T-A-I-P-Y.

00:01:31.620 --> 00:01:34.200
And it's also brought to you by User Interviews.

00:01:34.200 --> 00:01:38.380
Earn extra income for sharing your software developer opinion.

00:01:38.380 --> 00:01:42.840
Head over to talkpython.fm/userinterviews to participate today.

00:01:43.660 --> 00:01:44.540
Hey, Richie.

00:01:44.540 --> 00:01:46.140
Welcome to Talk Python to me.

00:01:46.140 --> 00:01:46.640
Hey, Michael.

00:01:46.640 --> 00:01:47.540
Thanks for having me.

00:01:47.540 --> 00:01:48.280
Great to be here.

00:01:48.280 --> 00:01:52.320
I feel like maybe I should rename my podcast TalkRust to me or something.

00:01:52.320 --> 00:01:52.780
I don't know.

00:01:52.780 --> 00:01:54.020
Rust is taking over.

00:01:54.020 --> 00:01:55.200
Yeah.

00:01:55.200 --> 00:01:59.520
As the low-level part of how do we make Python go fast?

00:01:59.520 --> 00:02:01.580
There's some kind of synergy with Rust.

00:02:01.580 --> 00:02:02.480
What's going on there?

00:02:02.480 --> 00:02:03.060
Yeah, there is.

00:02:03.060 --> 00:02:09.760
I'd say Python already was low-level languages that succeeded, that made Python a success.

00:02:09.760 --> 00:02:16.820
I mean, like NumPy, Pandas, everything that was reasonable fast was so because of C or Cyton, which is also C.

00:02:16.820 --> 00:02:23.800
But Rust, different from C, Rust has made low-level programming a lot more fun to use and a lot more safe.

00:02:24.360 --> 00:02:32.700
And especially if you regard multi-trend programming, parallel programming, concurrent programming, it is a lot easier in Rust.

00:02:32.700 --> 00:02:34.340
It opens a lot of possibilities.

00:02:34.340 --> 00:02:35.400
Yeah, my understanding.

00:02:35.400 --> 00:02:40.340
I've only given a cursory look to Rust to sort of scan some examples.

00:02:40.340 --> 00:02:45.000
And we're going to see some examples of code in a little bit, actually, related to pullers.

00:02:45.000 --> 00:02:47.160
But it's kind of a low-level language.

00:02:47.160 --> 00:02:48.840
It's not as simple as Python.

00:02:48.840 --> 00:02:49.380
No.

00:02:49.380 --> 00:02:51.200
It's a JavaScript.

00:02:51.540 --> 00:03:00.840
But it is easier than C, C++, not just in the syntax, but it does better memory tracking for you and the concurrency especially, right?

00:03:00.960 --> 00:03:07.200
Yeah, well, so Rust brings a whole new thing to the table, which is called ownership and a borer checker.

00:03:07.200 --> 00:03:08.640
And Rust is really strict.

00:03:08.640 --> 00:03:15.600
There are things in Rust you cannot do in C or C++ because at a time there can only be one owner of a piece of memory.

00:03:15.600 --> 00:03:16.500
And other people can.

00:03:16.500 --> 00:03:21.160
You can lend out this piece of memory to other users, but then they cannot mutate it.

00:03:21.240 --> 00:03:24.240
So there can be only one owner which is able to mutate something.

00:03:24.240 --> 00:03:29.040
And this restriction makes Rust a really hard language to learn.

00:03:29.040 --> 00:03:39.900
But once it's clicked, once you went over that steep learning curve, it becomes a lot easier because it doesn't allow you things that you could do in C and C++.

00:03:39.900 --> 00:03:46.980
But those things were also things you shouldn't do in C and C++ because they probably led to sac faults and to memory issues.

00:03:46.980 --> 00:03:51.860
And this borer checker also makes writing concurrent programming safe.

00:03:51.960 --> 00:03:55.700
You can have many threads reading a variable all they want.

00:03:55.700 --> 00:03:56.800
They can read concurrently.

00:03:56.800 --> 00:04:04.120
It's when you have writers and readers that this whole thread safety, critical section, take your locks or the locks re-entering.

00:04:04.120 --> 00:04:06.740
All of that really difficult stuff comes in.

00:04:06.740 --> 00:04:10.700
And so it sounds like an important key to make sure.

00:04:10.700 --> 00:04:16.300
Yeah, and Rust and the same borer checker also knows when memory has to be freed and not.

00:04:16.300 --> 00:04:25.200
But it doesn't have to, unlike in Go or Yaka where you have a garbage collector, it doesn't have to do garbage collection and it doesn't have to do reference counting like Python does.

00:04:25.200 --> 00:04:27.380
It does so by just statically.

00:04:27.380 --> 00:04:31.240
So at compile time, it knows when something is out of scope and not used anymore.

00:04:31.240 --> 00:04:32.340
And this is real power.

00:04:32.340 --> 00:04:41.200
I guess the takeaway for listeners who are wondering, you know, why is Rust seemingly taking over so much of the job that C and variations of C, right?

00:04:41.200 --> 00:04:44.060
Like you said, Cython have traditionally played in Python.

00:04:44.060 --> 00:04:47.980
It's easier to write modern, faster, safer code.

00:04:47.980 --> 00:04:48.260
Yeah.

00:04:48.260 --> 00:04:48.780
Yeah.

00:04:48.780 --> 00:04:50.320
And it's more fun too, right?

00:04:50.320 --> 00:04:51.000
Yeah, definitely.

00:04:51.000 --> 00:04:54.440
And it's a language which has got its tools right.

00:04:54.440 --> 00:04:57.380
So it's got a package manager, which is really great to use.

00:04:57.380 --> 00:05:01.580
So it's got a real creates.io, which is similar to the PyPy index.

00:05:01.580 --> 00:05:03.240
It feels like a modern language.

00:05:03.240 --> 00:05:03.520
Yeah.

00:05:03.520 --> 00:05:06.360
Builds low-level, more low-level code.

00:05:06.360 --> 00:05:18.240
You can also write high-level stuff like REST APIs, which is, I must say, also for high-level stuff, I like to write it in Rust because of the safety guarantees and also the correctness guarantees.

00:05:18.240 --> 00:05:27.700
If my program compiles on Rust, I'm much more certain it is correct than when I write my Python program, which is dynamic and types are not enforced.

00:05:27.700 --> 00:05:30.420
So it's always a bit graying on that side.

00:05:30.420 --> 00:05:34.320
Python is great to use, but it's harder to write correct code in Python.

00:05:34.320 --> 00:05:34.920
Yeah.

00:05:34.920 --> 00:05:45.200
And you can optionally write very loose code or you could opt in to things like type hints and even mypy and then you get closer to the static languages, right?

00:05:45.200 --> 00:05:47.460
Are you a fan of Python typing?

00:05:47.620 --> 00:05:48.060
Definitely.

00:05:48.060 --> 00:05:52.100
But because they're optional, they are as strong as the weakest link.

00:05:52.100 --> 00:05:58.740
So one library which you use, if it doesn't do this type correct or doesn't do it, it breaks.

00:05:58.740 --> 00:06:01.100
It's quite brittle because it's optional.

00:06:01.100 --> 00:06:05.580
I hope we get something that really enforces it and really can check it.

00:06:05.580 --> 00:06:09.180
I don't know if it's possible because of the dynamic nature of Python.

00:06:09.180 --> 00:06:13.660
Python can do so many things just dynamically and statically.

00:06:13.660 --> 00:06:15.540
We just cannot know probably.

00:06:15.940 --> 00:06:17.940
I don't know how far it can go.

00:06:17.940 --> 00:06:18.780
But yeah.

00:06:18.780 --> 00:06:28.380
In Polis as well, we use mypy type hints, which prevent us from having a lot of bugs and also make the IDE experience much nicer.

00:06:28.380 --> 00:06:29.120
Yeah.

00:06:29.120 --> 00:06:29.960
Type hints are great.

00:06:30.220 --> 00:06:32.420
They really help you also think about your library.

00:06:32.420 --> 00:06:39.100
I think you really see a shift in modern Python and Python 10 years ago where it was more dynamic.

00:06:39.100 --> 00:06:46.620
The dynamic of Python were more seen as a strength than currently, I believe.

00:06:46.700 --> 00:06:47.860
Yeah, I totally agree.

00:06:47.860 --> 00:06:54.060
And I feel like when type hints first came out, you know, this was, yes, wow, at this point, kind of early Python 3.

00:06:54.060 --> 00:06:56.460
But it didn't feel like it at the time, you know.

00:06:56.460 --> 00:06:58.400
Python 3 had been out for quite a while.

00:06:58.400 --> 00:07:02.100
When type hints were introduced, I feel like that was Python 3, 4.

00:07:02.260 --> 00:07:06.220
But anyway, that was put it maybe six years into the life cycle of Python 3.

00:07:06.220 --> 00:07:10.420
But still, I feel like a lot of people were suspicious of that at the moment.

00:07:10.420 --> 00:07:12.760
You know, they're like, oh, what is this weird thing?

00:07:12.760 --> 00:07:15.820
We're not really sure we want to put these types into our Python.

00:07:16.040 --> 00:07:17.360
And now, a lot less.

00:07:17.360 --> 00:07:19.680
There's a lot less of those reactions.

00:07:19.680 --> 00:07:20.220
Yeah, I see.

00:07:20.220 --> 00:07:21.140
Yeah, yeah.

00:07:21.140 --> 00:07:24.000
I see Python having two, probably more.

00:07:24.000 --> 00:07:35.480
But I often see Python as the really fun, nice, huge duct tape language where I can, in my, for instance, in Jupyter Notebook, I can just hack away and try interactively what happens.

00:07:35.480 --> 00:07:38.060
And for such code, type hints don't matter.

00:07:38.060 --> 00:07:43.480
But once I write more of a library or product or tool, then type hints are really great.

00:07:44.200 --> 00:07:47.600
I believe they came about that Dropbox really needed them.

00:07:47.600 --> 00:07:49.360
They had a huge Python code.

00:07:49.360 --> 00:07:52.280
These are really trouble maintaining it without the plugins.

00:07:52.280 --> 00:07:53.260
But I'm not really sure.

00:07:53.260 --> 00:07:55.920
Yeah, and I heard some guy who has something to do with Python used to work there.

00:07:55.920 --> 00:07:56.620
Yeah, yeah, yeah.

00:07:56.620 --> 00:07:59.220
Guido used to work there, I think even at that time.

00:07:59.220 --> 00:08:02.820
All right, so a bit of a diversion from how I often start the show.

00:08:02.820 --> 00:08:05.200
So let's just circle back real quick and get your story.

00:08:05.200 --> 00:08:08.540
How did you get into programming and Python and Rust as well, I suppose?

00:08:08.540 --> 00:08:09.720
I got into programming.

00:08:09.720 --> 00:08:11.560
I just wanted to learn programming.

00:08:11.680 --> 00:08:16.640
A friend of mine who did, who programmed a lot of PHP said, learn Python, like that.

00:08:16.640 --> 00:08:23.400
He gave me an interactive website where I could do some puzzles and I really got hooked to it.

00:08:23.400 --> 00:08:25.000
It was a fun summer.

00:08:25.500 --> 00:08:27.320
I was programming a lot.

00:08:27.320 --> 00:08:28.500
I started automating.

00:08:28.500 --> 00:08:31.360
My job was a civil engineer at the moment.

00:08:31.360 --> 00:08:32.040
Then I started.

00:08:32.040 --> 00:08:37.100
There was a lot of mundane tasks which were repetitive and I just found ways to automate my job.

00:08:37.100 --> 00:08:40.660
And eventually I was doing that for a year or three, four.

00:08:40.660 --> 00:08:43.620
And then I got into data science and I switched jobs.

00:08:43.620 --> 00:08:46.880
I became a data scientist and later a data engineer.

00:08:47.320 --> 00:08:48.980
Yeah, so that was Python mostly.

00:08:48.980 --> 00:08:52.600
I've always been looking for more languages.

00:08:52.600 --> 00:08:56.060
I've been playing with Haskell, I've been playing with Go, I've been playing with JavaScript.

00:08:56.060 --> 00:08:59.560
I've been playing with Scala.

00:08:59.560 --> 00:09:01.000
And then I found Rust.

00:09:01.000 --> 00:09:03.700
And Rust really, really made me happy.

00:09:03.700 --> 00:09:06.940
Like you learn a lot about how computers work.

00:09:07.160 --> 00:09:10.780
So I had a new renaissance of the first experience with Python.

00:09:10.780 --> 00:09:12.620
Another summer with Rust.

00:09:12.620 --> 00:09:14.320
And I've been doing a lot of toy projects.

00:09:14.320 --> 00:09:16.460
Like writing and interpreting.

00:09:16.460 --> 00:09:17.440
I don't know.

00:09:17.440 --> 00:09:18.440
A lot of projects.

00:09:18.440 --> 00:09:23.120
And Polus became one of those hobby projects just to use Rust more.

00:09:23.120 --> 00:09:25.180
Now it's got quite the following.

00:09:25.180 --> 00:09:27.360
And we're going to definitely dive into that.

00:09:27.360 --> 00:09:28.840
But let me pull it up.

00:09:28.840 --> 00:09:29.620
It does right here.

00:09:29.620 --> 00:09:31.480
13,000 GitHub stars.

00:09:31.480 --> 00:09:35.860
That's a good number of people using that project.

00:09:35.860 --> 00:09:36.200
Yeah.

00:09:36.200 --> 00:09:37.060
Crazy, isn't it?

00:09:37.120 --> 00:09:37.840
Yeah, it is.

00:09:37.840 --> 00:09:38.280
It is.

00:09:38.280 --> 00:09:42.020
On GitHub stars, it's the fastest growing data tool, I believe.

00:09:42.020 --> 00:09:42.800
Wow.

00:09:42.800 --> 00:09:43.580
Incredible.

00:09:43.580 --> 00:09:45.840
You must be really proud of that.

00:09:45.840 --> 00:09:46.380
Yeah.

00:09:46.380 --> 00:09:46.860
Yeah.

00:09:46.860 --> 00:09:50.640
If you would have told me this two years ago, I wouldn't have believed it.

00:09:50.640 --> 00:09:54.680
But it happens slow enough so you can get accustomed to that.

00:09:54.680 --> 00:09:54.920
Yeah.

00:09:54.920 --> 00:09:55.880
That's cool.

00:09:55.880 --> 00:09:57.340
Kind of like being a parent.

00:09:57.340 --> 00:09:59.920
The challenges of the kids are small.

00:09:59.920 --> 00:10:03.160
They're intense, but there are only a few things they need when they're small.

00:10:03.160 --> 00:10:04.280
And you kind of grow with it.

00:10:04.280 --> 00:10:05.920
So a couple of thoughts.

00:10:06.080 --> 00:10:11.900
One, you had the inverse style of learning to program that I think a lot of computer science

00:10:11.900 --> 00:10:12.560
people do.

00:10:12.560 --> 00:10:13.700
And certainly that I did.

00:10:13.700 --> 00:10:16.440
It could also just be that I learned it a long time ago.

00:10:16.560 --> 00:10:20.620
But when I learned programming, it was, I'm going to learn C and C++.

00:10:20.620 --> 00:10:24.480
And then you're kind of allowed to learn the easier languages.

00:10:24.480 --> 00:10:26.560
But you will learn your pointers.

00:10:26.560 --> 00:10:29.320
You'll have your void star star and you're going to like it.

00:10:29.320 --> 00:10:31.540
You're going to understand what a pointer to a pointer means.

00:10:31.540 --> 00:10:37.240
And we're going to get, I mean, you know, you start inside and you have the most complex,

00:10:37.240 --> 00:10:38.040
closest to the machine.

00:10:38.040 --> 00:10:38.820
You work your way out.

00:10:39.040 --> 00:10:40.240
You kind of took this opposite.

00:10:40.240 --> 00:10:43.060
Like, let me learn Python where it's much more high level.

00:10:43.060 --> 00:10:48.800
It's much, you know, if you choose to be often say very much more away from the hardware and

00:10:48.800 --> 00:10:51.220
the ideas of memory and threads and all that.

00:10:51.220 --> 00:10:52.560
And then you went to Rust.

00:10:52.560 --> 00:10:54.780
So was it kind of an intense experience?

00:10:54.780 --> 00:10:56.960
We're like, oh my gosh, this is intense.

00:10:56.960 --> 00:10:59.900
Or had you studied enough languages by then to become comfortable?

00:11:00.140 --> 00:11:01.260
Well, yeah, yeah, no.

00:11:01.260 --> 00:11:06.540
So the going from high level to low language, I think it makes natural sense.

00:11:06.540 --> 00:11:11.480
If you've learned it yourself, there's no professor telling me you learn your pointers.

00:11:11.480 --> 00:11:17.440
So I think this also helped a lot because at that point, you're really accustomed to programming,

00:11:17.440 --> 00:11:18.140
to algorithms.

00:11:18.140 --> 00:11:18.660
Yeah.

00:11:18.660 --> 00:11:23.320
So you can, I believe you should learn one thing, one new thing at a time, and then you

00:11:23.320 --> 00:11:25.080
can really own that knowledge.

00:11:25.960 --> 00:11:29.900
But Rust, I wouldn't say you should learn Rust as a first language.

00:11:29.900 --> 00:11:33.960
It would be really terrible because you need, that would be terrible.

00:11:33.960 --> 00:11:39.780
But other languages also don't help you much because the borrow checker is quite unique.

00:11:39.780 --> 00:11:42.260
It doesn't let you do things you can do in other languages.

00:11:42.260 --> 00:11:48.260
So what you learned there, the languages that allow you to do that, they just hurt you because

00:11:48.260 --> 00:11:49.600
you were...

00:11:49.600 --> 00:11:51.700
They encourage the wrong behavior, right?

00:11:51.700 --> 00:11:52.540
Well, yeah.

00:11:52.540 --> 00:11:58.560
So nine out of ten times, it turns out by the compiler not letting you do that one

00:11:58.560 --> 00:12:01.900
thing, that one thing you wanted was probably really bad to begin with.

00:12:01.900 --> 00:12:03.420
It led to really...

00:12:03.420 --> 00:12:05.800
So in Rust, your code is always a lot flatter.

00:12:05.800 --> 00:12:10.240
It's always really clear who owns the memory, how deep your nesting is.

00:12:10.240 --> 00:12:12.340
It's always one D deeper.

00:12:12.340 --> 00:12:15.740
Most of the times, it's not that complicated.

00:12:15.740 --> 00:12:19.740
You make things really flat and really easy to reason about.

00:12:19.740 --> 00:12:24.360
And in the beginning of a project, it seems, okay, a bit over-constraining.

00:12:24.360 --> 00:12:25.400
But when...

00:12:25.400 --> 00:12:30.440
I mean, software will become complex and complicated, and then you're happy that the compiler not

00:12:30.440 --> 00:12:31.180
to do this...

00:12:31.180 --> 00:12:31.880
Yeah, absolutely.

00:12:31.880 --> 00:12:32.700
...in this direction.

00:12:32.700 --> 00:12:34.320
It seems like a better way, honestly.

00:12:34.320 --> 00:12:39.980
You know, you get a sense of programming in a more simple language that doesn't ask so

00:12:39.980 --> 00:12:43.120
many low-level concepts of you, and then you're ready.

00:12:43.120 --> 00:12:44.900
You can add on these new ones.

00:12:44.900 --> 00:12:49.180
So I feel like a lot of how we teach programming and how people learn programming is a little

00:12:49.180 --> 00:12:50.320
bit backwards, to be honest.

00:12:50.320 --> 00:12:50.900
Yeah.

00:12:50.900 --> 00:12:51.820
Enough on that.

00:12:51.820 --> 00:12:56.460
So you were a civil engineer for a while, and then you became a data scientist, and now

00:12:56.460 --> 00:12:57.340
you've created this library.

00:12:57.340 --> 00:12:59.240
Still working as a data scientist now?

00:12:59.240 --> 00:12:59.960
No, no.

00:12:59.960 --> 00:13:07.140
I got sponsored two years ago for two days a week, and yeah, just use that time to develop

00:13:07.140 --> 00:13:07.400
Olar.

00:13:07.400 --> 00:13:13.220
And currently, I start all my day jobs and go full-time with Olar.

00:13:13.220 --> 00:13:17.220
I'm trying to live on sponsorships, which is not really working.

00:13:17.220 --> 00:13:22.240
It's not enough at this time, but I hope to start a foundation and get some proper sponsors

00:13:22.240 --> 00:13:22.480
in.

00:13:22.480 --> 00:13:23.600
Yeah, that'd be great.

00:13:23.600 --> 00:13:24.340
Yeah.

00:13:24.340 --> 00:13:25.520
That's awesome.

00:13:25.740 --> 00:13:29.940
It's still awesome that you're able to do that, even if you still needed to grow a

00:13:29.940 --> 00:13:30.340
little bit.

00:13:30.340 --> 00:13:30.740
Yeah.

00:13:30.740 --> 00:13:34.860
We'll have you on a podcast and let other people know out there who maybe are using your

00:13:34.860 --> 00:13:35.440
library.

00:13:35.440 --> 00:13:39.100
Maybe they can put a little sponsorship in GitHub sponsors.

00:13:39.100 --> 00:13:44.620
I feel like GitHub sponsors really made it a lot easier for people to support.

00:13:44.620 --> 00:13:50.200
Because there used to be PayPal donate buttons and other things like that.

00:13:50.200 --> 00:13:52.560
And one, those are not really recurring.

00:13:52.560 --> 00:13:55.720
And two, you've got to go find some place and put your credit card.

00:13:55.720 --> 00:13:59.180
Many of us already have a credit card registered at GitHub.

00:13:59.180 --> 00:14:02.440
It's just a matter of checking a box and monthly it'll just go.

00:14:02.440 --> 00:14:05.180
It's kind of like the app store versus buying independent apps.

00:14:05.180 --> 00:14:06.860
It just cuts down a lot of friction.

00:14:06.860 --> 00:14:10.380
I feel like it's been really positive, mostly for open source.

00:14:10.380 --> 00:14:13.860
Yeah, I think it's good as a way to say thank you.

00:14:13.860 --> 00:14:16.080
It isn't enough to pay the bills.

00:14:16.080 --> 00:14:17.900
I think for most developers, it isn't.

00:14:17.900 --> 00:14:19.520
But I hope we get there.

00:14:19.520 --> 00:14:24.240
I think companies who use it should give a bit more back.

00:14:24.240 --> 00:14:25.320
I mean, they have a lot of money.

00:14:25.320 --> 00:14:25.700
I agree.

00:14:25.700 --> 00:14:43.160
It's really, really ridiculous that there are banks and VC funded companies and things like that that have, not necessarily in terms of the VC ones, but definitely in terms of financial and other large companies that make billions and billions of dollars in profit on top of open source technology.

00:14:43.160 --> 00:14:58.480
And many of them don't give anything back, which is, it's not criminal because the licenses allow it, but it's certainly borders on immoral to say all this money and not at all support the people who are really building the foundations that we build upon.

00:14:58.480 --> 00:15:00.100
Most of my sponsors are developers.

00:15:00.100 --> 00:15:00.420
Yeah.

00:15:00.420 --> 00:15:01.320
Yeah.

00:15:01.320 --> 00:15:02.720
So, yeah.

00:15:02.720 --> 00:15:04.300
Let's hope it changes.

00:15:04.300 --> 00:15:05.060
I don't know.

00:15:05.060 --> 00:15:05.300
Yeah.

00:15:05.380 --> 00:15:07.740
Well, I'll continue to beat that drum.

00:15:07.740 --> 00:15:12.960
This portion of Talk Python to Me is brought to you by TypePi.

00:15:12.960 --> 00:15:16.860
TypePi is the next generation open source Python application builder.

00:15:16.860 --> 00:15:21.920
With TypePi, you can turn data and AI algorithms into full web apps in no time.

00:15:21.920 --> 00:15:22.900
Here's how it works.

00:15:22.900 --> 00:15:25.920
You start with a bare algorithm written in Python.

00:15:25.920 --> 00:15:32.860
You then use TypePi's innovative tool set that enables Python developers to build interactive end user applications quickly.

00:15:33.580 --> 00:15:37.420
There's a visual designer to develop highly interactive GUIs ready for production.

00:15:37.420 --> 00:15:42.200
And for inbound data streams, you can program against the TypePi core layer as well.

00:15:42.200 --> 00:15:48.200
TypePi core provides intelligent pipeline management, data caching, and scenario and cycle management facilities.

00:15:48.200 --> 00:15:49.300
That's it.

00:15:49.300 --> 00:15:54.880
You'll have transformed a bare algorithm into a full-fledged decision support system for end users.

00:15:54.880 --> 00:15:57.680
TypePi is pure Python and open source.

00:15:57.680 --> 00:16:00.280
And you install it with a simple pip install TypePi.

00:16:00.780 --> 00:16:07.240
For large organizations that need fine-grained control and authorization around their data, there is a paid TypePi Enterprise Edition.

00:16:07.240 --> 00:16:11.080
But the TypePi core and GUI described above is completely free to use.

00:16:11.080 --> 00:16:16.160
Learn more and get started by visiting talkpython.fm/TypePi.

00:16:16.160 --> 00:16:17.840
That's T-A-I-P-I.

00:16:17.840 --> 00:16:19.120
The link's in your show notes.

00:16:19.120 --> 00:16:21.960
Thank you to TypePi for sponsoring the show.

00:16:22.340 --> 00:16:23.600
Let's talk about your project.

00:16:23.600 --> 00:16:28.660
So, Polars and the RS is for Rust, I imagine, at the end.

00:16:28.660 --> 00:16:28.920
Yeah.

00:16:28.920 --> 00:16:32.160
But tell us about the name Polars, like Polar Bear, but Polars.

00:16:32.160 --> 00:16:32.500
Yeah.

00:16:32.500 --> 00:16:35.220
So, I started writing a data frame library.

00:16:35.220 --> 00:16:37.780
And initially, it was only for Rust.

00:16:37.780 --> 00:16:38.720
It was my idea.

00:16:38.720 --> 00:16:41.160
Until you get it.

00:16:41.160 --> 00:16:43.320
Until you saw all the people doing data science in Python.

00:16:43.320 --> 00:16:44.060
You're like, whoa.

00:16:44.060 --> 00:16:44.660
Yeah, yeah, yeah.

00:16:44.720 --> 00:16:45.960
What can I do for these people, right?

00:16:45.960 --> 00:16:46.620
Yeah, yeah.

00:16:46.620 --> 00:16:49.880
And I wanted to give a wink to the Pondus project.

00:16:49.880 --> 00:16:53.420
But I wanted a bear that was better, faster, I don't know, stronger.

00:16:53.420 --> 00:16:58.460
So, luckily, a panda bear isn't the most frightful bear.

00:16:58.460 --> 00:17:00.880
So, I had a few to choose.

00:17:00.880 --> 00:17:04.580
But the Grizzly, yeah, the Polars has the RS.

00:17:04.580 --> 00:17:06.160
So, that's a lucky coincidence.

00:17:06.160 --> 00:17:06.900
Yeah.

00:17:06.900 --> 00:17:12.560
Yeah, so the subtitle here is Lightning Fast Data Frame Library for Rust and Python.

00:17:12.940 --> 00:17:15.100
And you have two APIs that people can use.

00:17:15.100 --> 00:17:16.720
We'll get to dive into those.

00:17:16.720 --> 00:17:17.160
Yeah.

00:17:17.160 --> 00:17:21.480
Because we've written in Rust, it's a complete data frame library in Rust.

00:17:21.480 --> 00:17:23.460
And you can expose it to many frontends.

00:17:23.460 --> 00:17:27.020
So, it's already frontend in Rust, Python, Node.js.

00:17:27.020 --> 00:17:28.440
R is coming up.

00:17:28.440 --> 00:17:30.860
And normal JavaScript is coming up.

00:17:30.860 --> 00:17:33.880
And Ruby, there is also a Polar in Ruby.

00:17:33.880 --> 00:17:35.320
How interesting.

00:17:35.320 --> 00:17:38.720
So, for the JavaScript one, are you going to use WebAssembly?

00:17:38.720 --> 00:17:39.060
Yeah.

00:17:39.060 --> 00:17:39.340
Right?

00:17:39.340 --> 00:17:42.380
Which is pretty straightforward because Rust comes from Mozilla.

00:17:42.800 --> 00:17:44.920
WebAssembly, I believe, also originated.

00:17:44.920 --> 00:17:47.800
They kind of originated as a somewhat tied together story.

00:17:47.800 --> 00:17:48.080
Yeah.

00:17:48.080 --> 00:17:51.580
So, Rust C++ C can compile to WebAssembly.

00:17:51.580 --> 00:17:57.660
It's not really straightforward because the WebAssembly virtual machine isn't like your normal OS.

00:17:57.660 --> 00:17:59.220
So, there are a lot of things harder.

00:17:59.220 --> 00:18:02.040
But we are working on the challenges there.

00:18:02.040 --> 00:18:02.300
Okay.

00:18:02.300 --> 00:18:03.460
Well, that's pretty interesting.

00:18:03.460 --> 00:18:05.860
But for now, you've got Python and you've got Rust.

00:18:05.860 --> 00:18:06.980
And that's great.

00:18:07.180 --> 00:18:07.740
Let's...

00:18:07.740 --> 00:18:12.020
I think a lot of people listening, myself included, when I started looking into this,

00:18:12.020 --> 00:18:15.560
immediately go to, it's like pandas, but rust.

00:18:15.560 --> 00:18:16.420
You know?

00:18:16.420 --> 00:18:20.420
It's like pandas, but instead of C at the bottom, it's rust at the bottom.

00:18:20.420 --> 00:18:23.680
And that's somewhat true, but mostly not true.

00:18:23.680 --> 00:18:28.920
So, let's start with you telling us, you know, how is this like pandas and how is it different

00:18:28.920 --> 00:18:29.540
from pandas?

00:18:29.540 --> 00:18:29.900
Yeah.

00:18:29.900 --> 00:18:32.480
So, it's not like pandas.

00:18:32.480 --> 00:18:34.780
I think it's different on two ways.

00:18:34.780 --> 00:18:37.620
So, we have the API and we have the implementation.

00:18:37.620 --> 00:18:39.800
And which one should I start with?

00:18:39.800 --> 00:18:40.480
Bottom up?

00:18:40.480 --> 00:18:41.740
That's, I think, bottom up.

00:18:41.740 --> 00:18:42.600
Yeah, bottom up.

00:18:42.600 --> 00:18:42.780
Sure.

00:18:42.860 --> 00:18:42.980
Yeah.

00:18:42.980 --> 00:18:43.200
All right.

00:18:43.200 --> 00:18:48.460
So, that was my critique from pandas and that they didn't start bottom up.

00:18:48.460 --> 00:18:53.140
They took whatever was there already, which were good for that purpose.

00:18:53.140 --> 00:18:55.920
And pandas built on NumPy.

00:18:55.920 --> 00:18:57.740
And NumPy is a great library.

00:18:57.740 --> 00:19:01.680
But it's built for numerical processing and not for relational processing.

00:19:01.680 --> 00:19:04.160
Relational data is completely different.

00:19:04.160 --> 00:19:05.880
You have string data, you have message data.

00:19:05.880 --> 00:19:11.380
And this data is going to be just put as Python object in those NumPy arrays.

00:19:11.760 --> 00:19:17.420
And if you know anything about memory, then in this array, you have a pointer where each

00:19:17.420 --> 00:19:18.980
Python object is somewhere else.

00:19:18.980 --> 00:19:23.080
So, if you traverse this memory, every pointer you hit, you must look it up somewhere else.

00:19:23.080 --> 00:19:24.280
But memory is not in cache.

00:19:24.280 --> 00:19:28.860
So, you have a cache miss, which is a 200x slowdown per element to traverse.

00:19:28.860 --> 00:19:29.320
Yeah.

00:19:29.320 --> 00:19:35.560
So, for people listening, what you're saying the 200x slowdown is the L1, L2, L3 caches,

00:19:35.560 --> 00:19:37.040
which all have different speeds and stuff.

00:19:37.160 --> 00:19:43.540
The caches that are near the CPU versus main memory is like 200 to 400 times slower.

00:19:43.540 --> 00:19:43.980
Yeah.

00:19:43.980 --> 00:19:45.480
Not staging off a disk or something.

00:19:45.480 --> 00:19:47.040
It's really different, right?

00:19:47.040 --> 00:19:47.860
It's really a big deal.

00:19:47.860 --> 00:19:48.580
It's a big deal.

00:19:48.580 --> 00:19:49.520
It's terribly slow.

00:19:49.520 --> 00:19:51.820
It also, Python has a GIL.

00:19:51.820 --> 00:19:54.020
It also blocks multi-threading.

00:19:54.020 --> 00:19:57.820
If you want to read the string, you cannot do this from different threads.

00:19:57.980 --> 00:20:01.680
If you want to modify the string, there's only one thread that can access Python here.

00:20:01.680 --> 00:20:07.860
So, they also didn't take into account anything from databases.

00:20:07.860 --> 00:20:11.660
So, databases are basing from the 1950s.

00:20:11.660 --> 00:20:17.360
There's been a lot of research in databases and how we do things fast, write a query, and

00:20:17.360 --> 00:20:18.560
then optimize this query.

00:20:18.620 --> 00:20:21.900
Because the user that uses your library is not the expert.

00:20:21.900 --> 00:20:23.400
It doesn't write optimized query.

00:20:23.400 --> 00:20:23.860
No.

00:20:23.860 --> 00:20:29.980
But we have a lot of information, so we can optimize this query and execute this in a very efficient

00:20:29.980 --> 00:20:30.340
way.

00:20:30.340 --> 00:20:31.680
That's an interesting idea.

00:20:31.680 --> 00:20:32.080
Yeah.

00:20:32.080 --> 00:20:35.520
And Pandas just executes it and gives you what you ask.

00:20:35.520 --> 00:20:37.440
And what you ask is probably not the answer.

00:20:37.440 --> 00:20:43.280
Yeah, that's interesting because as programmers, when I have my Python hat on, I want my code

00:20:43.280 --> 00:20:45.060
to run exactly as I wrote it.

00:20:45.060 --> 00:20:48.440
I don't want it to get clever and change it.

00:20:48.440 --> 00:20:50.640
If I said do a loop, do a loop.

00:20:50.640 --> 00:20:53.420
If I said put it in a dictionary, put it in a dictionary.

00:20:53.420 --> 00:20:59.740
But when I write a database query, be that against Postgres with relational or MongoDB,

00:20:59.740 --> 00:21:01.440
there's a query planner.

00:21:01.440 --> 00:21:04.400
And the query planner looks at all the different steps.

00:21:04.400 --> 00:21:06.000
Should we do the filter first?

00:21:06.000 --> 00:21:07.160
Can we use an index?

00:21:07.300 --> 00:21:08.260
Can we use a compo?

00:21:08.260 --> 00:21:09.900
Which index should we choose?

00:21:09.900 --> 00:21:11.580
All of those things, right?

00:21:11.580 --> 00:21:16.960
And so what you tell it and what happens, you don't tell it how to do finding the data

00:21:16.960 --> 00:21:17.460
of the database.

00:21:17.460 --> 00:21:22.980
You just give it, here's kind of the expressions that I need, the predicates that I need you

00:21:22.980 --> 00:21:23.540
to work with.

00:21:23.540 --> 00:21:24.840
And then you figure it out.

00:21:24.840 --> 00:21:25.300
You're smart.

00:21:25.300 --> 00:21:26.000
You're the database.

00:21:26.000 --> 00:21:32.040
So one of the differences I got from reading what you've got here so far is it looks like,

00:21:32.040 --> 00:21:35.900
I don't know if it goes as far as this database stuff that we're talking about, but there's

00:21:35.900 --> 00:21:39.660
a way for it to build up the code it's supposed to run.

00:21:39.660 --> 00:21:44.060
And it can decide things like, you know, these two things could go in parallel or things along

00:21:44.060 --> 00:21:44.820
those lines, right?

00:21:44.820 --> 00:21:45.120
Yeah.

00:21:45.120 --> 00:21:45.460
Yeah.

00:21:45.560 --> 00:21:47.660
Well, it is actually very similar.

00:21:47.660 --> 00:21:49.160
It is a factorized query engine.

00:21:49.160 --> 00:21:54.680
And you can, the only thing that doesn't make us a database is that we don't have any, we don't bother

00:21:54.680 --> 00:21:57.120
with, with file structures.

00:21:57.120 --> 00:21:57.720
Right.

00:21:57.720 --> 00:22:00.040
Like the persistence and transactions and all.

00:22:00.040 --> 00:22:00.280
Yeah.

00:22:00.280 --> 00:22:02.340
So we have different kinds of databases.

00:22:02.340 --> 00:22:07.500
You have OLAP and OLTP, transactional modeling, which works often on one.

00:22:07.500 --> 00:22:12.700
So if you do a REST API query and you modify one user ID, then you're transactional.

00:22:12.700 --> 00:22:15.120
And if you do OLAP, that's more analytical.

00:22:15.120 --> 00:22:18.840
And then you do large aggregations of large whole tables.

00:22:18.840 --> 00:22:20.560
And then you need to process all the data.

00:22:20.560 --> 00:22:24.740
And those different database designs lead to different query optimizers.

00:22:24.740 --> 00:22:26.180
And Polis is focused on OLAP.

00:22:26.180 --> 00:22:30.820
But yeah, we, so as you described, you've got two ways of programming things.

00:22:30.940 --> 00:22:33.880
One is procedural, which Python mostly is.

00:22:33.880 --> 00:22:38.000
So you tell exactly if you want to get a cup of coffee, how many steps it should take

00:22:38.000 --> 00:22:42.440
forward, then rotate 90 degrees, take three steps and rotate 90 degrees.

00:22:42.440 --> 00:22:46.220
You can put, write down the whole algorithm how to get a coffee.

00:22:46.220 --> 00:22:49.740
Or you could just say, get me a coffee and I'd like some sugar.

00:22:49.740 --> 00:22:54.340
And then let the, let the algorithm, let the query engine decide how to best get it.

00:22:54.340 --> 00:22:54.720
Right.

00:22:54.720 --> 00:22:55.820
And that's more declarative.

00:22:55.820 --> 00:22:57.660
You describe the end result.

00:22:58.060 --> 00:23:02.220
And as it turns out, this is also very readable because you declare what you want and the

00:23:02.220 --> 00:23:05.040
intent is readable in the, in the query.

00:23:05.040 --> 00:23:10.160
And if you're doing more procedural programming, you describe what you're doing and the intent

00:23:10.160 --> 00:23:12.240
often needs to come from comments.

00:23:12.240 --> 00:23:15.060
Like what are we trying to do when we follow this out?

00:23:15.060 --> 00:23:15.380
Right.

00:23:15.380 --> 00:23:15.620
Yeah.

00:23:15.620 --> 00:23:16.480
That makes a lot of sense.

00:23:16.480 --> 00:23:18.300
And that's why, yeah, sorry.

00:23:18.300 --> 00:23:23.920
And that's why the, so the first thing is we write from, we write a database engine,

00:23:23.920 --> 00:23:29.500
a query engine from scratch and really think about multiprocessing, about cache, caches,

00:23:29.500 --> 00:23:31.260
about also out of core.

00:23:31.260 --> 00:23:33.520
We can process data that doesn't fit into memory.

00:23:33.520 --> 00:23:37.500
So we really built this from scratch with all those things in mind.

00:23:37.500 --> 00:23:41.880
And then in, at first we wanted to expose the Pondos API.

00:23:41.880 --> 00:23:45.880
And then we noticed how bad it was for writing fast data.

00:23:45.880 --> 00:23:51.500
The Pondos API just isn't really good for this declarative analyzing of what the user wants

00:23:51.500 --> 00:23:51.820
to do.

00:23:51.820 --> 00:23:56.900
So we just cut it off and took the freedom to design an API that makes most sense.

00:23:56.900 --> 00:23:58.240
Oh, that's interesting.

00:23:58.240 --> 00:24:02.540
I didn't realize that you had started trying to be closer to Pondos than you ended up.

00:24:02.540 --> 00:24:02.820
Yeah.

00:24:02.820 --> 00:24:05.040
Well, it was very short-lived, I must say.

00:24:05.040 --> 00:24:07.040
It was painful.

00:24:07.040 --> 00:24:07.320
Yeah.

00:24:07.320 --> 00:24:10.880
And that's not necessarily saying Pondos are bad, I don't think.

00:24:10.880 --> 00:24:14.240
It's approaching the problem differently and it has different goals, right?

00:24:14.240 --> 00:24:14.520
Yeah.

00:24:14.640 --> 00:24:19.140
So maybe we could look at an example of some of the code that we're talking about.

00:24:19.140 --> 00:24:25.300
I guess also one of the other differences there is much of this has to do with what you

00:24:25.300 --> 00:24:30.900
would call, I guess you refer to them as lazy APIs or streaming APIs, kind of like a generator.

00:24:31.200 --> 00:24:31.600
Yeah.

00:24:31.600 --> 00:24:36.700
So if you think about a join, for instance, in Pondos, if you would write a join and then

00:24:36.700 --> 00:24:43.160
take only one to the first 100 rows of that result, then it would first do the join and

00:24:43.160 --> 00:24:46.880
then that might produce 1 million or 10 million rows.

00:24:46.880 --> 00:24:49.000
And then you take only 100 of them.

00:24:49.000 --> 00:24:52.420
And then you have materialized a million, but you take only a fraction of that.

00:24:52.700 --> 00:24:58.400
By having that lazy, you can optimize for the whole query at a time and just see, oh,

00:24:58.400 --> 00:25:00.340
we do this join, but we only need 100 rows.

00:25:00.340 --> 00:25:02.500
So that's how we materialize 100 rows.

00:25:02.500 --> 00:25:04.300
So it gets you more realistic approach.

00:25:04.300 --> 00:25:04.940
That's really cool.

00:25:04.940 --> 00:25:09.660
I didn't realize it had so many similarities to databases, but yeah, it makes a lot of sense.

00:25:09.660 --> 00:25:10.180
All right.

00:25:10.180 --> 00:25:17.000
Let's look at maybe a super simple example you've got on fuller.rs.

00:25:17.000 --> 00:25:18.580
What country is RS?

00:25:18.580 --> 00:25:23.460
I always love how different countries that often have nothing to do with domain names

00:25:23.460 --> 00:25:28.180
get grabbed because they have a cool ending like Libya that was .ly for a while.

00:25:28.180 --> 00:25:31.260
It still is, but it was used frequently like Bitly and stuff.

00:25:31.260 --> 00:25:32.520
Do you know what RS is?

00:25:32.520 --> 00:25:34.460
I believe it's Serbia.

00:25:34.460 --> 00:25:34.900
Serbia.

00:25:34.900 --> 00:25:35.240
Okay.

00:25:35.240 --> 00:25:35.860
I'm not sure.

00:25:35.860 --> 00:25:36.160
Yeah.

00:25:36.160 --> 00:25:36.420
Yeah.

00:25:36.420 --> 00:25:36.760
Very cool.

00:25:36.760 --> 00:25:37.140
All right.

00:25:37.140 --> 00:25:39.140
So polar.rs.

00:25:39.140 --> 00:25:40.640
It's like polar.rs.

00:25:40.640 --> 00:25:45.680
Over here, you've got on the homepage here, the landing page, and then through the documentation

00:25:45.680 --> 00:25:48.560
as well, you've got a lot of places where you're like, show me the Rust API.

00:25:48.560 --> 00:25:50.060
Or show me the Python API.

00:25:50.060 --> 00:25:52.660
People can come and check out the Rust code.

00:25:52.660 --> 00:25:58.200
It's a little bit longer because it's that kind of language, but it's not terribly more

00:25:58.200 --> 00:25:58.620
complex.

00:25:58.620 --> 00:26:04.140
But maybe talk us through this little example here on the homepage in Python just to give

00:26:04.140 --> 00:26:05.920
people a sense of what the API looks like.

00:26:06.120 --> 00:26:06.260
Yeah.

00:26:06.260 --> 00:26:13.400
So we start with a scanned CSV, which is a lazy read, which is, so a read CSV tells what

00:26:13.400 --> 00:26:16.440
you do, and then it reads the CSV and you get the data frame.

00:26:16.440 --> 00:26:20.120
In a scanned CSV, we started a computation graph.

00:26:20.120 --> 00:26:21.400
We call this a lazy frame.

00:26:21.400 --> 00:26:26.200
A lazy frame is actually just, it remembers the steps of the operations you want to do.

00:26:26.380 --> 00:26:32.060
Then it sends it to Polar, but it looks at this very plan and optimize it and will think of

00:26:32.060 --> 00:26:33.060
how to execute it.

00:26:33.060 --> 00:26:34.020
And we have different engines.

00:26:34.020 --> 00:26:39.000
So you can have an engine that's more specialized for data that doesn't fit into memory, an engine

00:26:39.000 --> 00:26:41.840
that's more specialized for data that does fit into memory.

00:26:42.200 --> 00:26:48.100
So we start with a scan and then we do a dot filter and we want to use verbs.

00:26:48.100 --> 00:26:50.780
Verbs, that's the declarative part.

00:26:50.780 --> 00:26:52.820
In Pandas, we often do indexes.

00:26:52.820 --> 00:26:59.920
And those indexes are ambiguous in my opinion because you can pass in a numpy array with

00:26:59.920 --> 00:27:02.920
booleans, but you can also pass in a numpy array with integers.

00:27:02.920 --> 00:27:04.360
So you can do slicing.

00:27:04.360 --> 00:27:09.200
You can also pass in a numpy array, a list of strings, and then you do column selection.

00:27:09.200 --> 00:27:10.840
So it has three functions.

00:27:10.840 --> 00:27:15.660
One thing that I find really interesting about Pandas is it's so incredible.

00:27:15.660 --> 00:27:19.780
And people who are very good with Pandas, they can just make it fly.

00:27:19.780 --> 00:27:23.020
They can make it really right expressions that are super powerful.

00:27:23.020 --> 00:27:27.540
But it's not obvious that you should have been able to do that before you see it.

00:27:27.540 --> 00:27:32.720
You know, there's a lot of not quite magic, but stuff that doesn't seem to come really straight

00:27:32.720 --> 00:27:34.400
out of the API directly.

00:27:34.400 --> 00:27:41.180
You know, you pass in like some sort of like a Boolean expression that involves a vector

00:27:41.180 --> 00:27:44.100
and some other test into the brackets.

00:27:44.100 --> 00:27:45.900
Like, wait, how did I know I could do that?

00:27:45.900 --> 00:27:52.320
Whereas this, your API is a lot more of a fluent API where you say, you know, PD, you'd say

00:27:52.320 --> 00:27:57.980
PL, PL.scan, CSV.filter.groupby.aggregate.collect.

00:27:57.980 --> 00:27:59.960
And it kind of just flows together.

00:27:59.960 --> 00:28:05.760
Does that mean that the editors and IDEs can be more helpful suggesting what happens at each

00:28:05.760 --> 00:28:05.980
step?

00:28:06.120 --> 00:28:08.480
Yes, we are really strict on types.

00:28:08.480 --> 00:28:12.240
So we also only return a single type from a method.

00:28:12.240 --> 00:28:18.100
And we only, a .filter just expects a Boolean expression that produces a Boolean, not an integer,

00:28:18.100 --> 00:28:18.860
not a string.

00:28:18.860 --> 00:28:25.300
So we want our methods from reading or code, you should be able to understand what should

00:28:25.300 --> 00:28:25.960
go in there.

00:28:25.960 --> 00:28:27.320
That's really important to me.

00:28:27.320 --> 00:28:28.740
It should be unambiguous.

00:28:28.740 --> 00:28:29.780
It should be consistent.

00:28:29.780 --> 00:28:33.900
And your knowledge of the API should expand to different parts of the API.

00:28:34.260 --> 00:28:38.580
And that's where, I think we're going to talk about this later, but that's where expressions

00:28:38.580 --> 00:28:39.580
really come in.

00:28:39.580 --> 00:28:45.600
This portion of Talk Python to Me is brought to you by User Interviews.

00:28:45.600 --> 00:28:51.000
As a developer, how often do you find yourself talking back to products and services that you

00:28:51.000 --> 00:28:51.220
use?

00:28:51.220 --> 00:28:55.020
Sometimes it may be frustration over how it's working poorly.

00:28:55.020 --> 00:28:58.900
And if they just did such and such, it would work better.

00:28:58.900 --> 00:29:00.260
And it's easy to do.

00:29:00.260 --> 00:29:02.160
Other times it might be delight.

00:29:02.560 --> 00:29:04.360
Wow, they autofilled that section for me.

00:29:04.360 --> 00:29:05.740
How did they even do that?

00:29:05.740 --> 00:29:06.160
Wonderful.

00:29:06.160 --> 00:29:06.800
Thanks.

00:29:06.800 --> 00:29:11.400
While this verbalization might be great to get the thoughts out of your head, did you

00:29:11.400 --> 00:29:14.500
know that you can earn money for your feedback on real products?

00:29:14.500 --> 00:29:19.940
User Interviews connects researchers with professionals that want to participate in research studies.

00:29:19.940 --> 00:29:25.220
There is a high demand for developers to share their opinions on products being created for

00:29:25.220 --> 00:29:25.760
developers.

00:29:25.760 --> 00:29:30.360
Aside from the extra cash, you'll talk to people building products in your space.

00:29:30.360 --> 00:29:35.060
You will not only learn about new tools being created, but you'll also shape the future of

00:29:35.060 --> 00:29:36.500
the products that we all use.

00:29:36.500 --> 00:29:41.420
It's completely free to sign up and you can apply to your first study in under five minutes.

00:29:41.420 --> 00:29:43.740
The average study pays over $60.

00:29:43.740 --> 00:29:49.640
However, many studies specifically interested in developers pay several hundreds of dollars

00:29:49.640 --> 00:29:50.720
for a one-on-one interview.

00:29:50.720 --> 00:29:54.920
Are you ready to earn extra income from sharing your expert opinion?

00:29:54.920 --> 00:29:59.880
Head over to talkpython.fm/user interviews to participate today.

00:29:59.880 --> 00:30:02.180
The link is in your podcast player show notes.

00:30:02.180 --> 00:30:04.740
Thank you to user interviews for supporting the show.

00:30:06.540 --> 00:30:10.620
I just derailed you a little bit here as you were describing this.

00:30:10.620 --> 00:30:18.000
So you start out with scanning a CSV, which is sort of creating and kicking off a data frame equivalent here.

00:30:18.000 --> 00:30:18.580
A lazy frame.

00:30:18.580 --> 00:30:20.200
And then you, a lazy frame.

00:30:20.200 --> 00:30:20.480
Okay.

00:30:20.480 --> 00:30:25.680
And then you say a dot filter and you give it an expression like this column is greater than five.

00:30:25.680 --> 00:30:25.860
Right.

00:30:25.920 --> 00:30:26.120
Right.

00:30:26.120 --> 00:30:28.900
Or some expression that we would understand in Python.

00:30:28.900 --> 00:30:30.120
And that's the filter statement.

00:30:30.120 --> 00:30:30.380
Right.

00:30:30.380 --> 00:30:30.720
Yeah.

00:30:30.720 --> 00:30:37.660
And then we follow the group by argument and then an aggregation where we say, okay, take all columns and sum them.

00:30:37.660 --> 00:30:39.140
And this again is an expression.

00:30:39.140 --> 00:30:41.000
And these are really easy expressions.

00:30:41.000 --> 00:30:47.100
And then we take this lazy frame and we materialize it into a data frame that can collect on it.

00:30:47.100 --> 00:30:54.520
And collect means, okay, all those steps you recorded, now you can do your magic, query optimizer, get all the stuff.

00:30:54.580 --> 00:31:01.300
And what this will do here, it will recognize that, okay, we've taken the iris.csv, which got different columns.

00:31:01.300 --> 00:31:02.840
And now in this case, it won't.

00:31:02.840 --> 00:31:11.640
So if you would have finished with a select where we only select a few columns, it would have recognized, oh, we don't need all those columns in the CSV file.

00:31:11.640 --> 00:31:12.840
We only take the ones we need.

00:31:12.840 --> 00:31:17.320
What it will do, it will push the filter, the predicate, down to the scan.

00:31:17.320 --> 00:31:20.980
So during the reading of the CSV, we will take this predicate.

00:31:20.980 --> 00:31:24.140
We say, okay, the sample length is larger than five.

00:31:24.440 --> 00:31:27.460
The rows that don't match this predicate will not be materialized.

00:31:27.460 --> 00:31:37.380
So if you have a really large CSV file, this will really, let's say you have a CSV file with tens of gigabytes, but your predicate only selects 5% of that.

00:31:37.380 --> 00:31:40.680
Then you only materialize 5% of the 10 gigabytes.

00:31:41.040 --> 00:31:47.700
Yeah, so 500 megs instead of 10 gigabytes or something like that, or 200 megs, whatever it is, quite a bit less.

00:31:47.700 --> 00:31:48.920
That's really interesting.

00:31:48.920 --> 00:31:58.740
And this is all part of the benefits of what we were talking about with the lazy frames, lazy APIs, and building up all of the steps before you say go.

00:31:59.120 --> 00:32:00.880
Because in Pandas, you would say read CSV.

00:32:00.880 --> 00:32:02.620
So, okay, it's going to read the CSV.

00:32:02.620 --> 00:32:03.400
Now what?

00:32:03.400 --> 00:32:03.900
Yes.

00:32:03.900 --> 00:32:04.100
Right?

00:32:04.100 --> 00:32:09.680
And then you apply your filter if that's the order you want to do it in, and then you group, and so on and so on, right?

00:32:09.680 --> 00:32:09.920
Right.

00:32:10.020 --> 00:32:15.020
It's interesting in that it does allow more database-like behavior behind the scenes.

00:32:15.020 --> 00:32:15.380
Yeah.

00:32:15.380 --> 00:32:15.840
Yeah.

00:32:15.840 --> 00:32:21.240
In the end, in my opinion, the data frame should be seen as a table in a database.

00:32:21.240 --> 00:32:25.040
It's the final view of computation.

00:32:25.040 --> 00:32:27.380
Like, you can see it as a materialized view.

00:32:27.380 --> 00:32:36.800
We have some data on this, and we want to get it into another table, which we would feed into our machine learning models or whatever.

00:32:37.120 --> 00:32:41.000
And we do a lot of operations on them before we get there.

00:32:41.000 --> 00:32:44.080
So, I wouldn't see a data frame as a data.

00:32:44.080 --> 00:32:46.200
It's not only a data structure.

00:32:46.200 --> 00:32:48.380
It's not only a list or a dictionary.

00:32:48.380 --> 00:32:53.660
There are lots of steps before we get into those tables we eventually need.

00:32:53.660 --> 00:32:53.940
Right.

00:32:53.940 --> 00:32:56.640
So, here's an interesting challenge.

00:32:56.640 --> 00:33:00.160
There's a lot of visualization libraries.

00:33:00.160 --> 00:33:07.320
There are a lot of other data science libraries that know and expect Pandas data frames.

00:33:07.320 --> 00:33:15.540
So, like, okay, what you do is you send me the Pandas data frame here, or we're going to patch Pandas so that if you call this function on the data frame, it's going to do this thing.

00:33:15.540 --> 00:33:23.500
And they may say, Richie, fantastic job you've done here in Polars, but my stuff is already all built around Pandas, so I'm not going to use this.

00:33:23.700 --> 00:33:27.020
But it's worth pointing out there's some cool Pandas integration, right?

00:33:27.020 --> 00:33:27.340
Yeah.

00:33:27.340 --> 00:33:27.920
Yeah.

00:33:27.920 --> 00:33:30.820
So, Polars doesn't want to do plotting.

00:33:30.820 --> 00:33:33.380
I don't think it should be in a data frame library.

00:33:33.860 --> 00:33:38.120
Maybe another library can do it on top of Polars if they feel like it.

00:33:38.120 --> 00:33:39.700
It shouldn't be in Polars, in my opinion.

00:33:39.700 --> 00:33:44.700
But often when you do plotting, you're plotting, the number of rows will not be billions.

00:33:44.700 --> 00:33:47.880
I mean, there's no plotting engine that can deal with that.

00:33:47.880 --> 00:33:53.800
So, you will be reducing your big data set to something small, and then you can send it to a plot account.

00:33:53.800 --> 00:33:58.140
There's hardly a monitor that has enough pixels to show you that.

00:33:58.140 --> 00:33:58.640
Right.

00:33:58.640 --> 00:33:58.760
Right.

00:33:58.760 --> 00:33:59.460
So, yeah.

00:33:59.460 --> 00:34:06.100
We can call it to Pandas, and then we transform our Polars data frame to Pandas, and then you can integrate with scikit-learn.

00:34:06.100 --> 00:34:14.520
And we often find that progressively rewriting some Pandas code into Polars already is cheaper than keeping it in Pandas.

00:34:14.520 --> 00:34:21.960
If you go from Pandas to Polars, do a join in Polars, and then back to Pandas, we probably made up for those double copies.

00:34:22.360 --> 00:34:23.760
Pandas does a lot of internal copies.

00:34:23.760 --> 00:34:26.000
If you do a reset index, it copies all data.

00:34:26.000 --> 00:34:29.760
If you do, there are a lot of internal copies in Pandas which are implicit.

00:34:29.760 --> 00:34:37.220
So, I wouldn't worry about an explicit copy in the end of your ETL to go to plotting when the data is already small.

00:34:37.220 --> 00:34:37.640
Right, right.

00:34:37.640 --> 00:34:47.280
So, let's look at the benchmarks because it sounds like, to a large degree, even if you do have to do this conversion in the end, many times, it still might even be quicker.

00:34:47.280 --> 00:34:50.440
So, you've got some benchmarks over here, and you compared.

00:34:50.920 --> 00:34:52.620
I'm going to need some good vision for this one.

00:34:52.620 --> 00:34:58.440
You compared Polars, Pandas, Dask, and then two things which are too small for me to read.

00:34:58.440 --> 00:34:59.480
Tell us what you compared.

00:34:59.480 --> 00:35:00.200
Modding and facts.

00:35:00.200 --> 00:35:00.900
Modding and facts.

00:35:00.900 --> 00:35:01.180
Okay.

00:35:01.540 --> 00:35:07.640
And for people listening, you go out here and look at these benchmarks right off the homepage.

00:35:07.640 --> 00:35:13.440
There's like a little tiny purple thing and a whole bunch of really tall bar graphs up the rest.

00:35:13.440 --> 00:35:13.720
Yeah.

00:35:13.880 --> 00:35:20.020
And the little tiny thing that you can kind of miss if you don't look carefully, that's the time it takes for Polars.

00:35:20.020 --> 00:35:24.360
And then all the others are up there in like 60 seconds, 100 seconds.

00:35:24.760 --> 00:35:26.640
And then Polars is like a quarter of a second.

00:35:26.640 --> 00:35:29.160
So, you know, it's easy to miss it in the graph.

00:35:29.160 --> 00:35:32.640
But the quick takeaway here, I think, is there's some fast stuff.

00:35:32.640 --> 00:35:32.860
Yeah.

00:35:32.860 --> 00:35:33.360
Yeah.

00:35:33.360 --> 00:35:35.940
We're often orders of magnitudes faster than Pandas.

00:35:35.940 --> 00:35:39.620
So, it's not uncommon to hear it's 10 to 20x times faster.

00:35:39.900 --> 00:35:47.120
Especially if you do write proper Pandas and proper Polars, it's probably 20x if we deal with IO as well.

00:35:47.120 --> 00:35:49.960
So, what we see here are the TPCH benchmarks.

00:35:49.960 --> 00:35:58.900
And TPCH is a database query benchmark standard, which this is used by every query engine to show how fast it is.

00:35:58.900 --> 00:36:04.700
And those are really hard questions that really flex the muscles of a query engine.

00:36:05.100 --> 00:36:10.720
So, you have joints on several tables, different group buys, different nested group buys, etc.

00:36:10.720 --> 00:36:14.560
And, yeah, I really tried to make those other tools faster.

00:36:14.560 --> 00:36:21.240
So, in memory, Dask, and Modin, it was really hard to make stuff faster than Pandas, except for Polars.

00:36:21.240 --> 00:36:23.360
On a few occasions.

00:36:23.360 --> 00:36:28.920
Once we include IO, all those tools first needed to go via Pandas.

00:36:28.920 --> 00:36:29.660
And, yeah.

00:36:29.940 --> 00:36:36.760
What this sort of shows is that we have Pandas, which is a single-threaded data frame engine.

00:36:36.760 --> 00:36:39.400
And then we have tools that parallelize Pandas.

00:36:39.400 --> 00:36:44.840
And it's not always, they don't, just parallelizing Pandas doesn't make it faster.

00:36:44.840 --> 00:36:50.320
So, if we have a filter or a element-wise multiplication, parallelization is easy.

00:36:50.320 --> 00:36:53.120
You just split it up in chunks and do your parallelization.

00:36:53.120 --> 00:36:55.280
And then those tools win.

00:36:55.280 --> 00:37:02.480
You have 10 cores, you can start 10 threads, and they can take one-tenth of the data and start to answer yes or no for the filter question, for example.

00:37:02.480 --> 00:37:07.960
A lot of people don't realize that a lot of data frame operations are not embarrassingly parallel.

00:37:07.960 --> 00:37:11.000
A group by is definitely not embarrassingly parallel.

00:37:11.000 --> 00:37:14.340
A filter, or sorry, a join needs a shuffle.

00:37:14.340 --> 00:37:16.860
It doesn't, it's not embarrassingly parallel.

00:37:16.860 --> 00:37:24.420
And that's why you see those tools being slower than Pandas, because they're string data, and then you have a problem with those.

00:37:24.420 --> 00:37:31.240
Or we need to do multiprocessing, and we need to send those Python objects to another project, and we copy data, which is slow.

00:37:31.240 --> 00:37:35.040
Or we need to do multi-threading, and we're bound by the gill, and we're single-threaded.

00:37:35.040 --> 00:37:36.760
And then there is the expensive shuffle.

00:37:36.760 --> 00:37:37.060
Yeah.

00:37:37.060 --> 00:37:41.940
I think there's some interesting parallels for Dask and Polars.

00:37:41.940 --> 00:37:46.440
On these benchmarks, at least, you're showing much better performance than Dask.

00:37:46.440 --> 00:37:51.760
I've had Matthew Rockland on a couple times to talk about Dask and some of the work they're doing there, Coiled.

00:37:51.760 --> 00:38:08.600
And it's very cool, and one of the things that I think Dask is interesting for is allowing you to scale your code out to multi-cores on your machine, or to even distributed grid computing, or process data that doesn't fit in memory, and they can, behind the scenes, juggle all that for you.

00:38:08.600 --> 00:38:14.860
I feel like Polars kind of has a different way, but attempts to solve some of those problems as well.

00:38:14.860 --> 00:38:15.040
Yeah.

00:38:15.040 --> 00:38:18.260
The Polars has full control over everything.

00:38:18.720 --> 00:38:24.900
So it's built from the ground up, and it controls the IO, it controls their own memory, it controls which strap gets which data.

00:38:24.900 --> 00:38:34.640
And in Dask, it goes through, it takes this other tool and then parallelizes that, but it is limited by what this other tool also is limited by.

00:38:34.640 --> 00:38:38.400
But I think, so on a single machine, it has those challenges.

00:38:38.400 --> 00:38:41.040
I think Dask distributed doesn't have these challenges.

00:38:41.620 --> 00:38:44.340
And I think for distributed, it can work really well.

00:38:44.340 --> 00:38:44.840
Yeah.

00:38:44.840 --> 00:38:50.800
The interesting part with Dask, I think, is that it's kind of like Pandas, but it scales in all these interesting ways.

00:38:50.800 --> 00:38:56.960
Across cores, bigger memory, but also across machines, and then, you know, across cores, across machines, like all that.

00:38:56.960 --> 00:38:57.420
Yeah, and Dask.

00:38:57.680 --> 00:39:02.320
I feel like Dask is a little bit, maybe it's trying to solve like a little bit bigger computer problem.

00:39:02.320 --> 00:39:06.020
Like how can we use a cluster of computers to answer these questions?

00:39:06.020 --> 00:39:08.160
The documentation also says it themselves.

00:39:08.160 --> 00:39:12.680
They say that they're probably not faster than Pandas on a single machine.

00:39:12.680 --> 00:39:15.780
So they're more for the large, the big data.

00:39:16.020 --> 00:39:22.900
But Pandas wants to be, and a lot faster on a single machine, but also wants to be able to do out-of-core processing on a single machine.

00:39:22.900 --> 00:39:32.940
So if you, we don't support all queries yet, but we want to, we already do basic joins, group by sorts, predicates, element-wise operations.

00:39:32.940 --> 00:39:37.900
And then we can process, I process 500 gigabytes on my laptop.

00:39:37.900 --> 00:39:39.140
That's pretty good.

00:39:39.140 --> 00:39:41.100
Your laptop probably doesn't have 500.

00:39:41.100 --> 00:39:41.760
No, no, no, no.

00:39:41.760 --> 00:39:42.460
It's 16 gigs.

00:39:42.460 --> 00:39:43.380
Yeah.

00:39:43.380 --> 00:39:44.840
Nice.

00:39:44.840 --> 00:39:51.720
It's probably actually a value to, as you develop this product, to not have too massive of a computer to work on.

00:39:51.720 --> 00:39:59.480
If you had a $5,000 workstation, you know, you might be a little out of touch with many people using your code.

00:39:59.480 --> 00:39:59.980
Yeah.

00:39:59.980 --> 00:40:08.040
And so, although I think there, I think spoilers like scaling on a single machine makes sense for different reasons as well.

00:40:08.040 --> 00:40:13.220
I think a lot of people talk about distributed, but if you think about the complexity of distributed,

00:40:13.660 --> 00:40:16.900
you need to send data, shuffle data over the network to other machines.

00:40:16.900 --> 00:40:23.220
So there are a lot of people using Polars in our Discord who have one terabyte of RAM and say,

00:40:23.720 --> 00:40:37.480
it's cheaper and a lot faster than Spark because they can, one, Polars is faster on a single machine and one, two, they have a beefy machine with like 120 cores and they don't have to go over the network to parallelize.

00:40:37.480 --> 00:40:38.480
And yeah.

00:40:38.480 --> 00:40:40.480
So I think times are changing.

00:40:40.480 --> 00:40:44.820
I think also scaling out data on a single machine is getting more and more.

00:40:44.820 --> 00:40:48.060
It is one of the areas in which it's interesting is GPUs.

00:40:48.060 --> 00:40:51.820
Do you have any integration with GPUs or any of those sorts of things?

00:40:51.820 --> 00:40:51.820
No.

00:40:51.820 --> 00:40:54.380
Not suggesting that necessarily is even a good idea.

00:40:54.380 --> 00:40:55.380
I'm just wondering if it does.

00:40:55.380 --> 00:40:59.620
No, I get this question, but I'm not really convinced I can get the memory.

00:40:59.620 --> 00:41:02.140
I can get the data fast enough into the memory.

00:41:02.140 --> 00:41:05.140
Like we want to process gigabytes of data.

00:41:05.380 --> 00:41:12.500
And the challenge already on the CPU is getting the data from cache from memory fast enough on a CPU.

00:41:12.500 --> 00:41:14.180
This is, I don't know.

00:41:14.180 --> 00:41:14.960
I don't know.

00:41:14.960 --> 00:41:15.180
Yeah.

00:41:15.180 --> 00:41:19.300
So maybe we could talk really quickly about platforms that it runs on.

00:41:19.300 --> 00:41:25.760
You know, I just, this is the very first show that I'm doing on my M2 Pro processor, which is fun.

00:41:25.760 --> 00:41:30.120
I literally been using it for like an hour and a half, so I don't really have much to say, but it looks neat.

00:41:30.400 --> 00:41:40.460
Anyway, you know, that's very different than an Intel machine, which is different than a Raspberry Pi, which is different than, you know, some version of Linux running on ARM or on AMD.

00:41:40.460 --> 00:41:43.920
So where, where do these, what's the, the reach?

00:41:43.920 --> 00:41:45.220
Well, we support it.

00:41:45.220 --> 00:41:46.020
We support it.

00:41:46.020 --> 00:41:47.000
We don't.

00:41:47.000 --> 00:41:50.020
So Polaris also has a lot of like SIMD optimizations.

00:41:50.020 --> 00:41:58.480
SIMD stands for a single instruction log data where, for instance, if you do a floating point operation, instead of doing a single floating point at a time,

00:41:58.580 --> 00:42:06.760
you can fill in those vector lanes into your CPU, which can fit eight floating points and in a single operation can compute eight at a time.

00:42:06.760 --> 00:42:09.200
And then you have eight times the parallelism on a single core.

00:42:09.200 --> 00:42:13.280
Those instructions are only activated for Intel.

00:42:13.280 --> 00:42:18.220
So we don't have these instructions activated for ARM, but we do compile to ARM.

00:42:18.220 --> 00:42:19.400
How it performs?

00:42:19.400 --> 00:42:21.560
I think it performs fast.

00:42:22.520 --> 00:42:23.000
Yeah.

00:42:23.000 --> 00:42:25.620
But so if the standard machines, right?

00:42:25.620 --> 00:42:28.860
macOS, Windows, Linux, or where all that to go.

00:42:28.860 --> 00:42:30.180
And it ships as a wheel.

00:42:30.180 --> 00:42:33.980
So you don't have to have any, you don't have to have Rusty or anything like that hanging around.

00:42:33.980 --> 00:42:38.760
We also have Conda, but Conda is always a bit lagging behind.

00:42:38.760 --> 00:42:43.900
So I'd advise to install from pip because we can, we control this employment.

00:42:44.080 --> 00:42:44.660
Yeah, exactly.

00:42:44.660 --> 00:42:49.180
You push it out to IPI and that's what pip sees and it's going to go, right?

00:42:49.180 --> 00:42:50.180
Pretty much instantly.

00:42:50.180 --> 00:42:55.060
I guess it's worth pointing out while we're sitting here is, not that thing I highlighted this.

00:42:55.060 --> 00:43:02.000
You do have a whole section in your user guide, the Polar's book called Coming from Pandas that actually talks about the differences,

00:43:02.360 --> 00:43:09.900
not just how do I do this versus, you know, this operation in Pandas versus Polar's, but it also talks about some of the philosophy,

00:43:09.900 --> 00:43:14.260
like this lazy concepts that we've spoken about and query optimization.

00:43:14.260 --> 00:43:16.820
I feel like we covered it pretty well.

00:43:16.820 --> 00:43:17.220
Yeah.

00:43:17.220 --> 00:43:20.300
Unless there's maybe some other stuff that you want to throw in here really quick,

00:43:20.300 --> 00:43:26.540
but I mostly just want to throw this out as resource because I know many people are coming from Pandas and they may be interested in this.

00:43:26.540 --> 00:43:28.380
And this is probably a good place to start.

00:43:28.380 --> 00:43:29.380
I'll link to it in the show notes.

00:43:29.380 --> 00:43:33.560
I think the most controversial one is that we don't have the multi-index.

00:43:33.560 --> 00:43:36.980
You don't have anything other than zero-based, zero-one-two-three.

00:43:36.980 --> 00:43:38.760
You know, where is it in the array type of data?

00:43:38.760 --> 00:43:38.940
Yeah.

00:43:38.940 --> 00:43:45.180
Well, we can, we will support data structures that make lookups faster, like index in a database sense,

00:43:45.180 --> 00:43:50.020
but it will not involve the, it will not change the semantic query.

00:43:50.020 --> 00:43:51.980
That's an important thing.

00:43:51.980 --> 00:43:52.500
Okay.

00:43:52.500 --> 00:43:52.940
Yeah.

00:43:52.940 --> 00:43:57.880
So I encourage people who are mostly Pandas people that come down here and, you know, look through this.

00:43:57.880 --> 00:43:59.200
It's pretty straightforward.

00:43:59.200 --> 00:44:06.220
Another thing that I think is interesting and we're talking about maybe is we could touch a little bit on some of the,

00:44:06.220 --> 00:44:10.400
how can I, and your user guide, you've got, how can I work with IO?

00:44:10.400 --> 00:44:12.180
How can I work with time series?

00:44:12.180 --> 00:44:14.740
How can I work with multiprocessing and so on?

00:44:14.740 --> 00:44:16.740
What do you think is good to highlight out of here?

00:44:16.940 --> 00:44:18.960
The user guide is a bit outdated.

00:44:18.960 --> 00:44:20.860
So I think it's a hero.

00:44:20.860 --> 00:44:23.960
So the, for instance, IO is changing.

00:44:23.960 --> 00:44:28.340
Polar just writes as its own IO readers.

00:44:28.540 --> 00:44:36.760
So we've written our own CSV reader, JSON reader, or K, IPC, Arrow, and that's all in our control.

00:44:36.760 --> 00:44:41.460
But for interaction with databases, it's often a bit more complicated.

00:44:41.460 --> 00:44:43.740
Deal with different drivers, different ways.

00:44:44.020 --> 00:44:50.160
And currently we do this with connector X, which is really great and allows us to read from a lot of different databases.

00:44:50.160 --> 00:44:52.820
But it doesn't allow us to write from databases yet.

00:44:52.820 --> 00:44:54.480
And this is happy.

00:44:54.480 --> 00:44:55.660
This is not really changing.

00:44:55.660 --> 00:44:57.680
I want to explain a bit why.

00:44:57.680 --> 00:45:01.660
So Polar is built upon the arrow memory specification.

00:45:01.660 --> 00:45:12.780
And the arrow memory specification is sort of the standard of how memory for data, how memory for columnar data should look into, how columnar data should be represented in memory.

00:45:12.780 --> 00:45:14.700
And this is becoming a new standard.

00:45:14.700 --> 00:45:17.080
And Spark is using it.

00:45:17.080 --> 00:45:18.980
Dremel, Pandas itself.

00:45:18.980 --> 00:45:26.100
For instance, if you read a parquet in Pandas, it reads first into arrow memory and then copies that into Pandas memory.

00:45:26.100 --> 00:45:37.980
So the arrow memory specification is becoming a standard, and this is a way to share data to processes, also to other libraries within the process without copying data.

00:45:37.980 --> 00:45:41.960
We can just swap out pointers if we know that we both support arrow.

00:45:41.960 --> 00:45:47.060
Oh, so arrow defines basically in memory it looks like this.

00:45:47.060 --> 00:45:50.000
And if you both agree on that, we can just swap out pointers.

00:45:50.560 --> 00:45:59.240
Right, because a .NET object, a C++ object, and a Python object, those don't look like anything similar to any of them, right, in memory.

00:45:59.240 --> 00:46:00.620
And, yeah.

00:46:00.620 --> 00:46:04.160
So this is from the Apache Arrow project, yeah.

00:46:04.160 --> 00:46:09.440
And this is really, really used by a lot of different tools already.

00:46:09.440 --> 00:46:16.080
And currently there is coming the ADBC, which is the Apache Arrow database connector, which will solve all those problems,

00:46:16.080 --> 00:46:23.460
because then we can write, read and write from a lot of databases in Arrow, and then it will be really fast and really easy for us to do.

00:46:23.460 --> 00:46:37.320
So luckily we, that's one of those foundations of Polar's I'm really happy about, because supporting Arrow and using Arrow memory gives us a lot of interaction, interlock with other libraries.

00:46:37.440 --> 00:46:38.440
Yeah, that's interesting.

00:46:38.440 --> 00:46:38.440
Yeah, that's interesting.

00:46:38.440 --> 00:46:47.880
When you think of pandas, you know, it's kind of built on top of NumPy as its core foundation, and it can exchange NumPy arrays with other things to do that.

00:46:47.880 --> 00:46:51.920
So Apache Arrow is kind of your base.

00:46:51.920 --> 00:46:56.220
Yeah, well, it's kind of full circle, because Apache Arrow is started by Wes McKinney.

00:46:56.220 --> 00:47:00.160
Wes McKinney being known as the creator of pandas.

00:47:00.160 --> 00:47:07.400
And when he got out of pandas, he thought, okay, the memory representation of NumPy is just not, we should not use it.

00:47:07.400 --> 00:47:12.760
And then he was inspired to build Apache Arrow, which learned from pandas.

00:47:12.760 --> 00:47:13.400
And yeah.

00:47:13.400 --> 00:47:16.120
So that's how you learn about these projects, right?

00:47:16.120 --> 00:47:20.760
This is how you realize, oh, we had put this thing in place, maybe we work better, right?

00:47:20.760 --> 00:47:26.120
You work on a project for five years, and you're like, if I got a chance to start over, but it's too late now.

00:47:26.120 --> 00:47:30.160
But every now and then, you do actually get a chance to start over.

00:47:30.160 --> 00:47:30.960
Yeah, yeah.

00:47:30.960 --> 00:47:33.760
I didn't realize that Wes was involved with both.

00:47:33.760 --> 00:47:36.000
I mean, I knew from pandas, but I didn't realize he's part of mine.

00:47:36.000 --> 00:47:41.120
Yeah, he's the CEO of Voltron, which, no, he started Apache Arrow.

00:47:41.120 --> 00:47:46.840
And that's, Apache Arrow is sort of super big, like used everywhere, but sort of middleware.

00:47:46.840 --> 00:47:53.920
Like it's end users are developers, and end users are developers who build tools, and not developers who use libraries.

00:47:53.920 --> 00:47:54.960
That's something like that.

00:47:54.960 --> 00:47:55.160
Right.

00:47:55.440 --> 00:47:57.800
You might not even know that you're using it.

00:47:57.800 --> 00:47:59.600
You just use, I just use pullers.

00:47:59.600 --> 00:48:04.000
And oh, by the way, it happens to internally be better because of this.

00:48:04.000 --> 00:48:04.340
Yeah.

00:48:04.340 --> 00:48:05.140
Yeah, very cool.

00:48:05.140 --> 00:48:06.400
Okay, let's see.

00:48:06.400 --> 00:48:08.560
We've got a little bit of time left to talk about it.

00:48:08.560 --> 00:48:10.900
So, for example, some of these, how can I?

00:48:10.900 --> 00:48:13.080
Let me just touch on a couple that are nice here.

00:48:13.080 --> 00:48:15.140
So you talked about connector X.

00:48:15.140 --> 00:48:24.760
You talked about the database, but it's like three lines of code to define a connection string, define a SQL query, and then just, you can just say pl.readsql.

00:48:24.760 --> 00:48:25.260
Yeah.

00:48:25.260 --> 00:48:26.240
And there you go.

00:48:26.240 --> 00:48:29.420
You call it data frame, or what do you call the thing you get back here?

00:48:29.420 --> 00:48:31.340
So reading is always a data frame.

00:48:31.340 --> 00:48:32.900
Scanning will be a place.

00:48:32.900 --> 00:48:33.200
Got it.

00:48:33.200 --> 00:48:33.400
Okay.

00:48:33.400 --> 00:48:35.900
Is there a scan SQL as well?

00:48:35.900 --> 00:48:38.460
No, this might happen in the future.

00:48:38.980 --> 00:48:42.860
The challenge is, are we going to push back our optimizations?

00:48:42.860 --> 00:48:51.120
So we write a bonus query, and then we must translate that into SQL, into the SQL we send to the database.

00:48:51.120 --> 00:48:54.060
But that needs to be consistent over different databases.

00:48:54.520 --> 00:48:57.100
That's a whole other rabbit hole we might get into.

00:48:57.100 --> 00:48:58.700
I'm not sure.

00:48:58.700 --> 00:49:05.000
Because you can already do many of these operations in the SQL query that you're sending over, right?

00:49:05.000 --> 00:49:09.520
You have sort of two layers of query engines and optimizers and query plans.

00:49:09.520 --> 00:49:17.260
And it's not like you can't add on additional filters, joins, sorts, and so on before it ever gets back.

00:49:17.260 --> 00:49:23.300
It would be terrible if someone writes select star from table and then writes the filters in polars.

00:49:23.300 --> 00:49:26.880
And then the database has sent all those data over the network.

00:49:26.880 --> 00:49:32.520
So, yeah, ideally, we'd be able to push those predicates down into the SQL.

00:49:32.520 --> 00:49:39.440
Yeah, but you know somebody's going to do it because they're more comfortable writing polar API in Python than they are writing T-SQL.

00:49:39.440 --> 00:49:40.900
Yeah, you will not.

00:49:40.900 --> 00:49:41.340
Yeah.

00:49:41.340 --> 00:49:43.420
If it's possible, someone will write it.

00:49:43.420 --> 00:49:44.160
It's not optimal.

00:49:44.160 --> 00:49:45.580
That's right.

00:49:45.580 --> 00:49:46.240
That is right.

00:49:46.240 --> 00:49:48.040
Let's see what else can you do here.

00:49:48.040 --> 00:49:51.200
So we've already talked about the CSV files.

00:49:51.380 --> 00:49:57.400
And this is the part that I was talking about where you've got the toggle to see the Rust code and the Python code.

00:49:57.400 --> 00:49:59.260
So I think people might appreciate that.

00:49:59.260 --> 00:50:00.340
Parquet files.

00:50:00.340 --> 00:50:04.200
So Parquet files is a more efficient format.

00:50:04.200 --> 00:50:13.680
Maybe talk about using Parquet files versus CSV and why you want to get rid of your CSV and store these intermediate files and then load them.

00:50:13.680 --> 00:50:15.960
Parquet is a really fast CSV reader.

00:50:15.960 --> 00:50:18.000
I really did my best on that one.

00:50:18.400 --> 00:50:27.560
But if you can use Parquet or Arrow IPC because your data is typed, there's no ambiguity upon reading.

00:50:27.560 --> 00:50:28.840
We know which type it is.

00:50:28.840 --> 00:50:29.080
Right.

00:50:29.080 --> 00:50:33.200
Because CSV files, even though it might be representing a date, it's still a string.

00:50:33.200 --> 00:50:34.160
And we need to parse it.

00:50:34.160 --> 00:50:37.280
It's slow to parse it.

00:50:37.540 --> 00:50:44.020
There's also we can just so Parquet interacts really nicely with query optimization.

00:50:44.020 --> 00:50:48.680
So we can select just a single column from the file without touching any of the other columns.

00:50:48.680 --> 00:50:50.240
We can read statistics.

00:50:50.240 --> 00:50:57.500
And so Parquet file can write statistics, which knows, okay, this page has got this maximum value, this minimum value.

00:50:57.500 --> 00:51:04.440
And if you have written a photos query, which says, oh, so only give me the result where the value is larger than this.

00:51:04.440 --> 00:51:09.040
And we see that the statistics say it cannot be in this file.

00:51:09.040 --> 00:51:10.760
We can just skip the whole column.

00:51:10.760 --> 00:51:11.900
We don't have to read.

00:51:11.900 --> 00:51:12.380
Yeah.

00:51:12.380 --> 00:51:12.940
Oh, interesting.

00:51:12.940 --> 00:51:13.560
Wow.

00:51:13.560 --> 00:51:16.480
So there are a lot of optimizations, which.

00:51:16.480 --> 00:51:18.920
So the best work is work you don't have to do.

00:51:18.920 --> 00:51:20.000
And Parquet allows.

00:51:20.440 --> 00:51:20.880
Exactly.

00:51:20.880 --> 00:51:25.760
Or you've done it when you created the file and you never do it again or something like that.

00:51:25.760 --> 00:51:25.980
Yeah.

00:51:25.980 --> 00:51:26.340
Yeah.

00:51:26.340 --> 00:51:29.360
So you've got a read Parquet, a scan Parquet.

00:51:29.360 --> 00:51:32.120
I suppose that's the data frame versus lazy frame.

00:51:32.120 --> 00:51:33.980
And then you also have the ability to write them.

00:51:33.980 --> 00:51:34.700
That's pretty interesting.

00:51:34.700 --> 00:51:36.840
JSON, multiple files.

00:51:36.840 --> 00:51:37.280
Yeah.

00:51:37.280 --> 00:51:37.640
Yeah.

00:51:37.640 --> 00:51:41.700
There's just a whole bunch of how do I, how can I rather, a bunch of neat things.

00:51:41.700 --> 00:51:42.440
What else would you like?

00:51:42.440 --> 00:51:46.880
I think the most important thing I want to touch on is the expression API.

00:51:46.880 --> 00:51:48.900
So that's a bit, you go a bit higher.

00:51:49.420 --> 00:51:51.020
So just follow up.

00:51:51.020 --> 00:51:51.760
Polis expressions.

00:51:51.760 --> 00:51:53.320
They got their own chapter.

00:51:53.320 --> 00:52:01.300
One of the goals of the Polis API is to keep the API service small, but give you a lot of things you can do.

00:52:01.300 --> 00:52:03.340
And that's where the Polis expressions come in.

00:52:03.340 --> 00:52:10.560
So Polis expressions are expressions of what you want to do, which are run and parallelized on a query engine.

00:52:10.560 --> 00:52:12.760
And you can combine them indefinitely.

00:52:12.760 --> 00:52:15.840
So an expression takes a series and produces a series.

00:52:15.840 --> 00:52:19.180
And because the input is the same as the output, you can combine them.

00:52:19.360 --> 00:52:22.300
And as you can see, we can do pretty complicated stuff.

00:52:22.300 --> 00:52:24.660
And you can keep chaining them.

00:52:25.140 --> 00:52:28.740
And this is the same like how I'd like to see it.

00:52:28.740 --> 00:52:31.400
For instance, the Python vocabulary is quite small.

00:52:31.400 --> 00:52:34.700
So we have a while, we have a loop, we have a variable assignment.

00:52:34.700 --> 00:52:39.200
But if you, I think it fits into maybe two pieces of paper.

00:52:39.200 --> 00:52:46.220
But with this, you can write any program you want with the combination of all those, all those, yeah, this vocabulary.

00:52:46.220 --> 00:52:46.700
Yeah.

00:52:46.700 --> 00:52:49.500
And that's what we want to do with the Polis expressions as well.

00:52:49.500 --> 00:52:55.060
So you've got a lot of small building blocks, which can be combined into.

00:52:55.300 --> 00:52:55.540
Yeah.

00:52:55.540 --> 00:52:58.840
So somebody could say, I want to select a column back.

00:52:58.840 --> 00:53:01.080
But then I don't want the actual values.

00:53:01.080 --> 00:53:03.920
I want the unique ones, a uniqueness.

00:53:03.920 --> 00:53:06.160
So if there's duplicate, remove those.

00:53:06.160 --> 00:53:07.340
And then you can do a dot account.

00:53:07.340 --> 00:53:12.220
Then you can add an alias, which gives it a new, which basically defines the column name.

00:53:12.220 --> 00:53:14.460
Yeah, you could read it as, well, it's not named.

00:53:14.460 --> 00:53:16.020
You could read it as an ads.

00:53:16.020 --> 00:53:19.700
So take column names as unique names to in SQL.

00:53:19.700 --> 00:53:21.680
But as is a keyword environment.

00:53:21.680 --> 00:53:22.900
So I'm not allowed to use that.

00:53:22.900 --> 00:53:23.240
Right.

00:53:23.240 --> 00:53:25.200
It means something else, yeah.

00:53:25.200 --> 00:53:26.920
That's interesting.

00:53:26.920 --> 00:53:27.380
Okay.

00:53:27.380 --> 00:53:28.040
Yeah.

00:53:28.040 --> 00:53:35.240
So people, they use these expressions to do lots of transformations and filtering and things like that.

00:53:35.240 --> 00:53:35.560
Yeah.

00:53:35.560 --> 00:53:39.000
So these expressions can be used in a select on different places.

00:53:39.480 --> 00:53:43.340
But the knowledge of expressions extrapolates to different locations.

00:53:43.340 --> 00:53:45.480
So you can do it in a select statement.

00:53:45.480 --> 00:53:46.760
And then you select column name.

00:53:46.760 --> 00:53:49.500
You select this expression and you get a result.

00:53:49.500 --> 00:53:51.740
But you can also do this in a group by aggregation.

00:53:51.740 --> 00:53:53.640
And then the same logic applies.

00:53:53.640 --> 00:53:57.240
It runs on the same engine and we make sure everything is consistent.

00:53:57.240 --> 00:54:01.960
And this is really powerful because it's so expressive.

00:54:01.960 --> 00:54:05.300
People don't have to use custom apply with lambda.

00:54:05.300 --> 00:54:07.800
Because when you use a lambda, it's a black box to us.

00:54:08.120 --> 00:54:11.000
It will be slow because it's Python and we don't know what happens.

00:54:11.000 --> 00:54:13.460
So a lambda is, it will be slow.

00:54:13.460 --> 00:54:15.660
It will kill parallelization because it deals.

00:54:15.660 --> 00:54:18.500
But yeah, a lambda is three times that.

00:54:18.500 --> 00:54:19.180
Right.

00:54:19.180 --> 00:54:24.220
It gets in the way of a lot of your optimizations and a lot of your speed is there.

00:54:24.220 --> 00:54:28.060
That's why we want to make this expression API very complete.

00:54:28.060 --> 00:54:30.240
So you don't need them as much.

00:54:30.240 --> 00:54:30.440
Yeah.

00:54:30.440 --> 00:54:33.060
So people are wanting to get this, get seriously into this.

00:54:33.060 --> 00:54:35.660
They should check out chapter three expressions, right?

00:54:35.660 --> 00:54:36.760
And just go through there.

00:54:36.760 --> 00:54:44.540
Probably, especially, you know, sort of browse through the Python examples that they can see where, go back and see what they need to learn more about.

00:54:44.660 --> 00:54:46.420
But it's a very interesting API.

00:54:46.420 --> 00:54:48.820
The speed is very compelling.

00:54:48.820 --> 00:54:50.620
I think it's a cool project.

00:54:50.620 --> 00:54:52.520
Like I said, how many people we got here?

00:54:52.520 --> 00:54:54.360
13,000 people using it already.

00:54:54.360 --> 00:54:56.240
So that's a big community.

00:54:56.240 --> 00:54:56.580
Yeah.

00:54:56.640 --> 00:55:04.520
So if you're interested in project, we have a discord where, where you can chat with us and ask questions and see how you can best do things.

00:55:04.520 --> 00:55:05.680
It's pretty active there.

00:55:05.680 --> 00:55:05.880
Cool.

00:55:05.880 --> 00:55:07.860
The discord's linked right off the homepage.

00:55:07.860 --> 00:55:08.900
So that's awesome.

00:55:08.900 --> 00:55:09.860
People can find it there.

00:55:09.860 --> 00:55:10.800
Contributions.

00:55:10.800 --> 00:55:12.020
People want to make contributions.

00:55:12.020 --> 00:55:15.520
I'm sure you're willing to accept PRs and other feedback.

00:55:15.520 --> 00:55:24.100
Before you put in a really large PR, please first open an issue with a, with a, with a, to start the discussion.

00:55:24.280 --> 00:55:26.320
This is, this contribution is welcome.

00:55:26.320 --> 00:55:30.540
And we also have a few getting started good for new contributors.

00:55:30.540 --> 00:55:31.280
Okay.

00:55:31.280 --> 00:55:31.640
Yes.

00:55:31.640 --> 00:55:35.740
You've, you've tagged or labeled some of the issues as look here.

00:55:35.740 --> 00:55:37.680
If you want to get, get into this.

00:55:37.680 --> 00:55:37.840
Yeah.

00:55:37.840 --> 00:55:45.740
I must say, I think we're an interesting project to, to contribute to because we're, you can, it's not, not everything is set in stone.

00:55:45.740 --> 00:55:49.300
So there are still places where you can play.

00:55:49.300 --> 00:55:50.060
I'm not sure.

00:55:50.060 --> 00:55:52.320
There's still interesting work to be done.

00:55:52.380 --> 00:55:55.300
It's not completely 100% polished.

00:55:55.300 --> 00:55:56.500
Yeah.

00:55:56.500 --> 00:55:57.060
Yeah.

00:55:57.060 --> 00:55:57.920
On the periphery.

00:55:57.920 --> 00:55:59.020
Yeah.

00:55:59.020 --> 00:55:59.540
Yeah.

00:55:59.540 --> 00:55:59.820
Yeah.

00:55:59.820 --> 00:56:00.220
Very cool.

00:56:00.220 --> 00:56:02.500
Let's wrap it up with a comment from the audience here.

00:56:02.500 --> 00:56:04.760
Ajit says, excellent content guys.

00:56:04.760 --> 00:56:08.380
It certainly helps me kickstart my journey from pandas to pullers.

00:56:08.380 --> 00:56:09.080
Awesome.

00:56:09.080 --> 00:56:09.740
Awesome.

00:56:09.740 --> 00:56:11.040
Glad, glad to help.

00:56:11.040 --> 00:56:11.740
I'm sure it will.

00:56:11.740 --> 00:56:12.940
Many people do that.

00:56:12.940 --> 00:56:15.920
So Richie, let's close it out with final call action.

00:56:16.020 --> 00:56:17.400
People are interested in this project.

00:56:17.400 --> 00:56:19.780
They want to start playing and learning pullers.

00:56:19.780 --> 00:56:23.100
Maybe try it out on some of their code that is hand us at the moment.

00:56:23.100 --> 00:56:23.760
What do they do?

00:56:23.760 --> 00:56:26.980
I'd recommend if you have a new project, just start in pullers.

00:56:26.980 --> 00:56:34.800
Because you can also rewrite some pandas, but the most fun experience will just start a new project in pullers.

00:56:34.800 --> 00:56:38.160
Because then you can really enjoy what pandas offers.

00:56:38.160 --> 00:56:39.600
Learn the expression API.

00:56:39.600 --> 00:56:41.720
Learn how you use it declaratively.

00:56:41.720 --> 00:56:45.520
And yeah, then it will be most fun.

00:56:45.520 --> 00:56:45.980
Absolutely.

00:56:45.980 --> 00:56:46.720
Sounds great.

00:56:46.720 --> 00:56:51.540
And like we did point out, it has the to and from hand us data frames.

00:56:51.540 --> 00:56:55.480
So you can work on a section of your code and still have it consistent, right?

00:56:55.480 --> 00:56:57.440
With other parts that have to be handed.

00:56:57.440 --> 00:57:01.480
You can progressively rewrite some performance heavy parts.

00:57:01.480 --> 00:57:06.840
Or I also think pullers is really strict on the schema, on the types.

00:57:06.840 --> 00:57:11.560
It's also if you write any ETL, you will be really happy to do that in pullers.

00:57:11.560 --> 00:57:15.080
Because you can check the schema of a lazy frame before executing it.

00:57:15.080 --> 00:57:18.200
Then you know the vtypes before running the query.

00:57:18.200 --> 00:57:23.700
And if the data comes in and it doesn't apply to this schema, you can fail fast.

00:57:23.700 --> 00:57:25.560
And instead of having strange outputs.

00:57:25.560 --> 00:57:26.560
Oh, that's interesting.

00:57:26.560 --> 00:57:31.000
Because you definitely don't want zero when you expected something else.

00:57:31.160 --> 00:57:33.920
Because it could parse or other weird whatever, right?

00:57:33.920 --> 00:57:34.100
Yeah.

00:57:34.100 --> 00:57:35.920
So this was my...

00:57:35.920 --> 00:57:39.160
So missing data in pullers doesn't change the schema.

00:57:39.160 --> 00:57:41.720
So pullers is really...

00:57:41.720 --> 00:57:44.760
The schema is defined by the operations and the data.

00:57:44.760 --> 00:57:47.120
And not by the values in the data.

00:57:47.120 --> 00:57:49.940
So you can statically check your data.

00:57:49.940 --> 00:57:50.580
Excellent.

00:57:50.580 --> 00:57:51.340
All right.

00:57:51.340 --> 00:57:53.500
Well, congratulations on a cool project.

00:57:53.500 --> 00:57:55.100
I'm glad we got to share with everybody.

00:57:55.100 --> 00:57:56.060
Thanks for coming on the show.

00:57:56.060 --> 00:57:56.340
Bye.

00:57:56.340 --> 00:57:56.800
You bet.

00:57:56.800 --> 00:57:57.220
Bye.

00:57:57.220 --> 00:58:00.620
This has been another episode of Talk Python to Me.

00:58:00.840 --> 00:58:02.440
Thank you to our sponsors.

00:58:02.440 --> 00:58:04.060
Be sure to check out what they're offering.

00:58:04.060 --> 00:58:05.480
It really helps support the show.

00:58:05.480 --> 00:58:10.360
TypeI is here to take on the challenge of rapidly transforming a bare algorithm in Python

00:58:10.360 --> 00:58:13.520
into a full-fledged decision support system for end users.

00:58:13.520 --> 00:58:19.100
Get started with TypeI Core and GUI for free at talkpython.fm/typeI.

00:58:19.100 --> 00:58:20.520
T-A-I-P-Y.

00:58:20.520 --> 00:58:25.980
Earn extra income from sharing your software development opinion at user interviews.

00:58:25.980 --> 00:58:30.720
Head over to talkpython.fm/user interviews to participate today.

00:58:30.720 --> 00:58:32.660
Want to level up your Python?

00:58:32.660 --> 00:58:36.700
We have one of the largest catalogs of Python video courses over at Talk Python.

00:58:36.700 --> 00:58:41.880
Our content ranges from true beginners to deeply advanced topics like memory and async.

00:58:41.880 --> 00:58:44.560
And best of all, there's not a subscription in sight.

00:58:44.560 --> 00:58:47.460
Check it out for yourself at training.talkpython.fm.

00:58:47.620 --> 00:58:49.360
Be sure to subscribe to the show.

00:58:49.360 --> 00:58:52.140
Open your favorite podcast app and search for Python.

00:58:52.140 --> 00:58:53.440
We should be right at the top.

00:58:53.440 --> 00:58:58.600
You can also find the iTunes feed at /itunes, the Google Play feed at /play,

00:58:58.600 --> 00:59:02.820
and the direct RSS feed at /rss on talkpython.fm.

00:59:03.700 --> 00:59:06.240
We're live streaming most of our recordings these days.

00:59:06.240 --> 00:59:09.660
If you want to be part of the show and have your comments featured on the air,

00:59:09.660 --> 00:59:14.080
be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

00:59:14.080 --> 00:59:15.920
This is your host, Michael Kennedy.

00:59:15.920 --> 00:59:17.220
Thanks so much for listening.

00:59:17.220 --> 00:59:18.380
I really appreciate it.

00:59:18.380 --> 00:59:20.280
Now get out there and write some Python code.

00:59:20.280 --> 00:59:40.860
I'll see you next time.

00:59:40.860 --> 01:00:10.840
Thank you.

