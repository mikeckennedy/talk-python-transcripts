WEBVTT

00:00:00.001 --> 00:00:02.620
You're using Pydantic and it seems pretty straightforward, right?

00:00:02.620 --> 00:00:07.300
But could you adopt some simple changes to your code that would make it a lot faster and more

00:00:07.300 --> 00:00:12.460
efficient? Chances are you'll find a couple of the tips from Sydney Runkle that will do just that.

00:00:12.460 --> 00:00:20.560
Join us to talk about Pydantic performance tips here on Talk Python, episode 466, recorded June 13th,

00:00:20.560 --> 00:00:27.260
2024. Are you ready for your host, here he is! You're listening to Michael Kennedy on Talk Python

00:00:27.260 --> 00:00:31.340
to me. Live from Portland, Oregon, and this segment was made with Python.

00:00:31.340 --> 00:00:39.840
Welcome to Talk Python to Me, a weekly podcast on Python. This is your host, Michael Kennedy.

00:00:39.840 --> 00:00:44.960
Follow me on Mastodon, where I'm @mkennedy and follow the podcast using @talkpython,

00:00:44.960 --> 00:00:51.320
both on fosstodon.org. Keep up with the show and listen to over seven years of past episodes at

00:00:51.320 --> 00:00:57.220
Talk Python.fm. We've started streaming most of our episodes live on YouTube. Subscribe to our

00:00:57.220 --> 00:01:02.300
YouTube channel over at talkpython.fm/youtube to get notified about upcoming shows

00:01:02.300 --> 00:01:07.380
and be part of that episode. This episode is brought to you by Sentry. Don't let those errors

00:01:07.380 --> 00:01:13.200
go unnoticed. Use Sentry like we do here at Talk Python. Sign up at talkpython.fm/sentry.

00:01:13.200 --> 00:01:18.600
And it's brought to you by Code Comments, an original podcast from Red Hat. This podcast covers

00:01:18.600 --> 00:01:24.740
stories from technologists who've been through tough tech transitions and share how their teams

00:01:24.740 --> 00:01:29.300
survive the journey. Episodes are available everywhere you listen to your podcasts and

00:01:29.300 --> 00:01:35.280
at talkpython.fm/code dash comments. Hey folks, I got something pretty excellent for you.

00:01:35.280 --> 00:01:41.260
PyCharm Professional for six months for free. Over at Talk Python, we partnered with the JetBrains

00:01:41.260 --> 00:01:48.320
team to get all of our registered users free access to PyCharm Pro for six months. All you have to do is

00:01:48.320 --> 00:01:53.460
take one of our courses. That's it. However, do note that this is not valid for renewals over at

00:01:53.460 --> 00:01:59.240
JetBrains. Only new users there. And if you're not currently a registered user at Talk Python, well,

00:01:59.240 --> 00:02:05.120
no problem. This offer comes with all of our courses. So even if you just sign up for one of our free

00:02:05.120 --> 00:02:11.460
courses at talkpython.fm, click on courses in the menu, you're in. So how do you redeem it? Once you have

00:02:11.460 --> 00:02:16.340
an account over at Talk Python, then it's super easy. Just visit your account page on Talk Python

00:02:16.340 --> 00:02:21.560
training. And in the details tab, you'll have a code and a link to redeem your six months of PyCharm

00:02:21.560 --> 00:02:26.740
Pro. So why not take a course, even a free one and get six months free of PyCharm.

00:02:26.740 --> 00:02:31.500
Sydney, welcome back to Talk Python To Me. It's awesome to have you here.

00:02:31.500 --> 00:02:34.740
Thank you. Super excited to be here. And yeah, I'm excited for our chat.

00:02:34.740 --> 00:02:39.040
I am too. We're going to talk about Pydantic, one of my very favorite libraries that

00:02:39.040 --> 00:02:46.640
just makes working with Python data, data exchange so, so easy, which is awesome. And it's really

00:02:46.640 --> 00:02:51.720
cool that you're on the Pydantic team these days. Before then, I guess, you know, let's jump back

00:02:51.720 --> 00:02:58.320
just a little bit. A few weeks ago, got to meet up a little bit in Pittsburgh at PyCon. How was PyCon

00:02:58.320 --> 00:02:58.680
for you?

00:02:58.680 --> 00:03:04.340
It was great. So it was my first PyCon experience ever. It was a very, very large conference. So it

00:03:04.340 --> 00:03:09.720
was a cool kind of first introductory conference experience. I had just graduated not even a week

00:03:09.720 --> 00:03:15.900
before. So it was a fun way to kind of roll into full time work and get exposed really to the Python

00:03:15.900 --> 00:03:20.660
community. And it was, it was great to just kind of have a mix of getting to give a talk, getting to

00:03:20.660 --> 00:03:25.340
attend lots of awesome presentations, and then most of all, just like meeting a bunch of really awesome

00:03:25.340 --> 00:03:26.100
people in the community.

00:03:26.100 --> 00:03:33.400
Yeah, I always love how many people you get to meet from so many different places and perspectives

00:03:33.940 --> 00:03:38.980
and it's, it just reminds you the world is really big, but also really small, you know,

00:03:38.980 --> 00:03:41.820
get to meet your friends and new people from all over the place.

00:03:41.820 --> 00:03:46.000
Definitely. I was impressed by the number of like international attendees. I didn't really expect

00:03:46.000 --> 00:03:46.780
that. It was great.

00:03:46.780 --> 00:03:53.320
Yeah, same, same here. All right. Well, maybe a quick introduction for yourself, for those who

00:03:53.320 --> 00:03:57.720
didn't hear your previous episode, and then we'll, we'll talk a bit about this Pydantic library.

00:03:58.120 --> 00:04:03.320
Yeah, sure. Sounds great. So my name is Sydney. I just graduated from the University of Wisconsin.

00:04:03.320 --> 00:04:10.240
Last time I chatted with you, I was still pursuing my degree in computer science and working part time as an

00:04:10.240 --> 00:04:17.200
intern at the company Pydantic, which is kind of founded around the same ideas that inspired the open source tool.

00:04:17.240 --> 00:04:23.440
And now we're building commercial tools and now I've rolled over into full-time work with them, primarily on the open source side.

00:04:23.440 --> 00:04:32.300
So yeah, very excited to kind of be contributing to the open source community, but also getting to help with our commercial tools and development there.

00:04:32.300 --> 00:04:40.200
Yeah. Yeah. Awesome. We'll talk a bit about that later. Super cool to be able to work on open source as a job, as a proper job, right?

00:04:40.200 --> 00:05:06.160
Yeah. It's, it's awesome. It's really unique. I've kind of encouraged lots of people to contribute to open source as kind of a jumpstart into their software development careers, especially like young folks who are looking to get started with things and maybe don't have an internship or that sort of thing set up yet. I think it's a really awesome pipeline for like getting exposed to good code and collaborating with others and that sort of thing. But it's definitely special to get to do and get paid as well.

00:05:06.580 --> 00:05:18.960
Indeed. So it's, it's a little bit unbelievable to me, but I'm sure that it is true that there are folks out there listening to the podcast that are like, Pydantic, maybe you've heard of that. What is, what is this Pydantic thing?

00:05:18.960 --> 00:05:21.380
Yeah. What is Pydantic?

00:05:21.380 --> 00:05:37.800
So Pydantic is the leading data validation library for Python. And so Pydantic uses type hints, which are optional in Python, but kind of generally more and more encouraged to enforce constraints on data and kind of validate data structures, etc.

00:05:37.800 --> 00:05:56.380
So we're kind of looking at a very simple example together right now, where we're importing things like date time and tuple types from typing. And then kind of the core of Pydantic is you define these classes that inherit from this class called base model that's in Pydantic.

00:05:56.380 --> 00:06:05.020
And that inheritance is what ends up helping you use methods to validate data, build JSON schema, things like that.

00:06:05.020 --> 00:06:15.920
And so in our case, we have this delivery class that has a timestamp, which is of type date time, and then a dimensions tuple, which has two int parts.

00:06:15.920 --> 00:06:26.840
And so then when you pass data into this delivery class to create an instance, Pydantic handles validating that data to make sure that it conforms to those constraints we've specified.

00:06:26.840 --> 00:06:34.500
And so it's really a kind of intermediate tool that you can use for deserialization or loading data and then serialization, dumping data.

00:06:34.500 --> 00:06:41.840
Yeah, it's a thing of beauty. I really love the way that it works. If you've got JSON data, nested JSON data, right?

00:06:41.840 --> 00:06:51.360
If you go to Pydantic.dev slash open source, there's an example of here that we're talking about. And it's got a tuple, but the tuple contains integers, two of them.

00:06:51.360 --> 00:06:58.520
And so if there's a tuple of three things, it'll give you an error. If it's a tuple of a date time in an int, it'll give you an error.

00:06:58.520 --> 00:07:07.560
Like it reaches all the way inside. And, you know, things I guess it compares against. It's a little bit like data classes. Have you done much with data classes and compare them?

00:07:07.560 --> 00:07:12.820
Yeah, that's a great question. So we actually offer support for like Pydantic data classes.

00:07:13.200 --> 00:07:21.380
So I think data classes kind of took the first step of, you know, really supporting using type hints for model fields and things like that.

00:07:21.380 --> 00:07:26.480
And then Pydantic sort of takes the next jump in terms of like validation and schema support.

00:07:26.480 --> 00:07:36.660
And so I think one like very common use case is if you're defining like API request and response models, you can imagine like the JSON schema capabilities come in handy there.

00:07:36.660 --> 00:07:43.180
And just ensuring like the integrity of your API and the data you're dealing with. Very helpful on the validation front.

00:07:43.180 --> 00:07:54.740
Yeah, yeah. Very cool. Okay. Well, I guess one more thing for people who are not super familiar that Pydantic is, I think it's used every now and then.

00:07:54.740 --> 00:08:00.340
Let's check it out on GitHub here. I'm just trying to think of, you know, like some of the main places people have heard of it.

00:08:00.340 --> 00:08:06.780
Obviously, FastAPI, I think is the thing that really launched its popularity in the early days, if I had to guess.

00:08:06.780 --> 00:08:20.260
But if we go over to GitHub, GitHub says that for the open source things that Pydantic is a foundational dependency for 412,644 different projects.

00:08:20.260 --> 00:08:20.920
Yeah.

00:08:20.920 --> 00:08:21.960
That's unbelievable.

00:08:21.960 --> 00:08:30.520
Yeah, it's very exciting. We just got our May download numbers and heard that we have over 200 million downloads in May.

00:08:30.800 --> 00:08:40.560
So that's both version one and version two, but definitely exciting to see how kind of critical of a tool it's become for so many different use cases in Python, which is awesome.

00:08:40.560 --> 00:08:43.160
Yeah, absolutely. It's really, really critical.

00:08:43.160 --> 00:08:52.620
And I think we should probably talk a little bit about Pydantic v1, v2 as a way to get into the architecture conversation, right?

00:08:52.620 --> 00:08:57.460
That was a big thing. I talked to Samuel Colvin maybe a year ago or so, I would imagine.

00:08:57.460 --> 00:09:01.120
I think around PyCon, I think we did actually at PyCon last year as well.

00:09:01.120 --> 00:09:08.960
Yeah, for sure. So a lot of the benefit of using Pydantic is we promise some great performance.

00:09:08.960 --> 00:09:14.160
And a lot of those performance gains came during our jump from v1 to v2.

00:09:14.560 --> 00:09:23.360
So v1 was written solely in Python. We had some compiled options, but really it was mostly Pythonic data validation.

00:09:23.360 --> 00:09:27.820
Or I say Pythonic, it's always Pythonic, but data validation done solely in Python.

00:09:28.740 --> 00:09:34.760
And the big difference with v2 is that we rewrote kind of the core of our code in Rust.

00:09:34.760 --> 00:09:36.420
And so Rust is much faster.

00:09:36.420 --> 00:09:44.380
And so depending on what kind of code you're running, v2 can be anywhere from two to 20 times faster in certain cases.

00:09:45.200 --> 00:09:50.020
So right now we still have this Python wrapper around everything in v2.

00:09:50.020 --> 00:09:55.140
But then, and that's kind of used to define schemas for models and that sort of thing.

00:09:55.140 --> 00:10:01.680
And then the actual validation and serialization logic occurs in Pythonic core in Rust.

00:10:02.040 --> 00:10:08.500
Right. So I think the team did a really good job to make this major change, this major rewrite,

00:10:08.500 --> 00:10:15.460
and split the whole monolithic thing into a Pythonic core, Pythonic itself, which is Python-based,

00:10:15.460 --> 00:10:18.980
in a way that didn't break too many projects, right?

00:10:18.980 --> 00:10:20.220
Yeah, that was the goal.

00:10:20.220 --> 00:10:27.500
You know, every now and then there are breaking changes that I think are generally a good thing for the library moving forward, right?

00:10:27.500 --> 00:10:32.500
Like hopefully whenever we make a breaking change, it's because it's leading to a significant improvement.

00:10:32.500 --> 00:10:35.460
But we definitely do our best to avoid breaking changes.

00:10:35.460 --> 00:10:44.820
And certainly someday we'll launch a v3 and hopefully that'll be an even more seamless transition for v2 users to v3 users.

00:10:44.820 --> 00:10:48.920
Yeah, I would imagine that the switch to Rust probably, that big rewrite,

00:10:48.920 --> 00:10:54.660
it probably caused a lot of thoughts of reconsidering, how are we doing this?

00:10:54.820 --> 00:10:58.140
Or now that it's over in Rust, maybe it doesn't make sense this way or whatever.

00:10:58.140 --> 00:11:03.100
Yeah. And I think just kind of, you know, we got a lot of feedback and usage of Pythonic v1,

00:11:03.100 --> 00:11:10.260
so tried to do our best to incorporate all that feedback into a better v2 version in terms of both APIs and performance and that sort of thing.

00:11:10.260 --> 00:11:11.320
Sure, sure.

00:11:11.320 --> 00:11:15.900
John out in the audience asks, how do the team approach thread safety with this?

00:11:15.900 --> 00:11:18.900
So Rust can be multiple threads, easy.

00:11:18.900 --> 00:11:23.820
Python, not so much really, although maybe soon with free-threaded Python.

00:11:24.160 --> 00:11:25.300
Yeah, that's a good question.

00:11:25.300 --> 00:11:29.480
So our kind of Rust guru on the team is David Hewitt.

00:11:29.480 --> 00:11:35.060
And he's very in the know about all of the multi-threading and things happening on the Rust side of things.

00:11:35.060 --> 00:11:37.800
I myself have some more to learn about that, certainly.

00:11:37.800 --> 00:11:44.760
But I think in general, kind of our approach is that Rust is quite type-safe, both performant and type-safe,

00:11:44.760 --> 00:11:46.980
which is great and memory-safe as well.

00:11:48.240 --> 00:11:50.020
And I think most of our...

00:11:50.020 --> 00:11:56.920
I'll talk a little bit later about some, like, parallelization and vectorization that we're looking at for performance improvements.

00:11:56.920 --> 00:12:04.940
But in terms of safety, I think if you have any questions, feel free to open an issue on the Pydantic core repo and get a conversation going with David Hewitt.

00:12:05.200 --> 00:12:06.560
I would imagine it's not...

00:12:06.560 --> 00:12:08.420
You guys haven't had to do too much with it.

00:12:08.420 --> 00:12:17.400
just that Python currently, but soon, but currently doesn't really let you do much true multi-threading because of the GIL.

00:12:17.400 --> 00:12:19.200
But the whole...

00:12:19.200 --> 00:12:20.200
I think, you know...

00:12:20.200 --> 00:12:27.080
Yeah, I think Python 3.13 is going to be crazy with free-threaded Python, and it's going to be interesting to see how that evolves.

00:12:27.080 --> 00:12:27.640
Yep.

00:12:27.640 --> 00:12:28.020
Anyway.

00:12:28.020 --> 00:12:36.740
Yeah, I know we definitely do some jumping through hoops and just, you know, having to be really conscious of stuff with the GIL in Pydantic core and Py03.

00:12:36.740 --> 00:12:43.560
And Py03 is kind of the library that bridges Python and Rust, and so it's heavily used in Pydantic core, as you can imagine.

00:12:43.560 --> 00:12:45.940
So I'm excited to see what changes might look like there.

00:12:45.940 --> 00:12:46.880
Yeah, same.

00:12:46.880 --> 00:12:52.720
All right, well, let's jump into the performance because you're here to tell us all about Pydantic performance tips.

00:12:52.720 --> 00:12:53.980
And you got a whole bunch of these.

00:12:53.980 --> 00:12:55.260
Did you give this talk at PyCon?

00:12:55.260 --> 00:12:56.540
I did, partially.

00:12:56.540 --> 00:12:58.680
It's a little bit different, but some of the tips are the same.

00:12:58.680 --> 00:13:00.800
I don't think the videos are out yet, are they?

00:13:00.800 --> 00:13:03.700
As the time of recording on June 13th.

00:13:03.700 --> 00:13:04.520
Yeah.

00:13:04.520 --> 00:13:06.460
No, I actually checked a couple of minutes ago.

00:13:06.460 --> 00:13:10.800
I was like, I said one thing during my talk that I wanted to double check, but the videos are not out yet.

00:13:10.800 --> 00:13:12.180
No, I'm really excited.

00:13:12.180 --> 00:13:13.140
There's going to be a bunch.

00:13:13.140 --> 00:13:16.360
There was actually a bunch of good talks, including yours and some others.

00:13:16.500 --> 00:13:18.040
I want to watch, but they're not out yet.

00:13:18.040 --> 00:13:18.860
All right.

00:13:18.860 --> 00:13:21.580
Let's jump into Pydantic performance.

00:13:21.580 --> 00:13:22.800
Where should we start?

00:13:22.800 --> 00:13:25.400
I can start on the slideshow if we want.

00:13:25.400 --> 00:13:26.300
Yeah, let's do that.

00:13:26.300 --> 00:13:26.880
Awesome.

00:13:27.680 --> 00:13:37.720
So, yeah, I think kind of the categories of performance tips that we're going to talk about here kind of have some like fast one-liner type performance tips that you can implement in your own code.

00:13:37.720 --> 00:13:58.540
And then kind of the meat of the, like, how do I improve performance in my, in my, you know, application that uses Pydantic, we're going to talk a bit about discriminated unions, also called tag unions, and then kind of finally talk about on our end of the development, how are we continuously improving performance, you know, Pydantic internals wise, etc.

00:13:58.540 --> 00:14:03.440
Sure. Do you have the equivalent of unit tests for performance?

00:14:03.440 --> 00:14:04.700
Yeah, we do.

00:14:04.700 --> 00:14:05.440
Okay.

00:14:05.560 --> 00:14:10.140
We use a library called codspeed that I'm excited to touch on a bit more later.

00:14:10.140 --> 00:14:12.140
Yeah, all right. Let's talk about that later. Perfect.

00:14:12.140 --> 00:14:26.000
Yeah, sure thing. So I have this slide up right now, just kind of talking about why people use Pydantic. We've already covered some of these, but just kind of as a general recap, it's powered by type hints. And one of our biggest promises is speed.

00:14:26.000 --> 00:14:37.520
We also have these other great features like JSON schema compatibility and documentation comes in particularly handy when we talk about APIs, you know, support for custom validation and serialization logic.

00:14:37.580 --> 00:14:51.160
And then as we saw with the GitHub repository observations, a very robust ecosystem of libraries and other tools that use and depend on Pydantic that leads to this kind of extensive and large community, which is really great.

00:14:52.000 --> 00:14:57.440
But this all kind of lies on the foundation of like Pydantic is easy to use and it's very fast.

00:14:57.440 --> 00:14:58.080
Yeah.

00:14:58.080 --> 00:14:58.920
So let's talk some more.

00:14:58.920 --> 00:15:08.220
And this, yeah, well, the speed is really interesting in the multiplier that you all have for basically a huge swath of the Python ecosystem, right?

00:15:08.220 --> 00:15:11.860
We just saw the 412,000 things that depend on Pydantic.

00:15:11.860 --> 00:15:17.460
Well, a lot of those, their performance depends on Pydantic's performance as well, right?

00:15:17.460 --> 00:15:18.600
Yeah, certainly.

00:15:18.600 --> 00:15:25.060
Yeah, it's nice to have such a large ecosystem of folks to also, you know, contribute to the library as well, right?

00:15:25.140 --> 00:15:31.020
Like, you know, because other people are dependent on our performance, the community definitely becomes invested in it as well, which is great.

00:15:31.020 --> 00:15:37.840
This portion of Talk Python is brought to you by OpenTelemetry support at Sentry.

00:15:37.840 --> 00:15:50.020
In the previous two episodes, you heard how we use Sentry's error monitoring at Talk Python and how distributed tracing connects errors, performance and slowdowns and more across services and tiers.

00:15:50.380 --> 00:15:56.640
But you may be thinking, our company uses OpenTelemetry, so it doesn't make sense for us to switch to Sentry.

00:15:56.640 --> 00:16:01.200
After all, OpenTelemetry is a standard and you've already adopted it, right?

00:16:01.200 --> 00:16:08.960
Well, did you know, with just a couple of lines of code, you can connect OpenTelemetry's monitoring and reporting to Sentry's backend.

00:16:08.960 --> 00:16:16.700
OpenTelemetry does not come with a backend to store your data, analytics on top of that data, a UI or error monitoring.

00:16:16.700 --> 00:16:21.700
And that's exactly what you get when you integrate Sentry with your OpenTelemetry setup.

00:16:21.700 --> 00:16:23.220
Don't fly blind.

00:16:23.220 --> 00:16:26.160
Fix and monitor code faster with Sentry.

00:16:26.160 --> 00:16:30.240
Integrate your OpenTelemetry systems with Sentry and see what you've been missing.

00:16:30.240 --> 00:16:35.380
Create your Sentry account at talkpython.fm/sentry dash telemetry.

00:16:35.820 --> 00:16:39.400
And when you sign up, use the code TALKPYTHON, all caps, no spaces.

00:16:39.400 --> 00:16:46.820
It's good for two free months of Sentry's business plan, which will give you 20 times as many monthly events as well as other features.

00:16:46.820 --> 00:16:49.660
My thanks to Sentry for supporting Talk Python.

00:16:49.660 --> 00:16:55.520
But yeah, so kind of as that first category, we can chat about some basic performance tips.

00:16:55.520 --> 00:17:01.160
And I'll do my best here to kind of describe this generally for listeners who maybe aren't able to see the screen.

00:17:01.160 --> 00:17:04.440
So when you are validating...

00:17:04.440 --> 00:17:06.440
Can we share your slideshow later with the audience?

00:17:06.440 --> 00:17:07.440
Can we put it in the show notes?

00:17:07.440 --> 00:17:08.680
Yeah, yeah, absolutely.

00:17:08.680 --> 00:17:10.560
Okay, so people want to go back and check it out.

00:17:10.560 --> 00:17:11.920
But yeah, we'll describe it for everyone.

00:17:11.920 --> 00:17:12.460
Go ahead.

00:17:12.580 --> 00:17:24.620
So when you're validating data in Pydantic, you can either validate Python objects or like dictionary type data, or you can validate JSON formatted data.

00:17:24.620 --> 00:17:40.340
And so one of these kind of like one liner tips that we have is to use our built in model validate JSON method instead of calling this our model validate method and then separately loading the JSON data with the standard lib JSON package.

00:17:40.720 --> 00:17:50.880
And the reason that we recommend that is one of the like crux of the general performance patterns that we try to follow is not materializing things in Python when we don't have to.

00:17:50.880 --> 00:17:55.180
So we've already mentioned that our core is written in Rust, which is much faster than Python.

00:17:55.180 --> 00:18:01.260
And so with our model validate JSON built in method, whenever you pass in that string, we send it right to Rust.

00:18:01.260 --> 00:18:07.280
Whereas if you do the JSON loading by yourself, you're going to like materialize Python object and then have to send it over.

00:18:07.280 --> 00:18:08.260
Right.

00:18:08.260 --> 00:18:17.360
And so you're going to be using the built in JSON load S, which will then or load or whatever, and then it'll pull that in, turn it into a Python dictionary.

00:18:17.360 --> 00:18:23.900
Then you take it and try to convert that back to a Rust data structure and then validate it in Rust.

00:18:23.900 --> 00:18:25.580
That's where all the validation lives anyway.

00:18:25.580 --> 00:18:27.680
So just get out of the way, right?

00:18:27.680 --> 00:18:28.300
Exactly.

00:18:28.300 --> 00:18:28.620
Yep.

00:18:28.620 --> 00:18:30.920
It's like skip the Python step if you can.

00:18:30.920 --> 00:18:31.180
Right.

00:18:31.480 --> 00:18:36.440
And I will note there is one exception here, which is I mentioned we support custom validation.

00:18:36.440 --> 00:18:47.160
If you're using what we call like before and wrap validators that do something in Python and then call our internal validation logic and then maybe even do something after.

00:18:47.160 --> 00:18:47.900
It's OK.

00:18:47.900 --> 00:18:55.640
You can use model validate and the built in JSON dot load S because you're already kind of guaranteed to be materializing Python objects in that case.

00:18:55.640 --> 00:19:00.000
But for the vast majority of cases, it's great to just go with the built in model validate JSON.

00:19:00.000 --> 00:19:01.580
Yeah, that's really good advice.

00:19:01.580 --> 00:19:03.340
And they seem kind of equivalent.

00:19:03.340 --> 00:19:07.260
But once you know the internals, right, then it's well, maybe it's not exactly.

00:19:07.620 --> 00:19:07.820
Yeah.

00:19:07.820 --> 00:19:19.660
And I think implementing some of these tips is helpful in that if you understand some of the kind of like pydantic architectural context, it can also just help you think more about like, how can I write my pydantic code better?

00:19:19.660 --> 00:19:20.300
Absolutely.

00:19:20.300 --> 00:19:31.540
So the next tip I have here, very easy one liner fix, which is when you're using a type adapter, which is this structure you can use to basically validate one type.

00:19:32.000 --> 00:19:39.400
So we have base models, which we've chatted about before, which is like if you have a model with lots of fields, that's kind of the structure you use to define it.

00:19:39.400 --> 00:19:45.920
Well, type adapter is great if you're like, I just want to validate that this data is a list of integers, for example, as we're seeing on the screen.

00:19:45.920 --> 00:19:46.680
Right.

00:19:46.680 --> 00:19:48.760
Because let me give people an idea.

00:19:48.760 --> 00:19:53.300
Like if you accept if you've got a JSON, well, just JSON data from wherever.

00:19:53.300 --> 00:19:59.480
But, you know, a lot of times it's coming over an API or it's provided to you as a file and it's not your data you control, right?

00:19:59.500 --> 00:20:00.300
You're trying to validate it.

00:20:00.300 --> 00:20:08.100
You could get a dictionary JSON object that's got curly braces with a bunch of stuff, in which case that's easy to map to a class.

00:20:08.100 --> 00:20:12.440
But if you just have JSON, which is bracket thing, thing, thing, thing, close bracket.

00:20:12.440 --> 00:20:15.680
Well, how do you have class that represents a list?

00:20:15.680 --> 00:20:19.320
Like it gets really tricky, right, to be able to understand.

00:20:19.320 --> 00:20:21.120
You can't model that with classes.

00:20:21.120 --> 00:20:23.900
And so you all have this type adapter thing, right?

00:20:23.900 --> 00:20:25.820
That's what the role plays generally.

00:20:25.820 --> 00:20:26.220
Is that right?

00:20:26.560 --> 00:20:26.740
Yeah.

00:20:26.740 --> 00:20:29.840
And I think it's also really helpful in a testing context.

00:20:29.840 --> 00:20:38.380
Like, you know, when we want to check that our validation behavior is right for one type, there's no reason to go like build an entire model.

00:20:38.380 --> 00:20:43.420
If you're really just validating against one type or structure, type adapter is great.

00:20:43.420 --> 00:20:50.440
And so kind of the advice here is you only want to initialize your type adapter object once.

00:20:50.440 --> 00:20:58.720
And the reason behind that is we build a core schema in Python and then attach that to a class or type adapter, etc.

00:20:59.180 --> 00:21:11.720
And so if you can, you know, not build that type adapter within your loop, but instead of instead do it right before or not build it, you know, in your function, but instead outside of it, then you can avoid building the core schema over and over again.

00:21:11.720 --> 00:21:12.220
Yeah.

00:21:12.560 --> 00:21:19.260
So basically what you're saying is that the type adapter that you create might as well be a singleton because it's stateless, right?

00:21:19.260 --> 00:21:21.000
Like it doesn't store any data.

00:21:21.000 --> 00:21:24.180
It's kind of slightly expensive to create relatively.

00:21:25.040 --> 00:21:33.180
And so if you had a function that was called over and over again and that function had a loop and inside the loop, you're creating the type adapter, that'd be like worst case scenario almost, right?

00:21:33.180 --> 00:21:34.260
Yeah, exactly.

00:21:34.260 --> 00:21:37.780
And I think this kind of goes along with like general best programming tips, right?

00:21:37.780 --> 00:21:41.740
Which is like, if you only need to create something once, do that once.

00:21:41.740 --> 00:21:42.440
Exactly.

00:21:42.440 --> 00:21:49.280
You know, a parallel that maybe goes way, way back in time could be like a compiled regular expression.

00:21:49.280 --> 00:21:50.260
You know, right.

00:21:50.260 --> 00:21:52.280
You wouldn't do that over and over in a loop.

00:21:52.340 --> 00:21:57.000
You would just create a regular, the compiled regular expression and you use it throughout your program, right?

00:21:57.000 --> 00:22:00.740
Because it's kind of expensive to do that, but it's fast once it's created.

00:22:00.740 --> 00:22:01.840
Yeah, exactly.

00:22:01.840 --> 00:22:02.960
And funny that you mentioned that.

00:22:02.960 --> 00:22:11.320
I actually fixed a bug last week where we were compiling regular expressions twice when folks like specified that as a constraint on a field.

00:22:11.320 --> 00:22:16.460
So definitely just something to keep in mind and easy to fix or implement with type adapters here.

00:22:16.460 --> 00:22:16.900
Yeah.

00:22:16.900 --> 00:22:17.340
Awesome.

00:22:17.340 --> 00:22:17.900
Okay.

00:22:17.900 --> 00:22:18.780
I like this one.

00:22:18.780 --> 00:22:19.720
That's a good one.

00:22:19.720 --> 00:22:20.180
Yeah.

00:22:21.040 --> 00:22:28.160
So this next tip also kind of goes along with like general best practices, but the more specific you can be with your type hints, the better.

00:22:28.160 --> 00:22:40.340
And so specifically, if you know that you have a list of integers, it's better and more efficient to specify a type hint as a list of integers instead of a sequence of integers, for example.

00:22:40.340 --> 00:22:47.920
Or if you know you have a dictionary that maps strings to integers, specify that type hint as a dictionary, not a mapping.

00:22:48.320 --> 00:22:48.640
Interesting.

00:22:48.640 --> 00:22:48.720
Interesting.

00:22:48.720 --> 00:22:49.060
Yeah.

00:22:49.060 --> 00:22:53.980
So you could import a sequence from the typey module, which is the generic way.

00:22:53.980 --> 00:23:01.340
But I guess you probably have specific code that runs that can validate lists more efficiently than a general iterable type of thing, right?

00:23:01.340 --> 00:23:02.420
Yeah, exactly.

00:23:02.420 --> 00:23:07.500
So in the case of like a sequence versus a list, it's the like square and rectangle thing, right?

00:23:07.560 --> 00:23:10.880
Like a list is a sequence, but there are lots of other types of sequences.

00:23:10.880 --> 00:23:15.440
And so you can imagine for a sequence, we like have to check lots of other things.

00:23:15.440 --> 00:23:23.280
Whereas if you know with certainty, this is going to be a list or it should be a list, then you can have things be more efficient with specificity there.

00:23:23.280 --> 00:23:29.360
Does it make any difference at all whether you use the more modern type specifications?

00:23:29.360 --> 00:23:38.900
Like traditionally, people would say from typing import capital L list, but now you can just say lowercase L list with the built in and no import statement.

00:23:38.900 --> 00:23:42.600
Are those equivalent or is there some minor difference there?

00:23:42.600 --> 00:23:42.920
Do you know?

00:23:43.260 --> 00:23:44.280
Yeah, that's a good question.

00:23:44.280 --> 00:23:50.060
I wouldn't be surprised if there was a minor difference that was more a consequence of like Python version, right?

00:23:50.060 --> 00:23:56.020
Because there's like, I mean, I suppose you could import the old capital L list in a newer Python version.

00:23:56.020 --> 00:24:02.140
But I think the difference is like more related to specificity of a type hint rather than kind of like versioning.

00:24:02.140 --> 00:24:02.600
Yeah.

00:24:02.600 --> 00:24:13.060
If the use of that capital L list made you write an import statement, I mean, it would cause the program to start ever so slightly slower.

00:24:13.060 --> 00:24:14.160
Cause there's another import.

00:24:14.160 --> 00:24:15.380
It's got to run worse.

00:24:15.380 --> 00:24:16.340
It already knows.

00:24:16.340 --> 00:24:16.980
It's already imported.

00:24:16.980 --> 00:24:17.840
What list is.

00:24:17.840 --> 00:24:28.000
You wouldn't believe how many times I get messages on YouTube videos I've done, or even from courses saying, Michael, I don't know what you're doing, but your code is just wrong.

00:24:28.000 --> 00:24:31.600
I wrote lowercase L list bracket something.

00:24:31.600 --> 00:24:35.960
And it said list is not a sub indexable or something like that.

00:24:35.960 --> 00:24:37.380
And look, you've just done it wrong.

00:24:37.380 --> 00:24:38.320
You're going to need to fix this.

00:24:38.460 --> 00:24:44.360
Or, or you're on Python 3.7 or something super old before these new features were added.

00:24:44.360 --> 00:24:47.420
But there's just somewhere in the community.

00:24:47.420 --> 00:24:50.000
We haven't communicated this well.

00:24:50.000 --> 00:24:50.660
I don't know.

00:24:50.660 --> 00:24:52.080
Yeah, for sure.

00:24:52.080 --> 00:24:59.740
I was writing some code earlier today in a meeting and I used the, like, from typing import union and then union X and Y types.

00:24:59.740 --> 00:25:02.640
And my coworker was like, Cindy, use the pipe.

00:25:02.640 --> 00:25:03.440
Like, what are you doing?

00:25:03.440 --> 00:25:05.600
Use the, exactly.

00:25:05.860 --> 00:25:08.740
But here's the thing that was introduced in 3.10, I believe.

00:25:08.740 --> 00:25:11.320
And if people are in 3.9, that code doesn't run.

00:25:11.320 --> 00:25:15.380
Or if they're not familiar with the changes, it's, so there's all these trade-offs.

00:25:15.380 --> 00:25:22.460
I almost feel like it would be amazing to go back for any time there's a security release that releases, say, another 3.7 or something.

00:25:22.460 --> 00:25:31.240
And change the error message to say, this feature only works in the future version of Python rather than some arbitrary error if you're doing it wrong.

00:25:31.240 --> 00:25:32.100
You know, that would be great.

00:25:32.100 --> 00:25:33.020
Yeah, definitely.

00:25:33.020 --> 00:25:36.500
Yeah, some of those errors can be pretty cryptic with the syntax stuff.

00:25:36.500 --> 00:25:37.200
And they can.

00:25:37.200 --> 00:25:37.580
All right.

00:25:37.580 --> 00:25:39.200
So be specific.

00:25:39.200 --> 00:25:43.120
List tuple, not sequence if you know it's a list or a tuple or whatever.

00:25:43.520 --> 00:25:43.740
Yeah.

00:25:43.740 --> 00:25:54.240
And then kind of my last minor tip, which great that you brought up import statements and kind of adding general time to a program, is I don't have a slide for this one.

00:25:54.240 --> 00:26:03.980
But if we go back to the type adapter slide, we talked about the fact that initializing this type adapter builds a core schema and attaches it to that class.

00:26:03.980 --> 00:26:07.700
And that's kind of done at build time, at import time.

00:26:07.700 --> 00:26:09.700
So that's, like, already done.

00:26:10.540 --> 00:26:20.260
And if you really don't want to have that import or, like, build time take a long time, you can use the defer build flag.

00:26:20.260 --> 00:26:24.920
And so what that does is defers the core schema build until the first validation call.

00:26:24.920 --> 00:26:28.080
You can also set that on model config and things like that.

00:26:28.080 --> 00:26:31.960
But basically, the idea here is, like, striving to be lazier, right?

00:26:31.960 --> 00:26:32.680
I see.

00:26:32.680 --> 00:26:38.900
Like, if we don't need to build this core schema right at import time because we want our program to start up quickly, that's great.

00:26:38.900 --> 00:26:43.640
We might have a little bit of a delay on the first validation, but maybe startup time is more important.

00:26:43.640 --> 00:26:46.460
So that's a little bit more of a preferential validation.

00:26:46.460 --> 00:26:50.900
Sorry, preferential performance tip, but available for folks who need it.

00:26:50.900 --> 00:26:51.280
Yeah.

00:26:51.280 --> 00:26:52.620
Like, let me give you an example.

00:26:52.620 --> 00:26:54.940
Let's give people an example where I think this might be useful.

00:26:54.940 --> 00:27:02.340
So in the Talk Python training, the courses site, I think we've got 20,000 lines of Python code, which is probably more at this point.

00:27:02.400 --> 00:27:04.100
I checked a long time ago, but a lot.

00:27:04.100 --> 00:27:05.640
And it's a package.

00:27:05.640 --> 00:27:10.900
And so when you import it, it goes and imports all the stuff to, like, run the whole web app.

00:27:10.900 --> 00:27:14.400
But also little utilities like, oh, I just want to get a quick report.

00:27:14.400 --> 00:27:18.920
I want to just access this model and then use it on something real quick.

00:27:19.180 --> 00:27:23.680
It imports all that stuff so that app startup would be potentially slowed down by this.

00:27:23.680 --> 00:27:31.600
Where if you know, like, only sometimes is that type adapter used, you don't want to necessarily have it completely created until that function gets called.

00:27:31.600 --> 00:27:36.320
So then the first function call might be a little slow, but there'd be plenty of times where maybe it never gets called, right?

00:27:36.680 --> 00:27:37.440
Yep, exactly.

00:27:37.440 --> 00:27:38.400
Yeah, awesome.

00:27:38.400 --> 00:27:38.960
Okay.

00:27:38.960 --> 00:27:39.520
All right.

00:27:39.520 --> 00:27:45.000
So kind of a more complex performance optimization is using tagged unions.

00:27:45.000 --> 00:27:46.120
They're still pretty simple.

00:27:46.120 --> 00:27:48.760
It's just like a little bit more than a one line change.

00:27:48.760 --> 00:27:57.800
So kind of talking about tagged unions, we can go through a basic example why we're using tagged unions in the first place and then some more advanced examples.

00:27:57.800 --> 00:28:04.520
This portion of Talk Python To Me is brought to you by Code Comments, an original podcast from Red Hat.

00:28:04.520 --> 00:28:13.060
You know, when you're working on a project and you leave behind a small comment in the code, maybe you're hoping to help others learn what isn't clear at first.

00:28:13.060 --> 00:28:19.000
Sometimes that Code Comment tells a story of a challenging journey to the current state of the project.

00:28:19.000 --> 00:28:27.620
Code Comments, the podcast, features technologists who've been through tough tech transitions and they share how their teams survived that journey.

00:28:27.620 --> 00:28:32.020
The host, Jamie Parker, is a Red Hatter and an experienced engineer.

00:28:32.020 --> 00:28:40.240
In each episode, Jamie recounts the stories of technologists from across the industry who've been on a journey implementing new technologies.

00:28:40.240 --> 00:28:44.760
I recently listened to an episode about DevOps from the folks at Worldwide Technology.

00:28:44.760 --> 00:28:51.560
The hardest challenge turned out to be getting buy-in on the new tech stack rather than using that tech stack directly.

00:28:51.560 --> 00:28:57.540
It's a message that we can all relate to, and I'm sure you can take some hard-won lessons back to your own team.

00:28:58.100 --> 00:28:59.420
Give Code Comments a listen.

00:28:59.420 --> 00:29:06.780
Search for Code Comments in your podcast player or just use our link, talkpython.fm/code dash comments.

00:29:06.780 --> 00:29:09.300
The link is in your podcast player's show notes.

00:29:09.300 --> 00:29:13.000
Thank you to Code Comments and Red Hat for supporting Talk Python To Me.

00:29:13.000 --> 00:29:16.840
Let's start with what are tag unions because I honestly have no idea.

00:29:16.840 --> 00:29:19.220
I know what unions are, but tagging them, I don't know.

00:29:19.220 --> 00:29:20.220
Yeah, sure thing.

00:29:20.220 --> 00:29:23.640
So tag unions are a special type of union.

00:29:23.640 --> 00:29:25.920
We also call them discriminated unions.

00:29:25.920 --> 00:29:34.580
And they help you specify a member of a model that you can use for discrimination in your validation.

00:29:34.820 --> 00:29:43.540
So what that means is if you have two models that are pretty similar and your field can be either one of those types of models, model X or model Y.

00:29:43.820 --> 00:29:48.720
But you know that there's one tag or discriminator field that differs.

00:29:48.720 --> 00:29:54.440
You can specifically validate against that field and skip some of the other validation, right?

00:29:54.440 --> 00:29:57.380
So I'll move on to an example here in a second.

00:29:57.380 --> 00:30:03.620
But basically, it helps you validate more efficiently because you get to skip validation of some fields.

00:30:03.620 --> 00:30:09.100
So it's really helpful if you have models that have like 100 fields, but one of them is really indicative of what type it might be.

00:30:09.440 --> 00:30:16.020
I see. So instead of trying to figure out like, is it all of this stuff, once you know it has this aspect or that aspect,

00:30:16.020 --> 00:30:20.240
then you can sort of branch on a path and just treat it as one of the elements of the union.

00:30:20.240 --> 00:30:20.720
Is that right?

00:30:20.720 --> 00:30:21.960
Yes, exactly.

00:30:21.960 --> 00:30:31.540
And so one other note about discriminated unions is you specify this discriminator and it can either be a string, like literal type or callable type.

00:30:31.540 --> 00:30:33.180
And we'll look at some examples of both.

00:30:33.180 --> 00:30:37.220
So here's kind of a more concrete example so we can really better understand this.

00:30:37.980 --> 00:30:41.740
So let's say we have a, this is the classic example, right?

00:30:41.740 --> 00:30:43.600
A cat model and a dog model.

00:30:43.600 --> 00:30:45.600
And they both have...

00:30:45.600 --> 00:30:47.140
Cat people, dog people, you're going to start a debate here.

00:30:47.140 --> 00:30:48.320
Exactly, exactly.

00:30:48.320 --> 00:30:51.460
They both have this pet type field.

00:30:51.460 --> 00:30:56.320
And for the cat model, it's a literal that is just the string cat.

00:30:56.320 --> 00:30:59.480
And then for the dog model, it's the literal that's the string dog.

00:30:59.480 --> 00:31:03.220
So it's just kind of a flag on a model to indicate what type it is.

00:31:03.580 --> 00:31:08.120
And you can imagine, you know, in this basic case, we only have a couple of fields attached to each model.

00:31:08.120 --> 00:31:13.140
But maybe this is like data in a, like vet database.

00:31:13.140 --> 00:31:16.180
And so you can imagine like there's going to be tons of fields attached to this, right?

00:31:16.180 --> 00:31:20.340
So it'd be pretty helpful to just be able to look at it and say, oh, the pet type is dog.

00:31:20.340 --> 00:31:23.720
Let's make sure this data is valid for a dog type.

00:31:24.200 --> 00:31:25.980
Also note, we have a lizard in here.

00:31:25.980 --> 00:31:40.200
So what this looks like in terms of validation with Pydantic then is that when we specify this pet field, we just add one extra setting, which says that the discriminator is that pet type field.

00:31:40.900 --> 00:31:48.300
And so then when we pass in data that corresponds to a dog model, Pydantic is smart enough to say, oh, this is a discriminated union field.

00:31:48.300 --> 00:31:53.940
Let me go look for the pet type field on the model and just see what that is.

00:31:53.940 --> 00:31:58.380
And then use that to inform my decision for what type I should validate against.

00:31:58.380 --> 00:31:59.660
Okay, that's awesome.

00:31:59.660 --> 00:32:07.720
So if we don't set the discriminator keyword value in the field for the union, it'll still work, right?

00:32:07.720 --> 00:32:10.380
It just has to be more exhaustive and slow.

00:32:10.760 --> 00:32:11.520
Yeah, exactly.

00:32:11.520 --> 00:32:18.020
So it'll still validate and it'll say, hey, let's take this input data and try to validate it against the cat model.

00:32:18.020 --> 00:32:20.800
And then Pydantic will come back and say, oh, that's not a valid cat.

00:32:20.800 --> 00:32:22.000
Like, let's try the next one.

00:32:22.000 --> 00:32:30.100
Whereas with this discriminated pattern, we can skip right to the dog, which you can imagine helps us skip some of the unnecessary steps.

00:32:30.100 --> 00:32:30.200
Absolutely.

00:32:30.200 --> 00:32:31.420
Okay, that's really cool.

00:32:31.420 --> 00:32:32.620
I had no idea about this.

00:32:32.620 --> 00:32:33.420
Yeah, yeah.

00:32:33.420 --> 00:32:36.480
It's a cool, I'd say like moderate level feature.

00:32:36.880 --> 00:32:41.380
Like I think if you're just starting to use Pydantic, you probably haven't touched discriminated unions much.

00:32:41.380 --> 00:32:46.160
But we hope that it's simple enough to implement that most folks can use it if they're using unions.

00:32:46.160 --> 00:32:47.180
Yeah, that's cool.

00:32:47.180 --> 00:32:49.740
I don't use unions very often, which is probably why.

00:32:49.740 --> 00:32:53.160
Other than, you know, something pipe none, which is, you know, like optional.

00:32:53.160 --> 00:32:54.100
But yeah.

00:32:54.360 --> 00:32:54.760
Yeah.

00:32:54.760 --> 00:32:57.040
If I did, I'll definitely remember this.

00:32:57.040 --> 00:32:57.500
Yeah.

00:32:57.500 --> 00:32:58.820
Alrighty.

00:32:58.820 --> 00:33:02.120
So as I've mentioned, this helps for more efficient validation.

00:33:02.120 --> 00:33:10.240
And then where this really comes and has a lot of value is when you are dealing with lots of nested models or models that have tons of fields.

00:33:10.240 --> 00:33:15.400
So let's say you have a union with like 10 members and each member of the union has 100 fields.

00:33:15.600 --> 00:33:22.060
If you can just do validation against 100 fields instead of 1,000, that would be great in terms of a performance gain.

00:33:22.060 --> 00:33:29.460
And then once again, with nested models, you know, if you can skip lots of those union member validations, also going to boost your performance.

00:33:29.460 --> 00:33:30.480
Yeah, for sure.

00:33:30.480 --> 00:33:40.860
You know, an example where this seems very likely would be using it with Beanie or some other document database where the modeling structure is very hierarchical.

00:33:40.860 --> 00:33:45.060
You end up with a lot of nested sub-identic models in there.

00:33:45.060 --> 00:33:45.500
Yeah.

00:33:45.500 --> 00:33:46.960
Yeah, very much so.

00:33:46.960 --> 00:33:47.380
Cool.

00:33:47.380 --> 00:33:57.840
So as a little bit of an added benefit, we can talk about kind of this improved error handling, which is a great way to kind of visualize why the discriminated union pattern is more efficient.

00:33:58.260 --> 00:34:04.500
So right now we're looking at an example of validation against a model that doesn't use a discriminated union.

00:34:04.500 --> 00:34:07.320
And the errors are not very nice to look at.

00:34:07.320 --> 00:34:13.260
You basically see the errors for every single permutation of the different values.

00:34:13.260 --> 00:34:14.760
And we're using nested models.

00:34:14.760 --> 00:34:16.440
So it's very hard to interpret.

00:34:16.440 --> 00:34:19.100
So we don't have to look at this for too long.

00:34:19.100 --> 00:34:20.700
It's not very nice.

00:34:20.700 --> 00:34:21.780
But if we look at...

00:34:21.780 --> 00:34:26.220
But basically the error message says, look, there's something wrong with the union.

00:34:26.220 --> 00:34:28.920
If it was a string, it is missing these things.

00:34:28.920 --> 00:34:31.320
If it was this kind of thing, it misses those things.

00:34:31.320 --> 00:34:33.180
Like if it was a dog, it misses this.

00:34:33.180 --> 00:34:35.120
If it's a pet, a cat, it misses that.

00:34:35.120 --> 00:34:35.780
Right.

00:34:35.780 --> 00:34:37.840
It doesn't specifically tell you.

00:34:37.840 --> 00:34:38.440
Exactly.

00:34:38.440 --> 00:34:39.340
It's a dog.

00:34:39.340 --> 00:34:42.800
So it's missing like the collar size or whatever, right?

00:34:42.800 --> 00:34:43.380
Right.

00:34:43.380 --> 00:34:43.900
Exactly.

00:34:44.560 --> 00:34:49.220
But then, and I'll go back and kind of explain the discriminated model for this case in a second.

00:34:49.220 --> 00:35:01.780
But if you look at this is the model with the discriminated union instead, we have one very nice error that says, okay, you're trying to, you know, validate this X field and it's the wrong type.

00:35:01.780 --> 00:35:02.120
Right.

00:35:03.080 --> 00:35:08.960
So, yeah, the first example that we were looking at was using string type discriminators.

00:35:08.960 --> 00:35:14.100
So we just had this pet type thing that said, oh, this is a cat or this is a dog, that sort of thing.

00:35:14.100 --> 00:35:21.100
We also offer some more customization in terms of we also allow callable discriminators.

00:35:21.720 --> 00:35:29.820
So in this case, this field can be either a string or this instance of discriminated model.

00:35:29.820 --> 00:35:32.120
So it's kind of a recursive pattern, right?

00:35:32.120 --> 00:35:37.120
And that's where you can imagine the nested structures becoming very complex very easily.

00:35:37.120 --> 00:35:44.480
And we use this kind of callable to differentiate between, you know, which model we should validate against.

00:35:44.480 --> 00:35:46.480
And then we tag each of the cases.

00:35:47.120 --> 00:35:50.100
So a little bit more of a complex application here.

00:35:50.100 --> 00:35:57.380
But once again, when you kind of see the benefit in terms of errors and interpreting things and performance, I think it's generally a worthwhile investment.

00:35:57.380 --> 00:35:58.120
That's cool.

00:35:58.120 --> 00:36:04.360
So if you wanted something like a composite key equivalent of a discriminator, right?

00:36:04.360 --> 00:36:10.560
Like if it has this field and its nested model is of this type, it's one thing versus another.

00:36:10.560 --> 00:36:13.420
Like a free user versus a paying user.

00:36:13.520 --> 00:36:18.520
You might have to look and see their total lifetime value plus that they're a registered user.

00:36:18.520 --> 00:36:18.940
I don't know.

00:36:18.940 --> 00:36:24.300
Something like you could write code that would pull that information out and then discriminate which thing to validate against, right?

00:36:24.300 --> 00:36:25.420
Yeah, exactly.

00:36:25.420 --> 00:36:32.300
Yeah, it definitely comes in handy when you have like, you're like, okay, well, I still want the performance benefits of a discriminated union.

00:36:32.300 --> 00:36:37.540
But I kind of have three fields on each model that are indicative of which one I should validate against, right?

00:36:37.540 --> 00:36:37.840
Yeah.

00:36:37.840 --> 00:36:43.760
And it's like, well, you know, taking the time to look at those three fields over the hundred is definitely worth it.

00:36:43.760 --> 00:36:46.280
Just a little bit of complexity for the developer.

00:36:46.280 --> 00:36:47.700
Yeah, cool.

00:36:47.700 --> 00:36:50.400
One other note here is that discriminated unions are-

00:36:50.400 --> 00:36:52.640
Can we go back really quick on the previous one?

00:36:52.880 --> 00:36:53.600
So I got a quick question.

00:36:53.600 --> 00:36:55.820
So for this, you write a function.

00:36:55.820 --> 00:37:01.380
It's given the value that comes in, which could be a string.

00:37:01.380 --> 00:37:02.920
It could be a dictionary, et cetera.

00:37:02.920 --> 00:37:11.640
Could you do a little bit further performance improvements and add like a functools, LRU cache to cache the output?

00:37:11.640 --> 00:37:16.680
So every time it sees the same thing, if there's repeated data through your validation, it goes, I already know what it is.

00:37:16.680 --> 00:37:17.240
What do you think?

00:37:17.240 --> 00:37:17.360
Yeah.

00:37:17.360 --> 00:37:17.840
Yeah.

00:37:17.840 --> 00:37:19.500
I do think that would be possible.

00:37:19.500 --> 00:37:25.140
That's definitely an optimization we should try out and put in our docs for like the advanced, advanced performance tips.

00:37:25.140 --> 00:37:25.460
Yeah.

00:37:25.460 --> 00:37:36.760
Because if you've got a thousand strings and then, you know, that were like, it's maybe male, female, male, female, male, male, female, like that kind of where the data is repeated a bunch.

00:37:36.760 --> 00:37:37.480
Yeah.

00:37:37.840 --> 00:37:40.820
Then it could just go, yep, we already know that answer.

00:37:40.820 --> 00:37:41.540
Yeah.

00:37:41.540 --> 00:37:42.660
Potentially.

00:37:42.660 --> 00:37:43.080
I don't know.

00:37:43.080 --> 00:37:43.540
Yeah.

00:37:43.540 --> 00:37:44.540
No, definitely.

00:37:44.540 --> 00:37:47.400
And I will say, I don't know if it takes effect.

00:37:47.400 --> 00:37:52.800
I don't think it takes effect with discriminated unions because this logic is kind of in Python.

00:37:52.800 --> 00:38:01.480
But I will say we recently added a like string caching setting because we have kind of our own JSON parsing logic that we use in Pydantic Core.

00:38:01.480 --> 00:38:06.760
And so we added a string caching setting so that you don't have to rebuild the exact same strings every time.

00:38:07.660 --> 00:38:09.060
So that's a nice performance piece.

00:38:09.060 --> 00:38:09.380
Yeah, nice.

00:38:09.380 --> 00:38:10.360
Caching's awesome.

00:38:10.360 --> 00:38:11.120
Until it's not.

00:38:11.120 --> 00:38:12.480
Yeah, exactly.

00:38:12.480 --> 00:38:24.220
So one quick note here is just that discriminated unions are still JSON schema compatible, which is awesome for the case where you're once again defining like API request and responses.

00:38:24.220 --> 00:38:27.160
You want to still have valid JSON schema coming out of your models.

00:38:27.160 --> 00:38:28.120
Yeah, very cool.

00:38:28.120 --> 00:38:33.300
And that might show up in things like open API documentation and stuff like that, right?

00:38:33.300 --> 00:38:34.380
Yep, exactly.

00:38:35.660 --> 00:38:37.180
So I'll kind of skip over this.

00:38:37.180 --> 00:38:39.480
We already touched on the callable discriminators.

00:38:39.480 --> 00:38:43.620
And then I'll leave these slides up here as a reference.

00:38:43.620 --> 00:38:46.360
Again, I don't think this is worth touching in too much detail.

00:38:46.360 --> 00:38:53.660
But just kind of another comment about if you've got nested models, that still works well with discriminated unions.

00:38:53.660 --> 00:38:59.500
So we're still on the pet example, but let's say this time you have a white cat and a black cat model.

00:38:59.500 --> 00:39:03.360
And then you also have your existing dog model.

00:39:03.360 --> 00:39:10.340
You can still create a union of, you know, your cat union is a union of black cat and white cat.

00:39:10.340 --> 00:39:13.500
And then you can union that with the dogs and it still works.

00:39:13.500 --> 00:39:20.980
And once again, you can kind of imagine the exponential blow up that would occur if you didn't use some sort of discriminator here in terms of errors.

00:39:21.440 --> 00:39:22.700
Yeah. Very interesting.

00:39:22.700 --> 00:39:23.440
Okay, cool.

00:39:23.440 --> 00:39:23.940
Yeah.

00:39:23.940 --> 00:39:30.200
So that's kind of all in terms of my recommendations for discriminated union application.

00:39:30.200 --> 00:39:34.140
I would encourage folks who are interested in this to check out our documentation.

00:39:34.140 --> 00:39:35.900
It's pretty thorough in that regard.

00:39:35.900 --> 00:39:38.480
And I think we also have those links attached to the podcast.

00:39:38.920 --> 00:39:39.300
Yeah, definitely.

00:39:39.300 --> 00:39:42.000
And then performance improvements in the pipeline.

00:39:42.000 --> 00:39:44.580
Is this something that we can control from the outside?

00:39:44.580 --> 00:39:47.280
Is this something that you all are just adding for us?

00:39:47.280 --> 00:39:47.380
Yeah.

00:39:47.380 --> 00:39:48.220
Good question.

00:39:48.220 --> 00:39:57.840
This is hopefully maybe not all in the next version, but just kind of things we're keeping our eyes on in terms of requested performance improvements and ideas that we have.

00:39:58.380 --> 00:39:59.960
I'll go a little bit out of order here.

00:39:59.960 --> 00:40:07.580
We've been talking a bunch about core schema and kind of maybe deferring the build of that or just trying to optimize that.

00:40:07.580 --> 00:40:09.160
And that actually happens in Python.

00:40:09.160 --> 00:40:20.480
So one of the biggest things that we're trying to do is effectively speed up the core schema building process so that import times are faster and just Pydantic is more performant in general.

00:40:20.480 --> 00:40:32.260
Well, so one thing that I'd like to ask about, kind of back on the Python side a little bit, suppose I've got some really large document, right?

00:40:32.260 --> 00:40:33.420
Really nested document.

00:40:33.420 --> 00:40:38.200
Maybe I've converted some terrible XML thing into JSON or I don't know, something.

00:40:38.200 --> 00:40:42.100
And there's a little bit of structured schema that I care about.

00:40:42.100 --> 00:40:50.000
And then there's a whole bunch of other stuff that I could potentially create nested models to go to, but I don't really care about validating them.

00:40:50.000 --> 00:40:51.300
It's just whatever it is, it is.

00:40:51.300 --> 00:40:51.880
Yep.

00:40:51.880 --> 00:40:53.600
What if you just said that was a dictionary?

00:40:53.600 --> 00:40:58.640
Would that short circuit a whole bunch of validation and stuff that would make it faster potentially?

00:40:58.640 --> 00:40:59.280
Yeah.

00:40:59.280 --> 00:41:06.080
Can it turn off the validation for a subset of the model if it's really big and deep and you don't really care for that part?

00:41:06.080 --> 00:41:07.240
Yeah, good question.

00:41:07.240 --> 00:41:12.780
So we offer an annotation called skip validation that you can apply to certain types.

00:41:12.780 --> 00:41:14.440
So that's kind of one approach.

00:41:14.880 --> 00:41:25.460
I think in the future, it could be nice to offer kind of a config setting so that you can more easily list features that you want to skip validation for instead of applying those on a field-by-field basis.

00:41:25.460 --> 00:41:34.200
And then the other thing is, if you only define your model in terms of the fields that you really care about from that very gigantic amount of data,

00:41:34.700 --> 00:41:39.740
we will just ignore the extra data that you pass in and pull out the relevant information.

00:41:39.740 --> 00:41:40.540
Right.

00:41:40.540 --> 00:41:40.820
Okay.

00:41:40.820 --> 00:41:41.460
Yeah, good.

00:41:41.460 --> 00:41:43.080
Back to the pipeline.

00:41:43.080 --> 00:41:44.420
Yeah, back to the pipeline.

00:41:45.280 --> 00:41:52.060
So another improvement, we talked a little bit about potential parallelization of things or vectorization.

00:41:52.060 --> 00:41:58.940
One thing that I'm excited to learn more about in the future and that we've started working on is this thing called SIMD in Jitter.

00:41:58.940 --> 00:42:02.540
And that's our JSON iterable parser library that I was talking about.

00:42:02.540 --> 00:42:06.080
And so SIMD stands for single instruction, multiple data.

00:42:06.080 --> 00:42:09.620
Basically means that you can do operations faster.

00:42:10.280 --> 00:42:12.640
And that's with this kind of vectorization approach.

00:42:12.640 --> 00:42:22.140
I certainly don't claim to be an expert in SIMD, but I know that it's improving our validation speeds in the department of JSON parsing.

00:42:22.140 --> 00:42:27.960
So that's something that we're hoping to support for a broader set of architectures going forward.

00:42:27.960 --> 00:42:29.480
Yeah, that's really cool.

00:42:29.480 --> 00:42:32.480
Almost like what Pandas does for Python.

00:42:32.480 --> 00:42:37.820
Instead of looping over and validation and doing something to each piece, you just go, this whole column, multiply it by two.

00:42:37.820 --> 00:42:39.200
Yep, yep, exactly.

00:42:39.200 --> 00:42:43.180
I'm sure not implemented the same, but conceptually the same, just to be clear.

00:42:43.180 --> 00:42:44.540
Yep, very much so.

00:42:44.540 --> 00:42:53.260
And then the other two things in the pipeline that I'm going to mention are kind of related once again to the avoiding materializing things in Python if we can.

00:42:53.260 --> 00:42:58.960
And we're even kind of extending that to avoiding materializing things in Rust if we don't have to.

00:42:59.140 --> 00:43:10.720
So the first thing is when we're parsing JSON in Rust, can we just do the validation as we kind of chomp through the JSON instead of like materializing the JSON as a Rust object and then doing all the validation?

00:43:10.720 --> 00:43:12.780
It's like, can we just do it in one pass?

00:43:13.200 --> 00:43:21.320
Okay, is that almost like generators and iterables rather than loading all into memory at once and then processing it one at a time?

00:43:21.320 --> 00:43:22.480
Yeah, exactly.

00:43:22.480 --> 00:43:31.320
And it's kind of like, do you build the tree and then walk it three times or do you just do your operations every time you add something to the tree?

00:43:31.320 --> 00:43:31.720
Yeah.

00:43:32.380 --> 00:43:37.100
And then the last performance improvement in the pipeline that I'll mention is this thing called FastModel.

00:43:37.100 --> 00:43:38.740
Has not been released yet.

00:43:38.740 --> 00:43:41.540
Hasn't really even been significantly developed.

00:43:41.540 --> 00:43:45.980
But this is cool in that it's really approaching that kind of laziness concept again.

00:43:45.980 --> 00:43:50.900
So attributes would remain in Rust after validation until they're requested.

00:43:50.900 --> 00:44:01.060
So this is kind of along the lines of the defer build logic that we were talking about in terms of like, we're not going to send you the data or perform the necessary operations until they're requested.

00:44:01.060 --> 00:44:01.500
Right.

00:44:01.500 --> 00:44:02.160
Okay.

00:44:02.160 --> 00:44:05.980
Yeah, if you don't ever access the field, then why process all that stuff, right?

00:44:05.980 --> 00:44:07.340
And convert it into Python objects.

00:44:07.340 --> 00:44:08.420
Yeah, exactly.

00:44:08.420 --> 00:44:16.300
But yeah, we're kind of just excited in general to be looking at lots of performance improvements on our end, even after the big V2 speedup.

00:44:16.300 --> 00:44:19.360
Still have lots of other things to work on and improve.

00:44:19.360 --> 00:44:21.320
Yeah, it sure seems like it.

00:44:21.320 --> 00:44:26.040
And if this free-threaded Python thing takes off, who knows?

00:44:26.040 --> 00:44:34.720
Maybe there's even more craziness with parallel processing of different branches of the model alongside each other.

00:44:34.720 --> 00:44:35.200
Yeah.

00:44:35.660 --> 00:44:44.340
So I think this kind of dovetails nicely into like you asked earlier, like, is there a way that we kind of monitor the performance improvements that we're making?

00:44:44.960 --> 00:44:50.780
And we're currently using and getting started with two tools that are really helpful.

00:44:51.140 --> 00:44:55.400
And I can share some PRs if that's helpful and send links after.

00:44:55.400 --> 00:44:56.400
Yeah, sure.

00:44:56.400 --> 00:45:02.460
But one of them is Codspeed, which integrates super nicely with CI and GitHub.

00:45:02.460 --> 00:45:07.200
And it basically runs tests tagged with this, like, benchmark tag.

00:45:08.020 --> 00:45:11.560
And then it'll, you know, run them on main compared to on your branch.

00:45:11.560 --> 00:45:15.820
And then you can see, like, oh, this made my code, you know, 30% slower.

00:45:15.820 --> 00:45:17.800
Like, maybe let's not merge that right away.

00:45:17.800 --> 00:45:25.080
Or conversely, if, you know, there's a 30% improvement on some of your benchmarks, it's really nice to kind of track and see that.

00:45:25.080 --> 00:45:25.620
I see.

00:45:25.620 --> 00:45:30.620
So it looks like it sets up, this is Codspeed.io, right?

00:45:30.620 --> 00:45:31.080
Yeah.

00:45:31.080 --> 00:45:40.740
And then it sets up as, say, a GitHub action as part of your CI, CD, and, you know, probably automatically runs when a PR is open and things along those lines, right?

00:45:40.740 --> 00:45:41.760
Yep, exactly.

00:45:41.760 --> 00:45:42.340
All right.

00:45:42.340 --> 00:45:43.000
I've never heard of this.

00:45:43.000 --> 00:45:48.480
But, yeah, if it just does the performance testing for yourself automatically, why not, right?

00:45:48.480 --> 00:45:49.560
Let it do that.

00:45:49.560 --> 00:45:50.300
Yeah.

00:45:50.300 --> 00:46:04.260
And then, I guess, another tool that I'll mention while talking about kind of our, you know, continuous optimization is a one word for it, is this tool kind of similarly named called CodeFlash.

00:46:04.920 --> 00:46:21.000
So CodeFlash is a new tool that uses LLMs to kind of read your code and then develop potentially more performant versions, kind of analyze those in terms of, you know, is it passed, is this new code passing existing tests?

00:46:21.000 --> 00:46:23.820
Is it passing additional tests that we write?

00:46:23.820 --> 00:46:30.860
And then another great thing that it does is open PRs for you with those improvements and then explain the improvements.

00:46:30.860 --> 00:46:40.060
So I think it's a really pioneering tool in the space, and we're excited to kind of experiment with it more on our PRs and in our repository.

00:46:40.060 --> 00:46:41.180
Okay.

00:46:41.180 --> 00:46:42.320
I love it.

00:46:42.320 --> 00:46:43.880
Just tell me, why is this?

00:46:43.880 --> 00:46:45.340
Why did this slow down?

00:46:45.340 --> 00:46:46.200
Well, here's why.

00:46:46.200 --> 00:46:47.360
Yeah, exactly.

00:46:47.360 --> 00:46:47.940
Yeah.

00:46:47.940 --> 00:46:53.500
And they offer both, like, local runs of the tool and also built-in CI support.

00:46:54.140 --> 00:47:09.860
So those are just kind of two tools that we use to use and are increasingly using to help us kind of check our performance as we continue to develop and really inspire us to, you know, get those green check marks with the, like, performance improved on lots of PRs.

00:47:09.860 --> 00:47:09.920
Yeah.

00:47:09.920 --> 00:47:19.800
The more you can have it where if it passes the automated build, it's just ready to go and you don't have to worry a little bit and keep testing things and then have uncertainty.

00:47:20.260 --> 00:47:20.720
You know that.

00:47:20.720 --> 00:47:21.580
That's nice, right?

00:47:21.580 --> 00:47:22.100
Yeah.

00:47:22.100 --> 00:47:25.180
Because it lets you rest and sleep at night.

00:47:25.180 --> 00:47:25.780
Yeah.

00:47:25.780 --> 00:47:26.880
Most certainly.

00:47:26.880 --> 00:47:35.740
I mean, I said it before, but the number of people who are impacted by Pydantic, I don't know what that number is, but it has to be tremendous.

00:47:35.740 --> 00:47:40.340
Because if there's 400,000 projects that use it, like, think of the users of those projects, right?

00:47:40.340 --> 00:47:44.440
Like, that multiple has got to be big for, you know, I'm sure there's some really popular ones.

00:47:44.440 --> 00:47:45.520
For example, FastAPI.

00:47:45.520 --> 00:47:46.020
Yeah.

00:47:46.020 --> 00:47:46.280
Right?

00:47:46.280 --> 00:47:46.820
Yeah.

00:47:46.820 --> 00:47:47.820
Yeah.

00:47:47.820 --> 00:47:57.520
And it's just nice to, you know, know that there are other companies and tools out there that can help us to, you know, really boost the performance benefits for all those users, which is great.

00:47:58.200 --> 00:47:58.620
All right.

00:47:58.620 --> 00:47:58.780
Yeah.

00:47:58.780 --> 00:47:59.840
That is really cool.

00:47:59.840 --> 00:48:11.380
I think, you know, let's talk about one more performance benefit for people and not so much in how fast your code runs, but in how fast you go from raw data to Pydantic models.

00:48:11.960 --> 00:48:16.720
So, one thing, you probably have seen, we may have even spoken about this before.

00:48:16.720 --> 00:48:19.140
Are you familiar with JSON to Pydantic, the website?

00:48:19.140 --> 00:48:20.440
Yeah, it's a really cool tool.

00:48:20.440 --> 00:48:21.020
Yeah.

00:48:21.020 --> 00:48:21.840
It's such a cool tool.

00:48:21.840 --> 00:48:29.640
And if you've got some really complicated data, like, let's see, I'll pull up some weather data that's in JSON format or something, right?

00:48:29.640 --> 00:48:33.880
Like, if you just take this and you throw it in here, just don't even have to pretty print it.

00:48:33.880 --> 00:48:39.940
It'll just go, okay, well, it looks like what we've got is, you know, this really complicated nested model here.

00:48:40.220 --> 00:48:49.020
And it took, you know, we did this while I was talking, it took 10 seconds from me clicking the API to get a response to having, like, a pretty decent representation here.

00:48:49.020 --> 00:48:50.100
Yeah.

00:48:50.100 --> 00:48:53.260
It's great in terms of, like, developer agility, especially, right?

00:48:53.260 --> 00:48:55.980
It's like, oh, I've, you know, heard of this tool called Pydantic.

00:48:55.980 --> 00:48:56.880
I've seen it in places.

00:48:56.880 --> 00:49:02.160
Like, I don't really know if I want to manually go build all these models for my super complicated JSON data.

00:49:02.160 --> 00:49:04.900
It's like, boom, three seconds, done for you, basically.

00:49:04.900 --> 00:49:05.900
Exactly.

00:49:06.080 --> 00:49:07.720
Like, is it really worth it?

00:49:07.720 --> 00:49:11.520
Because I don't want to have to figure this thing out and figure out all the types.

00:49:11.520 --> 00:49:13.760
And like, no, just paste it in there and see what you get.

00:49:13.760 --> 00:49:15.760
You're going to be, it won't be perfect, right?

00:49:15.760 --> 00:49:20.660
Some things, if they're null in your data, but they could be something that would make them an optional element.

00:49:20.660 --> 00:49:22.440
Like, they could be an integer or they could be null.

00:49:22.440 --> 00:49:25.620
It won't know that it's going to be an integer, right?

00:49:25.620 --> 00:49:26.460
Right.

00:49:26.500 --> 00:49:29.000
So you kind of got to patch it up a tiny bit.

00:49:29.000 --> 00:49:30.920
But in general, I think this is really good.

00:49:30.920 --> 00:49:35.780
And then also, you know, just drop in with your favorite LLM, you know.

00:49:35.780 --> 00:49:38.080
I've been using LLM Studio, which is awesome.

00:49:38.080 --> 00:49:39.180
Nice.

00:49:39.180 --> 00:49:42.380
I heard you talk about that on one of the most recent podcasts, right?

00:49:42.380 --> 00:49:43.420
Yeah, yeah.

00:49:43.420 --> 00:49:43.900
It's super cool.

00:49:43.980 --> 00:49:51.920
You can just download Llama 3 and run it locally with like a, I think my computer can only handle 7 billion parameter models.

00:49:51.920 --> 00:49:53.600
But you know that you get pretty good answers.

00:49:53.600 --> 00:49:59.980
And if you give it a piece of JSON data and you say, convert that to Pydantic, you'll get really good results.

00:49:59.980 --> 00:50:03.620
You have a little more control over than what you just get with this tool.

00:50:03.620 --> 00:50:12.520
But I think those two things, while not about runtime performance, you know, going from I have data till I'm working with Pydantic, that's pretty awesome.

00:50:12.520 --> 00:50:13.620
Yeah, definitely.

00:50:13.800 --> 00:50:23.020
And if any, you know, passionate open source contributors are listening and want to create like a CLI tool for doing this locally, I'm sure that would be very much appreciated.

00:50:23.020 --> 00:50:34.620
I think this is based on something that I don't use, but I think it's based on this data model code generator, which I think might be a CLI tool or a library.

00:50:34.620 --> 00:50:35.200
Let's see.

00:50:35.200 --> 00:50:35.740
Yes.

00:50:35.740 --> 00:50:36.440
Oh, yeah.

00:50:36.440 --> 00:50:37.020
Very nice.

00:50:37.020 --> 00:50:41.080
But here's the problem that, you know, you go and define like a YAML file.

00:50:41.080 --> 00:50:44.020
Like it's, it's just not as easy as like there's a text field.

00:50:44.020 --> 00:50:48.720
I paste in my stuff, but it does technically, technically work, I suppose.

00:50:48.720 --> 00:50:49.800
Yeah.

00:50:49.800 --> 00:50:55.120
But no, definitely the LLM approach or just the basic website approaches are very quick, which is nice.

00:50:55.800 --> 00:50:56.040
Yeah.

00:50:56.040 --> 00:51:03.560
And speaking of LLMs, just really quick, like I feel, you know, you get some of the Python newsletters and other places like, here's the cool new packages.

00:51:03.560 --> 00:51:07.660
A lot of them are like nine out of 10 of them are about LLMs these days.

00:51:07.660 --> 00:51:09.520
I was like, that feels a little over the top to me.

00:51:09.580 --> 00:51:11.860
But I know there's other things going on in the world.

00:51:11.860 --> 00:51:15.740
But, you know, just put your thoughts on LLMs and coding these days.

00:51:15.740 --> 00:51:19.980
I know you write a lot of code and think about it a lot and probably use LLMs somewhere in there.

00:51:19.980 --> 00:51:20.420
Yeah.

00:51:20.420 --> 00:51:21.200
No, for sure.

00:51:22.580 --> 00:51:24.560
I'm pretty optimistic and excited about it.

00:51:24.560 --> 00:51:36.080
I think there's a lot of good that can be done and a lot of productivity boosting to be had from integrating with these tools, both in your like local development environment and also just in general.

00:51:36.080 --> 00:51:39.980
I think sometimes, you know, it's also great in the performance department, right?

00:51:39.980 --> 00:51:47.400
Like we can see with CodeFlash, using LLMs to help you write more performant code can also be really useful.

00:51:47.900 --> 00:52:04.180
And it's been exciting to see some libraries really leverage Pydantic as well in that space in terms of like validating LLM outputs or even using LLM calls in Pydantic validators to validate, you know, data along constraints that are more like language model friendly.

00:52:04.180 --> 00:52:06.660
So, yeah, I'm optimistic about it.

00:52:06.660 --> 00:52:13.420
I still have a lot to learn, but it's cool to see the variety of applications and kind of where you can plug in Pydantic in that process for fun.

00:52:13.420 --> 00:52:14.780
Yeah, I totally agree.

00:52:15.380 --> 00:52:22.920
Right now, the context window, like how much you can give it as information then to start asking questions is still a little bit small.

00:52:22.920 --> 00:52:28.800
Like you can't give it some huge program and say, you know, find me the bugs where this function is called or, you know, whatever.

00:52:28.800 --> 00:52:33.840
And it's like, it doesn't quite understand enough all at once, but that thing keeps growing.

00:52:33.840 --> 00:52:35.780
So eventually someday we'll all see.

00:52:35.780 --> 00:52:36.280
Yep.

00:52:36.520 --> 00:52:36.900
All right.

00:52:36.900 --> 00:52:47.260
Well, let's talk just for a minute, maybe real quick about what you all are doing at Pydantic, the company, rather than Pydantic, the open source library.

00:52:47.260 --> 00:52:48.640
Like what do you all got going on there?

00:52:48.640 --> 00:52:49.580
Yeah, sure.

00:52:49.580 --> 00:52:50.060
Sure.

00:52:50.060 --> 00:52:55.420
So Pydantic has, the company has released our first commercial tool.

00:52:55.420 --> 00:52:58.200
It's called Logfire and it's in open beta.

00:52:58.200 --> 00:53:00.460
So it's an observability platform.

00:53:01.120 --> 00:53:04.760
And we would really encourage anyone interested to try it out.

00:53:04.760 --> 00:53:12.420
It's super easy to get started with, you know, just the basic like pip install of the SDK and then start using it in your code base.

00:53:12.420 --> 00:53:19.020
And then we have the kind of Logfire dashboard where you're going to see the observability and results.

00:53:19.680 --> 00:53:30.700
And so we kind of adopt this like needle in the haystack philosophy where we want this to be a very easy to use observability platform that offers very like Python centric insights.

00:53:30.700 --> 00:53:37.620
And it's this kind of opinionated wrapper around open telemetry, if folks are familiar with that.

00:53:37.620 --> 00:53:47.160
But in kind of the context of performance, one of the great things about this tool is that it offers this like nested logging and profiling structure for code.

00:53:47.660 --> 00:53:54.440
And so it can be really helpful in kind of looking at your code and being like, we don't know where this, you know, performance slowdown is occurring.

00:53:54.440 --> 00:53:59.000
But if we integrate with Logfire, we can see that like very easily in the dashboard.

00:53:59.000 --> 00:54:09.320
Yeah, you have some interesting approaches like specifically targeting popular frameworks like instrument FastAPI or something like that, right?

00:54:09.320 --> 00:54:10.300
Yeah, definitely.

00:54:10.300 --> 00:54:15.840
Trying to kind of build integrations that work very well with FastAPI, other tools like that.

00:54:16.060 --> 00:54:20.900
And even also offering kind of like custom features in the dashboard, right?

00:54:20.900 --> 00:54:27.120
Like if you're looking at, you know, if you're using an observability tool, you're probably advanced enough to want to add some extra things to your dashboard.

00:54:27.120 --> 00:54:31.900
And we're working on supporting that with fast UI, which I know you've chatted with Samuel about as well.

00:54:31.900 --> 00:54:32.960
Yeah, absolutely.

00:54:32.960 --> 00:54:39.560
I got a chance to talk to Samuel about Logfire and some of the behind the scenes infrastructure was really interesting.

00:54:39.560 --> 00:54:43.820
But also speaking of fast UI, you know, I did speak to him.

00:54:43.820 --> 00:54:44.340
When was that?

00:54:44.340 --> 00:54:45.320
Back in February.

00:54:45.320 --> 00:54:48.560
So this is a really popular project.

00:54:48.560 --> 00:54:57.680
And even on the, I was like, quite a few people decided that they were interested in even watching the video on that one, which, yeah.

00:54:57.680 --> 00:54:59.420
Anything with fast UI?

00:54:59.420 --> 00:55:01.600
Sorry, did you say anything with fast UI?

00:55:01.600 --> 00:55:02.360
Yeah.

00:55:02.480 --> 00:55:02.660
Yeah.

00:55:02.660 --> 00:55:07.660
Are you doing anything on the fast UI side or are you on the Pydantic side of things?

00:55:07.660 --> 00:55:08.740
Yeah, good question.

00:55:08.740 --> 00:55:14.200
I've been working mostly on Pydantic, just, you know, larger user base, more feature requests.

00:55:14.200 --> 00:55:22.840
But excited to, I've done a little bit on the fast UI side and excited to kind of brush up on my TypeScript and build that out as a more robust and supported tool.

00:55:22.920 --> 00:55:29.580
I think, especially as we grow as a company and have more open source support in general, that'll be a priority for us, which is exciting.

00:55:29.580 --> 00:55:30.160
Yeah.

00:55:30.160 --> 00:55:32.660
It's an interesting project.

00:55:32.660 --> 00:55:42.680
Basically, a cool way to do JavaScript front ends and React and then plug those back into Python APIs, like FastAPI and those types of things, right?

00:55:42.680 --> 00:55:43.380
So, yeah.

00:55:43.380 --> 00:55:43.780
Yeah.

00:55:44.020 --> 00:55:56.340
And kind of a similarity with fast UI and Logfire, the new tool, is that there's pretty seamless integration with Pydantic, which is definitely going to be one of the kind of core tenets of any products or open source things that we're producing in the future.

00:55:56.340 --> 00:56:05.120
Yeah, I can imagine that's something you want to pay special attention to is like, how well do these things fit together as a whole rather than just, here's something interesting, here's something interesting.

00:56:05.120 --> 00:56:05.640
Yeah.

00:56:05.640 --> 00:56:06.100
Yeah.

00:56:06.100 --> 00:56:06.640
Awesome.

00:56:06.640 --> 00:56:07.260
All right.

00:56:07.260 --> 00:56:11.860
Well, I think that pretty much wraps it up for the time that we have to talk today.

00:56:12.240 --> 00:56:24.120
Let's close it out for us with maybe final call to action for people who are already using Pydantic and they want it to go faster or maybe they could adopt some of these tips.

00:56:24.120 --> 00:56:24.680
What do you tell them?

00:56:24.680 --> 00:56:25.100
Yeah.

00:56:25.100 --> 00:56:36.740
I would say, you know, inform yourself just a little bit about kind of the Pydantic architecture just in terms of like, what is core schema and why are we using Rust for validation and serialization?

00:56:37.060 --> 00:56:43.760
And then that can kind of take you to the next steps of when do I want to build my core schemas based on kind of the nature of my application?

00:56:43.760 --> 00:56:47.720
Is it okay if imports take a little bit longer or do I want to delay that?

00:56:47.720 --> 00:56:50.220
And then take a look at discriminated unions.

00:56:50.940 --> 00:57:00.840
And then maybe if you're really interested in improving performance across your application that supports Pydantic and other things, trying out Logfire and just seeing what sort of benefits you can get there.

00:57:00.840 --> 00:57:01.300
Yeah.

00:57:01.300 --> 00:57:12.880
See where you're spending your time is one of the very, you know, not just focused on Pydantic, but in general, our intuition is often pretty bad for where is your code slow and where is it not slow?

00:57:12.940 --> 00:57:14.260
You're like, that looks really complicated.

00:57:14.260 --> 00:57:15.060
That must be slow.

00:57:15.060 --> 00:57:15.540
Like, nope.

00:57:15.540 --> 00:57:19.980
It's that one call to like some sub module that you didn't realize was terrible.

00:57:19.980 --> 00:57:20.580
Yeah.

00:57:20.580 --> 00:57:21.080
Yeah.

00:57:21.080 --> 00:57:32.580
And I guess that kind of circles back to the like LLM tools and, you know, integrated performance analysis with Codspeed and CodeFlash and even just other LLM tools, which is like, use the tools you have at hand.

00:57:32.680 --> 00:57:40.720
And yeah, sometimes they're better at performance improvements than you might be, or it can at least give you good tips that give you, you know, a launching point, which is great.

00:57:40.720 --> 00:57:41.600
Yeah, for sure.

00:57:41.600 --> 00:57:44.420
Or even good old C profile built right in, right?

00:57:44.420 --> 00:57:46.660
If you really, if you want to do it that way.

00:57:46.660 --> 00:57:47.400
Awesome.

00:57:47.400 --> 00:57:47.780
Yeah.

00:57:47.780 --> 00:57:47.960
All right.

00:57:47.960 --> 00:57:55.840
Well, Sydney, thank you for being back on the show and sharing all these tips and congratulations on all the work you and the team are doing.

00:57:55.840 --> 00:57:58.240
You know, what a success Pydantic is.

00:57:58.240 --> 00:57:58.740
Yeah.

00:57:58.740 --> 00:57:59.740
Thank you so much for having me.

00:57:59.800 --> 00:58:04.640
It was wonderful to get to have this discussion with you and excited that I got to meet you in person at PyCon recently.

00:58:04.640 --> 00:58:05.680
Yeah, that was really great.

00:58:05.680 --> 00:58:06.400
Really great.

00:58:06.400 --> 00:58:08.320
Until next PyCon.

00:58:08.320 --> 00:58:08.840
See you later.

00:58:08.840 --> 00:58:12.500
This has been another episode of Talk Python to Me.

00:58:12.500 --> 00:58:14.320
Thank you to our sponsors.

00:58:14.320 --> 00:58:15.920
Be sure to check out what they're offering.

00:58:15.920 --> 00:58:17.340
It really helps support the show.

00:58:17.340 --> 00:58:19.500
Take some stress out of your life.

00:58:19.500 --> 00:58:25.280
Get notified immediately about errors and performance issues in your web or mobile applications with Sentry.

00:58:25.760 --> 00:58:30.280
Just visit talkpython.fm/sentry and get started for free.

00:58:30.280 --> 00:58:33.880
And be sure to use the promo code talkpython, all one word.

00:58:33.880 --> 00:58:36.860
Code comments and original podcast from Red Hat.

00:58:36.860 --> 00:58:45.540
This podcast covers stories from technologists who've been through tough tech transitions and share how their teams survived the journey.

00:58:46.220 --> 00:58:51.960
Podcasts are available everywhere you listen to your podcasts and at talkpython.fm/code dash comments.

00:58:51.960 --> 00:58:53.260
Want to level up your Python?

00:58:53.260 --> 00:58:57.300
We have one of the largest catalogs of Python video courses over at Talk Python.

00:58:57.300 --> 00:59:02.480
Our content ranges from true beginners to deeply advanced topics like memory and async.

00:59:02.480 --> 00:59:05.140
And best of all, there's not a subscription in sight.

00:59:05.140 --> 00:59:08.040
Check it out for yourself at training.talkpython.fm.

00:59:08.400 --> 00:59:10.160
Be sure to subscribe to the show.

00:59:10.160 --> 00:59:12.940
Open your favorite podcast app and search for Python.

00:59:12.940 --> 00:59:14.240
We should be right at the top.

00:59:14.240 --> 00:59:23.600
You can also find the iTunes feed at /itunes, the Google Play feed at /play, and the direct RSS feed at /rss on talkpython.fm.

00:59:23.600 --> 00:59:26.580
We're live streaming most of our recordings these days.

00:59:26.580 --> 00:59:34.360
If you want to be part of the show and have your comments featured on the air, be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

00:59:35.060 --> 00:59:36.460
This is your host, Michael Kennedy.

00:59:36.460 --> 00:59:37.760
Thanks so much for listening.

00:59:37.760 --> 00:59:38.920
I really appreciate it.

00:59:38.920 --> 00:59:40.820
Now get out there and write some Python code.

00:59:40.820 --> 01:00:01.700
I'll see you next time.

