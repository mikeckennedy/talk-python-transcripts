WEBVTT

00:00:00.001 --> 00:00:05.600
The world of AI is changing fast, and the AI-ML space is a bit out of the ordinary for software developers.

00:00:05.600 --> 00:00:11.700
Typically in software, we can prove that given a certain situation, the code will always behave the same.

00:00:11.700 --> 00:00:14.940
We can point to where and why a decision is made.

00:00:14.940 --> 00:00:17.960
ML isn't like that. We set it up and it takes on a life of its own.

00:00:17.960 --> 00:00:22.200
Regulators and governments are starting to step in and make rules over AI.

00:00:22.200 --> 00:00:24.100
The EU is one of the first to do so.

00:00:24.100 --> 00:00:26.960
That's why it's great to have Ines Montani and Catherine Jarmol,

00:00:27.280 --> 00:00:29.940
both awesome data scientists and EU residents,

00:00:29.940 --> 00:00:35.720
here to give us an overview of the coming regulations and other benefits and pitfalls of the AI-ML space.

00:00:35.720 --> 00:00:41.480
This is Talk Python to Me, episode 351, recorded December 17th, 2021.

00:00:54.860 --> 00:00:58.060
Welcome to Talk Python to Me, a weekly podcast on Python.

00:00:58.060 --> 00:00:59.760
This is your host, Michael Kennedy.

00:00:59.760 --> 00:01:05.960
Follow me on Twitter where I'm @mkennedy, and keep up with the show and listen to past episodes at talkpython.fm.

00:01:05.960 --> 00:01:09.020
And follow the show on Twitter via at Talk Python.

00:01:09.020 --> 00:01:13.180
This episode is brought to you by Sentry and Signal Wire.

00:01:13.180 --> 00:01:15.520
Please check out what they're offering during their segments.

00:01:15.520 --> 00:01:16.840
It really helps support the show.

00:01:16.840 --> 00:01:20.620
Catherine Ines, welcome to Talk Python to Me.

00:01:20.620 --> 00:01:22.200
Hey, it's great to be back.

00:01:22.300 --> 00:01:22.820
Hi, Michael.

00:01:22.820 --> 00:01:23.900
Yeah, it's great.

00:01:23.900 --> 00:01:25.020
Great to have you here.

00:01:25.020 --> 00:01:26.400
You've been here a bunch of times.

00:01:26.400 --> 00:01:27.660
Catherine, have I had you on before?

00:01:27.660 --> 00:01:28.880
Yeah, I think so.

00:01:28.880 --> 00:01:29.840
A while ago now.

00:01:29.840 --> 00:01:32.600
I think so as well, but it's been a really long time, hasn't it?

00:01:32.600 --> 00:01:33.240
Yeah, yeah.

00:01:33.240 --> 00:01:34.420
It's great to have you back.

00:01:34.420 --> 00:01:37.760
This is a very Berlin-focused podcast today.

00:01:37.760 --> 00:01:40.080
Just unrelated.

00:01:40.080 --> 00:01:42.100
Both of you happen to be there, so that's really cool.

00:01:42.100 --> 00:01:45.700
Thank you for taking time out of your evening to be part of the show.

00:01:45.700 --> 00:01:46.160
Of course.

00:01:46.260 --> 00:01:46.420
Yeah.

00:01:46.420 --> 00:01:47.320
All right.

00:01:47.320 --> 00:01:54.540
Well, we're going to talk about machine learning, some of the rules and regulations coming around

00:01:54.540 --> 00:01:55.700
there, especially in Europe.

00:01:55.700 --> 00:01:58.140
We're going to talk about fairness.

00:01:58.200 --> 00:02:03.660
We're going to talk even a little bit about interesting indirect implications like Google,

00:02:03.660 --> 00:02:07.020
or not Google, GitHub Copilot, and these types of things.

00:02:07.020 --> 00:02:11.040
We'll sort of go through the whole ML space and talk about some of these ideas.

00:02:11.040 --> 00:02:15.540
But you both are doing very interesting things in the machine learning space.

00:02:15.540 --> 00:02:17.760
Let's sort of just get a little bit of your background.

00:02:17.760 --> 00:02:20.220
Catherine, tell people a bit about what you're doing these days.

00:02:20.220 --> 00:02:26.240
Yeah, I'm here in Berlin and focused on how do we think about data privacy and data security

00:02:26.240 --> 00:02:27.780
concerns in machine learning.

00:02:27.780 --> 00:02:32.540
So for the past five years, I've worked on the space of how do we think about problems

00:02:32.540 --> 00:02:37.820
like anonymization, differential privacy, as well as how do we think about solutions like

00:02:37.820 --> 00:02:41.580
encrypted learning and building ways that we can learn from encrypted data.

00:02:41.580 --> 00:02:44.440
So it's been really fun and I'm excited.

00:02:44.440 --> 00:02:50.160
But first, first to publicly announce here that I'll be joining ThoughtWorks in January.

00:02:50.160 --> 00:02:51.040
That's awesome.

00:02:51.040 --> 00:02:51.940
Yeah.

00:02:51.940 --> 00:02:57.800
As a principal data scientist, their focus exactly on this problem, which they've been

00:02:57.800 --> 00:03:02.740
noticing here in Europe is how do we think about data privacy and data security problems

00:03:02.740 --> 00:03:04.100
when we think about machine learning.

00:03:04.100 --> 00:03:05.500
It's a growing concern.

00:03:05.500 --> 00:03:07.300
So it should be pretty exciting.

00:03:07.300 --> 00:03:07.680
Yeah.

00:03:07.680 --> 00:03:13.340
A company like ThoughtWorks is one of these companies that works with other companies a lot

00:03:13.340 --> 00:03:15.360
and sort of this consulting side of things.

00:03:15.360 --> 00:03:20.220
And I feel like you can have a pretty wide ranging impact through those channels.

00:03:20.220 --> 00:03:20.760
Yeah.

00:03:20.760 --> 00:03:21.200
Yeah.

00:03:21.200 --> 00:03:27.820
Do you think that being in Germany, there's more focus on these kinds of things in Europe,

00:03:27.820 --> 00:03:32.720
but especially in Germany, it seems like, than a pair, like say in the US?

00:03:32.720 --> 00:03:39.000
Does the US have more of a YOLO attitude towards privacy and machine learning stuff?

00:03:39.000 --> 00:03:39.280
Yeah.

00:03:39.280 --> 00:03:39.300
Yeah.

00:03:39.300 --> 00:03:45.560
I mean, I think just from a regulatory aspect, since we saw a passage of the GDPR, which is

00:03:45.560 --> 00:03:51.500
the big European privacy law in 2018 that it went into effect, we definitely saw kind of

00:03:51.500 --> 00:03:53.920
a growing trend here in Europe.

00:03:53.920 --> 00:03:59.480
Overall, I would say like actually France and the Netherlands have done quite a lot of good

00:03:59.480 --> 00:04:04.660
work, even Ireland at questioning, let's say larger tech usage.

00:04:04.660 --> 00:04:10.580
But the activism, I would say on the ground activism here in Germany from the chaos communication

00:04:10.580 --> 00:04:16.280
club and other types of activists that are here has been quite strong, which is exciting

00:04:16.280 --> 00:04:16.740
to see.

00:04:16.740 --> 00:04:22.800
And therefore, I think leads kind of ends up being in the headlines, maybe a bit more internationally.

00:04:22.800 --> 00:04:23.440
Yeah.

00:04:23.780 --> 00:04:27.140
Also actually a fun fact that the story I always like to tell Americans is that like,

00:04:27.140 --> 00:04:31.760
if you go on Google Street View here in Berlin, it's an awesome, I don't know, time travel

00:04:31.760 --> 00:04:34.560
because the day to day, it's like over 10 years old now.

00:04:34.560 --> 00:04:35.120
So you can be there.

00:04:35.120 --> 00:04:36.520
And Berlin's heavily gendered right now.

00:04:36.520 --> 00:04:40.340
So you can really say, wow, how did my neighborhood look 10, 12 years ago?

00:04:40.340 --> 00:04:41.460
Because Google did it once.

00:04:41.460 --> 00:04:45.160
They never came back because everyone wanted their buildings pixelated.

00:04:45.160 --> 00:04:47.260
And they were like, okay, fuck this.

00:04:47.260 --> 00:04:48.400
Germany's so difficult.

00:04:48.400 --> 00:04:50.260
We're never going to send our cars to here again.

00:04:50.640 --> 00:04:54.160
So, you know, I definitely encourage you to use Google Street View in Berlin.

00:04:54.160 --> 00:04:56.980
It's really fun from like the historical perspective.

00:04:56.980 --> 00:04:57.820
How funny.

00:04:57.820 --> 00:04:58.120
Yeah.

00:04:58.120 --> 00:05:02.160
So you can go and basically say, I want my house fuzzed out.

00:05:02.160 --> 00:05:05.380
So you can't see details about my personal residence.

00:05:05.380 --> 00:05:05.400
Yeah.

00:05:05.400 --> 00:05:06.700
And a lot of buildings will look like that.

00:05:06.700 --> 00:05:06.900
Yeah.

00:05:06.900 --> 00:05:07.320
Yeah.

00:05:07.320 --> 00:05:07.660
Yeah.

00:05:07.660 --> 00:05:11.660
If I went to my place on Google Maps, you can see it evolve over time.

00:05:11.660 --> 00:05:16.260
Like, oh, that's when I still have that other car before it broke down or crashed or whatever.

00:05:17.100 --> 00:05:22.980
And I can sort of judge how old the pictures are by, you know, is it what season is it?

00:05:22.980 --> 00:05:24.360
What's in the driveway?

00:05:24.360 --> 00:05:26.440
Or what's the porch look like?

00:05:26.440 --> 00:05:28.440
What kind of, you know, chairs do we have?

00:05:28.440 --> 00:05:29.780
There's all sorts of detail.

00:05:29.780 --> 00:05:32.200
Like, none of it's obscured, right?

00:05:32.200 --> 00:05:39.720
There's a fun fact that some researchers worked on in the U.S. of could they do the census just via Google Street View?

00:05:40.180 --> 00:05:46.540
And they found there was a heavy correlation between census data and the makes of cars that people had in their driveway.

00:05:46.540 --> 00:05:48.100
Oh, my goodness.

00:05:48.100 --> 00:05:48.720
Wow.

00:05:48.720 --> 00:05:50.260
It's an interesting paper.

00:05:50.260 --> 00:05:50.580
Yeah.

00:05:50.580 --> 00:05:54.080
I think, actually, Timnit Gabrou might have been on that paper as well.

00:05:54.080 --> 00:06:00.380
The kind of very well-known machine learning ethics researcher who's now running her own organization in the space.

00:06:00.380 --> 00:06:02.040
Anyways, it's a really cool paper.

00:06:02.040 --> 00:06:04.780
I'll see if I can find it and send it to you for the show notes, Michael.

00:06:04.780 --> 00:06:05.160
Yeah.

00:06:05.160 --> 00:06:06.300
Yeah, put it in the show notes.

00:06:06.300 --> 00:06:06.580
Awesome.

00:06:06.580 --> 00:06:07.100
All right.

00:06:07.100 --> 00:06:09.080
Well, congratulations on the ThoughtWorks thing.

00:06:09.260 --> 00:06:09.780
That's really cool.

00:06:09.780 --> 00:06:12.460
Ines, how about you tell people about yourself as well?

00:06:12.460 --> 00:06:16.660
It's been almost a year, I think, maybe since I had you on Talk Python.

00:06:16.660 --> 00:06:18.960
Yeah, I think it was the year in review.

00:06:18.960 --> 00:06:20.520
I was in Australia at the time.

00:06:20.520 --> 00:06:21.240
It was summer.

00:06:21.240 --> 00:06:22.520
Exactly.

00:06:22.520 --> 00:06:23.380
Now I'm in Berlin.

00:06:23.380 --> 00:06:24.080
Now it's winter.

00:06:24.080 --> 00:06:24.740
Cool.

00:06:24.740 --> 00:06:25.000
Yeah.

00:06:25.000 --> 00:06:26.720
I'm still the co-founder of Explosion.

00:06:26.720 --> 00:06:32.460
We're probably most known for our open source library at spaCy, which is an open source library for natural language processing.

00:06:33.000 --> 00:06:39.240
And one of the main things people do with our stack is build NLP and machine learning applications.

00:06:39.240 --> 00:06:44.900
We also published an annotation tool called 4G, which allows creating training data for machine learning models.

00:06:44.900 --> 00:06:50.460
And all our work and everything we do is very focused on running things on your own hardware, data privacy.

00:06:50.460 --> 00:06:55.580
And that's also something that's very important and something that we see our own users and customers do.

00:06:55.720 --> 00:07:00.360
So people want to train their own models and actually think about how do I create my data?

00:07:00.360 --> 00:07:02.740
What do I do to make my model good?

00:07:02.740 --> 00:07:04.740
What do I do to make my application work?

00:07:04.740 --> 00:07:09.620
And so this all ties in quite well with other questions about like, okay, what should I do?

00:07:09.620 --> 00:07:12.000
How should I reason about my data?

00:07:12.000 --> 00:07:14.620
Which we also see as a very, very important point.

00:07:14.620 --> 00:07:17.620
And I actually think this can even prevent a lot of problems that we see.

00:07:17.800 --> 00:07:23.580
If you actually just look at your data, think about what do I want my system and my pipeline to do?

00:07:23.580 --> 00:07:25.080
How do I break down my problem?

00:07:25.080 --> 00:07:30.280
And that's kind of, that's exactly what, yeah, the tools we're building, hopefully helping people to do.

00:07:30.280 --> 00:07:31.140
Yeah, fantastic.

00:07:31.140 --> 00:07:34.740
So you have spaCy and then you have some, that's the open source thing.

00:07:34.740 --> 00:07:36.860
You also have some products on top of that, right?

00:07:36.860 --> 00:07:37.600
Yeah, exactly.

00:07:37.600 --> 00:07:38.860
So we have Prodigy.

00:07:38.860 --> 00:07:41.880
If you scroll down a bit here, you'll link at the page.

00:07:41.880 --> 00:07:42.620
Yeah, exactly.

00:07:42.620 --> 00:07:44.300
Prodigy, that's an annotation tool.

00:07:44.300 --> 00:07:52.180
And we're currently working on an extension product for it that is a bit more like a SaaS cloud tool, but has a private cloud aspect.

00:07:52.180 --> 00:07:57.880
So you can bring your own hardware, you can run your code, your data on your own cloud resources.

00:07:57.880 --> 00:08:00.300
So no data, nothing has to leave your service.

00:08:00.300 --> 00:08:03.300
And that's something that people already find very appealing about Prodigy.

00:08:03.300 --> 00:08:04.620
You can just download it, run it.

00:08:04.620 --> 00:08:09.400
It doesn't send everything, anything across over the internet.

00:08:09.400 --> 00:08:11.860
And yeah, that's also what we're going to keep doing.

00:08:11.860 --> 00:08:16.400
Yeah, I love that you all are embracing that because there's such, you know, we'll get into this later.

00:08:16.400 --> 00:08:18.700
Not the first topic, but it's related.

00:08:18.700 --> 00:08:19.920
So I'll just talk a bit about it.

00:08:19.920 --> 00:08:25.960
I really like that you're not sending the people's data back because if they're going to trust your tools,

00:08:25.960 --> 00:08:31.880
they need to know that you're not sharing data that is either part of their competitive advantage

00:08:31.880 --> 00:08:33.840
or they have to protect for privacy reasons.

00:08:33.840 --> 00:08:42.540
I recently started, got into the GitHub Copilot trial and I installed that and it said, or preview, whatever it's called.

00:08:42.540 --> 00:08:51.220
And they said, oh, you just have to accept this agreement where it says, if you generate code from us, we're going to get some analytics.

00:08:51.220 --> 00:08:52.380
I'm like, all right, that's fine.

00:08:52.380 --> 00:08:52.620
Whatever.

00:08:52.620 --> 00:08:55.760
I ask it how to connect to SQLAlchemy because I forgot it'll just tell me.

00:08:55.760 --> 00:08:58.860
Oh, and if you make any edits, we're going to send those back.

00:08:58.860 --> 00:09:00.040
I'm like, wait a minute.

00:09:00.040 --> 00:09:03.240
What if one of the edits has put my AWS access key in there?

00:09:03.240 --> 00:09:08.960
Because it needs to be there or, you know, for a little not thing that I'm going to publish, but it's still going back.

00:09:08.960 --> 00:09:09.120
Right.

00:09:09.140 --> 00:09:11.640
So there's a lot of things and I just uninstalled.

00:09:11.640 --> 00:09:12.320
I'm like, you know what?

00:09:12.320 --> 00:09:17.640
No, this is, this is just too much risk for too little benefit for me in my world.

00:09:17.640 --> 00:09:17.940
Yeah.

00:09:17.940 --> 00:09:20.660
I think we also see a lot of it is also kind of pointless.

00:09:20.660 --> 00:09:25.340
I think there used to be this idea that like, oh, you can just like collect all the data.

00:09:25.340 --> 00:09:28.080
And then at some point you can do some magical AI with it.

00:09:28.080 --> 00:09:31.080
And I think for years, this used to be the classic pitch of every startup.

00:09:31.080 --> 00:09:34.940
Like, I don't know, it's almost used to be what we work pitched in some way.

00:09:34.940 --> 00:09:39.120
It's usually like, oh, we do X and then we collect data and then it's dot, dot, dot.

00:09:39.280 --> 00:09:41.380
And then it's AI and then it's profit.

00:09:41.380 --> 00:09:45.700
And that used to be how people would like map out their business.

00:09:45.700 --> 00:09:49.800
And I think this has changed a bit, but I think you can still see some of the leftovers where,

00:09:49.800 --> 00:09:53.200
you know, companies are like, oh, we might as well get as much data as possible because

00:09:53.200 --> 00:09:55.000
maybe there's something we can do with it.

00:09:55.000 --> 00:09:59.780
And we've always seen working in the field that like, nah, I don't want, I don't want

00:09:59.780 --> 00:10:00.560
new annotations.

00:10:00.560 --> 00:10:00.960
Yeah.

00:10:00.960 --> 00:10:04.040
Like, there's literally no advantage I get from that.

00:10:04.040 --> 00:10:07.220
So I might as well set up the tools so that you can keep them.

00:10:07.220 --> 00:10:07.780
That's perfect.

00:10:08.580 --> 00:10:14.200
And Catherine, Mr. Hypermagnetic says, hi, I thought KJ Amistan was a hip new tech stack.

00:10:14.200 --> 00:10:17.920
No, it's my company name.

00:10:17.920 --> 00:10:19.540
So yeah, yeah, yeah, yeah.

00:10:19.540 --> 00:10:24.480
It's like a good inside joke that lives on many, many decades later.

00:10:24.480 --> 00:10:25.080
I love it.

00:10:25.080 --> 00:10:25.680
All right.

00:10:25.700 --> 00:10:31.420
Well, let's kick things off with some regulation and then we can go and talk more thing about

00:10:31.420 --> 00:10:33.140
maybe some other laws.

00:10:33.140 --> 00:10:35.560
We could talk about things like copilot and other stuff.

00:10:35.560 --> 00:10:41.840
But I did say this was a European focused, at least kickoff to the show here.

00:10:42.040 --> 00:10:48.320
So I think one of the biggest tech stories out of the EU in the last couple of years has got

00:10:48.320 --> 00:10:49.400
to be GDPR, right?

00:10:49.400 --> 00:10:55.260
Like that's, I still heard just yesterday people talking about, well, this because of GDPR and

00:10:55.260 --> 00:10:56.140
because we're doing that.

00:10:56.140 --> 00:10:57.680
And this company is not right.

00:10:57.680 --> 00:11:01.820
There's still just so much awareness of privacy because of GDPR.

00:11:01.920 --> 00:11:06.440
And I do think there are some benefits and I think there are some drawbacks to it, but

00:11:06.440 --> 00:11:08.720
it certainly is making a difference, right?

00:11:08.720 --> 00:11:15.040
And so now there's something that might be sort of an equivalent on the AI side of things,

00:11:15.040 --> 00:11:15.260
right?

00:11:15.260 --> 00:11:18.320
I mean, not exactly the same, but that kind of regulation.

00:11:18.320 --> 00:11:19.180
Yeah, yeah.

00:11:19.180 --> 00:11:24.840
It's interesting because it's been a work in progress for some years, like the GDPR was.

00:11:24.840 --> 00:11:30.680
So the initial talks for the GDPR started, I think, in like 2014, 2015, didn't get written

00:11:30.680 --> 00:11:35.980
until 2016, went into effect in 2018 and still a topic of conversation.

00:11:35.980 --> 00:11:40.100
Now, many years later, some pluses, some minuses, right?

00:11:40.100 --> 00:11:45.120
We can talk about how GDPR was intended versus how we've seen it rolled out across the web,

00:11:45.120 --> 00:11:48.440
which is quite different than what was intended, obviously.

00:11:48.440 --> 00:11:53.180
I think that's always a problem with a lot of regulations and the EU in general.

00:11:53.180 --> 00:11:58.340
You see, like, you know, I'm very pro-regulation and I think the idea of GDPR is great, but of course,

00:11:58.900 --> 00:12:04.520
you know, once a large organization like the EU rolls things out, it can kind of go a bit

00:12:04.520 --> 00:12:05.120
wrong here and there.

00:12:05.120 --> 00:12:09.700
Let me set a little foundation about why I sort of said, I think there are some negatives.

00:12:09.700 --> 00:12:12.060
I think the privacy stuff is all great.

00:12:12.060 --> 00:12:14.280
I think the ability to export your data is great.

00:12:14.280 --> 00:12:17.080
The ability to have it erased, to know what's being done with it.

00:12:17.080 --> 00:12:18.240
These are all good things.

00:12:18.460 --> 00:12:24.980
I feel, though, that it disproportionately was difficult for small businesses, but was

00:12:24.980 --> 00:12:27.880
aimed at places like Facebook and Google and stuff.

00:12:27.880 --> 00:12:33.400
So, for example, for like my courses and stuff, I had to stop doing anything else for almost

00:12:33.400 --> 00:12:36.720
two weeks and rewrite a whole bunch of different things to comply.

00:12:36.720 --> 00:12:39.940
And to the best of my knowledge, I'm doing it right.

00:12:40.080 --> 00:12:41.060
But who knows?

00:12:41.060 --> 00:12:44.040
Whereas Facebook didn't shut down for two weeks.

00:12:44.040 --> 00:12:46.840
You know, they had a small team who did it, right?

00:12:46.840 --> 00:12:47.600
Yeah.

00:12:47.600 --> 00:12:51.400
Well, no, they had quite a bit internal engineering work.

00:12:51.400 --> 00:12:52.360
Only as a percentage.

00:12:52.360 --> 00:12:54.000
Only as a percentage of total employees.

00:12:54.000 --> 00:12:54.480
I mean, small.

00:12:54.480 --> 00:12:59.060
But they actually had to shut down several products that are no longer available in Europe

00:12:59.060 --> 00:13:01.200
that are available in other jurisdictions.

00:13:01.700 --> 00:13:06.640
And also, when we look at who's been fined, it's been predominantly the FANGs and other

00:13:06.640 --> 00:13:07.780
large operators.

00:13:07.780 --> 00:13:12.040
I do think that the enforcement is focused on the FANG side of things.

00:13:12.040 --> 00:13:12.660
Yeah.

00:13:12.660 --> 00:13:13.220
Which is fair.

00:13:13.220 --> 00:13:17.820
It's like, you know, which is basically what most folks said when it went into enforcement

00:13:17.820 --> 00:13:22.740
is like, yes, we believe these are things that everybody should be doing to better look

00:13:22.740 --> 00:13:27.660
after the security of sensitive data, regardless of, you know, the provenance, so to speak.

00:13:27.660 --> 00:13:34.560
But also, we intend to employ this legislation to look after these problems, right?

00:13:34.560 --> 00:13:40.380
And everything that Max Scrams has been doing, he's based here in Germany, and he's been

00:13:40.380 --> 00:13:46.340
filing quite a lot of amazing lawsuits against a variety of the FANGs and been getting some

00:13:46.340 --> 00:13:50.320
interesting rulings, let's say, from the European courts.

00:13:50.320 --> 00:13:50.880
Yeah, good.

00:13:50.880 --> 00:13:52.560
Ines, how was the GDPR for you?

00:13:52.560 --> 00:13:56.380
Before we get to the next law, at Explosion, was it a big deal?

00:13:56.380 --> 00:14:01.320
Not so much, because I think actually already our standards were such that we weren't really

00:14:01.320 --> 00:14:06.460
doing anything that was violating or intended to violate what then became GDPR.

00:14:06.460 --> 00:14:10.340
I think it was just, you know, we had to go through some things to make, I don't know,

00:14:10.340 --> 00:14:14.180
we've always intended to not have any cookies on our sites.

00:14:14.180 --> 00:14:16.460
So we got in the first place.

00:14:16.460 --> 00:14:21.140
And then, you know, it's actually a lot of work to make sure that nothing you use tries

00:14:21.140 --> 00:14:22.820
to sneak some cookies in there.

00:14:22.820 --> 00:14:25.300
And then you're like, ah, I used the wrong URL here.

00:14:25.300 --> 00:14:27.600
Now I have all these YouTube cookies again.

00:14:27.600 --> 00:14:34.200
But in general, you know, we were already, even before it came up or before we, you know,

00:14:34.200 --> 00:14:38.760
GDPR really came out, we realized that, oh, we're actually quite compliant or like, at least

00:14:38.760 --> 00:14:40.220
we already aim to be compliant.

00:14:40.500 --> 00:14:41.820
We didn't have to do very much.

00:14:41.820 --> 00:14:47.680
I think I was too, in terms of principle, but not exactly in practice.

00:14:47.680 --> 00:14:48.940
There were those types of things.

00:14:48.940 --> 00:14:53.360
Like, for example, I had the discuss comment section at the bottom of all the pages.

00:14:53.780 --> 00:15:00.320
And then I realized they were putting double click cookies and Facebook cookies and all

00:15:00.320 --> 00:15:00.820
sorts of stuff.

00:15:00.820 --> 00:15:01.400
I'm like, wait a minute.

00:15:01.400 --> 00:15:05.180
I don't want people who come to my page to get that, but I'm not trying to use it.

00:15:05.180 --> 00:15:06.760
It's like this cascading chain.

00:15:06.760 --> 00:15:09.280
And yeah, like embedding YouTube videos.

00:15:09.280 --> 00:15:13.640
We go to a lot of work to make sure that it doesn't actually embed YouTube.

00:15:13.640 --> 00:15:16.640
It does a thing that then allows you to get to the YouTube without that.

00:15:16.640 --> 00:15:18.480
It's like that kind of stuff, right?

00:15:18.480 --> 00:15:19.860
But still, I think it's good.

00:15:19.860 --> 00:15:21.020
I think it's pretty good in general.

00:15:21.020 --> 00:15:23.240
But let's talk about machine learning.

00:15:23.600 --> 00:15:24.700
AI stuff.

00:15:24.700 --> 00:15:27.020
So I pulled out some highlights here.

00:15:27.020 --> 00:15:30.700
Let me maybe throw them out and you all can react to them.

00:15:30.700 --> 00:15:35.700
So we've got this article on techmonitor.ai called,

00:15:35.700 --> 00:15:40.620
The EU's leaked AI regulation is ambitious, but disappointingly vague.

00:15:40.620 --> 00:15:44.780
New rules for AI in Europe are simultaneously bold and underwhelming.

00:15:44.780 --> 00:15:49.620
I think they interviewed different people who probably have those opinions.

00:15:49.620 --> 00:15:52.320
As you could see through the article, it's not the same person necessarily.

00:15:52.460 --> 00:15:53.620
It holds both those at once.

00:15:53.620 --> 00:15:57.560
But this was leaked in April 15th of this year.

00:15:57.560 --> 00:16:00.840
And I think seven days later or something, the actual thing was published.

00:16:00.840 --> 00:16:05.100
So it's not so much about the leak, just that the article kind of covers the details, right?

00:16:05.100 --> 00:16:07.000
This is still not unknown, is it?

00:16:07.000 --> 00:16:08.000
No, no, no, no.

00:16:08.000 --> 00:16:09.840
The full text is available.

00:16:09.840 --> 00:16:15.060
And there's been a lot of good kind of deeper analysis from variety perspectives.

00:16:15.060 --> 00:16:20.860
This portion of Talk Python To Me is brought to you by Sentry.

00:16:21.120 --> 00:16:23.720
How would you like to remove a little stress from your life?

00:16:23.720 --> 00:16:29.720
Do you worry that users may be encountering errors, slowdowns, or crashes with your app right now?

00:16:29.720 --> 00:16:32.760
Would you even know it until they sent you that support email?

00:16:32.760 --> 00:16:37.520
How much better would it be to have the error or performance details immediately sent to you,

00:16:37.520 --> 00:16:43.160
including the call stack and values of local variables and the active user recorded in the report?

00:16:43.620 --> 00:16:46.580
With Sentry, this is not only possible, it's simple.

00:16:46.580 --> 00:16:50.140
In fact, we use Sentry on all the Talk Python web properties.

00:16:50.140 --> 00:16:56.700
We've actually fixed a bug triggered by a user and had the upgrade ready to roll out as we got the support email.

00:16:56.700 --> 00:16:58.680
And that was a great email to write back.

00:16:58.680 --> 00:17:02.060
Hey, we already saw your error and have already rolled out the fix.

00:17:02.060 --> 00:17:03.460
Imagine their surprise.

00:17:03.460 --> 00:17:05.680
Surprise and delight your users.

00:17:05.680 --> 00:17:09.760
Create your Sentry account at talkpython.fm/sentry.

00:17:09.760 --> 00:17:13.200
And if you sign up with the code talkpython, all one word,

00:17:13.580 --> 00:17:16.780
it's good for two free months of Sentry's business plan,

00:17:16.780 --> 00:17:21.420
which will give you up to 20 times as many monthly events as well as other features.

00:17:21.420 --> 00:17:25.840
Create better software, delight your users, and support the podcast.

00:17:25.840 --> 00:17:30.820
Visit talkpython.fm/sentry and use the coupon code talkpython.

00:17:30.820 --> 00:17:36.500
Catherine, you want to give us a quick summary of what the goal of this is?

00:17:36.500 --> 00:17:42.160
Yeah, so I think, I mean, when I first got wind that this was going to be happening,

00:17:42.540 --> 00:17:52.040
I was talking with some folks at the Bundesministerium intern, which is basically the intern German administration here.

00:17:52.200 --> 00:18:02.020
So you could think of like the, if we had a, in the U.S., sorry, U.S.-centric, in the U.S., if we had like an office of Homeland Security and the interior,

00:18:02.020 --> 00:18:07.080
and they were all together, and they like also did like FTC-like things, that's what it would be.

00:18:07.080 --> 00:18:07.700
Anyways.

00:18:08.160 --> 00:18:21.980
And they have a group called the Dotton Ethic Commission, which is Data Ethics Commission, and they had built several large reports on thinking and analyzing about the risk of algorithmic-based systems and algorithmic-based decision-making,

00:18:21.980 --> 00:18:25.320
which has been a topic of conversation, obviously, for a long time.

00:18:25.840 --> 00:18:32.700
Eventually, all of that, what I found out was that they were talking then with other groups in the EU about forming a regulation like this.

00:18:33.200 --> 00:18:40.240
And if anybody wants to read the German Dotton Ethic Commission report, which also is available in English,

00:18:40.240 --> 00:18:44.740
you can see that a lot of the ideas are kind of taken and transferred there,

00:18:44.740 --> 00:18:48.540
which is basically like when we think about AI systems,

00:18:49.060 --> 00:18:55.020
can we analyze the level of risk that they would have, let's say, in use in society?

00:18:55.020 --> 00:19:00.420
So you can think of very high risk being like bombing people and very low risk.

00:19:00.420 --> 00:19:04.580
Bombing people like drones or self-flying planes.

00:19:04.580 --> 00:19:05.520
Absolutely.

00:19:05.520 --> 00:19:07.820
I mean, we have drones that bomb people.

00:19:07.820 --> 00:19:08.600
Yes, it's true.

00:19:08.600 --> 00:19:09.740
That's the thing that happens in the world.

00:19:09.740 --> 00:19:10.140
That is the thing.

00:19:10.140 --> 00:19:14.960
But what is less common is that you just send the drone out and say,

00:19:14.960 --> 00:19:17.780
go find the, quote, bad people and take care of them, right?

00:19:17.800 --> 00:19:21.420
There's still usually a person somewhere that makes a decision.

00:19:21.420 --> 00:19:27.900
And so I don't think we want a world where we just send out robots to take care of stuff and just tell them to go.

00:19:27.900 --> 00:19:32.180
Some people want that because it can be very nice to like, you know,

00:19:32.180 --> 00:19:34.840
absolve yourself of that responsibility.

00:19:34.840 --> 00:19:38.160
Because if you're the one pressing the button, you have to answer for that.

00:19:38.160 --> 00:19:40.160
And you have to take accountability.

00:19:40.160 --> 00:19:44.340
If the machine did that, well, I mean, it's kind of like this problem of,

00:19:44.340 --> 00:19:46.960
okay, whose fault is it if the self-driving car kills?

00:19:47.480 --> 00:19:48.200
Yeah.

00:19:48.200 --> 00:19:49.040
Yeah.

00:19:49.040 --> 00:19:54.860
There's a really great psychological theorem around that too called the moral crumple zone,

00:19:54.860 --> 00:20:01.180
which is basically talks about how the nearest human to an automated system gets blamed.

00:20:01.180 --> 00:20:03.160
So, yeah.

00:20:03.160 --> 00:20:06.140
So, you know, it's like, well, why didn't you do something?

00:20:06.140 --> 00:20:06.700
I don't know.

00:20:06.700 --> 00:20:08.140
The computer said yes.

00:20:08.140 --> 00:20:13.780
So, you know, it's interesting psychology that we use to like judge people.

00:20:14.000 --> 00:20:15.620
You should have done something.

00:20:15.620 --> 00:20:16.100
Yeah.

00:20:16.100 --> 00:20:19.780
And I do think actually in the self-driving car example, I think a lot of people would actually say,

00:20:19.780 --> 00:20:26.420
yeah, the developer who built that system that made the decision to drive forward and not stop is to blame.

00:20:26.420 --> 00:20:28.260
So, you know, that does check out.

00:20:28.260 --> 00:20:28.440
Yeah.

00:20:28.520 --> 00:20:28.920
It could be.

00:20:28.920 --> 00:20:30.600
I really like the crumple zone analogy.

00:20:30.600 --> 00:20:37.740
I don't know if I'm receiving it correctly, but, you know, if you're in a car crash, like the radiator is going to get smashed straight away.

00:20:37.740 --> 00:20:39.540
Like that's the first thing when it caves in.

00:20:39.620 --> 00:20:44.060
But the driver back in the middle might be the one who did it in the software world.

00:20:44.060 --> 00:20:54.380
Maybe the equivalent is, yeah, the developer made that choice, but they made that choice because the CEO and the manager said, we're optimizing for this and we don't care.

00:20:54.380 --> 00:20:58.660
We want to ship faster or we want to make sure this scenario is perfect.

00:20:58.660 --> 00:20:59.580
And they're like, you know what?

00:20:59.580 --> 00:21:02.480
That's going to have a problem when it gets snowy and we can't see this line.

00:21:02.480 --> 00:21:02.920
Like, you know what?

00:21:02.920 --> 00:21:04.480
This is what we're aiming for.

00:21:04.480 --> 00:21:09.280
And like they don't necessarily make them do it, but they say this is really where you got to go.

00:21:09.280 --> 00:21:10.960
So crumple zone, I like it.

00:21:10.960 --> 00:21:11.140
Yeah.

00:21:11.140 --> 00:21:11.360
Yeah.

00:21:11.360 --> 00:21:19.480
And actually, I think just to make it clear, the crumple zone was like that the driver would get blamed rather than the company that produced the software.

00:21:19.480 --> 00:21:33.360
And so like, or the operator of the radiology machine would get blamed, like rather than the producers, because you kind of create this like trust, inherent trust, like, well, you know, like they're building a self-driving car.

00:21:33.360 --> 00:21:34.780
Clearly, it's not their fault.

00:21:34.780 --> 00:21:35.800
It's the driver's fault.

00:21:35.800 --> 00:21:37.940
Why weren't you paying attention or something like this?

00:21:37.940 --> 00:21:38.180
Right.

00:21:38.180 --> 00:21:38.600
Yeah.

00:21:38.600 --> 00:21:38.940
Yeah.

00:21:38.940 --> 00:21:39.820
Yeah.

00:21:39.820 --> 00:21:40.360
Absolutely.

00:21:40.360 --> 00:21:45.600
So really quickly, I just want to say none of us are lawyers.

00:21:45.600 --> 00:21:49.360
So don't take any of this advice and go do legal things.

00:21:49.360 --> 00:21:50.080
Talk to real lawyers.

00:21:50.080 --> 00:21:52.480
But I do want to talk about the law.

00:21:52.580 --> 00:21:58.660
And so one of the things the article points out is these rules apply to EU companies and those that operate in the EU.

00:21:58.660 --> 00:22:03.940
And then what is way more broadly for tech companies or impact EU citizens.

00:22:03.940 --> 00:22:04.460
Right.

00:22:04.560 --> 00:22:12.400
So if you have a website and EU citizens use it or you have an app and EU citizens use your API and it makes decisions, probably this applies to you as well.

00:22:12.400 --> 00:22:13.180
I'm guessing.

00:22:13.180 --> 00:22:13.660
Yeah.

00:22:13.660 --> 00:22:16.360
We'll see in practice how it gets rolled out.

00:22:16.360 --> 00:22:19.160
But yeah, it's always about the case law afterwards.

00:22:19.580 --> 00:22:21.080
But in theory, yes.

00:22:21.080 --> 00:22:22.000
Yeah, yeah, yeah.

00:22:22.000 --> 00:22:29.400
And it's about it's mainly about the, you know, documenting risk is, I guess, what I would say, like documenting and addressing risk.

00:22:29.400 --> 00:22:48.940
So one interesting thing about it, and I'd be curious to hear both of y'all's thoughts around it, is kind of bringing to the forefront the idea of auditing AI systems and what should be done to better audit and document problems in automated systems like AI systems or machine learning systems.

00:22:49.200 --> 00:22:51.000
That I find quite interesting.

00:22:51.000 --> 00:22:53.020
Would be curious to hear y'all's take.

00:22:53.020 --> 00:22:53.800
Yeah.

00:22:53.800 --> 00:23:05.220
I mean, I think even fundamentally, I think that's also something that's pointed out in the article is that there's already this problem of like, how do you even, you know, define what, when AI system starts, where it ends, what is the system?

00:23:05.220 --> 00:23:07.240
Is it the, just the component?

00:23:07.240 --> 00:23:09.160
Is it the model by itself?

00:23:09.160 --> 00:23:12.300
The same model can be used in all kinds of different use cases.

00:23:12.300 --> 00:23:14.680
And some of them can be bad and some of them can be good.

00:23:14.680 --> 00:23:16.880
So it has to be the larger component.

00:23:17.020 --> 00:23:22.480
But then, you know, so even where does AI really like, you could have a robot system that does the exact same thing.

00:23:22.480 --> 00:23:26.880
It's that like exempt from, even if the outcomes are like pretty much identical.

00:23:26.880 --> 00:23:29.400
I think that's already where it gets pretty problematic.

00:23:29.400 --> 00:23:34.180
And where I think you also probably see a lot of people being able to get away with things.

00:23:34.300 --> 00:23:34.380
Yeah.

00:23:34.380 --> 00:23:42.420
The law seems to try to characterize the behavior of the software rather than the implementation or the technology of it.

00:23:42.420 --> 00:23:42.640
Right.

00:23:42.640 --> 00:23:48.740
It doesn't say, you know, when a neural network comes up with an outcome or something along those lines.

00:23:49.160 --> 00:23:55.480
And they talked about how the idea is to try to make it more long-lived and more broadly applicable.

00:23:55.480 --> 00:24:00.000
But also, you know, that could result in ways to sneak around it.

00:24:00.000 --> 00:24:04.000
You know, technically it's still doing what the law is trying to cover.

00:24:04.000 --> 00:24:07.600
But, you know, somehow we built software that doesn't quite line up.

00:24:07.700 --> 00:24:07.800
Yeah.

00:24:07.800 --> 00:24:12.280
To get back to this auditing question, I do think this is, it's definitely a very interesting part of it.

00:24:12.280 --> 00:24:20.780
And I do think that it also shows that stuff like interpretability will become much more relevant and interesting, even more relevant going forward.

00:24:20.780 --> 00:24:30.920
Because I do think if, you know, we end up with laws like that, companies will have to be able to either just explain internally why did my system make a certain decision.

00:24:30.920 --> 00:24:39.620
Or maybe even this has to be communicated to the user or to, you know, to the citizen in a similar way of how with GDPR you can request your data.

00:24:40.020 --> 00:24:50.620
Maybe you can be able to request more information on why a certain outcome was produced for you or which features of you the system is using.

00:24:50.620 --> 00:24:52.640
And I think that all that does require.

00:24:52.640 --> 00:24:54.400
The feature seems completely doable.

00:24:54.400 --> 00:25:01.700
It seems entirely reasonable to say, well, we used your age, your gender and your income and your education level to make this decision.

00:25:01.700 --> 00:25:07.020
I think maybe more tricky is why did it make a decision?

00:25:07.460 --> 00:25:15.660
I mean, you two know better than I do, but it seems really, I can sit down and read code and say, well, there's an if statement that if this number is greater than this, we do that.

00:25:15.660 --> 00:25:18.260
But it's really hard to do that with neural networks, right?

00:25:18.260 --> 00:25:18.580
Yeah.

00:25:18.580 --> 00:25:21.440
And machine learning gets tricky because machine learning is always cold in data.

00:25:21.440 --> 00:25:27.560
And yeah, so it's kind of, it's this software 2.0 idea where even, you know, testing a system is much more difficult.

00:25:27.560 --> 00:25:33.480
Like if you're just writing straightforward Python code, you can then write a test and you can say, oh, if this goes in, this should come out.

00:25:33.820 --> 00:25:36.840
And if that's true, then yeah, you have a green test and then exactly.

00:25:36.840 --> 00:25:37.360
Exactly.

00:25:37.360 --> 00:25:44.580
But I mean, it kind of is a testament to the fact that we don't really test machine learning systems today very well.

00:25:44.720 --> 00:25:50.320
Like we have very, you know, early in the whole ML upside of the equation.

00:25:50.320 --> 00:25:57.240
And I think one of the things, so first off is a lot of these audits, people think they're going to be self-assessments.

00:25:57.240 --> 00:26:02.900
So leave that to question mark of how a self-assessment is going to work at any type of scale.

00:26:03.020 --> 00:26:16.700
But then also they put forward, one thing that I really liked about this law is they actually put forward things that people should be testing for, like security of the models, like privacy of the models, like the interpretability of the models and so forth.

00:26:16.700 --> 00:26:24.580
And I would say that most places that are throwing machine learning into production today do not test for any of those things before it's done.

00:26:25.160 --> 00:26:31.040
No, I think it's mostly like, oh, some accuracy score and then maybe some feedback of a music and then it's what you go.

00:26:31.040 --> 00:26:33.260
Does it seem like a reasonable outcome?

00:26:33.260 --> 00:26:33.900
Okay, good.

00:26:33.900 --> 00:26:34.360
It's working.

00:26:34.360 --> 00:26:34.720
Yeah.

00:26:34.720 --> 00:26:42.900
And I think that there's also a lot of, you know, we see this kind of disconnect if you go from academia to industry where like, you know, you can't like academia works differently.

00:26:42.900 --> 00:26:44.560
You have very different objectives and that's great.

00:26:44.640 --> 00:26:46.540
You're trying to explore new algorithms.

00:26:46.540 --> 00:26:53.840
You're trying to solve problems that are really hard and then see what works best on them and, you know, rank different technical solution.

00:26:53.840 --> 00:26:56.540
In industry, you're actually shipping something that affects people.

00:26:56.540 --> 00:27:00.420
And yeah, if you just apply the exact same metrics, that's just not enough.

00:27:00.420 --> 00:27:06.320
What do you think about testing by coming up with scenarios that should pass or not pass?

00:27:06.780 --> 00:27:21.040
For example, if you're testing some kind of algorithm that says yes or no on a mortgage for a house or something like that, just say, look, okay, I would expect that a single mom would still be able to get a mortgage.

00:27:21.040 --> 00:27:26.300
So let's, you know, have a single mom apply to the model and see what you come up with these scenarios.

00:27:26.300 --> 00:27:31.180
If it fails any of these, it's unfair and try to give it examples.

00:27:31.180 --> 00:27:31.820
Is it possible?

00:27:31.820 --> 00:27:32.940
I don't want to call it cute.

00:27:32.940 --> 00:27:39.140
Like, you know, it's a very idealistic view and that's all very nice, but I do think, I think I personally, I see two problems with this.

00:27:39.140 --> 00:27:42.360
One is that a lot of, you know, AI systems are usually quite different.

00:27:42.360 --> 00:27:46.820
Not everything is as straightforward as like, oh, I have a pipeline here that predicts whether you should get a mortgage.

00:27:46.820 --> 00:27:48.900
It's often like lots of different components.

00:27:48.900 --> 00:27:51.380
Every company tries to solve very, very different problems.

00:27:51.380 --> 00:27:55.720
So you can't like easily develop a framework where, you know, you have one input and one output.

00:27:55.720 --> 00:28:00.420
Usually predicting the thing is one tiny part of the much larger application.

00:28:00.700 --> 00:28:05.480
And then also, okay, if you have like, you know, it seems like, oh, should someone get a mortgage?

00:28:05.480 --> 00:28:07.600
Should a private company give you a mortgage or not?

00:28:07.600 --> 00:28:13.480
I think a lot of, you know, companies would find that, oh, maybe it's up to us whether, you know, you get a mortgage or not.

00:28:13.480 --> 00:28:18.980
There's no, you know, there's no general framework for, and, you know, maybe with a mortgage, it's a bit different.

00:28:18.980 --> 00:28:29.220
But there are so many applications where, yes, you can say it's really unfair, but it's still, you know, within the realms of like what a company would argue is, you know, up to their discretion.

00:28:29.220 --> 00:28:30.820
And I'm not defending that.

00:28:30.820 --> 00:28:33.680
I'm just saying it's very, very difficult to say, oh, you're being unfair.

00:28:33.680 --> 00:28:36.700
It's not as straightforward as my naive example.

00:28:36.700 --> 00:28:40.520
At least in the U.S., like there's actually laws for this.

00:28:40.520 --> 00:28:43.500
So like equal treatment or disparate treatment, we would say.

00:28:43.500 --> 00:28:50.100
So like there's actual mathematical, it's a statistical relationship, like 70, 70, 30 or 80, 20.

00:28:50.200 --> 00:28:53.300
I think that you can show that there's disparate treatment.

00:28:53.300 --> 00:29:04.300
So for example, if you could prove that there's that much of a difference if you're a single mother, let's say, versus other groups and you actually have a legal court case, you can take the bank to court and you can sue them.

00:29:04.300 --> 00:29:10.460
So there's some precedence for equal treatment, at least in some countries and some jurisdictions.

00:29:10.460 --> 00:29:28.620
And I think, but from thinking about the mathematical problem of fairness, I mean, in all of the research that a lot of really amazing, intelligent researchers have done is they've shown that fairness definitions and the choice of fairness and how you define it can actually be mathematically diametrically opposed.

00:29:28.620 --> 00:29:32.260
So depending on what definition you choose, and there's a whole bunch.

00:29:32.620 --> 00:29:37.320
So Arvind Narayanan and his group in the U.S. have been doing a ton of research on this.

00:29:37.320 --> 00:29:41.000
There's a bunch of folks that have been doing research for more than a decade on this.

00:29:41.000 --> 00:29:51.080
All the fat ML people, the fairness, accountability and transparency and ML conferences that run every year have been doing, again, two decades nearly of research on this stuff.

00:29:51.080 --> 00:30:00.700
But it's not a solved problem, even if, let's say, you choose a fairness definition mathematically, you measure the model, you have met that requirement.

00:30:01.180 --> 00:30:10.740
It doesn't mean that actually what you're trying to show in the world or what you're trying to do in the world from like how we humans would define fairness is what you've met.

00:30:10.740 --> 00:30:11.180
Right.

00:30:11.180 --> 00:30:12.340
So, yeah.

00:30:12.340 --> 00:30:12.860
Yeah.

00:30:12.860 --> 00:30:16.100
Statistics and intuition are not necessarily the same for sure.

00:30:16.100 --> 00:30:16.440
Yeah.

00:30:16.440 --> 00:30:23.760
Catherine, you had an interesting comment about fairness is not the only metric before we started recording.

00:30:23.960 --> 00:30:24.520
Oh, yeah.

00:30:24.520 --> 00:30:24.900
You want to share that?

00:30:24.900 --> 00:30:25.700
Yeah.

00:30:25.700 --> 00:30:37.420
I mean, the question that I like to ask people is, let's say you're building a computer vision system to bomb people, to identify people and bomb the right targets.

00:30:37.560 --> 00:30:48.200
If you said it performed fairly, let's say, in relation to gender or in relation to skin color, to the darkness of your skin color, would that be an ethical or fair system?

00:30:48.200 --> 00:30:49.360
Yeah.

00:30:49.360 --> 00:30:50.280
It's hard.

00:30:50.280 --> 00:30:50.800
It's not.

00:30:50.800 --> 00:30:52.200
It's certainly not an easy answer.

00:30:52.200 --> 00:30:52.600
Yeah.

00:30:52.600 --> 00:30:52.860
Yeah.

00:30:52.860 --> 00:30:54.220
So there's more to it.

00:30:54.220 --> 00:31:04.020
So one of the things that's interesting about this law is that it talks about high risk AI systems and it refers to those pretty frequently through there.

00:31:04.020 --> 00:31:13.740
So high risk AI systems include those used to manipulate human behavior, conduct social scoring or for indiscriminate surveillance.

00:31:13.740 --> 00:31:17.700
Those are actually banned in the EU, according to this law.

00:31:17.700 --> 00:31:17.940
Right.

00:31:18.160 --> 00:31:27.140
I mean, I guess you can, you can tell us read, like you can, you can, by reading, you can read who this was written for and who, you know, you know, who they had in mind when they wrote that.

00:31:27.140 --> 00:31:32.020
I think it's quite, you know, it's quite clear what, what types of applications and what types of companies.

00:31:32.020 --> 00:31:32.380
Yeah.

00:31:32.380 --> 00:31:32.780
Yeah.

00:31:32.780 --> 00:31:35.760
The social scoring stuff is really creepy, but yeah.

00:31:35.760 --> 00:31:37.780
Indiscriminate surveillance also.

00:31:37.780 --> 00:31:46.020
And then it also talks about how special authorization will be required for remote biometric identification.

00:31:46.180 --> 00:31:52.280
This is, I'm guessing, types of biometric identification that you don't actively participate in.

00:31:52.280 --> 00:31:52.860
Right.

00:31:52.860 --> 00:31:56.040
You don't put your fingerprint on something, but you just happen to be there.

00:31:56.040 --> 00:32:03.840
They call out specifically facial recognition, but I've also heard things like gait, the way that you walk and weird stuff like that.

00:32:03.840 --> 00:32:06.960
So it's not banned, but special authorization will be required.

00:32:06.960 --> 00:32:07.600
Oh yeah.

00:32:07.600 --> 00:32:08.500
You're typing too, right?

00:32:08.500 --> 00:32:08.760
Yeah.

00:32:08.760 --> 00:32:13.140
Even your, your typing pattern is quite, or is more unique than you think.

00:32:13.140 --> 00:32:13.540
Yeah.

00:32:13.540 --> 00:32:14.080
Yeah.

00:32:14.080 --> 00:32:14.320
Yeah.

00:32:14.320 --> 00:32:14.560
Yeah.

00:32:14.560 --> 00:32:14.800
Yeah.

00:32:14.800 --> 00:32:17.180
Actually, even, even more than sort of old school fingerprinting.

00:32:17.180 --> 00:32:22.320
I'm always like amazed at what life can be done to like uniquely identify you on the internet,

00:32:22.320 --> 00:32:26.240
even without having any personal identifiable information.

00:32:28.560 --> 00:32:31.500
This portion of Talk Python To Me is brought to you by SignalWire.

00:32:31.500 --> 00:32:33.400
Let's kick this off with a question.

00:32:33.400 --> 00:32:36.980
Do you need to add multi-party video calls to your website or app?

00:32:36.980 --> 00:32:49.520
I'm talking about live video conference rooms that host 500 active participants, run in the browser, and work within your existing stack, and even support 1080p without devouring the bandwidth and CPU on your users' devices.

00:32:49.520 --> 00:32:59.900
SignalWire offers the APIs, the SDKs, and edge networks around the world for building the realest of real-time voice and video communication apps with less than 50 milliseconds of latency.

00:32:59.900 --> 00:33:10.660
Their core products use WebSockets to deliver 300% lower latency than APIs built on REST, making them ideal for apps where every millisecond of responsiveness makes a difference.

00:33:10.660 --> 00:33:14.480
Now, you may wonder how they get 500 active participants in a browser-based app.

00:33:14.840 --> 00:33:20.360
Most current approaches use a limited but more economical approach called SFU, or Selective Forwarding Units,

00:33:20.360 --> 00:33:26.920
which leaves the work of mixing and decoding all those video and audio streams of every participant to each user's device.

00:33:26.920 --> 00:33:32.660
Browser-based apps built on SFU struggle to support more than 20 interactive participants.

00:33:32.660 --> 00:33:39.380
So SignalWire mixes all the video and audio feeds on the server and distributes a single unified stream back to every participant.

00:33:39.680 --> 00:33:45.520
So you can build things like live streaming fitness studios where instructors demonstrate every move from multiple angles,

00:33:45.520 --> 00:33:52.460
or even live shopping apps that highlight the charisma of the presenter and the charisma of the products they're pitching at the same time.

00:33:52.460 --> 00:33:59.600
SignalWire comes from the team behind FreeSwitch, the open-source telecom infrastructure toolkit used by Amazon, Zoom,

00:33:59.600 --> 00:34:03.080
and tens of thousands of more to build mass-scale telecom products.

00:34:03.200 --> 00:34:07.580
So sign up for your free account at talkpython.fm/signalwire,

00:34:07.580 --> 00:34:12.300
and be sure to mention Talk Python To Me to receive an extra 5,000 video minutes.

00:34:12.300 --> 00:34:16.600
That's talkpython.fm/signalwire, and mention Talk Python To Me for all those credits.

00:34:16.600 --> 00:34:26.800
Another thing that stood out to me I thought was fun is that people have to be told when they're interacting with an AI system.

00:34:27.240 --> 00:34:36.320
So you have to explicitly say, hey, this thing that you're talking to here, this one, you're not talking to a human right now, you're talking to a machine.

00:34:36.320 --> 00:34:38.740
Yeah, we'll see if it gets rolled out like cookies.

00:34:41.880 --> 00:34:49.540
It's like a big blurb of text that says there may or may not be automated systems that you interact with on this product or something.

00:34:49.540 --> 00:34:50.060
Yeah.

00:34:50.060 --> 00:34:57.480
Yeah, I think it's almost like, I don't know, like a classic disclaimer, because I think the way it's written probably makes people think more about conversational AI,

00:34:57.480 --> 00:35:00.040
but I do think this also covers everything else.

00:35:00.040 --> 00:35:07.320
And if you use some component that does some arbitrary predictions in order to, I don't know, this can go like 20 levels deep,

00:35:07.400 --> 00:35:08.760
then you need this disclaimer on it.

00:35:08.760 --> 00:35:14.360
And then, unfortunately, I think the appropriate side effect of that will be that, okay, people are less likely to notice it,

00:35:14.360 --> 00:35:16.080
because it will have to be on everything.

00:35:16.080 --> 00:35:22.820
Like, you know, even really small features you might want to have on your website that do use AI in some form or another.

00:35:22.820 --> 00:35:24.100
It seems totally reasonable.

00:35:24.100 --> 00:35:26.760
Maybe unnecessary, but certainly reasonable.

00:35:26.760 --> 00:35:28.260
But yeah, you're right.

00:35:28.260 --> 00:35:30.200
It's going to be like the cookie notices.

00:35:30.400 --> 00:35:39.440
You know, so if you go to say on Netflix, and you go to watch a movie, well, that list of recommended for you, do you have to like, okay, this?

00:35:39.440 --> 00:35:41.020
I'm not sure if I can find it.

00:35:41.020 --> 00:35:43.420
Just it will just be a Netflix is like terms and conditions.

00:35:43.420 --> 00:35:49.780
When you would set those terms and conditions that most people probably don't agree, you will accept that yes, everything you're interacting with is AI.

00:35:49.780 --> 00:35:57.540
By the way, here's a very long 20 page document about how we may or may not use automated systems in your use of this website.

00:35:57.540 --> 00:35:58.900
Yeah, exactly.

00:35:58.900 --> 00:35:59.020
Exactly.

00:35:59.020 --> 00:36:01.620
Exactly.

00:36:01.620 --> 00:36:03.440
Have fun.

00:36:03.440 --> 00:36:05.120
Let me know how the reading goes.

00:36:05.120 --> 00:36:10.200
I said I thought there was a lot of stuff that came out of the GDPR that was pretty good.

00:36:10.200 --> 00:36:12.100
This website may use cookies.

00:36:12.100 --> 00:36:14.740
That to me is the worst.

00:36:14.740 --> 00:36:15.920
It's the worst.

00:36:15.920 --> 00:36:17.840
It's like, do you want to be able to log in?

00:36:17.840 --> 00:36:20.520
Or do you want to not be able to log in?

00:36:20.520 --> 00:36:21.340
Well, I want to be able to log in.

00:36:21.340 --> 00:36:22.780
Okay, we've got to use cookies.

00:36:22.780 --> 00:36:28.220
So I actually got this thing called I Don't Care About Cookies as a browser extension.

00:36:28.220 --> 00:36:31.400
And if it sees that, it tries to agree to it automatically.

00:36:31.400 --> 00:36:32.520
Like every side of it.

00:36:32.520 --> 00:36:34.820
Just so to cut down on the cookie.

00:36:34.820 --> 00:36:35.280
Okay.

00:36:35.280 --> 00:36:35.820
Okay.

00:36:35.820 --> 00:36:39.420
By the way, this was by no means the intention of the law.

00:36:39.420 --> 00:36:41.340
Just to make it clear to everyone.

00:36:41.340 --> 00:36:43.360
It's important to bring that up.

00:36:43.360 --> 00:36:46.740
Because also, you know, especially I think, yeah, from the European perspective, you know,

00:36:46.760 --> 00:36:48.240
I'm genuinely a fan of GDPR.

00:36:48.240 --> 00:36:51.540
And then often, yeah, people go like, oh, it's all these cookie pop-ups.

00:36:51.540 --> 00:36:54.620
You know, it's like, yeah, no, that wasn't, that's not GDPR.

00:36:54.620 --> 00:36:55.140
That's just.

00:36:55.140 --> 00:36:58.040
Weren't the cookie pop-ups predated the GDPR, didn't it?

00:36:58.040 --> 00:37:04.940
It was mainly, so some people did it before, but it was deeply rolled out for GDPR because

00:37:04.940 --> 00:37:05.820
there's all this compliance.

00:37:06.060 --> 00:37:10.560
Now, a tip for folks is somebody got sued.

00:37:10.560 --> 00:37:16.240
It was Google or Facebook that it was too hard to just do the least possible.

00:37:16.240 --> 00:37:22.120
So now if you're, if you haven't installed this extension, there's usually a big button

00:37:22.120 --> 00:37:24.000
that says legitimate interest.

00:37:24.000 --> 00:37:25.440
And you can just press that one.

00:37:25.440 --> 00:37:26.700
It's the least amount.

00:37:26.700 --> 00:37:27.480
Yes.

00:37:27.520 --> 00:37:30.380
Does it usually now involve two clicks rather than one?

00:37:30.380 --> 00:37:30.720
Yeah.

00:37:30.720 --> 00:37:35.220
I wonder if this extension does it, because I think as far as I know, it's also, if you

00:37:35.220 --> 00:37:39.140
get to a set, if it offers you to go to a settings page, everything has to be unchecked

00:37:39.140 --> 00:37:39.720
by default.

00:37:39.720 --> 00:37:42.720
So it's actually, yeah, it's actually quite convenient.

00:37:42.720 --> 00:37:48.440
You just go to the button that's not accept all, and then you accept whatever is there, and

00:37:48.440 --> 00:37:49.180
then you get nothing.

00:37:49.180 --> 00:37:53.360
It might be this big and the same color as the page.

00:37:53.360 --> 00:37:55.600
So just be like really looking hard.

00:37:55.600 --> 00:37:57.720
I didn't see if there's a little, like a shadow.

00:37:57.720 --> 00:38:02.280
Which just talks about, I don't know if you've talked about this on the podcast recently,

00:38:02.280 --> 00:38:07.740
but I've been reading a lot about dark patterns and like dark patterns and privacy are like

00:38:07.740 --> 00:38:14.240
in a very deep love relationship on the internet of like, no, you really want to give us all

00:38:14.240 --> 00:38:14.820
your data.

00:38:14.820 --> 00:38:17.520
You're going to be so sad if you don't.

00:38:17.520 --> 00:38:17.860
Yeah.

00:38:17.860 --> 00:38:18.160
Yeah.

00:38:18.160 --> 00:38:20.440
The dark patterns and the lack of privacy.

00:38:20.440 --> 00:38:20.920
Yeah.

00:38:20.920 --> 00:38:21.140
Yeah.

00:38:21.140 --> 00:38:21.380
Yeah.

00:38:21.380 --> 00:38:21.860
Absolutely.

00:38:21.860 --> 00:38:25.560
The same color as foreground and background that ties into another compliance.

00:38:25.560 --> 00:38:26.680
thing, which is accessibility.

00:38:26.680 --> 00:38:31.180
And at least if like in the US, you can get sued for having a not accessible website.

00:38:31.180 --> 00:38:34.060
Even companies will at least not do this.

00:38:34.060 --> 00:38:37.540
Even if it's only if they don't care about anyone accessing their website and all they

00:38:37.540 --> 00:38:38.680
care about is not getting sued.

00:38:38.680 --> 00:38:42.040
You won't have buttons with the same foreground and background color anymore.

00:38:42.040 --> 00:38:43.380
Yeah, indeed.

00:38:43.560 --> 00:38:49.500
And I'm okay with my, I'm okay with cookies, little clicker thing, because I also have

00:38:49.500 --> 00:38:52.740
like network level tracking blocking.

00:38:52.740 --> 00:38:56.320
So, so that if then they say, sure, well, fine.

00:38:56.320 --> 00:38:57.300
Here's your Facebook cookie.

00:38:57.300 --> 00:38:58.320
It's like, no, it's blocked.

00:38:58.320 --> 00:39:00.260
So like, it's my weird setup.

00:39:00.260 --> 00:39:06.680
So Vincent out in the audience says, just to mention Raza, a Python tool for open source

00:39:06.680 --> 00:39:11.440
chatbots took the effort for writing down some ethical principles of good design.

00:39:11.440 --> 00:39:17.340
And one of those is that it lists a conversational assistant should identify itself as one.

00:39:17.340 --> 00:39:18.040
Hi, Vincent.

00:39:18.040 --> 00:39:18.840
Yeah.

00:39:18.840 --> 00:39:19.880
No, Raza, Stephanie.

00:39:19.880 --> 00:39:21.640
Raza is a great open source library as well.

00:39:21.640 --> 00:39:24.380
We kind of, you know, friends with spaCy.

00:39:24.380 --> 00:39:25.880
It's kind of the same ecosystem.

00:39:25.880 --> 00:39:28.560
No, I think it's, yeah, I think this is a good principle.

00:39:28.560 --> 00:39:32.680
Actually, often when I use like a, like a chat, a bot or whatever, and I'm not sure it's

00:39:32.680 --> 00:39:36.860
a chat bot, they're like a lot of, you know, things and ways of things you can write to

00:39:36.860 --> 00:39:40.020
check if it's a human or not, because they're like, you know, there's certain things where

00:39:40.020 --> 00:39:42.980
like, usually these things are quite bad at like resolving references.

00:39:42.980 --> 00:39:47.800
So, you know, if you use like a pronoun or like you, to refer to something you previously

00:39:47.800 --> 00:39:51.760
said or a person or something like, there are a lot of things that often these things are

00:39:51.760 --> 00:39:52.400
quite bad at.

00:39:52.400 --> 00:39:57.620
And if you, if you vague enough, a human will always get it, but like a machine might not.

00:39:57.940 --> 00:40:04.300
So if you use your text processing ML skills and experience for good use, that's right.

00:40:04.300 --> 00:40:04.660
Yeah.

00:40:04.660 --> 00:40:08.540
No, because there was a, there was a case where I was like, this person, this, this agent is

00:40:08.540 --> 00:40:09.280
so incompetent.

00:40:09.280 --> 00:40:11.360
It must be like, it must be like a machine.

00:40:11.360 --> 00:40:15.320
And then it's actually, it would be a pretty good chat bot because you know, the chat bot passed

00:40:15.320 --> 00:40:19.520
as an incompetent human, but no, it turned out it was just an incompetent human.

00:40:19.520 --> 00:40:20.220
Oh no.

00:40:20.220 --> 00:40:22.120
Yeah, that's true.

00:40:22.180 --> 00:40:27.860
The chat bots are very bad at, at retaining, like building up a state of the conversation.

00:40:27.860 --> 00:40:30.680
It's like, they see the message and then they respond to it.

00:40:30.680 --> 00:40:36.920
It's like you ask a question and then it does, did you say, what exactly is this about?

00:40:36.920 --> 00:40:38.580
Well, I said it was about this above.

00:40:38.580 --> 00:40:42.060
So what do you think it's, you know, it's, it's just those kinds of tie-ins that they don't

00:40:42.060 --> 00:40:42.800
carry on.

00:40:42.800 --> 00:40:43.060
Yeah.

00:40:43.060 --> 00:40:47.180
These systems are getting, getting better at this, but there's just some like, you know,

00:40:47.180 --> 00:40:51.840
if you really try to be as vague as possible, you can like trick them and then you find

00:40:51.840 --> 00:40:52.860
out if it's a human or not.

00:40:52.860 --> 00:40:53.600
Yeah, exactly.

00:40:53.600 --> 00:40:57.640
So let's see some more things about the law is over here.

00:40:57.640 --> 00:41:00.740
The military, AI in the military is exempt.

00:41:00.740 --> 00:41:02.580
So that's not a surprise.

00:41:02.580 --> 00:41:05.380
I mean, there's probably top secret stuff.

00:41:05.380 --> 00:41:06.840
How are you going to submit that?

00:41:06.840 --> 00:41:07.760
I don't know.

00:41:07.760 --> 00:41:07.980
Yeah.

00:41:07.980 --> 00:41:08.300
Yeah.

00:41:08.300 --> 00:41:11.620
But then it's like, oh, and you know, a lot of the worst things that happen, happen in

00:41:11.620 --> 00:41:12.280
this context.

00:41:12.280 --> 00:41:16.040
So it's like, you know, as like, you know, as a little anecdote, for example, we've for

00:41:16.040 --> 00:41:17.800
a long time, we've always had, we've had exposure.

00:41:17.800 --> 00:41:23.260
We've had this policy that we do not sell to organizations who are primarily engaged in

00:41:23.260 --> 00:41:28.420
government, military or intelligence, national security, because, and our reasoning for that

00:41:28.420 --> 00:41:33.560
has always been that, well, in the free market, you have a lot of other ways that companies

00:41:33.560 --> 00:41:38.440
and applications can be regulated by regulations like this, but also just by market pressures

00:41:38.440 --> 00:41:40.120
and by things just being more public.

00:41:40.260 --> 00:41:45.820
All of these things you do not have, the work is military intelligence or certain government

00:41:45.820 --> 00:41:46.160
work.

00:41:46.160 --> 00:41:50.380
So we see that as very problematic because you have absolutely no idea what the software

00:41:50.380 --> 00:41:53.700
is used for, and there's absolutely no way to regulate it ever.

00:41:53.700 --> 00:41:58.560
And then we'd say, okay, that's not what we want to sell our software to.

00:41:58.560 --> 00:42:01.860
But the other use cases, you know, it's not, some government things are fine.

00:42:01.860 --> 00:42:07.180
You know, we'd happily sell to the IRS and equivalents or, you know, federal reserves.

00:42:07.180 --> 00:42:11.040
There are a lot of things that are like not terrible that are government adjacent or just

00:42:11.040 --> 00:42:12.320
a lot of research labs as well.

00:42:12.320 --> 00:42:14.720
But yeah, military, that's quite obvious.

00:42:14.720 --> 00:42:14.980
Yeah.

00:42:14.980 --> 00:42:19.960
Well, and then you think like how many companies that work on machine learning today that focus

00:42:19.960 --> 00:42:23.280
on selling explicitly into the military.

00:42:23.280 --> 00:42:25.340
And it's like, well, are they exempt?

00:42:25.340 --> 00:42:30.160
Because just, you know, basically an extent like it's Palantir exempt from this.

00:42:30.340 --> 00:42:31.620
Interesting, right?

00:42:31.620 --> 00:42:36.380
Because yeah, the law would otherwise apply to them, but sort of indirectly.

00:42:36.380 --> 00:42:39.740
So you're asking about the transitive property, basically.

00:42:39.740 --> 00:42:40.960
Yeah.

00:42:40.960 --> 00:42:41.360
Yeah.

00:42:41.360 --> 00:42:45.840
Like, well, it's only in used in military use, so it's probably okay.

00:42:45.840 --> 00:42:47.640
It's probably exempt or whatever.

00:42:47.640 --> 00:42:47.980
Yeah.

00:42:47.980 --> 00:42:48.320
Yeah.

00:42:48.320 --> 00:42:52.040
Well, I guess if you can make the case that it's, you know, it's classified, that that's

00:42:52.040 --> 00:42:54.940
probably what companies like that will have, you know, the means.

00:42:54.940 --> 00:42:59.120
They would make sure that every project they're taking on, it's classified in some way.

00:42:59.120 --> 00:43:00.840
And then, you know, they get a brand bad.

00:43:00.840 --> 00:43:01.100
Yeah.

00:43:01.100 --> 00:43:02.020
That's probably true.

00:43:02.020 --> 00:43:02.720
All right.

00:43:02.720 --> 00:43:04.880
Another thing that I thought was interesting.

00:43:04.880 --> 00:43:09.580
So all the stuff we talked about so far is sort of just laying out the details.

00:43:09.580 --> 00:43:14.820
On the imprecision and subjectivity side, one of the quotes was,

00:43:14.820 --> 00:43:17.800
one area that raised eyebrows is part of the report, which reads,

00:43:17.800 --> 00:43:23.060
AI systems designed or used in a manner that exploits information or prediction about a person

00:43:23.060 --> 00:43:27.420
or group of persons in order to target their vulnerabilities or special circumstances,

00:43:27.420 --> 00:43:32.860
causing a person to behave or form an opinion or take a decision to their detriment.

00:43:32.860 --> 00:43:33.700
Yeah.

00:43:33.700 --> 00:43:37.260
That sounds like a lot of big tech, honestly, like a lot of the social networks,

00:43:37.260 --> 00:43:42.460
maybe they even suggest maybe that's even like Amazon shopping recommendations, right?

00:43:42.460 --> 00:43:46.440
Encourage you to buy something that you don't need or whatever.

00:43:46.440 --> 00:43:47.560
What do you think about that?

00:43:47.560 --> 00:43:47.780
Yeah.

00:43:47.780 --> 00:43:51.680
I mean, I guess it's quite vague and it's like, okay, how do you define, you know,

00:43:51.720 --> 00:43:55.780
we have to wait for like the actual cases to come up and someone making the case that like,

00:43:55.780 --> 00:44:01.760
oh, I don't know, my wife divorced me because X happened and that was, you know,

00:44:01.760 --> 00:44:06.200
the outcome and it's clearly that company's fault and then someone can decide whether that's true or not.

00:44:06.200 --> 00:44:08.780
And then, you know, it's quite, and, you know, of course,

00:44:08.780 --> 00:44:12.480
these are not cases that this was like designed for or written for, but like there's,

00:44:12.480 --> 00:44:15.320
you know, it is vague to this extent where like, yes,

00:44:15.320 --> 00:44:19.600
this would probably be a legit case that a judge has to decide over and maybe the person would win.

00:44:19.600 --> 00:44:21.360
Wouldn't it be great if they gave examples?

00:44:21.360 --> 00:44:23.380
I didn't want to accept the cookies.

00:44:23.380 --> 00:44:25.780
So I'm sitting under the new law.

00:44:25.780 --> 00:44:26.820
Exactly.

00:44:26.820 --> 00:44:27.100
Yeah.

00:44:27.100 --> 00:44:28.400
That made me feel bad.

00:44:28.400 --> 00:44:28.600
Yeah.

00:44:28.600 --> 00:44:34.300
But I mean, I think some of it is like really feel like the conversation here.

00:44:34.300 --> 00:44:37.680
And I'm being curious to the opinion on the conversation in the U.S.

00:44:37.720 --> 00:44:43.240
is around kind of the political ad manipulation and the amount, let's say,

00:44:43.240 --> 00:44:46.340
when we think about topics like disinformation and misinformation,

00:44:46.340 --> 00:44:54.960
the amount of kind of algorithmic use of, let's say, opinion pieces to kind of push particular agendas.

00:44:54.960 --> 00:44:59.980
And when I read this, I'm guessing that's like one of the things they had in mind.

00:44:59.980 --> 00:45:07.360
I had misinformation and fake news and all that kind of stuff is what popped to mind when I saw this.

00:45:07.360 --> 00:45:07.600
Yeah.

00:45:07.600 --> 00:45:12.240
And I was also thinking of recommendation systems and like even, I mean, I don't know,

00:45:12.240 --> 00:45:17.500
not even fake news, but like, okay, you can, you know, manipulate people into, you know,

00:45:17.500 --> 00:45:18.580
joining certain groups.

00:45:18.580 --> 00:45:19.420
Yeah, exactly.

00:45:19.420 --> 00:45:20.200
Yeah.

00:45:20.200 --> 00:45:23.860
You're a relatively just stable, normal person.

00:45:23.860 --> 00:45:24.300
Yeah.

00:45:24.300 --> 00:45:29.600
And then, then you, you read some posts, they suggest you join a group.

00:45:29.600 --> 00:45:34.460
Three months later, you'll, you know, you're in the wilderness training with a gun or something.

00:45:34.460 --> 00:45:39.580
Like, it's just, it's so easy to like send people down these, these holes, I think, you know,

00:45:39.580 --> 00:45:44.900
on a much more relatable note, I would say, even though I really love YouTube, you know,

00:45:44.900 --> 00:45:47.740
one of the sort of sayings, I think I heard it somewhere.

00:45:47.740 --> 00:45:51.160
I don't know where it came from, so I can't attribute it as, but you're never extreme enough

00:45:51.160 --> 00:45:51.820
for YouTube.

00:45:52.080 --> 00:45:57.500
If you watch three YouTube videos on like some topic, like let's suppose your washer

00:45:57.500 --> 00:45:57.820
broke.

00:45:57.820 --> 00:46:00.700
And so you need to figure out how does my dishwasher work?

00:46:00.700 --> 00:46:03.140
And so you watch several videos to try to fix it.

00:46:03.140 --> 00:46:05.180
Well, your feed is full of dishwasher stuff.

00:46:05.180 --> 00:46:07.620
And if you watch a few more, it's nothing but dishwashers.

00:46:07.620 --> 00:46:10.000
There's a lot of other videos besides dishwasher.

00:46:10.000 --> 00:46:15.600
So any little, like, it's almost like the butterfly effect, the chaos theory effect of like, I washed

00:46:15.600 --> 00:46:19.320
a little bit this way and then you just, you end up down that, that channel.

00:46:19.320 --> 00:46:23.700
One of the interesting things I think about that is I've been talking with a few folks

00:46:23.700 --> 00:46:29.280
that like where, you know, a friend's, you know, family has been kind of like radicalized

00:46:29.280 --> 00:46:33.420
around some of the topics that let's say are very radical online right now.

00:46:33.420 --> 00:46:35.780
And they're like, I just don't know how it happens.

00:46:35.780 --> 00:46:41.320
And it's kind of like, well, the internet that they're experiencing is incredibly different

00:46:41.320 --> 00:46:43.320
than the internet that you're experiencing.

00:46:43.320 --> 00:46:48.840
And so it's kind of like when we think about lockdown or where the internet is going to

00:46:48.840 --> 00:46:54.840
be like a major source of people's life and then their internet is just a completely different

00:46:54.840 --> 00:47:00.800
experience than yours based off of some related search terms across maybe four or five different

00:47:00.800 --> 00:47:04.700
sites that have been linked via cookies or other types of information.

00:47:04.700 --> 00:47:12.500
I mean, it's like, yeah, you can say, well, I have this experience, but if your entire world

00:47:12.500 --> 00:47:15.580
online was different, maybe you wouldn't have the same experience.

00:47:15.580 --> 00:47:21.460
I think it would be very hard to say what, how you would think and feel if your entire information

00:47:21.460 --> 00:47:23.220
experience was completely different.

00:47:23.220 --> 00:47:26.260
Don't make me think about weird alternate realities of myself.

00:47:26.260 --> 00:47:28.980
What if just one decision was made differently?

00:47:28.980 --> 00:47:30.980
What world would you be in?

00:47:30.980 --> 00:47:32.540
You know, it could be really different, honestly.

00:47:32.540 --> 00:47:35.860
No, I mean, you wouldn't even necessarily know when I think, but I think that's also kind

00:47:35.860 --> 00:47:39.140
of a problem where I like, in that sense, I do like that it's relatively vague.

00:47:39.140 --> 00:47:42.020
And I think laws can be vague because, you know, you don't know what's going to happen.

00:47:42.020 --> 00:47:45.860
And you might have people who are in a situation where they don't necessarily feel like, oh,

00:47:45.860 --> 00:47:48.980
I've been like, you know, tricked or like treated badly here.

00:47:49.300 --> 00:47:54.420
And maybe, you know, the outcome, maybe the outcomes of the behavior are bad, but,

00:47:54.420 --> 00:47:57.700
you know, maybe what the platform did wasn't necessarily illegal.

00:47:57.700 --> 00:47:58.820
Like that's also the problem.

00:47:58.820 --> 00:48:03.380
It's like, you know, you can what, like a lot of the content you can watch on YouTube is not,

00:48:03.380 --> 00:48:07.700
it's legal and it's your right as like, you know, a free citizen, especially, you know,

00:48:07.700 --> 00:48:13.380
in the U S where people take this even more seriously to some degree than people in Europe.

00:48:13.380 --> 00:48:17.780
Like you can, you can watch like anti-vax videos all day and that's your rights.

00:48:17.780 --> 00:48:19.460
And, nobody can keep you from that.

00:48:19.460 --> 00:48:19.860
It's not good for you.

00:48:19.860 --> 00:48:20.420
You can do it.

00:48:20.420 --> 00:48:25.620
No, it's not good for you, but like, you know, and so otherwise I think, yeah,

00:48:25.620 --> 00:48:31.220
terms that are maybe, you know, less fake in that respect, it would be much harder to actually,

00:48:31.220 --> 00:48:36.180
you know, go after cases where yes, it's clearly the platform is clearly to blame or the platform

00:48:36.180 --> 00:48:39.620
should be regulated, which obviously it's very clear that it's what they had in mind.

00:48:39.620 --> 00:48:39.940
Right.

00:48:39.940 --> 00:48:40.500
Absolutely.

00:48:40.500 --> 00:48:42.020
I really liked the part here.

00:48:42.020 --> 00:48:49.220
That's like exploit information, target vulnerabilities, because it's kind of like, okay, I know these,

00:48:49.220 --> 00:48:52.740
you know, I mean, what we saw with Cambridge Analytica and then a bunch of the targeted

00:48:52.740 --> 00:48:59.220
stuff after that was like, we can figure out exactly how to target undecided voters of these

00:48:59.220 --> 00:49:01.380
different racial groups in these counties.

00:49:01.380 --> 00:49:04.660
And we can like feed them as many Facebook ads as possible.

00:49:04.660 --> 00:49:06.180
And it's just like, wow.

00:49:06.180 --> 00:49:06.500
Okay.

00:49:06.500 --> 00:49:13.060
I don't think people realize that that was so easy to put together and do given like a fairly

00:49:13.060 --> 00:49:15.700
small amount of information about a person.

00:49:15.700 --> 00:49:20.100
So, I mean, and, and it's not personal information, right?

00:49:20.100 --> 00:49:24.980
Because usually it's what we would call, you know, profiles of individuals.

00:49:24.980 --> 00:49:31.780
So you fit a profile because, you know, you like these three brands on Facebook and you live in these

00:49:31.780 --> 00:49:37.700
districts and you're this age and this race, or you report this race, or we, we can infer your race

00:49:37.700 --> 00:49:39.620
because of these other things that you've liked.

00:49:39.620 --> 00:49:44.900
It means, you know, it adds a lot of information that I don't think most people know that,

00:49:44.900 --> 00:49:48.020
that you can get that specific in the advertising world.

00:49:48.020 --> 00:49:48.340
Yeah.

00:49:48.340 --> 00:49:52.260
How do you ladies feel about the whole flock thing?

00:49:52.260 --> 00:49:52.980
Oh yeah.

00:49:52.980 --> 00:49:55.540
That Chrome was, Chrome was doing to replace cookies.

00:49:55.540 --> 00:49:58.260
I mean, we wouldn't even have to have those little buttons or my add in.

00:49:58.260 --> 00:49:59.540
It'd be a good world.

00:49:59.540 --> 00:50:01.540
Oh yeah.

00:50:01.540 --> 00:50:02.580
Oh yeah.

00:50:02.580 --> 00:50:02.980
Oh yeah.

00:50:02.980 --> 00:50:08.500
There's been a lot of important writing about flocks and, and vulnerabilities in the design.

00:50:08.500 --> 00:50:10.420
Just so people, if they, sorry, if they don't know.

00:50:10.420 --> 00:50:10.420
Yeah.

00:50:10.420 --> 00:50:10.900
I'm sorry.

00:50:10.900 --> 00:50:11.220
Yeah.

00:50:11.220 --> 00:50:11.620
Yeah.

00:50:11.620 --> 00:50:13.940
Federated learning of cohorts.

00:50:13.940 --> 00:50:14.340
Learning of cohorts.

00:50:14.340 --> 00:50:14.660
I believe.

00:50:14.660 --> 00:50:15.140
Yeah.

00:50:15.140 --> 00:50:15.220
Yeah.

00:50:15.220 --> 00:50:15.620
Yeah.

00:50:15.620 --> 00:50:16.100
Yeah.

00:50:16.100 --> 00:50:22.260
And federated learning is essentially a tool that can be privacy preserving, but doesn't

00:50:22.260 --> 00:50:22.980
have to be.

00:50:22.980 --> 00:50:29.780
And it basically means that the data stays on device and the things that we send to a centralized

00:50:29.780 --> 00:50:34.260
location or several centralized location are usually gradient updates.

00:50:34.260 --> 00:50:36.100
So these are small updates to the model.

00:50:36.100 --> 00:50:40.020
All the model is then shared amongst participants and the process repeats.

00:50:40.020 --> 00:50:48.020
The exact design of how flocks was rolled out and is rolled out is I think not fully clear.

00:50:48.020 --> 00:50:54.740
And in general, I'm a fan of some parts of federated learning, but there's a lot of loopholes in

00:50:54.740 --> 00:51:01.940
flux design that would still involve the ability for people to both reverse engineer the models, but also

00:51:01.940 --> 00:51:03.940
to fingerprint people.

00:51:03.940 --> 00:51:04.820
Yeah.

00:51:04.820 --> 00:51:04.820
Yeah.

00:51:04.820 --> 00:51:10.900
So to take your cohort plus your browser fingerprint, you combine the two, it becomes

00:51:10.900 --> 00:51:13.060
fairly easy to re-identify individuals.

00:51:13.060 --> 00:51:13.460
So.

00:51:13.460 --> 00:51:13.780
Yeah.

00:51:13.780 --> 00:51:17.220
And I think the more underlying problem is also that, well, you know, are you going to trust

00:51:17.220 --> 00:51:21.140
something that comes out of Google that's marketed as like, oh, it will like, you know,

00:51:21.140 --> 00:51:25.140
preserve your privacy and be like really great for you and the internet.

00:51:25.140 --> 00:51:29.220
And I mean, that's, that's just like screams of like red flags.

00:51:29.220 --> 00:51:29.460
Yeah.

00:51:29.460 --> 00:51:36.660
You know, to me, it feels like, it feels like we've been presented a false dichotomy.

00:51:36.660 --> 00:51:43.380
We could either have this creepy cookie world or because we must still have tracking or we

00:51:43.380 --> 00:51:44.260
could have this flock.

00:51:44.260 --> 00:51:45.860
It's like, well, or we could just not have tracking.

00:51:45.860 --> 00:51:47.780
Like that's also a possible future.

00:51:47.780 --> 00:51:49.540
We don't have to have tracking.

00:51:49.540 --> 00:51:51.140
And here's a better tracking mechanism.

00:51:51.140 --> 00:51:52.820
We can just have not have tracking.

00:51:52.820 --> 00:51:53.300
How about that?

00:51:53.300 --> 00:51:57.620
I was reading a wonderful article about IE6.

00:51:57.620 --> 00:51:58.100
Okay.

00:51:58.100 --> 00:51:58.820
I don't know.

00:51:58.820 --> 00:52:00.340
That's some, that's some.

00:52:00.340 --> 00:52:01.380
Young children.

00:52:01.380 --> 00:52:02.340
I'm sorry.

00:52:02.340 --> 00:52:03.060
I'm sorry.

00:52:03.060 --> 00:52:08.420
I'm bringing up ancient history, but there was a browser once and it was called Internet Explorer

00:52:08.420 --> 00:52:08.900
6.

00:52:08.900 --> 00:52:12.740
It was the bane of every web developer's existence for a long time.

00:52:12.740 --> 00:52:18.340
But one thing that I didn't know about it until recently is it actually had privacy standards

00:52:18.340 --> 00:52:19.700
built into the browser.

00:52:19.700 --> 00:52:24.740
You could set up certain privacy preferences and it would like block cookies and websites

00:52:24.740 --> 00:52:26.740
and stuff for you automatically.

00:52:26.740 --> 00:52:36.420
Like there was this whole standard called P3P that would the WC3 put together around like

00:52:36.420 --> 00:52:40.100
everybody's going to have your local stored privacy preferences.

00:52:40.100 --> 00:52:43.940
And then when you browse the web, it's just going to automatically block stuff and all

00:52:43.940 --> 00:52:44.260
this stuff.

00:52:44.260 --> 00:52:47.940
And I was like, we figured this out during IE6.

00:52:47.940 --> 00:52:48.900
What happened?

00:52:48.900 --> 00:52:52.420
So yeah, just let you know a little bit of history.

00:52:52.420 --> 00:52:53.540
Look up P3P.

00:52:53.540 --> 00:52:54.500
So yeah.

00:52:54.500 --> 00:52:55.700
Absolutely.

00:52:55.700 --> 00:52:56.740
All right.

00:52:56.740 --> 00:53:00.660
I guess to close out the flock thing, the thing that scares me about this is if I really wanted

00:53:00.660 --> 00:53:06.020
I could open up a private window and I could go, I could even potentially fire up a VPN

00:53:06.020 --> 00:53:08.180
on a different location and go visit a place.

00:53:08.180 --> 00:53:14.260
And when I show up there, no matter how creepy tracking that place happens to be, I am basically

00:53:14.260 --> 00:53:16.100
an unknown to that location.

00:53:16.100 --> 00:53:20.900
Whereas this stuff, if your browser constantly puts you into this category, well, you show up

00:53:20.900 --> 00:53:22.100
already in that category.

00:53:22.100 --> 00:53:26.100
There's really no way to sort of have a fresh start, I guess.

00:53:26.100 --> 00:53:26.900
All right.

00:53:26.900 --> 00:53:31.300
So one thing, Ines, maybe you could speak to this since you're right in the middle of it,

00:53:31.300 --> 00:53:37.700
they talk about how one of the things that's not mentioned in here is, you know, how does it,

00:53:37.700 --> 00:53:41.700
basically they say the regulation does little to incentivize or support EU innovation and

00:53:41.700 --> 00:53:43.060
entrepreneurship in this space.

00:53:43.860 --> 00:53:50.500
There's nothing in here and this law that specifically is to promote EU-based

00:53:50.500 --> 00:53:52.180
ML companies.

00:53:52.180 --> 00:53:53.300
I guess, I think.

00:53:53.300 --> 00:53:55.620
Well, does it even belong there or is it, or is it okay?

00:53:55.620 --> 00:53:56.180
Or what do you think?

00:53:56.180 --> 00:53:56.660
I don't know.

00:53:56.660 --> 00:53:58.260
I was actually, I was a bit confused by that.

00:53:58.260 --> 00:54:01.620
It does remind me of like, well, in general, for a long time, a lot of people have said,

00:54:01.620 --> 00:54:04.100
oh, the EU is like a bad place for startups.

00:54:04.100 --> 00:54:08.740
I think actually regulation is a big part of that, which is sort of, you know, goes full circle.

00:54:08.740 --> 00:54:12.260
Like a lot of people find that, well, the EU is more difficult.

00:54:12.260 --> 00:54:16.100
You have to stick to all of these roles and people actually enforce them and you're less

00:54:16.100 --> 00:54:18.340
free and you can't be whatever the fuck you want.

00:54:18.340 --> 00:54:21.380
So you should go to the US where people are a bit more like chill.

00:54:21.380 --> 00:54:26.500
And it's a bit more common to like, I don't know, ask for forgiveness later and just like,

00:54:26.500 --> 00:54:29.940
so I think, I think that is definitely kind of a mentality that people have.

00:54:29.940 --> 00:54:35.940
So I'm like, I'm not sure, honestly, I'm not sure what like, incentivizing

00:54:35.940 --> 00:54:38.100
EU entrepreneurship could be.

00:54:38.100 --> 00:54:40.740
I actually, I mean, for me personally, like it was a very, for me,

00:54:40.740 --> 00:54:44.180
it was a very conscious decision for us to start a company in Berlin.

00:54:44.180 --> 00:54:47.220
And the EU was like a big part in that.

00:54:47.220 --> 00:54:50.420
I know that like, maybe, you know, I'm not the typical entrepreneur and I'm,

00:54:50.420 --> 00:54:52.900
we're doing things quite differently with our company as well.

00:54:52.900 --> 00:54:57.940
We're not like, you know, your typical startup, but being in the EU was actually very attractive

00:54:57.940 --> 00:54:58.500
to us.

00:54:58.500 --> 00:55:02.500
And even, you know, recently, as we sold some shares in the company, it was incredibly

00:55:02.500 --> 00:55:07.460
important to us to stay a German company and be a company paying taxes to the country that

00:55:07.460 --> 00:55:10.580
we actually incorporated in and not just become a US company.

00:55:10.580 --> 00:55:10.980
Yeah.

00:55:10.980 --> 00:55:14.500
But I know that's not necessarily true for like, everyone.

00:55:14.500 --> 00:55:16.580
But are you maximizing shareholder value?

00:55:16.580 --> 00:55:18.500
No, but I mean, yeah.

00:55:18.500 --> 00:55:23.460
That leads to so many wrongs that this short, short-sighted thinking, I think that's, that's

00:55:23.460 --> 00:55:25.380
great that you have principles about this stuff.

00:55:25.380 --> 00:55:25.540
Yeah.

00:55:25.540 --> 00:55:29.380
But I mean, you know, you did that capitalism is going to capitalism, like, you know, and

00:55:29.380 --> 00:55:33.700
then I say that as like, you know, someone who is also participating in capitalism.

00:55:33.700 --> 00:55:34.100
Sure.

00:55:34.100 --> 00:55:36.740
And yeah, it's like, I mean, I don't know.

00:55:36.740 --> 00:55:41.460
I do think, you know, Europe is becoming more attractive as a location for companies.

00:55:41.460 --> 00:55:46.580
I think Berlin is becoming more attractive as a location to be based in and start a company.

00:55:46.580 --> 00:55:53.300
But it is also true that there are a lot of more general things that make it harder to actually

00:55:53.300 --> 00:55:56.660
run a business here, especially if you directly compare it to the US.

00:55:56.660 --> 00:55:59.380
And yes, a lot of that is also the bureaucracy.

00:55:59.380 --> 00:56:04.500
It is a lot of the, you know, structures not being, you know, as developed.

00:56:04.500 --> 00:56:09.300
It's also, yeah, if you are looking to get investment in your company, it often makes a

00:56:09.300 --> 00:56:14.100
lot more sense to look in the US for that, which then causes other difficulties.

00:56:14.100 --> 00:56:17.460
If you, you know, especially a young company and you don't, you know, you can't have as

00:56:17.460 --> 00:56:21.700
many demands, like in our case, we could be like, okay, here's, here's what we want to do.

00:56:21.700 --> 00:56:23.460
If you're not in that position, you can't do that.

00:56:23.460 --> 00:56:28.740
And so I, I agree with like the problems here, but I don't know how this law and this proposal.

00:56:28.740 --> 00:56:31.460
Yeah. Well, what was it supposed to do? Right?

00:56:31.460 --> 00:56:34.900
Yeah. I mean, say, oh, you're sort of, you're exempt from some of the things.

00:56:34.900 --> 00:56:40.580
If you are like, I don't know, a startup coming to the EU or like...

00:56:40.580 --> 00:56:46.180
Here's how it advantages the EU. Like all companies that are not EU based have to follow this law

00:56:46.180 --> 00:56:47.860
and no rules for the EU based ones.

00:56:47.860 --> 00:56:48.580
No. Yeah.

00:56:48.580 --> 00:56:50.580
Of course that wouldn't be with the principles of it, but right.

00:56:50.580 --> 00:56:51.780
Yeah. I don't know what it would do.

00:56:51.780 --> 00:56:54.980
Company moving to the EU, you would like get like, you know, you don't only have to follow

00:56:54.980 --> 00:56:58.580
half of these things. And then, you know, you can, you know, then everyone's like back in like,

00:56:58.580 --> 00:57:02.580
I don't know. Yeah. Having their like mailbox companies all over Europe.

00:57:02.580 --> 00:57:07.300
Yeah. Exactly. I guess one other thing I would just want to touch on with this law,

00:57:07.300 --> 00:57:12.260
speaking of what is absent. And this also surprises me a little bit is that there's

00:57:12.260 --> 00:57:19.300
nothing in here about climate change and model training and sort of the cost of operation

00:57:19.300 --> 00:57:24.740
of these things. Does that surprise you? Would it belong here? What do you all think?

00:57:24.740 --> 00:57:30.420
I mean, is it a high risk? This is my ask is like when I saw it wasn't in there at all,

00:57:30.420 --> 00:57:37.300
not even lightly mentioned, I was like, how many like carbon emissions do we have to go until it's

00:57:37.300 --> 00:57:42.980
high risk? But evidently the thinking of human side of high risk, actually climate change is also

00:57:42.980 --> 00:57:47.780
human side of high risk on humans. Maybe it's too many steps.

00:57:47.780 --> 00:57:51.460
Is the AI going to kill me eventually or tomorrow?

00:57:51.460 --> 00:57:54.500
Exactly. Is it armed or is it just

00:57:54.500 --> 00:57:59.300
Is it like just 30 years from now when it floods or something? Yeah. Yeah. Yeah.

00:57:59.300 --> 00:58:04.100
And so, yeah, I think like I was definitely curious to see that they didn't include it,

00:58:04.100 --> 00:58:09.540
despite all of the kind of work here from the Greens and other parties like them for,

00:58:09.540 --> 00:58:14.500
you know, climate change awareness when we talk about what is a risk, right? Obviously,

00:58:14.500 --> 00:58:18.180
there's a huge risk for the entire world, right? Yeah.

00:58:18.180 --> 00:58:23.140
I guess it also seems like maybe it was too difficult to like, you know, implement in terms of how do we

00:58:23.140 --> 00:58:29.860
police that? Like, would this then imply, I don't know, would AWS have to report to the EU about like,

00:58:29.860 --> 00:58:32.420
who's using what compute? Yeah.

00:58:32.420 --> 00:58:37.860
And, or I don't know, report if the compute exceeds a certain limit, so that then you can be audited.

00:58:37.860 --> 00:58:42.660
Like, these could all be potential implications, which again then tie into other privacy concerns.

00:58:42.660 --> 00:58:48.420
Yeah. Because yeah, I would, you know, I wouldn't necessarily want like, you know, AWS to sniff around

00:58:48.420 --> 00:58:50.180
my compute. Yeah.

00:58:50.180 --> 00:58:52.900
But maybe they have to, if the EU needs it.

00:58:52.900 --> 00:58:59.300
Like, wait a minute. We just had to reveal that this company did $2 million worth of GPU training.

00:58:59.300 --> 00:59:02.980
And we thought they were just a little small company. What's going on, right? Like some,

00:59:02.980 --> 00:59:08.500
something like that could come out. But you know, I don't know. Something I had in mind is maybe if you

00:59:08.500 --> 00:59:15.860
create ML models for European citizens, those models must be trained with renewable energy or something to

00:59:15.860 --> 00:59:20.420
that effect, right? That you don't have to report it, but that has to be the case. I don't know.

00:59:20.420 --> 00:59:24.580
Yeah. But I mean, I don't know. It's interesting. It's an interesting question because the thing is,

00:59:24.580 --> 00:59:28.900
if you had like, you know, too many restrictions around this, this would encourage people to like,

00:59:28.900 --> 00:59:34.660
I don't know, train less, which then in turn is quite bad. I think actually what's quite important is that,

00:59:34.660 --> 00:59:39.780
like, if you are developing these systems, you should train, you know, you know, you should care

00:59:39.780 --> 00:59:44.100
about like what you're training. You shouldn't like, you know, constantly train these large language

00:59:44.100 --> 00:59:49.540
models for no reason, just so you can, you know, say, oh, look at my, my model that's bigger than yours.

00:59:49.540 --> 00:59:54.420
But it is, you know, on a smaller scale, it's very important to keep training your model, keep

00:59:54.420 --> 01:00:00.340
collecting data and to keep improving it. And to also train models that are really specific for your

01:00:00.340 --> 01:00:04.740
problems and not just find something that you download off the internet or that, you know,

01:00:04.740 --> 01:00:09.940
someone gives you by an API that kind of sort of does what you do. And then you're like, ah,

01:00:09.940 --> 01:00:14.100
that's good enough because that's how you end up with a lot more problems. Like being able to,

01:00:14.100 --> 01:00:18.980
like creating data and being able to train a system for your really, really specific use case.

01:00:18.980 --> 01:00:23.380
That's an advantage. That's not like, you know, a disadvantage that you're trying to avoid.

01:00:23.380 --> 01:00:29.140
Yeah, that's a really good point. It could be absolutely in contrast with some of the other

01:00:29.140 --> 01:00:35.620
things. Like it has to be fair, but if it uses too much training, that's going to go over the

01:00:35.620 --> 01:00:39.140
other one. So let's do less trainings. It's kind of close enough to be unfair, right?

01:00:39.140 --> 01:00:44.420
Yeah. And then that, that encourages people to use, I don't know, just like some arbitrary API

01:00:44.420 --> 01:00:49.620
that they can find, which again is also, you know, not great or like, yeah, I don't know. I think the

01:00:49.620 --> 01:00:54.820
the bigger takeaway or the very important takeaway from these really large language models, in my

01:00:54.820 --> 01:00:58.820
opinion, is it's not necessarily that like, whoa, if we just make it bigger and bigger, we can get a

01:00:58.820 --> 01:01:03.380
system that then can cut us pretty good at pretty much everything considering it's never learned

01:01:03.380 --> 01:01:07.940
anything about any of these things. I think the takeaway, I know, and I think that many people are

01:01:07.940 --> 01:01:13.220
still seeing it this way. And I think the more reasonable takeaway is if it's a model that was just

01:01:13.220 --> 01:01:19.380
trained on like tons of texts can do pretty good things with stuff it's never seen before, how well could a

01:01:19.380 --> 01:01:24.340
much smaller, more specific system do if we actually, you know, trained it on very,

01:01:24.340 --> 01:01:28.420
very small subset of only what we want to do and that will be more efficient. And, you know,

01:01:28.420 --> 01:01:33.220
I think people, we should stop like, you know, hoping that there'll be one model that can magically do,

01:01:33.220 --> 01:01:39.460
I don't know, your arbitrary, like, you know, accounting task and also, I don't know, decide whether

01:01:39.460 --> 01:01:43.860
Michael should get a mortgage or not. Like, I think that's, that's kind of this weird idea. It's like,

01:01:43.860 --> 01:01:47.540
you want a specific system that requires training. I think training is good.

01:01:47.540 --> 01:01:51.300
Yeah. Put a good word in with the mortgage AI for me, will you?

01:01:51.300 --> 01:01:56.660
Catherine, I think you want to have a quick comment on this and maybe we should wrap it up after that.

01:01:56.660 --> 01:02:02.500
Yeah. I mean, I guess I was just going to reference the opportunity and risks of foundation models,

01:02:02.500 --> 01:02:08.580
which I think touches on some of these things. So it's this mega paper and it's exactly, well,

01:02:08.580 --> 01:02:14.340
some of the sections are about exactly this problem of like, why do we believe that we need to have these

01:02:14.340 --> 01:02:21.060
foundational models with these large, extremely large, even larger than the last largest models

01:02:21.060 --> 01:02:26.740
to do all of the things with it also has all these other implications, environmental factors being one

01:02:26.740 --> 01:02:31.300
of them. Because obviously when you train one of these models, it's like driving your car around for like 10

01:02:31.300 --> 01:02:37.060
years or something like this. So, you know, there's big implications. And I think the point of,

01:02:37.060 --> 01:02:41.460
can you build a smaller targeted model to do the same thing? And then the other point of,

01:02:41.460 --> 01:02:48.100
if we need these big models, are there ways for us to hook in and do small bits of training rather than

01:02:48.100 --> 01:02:53.860
to retrain from the start, from the very beginning? I mean, these are like the hard problems that I

01:02:53.860 --> 01:02:59.860
think need solving, not maybe not always building a better recommendation machine. So yeah, if you're

01:02:59.860 --> 01:03:02.260
looking for a problem, solve some of these problems.

01:03:02.260 --> 01:03:07.780
Yeah. Fantastic. This is a big article. The PDF is published. People can check it out. We'll link to

01:03:07.780 --> 01:03:08.340
it in the show notes.

01:03:08.340 --> 01:03:12.420
Yeah. Also, actually, a good point on the foundation. Sorry, no, I just wanted to say,

01:03:12.420 --> 01:03:16.900
sorry, I've been referring to these as language models. I've been trying to train myself to use the

01:03:16.900 --> 01:03:21.380
like, you know, more explicit term because I think foundation models are a much better way

01:03:21.380 --> 01:03:24.980
to expect this. And I'm so happy this term was introduced because it finally solves

01:03:24.980 --> 01:03:29.140
a lot of these problems of everything being a model that I think causes a lot of confusion

01:03:29.140 --> 01:03:32.660
when talking about machine learning. Yeah. Excellent. Haley asks what,

01:03:32.660 --> 01:03:36.180
in the audience asks, what does climate change have to do with this? The reason I brought it up,

01:03:36.180 --> 01:03:42.020
one, is because Europe seems to be leading at least in the consensus side of things,

01:03:42.020 --> 01:03:46.420
trying to address climate change. I feel like there's a lot of citizens there where it's

01:03:46.420 --> 01:03:50.420
on their mind and they want the government to do something about it and stuff.

01:03:50.420 --> 01:03:56.020
The governments do a lot there. So as a law, I thought, you know, maybe it would touch on that

01:03:56.020 --> 01:04:00.340
because Catherine, you pointed out some crazy number. You want to like, just reemphasize

01:04:00.340 --> 01:04:05.380
the cost of some of these things. It's not just like, oh, well, it's like leaving a few lights on.

01:04:05.380 --> 01:04:10.820
It's a lot. It's a whole lot. No, it's just huge. Yeah. And they keep getting bigger. I forget who

01:04:10.820 --> 01:04:16.100
released the newest one. I don't know if you remember Ines, but it's like they keep getting

01:04:16.100 --> 01:04:20.660
bigger and bigger. So some of these have like billions and billions and billions of parameters.

01:04:20.660 --> 01:04:27.140
they sometimes have extremely large amounts of data, either as external reference or in the model

01:04:27.140 --> 01:04:34.020
itself. And yeah, Tivnick Gebru's paper that essentially she was basically fired from Google

01:04:34.020 --> 01:04:41.940
for researching was around the, or one of the parts of the paper was around how much carbon emissions come

01:04:41.940 --> 01:04:47.140
from training these models. They've only gotten bigger since that paper. And yeah, it's just,

01:04:47.140 --> 01:04:54.420
I may have the statistic wrong, but it is as it is almost as bad as driving a car around, you know,

01:04:54.420 --> 01:05:00.100
with the motor on every day, you know, with your normal commute for like 10 plus years to just train

01:05:00.100 --> 01:05:06.340
one model. And it's really absurd because some of these models were just training to prove that we can

01:05:06.340 --> 01:05:08.340
train them. And so it's like, yeah.

01:05:08.340 --> 01:05:12.260
Yeah. Yeah. And often it's not, the, the, the, the artifact isn't even as useful where it's like, okay,

01:05:12.260 --> 01:05:16.500
with a lot of the bird models, we can at least, I think it's good that we just reuse these weights.

01:05:16.500 --> 01:05:21.860
And I think often in practice, that's what's done, you know, some, you take some of these weights that

01:05:21.860 --> 01:05:26.020
someone else has trained or use these embeddings, and then you train something else on top of that.

01:05:26.020 --> 01:05:27.700
Like transfer learning or something like that.

01:05:27.700 --> 01:05:32.980
Yeah. Or even just like you use these embeddings to initialize your model, and then you train different

01:05:32.980 --> 01:05:38.260
components using these embeddings. And that's, that is efficient, but it also makes

01:05:38.260 --> 01:05:42.740
that, okay, we kind of stuck with a lot of these, like, you know, artifacts that are getting,

01:05:42.740 --> 01:05:44.100
you know, stale all the time.

01:05:44.100 --> 01:05:49.140
Yeah. So the comment in the audience was, I could train one on my laptop and use electricity, like,

01:05:49.140 --> 01:05:54.180
true, but it's like 50,000 laptops, or is, I mean, it's an, it's a much different thing.

01:05:54.180 --> 01:05:56.740
I mean, it's a lot more. Exactly. No. And I think training on a laptop is great. Like,

01:05:56.740 --> 01:06:02.820
for example, we recently did some work to be able to hook into the accelerate library on the new M1

01:06:02.820 --> 01:06:08.180
MacBooks, which make things a lot faster in space. And that was quite cool to see. And we want to do a

01:06:08.180 --> 01:06:13.780
bit more there because like, oh, you can really, you know, if we optimize this further, you can actually train a model on your

01:06:13.780 --> 01:06:19.220
MacBook. And this can be really accurate. And you don't necessarily need like all this computer power.

01:06:19.220 --> 01:06:20.180
Yeah.

01:06:20.180 --> 01:06:22.500
And yep. So training on your laptop is good.

01:06:22.500 --> 01:06:26.900
It is, if you could do it. But a lot of the ones that we're actually talking about use,

01:06:26.900 --> 01:06:31.700
like these huge, huge modules that take a lot. So you can say you don't really care about climate

01:06:31.700 --> 01:06:39.140
change or whatever. But if you do, the ML training side has a pretty significant impact. And I was

01:06:39.140 --> 01:06:43.460
unsure whether or not to see it. But yeah, I guess it makes sense that it's not there. Who knows?

01:06:43.460 --> 01:06:50.100
It's also, they also, they said, this is the foundation for potentially future AI laws in Europe.

01:06:50.100 --> 01:06:54.020
Yeah. And also appreciate that. Okay. They didn't want to tie, you know, tie everything together.

01:06:54.020 --> 01:06:58.020
Like I can't even, I think from a political perspective, if you are proposing this pretty

01:06:58.020 --> 01:07:04.660
bold framework for regulation, tying it into too many other topics can easily, I don't know,

01:07:04.660 --> 01:07:09.300
distract from like the core point that they want to make. So I think it might've actually been like a,

01:07:09.300 --> 01:07:10.740
you know, stupid decision.

01:07:10.740 --> 01:07:17.060
Yep. Absolutely. All right, ladies, this has been a fantastic conversation. I've learned a lot and

01:07:17.060 --> 01:07:20.740
really enjoyed having you here. Now, before we get out of here, maybe since there's two of you

01:07:20.740 --> 01:07:25.300
and we're sort of over time, I'll just ask you one question. So the two. So if you're going to write

01:07:25.300 --> 01:07:28.340
some Python code, what editor are you using these days? Catherine, you go first.

01:07:28.340 --> 01:07:32.020
I'm still in Vim. Am I old? I think I'm old now.

01:07:32.020 --> 01:07:34.020
I still use Vim.

01:07:34.020 --> 01:07:35.300
Oh, you're classic. Come on.

01:07:35.300 --> 01:07:47.860
No, I'm quite boring. Visual Studio Code. I've been using that for years. It's very nice. I think,

01:07:47.860 --> 01:07:50.900
I think it's probably the most common answer you get. And it's quite, yeah.

01:07:50.900 --> 01:07:52.180
It's certainly the last year.

01:07:52.180 --> 01:07:54.580
I think using Vim is a lot edgier and cooler. I wish, you know,

01:07:54.580 --> 01:07:57.860
maybe for that reason alone, I should just like, you know.

01:07:57.860 --> 01:08:02.740
She edits code without even a window. It just appears on this black surface.

01:08:02.740 --> 01:08:08.660
I mean, my microphone, he programs that way. And I'm like, okay, if it makes you happy.

01:08:08.660 --> 01:08:10.180
Yeah, yeah, you do. Awesome.

01:08:10.180 --> 01:08:13.700
Some people just like to suffer. That's okay.

01:08:13.700 --> 01:08:15.140
Oh, damn.

01:08:15.140 --> 01:08:17.300
All right.

01:08:17.300 --> 01:08:21.220
I'm sorry. No offense. Like, I don't know. No offense. This was a joke. No offense to anyone who's

01:08:21.220 --> 01:08:25.300
programming Vim. I know lots of great people who do that. I don't want to get any paid messages.

01:08:25.300 --> 01:08:32.100
Please. Don't email me. All right. All right. Well, Catherine Ines,

01:08:32.100 --> 01:08:34.900
thanks for coming back on the show and sharing your thoughts.

01:08:34.900 --> 01:08:37.140
Yeah. Thanks for having me. Yeah. Thanks for having me.

01:08:37.140 --> 01:08:37.540
Yeah. Bye.

01:08:37.540 --> 01:08:37.860
Ciao.

01:08:37.860 --> 01:08:43.940
This has been another episode of Talk Python to Me. Thank you to our sponsors.

01:08:43.940 --> 01:08:46.740
Be sure to check out what they're offering. It really helps support the show.

01:08:46.740 --> 01:08:52.820
Take some stress out of your life. Get notified immediately about errors and performance issues

01:08:52.820 --> 01:08:59.620
in your web or mobile applications with Sentry. Just visit talkpython.fm/sentry and get started for

01:08:59.620 --> 01:09:05.620
free. And be sure to use the promo code talkpython, all one word. Add high-performance,

01:09:05.620 --> 01:09:11.620
multi-party video calls to any app or website with SignalWire. Visit talkpython.fm/signalwire

01:09:11.620 --> 01:09:15.940
and mention that you came from Talk Python To Me to get started and grab those free credits.

01:09:15.940 --> 01:09:21.620
Want to level up your Python? We have one of the largest catalogs of Python video courses over at

01:09:21.620 --> 01:09:27.540
Talk Python. Our content ranges from true beginners to deeply advanced topics like memory and async.

01:09:27.540 --> 01:09:33.220
And best of all, there's not a subscription in sight. Check it out for yourself at training.talkpython.fm.

01:09:33.220 --> 01:09:48.260
Be sure to subscribe to the show, open your favorite podcast app, and search for Python. We should be right at the top. You can also find the iTunes feed at /itunes, the Google Play feed at /play, and the direct RSS feed at /rss on talkpython.fm.

01:09:49.220 --> 01:09:53.300
We're live streaming most of our recordings these days. If you want to be part of the show and have

01:09:53.300 --> 01:10:00.100
your comments featured on the air, be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

01:10:00.100 --> 01:10:14.900
This is your host, Michael Kennedy. Thanks so much for listening. I really appreciate it. Now get out there and write some Python code.

01:10:14.900 --> 01:10:27.780
I'll see you next time. Bye.

