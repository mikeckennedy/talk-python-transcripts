WEBVTT

00:00:00.001 --> 00:00:04.800
On this episode, Rob Emanuel and Tom Augsberger join us to talk about building and running

00:00:04.800 --> 00:00:10.360
Microsoft's Planetary Computer Project. This project is dedicated to providing the data around

00:00:10.360 --> 00:00:15.300
climate records and the compute necessary to process it with the mission of helping us all

00:00:15.300 --> 00:00:20.860
understand climate change better. It combines multiple petabytes of data with a powerful

00:00:20.860 --> 00:00:28.120
hosted JupyterLab notebook environment to process it. This is Talk Python to Me, episode 334,

00:00:28.120 --> 00:00:30.560
recorded September 9th, 2021.

00:00:30.560 --> 00:00:48.520
Welcome to Talk Python to Me, a weekly podcast on Python. This is your host, Michael Kennedy.

00:00:48.520 --> 00:00:53.140
Follow me on Twitter where I'm @mkennedy and keep up with the show and listen to past episodes

00:00:53.140 --> 00:00:57.820
at talkpython.fm and follow the show on Twitter via at talkpython.

00:00:57.820 --> 00:01:03.200
We've started streaming most of our episodes live on YouTube. Subscribe to our YouTube channel over

00:01:03.200 --> 00:01:08.960
at talkpython.fm/youtube to get notified about upcoming shows and be part of that episode.

00:01:08.960 --> 00:01:14.000
This episode is brought to you by Shortcut, formerly known as clubhouse.io,

00:01:14.000 --> 00:01:19.040
and us over at Talk Python Training. And the transcripts are brought to you by Assembly AI.

00:01:19.040 --> 00:01:22.760
Rob, Tom, welcome to Talk Python to Me.

00:01:22.760 --> 00:01:23.240
Thank you.

00:01:23.240 --> 00:01:29.140
Good to have you both here. We get to combine a bunch of fun topics and important topics,

00:01:29.140 --> 00:01:36.400
data science, Python, the cloud, big data, as in physically lots of data to deal with. And then

00:01:36.400 --> 00:01:42.180
also climate change and being proactive about studying that, make predictions and do science

00:01:42.180 --> 00:01:43.340
on huge amounts of data.

00:01:43.340 --> 00:01:44.420
For sure. Looking forward to it.

00:01:44.420 --> 00:01:45.080
Yeah, this will be fun.

00:01:45.080 --> 00:01:49.360
Yeah, absolutely. Before we get into those, let's just start real quickly. How do you two get into

00:01:49.360 --> 00:01:51.540
programming and Python? Rob, start with you?

00:01:51.540 --> 00:01:58.100
Yeah, sure. So I've been a developer for, I don't know, let's say 14 years. I started at a shop that

00:01:58.100 --> 00:02:00.200
was doing Sybase Power Builder.

00:02:00.200 --> 00:02:02.000
That goes back a ways.

00:02:02.000 --> 00:02:07.000
That's back a ways. And I actually, I come from a math background, so I didn't know a lot about

00:02:07.000 --> 00:02:13.180
programming and started using Python just sort of like on the side to parse some bank statements and

00:02:13.180 --> 00:02:18.840
do some personal stuff and started actually like integrating some of our source control at the

00:02:18.840 --> 00:02:24.080
company with Python and had to write some C extensions. So got into the Python source code

00:02:24.080 --> 00:02:29.320
and started reading, you know, that code and being like, oh, this is how programming should

00:02:29.320 --> 00:02:35.640
work. Like this is really good code. And that year went to my first PyCon. It was just like all in.

00:02:35.640 --> 00:02:39.080
I need to get, you know, a different job where I'm not doing Power Builder.

00:02:39.080 --> 00:02:44.520
And yeah, really, I kind of credit Python and the code base and setting me on, you know,

00:02:44.520 --> 00:02:45.960
a better development path for sure.

00:02:45.960 --> 00:02:48.960
Oh, that's super cool. Python's a fun experience, isn't it?

00:02:48.960 --> 00:02:49.280
Oh, yeah.

00:02:49.280 --> 00:02:54.600
Yeah. It's like my geek holiday, but sadly, the geek holiday has been canceled the last two years.

00:02:54.600 --> 00:02:55.060
Oh, no.

00:02:55.060 --> 00:02:55.800
Yeah.

00:02:55.800 --> 00:02:57.300
Yeah. Tom, how about you?

00:02:57.300 --> 00:03:01.780
Kind of similar to a lot of your guests, I think. I was in grad school and had to pick up

00:03:01.780 --> 00:03:07.140
programming for research and simulations. This is for economics.

00:03:07.140 --> 00:03:12.920
They started us on MATLAB and Fortran. It goes back maybe further, almost as far as you can go.

00:03:12.920 --> 00:03:18.640
And anyway, I didn't really care for MATLAB. So moved over to Python pretty quickly and then

00:03:18.640 --> 00:03:25.820
just started enjoying the data analysis side more than the research side and got into like that whole

00:03:25.820 --> 00:03:32.280
open source ecosystem around pandas and stats models and econometrics library. So I started

00:03:32.280 --> 00:03:38.360
contributing to open source, dropped out, got a job in data science stuff, and then moved on to

00:03:38.360 --> 00:03:43.880
Anaconda where I worked on open source libraries like Pandas and Dask for a few years.

00:03:43.880 --> 00:03:48.700
Yeah. In a weird turn of coincidence, a weird coincidence, I was just the previous episode

00:03:48.700 --> 00:03:51.820
with Stan Sievert, who you worked with over there, right?

00:03:51.820 --> 00:03:52.300
Yeah.

00:03:52.960 --> 00:03:58.140
Yeah. So he's the director of community innovation. And then, yeah, it was a great place to work at.

00:03:58.140 --> 00:04:02.840
Really enjoyed it. And then came on to this team at Microsoft almost a year ago now working on the

00:04:02.840 --> 00:04:03.480
planetary computer.

00:04:03.480 --> 00:04:08.740
Yeah. Cool. Well, the planetary computer stuff sounds super neat. You get to play with all the

00:04:08.740 --> 00:04:11.280
high-end computers and the big data and whatnot, right?

00:04:11.420 --> 00:04:15.900
Yeah. It's a lot of fun. Although I did have a chance to play on, I think it was Summit, which is one of our

00:04:15.900 --> 00:04:19.780
nation's supercomputers at my last show. So that was a lot of fun too.

00:04:19.780 --> 00:04:25.160
Okay. Well, it's hard to beat that, right? That's one of the ones that's like, it takes up a whole room,

00:04:25.160 --> 00:04:29.220
a huge room. That's pretty fantastic. Awesome. All right. Well, what are you two doing today?

00:04:29.220 --> 00:04:34.500
You know, you're both on the planetary computer project. Are you working at Microsoft? What are you doing there?

00:04:34.500 --> 00:04:39.780
Yeah. So we're on a pretty small team that's building out a planetary computer, which really

00:04:39.780 --> 00:04:45.620
is sort of three components, which is a data catalog, you know, hosting a lot, you know,

00:04:45.620 --> 00:04:51.540
petabytes, petabytes of data, openly licensed satellite imagery and other datasets on Azure's

00:04:51.540 --> 00:04:58.760
blob storage. We're building APIs and running API services that ETL the data, encode metadata,

00:04:58.760 --> 00:05:03.600
according to the stack specification, which we can get into later about that, those datasets,

00:05:03.840 --> 00:05:08.640
putting them into a Postgres database and then building API services on top of that.

00:05:08.640 --> 00:05:16.380
That's a lot of what I do is manage the ETL pipelines and the APIs and then expose that data

00:05:16.380 --> 00:05:21.160
to users, environmental data scientists, and really anybody. It's just publicly accessible.

00:05:21.160 --> 00:05:26.040
And yeah, that's sort of my side. And then there's a compute platform, which Tom can talk about.

00:05:26.040 --> 00:05:32.920
Yeah. So all this is like in service of environmental sustainability. And so we have our primary users are like

00:05:32.920 --> 00:05:44.460
people who know how to code mostly in Python, but they're not developers. And so, you know, we don't want them having to worry about things like Kubernetes or whatever to set up a distributed compute cluster.

00:05:44.720 --> 00:06:07.520
So that's where we're kind of this hub comes as it's a place where users can go log in, get a nice convenient computing platform built on top of Jupyter hub and dash where they can scale out to these really large workflows to do whatever analysis they need, produce whatever derived data sets they need for them to pass along to their decision makers and environmental sustainability.

00:06:07.520 --> 00:06:19.060
Yeah, that's super cool. The platform miller building people who might have some Python skills, some data science skills, but not necessarily high end cloud programming.

00:06:19.060 --> 00:06:29.520
Right. Yeah. Handling lots of data, setting up clusters, all those kinds of things. You just push a button, end up in a notebook. The notebook is nearby petabytes of data. Right. Right. Exactly.

00:06:29.520 --> 00:06:40.680
So we'll talk a lot about like cloud native computing data analysis. And so really what that means is just putting the compute as close as the data as possible. So in the same Azure region.

00:06:40.680 --> 00:06:41.980
So you just need a big hard drive.

00:06:41.980 --> 00:06:44.700
A really, really, really, really big hard drive.

00:06:44.700 --> 00:06:46.460
That's what the cloud is. It's one third hard drive.

00:06:46.460 --> 00:07:09.080
Exactly. It is. Yeah. So super neat. Before we get into it, though, let's just maybe talk real briefly about, you know, Microsoft and the environment. This obviously is an initiative you all are putting together to help client climate scientists study the climate and whatnot. But, you know, I was really excited to see last year that you all announced that Microsoft will be carbon negative by 2030.

00:07:09.080 --> 00:07:21.040
Yeah, for sure. I mean, Microsoft and, you know, prior to me joining Microsoft, I didn't I didn't know any of this, but Microsoft's been on the forefront of, you know, corporate efforts and environmental sustainability for a long time.

00:07:21.040 --> 00:07:32.180
And, you know, there's been an internal carbon tax that we place on business groups that were, you know, there's actual payments made based on how much carbon emission each business group creates.

00:07:32.180 --> 00:07:37.180
And that's been used to fund the environmental sustainability team and all these efforts.

00:07:37.180 --> 00:07:44.240
And that sort of culminated into these four focus areas and commitments that were announced in 2020.

00:07:44.240 --> 00:07:55.300
So carbon is a big one, not just carbon negative by 2030, but by 2050, actually having removed more carbon than Microsoft has ever produced since its inception.

00:07:55.460 --> 00:08:02.300
And that's over scope one, scope two and scope three, which means accounting for, you know, downstream and upstream providers.

00:08:02.300 --> 00:08:06.840
And then there's a couple more focus areas around waste.

00:08:06.840 --> 00:08:18.360
So by 2030, achieving zero waste and around water becoming water positive and ensuring accessibility to clean drinking and sanitation water for more than 1.5 million people.

00:08:18.360 --> 00:08:23.260
There's an ecosystem element to by 2025, protecting more land than we use.

00:08:23.260 --> 00:08:36.240
And then also creating a planetary computer, which is really using Azure's resources in the effort to model, monitor and ultimately manage Earth's natural systems.

00:08:36.240 --> 00:08:36.780
That's awesome.

00:08:36.780 --> 00:08:38.020
And that's the part you all come in, right?

00:08:38.020 --> 00:08:38.960
Yeah, exactly.

00:08:38.960 --> 00:08:41.180
The planetary computers in that ecosystem commitment.

00:08:41.180 --> 00:08:42.700
And that's what we're working towards.

00:08:42.700 --> 00:08:43.640
Yeah, very cool.

00:08:43.640 --> 00:08:47.780
The removing all the historical carbon, I think is pretty fantastic.

00:08:47.960 --> 00:08:50.180
And being carbon negative, right?

00:08:50.180 --> 00:08:58.900
So much stuff runs on Azure and on these couple of large clouds that that actually is a statement about a large portion of the data center usage as well.

00:08:58.900 --> 00:08:59.400
For sure.

00:08:59.400 --> 00:09:00.940
How many data centers does Azure have?

00:09:00.940 --> 00:09:02.280
Like three or four, right?

00:09:02.280 --> 00:09:04.140
I think it's the most out of any of them.

00:09:04.140 --> 00:09:05.100
It's a lot, right?

00:09:05.100 --> 00:09:06.960
It's over 50 or something like that?

00:09:06.960 --> 00:09:08.080
Large data centers?

00:09:08.080 --> 00:09:08.960
I don't know.

00:09:08.960 --> 00:09:09.800
But it's a big number.

00:09:09.800 --> 00:09:10.760
They're building them all the time too.

00:09:10.760 --> 00:09:11.100
Yeah.

00:09:11.100 --> 00:09:11.980
Yeah, yeah.

00:09:11.980 --> 00:09:12.860
It's like constant.

00:09:12.860 --> 00:09:14.300
So that's a big deal.

00:09:14.300 --> 00:09:15.100
Super cool.

00:09:15.100 --> 00:09:15.720
All right.

00:09:15.720 --> 00:09:18.100
Let's talk about this planetary computer.

00:09:18.100 --> 00:09:20.860
You told us a little bit about the motivation there.

00:09:20.860 --> 00:09:23.780
And it's made up of three parts, right?

00:09:23.780 --> 00:09:24.400
All right.

00:09:24.400 --> 00:09:24.560
Yeah.

00:09:24.560 --> 00:09:25.180
So tell us about it.

00:09:25.180 --> 00:09:27.140
So there's technically four parts.

00:09:27.140 --> 00:09:32.440
We recognize that technology for technology's sake is just kind of spinning your wheels, right?

00:09:32.820 --> 00:09:38.800
We have to be building all of this data, all this data access, the analytics platform towards

00:09:38.800 --> 00:09:45.000
applying data and insights to actually making an impact on environmental sustainability concerns.

00:09:45.000 --> 00:09:49.580
And that's done not by us, an engineering team, like kind of, you know, trying to figure out

00:09:49.580 --> 00:09:50.800
the climate scientists, right?

00:09:50.800 --> 00:09:56.780
We're engaging with organizations to build out applications specifically on these data and

00:09:56.780 --> 00:10:00.820
services, you know, and partnering with organizations that have specific goals.

00:10:00.940 --> 00:10:04.640
So there's an applications pillar to the planetary computer.

00:10:04.640 --> 00:10:10.660
But from an engineering standpoint, we're mostly focused on the data catalog, the APIs,

00:10:10.660 --> 00:10:12.600
and the hub that we had touched on briefly.

00:10:12.600 --> 00:10:12.900
Yeah.

00:10:12.900 --> 00:10:13.460
Yeah.

00:10:13.460 --> 00:10:17.720
And then the applications is what the partners and other people building on top of it are really

00:10:17.720 --> 00:10:18.180
doing, right?

00:10:18.180 --> 00:10:18.640
Exactly.

00:10:18.780 --> 00:10:23.900
And we participate in that and, you know, help bring different organizations together

00:10:23.900 --> 00:10:30.280
to build out the applications and, you know, use the money that we have to actually fund

00:10:30.280 --> 00:10:34.280
applications that are, you know, specifically aimed at different use cases.

00:10:34.280 --> 00:10:34.640
Yeah.

00:10:34.640 --> 00:10:34.980
Yeah.

00:10:34.980 --> 00:10:35.360
Very cool.

00:10:35.360 --> 00:10:38.040
There are some other things that are somewhat like this, right?

00:10:38.040 --> 00:10:42.120
Like Google Earth Engine and AWS and probably could just grab this yourself.

00:10:42.120 --> 00:10:44.540
You want to do a compare and contrast for us?

00:10:44.540 --> 00:10:44.820
Sure.

00:10:44.900 --> 00:10:45.460
Yeah.

00:10:45.460 --> 00:10:51.860
So Google Earth Engine is sort of the bar that's set as far as using cloud compute resources

00:10:51.860 --> 00:10:53.340
for earth science.

00:10:54.040 --> 00:10:59.800
And it's an amazing platform that's been around for a long time and is really just like a

00:10:59.800 --> 00:11:05.700
giant compute cluster that has interfaces into an API and sort of like a JavaScript interface

00:11:05.700 --> 00:11:08.580
into it that you can run geospatial analytics.

00:11:08.580 --> 00:11:12.280
And so it's a great, you know, like I said, a great tool.

00:11:12.280 --> 00:11:13.760
Can't sing its praises enough.

00:11:13.760 --> 00:11:21.020
One of the aspects of it that make it less useful in certain contexts is that it is a little

00:11:21.020 --> 00:11:22.480
bit of a black box, right?

00:11:22.600 --> 00:11:26.240
The operations, the geospatial operations that you can do on it, the way that you can

00:11:26.240 --> 00:11:30.580
manipulate the data are sort of whatever Google Earth Engine provides.

00:11:30.580 --> 00:11:35.720
If you wanted to run a PyTorch model against a large set of satellite imagery, that's like

00:11:35.720 --> 00:11:36.580
a lot more difficult.

00:11:36.580 --> 00:11:38.820
You can't really do that inside a Google Earth Engine.

00:11:38.820 --> 00:11:42.400
You have to like ship data out and ship data in and getting data in and out of the system

00:11:42.400 --> 00:11:45.460
is a little tough because it's like sort of a singular solution.

00:11:45.460 --> 00:11:48.040
And they can optimize a lot based on that.

00:11:48.040 --> 00:11:53.380
So the approach we're taking is more a modular approach, leaning heavily on the open source

00:11:53.380 --> 00:12:00.080
ecosystems of tools, trying to, you know, make sure that the open source users are first class

00:12:00.080 --> 00:12:01.580
users that we're thinking of first.

00:12:01.900 --> 00:12:06.940
And that if people want to just use our data, we just have cloud optimized geotiff, you know,

00:12:06.940 --> 00:12:10.300
these, these flat file formats on blob storage, go ahead and use it.

00:12:10.300 --> 00:12:11.960
You don't have to use any of the other stuff that we're building.

00:12:12.160 --> 00:12:17.740
But if you want to do space to temporal searches over it, we provide an API that's free access

00:12:17.740 --> 00:12:22.740
that allows you to do searches and get metadata about the data so you don't have to actually

00:12:22.740 --> 00:12:24.100
read in the bytes.

00:12:24.100 --> 00:12:30.500
And then also providing the hub experience, which brings together that really rich open source

00:12:30.500 --> 00:12:33.820
ecosystem of Python tooling, including our tooling.

00:12:33.820 --> 00:12:37.480
And we're also building out, you know, other mechanisms to access this data.

00:12:37.640 --> 00:12:41.000
But those current focuses is really on that Python data science.

00:12:41.000 --> 00:12:47.340
But yeah, considering the open source ecosystem sort of as our user experience and trying to

00:12:47.340 --> 00:12:49.640
treat that as like the first class use case.

00:12:49.640 --> 00:12:50.660
Yeah, that's fantastic.

00:12:50.660 --> 00:12:52.720
Tom, tell me if I have this right.

00:12:52.720 --> 00:12:57.120
I feel like my limited experience working with this is you've got these incredible amounts

00:12:57.120 --> 00:12:58.580
of data, but they're super huge.

00:12:58.580 --> 00:13:04.040
You all built these APIs that let you ask questions and filter it down into like, like, I just want

00:13:04.040 --> 00:13:07.120
the map data for this, you know, polygon or whatever.

00:13:07.120 --> 00:13:12.200
And then you provide a Jupyter notebook and the compute to do stuff on that result.

00:13:12.200 --> 00:13:13.860
Is that pretty good?

00:13:13.860 --> 00:13:14.120
Yeah.

00:13:14.120 --> 00:13:15.140
Yeah, that's pretty good.

00:13:15.140 --> 00:13:20.380
If you just think like the API is so crucial to have and we'll get into what it's built on.

00:13:20.380 --> 00:13:26.040
But just for like the Python analogy here is like, imagine that you only had lists for your

00:13:26.040 --> 00:13:26.660
data structure.

00:13:26.660 --> 00:13:27.740
You don't have dictionaries.

00:13:27.740 --> 00:13:34.280
And now you have to like traverse this entire list of files to figure out where is this one

00:13:34.280 --> 00:13:35.960
at like in space on Earth?

00:13:35.960 --> 00:13:36.600
Where is it at?

00:13:36.600 --> 00:13:38.000
Or what time period is it covering?

00:13:38.000 --> 00:13:43.580
And, you know, the nice thing about the API is you're able to do very fast lookups over

00:13:43.580 --> 00:13:47.560
space and time with that to get down to your subset that you care about.

00:13:47.680 --> 00:13:53.140
And then bring it into memory on ideally on machines that are in the same Azure region,

00:13:53.140 --> 00:13:59.020
bring those data sets into memory using tools like Xarray or Pandas and Dask, things like

00:13:59.020 --> 00:13:59.220
that.

00:13:59.220 --> 00:13:59.540
Yeah.

00:13:59.540 --> 00:14:00.120
Very cool.

00:14:00.120 --> 00:14:03.060
So, Rob, you mentioned the Postgres database.

00:14:03.060 --> 00:14:08.280
Does that do you parse this data and generate the metadata and all that and then store some

00:14:08.280 --> 00:14:11.720
of that information in the database so you get to it super quick and then you've got the

00:14:11.720 --> 00:14:13.920
raw files as blob storage, something like that?

00:14:14.040 --> 00:14:14.580
Yeah, for sure.

00:14:14.580 --> 00:14:21.280
I mean, that's as much metadata that you can capture and to describe the data so that you

00:14:21.280 --> 00:14:24.600
can kind of, you know, do what Tom said and like ignore the stuff that you don't care about

00:14:24.600 --> 00:14:26.120
and just get to the area that you care about.

00:14:26.120 --> 00:14:32.680
We try to extract that and we do that according to a spec that is this really interesting, like

00:14:32.680 --> 00:14:37.440
community driven spec that one of the biggest complaints about dealing with satellite imagery

00:14:37.440 --> 00:14:40.880
and this Earth's observation imagery is that it's kind of a mess.

00:14:41.000 --> 00:14:45.140
There's a lot of different scientific variables and sensor variables and things.

00:14:45.140 --> 00:14:50.780
So there's been a community effort over the past, say, three or four years to develop specifications

00:14:50.780 --> 00:14:53.380
that make this type of information machine readable.

00:14:53.380 --> 00:15:00.740
And so we've kind of bought fully into that and have processes to look at the data, extract

00:15:00.740 --> 00:15:06.940
the stack metadata, which is just a JSON schema specification with extensions, and then write

00:15:06.940 --> 00:15:08.480
that into Postgres.

00:15:08.480 --> 00:15:13.380
And one of the things that we, you know, I've been trying to do for like transparency and

00:15:13.380 --> 00:15:19.340
contribution to open source is a lot of that ETL code base, those Python, the Python code that

00:15:19.340 --> 00:15:25.660
actually works over the files and extracts the metadata is open source in the stack utils,

00:15:25.660 --> 00:15:26.800
GitHub organization.

00:15:26.800 --> 00:15:32.540
So we're trying to contribute to that sort of body of work of how to generate stack metadata

00:15:32.540 --> 00:15:34.560
for these different data types.

00:15:34.780 --> 00:15:40.100
You want like the metadata for the exact same image that's coming from like the USGS public

00:15:40.100 --> 00:15:41.160
sector data set.

00:15:41.160 --> 00:15:47.880
You want the stack metadata to be identical for that, whether you're using our API or Google

00:15:47.880 --> 00:15:50.320
Earth engines, who also provides a stack API.

00:15:50.320 --> 00:15:55.320
And so like we're working together on these kind of like shared core infrastructure libraries.

00:15:57.160 --> 00:16:02.440
This portion of Talk Python to Me is brought to you by Shortcut, formerly known as clubhouse.io.

00:16:02.440 --> 00:16:04.300
Happy with your project management tool?

00:16:04.300 --> 00:16:09.540
Most tools are either too simple for a growing engineering team to manage everything or way

00:16:09.540 --> 00:16:12.680
too complex for anyone to want to use them without constant prodding.

00:16:12.680 --> 00:16:15.600
Shortcut is different though, because it's worse.

00:16:15.600 --> 00:16:16.240
No, wait.

00:16:16.240 --> 00:16:17.060
No, I mean, it's better.

00:16:17.060 --> 00:16:20.820
Shortcut is project management built specifically for software teams.

00:16:20.900 --> 00:16:25.580
It's fast, intuitive, flexible, powerful, and many other nice positive adjectives.

00:16:25.580 --> 00:16:28.180
Key features include team-based workflows.

00:16:28.180 --> 00:16:33.260
Individual teams can use default workflows or customize them to match the way they work.

00:16:33.260 --> 00:16:35.280
Org-wide goals and roadmaps.

00:16:35.280 --> 00:16:39.400
The work in these workflows is automatically tied into larger company goals.

00:16:39.720 --> 00:16:44.960
It takes one click to move from a roadmap to a team's work to individual updates and back.

00:16:44.960 --> 00:16:46.840
Type version control integration.

00:16:46.840 --> 00:16:52.180
Whether you use GitHub, GitLab, or Bitbucket, clubhouse ties directly into them so you can update

00:16:52.180 --> 00:16:53.680
progress from the command line.

00:16:53.680 --> 00:16:55.500
Keyboard-friendly interface.

00:16:55.500 --> 00:17:00.480
The rest of Shortcut is just as friendly as their power bar, allowing you to do virtually

00:17:00.480 --> 00:17:02.400
anything without touching your mouse.

00:17:02.400 --> 00:17:03.660
Throw that thing in the trash.

00:17:03.660 --> 00:17:05.260
Iteration planning.

00:17:05.800 --> 00:17:10.240
Set weekly priorities and let Shortcut run the schedule for you with accompanying burndown

00:17:10.240 --> 00:17:11.380
charts and other reporting.

00:17:11.380 --> 00:17:16.220
Give it a try over at talkpython.fm/shortcut.

00:17:16.220 --> 00:17:19.800
Again, that's talkpython.fm/shortcut.

00:17:19.800 --> 00:17:24.400
Choose Shortcut because you shouldn't have to project manage your project management.

00:17:24.400 --> 00:17:30.920
Well, let's dive into some of the data, actually, and talk a little bit about all these data sets.

00:17:30.920 --> 00:17:34.380
So a lot of data, as we said over here.

00:17:34.380 --> 00:17:39.120
Maybe highlight some of the important data sets that you all have on offer.

00:17:39.120 --> 00:17:44.220
So Sentinel-2 is our largest and is incredibly important.

00:17:44.220 --> 00:17:50.800
It's multispectral imagery, optical imagery that is 10 meter resolution.

00:17:50.800 --> 00:17:53.660
So it's the highest resolution.

00:17:53.660 --> 00:17:57.600
And when we talk about satellites, we often talk about what is the resolution that's captured.

00:17:57.600 --> 00:18:02.600
Because, you know, something like Landsat, which we also have Landsat 8, is 30 meter resolution.

00:18:02.600 --> 00:18:06.300
So once you get down to like street level, you can't really see.

00:18:06.300 --> 00:18:07.620
Everything's blurry, right?

00:18:07.620 --> 00:18:07.960
Right.

00:18:07.960 --> 00:18:11.620
Each pixel represents 30 meters on the ground.

00:18:11.620 --> 00:18:12.320
Right, right.

00:18:12.320 --> 00:18:12.680
Okay.

00:18:12.680 --> 00:18:14.020
So Sentinel is 10 meter.

00:18:14.020 --> 00:18:15.740
You get a lot clearer picture.

00:18:15.740 --> 00:18:16.860
You can, you know, track.

00:18:16.860 --> 00:18:20.600
I mean, if you're doing, you know, sort of deforestation monitoring, for instance,

00:18:20.600 --> 00:18:24.940
like you can really track the edge of the deforestation a lot better with 10 meter imagery.

00:18:25.200 --> 00:18:25.440
Right.

00:18:25.440 --> 00:18:28.320
Or glaciers and you want to understand the boundary of it or something.

00:18:28.320 --> 00:18:28.980
Exactly.

00:18:28.980 --> 00:18:34.000
And, you know, it's still pretty low resolution compared to commercially available imagery.

00:18:34.000 --> 00:18:37.160
But as far as open data sets, it's high resolution.

00:18:37.160 --> 00:18:38.360
It's passively collected.

00:18:38.360 --> 00:18:42.620
I think the revisit rate is, I should have this offhand, I think it's eight days.

00:18:42.760 --> 00:18:45.660
So you can really do like monitoring use cases with that.

00:18:45.660 --> 00:18:48.940
It generates petabytes and petabytes of data.

00:18:48.940 --> 00:18:50.980
So it's a lot to sort of work over.

00:18:50.980 --> 00:18:55.220
I mean, generating the stack metadata for that, you know, it's like you got to fire up like

00:18:55.220 --> 00:18:58.740
10,000 cores to kind of run through that.

00:18:58.740 --> 00:19:04.060
And you end up actually reaching the limits of how fast you can read and write from different

00:19:04.060 --> 00:19:04.580
services.

00:19:04.580 --> 00:19:05.400
But my gosh.

00:19:05.620 --> 00:19:05.820
Yeah.

00:19:05.820 --> 00:19:08.520
But it's a really great, really great data set.

00:19:08.520 --> 00:19:11.560
A lot of work is being done against Sentinel too.

00:19:11.560 --> 00:19:17.780
So a lot of what I'm seeing and I'm reading through here is this annually or this from 2000

00:19:17.780 --> 00:19:22.680
to 2006 or like the one we were just speaking about is since, you know, from 2016.

00:19:22.680 --> 00:19:24.660
This data is getting refreshed.

00:19:24.660 --> 00:19:31.200
And can I ask questions like, how did this polygon of map look two years ago versus last

00:19:31.200 --> 00:19:31.960
year versus today?

00:19:31.960 --> 00:19:32.400
Totally.

00:19:32.400 --> 00:19:32.880
Yeah.

00:19:32.880 --> 00:19:36.940
And you can do that with the sort of API to say, okay, here's my polygon of interest.

00:19:36.940 --> 00:19:40.940
This is over my house or whatever, you know, fetch me all the images.

00:19:40.940 --> 00:19:44.940
But a lot of satellite imagery, I mean, most of it is clouds.

00:19:44.940 --> 00:19:46.880
It's just the earth is covered with clouds.

00:19:46.880 --> 00:19:47.820
You're going to get a lot of clouds.

00:19:47.820 --> 00:19:51.640
So there's also a metadata about the cloudiness.

00:19:51.640 --> 00:19:56.560
So you can say, okay, well, give me these images over time, but I want the scenes to

00:19:56.560 --> 00:19:58.060
be under 10% cloudy.

00:19:58.060 --> 00:19:58.440
Right.

00:19:58.440 --> 00:20:04.340
I'm willing for it to not be exactly 365 days apart, but maybe 350 because I get a clear

00:20:04.340 --> 00:20:05.220
view if I do that.

00:20:05.220 --> 00:20:05.840
Something like this.

00:20:05.840 --> 00:20:06.320
Exactly.

00:20:06.320 --> 00:20:11.000
And then you can make a little time lapse of how that area has changed over time.

00:20:11.000 --> 00:20:16.600
And in fact, I think there was somebody who actually demoed a time lapse, a similar type

00:20:16.600 --> 00:20:21.040
of time lapse, just grabbing the satellite imagery and turning it into a video over an area.

00:20:21.040 --> 00:20:21.400
I forget.

00:20:21.400 --> 00:20:21.960
Very neat.

00:20:21.960 --> 00:20:22.400
That was.

00:20:22.400 --> 00:20:22.740
Yeah.

00:20:22.740 --> 00:20:27.180
That one, the Sentinel, the large one, the revisit time is every five days.

00:20:27.180 --> 00:20:28.180
That's a lot of data.

00:20:28.180 --> 00:20:29.540
That was a, yep.

00:20:29.540 --> 00:20:30.220
Yeah.

00:20:30.220 --> 00:20:30.660
Yeah.

00:20:30.660 --> 00:20:31.100
Yeah.

00:20:31.100 --> 00:20:32.240
It ends up a lot of data.

00:20:32.240 --> 00:20:34.380
A lot of clouds in the cloud.

00:20:34.380 --> 00:20:35.060
Yeah.

00:20:35.060 --> 00:20:36.860
What about some of these other ones here?

00:20:36.860 --> 00:20:41.520
The DayMet, which is gridded estimates of weather parameters in North America.

00:20:41.520 --> 00:20:43.160
That's pretty interesting.

00:20:43.380 --> 00:20:43.540
Yeah.

00:20:43.540 --> 00:20:43.560
Yeah.

00:20:43.560 --> 00:20:49.580
So DayMet's actually an example of a lot of our data is geospatial, like satellite imagery

00:20:49.580 --> 00:20:54.660
or things that are derived from that, like elevation data sets where you're using the

00:20:54.660 --> 00:21:01.280
imagery to figure out how, what's the elevation of the land or things like land cover data sets.

00:21:01.420 --> 00:21:06.960
So if you scroll down just a tad, the land cover data set there, that's based off Sentinel, actually.

00:21:06.960 --> 00:21:11.740
And so there's a saying, you know, for every pixel in Sentinel, they took like a mosaic over

00:21:11.740 --> 00:21:12.180
a year.

00:21:12.180 --> 00:21:15.080
What is the, that pixel being used for?

00:21:15.160 --> 00:21:19.000
Is it water, trees, buildings, roads, things like that.

00:21:19.000 --> 00:21:24.240
So those are examples based off of, of satellite imagery or aerial photography.

00:21:24.240 --> 00:21:29.800
And then DayMet's an example of something that's like the output of a climate or a weather model.

00:21:29.800 --> 00:21:32.640
So these are typically higher dimensional.

00:21:32.640 --> 00:21:36.960
You're going to have things like, you know, temperature or maximum, minimum temperature,

00:21:36.960 --> 00:21:42.600
water, pressure, vapor, all sorts of things that are stored in this really, you know, big

00:21:42.600 --> 00:21:45.700
in dimensional cube at various coordinates.

00:21:45.700 --> 00:21:49.380
So latitude, longitude, time, maybe height above.

00:21:49.380 --> 00:21:55.720
So those are stored in typically in formats like Czar, which is this cloud native, very

00:21:55.720 --> 00:21:59.380
friendly to object storage way of storing chunked in dimensional arrays.

00:21:59.380 --> 00:22:01.140
Is it like, like streaming friendly?

00:22:01.140 --> 00:22:03.660
You can stream part of it and seek into it, that kind of thing.

00:22:03.660 --> 00:22:04.340
Exactly.

00:22:04.340 --> 00:22:05.880
And all the metadata is consolidated.

00:22:05.880 --> 00:22:10.680
So you can load in the whole data set and like, you know, less than a few hundred milliseconds,

00:22:11.220 --> 00:22:13.660
but then access a specific subset very efficiently.

00:22:13.660 --> 00:22:14.100
Sure.

00:22:14.100 --> 00:22:14.660
Yeah.

00:22:14.660 --> 00:22:14.980
Very neat.

00:22:14.980 --> 00:22:21.320
Another one that's not directly based off of satellites is the high resolution electricity

00:22:21.320 --> 00:22:22.360
access, I'm guessing.

00:22:22.360 --> 00:22:26.680
And I guess you could sort of approximate it from lights, but is it, do you think it's light?

00:22:26.680 --> 00:22:28.780
I think it is from Rod Dino.

00:22:28.780 --> 00:22:29.420
Yeah.

00:22:29.420 --> 00:22:30.460
So I think it's from.

00:22:30.460 --> 00:22:30.800
Yeah.

00:22:30.800 --> 00:22:32.340
It's V-I-I-R-S.

00:22:32.340 --> 00:22:32.720
Satellite.

00:22:32.720 --> 00:22:32.960
Okay.

00:22:33.100 --> 00:22:35.080
So it is off of basically just steady and light.

00:22:35.080 --> 00:22:35.520
Interesting.

00:22:35.520 --> 00:22:39.740
And we have a few more that are coming online shortly, which are kind of more tabular.

00:22:39.740 --> 00:22:41.460
So there's things like U.S.

00:22:41.460 --> 00:22:43.740
Census gives you like the polygon.

00:22:43.740 --> 00:22:50.320
So, you know, the state of Iowa has these counties or census blocks, which are this shape.

00:22:50.320 --> 00:22:53.900
So giving you all those shapes and it has this population, things like that.

00:22:53.900 --> 00:22:58.740
Things like GBIF has, which is, I think on there now, has occurrences of like, I think

00:22:58.740 --> 00:23:03.920
they're like observations of somebody spotted this animal or plant at this latitude, longitude

00:23:03.920 --> 00:23:05.680
at this time, things like that.

00:23:05.740 --> 00:23:07.680
So lots of different types of data.

00:23:07.680 --> 00:23:10.180
A mink was spotted running through the streets.

00:23:10.180 --> 00:23:10.440
Okay.

00:23:10.440 --> 00:23:10.780
Yeah.

00:23:10.780 --> 00:23:11.300
Yeah.

00:23:11.300 --> 00:23:12.640
Oh, you have one for agriculture.

00:23:12.640 --> 00:23:13.700
That's pretty interesting.

00:23:13.700 --> 00:23:18.320
If you're doing something with agriculture and farming and trying to do ML against that.

00:23:18.320 --> 00:23:22.180
That's interesting because that's actually run by the National Agriculture.

00:23:22.180 --> 00:23:28.580
That's actually aerial imagery, RGB, red, green, blue, and then also infrared aerial imagery

00:23:28.580 --> 00:23:30.920
that's collected every about every three years.

00:23:31.420 --> 00:23:36.480
So that's an example of high resolution imagery that's, you know, more than a, I think it's

00:23:36.480 --> 00:23:37.760
one meter resolution.

00:23:37.760 --> 00:23:38.160
Yeah.

00:23:38.160 --> 00:23:39.960
You can see the little trees and stuff.

00:23:39.960 --> 00:23:40.660
Exactly.

00:23:40.660 --> 00:23:42.300
It's very, very accurate.

00:23:42.300 --> 00:23:42.620
Yeah.

00:23:42.620 --> 00:23:44.840
Great data set specific to the US.

00:23:44.840 --> 00:23:50.680
So again, like Sentinel-2 is global in scope, but if you are doing things in the United States,

00:23:50.680 --> 00:23:52.520
NAEP is a great data set to use.

00:23:52.520 --> 00:23:52.800
Yeah.

00:23:52.800 --> 00:23:56.680
You've got the USGS 3D elevation for topology.

00:23:56.680 --> 00:23:57.400
That's cool.

00:23:57.400 --> 00:24:00.300
And then you have some additional data sets.

00:24:00.520 --> 00:24:03.160
What's the difference between the main ones and these additional ones?

00:24:03.160 --> 00:24:04.040
Why are they separated?

00:24:04.040 --> 00:24:11.060
We're catching up to where our stack API has all of the data sets we host, but the AI for

00:24:11.060 --> 00:24:15.260
Earth program, which hosts all these data sets has been going on since 2017.

00:24:15.260 --> 00:24:19.820
So there's plenty of data sets that they've been hosting that haven't yet made their way

00:24:19.820 --> 00:24:20.660
into the API.

00:24:20.660 --> 00:24:23.520
And that's, you know, just because we're getting there.

00:24:23.520 --> 00:24:24.760
It's a bunch of work.

00:24:24.760 --> 00:24:25.120
I see.

00:24:25.120 --> 00:24:28.780
So for these additional ones, maybe I could directly access them out of blob storage, but

00:24:28.780 --> 00:24:30.120
I can't ask API questions.

00:24:30.320 --> 00:24:30.660
Exactly.

00:24:30.660 --> 00:24:31.260
Okay.

00:24:31.260 --> 00:24:36.540
And then another point, which is kind of interesting talking back to the tabular data is that some

00:24:36.540 --> 00:24:41.560
of these data formats aren't quite, I mean, rasters and imagery is like fits really nicely

00:24:41.560 --> 00:24:42.180
in stack.

00:24:42.180 --> 00:24:44.860
And we know how to do spatial temporal queries over them.

00:24:44.960 --> 00:24:56.200
But some of these data formats, you know, they're not as mature as maybe the raster data format, or it's not as clear how to host them in a cloud optimized format and then host them in a spatial temporal API.

00:24:56.200 --> 00:24:59.820
So we're actually having to do work to say, okay, what are the standards?

00:24:59.820 --> 00:25:01.320
Is it like geo parquet?

00:25:01.320 --> 00:25:05.480
Or, you know, what are the formats that we're going to be using and hosting these data sets?

00:25:05.480 --> 00:25:08.440
And then how do we actually index the metadata through the API?

00:25:08.660 --> 00:25:15.500
So there's a lot of sort of data format and specification, metadata specification work before we can actually host all of these in the API.

00:25:15.500 --> 00:25:16.300
Yeah, really nice.

00:25:16.300 --> 00:25:18.740
A lot of good data here and quite large.

00:25:18.740 --> 00:25:23.940
Let's talk about the ETL for just a minute because you threw out some crazy numbers there.

00:25:23.940 --> 00:25:29.780
We're looking at the Sentinel-2 data and it gets refreshed every five days and it's the Earth.

00:25:29.780 --> 00:25:31.300
Talk us through what has to happen there.

00:25:31.300 --> 00:25:31.660
Yeah.

00:25:31.660 --> 00:25:34.740
So for the Sentinel, it's actually, you know, daily.

00:25:34.740 --> 00:25:37.480
So it's passive satellite collections.

00:25:37.480 --> 00:25:41.800
So the satellites are just always monitoring, always grabbing new imagery.

00:25:42.480 --> 00:25:46.020
And so that comes off to ground stations through the European Space Agency.

00:25:46.020 --> 00:26:02.140
And then we have some partners who are taking that, converting it to the Cloud Optimus GeoTiff format, putting it on blob storage, at which point we run our ingest pipelines, look for new imagery, extract the stack metadata, insert that into the database.

00:26:02.140 --> 00:26:11.060
And we just have that running in an Azure service called Azure Batch, which allows us to run parallel tasks on clusters that can auto scale.

00:26:11.660 --> 00:26:19.300
So if we're doing an ingest of a dataset for the first time, there's going to be a lot of files to process and we can scale that up.

00:26:19.300 --> 00:26:21.220
And it runs Docker containers.

00:26:21.220 --> 00:26:25.220
So we just have a project that, you know, defines the Docker commands that can run.

00:26:25.220 --> 00:26:30.160
And then we can submit tasks for chunks of the files that we are processing.

00:26:30.160 --> 00:26:32.120
That creates the stack items.

00:26:32.120 --> 00:26:37.820
And then another separate process actually takes the stack items and inserts it into the database.

00:26:37.820 --> 00:26:38.400
That's cool.

00:26:38.400 --> 00:26:45.300
So it's a little bit like data driven rather than a little bit like Azure Functions or AWS Lambda.

00:26:45.300 --> 00:26:50.640
But processing, we just got to get all this data and just work through it kind of at scale.

00:26:50.640 --> 00:26:51.380
Interesting.

00:26:51.380 --> 00:26:52.120
Yeah, for sure.

00:26:52.120 --> 00:26:53.400
Right now it's a little bit.

00:26:53.460 --> 00:26:56.660
We're still building the plane as we're flying it.

00:26:56.660 --> 00:27:13.420
But the next iteration is actually going to be a lot more reactive and based on another Azure service called Event Grid, where you can get notifications of new blobs going into storage and then put messages into queues that can then turn into these Azure Batch tasks that are running.

00:27:13.420 --> 00:27:13.600
Right.

00:27:13.940 --> 00:27:14.220
I see.

00:27:14.220 --> 00:27:19.340
So you just get something that drops it in the blob storage and it kicks off everything from there and you don't have to worry about it.

00:27:19.340 --> 00:27:19.600
Yep.

00:27:19.600 --> 00:27:24.900
And then we publish those to, you know, our users saying, hey, this is ready now.

00:27:24.900 --> 00:27:27.840
If they subscribe to that, that Event Grid topic.

00:27:27.840 --> 00:27:28.400
Oh, that's cool.

00:27:28.400 --> 00:27:32.260
There's a way to get notified of refreshes and things like that.

00:27:32.260 --> 00:27:32.980
Not yet.

00:27:32.980 --> 00:27:34.480
We're hoping to get that end of year.

00:27:34.480 --> 00:27:39.120
But yeah, the idea is that we would have basically a live feed of new imagery.

00:27:39.120 --> 00:27:43.920
I mean, what I would really like to see just for myself, my own interest is like,

00:27:43.920 --> 00:27:52.200
to be able to have my areas of interest and then just go to a page that shows like almost an Instagram feed of Sentinel images over that area.

00:27:52.200 --> 00:27:54.440
It's like, oh, this new one, it's not cloudy.

00:27:54.440 --> 00:27:55.060
Look at that.

00:27:55.060 --> 00:27:55.800
Look at that one.

00:27:55.800 --> 00:27:57.200
You know, it's something I'm monitoring.

00:27:57.200 --> 00:28:06.700
But yeah, generally we'll be publishing new stack items so that if you're running AI models off of the imagery as it comes in, you can do that processing based off of events.

00:28:06.700 --> 00:28:07.500
Yeah, that'd be cool.

00:28:07.500 --> 00:28:08.960
I'm only interested in Greenland.

00:28:08.960 --> 00:28:11.160
I don't care if you've updated Arizona or not.

00:28:11.160 --> 00:28:13.240
Just tell me if Greenland has changed.

00:28:13.320 --> 00:28:14.980
Then I'm going to rerun my model on it or something.

00:28:14.980 --> 00:28:15.240
Right.

00:28:15.240 --> 00:28:15.520
Cool.

00:28:15.520 --> 00:28:16.160
All right.

00:28:16.160 --> 00:28:16.580
Let's see.

00:28:16.580 --> 00:28:19.860
So that's the data part, data catalog.

00:28:19.860 --> 00:28:22.440
And then we have the API and the hub, which I want to get to.

00:28:22.440 --> 00:28:30.000
But I kind of want to just sort of put some perspective on what people have been doing with this, some of your partner stuff under the applications thing.

00:28:30.000 --> 00:28:34.700
So, Tom, which ones do you think we should highlight from those that are interesting?

00:28:34.700 --> 00:28:37.460
We kind of talked about the land cover data set.

00:28:37.460 --> 00:28:40.760
So we worked with Impact Observatory to do that.

00:28:40.760 --> 00:28:48.440
And so we had some tips about how to use Azure Batch because that's a very big Azure Batch job to generate that land cover map.

00:28:48.680 --> 00:28:52.980
So pulling down the Sentinel data that we're hosting and then running their model over it.

00:28:53.140 --> 00:28:58.480
So that was a fun data set to see come together and then use now.

00:28:58.480 --> 00:29:03.100
The Carbon Plan, Carbon Monitoring Risk Assessment application.

00:29:03.100 --> 00:29:04.720
That's like a really cool.

00:29:04.720 --> 00:29:09.540
It's a cool like JavaScript application that you can view risks on.

00:29:09.540 --> 00:29:17.100
So these companies are buying like carbon offsets that are forest trees that are planted to offset carbon.

00:29:17.100 --> 00:29:25.200
But there's a problem that, you know, that we know about now is like the wildfires are burning down those some of those forests.

00:29:25.200 --> 00:29:25.600
And so.

00:29:25.600 --> 00:29:25.960
Right.

00:29:25.960 --> 00:29:29.920
It doesn't help if you planted a bunch of trees to offset your carbon if they go up and smoke, right?

00:29:29.920 --> 00:29:30.380
Right.

00:29:30.380 --> 00:29:30.840
Yeah.

00:29:30.840 --> 00:29:39.400
So Carbon Plan did a bunch of research, first of all, on essentially they did the research before that before our hub existed.

00:29:39.400 --> 00:29:49.420
But, you know, we were working with these community members to and they have a very similar setup to what we have now to do the research, to, you know, train the models and all of that.

00:29:49.420 --> 00:29:57.740
That goes into this visualization here of how likely, you know, what are the different risks for each plot of land in the U.S.?

00:29:57.740 --> 00:29:57.900
Yeah.

00:29:57.900 --> 00:29:59.780
So that was a great collaboration there.

00:29:59.780 --> 00:30:07.220
One of the things I was wondering when I was looking at these is you all are hosting this large amounts of data and you're offering compute to study them.

00:30:07.600 --> 00:30:13.040
How does something like Carbon Plan take that data and build this seemingly independent website?

00:30:13.040 --> 00:30:13.580
Yeah.

00:30:13.580 --> 00:30:19.820
Does that run directly on that data or do they like export some stuff and then run it on their side or what's the story?

00:30:19.820 --> 00:30:29.480
They would have been doing all like the heavy duty compute ahead of time to train the models and everything to gather the statistics necessary to power this.

00:30:29.480 --> 00:30:35.040
So then at that point, it's just a static JavaScript application just running in your browser now.

00:30:35.040 --> 00:30:35.640
Oh, interesting.

00:30:35.880 --> 00:30:40.980
And I think that's a good point because it's running against our data, but it's running in their own infrastructure.

00:30:40.980 --> 00:30:41.780
Right.

00:30:41.860 --> 00:30:54.540
So it's sort of on the planetary computer, but it's like really in this case, like using the planetary computer data sets in sort of a production setting that an infrastructure that they own, which is a use case we really want to support.

00:30:54.640 --> 00:31:01.220
If, you know, if they need to use search in order to find the images that they need, they can use our stack APIs.

00:31:01.220 --> 00:31:13.220
But really, it's like just an application running in Azure that, you know, in certain cases with our grants program, we'll end up supporting and sponsoring Azure subscriptions to run this type of infrastructure.

00:31:13.420 --> 00:31:16.840
But at the end of the day, it's really just applications running in the cloud.

00:31:16.840 --> 00:31:17.200
Right.

00:31:17.200 --> 00:31:21.040
It's just better if that it's in Azure that it's nearby, but they could run it anywhere technically.

00:31:21.040 --> 00:31:21.300
Right.

00:31:21.300 --> 00:31:24.140
And just get signed blob storage access or whatever.

00:31:24.140 --> 00:31:24.740
Yeah.

00:31:24.740 --> 00:31:28.280
We'll throttle access at a certain point if you're trying to egress too much.

00:31:28.280 --> 00:31:29.700
But yeah, I can imagine.

00:31:29.700 --> 00:31:29.960
Yeah.

00:31:29.960 --> 00:31:30.520
Yeah.

00:31:30.600 --> 00:31:30.840
Yeah.

00:31:30.840 --> 00:31:31.360
Yeah.

00:31:31.360 --> 00:31:31.760
Very cool.

00:31:31.760 --> 00:31:37.200
I can come over here and zoom in on Portland and it looks like we're in a decent bit of greenness still.

00:31:37.200 --> 00:31:38.320
It does rain up here a lot.

00:31:38.320 --> 00:31:39.140
Same for Seattle.

00:31:39.140 --> 00:31:39.480
Yeah.

00:31:39.480 --> 00:31:40.060
Yeah.

00:31:40.060 --> 00:31:40.380
Yeah.

00:31:40.380 --> 00:31:41.260
Quite cool.

00:31:41.260 --> 00:31:42.960
You talked about this grant program.

00:31:42.960 --> 00:31:47.800
What's the story that people out there listening, they're like, I want to get into working with this data and building things.

00:31:47.800 --> 00:31:49.360
Grant might sound good to them.

00:31:49.360 --> 00:31:49.920
What is that?

00:31:49.920 --> 00:31:50.260
Awesome.

00:31:50.260 --> 00:31:50.540
Yeah.

00:31:50.540 --> 00:31:52.660
Look up AI for Earth grants.

00:31:52.660 --> 00:31:58.260
We have rounds of supporting folks that are doing environmental sustainability work.

00:31:58.440 --> 00:32:01.620
And there's a sort of a range of grant rewards.

00:32:01.620 --> 00:32:04.780
The lowest level is like giving Azure credits.

00:32:04.780 --> 00:32:14.740
You know, being able to sponsor an account or sponsor resources for applications that are being developed or research that's being done for environmental sustainability.

00:32:14.740 --> 00:32:18.380
And we have folks running the grants program and go take the applications.

00:32:18.380 --> 00:32:23.020
And there's different classes that we have and summits for each of the classes.

00:32:23.740 --> 00:32:36.380
And then there's more involved grants and larger grants as usually as people sort of show progress, we can we actually can end up bringing additional resources or, you know, paid projects to accomplish specific goals.

00:32:36.380 --> 00:32:44.420
But yeah, if anybody's out there and they're doing work in environmental sustainability that could benefit from the cloud, we'd love to work with you.

00:32:44.420 --> 00:32:45.320
Well, just to clarify.

00:32:45.520 --> 00:32:59.180
So there's, you know, the grants are great for like if you have like a complex deployment that's using a ton of Azure services and you want to like integrate this all together and use the planetary computer data, then the grants are a great approach.

00:32:59.180 --> 00:33:07.340
If you're just like an individual researcher, a team of researchers or whoever who wants to use this data, the data is there.

00:33:07.460 --> 00:33:08.460
It's publicly accessible.

00:33:08.460 --> 00:33:18.300
And if you need a place to compute from that's in Azure, so close to the data and you don't already have an Azure subscription, then you can sign up for a planetary computer account.

00:33:18.300 --> 00:33:21.860
And so like that's a way lower bar of barrier to entry.

00:33:21.860 --> 00:33:26.180
There is you just sign up for an account, you get approved by us and then you're off to the races.

00:33:26.180 --> 00:33:27.040
That's a great point.

00:33:27.040 --> 00:33:33.360
If you think you need a grant to if you think you need a grant to use the cloud, try using the planetary computer first because you might not.

00:33:33.460 --> 00:33:33.940
Yeah, very good.

00:33:33.940 --> 00:33:38.500
Talk Python to me is partially supported by our training courses.

00:33:38.500 --> 00:33:50.140
When you need to learn something new, whether it's foundational Python, advanced topics like async or web apps and web APIs, be sure to check out our over 200 hours of courses at Talk Python.

00:33:50.140 --> 00:33:56.240
And if your company is considering how they'll get up to speed on Python, please recommend they give our content a look.

00:33:56.240 --> 00:33:56.920
Thanks.

00:33:56.920 --> 00:34:02.060
So what's the business model around this?

00:34:02.300 --> 00:34:04.380
Is there going to be a fee for it?

00:34:04.380 --> 00:34:05.640
Is there some free level?

00:34:05.640 --> 00:34:08.620
Is it always free, but restricted how you can use it?

00:34:08.620 --> 00:34:11.080
Because right now it's in like a private beta, right?

00:34:11.080 --> 00:34:12.940
I can come down and request access to it.

00:34:12.940 --> 00:34:13.840
Yeah, it's a preview.

00:34:13.840 --> 00:34:17.440
We're still like getting access by requiring requests.

00:34:17.440 --> 00:34:21.720
And, you know, there's a larger number of requests we're approving over time.

00:34:21.720 --> 00:34:25.580
We're still coming up with the eventual final sort of target.

00:34:25.580 --> 00:34:35.180
Most likely it will be some sort of limits around what you can do as far as compute, as far as data storage, once we have features around that.

00:34:35.180 --> 00:34:48.260
And with clear offboarding of like if you're an enterprise organization that wants to utilize this technology, there should be paid services that allow you to just as easily do it as you're doing on the planetary computer.

00:34:48.260 --> 00:35:03.620
But if you're doing low usage use cases or if your use cases is super environmental sustainability focused and you apply for a grant, we could end up, you're still using a paid service, but we're covering those costs through our grants program.

00:35:04.180 --> 00:35:12.660
So we're still figuring that out as far as, you know, we're not, I don't see this as something that we're, you know, trying to turn into a paid service necessarily.

00:35:12.660 --> 00:35:19.860
I think that there's a number of, you know, enterprise level services that could end up, you know, looking a lot like the planetary computer.

00:35:19.860 --> 00:35:28.220
But really, we want to continue to support uses, particularly for environmental sustainability use cases through this avenue.

00:35:28.220 --> 00:35:29.220
Yeah.

00:35:29.220 --> 00:35:42.700
One of the nice things about our overall approach is since we're so invested in the open source side of things is if we're, you know, you might have requested an account a while ago and we're like very slowly going through them because there's just like so much to do.

00:35:42.700 --> 00:35:48.380
But if we're too slow approving your account, then you can replicate the hub in your own Azure subscription.

00:35:48.380 --> 00:36:02.500
If we're blocking you or if your needs are just like so vastly beyond what we can provide within this one subscription, then you can go ahead and do your own setup on Azure and get access to our data from your own subscription.

00:36:02.500 --> 00:36:02.860
Right.

00:36:02.860 --> 00:36:05.920
Because the blob storage is public, right?

00:36:05.920 --> 00:36:06.500
Exactly.

00:36:06.500 --> 00:36:06.960
Yep.

00:36:06.960 --> 00:36:07.400
Okay.

00:36:07.400 --> 00:36:07.920
Yeah.

00:36:07.920 --> 00:36:08.380
Very nice.

00:36:08.380 --> 00:36:16.180
Maybe the next two things to talk about are the API and the hub, but I think maybe those would be good to see together.

00:36:16.180 --> 00:36:17.040
What do you think?

00:36:17.040 --> 00:36:17.580
Yeah.

00:36:17.580 --> 00:36:18.140
Yeah, definitely.

00:36:18.140 --> 00:36:21.840
I think I'll let you talk us through some scenarios here, Tom.

00:36:21.840 --> 00:36:22.140
Cool.

00:36:22.140 --> 00:36:22.520
Yeah.

00:36:22.520 --> 00:36:25.740
So I'm, in this case, I've, you know, logged into the hub here.

00:36:25.740 --> 00:36:28.320
So I've, this is a, yeah.

00:36:28.320 --> 00:36:32.220
Before you go further, there is a choice you get when you go there.

00:36:32.220 --> 00:36:35.440
You've got an account and you click start my notebook up.

00:36:35.440 --> 00:36:35.760
Yeah.

00:36:35.760 --> 00:36:38.860
It's actually going to fire up a machine and it gives you four choices, right?

00:36:38.860 --> 00:36:43.440
Python with four cores and 32 gigs of memory and a Pangeo notebook.

00:36:43.440 --> 00:36:51.860
It gives you R with eight cores and R geospatial and GPU PyTorch, as well as UGIS, which I don't really know what that is.

00:36:51.860 --> 00:36:53.620
Maybe tell us about getting started.

00:36:53.620 --> 00:36:54.300
Yeah.

00:36:54.300 --> 00:36:54.880
Got it.

00:36:54.880 --> 00:36:57.360
So this is a Jupyter Hub deployment.

00:36:57.360 --> 00:36:59.800
So Jupyter Hub's this really nice project.

00:36:59.960 --> 00:37:07.220
I think it came out of UC Berkeley when they were kind of teaching classes, data science courses to like thousands of students at once.

00:37:07.640 --> 00:37:16.420
And, you know, even with like Condor or whatever, you don't want to be trying to manage a thousand students, Condor installations or whatever.

00:37:16.420 --> 00:37:18.300
So that's just a nightmare.

00:37:18.300 --> 00:37:31.180
So they had this kind of cloud-based setup where you just log in with your credentials or whatever, and you get access to a compute environment to do your homework in that case or do your geospatial data analysis in this case.

00:37:31.180 --> 00:37:43.420
And so this kind of, you mentioned Pangeo, this is this ecosystem of geobusinesses, geoscientists who are trying to do scalable geoscience on the cloud that Anaconda was involved with.

00:37:43.420 --> 00:37:51.440
And so they kind of pioneered this concept of a Jupyter Hub deployment on Kubernetes that's tied to Dask.

00:37:51.660 --> 00:38:03.380
So you can create, easily get a single node compute environment here, in this case, he's in the Python environment, or multiple nodes, a cluster of machines to do your analysis using Dask and Dask gateway.

00:38:03.380 --> 00:38:07.460
Yeah, it's just a Kubernetes-based computing environment.

00:38:07.460 --> 00:38:08.020
That's cool.

00:38:08.020 --> 00:38:13.020
And I noticed right away the Dask integration, which is good for like this massive amount of data, right?

00:38:13.020 --> 00:38:19.280
Because it allows you to scale across machines or, you know, more stream data where you don't have enough to store it, memory and things like that.

00:38:19.400 --> 00:38:20.020
Yeah, exactly.

00:38:20.020 --> 00:38:23.140
So this is a great thing that we get for Python.

00:38:23.140 --> 00:38:24.900
So Dask is Python-specific.

00:38:24.900 --> 00:38:33.960
We do have the other environments like R for, if you're doing geospatial in R, which there's a lot of really great libraries there, that's an option.

00:38:33.960 --> 00:38:36.180
That is, unfortunately, single node.

00:38:36.180 --> 00:38:38.900
There's not really a Dask equivalent there.

00:38:38.900 --> 00:38:43.940
But there's some cool stuff that's being worked on, like multi-D plier and things like that.

00:38:43.940 --> 00:38:44.200
Cool.

00:38:44.200 --> 00:38:49.260
And if people haven't seen Dask running in Jupyter Notebook, there's the whole cluster visualization.

00:38:49.260 --> 00:38:53.440
And the sort of progress computation stuff is super neat to see it go.

00:38:53.440 --> 00:38:53.840
Yeah.

00:38:53.840 --> 00:38:54.240
Yeah.

00:38:54.240 --> 00:39:01.700
So it's when you're doing these distributed computations, it's really key to have an understanding of what your cluster's up to.

00:39:01.700 --> 00:39:05.440
It's just crucial to be able to have that information there.

00:39:05.440 --> 00:39:22.960
And then the example code that you've got there, the cloudless Mosaic Sentinel-2 notebook, it just has basic create me a cluster in Dask, get the client, create four to 24 workers, and then off it goes, right?

00:39:22.960 --> 00:39:23.700
Yeah, exactly.

00:39:24.020 --> 00:39:26.540
What is the limits and how does that work, right?

00:39:26.540 --> 00:39:30.100
As part of getting an account on there, you get access to this cluster?

00:39:30.100 --> 00:39:30.460
Yep.

00:39:30.460 --> 00:39:33.520
So this is the first thing that we've talked about today that does require an account.

00:39:33.520 --> 00:39:41.300
So the hub requires an account, but accessing the Stack API, which we'll see in a second, and even downloading the data does not require an account.

00:39:41.300 --> 00:39:42.620
You can just do that anonymously.

00:39:42.780 --> 00:39:50.920
Yeah, and in this case, I think the limit's like a thousand cores, something like that, some memory limit as well.

00:39:50.920 --> 00:39:53.560
So that's the limit that you're into there.

00:39:53.560 --> 00:39:55.880
So you can get quite a bit out of this.

00:39:55.880 --> 00:39:57.220
That's real computing right there.

00:39:57.220 --> 00:39:58.880
Yeah, definitely.

00:39:58.880 --> 00:40:02.360
And in this case, we're using Dask's adaptive mode.

00:40:02.360 --> 00:40:05.280
So we're saying, right now there's nothing to do.

00:40:05.280 --> 00:40:06.320
It's just sitting around idly.

00:40:06.320 --> 00:40:07.880
So I have three or four workers.

00:40:08.720 --> 00:40:16.220
But once I start to actually do a computation that's using Dask, it'll automatically scale up in the background, which is a neat feature of Dask.

00:40:16.220 --> 00:40:26.500
Yeah, and so the basic computation, the problem that we're trying to do here is we have some area of interest, which I think is over Redmond, Washington, Microsoft headquarters, which we're defining as this.

00:40:26.500 --> 00:40:28.020
It's an exact square, yeah.

00:40:28.020 --> 00:40:28.820
Yeah, maybe.

00:40:28.820 --> 00:40:29.500
Heck.

00:40:29.500 --> 00:40:32.980
Yeah, some sort of, I think it's a square polygon.

00:40:32.980 --> 00:40:41.320
Anyway, we draw that out and then we say, okay, give me all of the Sentinel-2 items that cover that area.

00:40:41.320 --> 00:40:48.640
So again, back to what we were talking about at the start is like, if you just had files in blob storage, that'd be extremely difficult to do.

00:40:48.640 --> 00:41:04.800
But thanks to this nice stack API, which we can connect to here at planetarycomputer.micsop.com, we're able to quickly say, hey, give me all the images from 2016 to 2020 from Sentinel that cover, that intersect with our area of interest here.

00:41:04.800 --> 00:41:13.420
And we're even throwing in a query here saying, hey, I only want scenes where the cloud cover is less than 25%, according to the metadata.

00:41:13.760 --> 00:41:17.740
Very likely summer in Seattle because the winter, not so much.

00:41:17.740 --> 00:41:18.040
Much fewer.

00:41:18.040 --> 00:41:18.920
Yeah, much fewer.

00:41:18.920 --> 00:41:33.260
So, you know, quickly within a second or two, we get back the 138 scenes items out of the, I don't know how many there are in total, but like hundreds of thousands, millions of individual stack items that comprise.

00:41:33.260 --> 00:41:34.000
20 million?

00:41:34.000 --> 00:41:34.960
20 million.

00:41:34.960 --> 00:41:35.240
Okay.

00:41:35.240 --> 00:41:36.720
That comprise Sentinel-2.

00:41:36.720 --> 00:41:38.720
So we're quickly able to filter that down.

00:41:39.200 --> 00:41:40.840
Next up, we have a bit of signing.

00:41:40.840 --> 00:41:51.720
So this is that bit that we talked about where you can do all this anonymous, but in order to actually access the data, we have you sign the items, which basically appends this little token to the URLs.

00:41:51.720 --> 00:41:58.200
And then at that point, they can be opened up by any geospatial program like QGIS or...

00:41:58.200 --> 00:41:58.400
Right.

00:41:58.400 --> 00:42:02.980
It converts a private block storage URL to a temporary public one.

00:42:02.980 --> 00:42:03.380
Yep.

00:42:03.380 --> 00:42:03.920
Exactly.

00:42:03.920 --> 00:42:04.820
Exactly.

00:42:04.820 --> 00:42:05.760
So you do that.

00:42:05.760 --> 00:42:12.480
It's just like this kind of incidental happenstance that stack and dask actually pair extremely nicely.

00:42:12.480 --> 00:42:20.760
If you think about dask, the way it operates is it's all about lazily operating, lazily constructing a task graph of computations.

00:42:20.760 --> 00:42:26.420
And then at the end of your whatever you're doing, computing that all at once.

00:42:26.420 --> 00:42:31.760
That just gives really nice rooms for optimizations and maximizing parallelization wherever possible.

00:42:31.880 --> 00:42:40.700
The thing about geospatial is, again, if you didn't have stack, you'd have to open up these files to understand where on earth is it?

00:42:40.700 --> 00:42:43.320
What latitude, longitude does it cover?

00:42:43.320 --> 00:42:43.680
Right.

00:42:43.680 --> 00:42:47.800
You have to open up all 20 million files and then look and see what its metadata says in it, right?

00:42:47.800 --> 00:42:48.040
Yeah.

00:42:48.040 --> 00:42:48.360
Okay.

00:42:48.360 --> 00:42:52.000
And in this case, we have like 138 times three files.

00:42:52.240 --> 00:42:56.380
Those 600, you know, whatever, 450, 600 items files here.

00:42:56.380 --> 00:43:01.740
You know, each opening, each one of those takes a few, maybe 200, 400, 500 milliseconds.

00:43:01.740 --> 00:43:11.640
So it's not awful, but it's like too slow to really do interactively on any scale of any large number of stack items.

00:43:11.840 --> 00:43:13.460
So that's where it stacks great.

00:43:13.460 --> 00:43:14.380
It has all the metadata.

00:43:14.380 --> 00:43:22.300
So we know that this TIFF file, this cloud-optimized geotiff file that contains the actual data, we know exactly where it is on earth.

00:43:22.300 --> 00:43:27.800
What latitude, longitude it covers, what time period it covers, what asset it actually represents wavelength.

00:43:27.800 --> 00:43:32.840
So we're able to very quickly stack these together into this X-ray data array.

00:43:32.840 --> 00:43:36.560
That's in this case, it's fairly small since we've chopped it down.

00:43:36.560 --> 00:43:41.660
If we leave out the filtering, it'd be much, much larger because these are really large scenes.

00:43:41.660 --> 00:43:45.720
But anyway, we're able to really quickly generate these data arrays.

00:43:45.720 --> 00:43:53.880
And then using Dask, using our Dask cluster, we can actually load those, persist those in distributed memory on all the workers on our cluster.

00:43:53.880 --> 00:43:56.320
So that's like very easy.

00:43:56.320 --> 00:44:07.920
It's like a few lines of code, a single function call, but it represents years of effort to build up these stack specification and all the metadata and then the integration into Dask.

00:44:07.920 --> 00:44:10.800
So it's just a fantastic, fantastic result that we have.

00:44:10.800 --> 00:44:11.940
Yeah, and it's super cool.

00:44:11.940 --> 00:44:24.320
Once you just call data.persist on the Dask array, you can just see in the dashboard of Dask, like all these clusters firing up and all this data getting processed.

00:44:24.320 --> 00:44:25.680
Yeah, very neat.

00:44:25.680 --> 00:44:26.580
Yeah, exactly.

00:44:26.580 --> 00:44:31.920
So in this case, since we have that adaptive mode, we'll see additional workers come online here.

00:44:31.920 --> 00:44:35.700
As we start to stress the cluster, it's saying, oh, I've got a bunch of unfinished tasks.

00:44:35.700 --> 00:44:38.240
I should bring online some more workers.

00:44:38.240 --> 00:44:43.660
And that'll take either a few seconds if there's empty space on our cluster or a bit longer.

00:44:43.920 --> 00:44:51.320
Yeah, I feel like with this, if it just sat there and said, it's going to take two minutes and just spun with a little star, the Jupyter star, that would be boring.

00:44:51.320 --> 00:44:54.740
It has this cool animated little dashboard like, oh, I'm going to just watch it go.

00:44:54.740 --> 00:44:55.320
Look at it go.

00:44:55.320 --> 00:44:55.960
No.

00:44:56.240 --> 00:44:58.480
It's kind of like defragging your hard dive with the old days.

00:44:58.480 --> 00:45:01.400
It's just you watch these little bars go across.

00:45:01.400 --> 00:45:01.920
It's very bizarre.

00:45:01.920 --> 00:45:03.180
Bizarrely satisfying.

00:45:03.180 --> 00:45:06.700
Yeah, I will definitely just spend some time sitting here watching it.

00:45:06.700 --> 00:45:08.620
Essentially, like monitoring.

00:45:08.620 --> 00:45:10.740
There's like a lot of communication here.

00:45:10.740 --> 00:45:13.520
There shouldn't be, but really, I'm just watching the lines move.

00:45:14.060 --> 00:45:17.720
While the thing is working, let me take a question from the live stream.

00:45:17.720 --> 00:45:25.780
Sam Paria asks, can users bring their own data to this sort of processing or, you know, because you've got the data sets that you have.

00:45:25.780 --> 00:45:28.400
Is there a way to bring other research data over?

00:45:28.400 --> 00:45:28.700
Yeah.

00:45:28.700 --> 00:45:35.260
So the answer now is like, yes, but you kind of have to do a lot of effort to get it there.

00:45:35.260 --> 00:45:42.700
Like, so your own data, you probably, maybe you do have like your own stack API and database setup and all of that.

00:45:43.020 --> 00:45:46.120
But that's publicly accessible or you have a token for.

00:45:46.120 --> 00:45:48.700
So most users don't already have that.

00:45:48.700 --> 00:45:56.040
So you can't, this real divide between the data sets that we provide with our nice stack API and like your own custom data set.

00:45:56.040 --> 00:45:58.200
That might be a pile of files and blob storage.

00:45:58.200 --> 00:46:00.200
And you could access it that way, certainly.

00:46:00.200 --> 00:46:02.000
But there's kind of a divide there.

00:46:02.000 --> 00:46:12.260
So that is definitely something that we're interested in improving is making user data sets like that are private to you feel as nice to work with as our own public data sets.

00:46:12.440 --> 00:46:12.560
Yeah.

00:46:12.560 --> 00:46:20.440
Another thing that I saw when I was looking through, it said, under the data sets available, it says, or if you have your own data and you'd like to contribute, contact us.

00:46:20.440 --> 00:46:23.700
And that's a slightly different question than they were just asking.

00:46:23.700 --> 00:46:25.940
That's the one question was, well, I have my own data.

00:46:25.940 --> 00:46:26.540
I want to use it.

00:46:26.540 --> 00:46:29.620
This is like how I've, I work at a university or something.

00:46:29.620 --> 00:46:30.340
I've got all this data.

00:46:30.340 --> 00:46:31.780
I want to make it available to the world.

00:46:31.880 --> 00:46:32.760
What's the story of that?

00:46:32.760 --> 00:46:39.360
We have a backlog of data sets that we're onboarding onto Azure Blob Storage and then importing into the API.

00:46:39.360 --> 00:46:46.380
We're still working through that backlog, but always on the lookout for good data sets that have real use cases in environmental sustainability.

00:46:46.880 --> 00:46:56.120
If there's, you know, a group that's doing some research or doing building applications that have environmental sustainability impact and they need a data set, that certainly bumps it up on our list.

00:46:56.120 --> 00:47:05.320
So, yeah, I would love to hear from anybody that has data sets that you're looking to expose publicly, hosts on the Planetary computer for anybody to use and need a place to host it.

00:47:05.400 --> 00:47:06.240
Yeah, very cool.

00:47:06.240 --> 00:47:08.760
All right, Tom, your graph stopped moving around.

00:47:08.760 --> 00:47:09.800
It might be done.

00:47:09.800 --> 00:47:15.080
Yeah, so we spent quite a while loading up the data and then that's like, yeah, just how it goes.

00:47:15.080 --> 00:47:20.200
You spend a bunch of time loading up data and then once it's in memory, computations tend to be pretty quick.

00:47:20.200 --> 00:47:23.660
So in this case, we're taking a median over time.

00:47:23.660 --> 00:47:25.480
Is this the median of the image?

00:47:25.480 --> 00:47:27.380
What is that a median of?

00:47:27.380 --> 00:47:29.680
Yeah, so right now we have for like a list of numbers.

00:47:29.680 --> 00:47:31.040
I'm not sure what it means for an image.

00:47:31.040 --> 00:47:32.760
Yeah, so this is a median over time.

00:47:32.760 --> 00:47:39.720
So our stack here, our data arrays, a four-dimensional array and the dimensions are time, first of all.

00:47:39.720 --> 00:47:42.940
So we had like 138 time slices, wavelength.

00:47:42.940 --> 00:47:51.240
So these, you know, red, green, blue, near-infrared, Sentinel captures like 10 or 12 wavelengths and then latitude and longitude.

00:47:51.240 --> 00:47:54.480
So we took the median over time.

00:47:55.020 --> 00:48:03.080
And the idea here is that like stuff like roads and mountains and forests tend not to move over time.

00:48:03.080 --> 00:48:06.620
They're static relatively compared to something like clouds.

00:48:06.620 --> 00:48:08.620
So again, clouds are always a problem.

00:48:08.620 --> 00:48:19.460
And once you take the median over time, you kind of get like the average image over this entire time period, which turns out to be an image that doesn't have too many clouds in it.

00:48:19.600 --> 00:48:24.920
Yeah, it might have no clouds because if you kind of averaged them out across all of them because you already filtered it down pretty low.

00:48:24.920 --> 00:48:25.260
Yeah.

00:48:25.260 --> 00:48:25.580
Yeah.

00:48:25.580 --> 00:48:31.680
So now we can see a picture of the Seattle area where it's a cloud-free composite or a cloudless mosaic.

00:48:31.680 --> 00:48:31.960
Yeah, beautiful.

00:48:31.960 --> 00:48:38.680
Looks like you got maybe that's, what is that, Lake Washington and you got Rainier there and all sorts of good stuff.

00:48:38.780 --> 00:48:38.920
Yeah.

00:48:38.920 --> 00:48:39.260
Yeah.

00:48:39.260 --> 00:48:44.020
I'm sure I actually do not know the geography of that well, but I have been looking at lots of pictures.

00:48:44.020 --> 00:48:47.660
We tend to use this as our example area a lot.

00:48:47.660 --> 00:48:48.120
Yeah.

00:48:48.120 --> 00:48:48.860
Super cool.

00:48:48.860 --> 00:48:57.560
Anyway, and one nice thing here is like, so we're, again, investing heavily in open source, investing in building off of open source.

00:48:57.560 --> 00:49:00.920
So we have like all the power of Xarray to use.

00:49:00.920 --> 00:49:05.120
Xarray is this like very general purpose, in-dimensional array computing library.

00:49:05.120 --> 00:49:07.740
It kind of combines the best of NumPy and pandas.

00:49:07.740 --> 00:49:10.700
In this case, you know, we can do something like group by.

00:49:10.700 --> 00:49:14.320
So if you're familiar with pandas, you're familiar with group bys, we can group by a time dot month.

00:49:14.320 --> 00:49:16.500
So I want to do like a monthly mosaic.

00:49:16.500 --> 00:49:23.520
Maybe I don't want to combine images from January, which might have snow in them with images from July, which wouldn't have as much.

00:49:23.520 --> 00:49:24.680
So I can do a.

00:49:24.680 --> 00:49:27.520
So you'll get like 12 different images or something like that.

00:49:27.520 --> 00:49:30.500
Here's what it kind of averaged out to be in February.

00:49:30.500 --> 00:49:31.040
Exactly.

00:49:31.040 --> 00:49:37.040
And so now we have a stack of images, 12 of them, and we can go ahead and representing a median.

00:49:37.040 --> 00:49:43.320
So we have multiple years and we group all of the ones from January together and take the median of those.

00:49:43.320 --> 00:49:48.440
And then we get a nice little group of cloud free mosaics here, one for each month.

00:49:48.440 --> 00:49:48.720
Yeah.

00:49:48.720 --> 00:49:53.880
And sure enough, there is a little less snow around Rainier in the summer than in the winter as you would, you know, the cascades.

00:49:53.880 --> 00:49:54.400
Yep, definitely.

00:49:54.740 --> 00:49:59.140
So that's like a fun little introductory example to what the hub gives you.

00:49:59.140 --> 00:50:03.400
It gets you the single node environment, which that alone is quite a bit.

00:50:03.400 --> 00:50:13.300
You know, you don't have to mess with, you know, fighting to get like the right set of libraries installed, which can be especially challenging when you're interfacing with like the C and C++ libraries like GDAL.

00:50:13.540 --> 00:50:18.480
So that environment is all set up, mostly compatible, should all work for you on a single node.

00:50:18.480 --> 00:50:29.360
And then if you do have these larger computations, you know, we saw it took a decent while to load the data, even with these fast interop between the storage machines and the compute machines in the same Azure region.

00:50:29.540 --> 00:50:34.400
But you can scale that out on enough machines that your computations complete in a reasonable amount of time.

00:50:34.400 --> 00:50:34.640
Yeah.

00:50:34.640 --> 00:50:37.120
And because the animations, I don't need, you don't even mind.

00:50:37.120 --> 00:50:38.680
Now it's, it's super cool.

00:50:38.680 --> 00:50:46.220
So you use the API to really narrow it down from 20 million to like 150 or 138 images and then keep, keep running on.

00:50:46.620 --> 00:51:00.060
So one thing that I was wondering when I was looking at this is, you know, what libraries come included that I can import and which ones, you know, if there's something that's not there, maybe I really want to use HTTPX and you only have requests or whatever.

00:51:00.060 --> 00:51:03.980
Like, is there a way to get additional libraries and packages and stuff in there?

00:51:03.980 --> 00:51:06.180
We do have a focus on geospatial.

00:51:06.180 --> 00:51:09.120
So that's like, we'll have most of that there already.

00:51:09.120 --> 00:51:13.460
So, you know, XRA, Dask, Rasterio and all those things.

00:51:13.460 --> 00:51:16.480
But if there is something there, our container.

00:51:16.480 --> 00:51:19.940
So these are all Docker images built from Conda environments.

00:51:19.940 --> 00:51:24.920
That's all comes from this repository, Microsoft slash planetary computer containers.

00:51:24.920 --> 00:51:33.740
So if you just, you know, you want HTTPX, you add it to the environment.yaml and we'll get a new image built and then available from the planetary computer.

00:51:33.740 --> 00:51:35.920
And so these are public images.

00:51:35.920 --> 00:51:38.180
They're just on the Microsoft container registry.

00:51:38.180 --> 00:51:47.600
So if you want to, you know, we use our image, like you don't want to fight with getting a compatible version of, say, PyTorch and lib JPEG.

00:51:47.600 --> 00:51:55.300
Not that I was doing that recently, but if you want to, if you want to avoid that pain, then you can just use our images locally, like from your laptop.

00:51:55.300 --> 00:52:02.680
And you can even like connect to our Dask gateway using our images from your local laptop and do like some really fun setups there.

00:52:02.680 --> 00:52:03.300
Yeah, I see.

00:52:03.300 --> 00:52:08.280
Because most of the work would be happening in the clusters, the Dask clusters, not locally anyway.

00:52:08.280 --> 00:52:08.660
Yeah.

00:52:08.660 --> 00:52:13.400
So all the compute happens there and then you bring back this little image that's your plot, your result.

00:52:13.400 --> 00:52:13.840
Okay.

00:52:13.840 --> 00:52:14.600
Yeah, very cool.

00:52:14.600 --> 00:52:16.620
So how do I get mine in here?

00:52:16.620 --> 00:52:18.020
Like I see the containers.

00:52:18.020 --> 00:52:20.120
I see you have the last commit here.

00:52:20.120 --> 00:52:20.440
Yeah.

00:52:20.440 --> 00:52:22.160
So there's one per, one per.

00:52:22.320 --> 00:52:27.380
Right now, honestly, the easiest way is to send me, you know, open up an issue and I'll take care of it for you.

00:52:27.380 --> 00:52:27.980
I see.

00:52:27.980 --> 00:52:30.520
Just because I haven't got this continuous deployment quite working out.

00:52:30.520 --> 00:52:33.260
There's an environment, you know, DIMO file there.

00:52:33.260 --> 00:52:33.980
That gets.

00:52:33.980 --> 00:52:34.720
Oh, yeah.

00:52:34.720 --> 00:52:37.680
So you go see, yeah, there's quite a few packages in here already.

00:52:37.680 --> 00:52:38.100
Yep.

00:52:38.160 --> 00:52:40.680
And those are just the ones we explicitly asked for.

00:52:40.680 --> 00:52:45.140
And then all their dependencies get pulled into a lock file and then built into a Docker images.

00:52:45.140 --> 00:52:54.300
And so this is building off a project from Pangeo, that group of geo scientists that I mentioned earlier, who have been struggling with this problem for several years now.

00:52:54.300 --> 00:52:57.240
So they have a really nice Docker eyes set up.

00:52:57.240 --> 00:52:57.560
Right.

00:52:57.640 --> 00:52:59.240
And we're just building off that base image.

00:52:59.240 --> 00:52:59.520
Cool.

00:52:59.520 --> 00:52:59.860
Yeah.

00:52:59.860 --> 00:53:01.960
Based on the Pangeo container.

00:53:01.960 --> 00:53:02.480
Very cool.

00:53:02.480 --> 00:53:08.820
Sam Aparia asks, how long is the temporary URL active for the signed URL, the blob storage?

00:53:08.820 --> 00:53:12.240
So that actually depends on whether or not you're authenticated.

00:53:12.240 --> 00:53:21.100
We have some controls to say the planetary computer hub requires access, but also you get an API token, which gives you a little bit longer lasting tokens.

00:53:21.100 --> 00:53:24.900
But forget what the actual current expiries are.

00:53:25.160 --> 00:53:35.800
If you use the planetary computer Python library, you just pip install planetary underscore computer and use that dot sign method, it will actually request a token.

00:53:35.800 --> 00:53:40.800
And then as the token is going to expire, it requests a new token.

00:53:40.800 --> 00:53:43.040
So it reaches the token and caches it.

00:53:43.040 --> 00:53:49.060
But it should be long enough for actually pulling down the data files that we have available.

00:53:49.060 --> 00:53:53.680
Because we're working against smaller cloud optimized formats.

00:53:54.060 --> 00:54:01.440
There aren't these 100 gig files that you should have to pull down and need a single SAS token to last for a really long time.

00:54:01.440 --> 00:54:04.940
So you can re-request if you need a new one as it expires.

00:54:04.940 --> 00:54:08.900
And like I said, that library actually takes care of the logic for you there.

00:54:08.900 --> 00:54:09.420
That's cool.

00:54:09.420 --> 00:54:10.120
Yeah, very nice.

00:54:10.120 --> 00:54:10.980
All right, guys.

00:54:10.980 --> 00:54:12.400
Really good work with this.

00:54:12.400 --> 00:54:14.160
And it seems like it's early days.

00:54:14.160 --> 00:54:16.060
It seems like it's getting started.

00:54:16.060 --> 00:54:17.840
There's probably going to be a lot more going on with this.

00:54:17.840 --> 00:54:18.400
Yeah, for sure.

00:54:18.700 --> 00:54:26.520
I'm going to go out on a limb and make a big prediction that understanding the climate and climate change is going to be more important, not less important in the future.

00:54:26.520 --> 00:54:29.440
So I suspect that's also going to grow some interest.

00:54:29.800 --> 00:54:29.980
It might.

00:54:29.980 --> 00:54:30.620
It might.

00:54:30.620 --> 00:54:30.620
It might.

00:54:30.620 --> 00:54:36.860
You know, the new report at PCC is making some heavy predictions.

00:54:36.860 --> 00:54:41.420
And, you know, within the decade, you know, we might reach, you know, plus 1.5 Celsius.

00:54:41.420 --> 00:54:44.360
And, you know, we're already in it.

00:54:44.360 --> 00:54:45.560
We're already feeling the effects.

00:54:45.560 --> 00:54:48.820
And, you know, this is the data about our Earth.

00:54:48.980 --> 00:54:54.200
And it's going to become more and more important as we mitigate and adapt to these effects.

00:54:54.200 --> 00:54:55.300
So, yeah, I agree.

00:54:55.300 --> 00:54:56.260
I think that's a good question.

00:54:56.260 --> 00:54:57.820
Yeah, if we, thanks.

00:54:57.820 --> 00:55:05.040
If we are going to plan our way out of it and plan for the future and, you know, science our way out of it, we're going to need stuff like this.

00:55:05.040 --> 00:55:06.200
So, well done.

00:55:06.200 --> 00:55:06.800
All right.

00:55:06.800 --> 00:55:07.960
I think we're about out of time.

00:55:07.960 --> 00:55:10.800
So let me ask you both the final two questions here.

00:55:10.800 --> 00:55:13.440
If you're going to write some Python code, what editor do you use?

00:55:13.440 --> 00:55:14.000
Rob?

00:55:14.000 --> 00:55:14.800
VS Code.

00:55:14.800 --> 00:55:17.120
I suspect I could guess that, but yeah.

00:55:17.120 --> 00:55:19.960
Yeah, actually, I was a big Emacs user.

00:55:19.960 --> 00:55:23.480
And then when I got this job, switched over to VS Code.

00:55:23.480 --> 00:55:25.620
It just integrated better with Windows.

00:55:25.620 --> 00:55:33.720
And then really got into the PyLance and the typing system, you know, doing type annotations and basically having a compiler for the Python code.

00:55:33.720 --> 00:55:34.640
Like really a change.

00:55:34.640 --> 00:55:40.600
Instead of having all of the types in my head and having to like worry about all that, actually having the type hinting.

00:55:40.600 --> 00:55:40.880
Yeah.

00:55:40.880 --> 00:55:42.940
It was something I wasn't doing a year ago.

00:55:42.940 --> 00:55:46.920
And now it's like drastically improved my development experience.

00:55:46.920 --> 00:55:48.020
It's a huge difference.

00:55:48.020 --> 00:55:48.280
Yeah.

00:55:48.280 --> 00:55:49.960
And I'm all about that as well.

00:55:49.960 --> 00:55:55.600
People talk about the types being super important for things like mypy and other stuff.

00:55:55.600 --> 00:55:57.480
And, you know, in a lot of cases it can be.

00:55:57.480 --> 00:56:02.720
But to me, the primary use case is when I hit dot after a thing, I wanted to tell me what I can do.

00:56:02.720 --> 00:56:07.100
And if I have to go to the documentation, then it's kind of like something is failing.

00:56:07.100 --> 00:56:08.360
I shouldn't need documentation.

00:56:08.360 --> 00:56:11.860
I should be able to just, you know, auto complete my way through the world mostly.

00:56:11.860 --> 00:56:12.700
Totally.

00:56:12.700 --> 00:56:18.680
And I come from a, I was a Scala developer for, you know, six, about six years.

00:56:18.680 --> 00:56:23.720
So I was used to very heavy, heavily typed system and kind of got away with it from Python.

00:56:23.720 --> 00:56:24.480
I was like, you know what?

00:56:24.480 --> 00:56:32.640
I like that there's not types, but I feel like the Python ecosystem is really hitting that sweet spot of like introducing enough typing, which is really great.

00:56:32.740 --> 00:56:35.300
And then the inference flies along for the rest of the program.

00:56:35.300 --> 00:56:35.540
Yeah.

00:56:35.540 --> 00:56:35.980
Totally.

00:56:35.980 --> 00:56:36.420
Yeah.

00:56:36.420 --> 00:56:37.540
All right, Tom, how about you?

00:56:37.540 --> 00:56:39.500
VS Code as well for most stuff.

00:56:39.500 --> 00:56:45.740
And then Emacs for Magit, Magit, the Git client, and then a bit of Vim every now and then.

00:56:45.740 --> 00:56:46.220
Right on.

00:56:46.220 --> 00:56:46.960
Very cool.

00:56:46.960 --> 00:56:47.500
Yeah.

00:56:47.500 --> 00:56:47.960
All right.

00:56:47.960 --> 00:56:56.540
And then the other question is for either of you, there's like a cool, notable IPI or Conda package that you're like, oh, I came across this.

00:56:56.540 --> 00:56:57.120
It was amazing.

00:56:57.120 --> 00:56:58.120
People should know about it.

00:56:58.120 --> 00:56:58.760
Any ideas?

00:56:58.760 --> 00:56:59.280
Have you got one?

00:56:59.280 --> 00:56:59.960
Sure.

00:56:59.960 --> 00:57:01.520
I'll go for Seaborn.

00:57:01.960 --> 00:57:06.020
It's a plotting library from Michael Wascom built on top of Matplotlib.

00:57:06.020 --> 00:57:09.160
It's just really great for exploratory data analysis.

00:57:09.160 --> 00:57:15.320
Easily create these great visualizations for mostly tabular data sets, but not exclusively.

00:57:15.320 --> 00:57:16.200
Oh, that's interesting.

00:57:16.200 --> 00:57:16.960
I knew Seaborn.

00:57:16.960 --> 00:57:17.780
I knew Matplotlib.

00:57:17.780 --> 00:57:20.940
I didn't realize that Seaborn was like, let's make Matplotlib easier.

00:57:20.940 --> 00:57:21.240
Yeah.

00:57:21.240 --> 00:57:29.980
Essentially, for this very specific use case, Matplotlib is extremely flexible, but there's a lot of boilerplate and Seaborn just wraps that all up nicely.

00:57:29.980 --> 00:57:30.400
Yeah.

00:57:30.400 --> 00:57:30.980
Super cool.

00:57:31.440 --> 00:57:31.680
All right.

00:57:31.680 --> 00:57:33.280
Well, thank you so much for being here.

00:57:33.280 --> 00:57:34.080
Final call to action.

00:57:34.080 --> 00:57:36.740
People want to get started with Microsoft Planetary Computer.

00:57:36.740 --> 00:57:38.660
Maybe they've got some climate research.

00:57:38.660 --> 00:57:39.340
What do they do?

00:57:39.340 --> 00:57:41.280
Planetarycomputer.microsoft.com.

00:57:41.280 --> 00:57:43.380
That'll get you anywhere you need to go.

00:57:43.380 --> 00:57:47.580
And then if you want an account, then it's slash account slash request, I believe.

00:57:47.580 --> 00:57:47.880
Yeah.

00:57:47.880 --> 00:57:50.280
There's a big request access right at the top.

00:57:50.280 --> 00:57:51.100
So you can click that.

00:57:51.100 --> 00:57:51.540
Awesome.

00:57:51.540 --> 00:57:52.340
Yeah, exactly.

00:57:52.560 --> 00:57:52.780
All right.

00:57:52.780 --> 00:57:54.140
Rob, Tom, thank you for being here.

00:57:54.140 --> 00:57:55.800
And thanks for all the good work.

00:57:55.800 --> 00:57:56.440
Thanks for having us.

00:57:56.440 --> 00:57:56.880
This is great.

00:57:56.880 --> 00:57:57.220
Awesome.

00:57:57.220 --> 00:57:57.720
Thanks so much.

00:57:57.720 --> 00:57:58.120
Yeah.

00:57:58.120 --> 00:57:58.700
Bye.

00:57:58.700 --> 00:57:59.140
See ya.

00:58:00.460 --> 00:58:03.320
This has been another episode of Talk Python to Me.

00:58:03.320 --> 00:58:07.060
Our guests on this episode were Rob Emanuel and Tom Augsberger.

00:58:07.560 --> 00:58:12.100
And it's been brought to you by Shortcut, formerly Clubhouse I.O., us over at Talk Python Training,

00:58:12.100 --> 00:58:14.700
and the transcripts were brought to you by Assembly AI.

00:58:14.700 --> 00:58:20.260
Choose Shortcut, formerly Clubhouse I.O., for tracking all of your project's work.

00:58:20.260 --> 00:58:23.640
Because you shouldn't have to project manage your project management.

00:58:23.640 --> 00:58:26.440
Visit talkpython.fm/shortcut.

00:58:26.440 --> 00:58:29.460
Do you need a great automatic speech-to-text API?

00:58:29.460 --> 00:58:31.980
Get human-level accuracy in just a few lines of code.

00:58:31.980 --> 00:58:34.840
Visit talkpython.fm/assembly AI.

00:58:34.840 --> 00:58:36.620
Want to level up your Python?

00:58:37.000 --> 00:58:40.740
We have one of the largest catalogs of Python video courses over at Talk Python.

00:58:40.740 --> 00:58:45.840
Our content ranges from true beginners to deeply advanced topics like memory and async.

00:58:45.840 --> 00:58:48.520
And best of all, there's not a subscription in sight.

00:58:48.520 --> 00:58:51.420
Check it out for yourself at training.talkpython.fm.

00:58:51.420 --> 00:58:56.100
Be sure to subscribe to the show, open your favorite podcast app, and search for Python.

00:58:56.100 --> 00:58:57.400
We should be right at the top.

00:58:57.400 --> 00:59:02.560
You can also find the iTunes feed at /itunes, the Google Play feed at /play,

00:59:02.560 --> 00:59:06.780
and the direct RSS feed at /rss on talkpython.fm.

00:59:07.680 --> 00:59:10.200
We're live streaming most of our recordings these days.

00:59:10.200 --> 00:59:13.620
If you want to be part of the show and have your comments featured on the air,

00:59:13.620 --> 00:59:17.980
be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

00:59:17.980 --> 00:59:19.880
This is your host, Michael Kennedy.

00:59:19.880 --> 00:59:21.180
Thanks so much for listening.

00:59:21.180 --> 00:59:22.340
I really appreciate it.

00:59:22.600 --> 00:59:24.260
Now get out there and write some Python code.

00:59:24.260 --> 00:59:24.260
Now get out there and write some Python code.

00:59:24.260 --> 00:59:25.260
Thank you.

00:59:25.260 --> 00:59:55.240
Thank you.

