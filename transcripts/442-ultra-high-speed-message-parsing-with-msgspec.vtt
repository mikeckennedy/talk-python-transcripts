WEBVTT

00:00:00.001 --> 00:00:04.540
If you're a fan of Pydantic or data classes, you'll definitely be interested in this episode.

00:00:04.540 --> 00:00:09.800
We are talking about a super fast data modeling and validation framework called msgspec.

00:00:09.800 --> 00:00:15.120
And some of the types in here might even be better for general purpose use than Python's

00:00:15.120 --> 00:00:15.800
native classes.

00:00:15.800 --> 00:00:19.740
Join me and Jim, Chris Hariff, to talk about his framework, msgspec.

00:00:19.740 --> 00:00:25.460
This is Talk Python to Me, episode 442, recorded November 2nd, 2023.

00:00:25.460 --> 00:00:43.020
Welcome to Talk Python to Me, a weekly podcast on Python.

00:00:43.020 --> 00:00:44.760
This is your host, Michael Kennedy.

00:00:44.760 --> 00:00:49.800
Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using at Talk Python,

00:00:49.800 --> 00:00:52.240
both on fosstodon.org.

00:00:52.460 --> 00:00:57.340
Keep up with the show and listen to over seven years of past episodes at talkpython.fm.

00:00:57.340 --> 00:01:01.120
We've started streaming most of our episodes live on YouTube.

00:01:01.120 --> 00:01:06.840
Subscribe to our YouTube channel over at talkpython.fm/youtube to get notified about upcoming

00:01:06.840 --> 00:01:08.660
shows and be part of that episode.

00:01:08.660 --> 00:01:14.200
This episode is sponsored by Posit Connect from the makers of Shiny.

00:01:14.200 --> 00:01:18.700
Publish, share, and deploy all of your data projects that you're creating using Python.

00:01:19.180 --> 00:01:24.640
Streamlit, Dash, Shiny, Bokeh, FastAPI, Flask, Reports, Dashboards, and APIs.

00:01:24.640 --> 00:01:27.040
Posit Connect supports all of them.

00:01:27.040 --> 00:01:32.700
Try Posit Connect for free by going to talkpython.fm/posit, P-O-S-I-T.

00:01:32.700 --> 00:01:36.960
And it's brought to you by us over at Talk Python Training.

00:01:36.960 --> 00:01:41.580
Did you know that we have over 250 hours of Python courses?

00:01:41.580 --> 00:01:42.760
Yeah, that's right.

00:01:42.860 --> 00:01:45.300
Check him out at talkpython.fm/courses.

00:01:45.300 --> 00:01:48.000
Jim.

00:01:48.000 --> 00:01:48.580
Hello.

00:01:48.580 --> 00:01:48.920
Hello.

00:01:48.920 --> 00:01:49.920
Welcome to Talk Python.

00:01:49.920 --> 00:01:50.960
It's awesome to have you here.

00:01:50.960 --> 00:01:51.260
Yeah.

00:01:51.260 --> 00:01:51.920
Thanks for having me.

00:01:51.920 --> 00:01:52.640
Yeah, of course.

00:01:52.640 --> 00:01:57.140
I spoke to the Litestar guys, you know, at Litestar.dev and had them on the show.

00:01:57.140 --> 00:02:03.200
And I was talking about their DTOs, different types of objects they can pass around in their

00:02:03.200 --> 00:02:04.300
APIs and their web apps.

00:02:04.300 --> 00:02:09.880
And like FastAPI, they've got this concept where you kind of bind a type, like a class

00:02:09.880 --> 00:02:12.700
or something, to an input, to a web API.

00:02:12.700 --> 00:02:15.440
And it does all that sort of magic like FastAPI.

00:02:15.440 --> 00:02:17.160
And I said, oh, so you guys probably work with PyDanty.

00:02:17.160 --> 00:02:19.780
It's like, yes, but let me tell you about msgspec.

00:02:19.780 --> 00:02:21.520
Because that's where the action is.

00:02:21.520 --> 00:02:25.320
They were so enamored with your project that I just had to reach out and have you on.

00:02:25.320 --> 00:02:26.540
It looks super cool.

00:02:26.540 --> 00:02:28.600
I think people are going to really enjoy learning about it.

00:02:28.600 --> 00:02:29.400
Thanks.

00:02:29.400 --> 00:02:30.880
Yeah, it's nice to hear that.

00:02:30.880 --> 00:02:31.160
Yeah.

00:02:31.160 --> 00:02:33.080
We're going to dive into the details.

00:02:33.080 --> 00:02:33.960
It's going to be a lot of fun.

00:02:33.960 --> 00:02:37.800
Before we get to them, though, give us just a quick introduction on who you are.

00:02:37.800 --> 00:02:39.760
So people don't know you yet.

00:02:39.760 --> 00:02:41.060
So my name is Jim Christreif.

00:02:41.060 --> 00:02:47.400
I am currently an engineering manager doing actually mostly dev work at Voltron Data, working

00:02:47.400 --> 00:02:51.700
on the IBIS project, which is a completely different conversation than what we're going to have today.

00:02:51.700 --> 00:02:58.000
Prior to that, I popped around a couple of startups and was most of them doing Dask was the main

00:02:58.000 --> 00:03:01.240
thing I've contributed to in the past on an open source Python front.

00:03:01.240 --> 00:03:04.360
For those not aware, Dask is a distributed compute ecosystem.

00:03:04.360 --> 00:03:09.200
I come from the PyData side of the Python ecosystem, not the web dev side.

00:03:09.200 --> 00:03:09.600
Nice.

00:03:09.600 --> 00:03:13.340
Yeah, I've had Matthew Rocklin on a couple of times, but it's been a while, so people don't

00:03:13.340 --> 00:03:13.900
necessarily know.

00:03:13.900 --> 00:03:18.120
But it's like super distributed pandas, kind of.

00:03:18.120 --> 00:03:19.920
Grid computing for pandas, sort of.

00:03:20.040 --> 00:03:22.880
Or say like Spark written in Python.

00:03:22.880 --> 00:03:23.300
Sure.

00:03:23.300 --> 00:03:27.820
You know, another thing that's been on, kind of on my radar, but I didn't really necessarily

00:03:27.820 --> 00:03:29.720
realize it was associated with you.

00:03:29.720 --> 00:03:31.260
Tell people just a bit about IBIS.

00:03:31.260 --> 00:03:33.000
Like IBIS is looking pretty interesting.

00:03:33.000 --> 00:03:35.280
IBIS is, I don't want to say the wrong thing.

00:03:35.280 --> 00:03:38.740
IBIS is a portable data frame library is the current tagline we're using.

00:03:39.260 --> 00:03:42.500
If you're coming from R, it's dplyr for Python.

00:03:42.500 --> 00:03:43.500
It's more than that.

00:03:43.500 --> 00:03:46.800
And it's not exactly that, but that's a quick mental model.

00:03:46.800 --> 00:03:49.420
So you write data frame like code.

00:03:49.420 --> 00:03:50.720
We're not pandas compatible.

00:03:50.720 --> 00:03:53.140
We're pandas like enough that you might find something familiar.

00:03:53.140 --> 00:03:58.640
And it can compile down to generate SQL for 18 plus different database backends.

00:03:58.640 --> 00:04:01.000
Also like PySpark and a couple other things.

00:04:01.000 --> 00:04:01.320
Okay.

00:04:01.320 --> 00:04:03.260
So you write your code once and you kind of run it on whatever.

00:04:03.260 --> 00:04:03.700
I see.

00:04:03.820 --> 00:04:08.320
And you do pandas like things, but it converts those into database queries.

00:04:08.320 --> 00:04:08.740
Is that?

00:04:08.740 --> 00:04:09.180
Yeah.

00:04:09.180 --> 00:04:09.600
Yeah.

00:04:09.600 --> 00:04:10.980
So it's a data frame API.

00:04:10.980 --> 00:04:16.100
It's not pandas compatible, but if you're familiar with pandas, you should be able to pick it up.

00:04:16.100 --> 00:04:19.120
You know, we cleaned up what we thought as a bunch of rough edges of the pandas API.

00:04:19.120 --> 00:04:19.480
Yeah.

00:04:19.480 --> 00:04:22.060
Were those pandas one or pandas two rough edges?

00:04:22.060 --> 00:04:22.880
Both.

00:04:22.880 --> 00:04:23.720
It's, I don't know.

00:04:23.720 --> 00:04:25.020
It's pandas like.

00:04:25.020 --> 00:04:25.760
Sure.

00:04:25.760 --> 00:04:26.300
Yeah.

00:04:26.300 --> 00:04:27.120
This looks really cool.

00:04:27.120 --> 00:04:29.820
That's a topic for another day, but awesome.

00:04:29.820 --> 00:04:30.660
People can check that out.

00:04:30.660 --> 00:04:37.420
But this time you're here to talk about your personal project, msgspec.

00:04:37.420 --> 00:04:38.620
Am I saying that right?

00:04:38.620 --> 00:04:40.480
Or you say MSG or message spec?

00:04:40.480 --> 00:04:42.540
Message spec is how it is.

00:04:42.540 --> 00:04:48.900
I think a lot of these projects sometimes need a little, like here's the MP3 you can press play on.

00:04:48.900 --> 00:04:50.680
Like how it's meant to be said, you know?

00:04:50.680 --> 00:04:55.140
I mean, sometimes it's kind of obvious, like PyPI versus PyPI.

00:04:55.140 --> 00:04:58.980
Other times it's just like, okay, I know you have a really clever name.

00:04:58.980 --> 00:04:59.800
Yes, I know.

00:04:59.800 --> 00:05:01.140
People say NumPy all the time.

00:05:01.140 --> 00:05:05.540
I'm like, I don't want to, I try to not correct guests because it's, it's not kind.

00:05:05.540 --> 00:05:06.480
But I also feel awkward.

00:05:06.480 --> 00:05:08.500
They will say NumPy and I'll say, how do you feel about NumPy?

00:05:08.500 --> 00:05:09.200
Like NumPy's great.

00:05:09.200 --> 00:05:12.220
I'm like, okay, we're just going back and forth like this for the next hour.

00:05:12.220 --> 00:05:12.660
It's fine.

00:05:12.660 --> 00:05:17.600
But yeah, it's, it's always, I think some of these could use a little, like a little play.

00:05:17.600 --> 00:05:18.640
So msgspec.

00:05:18.640 --> 00:05:19.940
Tell people about what it is.

00:05:19.940 --> 00:05:20.180
Yeah.

00:05:20.380 --> 00:05:22.240
So gone through a couple of different taglines.

00:05:22.240 --> 00:05:26.900
The current one is a fast serialization and validation library with a built-in support for

00:05:26.900 --> 00:05:29.160
JSON, message pack, YAML, and TOML.

00:05:29.160 --> 00:05:34.760
If you are familiar with Pydantic, that's probably one of the closest, you know, most popular libraries

00:05:34.760 --> 00:05:35.740
that does a similar thing.

00:05:35.740 --> 00:05:40.940
You define kind of a structure of your data using type annotations and msgspec will parse

00:05:40.940 --> 00:05:44.400
your data to ensure it is that structure and does so efficiently.

00:05:44.400 --> 00:05:48.640
It's also compatible with a lot of the other serialization libraries.

00:05:48.640 --> 00:05:53.320
You could also use it as a stand-in for JSON, you know, with the JSON dumps, JSON loads.

00:05:53.320 --> 00:05:55.020
You don't need to specify the types.

00:05:55.020 --> 00:05:55.400
Right.

00:05:55.400 --> 00:06:01.400
It's, I think the mental model of kind of like, it swims in the same water or the same pond

00:06:01.400 --> 00:06:06.080
as Pydantic, but it's also fairly distinguished from Pydantic, right?

00:06:06.080 --> 00:06:09.100
As we're going to explore throughout our chat here.

00:06:09.280 --> 00:06:14.540
The goal from my side, one of the goals was to replicate more of the experience writing

00:06:14.540 --> 00:06:19.880
Rust or Go with Rust-SERDE or Go's JSON, where the serializer kind of stands in the background

00:06:19.880 --> 00:06:24.840
rather than my experience working with Pydantic, where it felt like the base model kind of stood

00:06:24.840 --> 00:06:25.340
in the foreground.

00:06:25.340 --> 00:06:27.040
You're defining the model.

00:06:27.040 --> 00:06:30.460
Serialization kind of comes onto the types you've defined, but you're not actually working

00:06:30.460 --> 00:06:32.020
with the serializers on the types themselves.

00:06:32.020 --> 00:06:32.500
Got it.

00:06:32.500 --> 00:06:35.820
So an example, let me see if I do have it.

00:06:35.820 --> 00:06:41.560
An example might be if I want to take some message I got from, some response I got from

00:06:41.560 --> 00:06:44.400
an API, I want to turn it into a Pydantic model or I'm writing an API.

00:06:44.400 --> 00:06:46.100
I want to take something from a client, whatever.

00:06:46.100 --> 00:06:48.360
I'll go and create a Pydantic class.

00:06:48.800 --> 00:06:55.100
And then the way I use it is I go to that class and I'll say star, star, dictionary I got.

00:06:55.100 --> 00:06:57.840
And then it comes to life like that, right?

00:06:57.840 --> 00:07:03.100
Where there's a little more focus on just the serialization and it has this capability.

00:07:03.100 --> 00:07:06.120
But like you said, it's optional in the sense.

00:07:06.120 --> 00:07:06.480
Yeah.

00:07:06.480 --> 00:07:11.120
In message spec, all types are on equal footing.

00:07:11.120 --> 00:07:17.060
So we use functions, not methods, because if you want to decode into a list of ints, I

00:07:17.060 --> 00:07:18.620
can't add a method to a list.

00:07:18.620 --> 00:07:20.020
You know, it's a Python built-in type.

00:07:20.020 --> 00:07:20.220
Yeah.

00:07:20.220 --> 00:07:26.920
So you'd say msgspec dot JSON dot decode your message and then you'd specify the type

00:07:26.920 --> 00:07:29.100
annotation as part of that function call.

00:07:29.100 --> 00:07:30.960
So it could be, you know, list bracket int.

00:07:31.100 --> 00:07:31.260
Right.

00:07:31.260 --> 00:07:37.200
So you'll say decode and then you might say type equals list of your type or like you

00:07:37.200 --> 00:07:37.900
say, list of int.

00:07:37.900 --> 00:07:42.860
And that's hard when you have to have a class that knows how to basically become what the

00:07:42.860 --> 00:07:48.120
model, the data passed in is, even if it's just a list, some Pydantic classes, you got

00:07:48.120 --> 00:07:52.260
to kind of jump through some hoops to say, hey, Pydantic, I don't have a thing to give

00:07:52.260 --> 00:07:52.480
you.

00:07:52.480 --> 00:07:54.180
I want a list of those things.

00:07:54.180 --> 00:07:56.940
And that's the top level thing is, you know, bracket bracket.

00:07:56.940 --> 00:08:00.600
It's not, it's not any one thing I can specify in Python easily.

00:08:00.860 --> 00:08:00.900
Yeah.

00:08:00.900 --> 00:08:04.820
To be fair to the Pydantic project, I believe in V2, the type adapter.

00:08:04.820 --> 00:08:05.400
Yes, exactly.

00:08:05.400 --> 00:08:06.940
Object can work with that.

00:08:06.940 --> 00:08:09.920
But that is, you know, it's a different way of working with it.

00:08:09.920 --> 00:08:12.020
I wanted to have one API that did it all.

00:08:12.020 --> 00:08:12.320
Sure.

00:08:12.320 --> 00:08:13.360
And it's awesome.

00:08:13.360 --> 00:08:13.900
They made it.

00:08:13.900 --> 00:08:15.900
I mean, I want to just put this out front.

00:08:15.900 --> 00:08:17.720
Like, I'm a massive fan of Pydantic.

00:08:17.720 --> 00:08:19.820
What Samuel's done there is incredible.

00:08:19.820 --> 00:08:24.480
And it's just, it's really made a big difference in the way that people work with data and Python.

00:08:24.480 --> 00:08:25.260
It's awesome.

00:08:25.260 --> 00:08:29.980
But it's also awesome that you have this project that is an alternative and it makes different assumptions.

00:08:29.980 --> 00:08:33.880
And you can see those really play out in like the performance or the APIs.

00:08:33.880 --> 00:08:39.360
So, you know, like Pydantic encourages you to take your classes and then send them the data.

00:08:39.520 --> 00:08:45.700
But you've kind of got to know, like, oh, there's this type adapter thing that I can give a list of my class and then make it work.

00:08:45.700 --> 00:08:45.860
Right.

00:08:45.860 --> 00:08:50.820
But it's not just, oh, you just fall into that by trying to play with the API, you know?

00:08:50.820 --> 00:08:51.080
Yeah.

00:08:51.080 --> 00:08:51.480
Yeah.

00:08:51.480 --> 00:08:56.260
And I think having, being able to specify any type means we work with standard library data classes.

00:08:56.260 --> 00:09:00.640
The same as we work with our built-in struct type or we also work with adders types.

00:09:00.640 --> 00:09:02.460
You know, everything is kind of on equal footing.

00:09:02.460 --> 00:09:02.760
Yeah.

00:09:02.980 --> 00:09:10.000
And what I want to really dig into is your custom struct type that has some really cool properties.

00:09:10.000 --> 00:09:11.880
Not class properties, but components.

00:09:11.880 --> 00:09:14.680
Features of the class of the type there.

00:09:14.680 --> 00:09:14.820
Yeah.

00:09:14.820 --> 00:09:16.460
Let's look at a couple of things here.

00:09:16.560 --> 00:09:23.940
So, as you said, it's fast and I love how somehow italicies on the word fast makes it feel even faster.

00:09:23.940 --> 00:09:26.560
Like it's leaning forward, you know, it's leaning into the speed.

00:09:26.560 --> 00:09:28.980
A fast serialization and validation library.

00:09:28.980 --> 00:09:32.340
The validation is kind of can be, but not required, right?

00:09:32.340 --> 00:09:34.400
The types can be, but they don't have to be.

00:09:34.400 --> 00:09:37.840
So, I think that's one of the ways it really differs from Pydantic.

00:09:37.840 --> 00:09:45.580
But the other is Pydantic is quite focused on JSON, whereas this is JSON, message pack, YAML, and TOML.

00:09:45.900 --> 00:09:47.020
Everyone knows what JSON is.

00:09:47.020 --> 00:09:49.940
I always thought of TOML as kind of like YAML.

00:09:49.940 --> 00:09:51.000
Are they really different?

00:09:51.000 --> 00:09:54.120
It's another configuration focused language.

00:09:54.120 --> 00:09:59.340
I think some people do JSON for config files, but I personally don't like to handwrite JSON.

00:09:59.340 --> 00:10:04.240
YAML and TOML are like more human friendly, in quotes, forms of that.

00:10:04.240 --> 00:10:06.820
YAML is a superset of JSON.

00:10:06.820 --> 00:10:08.180
TOML is its own thing.

00:10:08.180 --> 00:10:08.540
Got it.

00:10:08.540 --> 00:10:12.500
And then message pack is a binary JSON-like file format.

00:10:12.500 --> 00:10:13.440
Yeah, message pack.

00:10:13.440 --> 00:10:14.920
I don't know how many people work with that.

00:10:15.080 --> 00:10:16.840
Where would people run into message pack?

00:10:16.840 --> 00:10:17.240
Yeah.

00:10:17.240 --> 00:10:22.660
If they were, say, consuming an API, or what API framework would people be generating message

00:10:22.660 --> 00:10:23.580
pack in Python, typically?

00:10:23.580 --> 00:10:24.500
That's a good question.

00:10:24.500 --> 00:10:29.740
So, going back to the creation of this project, actually, msgspec sounds a lot like message

00:10:29.740 --> 00:10:30.020
pack.

00:10:30.020 --> 00:10:30.740
And that was intentional.

00:10:30.740 --> 00:10:31.000
It does, yeah.

00:10:31.000 --> 00:10:33.240
Because that's what I wrote it for originally.

00:10:33.240 --> 00:10:37.520
So, as I said at the beginning, I'm one of the original contributors to Dask.

00:10:37.520 --> 00:10:38.660
Worked on Dask forever.

00:10:38.660 --> 00:10:43.860
And the Dask distributed scheduler uses message pack for its RPC serialization layer.

00:10:43.860 --> 00:10:46.040
That kind of fell out of what was available at the time.

00:10:46.040 --> 00:10:47.620
We benchmarked a bunch of different libraries.

00:10:48.340 --> 00:10:52.700
And that was the fastest way to send bytes between nodes in 2015.

00:10:52.700 --> 00:10:53.100
Sure.

00:10:53.100 --> 00:10:58.800
The distributed scheduler's RPC framework has kind of grown haphazardly over time.

00:10:58.800 --> 00:11:01.720
And there were a bunch of bugs due to some hacky things we were doing with it.

00:11:01.720 --> 00:11:03.620
And also, it was slower than we would have wanted.

00:11:03.900 --> 00:11:09.720
So, this was an attempt to write a faster message pack library for Python that also did

00:11:09.720 --> 00:11:10.700
fancier things.

00:11:10.700 --> 00:11:11.680
Supported more types.

00:11:11.680 --> 00:11:16.320
Did some schema validation because we wanted to catch the worker is sending this data and

00:11:16.320 --> 00:11:18.000
the scheduler is getting it and saying it's wrong.

00:11:18.000 --> 00:11:25.020
And we wanted to also add in a way to make schema evolution, meaning that I can have different

00:11:25.020 --> 00:11:28.900
versions of my worker and scheduler and client process and things kind of work.

00:11:28.900 --> 00:11:33.240
If I add new features to the scheduler, they don't break the client.

00:11:33.240 --> 00:11:36.460
You know, we have a nice forward and backward compatibility story.

00:11:36.460 --> 00:11:38.100
And so, that's what kind of fell out.

00:11:38.100 --> 00:11:39.640
Yeah, it's a really nice feature.

00:11:39.640 --> 00:11:40.720
We're going to dive into that.

00:11:40.720 --> 00:11:45.360
But, you know, you might think, oh, well, just update your client or update the server.

00:11:45.360 --> 00:11:47.860
But there's all sorts of situations that get really weird.

00:11:47.860 --> 00:11:54.060
Like, if you have Redis as a caching layer and you create a message pack object and stick

00:11:54.060 --> 00:11:58.820
it in there and then you deploy a new version of the app, it could maybe can't,

00:11:58.820 --> 00:12:02.820
deserialize anything in the cache anymore because it says something's missing or something's

00:12:02.820 --> 00:12:04.160
there that it doesn't expect.

00:12:04.160 --> 00:12:04.480
Right.

00:12:04.480 --> 00:12:07.100
And so, this evolution is important there.

00:12:07.100 --> 00:12:10.800
If you've got long running work and you stash it into a database and you pull it back out,

00:12:10.800 --> 00:12:14.820
like all these things where it kind of lives a little outside the process, all of a sudden

00:12:14.820 --> 00:12:19.400
it starts to matter that before you even consider like clients that run separate code.

00:12:19.400 --> 00:12:19.620
Right.

00:12:19.620 --> 00:12:22.440
Like you could be the client, just different places in time.

00:12:22.440 --> 00:12:22.700
Yeah.

00:12:22.700 --> 00:12:23.100
Yeah.

00:12:23.100 --> 00:12:26.820
So, adding a little bit more structure to how you define messages in a way to make the

00:12:26.820 --> 00:12:27.680
scheduler more maintainable.

00:12:27.680 --> 00:12:28.880
That work never landed.

00:12:28.880 --> 00:12:31.680
It's as it is with open source projects.

00:12:31.680 --> 00:12:33.760
It's a democracy and also a democracy.

00:12:33.760 --> 00:12:36.580
And, you know, you don't always, paths can be done at dead ends.

00:12:36.580 --> 00:12:40.680
I still think it'll be valuable in the future, but some stuff was changing the scheduler and

00:12:40.680 --> 00:12:45.680
serialization is no longer the bottleneck that it was two and a half years ago when this originally

00:12:45.680 --> 00:12:46.000
started.

00:12:46.000 --> 00:12:49.520
So, let me put this in context for people to maybe make it relevant.

00:12:49.520 --> 00:12:55.740
Like maybe right now someone's got a FastAPI API and they're using Pydantic and obviously

00:12:55.740 --> 00:12:58.680
it generates all the awesome JSON they want.

00:12:58.680 --> 00:13:06.060
Is there a way to, how would you go about creating, say, a Python server-based system set

00:13:06.060 --> 00:13:11.400
of APIs that maybe as an option take message pack or maybe use that as a primary way?

00:13:11.400 --> 00:13:14.360
Like it could be maybe, you know, passing an accept header.

00:13:14.360 --> 00:13:15.740
To take message pack?

00:13:15.740 --> 00:13:19.460
If you want to exchange message pack, client server, Python right now, what do you do?

00:13:19.460 --> 00:13:20.720
That's a good question.

00:13:20.720 --> 00:13:22.060
To be clear, I am not a web dev.

00:13:22.060 --> 00:13:23.400
I do not do this for a living.

00:13:23.400 --> 00:13:26.820
I think there is no standard application slash message pack.

00:13:26.820 --> 00:13:31.480
I think people can use it if they want, but that's not a, it's a standardized thing the

00:13:31.480 --> 00:13:32.400
same way that JSON is.

00:13:32.400 --> 00:13:32.720
Yeah.

00:13:32.720 --> 00:13:35.920
I think that Litestar as a framework does support this out of the box.

00:13:35.920 --> 00:13:37.220
I don't know about FastAPI.

00:13:37.220 --> 00:13:41.520
I'm sure there's a way to hack it in as there is with any ASCII server.

00:13:41.520 --> 00:13:45.620
Yeah, Litestar, like I said, I had Litestar on those guys maybe a month ago and...

00:13:45.620 --> 00:13:47.380
Yeah, super, super cool about that.

00:13:47.380 --> 00:13:52.440
So, yeah, I know that they support msgspec and a lot of different options there, but,

00:13:52.440 --> 00:13:57.300
you know, you could just, I imagine you could just return binary bits between you and your

00:13:57.300 --> 00:13:58.100
client.

00:13:58.100 --> 00:14:04.360
I'm thinking of like latency sensitive microservice type things sort of within your data center.

00:14:04.360 --> 00:14:08.920
How can you lower serialization, deserialization, serialization, like all that cost that could

00:14:08.920 --> 00:14:14.060
be the max, you know, the biggest part of what's making your app spend time and energy?

00:14:14.060 --> 00:14:18.960
Michael out there says, would love PyArrow parquet support for large data.

00:14:18.960 --> 00:14:22.840
There's been a request for Aero integration with msgspec.

00:14:22.840 --> 00:14:24.440
I'm not exactly sure what that would look like.

00:14:24.440 --> 00:14:26.700
Aero containers are pretty efficient on their own.

00:14:26.700 --> 00:14:31.620
Breaking them out into a bunch of objects or stuff to work with msgspec doesn't necessarily

00:14:31.620 --> 00:14:32.500
make sense in my mind.

00:14:32.500 --> 00:14:36.700
But anyway, if you have ideas on that, please open an issue or comment on the existing issue.

00:14:36.900 --> 00:14:37.700
Yeah, indeed.

00:14:37.700 --> 00:14:38.380
All right.

00:14:38.380 --> 00:14:39.800
So let's see.

00:14:39.800 --> 00:14:43.960
Some of the highlights are high performance encoders and decoders across those protocols

00:14:43.960 --> 00:14:44.440
we talked.

00:14:44.440 --> 00:14:45.300
You have benchmarks.

00:14:45.300 --> 00:14:46.920
We'll look at them for in a minute.

00:14:46.920 --> 00:14:51.960
You have a really nice, a lot of support for different types that can go in there that

00:14:51.960 --> 00:14:56.680
can be serialized, but there's also a way to extend it to say, I've got a custom type that

00:14:56.680 --> 00:15:01.400
you don't think is serializable to whatever end thing, a message pack, JSON, whatever.

00:15:01.400 --> 00:15:04.500
But I can write a little code that'll take it either way.

00:15:04.660 --> 00:15:09.140
You know, dates are something that drive me crazy, but it could be like an object ID out

00:15:09.140 --> 00:15:13.440
of MongoDB or other things that seem like they should go back and forth, but don't, you know,

00:15:13.440 --> 00:15:13.740
right?

00:15:13.740 --> 00:15:14.680
So that's really nice.

00:15:14.680 --> 00:15:18.040
And then zero cost schema validation, right?

00:15:18.040 --> 00:15:22.980
It validates, decodes and validates JSON two times as fast as ORJSON, which is one of

00:15:22.980 --> 00:15:24.740
the high performance JSON decoders.

00:15:24.740 --> 00:15:26.580
And that's just decoding, right?

00:15:26.580 --> 00:15:30.660
And then the struct thing that we're going to talk about, which the struct type is kind

00:15:30.660 --> 00:15:33.720
of what brings the parody with Pydantic, right?

00:15:33.720 --> 00:15:34.060
Yeah.

00:15:34.060 --> 00:15:36.100
You could think of it as Pydantic's base model.

00:15:36.100 --> 00:15:38.600
It's our built-in data class-like type.

00:15:38.600 --> 00:15:38.960
Nice.

00:15:38.960 --> 00:15:40.820
So structs are data class-like.

00:15:40.820 --> 00:15:44.660
Like everything in msgspec are implemented fully as a C extension.

00:15:45.000 --> 00:15:50.400
Getting these to work required reading a lot of the CPython source code because we're doing

00:15:50.400 --> 00:15:54.060
some things that I don't want to say that they don't want you to do.

00:15:54.060 --> 00:15:57.200
We're not doing them wrong, but they're not really documented.

00:15:57.200 --> 00:16:03.980
So for example, when you subclass for msgspec.struct, that's using a metaclass mechanism,

00:16:03.980 --> 00:16:06.320
which is a way of defining types to define types.

00:16:06.880 --> 00:16:11.880
And the metaclass is written in C, which CPython doesn't make easy to do.

00:16:11.880 --> 00:16:16.540
So it's a C extension metaclass that creates new C types.

00:16:16.540 --> 00:16:17.580
They're pretty speedy.

00:16:17.580 --> 00:16:23.320
They are 10 to 100x faster for most operations than even handwriting a class that does the

00:16:23.320 --> 00:16:25.920
same thing, but definitely more than data classes or adders.

00:16:25.920 --> 00:16:26.240
Yeah.

00:16:26.240 --> 00:16:27.160
It's super interesting.

00:16:27.160 --> 00:16:28.540
And I really want to dive into that.

00:16:28.540 --> 00:16:34.820
I almost can see the struct type being relevant even outside of msgspec in general, potentially.

00:16:34.820 --> 00:16:36.720
So yeah, we'll see about that.

00:16:36.760 --> 00:16:37.360
But it's super cool.

00:16:37.360 --> 00:16:40.980
And Michael also points out, like, he's the one who made the issue.

00:16:40.980 --> 00:16:42.800
So sorry about that.

00:16:42.800 --> 00:16:45.920
He's commented already, I suppose, in a sense.

00:16:45.920 --> 00:16:46.720
But yeah, awesome.

00:16:46.720 --> 00:16:47.340
Cool.

00:16:47.340 --> 00:16:47.620
All right.

00:16:47.620 --> 00:16:49.000
So let's do this.

00:16:49.000 --> 00:16:53.620
I think probably the best way to get started is we could talk through an example.

00:16:53.620 --> 00:16:59.380
And there's a really nice article by Itmar Turner-Trowing, who's been on the show a couple

00:16:59.380 --> 00:17:04.360
of times, called Faster, More Memory-Efficient Python, JSON parsing with msgspec.

00:17:04.360 --> 00:17:06.680
And just as a couple of examples that I thought maybe we could throw up.

00:17:06.720 --> 00:17:11.460
And you could talk to, speak to your thoughts, like, why does the API work this way?

00:17:11.460 --> 00:17:13.020
Here's the advantages and so on.

00:17:13.020 --> 00:17:13.140
Yeah.

00:17:13.140 --> 00:17:17.880
So there's this big, I believe this is the GitHub API, just returning these giant blobs of stuff

00:17:17.880 --> 00:17:18.600
about users.

00:17:18.600 --> 00:17:19.340
Okay.

00:17:19.660 --> 00:17:24.400
And it says, well, if we want to find out what users follow what repos or how many,

00:17:24.400 --> 00:17:27.180
given a user, how many repos do they follow, right?

00:17:27.180 --> 00:17:31.820
We could just say, with open, read this, and then just do a JSON load.

00:17:31.820 --> 00:17:34.400
And then do the standard dictionary stuff, right?

00:17:34.400 --> 00:17:38.780
Like, for everything, we're going to go to the element that we got out and say, bracket some

00:17:38.780 --> 00:17:40.440
key, bracket some key.

00:17:40.660 --> 00:17:44.660
You know, it looks like key not found errors are just lurking in here all over the place.

00:17:44.660 --> 00:17:48.060
But, you know, it's, you should know that maybe it'll work, right?

00:17:48.060 --> 00:17:49.220
If you know the API, I guess.

00:17:49.220 --> 00:17:51.020
So it's like, this is the standard way.

00:17:51.020 --> 00:17:52.320
How much memory does this use?

00:17:52.320 --> 00:17:53.700
How much time does it take?

00:17:53.700 --> 00:17:57.260
Look, we can basically swap out ORJSON.

00:17:57.620 --> 00:17:59.240
I'm not super familiar with ORJSON.

00:17:59.240 --> 00:17:59.940
Are you?

00:17:59.940 --> 00:18:00.440
Yeah.

00:18:00.440 --> 00:18:05.940
ORJSON is compatible-ish with the standard lib JSON, except that it returns bytes rather

00:18:05.940 --> 00:18:06.520
than strings.

00:18:06.520 --> 00:18:06.940
Got it.

00:18:06.940 --> 00:18:07.200
Okay.

00:18:07.200 --> 00:18:10.380
There's also iJSON, I believe, which makes it streaming.

00:18:10.380 --> 00:18:11.500
So there's that.

00:18:11.500 --> 00:18:15.720
And then it says, okay, well, how would this look if we're going to use message spec?

00:18:15.720 --> 00:18:19.140
And in his example, he's using structured data.

00:18:19.140 --> 00:18:23.700
So the structs, this is like the Pydantic version, but it doesn't have to be this way,

00:18:23.700 --> 00:18:25.280
but it is this way, right?

00:18:25.280 --> 00:18:25.900
This is the one he chose.

00:18:26.280 --> 00:18:30.220
So maybe just talk us through, like, how would you solve this problem using message

00:18:30.220 --> 00:18:31.340
spec and classes?

00:18:31.340 --> 00:18:31.860
Yeah.

00:18:31.860 --> 00:18:37.340
So as he's done here in this blog post, he's defined a couple struct types for the various

00:18:37.340 --> 00:18:38.720
levels of this message.

00:18:38.720 --> 00:18:45.360
So repos, actors, and interactions, and then parses the message directly into those types.

00:18:45.360 --> 00:18:51.460
So the final call there is passing in the red message and then specifying the type as a list

00:18:51.460 --> 00:18:54.460
of interactions, which are tree down into actors and repos.

00:18:54.460 --> 00:18:54.880
Exactly.

00:18:55.000 --> 00:18:58.300
So this is what you mentioned earlier about having more function-based.

00:18:58.300 --> 00:19:04.940
So you just say decode, give it the string or the bytes, and you say type equals list of

00:19:04.940 --> 00:19:06.580
bracket, top-level class.

00:19:06.580 --> 00:19:09.200
And just like Pydantic, these can be nested.

00:19:09.200 --> 00:19:10.920
So there's an interaction, which has an actor.

00:19:10.920 --> 00:19:13.660
There's an actor class, which has a login, which has a type.

00:19:13.780 --> 00:19:19.040
So your Pydantic model for how those kind of fit together is pretty straightforward, right?

00:19:19.040 --> 00:19:19.720
Pretty similar.

00:19:19.720 --> 00:19:20.220
Yeah.

00:19:20.220 --> 00:19:21.600
And then you're just programming with classes.

00:19:21.600 --> 00:19:22.200
Awesome.

00:19:22.200 --> 00:19:22.420
Yep.

00:19:22.420 --> 00:19:26.280
And it'll all work well with, like, mypy or PyRite or whatever you're using if you're doing

00:19:26.280 --> 00:19:27.140
static analysis tools.

00:19:27.140 --> 00:19:27.520
Yeah.

00:19:27.520 --> 00:19:32.640
So you've thought about making sure that not just does it work well from a usability perspective,

00:19:32.640 --> 00:19:34.820
but it, like, the type checkers don't go crazy.

00:19:35.120 --> 00:19:35.480
Yeah.

00:19:35.480 --> 00:19:38.800
And any, you know, editor integration you have should just work.

00:19:38.800 --> 00:19:39.120
Nice.

00:19:39.120 --> 00:19:45.120
Because there's sometimes, oh gosh, I think maybe FastAPIs changes, but you'll have things

00:19:45.120 --> 00:19:50.360
like you would say the type of an argument being passed in, if it's, say, coming off the

00:19:50.360 --> 00:19:52.260
query string, you would say it's depend.

00:19:52.260 --> 00:19:56.160
It's a type depends, not an int, for example.

00:19:56.160 --> 00:19:58.620
It's because it's being pulled out of the query string.

00:19:58.740 --> 00:19:59.700
I think that's FastAPI.

00:19:59.700 --> 00:20:04.780
And while it makes the runtime happy and the runtime says, oh, I see you want to get this

00:20:04.780 --> 00:20:09.060
int from the query string, the type checkers and stuff are like, depends.

00:20:09.060 --> 00:20:09.900
What is this?

00:20:09.900 --> 00:20:10.720
Like, this is an int.

00:20:10.720 --> 00:20:12.540
Why are you trying to use this depends as an int?

00:20:12.540 --> 00:20:13.400
This doesn't make any sense.

00:20:13.400 --> 00:20:17.560
I think it's a bit of a challenge to have the runtime, the types drive the runtime, but

00:20:17.560 --> 00:20:19.540
still not freak it out, you know?

00:20:19.540 --> 00:20:19.760
Yeah.

00:20:19.760 --> 00:20:25.700
I think that the Python typing ecosystem, especially with the recent changes in new versions and

00:20:25.700 --> 00:20:30.540
the annotated wrapper, are moving towards a system where these kinds of APIs can be spelled

00:20:30.540 --> 00:20:33.600
natively in ways that the type checkers will understand.

00:20:33.600 --> 00:20:38.460
But if your project that existed before these changes, you obviously had some preexisting

00:20:38.460 --> 00:20:41.380
way to make those work that might not play as nicely.

00:20:41.380 --> 00:20:43.320
So there's the upgrade cost of the project.

00:20:43.320 --> 00:20:49.020
I'm not envious of the work that Samuel Covenant team have had to do to upgrade Pydantic to erase

00:20:49.020 --> 00:20:50.680
some old warts in the API that they found.

00:20:50.680 --> 00:20:52.840
It's nice to see what they've done and it's impressive.

00:20:52.840 --> 00:20:56.780
But it's, I have the benefit of starting this project after those changes in typing

00:20:56.780 --> 00:20:57.680
ecosystem existed.

00:20:57.680 --> 00:21:01.040
You know, can look at hindsight mistakes others have made and learn from them.

00:21:01.040 --> 00:21:01.980
Yeah, that's really excellent.

00:21:01.980 --> 00:21:06.060
They have done, like I said, I'm a big fan of Pydantic and it took them almost a year.

00:21:06.060 --> 00:21:09.760
I interviewed Samuel about that change and it was no joke.

00:21:09.760 --> 00:21:10.820
You know, it was a lot of work.

00:21:10.820 --> 00:21:15.480
But, you know, what they came up with, pretty compatible, pretty much feels like the same

00:21:15.480 --> 00:21:15.920
Pydantic.

00:21:15.920 --> 00:21:18.740
But, you know, if you peel back the covers, it's definitely not.

00:21:18.740 --> 00:21:19.260
All right.

00:21:19.260 --> 00:21:23.480
So the other interesting thing about Ipmar's article here is the performance side.

00:21:23.480 --> 00:21:23.980
So it's okay.

00:21:23.980 --> 00:21:28.440
Do you get fixed memory usage or does it vary based on the size of the data?

00:21:28.440 --> 00:21:29.800
And do you get schema validation?

00:21:29.800 --> 00:21:30.420
Right.

00:21:30.420 --> 00:21:35.060
So standard lib, just straight JSON module, 420 milliseconds.

00:21:35.060 --> 00:21:39.860
OR JSON, the fast one, a little less than twice as fast, 280 milliseconds.

00:21:39.860 --> 00:21:42.720
iJSON for iterable JSON.

00:21:42.720 --> 00:21:45.640
300, so a little more than the fast one.

00:21:45.960 --> 00:21:47.760
Message spec, 90 milliseconds.

00:21:47.760 --> 00:21:48.640
That's awesome.

00:21:48.640 --> 00:21:51.420
That's like three times as fast as the better one.

00:21:51.420 --> 00:21:54.160
Over four times as fast as the built-in one.

00:21:54.160 --> 00:21:56.460
It also is doing, you know, quote unquote, more work.

00:21:56.460 --> 00:21:58.900
It's validating the response as it comes in.

00:21:58.900 --> 00:21:59.400
Exactly.

00:21:59.400 --> 00:22:01.140
So you're sure that it's correct then too.

00:22:01.140 --> 00:22:04.480
All those other ones are just giving you dictionaries and YOLO.

00:22:04.480 --> 00:22:06.080
Do what you want with them, right?

00:22:06.080 --> 00:22:09.000
But here you're actually, all those types that you described, right?

00:22:09.000 --> 00:22:13.240
The interaction and the actors and the repos and the class structure, that's all validation.

00:22:13.520 --> 00:22:18.760
So in on top of that, you've created classes which are heavier weight than dictionaries because

00:22:18.760 --> 00:22:23.900
general classes are heavier weight than dictionaries because they have the dunder dict that has

00:22:23.900 --> 00:22:26.280
all the fields in there effectively anyway, right?

00:22:26.280 --> 00:22:28.260
That's not true for structs.

00:22:28.260 --> 00:22:29.540
Structs are slot classes.

00:22:29.540 --> 00:22:30.140
Yes.

00:22:30.140 --> 00:22:30.820
Structs.

00:22:30.820 --> 00:22:34.180
They are a lighter weight to allocate than a dictionary or a standard class.

00:22:34.180 --> 00:22:35.220
That's one of the reasons they're faster.

00:22:35.220 --> 00:22:35.560
Yeah.

00:22:35.560 --> 00:22:36.520
Structs are awesome.

00:22:36.520 --> 00:22:39.980
And so the other thing I was going to point out is, you know, you've got 40 megabytes

00:22:39.980 --> 00:22:41.760
of memory usage versus 130.

00:22:41.760 --> 00:22:45.120
So almost four times less than the standard module.

00:22:45.120 --> 00:22:50.160
And the only thing that beats you is the iterative one because it literally only has one in memory

00:22:50.160 --> 00:22:50.940
at a time, right?

00:22:50.940 --> 00:22:51.640
One element.

00:22:51.640 --> 00:22:52.020
Yeah.

00:22:52.180 --> 00:22:56.140
So this benchmark is kind of hiding two things together.

00:22:56.140 --> 00:22:59.180
So there is the output, what you're parsing.

00:22:59.180 --> 00:23:03.240
Everything here except for iJSON is going to parse the full input into something.

00:23:03.240 --> 00:23:04.540
One big batch.

00:23:04.540 --> 00:23:08.300
It's more efficient than orJSON or the standard lib in this respect because we're only extracting

00:23:08.300 --> 00:23:09.200
the fields we care about.

00:23:09.200 --> 00:23:11.840
But you're still going to end up with a list of a bunch of objects.

00:23:11.840 --> 00:23:14.780
iJSON is only going to pull one in a memory at a time.

00:23:14.780 --> 00:23:16.680
So it's going to have less in memory there.

00:23:16.680 --> 00:23:21.020
And then you have the memory usage of the parsers themselves, which can also vary.

00:23:21.020 --> 00:23:26.880
So orJSON's memory usage in its parser is a lot higher than message specs, regardless

00:23:26.880 --> 00:23:27.900
of the output size.

00:23:27.900 --> 00:23:29.840
There's a little more internal state.

00:23:29.840 --> 00:23:33.280
So this is a pretty interesting distinction that you're calling out here.

00:23:33.800 --> 00:23:39.200
So for example, if people check out this article, which I'll link, there's like tons of stuff

00:23:39.200 --> 00:23:45.340
that people don't care about in the JSON, like the avatar URL, the gravatar ID, you know,

00:23:45.340 --> 00:23:49.080
the reference type, whether it's a brand, like this stuff that you just don't care about,

00:23:49.080 --> 00:23:49.300
right?

00:23:49.300 --> 00:23:51.160
But to parse it in, you got to read that.

00:23:51.160 --> 00:23:56.940
But what's pretty cool you're saying is like, in this case, the class that Edmar came up with

00:23:56.940 --> 00:23:59.140
is just repo driving from struct.

00:23:59.140 --> 00:23:59.980
It just has a name.

00:23:59.980 --> 00:24:02.560
There's a bunch of other stuff in there, but you don't care about it.

00:24:02.620 --> 00:24:06.680
And so what you're saying is like, if you say that that's the decoder, it looks at that

00:24:06.680 --> 00:24:07.880
and goes, there's a bunch of stuff here.

00:24:07.880 --> 00:24:09.040
We're not loading that.

00:24:09.040 --> 00:24:13.360
We're just going to look for the things you've explicitly asked us to model, right?

00:24:13.360 --> 00:24:16.160
There's no sense in doing the work if you're never going to look at it.

00:24:16.160 --> 00:24:18.240
A lot of different serialization frameworks.

00:24:18.500 --> 00:24:23.700
I can't remember how Pydantic responds when you do this, but, you know, the comments beyond

00:24:23.700 --> 00:24:27.860
Pydantic, so it doesn't really matter, is they'll freak out to say, oh, there's extra

00:24:27.860 --> 00:24:28.400
stuff here.

00:24:28.400 --> 00:24:32.800
What am I supposed, you know, for example, this repo, it just has name, but in the data model,

00:24:32.800 --> 00:24:35.660
it has way more in the JSON data.

00:24:35.660 --> 00:24:37.120
So you try to deserialize it.

00:24:37.120 --> 00:24:39.000
It'll go, well, I don't have room to put all this other stuff.

00:24:39.000 --> 00:24:40.420
Things are, you know, freak out.

00:24:40.500 --> 00:24:43.080
And this one is just like, no, we're just going to filter down to what you asked for.

00:24:43.080 --> 00:24:45.420
I really, it's nice in a couple of ways.

00:24:45.420 --> 00:24:47.860
It's nice from performance, nice from clean code.

00:24:47.860 --> 00:24:51.720
I don't have to put all those other fields I don't care about, but also from, you talked

00:24:51.720 --> 00:24:53.680
about the evolution friendliness, right?

00:24:53.680 --> 00:24:59.100
Because what's way more common is that things get added rather than taken away or changed.

00:24:59.100 --> 00:25:01.180
It's like, well, the complexity grows.

00:25:01.180 --> 00:25:06.220
Now repos also have this, you know, related repos or sub repos or whatever the heck they

00:25:06.220 --> 00:25:06.660
have, right?

00:25:06.660 --> 00:25:10.200
And this model here will just let you go, whatever, don't care.

00:25:10.400 --> 00:25:10.760
Yeah.

00:25:10.760 --> 00:25:14.760
If GitHub updates their API and adds new fields, you're not going to get an error.

00:25:14.760 --> 00:25:19.380
And if they remove a field, you should get a nice error that says expected, you know,

00:25:19.380 --> 00:25:20.820
field name, and now it's missing.

00:25:20.820 --> 00:25:24.300
You can track that down a lot easier than a random key error.

00:25:24.300 --> 00:25:24.740
I agree.

00:25:24.740 --> 00:25:28.560
I think, okay, let's, let's dive into the struct a little bit because that's where we're

00:25:28.560 --> 00:25:29.380
kind of on that now.

00:25:29.380 --> 00:25:32.140
And I think this is one of the highlights of what you built.

00:25:32.140 --> 00:25:36.780
Again, it's kind of the same mental model as people are familiar with some data classes

00:25:36.780 --> 00:25:38.940
with Pydantic and adders and so on.

00:25:39.300 --> 00:25:43.920
So when I saw your numbers, I won't come back and talk about benchmarks with numbers.

00:25:43.920 --> 00:25:45.900
But I just thought like, wow, this is fast.

00:25:45.900 --> 00:25:49.320
And while the memory usage is low, you must be doing something native.

00:25:49.320 --> 00:25:51.420
You must be doing something crazy in here.

00:25:51.420 --> 00:25:53.540
That's not just Dunder slots.

00:25:53.540 --> 00:25:54.960
While Dunder slots is awesome.

00:25:55.660 --> 00:25:57.200
There's more to it than that, right?

00:25:57.200 --> 00:26:01.000
And so they're written in C, quite speedy and lightweight.

00:26:01.540 --> 00:26:04.640
So measurably faster than data classes, adders and Pydantic.

00:26:04.640 --> 00:26:06.180
Like, tell us about these classes.

00:26:06.180 --> 00:26:07.720
Like, this is, this is pretty interesting.

00:26:07.720 --> 00:26:08.520
It's mentioned earlier.

00:26:08.520 --> 00:26:11.100
They're not exactly, but they're, they're basically slots classes.

00:26:11.100 --> 00:26:16.600
So Python's data model, actually CPython's data model is either a class is a standard class

00:26:16.600 --> 00:26:19.360
where it stores its attributes in a dict.

00:26:19.580 --> 00:26:20.900
That's not exactly true.

00:26:20.900 --> 00:26:25.160
There's been some optimizations where the keys are stored separately alongside the class structure

00:26:25.160 --> 00:26:27.220
and all the values are stored on the object instances.

00:26:27.220 --> 00:26:32.520
But in model, there's dict classes and there's slots classes, which you pre-declare your attributes

00:26:32.520 --> 00:26:35.580
to be in this, this Dunder slots interval.

00:26:35.800 --> 00:26:40.240
And those get stored in line in the same allocation as the object instance.

00:26:40.240 --> 00:26:41.500
There's no pointer chasing.

00:26:41.500 --> 00:26:45.780
What that means is that you can't set extra attributes on them that weren't pre-declared,

00:26:45.780 --> 00:26:48.120
but also things are a little bit more efficient.

00:26:48.120 --> 00:26:51.540
We create those automatically when you subclass from a struct type.

00:26:51.540 --> 00:26:55.700
And we do a bunch of other interesting things that are stored on the type.

00:26:55.700 --> 00:26:58.480
That is why we had to write a metaclass in C.

00:26:58.480 --> 00:26:59.320
I went to read it.

00:26:59.320 --> 00:27:00.100
I'm like, whoa, okay.

00:27:00.100 --> 00:27:01.680
Maybe we'll come back to this.

00:27:01.680 --> 00:27:03.420
There's a lot of stuff going on in that type.

00:27:03.420 --> 00:27:07.600
One of the problems with this hobby project is that I wrote this for fun

00:27:07.600 --> 00:27:09.460
and a little bit of work related, but mostly fun.

00:27:09.460 --> 00:27:12.660
And it's not the easiest code base for others to step into.

00:27:12.660 --> 00:27:15.360
It fits my mental model, not necessarily everyone's.

00:27:15.360 --> 00:27:15.540
Yeah.

00:27:15.540 --> 00:27:18.780
I can tell you weren't looking for VC funding because you didn't write it in Rust.

00:27:18.780 --> 00:27:22.320
Seems to be the common denominator these days.

00:27:22.320 --> 00:27:22.620
Yeah.

00:27:22.620 --> 00:27:23.260
Why C?

00:27:23.260 --> 00:27:25.980
Just because CPython's already in C?

00:27:25.980 --> 00:27:26.980
And that's the...

00:27:26.980 --> 00:27:27.540
And I use C.

00:27:27.540 --> 00:27:31.560
I do know Rust, but for what I wanted to do in the use case I had in mind,

00:27:31.560 --> 00:27:34.880
I wanted to be able to touch the C API directly.

00:27:34.880 --> 00:27:38.140
And that felt like the easiest way to go about doing it.

00:27:39.740 --> 00:27:47.720
This portion of Talk Python to Me is brought to you by Posit, the makers of Shiny, formerly RStudio, and especially Shiny for Python.

00:27:47.720 --> 00:27:49.640
Let me ask you a question.

00:27:49.640 --> 00:27:51.340
Are you building awesome things?

00:27:51.340 --> 00:27:52.400
Of course you are.

00:27:52.400 --> 00:27:53.960
You're a developer or a data scientist.

00:27:53.960 --> 00:27:54.880
That's what we do.

00:27:54.880 --> 00:27:56.920
And you should check out Posit Connect.

00:27:56.920 --> 00:28:03.860
Posit Connect is a way for you to publish, share, and deploy all the data products that you're building using Python.

00:28:04.600 --> 00:28:07.080
People ask me the same question all the time.

00:28:07.080 --> 00:28:10.240
Michael, I have some cool data science project or notebook that I built.

00:28:10.240 --> 00:28:13.540
How do I share it with my users, stakeholders, teammates?

00:28:13.540 --> 00:28:18.340
Do I need to learn FastAPI or Flask or maybe Vue or React.js?

00:28:18.340 --> 00:28:19.560
Hold on now.

00:28:19.560 --> 00:28:22.480
Those are cool technologies, and I'm sure you'd benefit from them.

00:28:22.480 --> 00:28:24.340
But maybe stay focused on the data project?

00:28:24.340 --> 00:28:26.840
Let Posit Connect handle that side of things.

00:28:27.060 --> 00:28:31.560
With Posit Connect, you can rapidly and securely deploy the things you build in Python.

00:28:31.560 --> 00:28:38.040
Streamlit, Dash, Shiny, Bokeh, FastAPI, Flask, Quattro, Reports, Dashboards, and APIs.

00:28:38.040 --> 00:28:40.320
Posit Connect supports all of them.

00:28:40.320 --> 00:28:46.160
And Posit Connect comes with all the bells and whistles to satisfy IT and other enterprise requirements.

00:28:46.160 --> 00:28:50.540
Make deployment the easiest step in your workflow with Posit Connect.

00:28:50.540 --> 00:28:56.660
For a limited time, you can try Posit Connect for free for three months by going to talkpython.fm.posit.

00:28:57.060 --> 00:28:59.940
That's talkpython.fm.posit.

00:28:59.940 --> 00:29:02.200
The link is in your podcast player show notes.

00:29:02.200 --> 00:29:05.460
Thank you to the team at Posit for supporting Talk Python.

00:29:05.460 --> 00:29:11.120
Okay, so from a consumer of this struct class, I just say class.

00:29:11.120 --> 00:29:12.380
And your example is user.

00:29:12.380 --> 00:29:14.660
You say class, user, parentheses, derived from struct.

00:29:14.660 --> 00:29:16.300
In the field, colon, type.

00:29:16.300 --> 00:29:20.540
So like name, colon, string, groups, colon, set of, str, and so on.

00:29:20.540 --> 00:29:23.600
It looks like standard data classes type of stuff.

00:29:23.800 --> 00:29:27.840
But what you're saying is your meta class goes through and looks at that and says, okay,

00:29:27.840 --> 00:29:31.820
we're going to create a class called user, but it's going to have slots called name, email,

00:29:31.820 --> 00:29:33.560
and groups, among other things, right?

00:29:33.560 --> 00:29:35.120
Like does that magic for us?

00:29:35.120 --> 00:29:35.420
Yeah.

00:29:35.420 --> 00:29:39.640
And then it sets up a bunch of internal data structures that are stored on the type.

00:29:39.640 --> 00:29:40.260
Okay.

00:29:40.260 --> 00:29:43.760
Give me a sense of like, like, what's something, why do you got to put that in there?

00:29:43.760 --> 00:29:44.440
What's in there?

00:29:44.480 --> 00:29:48.760
So the way data classes work, after they do all the type parsing stuff, which we have

00:29:48.760 --> 00:29:54.520
to do too, they then generate some code and eval it to generate each of the methods.

00:29:54.520 --> 00:29:59.020
So when you're importing or when you define a new data class, it generates an init method

00:29:59.020 --> 00:30:01.000
and evals it and then stores it on the instance.

00:30:01.000 --> 00:30:05.140
That means that you have little bits of bytecode floating around for all of your new methods.

00:30:05.600 --> 00:30:10.580
msgspec structs instead, each of the standard methods that the implementation provides, which

00:30:10.580 --> 00:30:16.820
would be, you know, init, wrapper, equality checks, copies, you know, various things are

00:30:16.820 --> 00:30:19.320
single C functions.

00:30:19.320 --> 00:30:25.940
And then the type has some data structures on it that we can use to define those.

00:30:25.940 --> 00:30:29.440
So we have a single init method for all struct types that's used everywhere.

00:30:29.440 --> 00:30:34.100
And as part of the init method, we need to know the fields that are defined on the struct.

00:30:34.100 --> 00:30:38.120
So we have some data stored on there about like the field names, default values, various

00:30:38.120 --> 00:30:38.560
things.

00:30:38.560 --> 00:30:38.960
Nice.

00:30:38.960 --> 00:30:42.440
Because they're written in C rather than, you know, Python bytecode, they could be a lot

00:30:42.440 --> 00:30:43.020
faster.

00:30:43.020 --> 00:30:47.860
And because we're not having to eval a new method every time we define a struct, importing

00:30:47.860 --> 00:30:49.360
structs is a lot faster than data classes.

00:30:49.360 --> 00:30:54.400
Something, I'm not going to guess, I have to look up on my benchmarks, but they are basically

00:30:54.400 --> 00:30:59.500
as efficient to define as a handwritten class where data classes have a bunch of overhead.

00:30:59.500 --> 00:31:02.500
If you've ever written a project that has, you know, a hundred of them, importing

00:31:02.500 --> 00:31:03.140
can slow down.

00:31:03.140 --> 00:31:03.340
Yeah.

00:31:03.340 --> 00:31:03.620
Okay.

00:31:03.820 --> 00:31:06.720
Because you basically are dynamically building them up, right?

00:31:06.720 --> 00:31:06.880
Yeah.

00:31:06.880 --> 00:31:07.740
In data class story.

00:31:07.740 --> 00:31:08.000
Yeah.

00:31:08.000 --> 00:31:10.720
So you've got kind of the data class stuff.

00:31:10.720 --> 00:31:13.540
You got, as you said, dunder net, repper, copy, et cetera.

00:31:13.540 --> 00:31:16.980
But you also have dunder match args for pattern matching.

00:31:16.980 --> 00:31:18.000
That's pretty cool.

00:31:18.000 --> 00:31:21.760
And dunder rich repper for pretty printing support with rich.

00:31:21.760 --> 00:31:22.220
Yeah.

00:31:22.220 --> 00:31:24.940
If you just rich.print, it'll take that, right?

00:31:24.940 --> 00:31:25.720
What happens then?

00:31:25.720 --> 00:31:28.040
It preprints it similar to like how a data class should be rendered.

00:31:28.040 --> 00:31:29.620
Rich is making a pretty big impact.

00:31:29.620 --> 00:31:31.840
So rich is special.

00:31:31.840 --> 00:31:32.740
I enjoy using it.

00:31:32.820 --> 00:31:33.280
This is excellent.

00:31:33.280 --> 00:31:35.060
You've got all the stuff generated.

00:31:35.060 --> 00:31:38.560
So much of it isn't C and super lightweight and fast.

00:31:38.560 --> 00:31:43.640
But from the way we think of it, it's just a Python class, even a little less weird than

00:31:43.640 --> 00:31:44.420
data classes, right?

00:31:44.420 --> 00:31:45.920
Because you don't have to put a decorator on it.

00:31:45.920 --> 00:31:47.860
You just derive from this thing.

00:31:47.860 --> 00:31:48.940
So that's super cool.

00:31:48.940 --> 00:31:49.520
Yeah.

00:31:49.520 --> 00:31:50.300
Super neat.

00:31:50.540 --> 00:31:54.900
The hope was that these would feel familiar enough to users coming from data classes or

00:31:54.900 --> 00:32:00.500
adders or Pydantic or all the various models that learning a new one wouldn't be necessary.

00:32:00.500 --> 00:32:02.080
They're the same.

00:32:02.080 --> 00:32:02.420
Excellent.

00:32:02.420 --> 00:32:06.220
One difference if you're coming from Pydantic is there is no method defined on these by default.

00:32:06.720 --> 00:32:13.000
So you define a struct with fields A, B, and C. Only A, B, and C exist as attributes on that class.

00:32:13.000 --> 00:32:14.980
You don't have to worry about any conflicting names.

00:32:14.980 --> 00:32:15.380
Okay.

00:32:15.380 --> 00:32:22.000
So for example, like the Pydantic ones have, I can't remember, the V1 versus V2.

00:32:22.000 --> 00:32:25.020
It's like, I can't remember, like two dictionary effectively, right?

00:32:25.020 --> 00:32:28.180
Where they'll like dump out the JSON or strings or things like that.

00:32:28.180 --> 00:32:30.320
In V1, there's a method dot JSON.

00:32:30.320 --> 00:32:31.200
Yeah, that's right.

00:32:31.200 --> 00:32:33.080
Which if you have a field name JSON will conflict.

00:32:33.080 --> 00:32:38.280
They are remedying that by adding a model prefix for everything, which I think is a good idea.

00:32:38.280 --> 00:32:40.080
I think that's a good way of handling it.

00:32:40.080 --> 00:32:40.420
Yeah.

00:32:40.420 --> 00:32:40.660
Yeah.

00:32:40.660 --> 00:32:43.220
It's like model underscore JSON or dict or something like that.

00:32:43.220 --> 00:32:43.380
Yeah.

00:32:43.380 --> 00:32:43.820
Cool.

00:32:43.820 --> 00:32:47.800
Yeah, that's one of the few breaking changes they actually, unless you're deep down in

00:32:47.800 --> 00:32:49.920
the guts of Pydantic that you might encounter.

00:32:49.920 --> 00:32:50.420
Yeah.

00:32:50.420 --> 00:32:53.880
You don't have to worry about that stuff because you're more function-based, right?

00:32:53.880 --> 00:32:57.200
You would say decode, or I guess, yeah, decode.

00:32:57.200 --> 00:33:00.280
Here's some data, some JSON or something.

00:33:00.280 --> 00:33:03.720
And then the thing you decode it into would be your user type.

00:33:03.720 --> 00:33:07.620
You'd say type equals user rather than going to the user directly, right?

00:33:07.620 --> 00:33:12.920
Can we put our own properties and methods and stuff on these classes and that'll work all right?

00:33:12.920 --> 00:33:13.180
Yeah.

00:33:13.180 --> 00:33:17.460
To a user, you should think of this as a data class that doesn't use a decorator.

00:33:17.460 --> 00:33:17.760
Okay.

00:33:17.760 --> 00:33:23.400
They should be identical unless you're ever trying to touch the dunder data class fields attribute

00:33:23.400 --> 00:33:24.520
that exists on data classes.

00:33:24.520 --> 00:33:27.400
There should be no runtime differences as far as you can tell.

00:33:27.400 --> 00:33:31.900
And when you're doing the schema validation, it sounds like you're basically embracing the

00:33:31.900 --> 00:33:35.040
optionality of the type system.

00:33:35.040 --> 00:33:36.360
If you say int, it has to be there.

00:33:36.360 --> 00:33:40.260
If you say optional int or int pipe none, it may not be there, right?

00:33:40.380 --> 00:33:40.780
No.

00:33:40.780 --> 00:33:41.940
It's close.

00:33:41.940 --> 00:33:43.780
I'm going to be pedantic here a little bit.

00:33:43.780 --> 00:33:46.800
The optional fields are ones that have default values set.

00:33:46.800 --> 00:33:50.500
So optional bracket int without a default is still a required field.

00:33:50.500 --> 00:33:52.180
It's just one that could be an int or none.

00:33:52.180 --> 00:33:53.920
You'd have to have a literal none passed in.

00:33:53.920 --> 00:33:54.560
Otherwise, we'd error.

00:33:54.560 --> 00:33:57.600
This more matches with how mypy interprets the type system.

00:33:57.600 --> 00:33:57.960
Got it.

00:33:57.960 --> 00:33:58.200
Okay.

00:33:58.520 --> 00:34:02.440
So if I had an optional thing, but it had no value, I'd have to explicitly set it to

00:34:02.440 --> 00:34:02.700
none.

00:34:02.700 --> 00:34:03.060
Yes.

00:34:03.060 --> 00:34:03.900
Or would, yeah.

00:34:03.900 --> 00:34:06.320
Or it'd have to be there in the data every time.

00:34:06.320 --> 00:34:09.140
Like other things, you have default factories, right?

00:34:09.140 --> 00:34:10.400
Passing a function that gets called.

00:34:10.400 --> 00:34:12.620
If it does, I guess if it doesn't exist, right?

00:34:12.620 --> 00:34:15.120
If the data's in there, it's being deserialized, it won't.

00:34:15.120 --> 00:34:15.620
Okay.

00:34:15.620 --> 00:34:16.200
Excellent.

00:34:16.420 --> 00:34:19.560
And I guess your decorator creates the initializer.

00:34:19.560 --> 00:34:24.500
But another thing that I saw that you had was you have this post init, which is really nice.

00:34:24.500 --> 00:34:26.920
Like a way to say like, okay, it's been deserialized.

00:34:26.920 --> 00:34:28.580
Let me try a little further.

00:34:28.580 --> 00:34:29.540
Tell us about this.

00:34:29.540 --> 00:34:29.920
This is cool.

00:34:29.920 --> 00:34:31.240
Yeah, it's coming from data classes.

00:34:31.240 --> 00:34:32.120
They have the same method.

00:34:32.120 --> 00:34:37.940
So if you need to do any extra thing after init, you can use it here rather than trying to override

00:34:37.940 --> 00:34:40.100
the built-in init, which we don't let you do.

00:34:40.100 --> 00:34:40.340
Right.

00:34:40.340 --> 00:34:42.480
Because it has so much magic to do.

00:34:42.480 --> 00:34:43.320
Like let it do it.

00:34:43.320 --> 00:34:45.560
And you don't want to override that anyway.

00:34:45.880 --> 00:34:48.360
You'll have to deal with like passing all the arguments.

00:34:48.360 --> 00:34:51.980
Yeah, it runs Python instead of maybe C, all these things, right?

00:34:51.980 --> 00:34:56.560
So post init would exist if you have more complex constraints, right?

00:34:56.560 --> 00:34:58.340
Currently, that's one reason to use it.

00:34:58.340 --> 00:35:01.940
We currently don't support custom validation functions.

00:35:01.940 --> 00:35:06.080
There's no .validate, decorator, various frameworks, different ways of defining these.

00:35:06.080 --> 00:35:07.900
We have some constraints that are built in.

00:35:07.900 --> 00:35:12.920
You can constraint the number to be greater than some value, but there's no way to specify

00:35:12.920 --> 00:35:14.520
custom constraints currently.

00:35:14.520 --> 00:35:15.400
It's on the roadmap.

00:35:15.820 --> 00:35:16.500
It's a thing we want to add.

00:35:16.500 --> 00:35:18.220
Post init's a way to hack around that.

00:35:18.220 --> 00:35:20.700
So right now, you're looking at the screen.

00:35:20.700 --> 00:35:25.480
You have a post init defined, and you're checking if low is greater than high, raise an error.

00:35:25.480 --> 00:35:29.700
And that'll bubble up through decodes and raise an ice user facing validation error.

00:35:29.700 --> 00:35:32.900
In the long run, we'd like that to be done a little bit more field-based.

00:35:32.900 --> 00:35:34.320
Somewhere to come from other frameworks.

00:35:34.480 --> 00:35:38.680
It is tricky, though, because the validation goes onto one field or the other.

00:35:38.680 --> 00:35:41.980
You don't have composite validators necessarily, right?

00:35:41.980 --> 00:35:47.160
And so there's totally valid values of this low, but whatever it is, it has to be lower than high.

00:35:47.160 --> 00:35:49.360
But how do you express that relationship?

00:35:49.360 --> 00:35:50.460
So I think this is awesome.

00:35:50.820 --> 00:35:59.300
Other areas where it could be interesting is under some circumstances, maybe you've got to compute some field also that's in there that's not set.

00:35:59.300 --> 00:35:59.600
I don't know.

00:35:59.600 --> 00:36:00.920
There's some good options in here.

00:36:00.920 --> 00:36:01.560
I like it a lot.

00:36:01.840 --> 00:36:08.440
Yeah, I guess the errors just come out as just straight out of like something went wrong with under post init rather than field low has this problem.

00:36:08.440 --> 00:36:13.640
It's a little harder to relate an error being raised to a specific field if you raise it in a post init.

00:36:13.640 --> 00:36:13.920
Yeah.

00:36:13.920 --> 00:36:20.480
Also, since you're looking at this, and I'm proud that I got this to work, the errors raised in post init use chained exceptions.

00:36:20.480 --> 00:36:22.740
So you can see a little bit of the cause of where it comes from.

00:36:22.740 --> 00:36:27.760
And getting those to work at the Python C API is completely undocumented and a little tricky to figure out.

00:36:28.100 --> 00:36:34.720
It's a lot of reading how the interpreter does it and making me write, you know, 12 incantations to get them to bubble up right.

00:36:34.720 --> 00:36:38.860
Yeah, I do not envy you working on this struct, this base class.

00:36:38.860 --> 00:36:41.420
But I mean, that's where part of the magic is, right?

00:36:41.420 --> 00:36:49.240
And that's why I wanted to dive into this because I think it behaves like Python classes, but it has these really special features that we don't normally get, right?

00:36:49.240 --> 00:36:53.620
Like low memory usage, high performance, accessing the fields.

00:36:53.620 --> 00:36:57.280
Is that any quicker or is it like standard struct level quick?

00:36:57.480 --> 00:37:00.460
Attribute access and settings should be the same as any other class.

00:37:00.460 --> 00:37:02.000
Things that are faster are init.

00:37:02.000 --> 00:37:04.100
Reper, not that that should matter.

00:37:04.100 --> 00:37:06.160
If you're looking for a high performance reper, that's...

00:37:06.160 --> 00:37:07.320
You're doing it wrong.

00:37:07.320 --> 00:37:08.240
Seems like you're doing something wrong.

00:37:08.240 --> 00:37:12.020
Equality checks, comparisons, so sorting, you know, less than, greater than.

00:37:12.020 --> 00:37:12.960
I think that's it.

00:37:12.960 --> 00:37:14.440
Everything else should be about the same.

00:37:14.440 --> 00:37:17.840
So field ordering, you talked about like evolution over time.

00:37:17.840 --> 00:37:19.280
Does it, does this matter?

00:37:19.280 --> 00:37:22.520
Field ordering is mostly defining how, what happens if you do subclasses and stuff.

00:37:22.520 --> 00:37:27.000
This whole section is, if you're not subclassing, shouldn't hopefully be relevant to you.

00:37:27.000 --> 00:37:30.060
So we match how data class handles things for ordering.

00:37:30.060 --> 00:37:30.300
Okay.

00:37:30.300 --> 00:37:35.400
So I could have my user, but I could have a super user that derives from my user that derives

00:37:35.400 --> 00:37:37.060
from struct and things will still hang together.

00:37:37.060 --> 00:37:41.200
And so figuring out how all the fields order out through that subclassing is what this doc

00:37:41.200 --> 00:37:41.540
is about.

00:37:41.540 --> 00:37:41.820
Yeah.

00:37:42.180 --> 00:37:46.860
Another type typing system thing you can do a lot is have explicitly claimed something

00:37:46.860 --> 00:37:47.940
as a class variable.

00:37:47.940 --> 00:37:53.940
You know, Python is weird about its classes and what makes a variable that's associated

00:37:53.940 --> 00:37:55.820
with a class and or not.

00:37:55.820 --> 00:37:56.060
Right.

00:37:56.060 --> 00:37:59.580
So with these type of classes, you would say like class example colon, and then you have

00:37:59.580 --> 00:38:00.660
x colon int.

00:38:00.660 --> 00:38:01.120
Right.

00:38:01.120 --> 00:38:06.800
And that appears, will appear on the static type, like example.x, but it also imbues each

00:38:06.800 --> 00:38:08.940
object with its own copy of that x.

00:38:08.940 --> 00:38:09.500
Right.

00:38:09.500 --> 00:38:14.060
Which is like a little bit, is it a static thing or part of the type or is it not?

00:38:14.060 --> 00:38:14.680
It's kind of funky.

00:38:14.820 --> 00:38:19.040
But you also can say that explicitly from the typing, you say this is a class variable.

00:38:19.040 --> 00:38:20.240
What happens then?

00:38:20.240 --> 00:38:20.440
Right.

00:38:20.440 --> 00:38:26.400
Like, so standard attributes exist on the instances where a class var exists on the class itself.

00:38:26.400 --> 00:38:31.840
Class fars are accessible on an instance, but the actual data is stored on the class.

00:38:31.840 --> 00:38:33.280
So you don't have an extra copy.

00:38:33.280 --> 00:38:33.780
I see.

00:38:33.780 --> 00:38:37.440
So if there's some kind of singleton type of thing or just one of them.

00:38:37.440 --> 00:38:37.560
Yeah.

00:38:37.560 --> 00:38:37.780
Yeah.

00:38:37.780 --> 00:38:43.180
It has to do with how Python does attribute resolution where it'll check on the instance

00:38:43.180 --> 00:38:46.520
and then on the type and also there's descriptors in there, you know, somewhere.

00:38:46.520 --> 00:38:47.040
Interesting.

00:38:47.040 --> 00:38:47.580
Okay.

00:38:47.580 --> 00:38:53.020
Like other things, I suppose it's pretty straightforward that you take these types and you use them

00:38:53.020 --> 00:38:53.660
to validate them.

00:38:53.660 --> 00:39:00.840
But one of the big differences with msgspec.struct versus pydantic.base model and others is the

00:39:00.840 --> 00:39:02.700
validation doesn't happen all the time.

00:39:02.700 --> 00:39:04.860
It just happens on encode decode.

00:39:04.860 --> 00:39:05.560
Right.

00:39:05.560 --> 00:39:10.960
Like you could call the constructor and pass in bad data or like it just doesn't pay attention.

00:39:10.960 --> 00:39:11.260
Right.

00:39:11.260 --> 00:39:11.540
Yeah.

00:39:11.540 --> 00:39:12.440
Why is it like that?

00:39:12.580 --> 00:39:17.340
So this is one of the reasons I wrote my own thing rather than building off of something

00:39:17.340 --> 00:39:18.340
existing like pydantic.

00:39:18.340 --> 00:39:21.020
Side tangent here, just to add history context here.

00:39:21.020 --> 00:39:22.820
Message spec was started about three years ago.

00:39:22.820 --> 00:39:26.000
The JSON and it kind of fell into its full model about two years ago.

00:39:26.000 --> 00:39:27.840
So this has existed for around two years.

00:39:27.840 --> 00:39:28.100
Yeah.

00:39:28.100 --> 00:39:30.000
We're pre the pydantic rewrite.

00:39:30.000 --> 00:39:36.520
Anyway, the reason I wanted all of this was when you have your own code, where bugs can

00:39:36.520 --> 00:39:36.840
come up.

00:39:36.840 --> 00:39:37.900
Are bugs in your own code?

00:39:37.900 --> 00:39:39.040
I've typed something wrong.

00:39:39.040 --> 00:39:41.620
I've made a mistake and I want that to be checked.

00:39:41.620 --> 00:39:43.360
Or it can be user data that's coming in.

00:39:43.360 --> 00:39:46.000
Or, you know, maybe it's a distributed system and it's still my own code.

00:39:46.000 --> 00:39:47.580
It's just a file or database.

00:39:47.580 --> 00:39:47.860
Yeah.

00:39:47.860 --> 00:39:48.060
Whatever.

00:39:48.060 --> 00:39:48.500
Yeah.

00:39:48.500 --> 00:39:51.040
We have many mechanisms of testing our own code.

00:39:51.040 --> 00:39:52.040
You can write tests.

00:39:52.040 --> 00:39:55.420
You have static analysis tools like mypy, pyright, or checking.

00:39:55.420 --> 00:39:59.460
It's a lot easier for me to validate that a function I wrote is correct.

00:39:59.460 --> 00:39:59.960
Got it.

00:39:59.960 --> 00:40:05.280
There are other tools I believe that we should lean on rather than runtime validation in those

00:40:05.280 --> 00:40:05.700
cases.

00:40:05.700 --> 00:40:10.340
But when we're reading in external data, whether it's coming over the wire, coming from a file,

00:40:10.340 --> 00:40:13.840
coming from user input in some way, we do need to validate because the user could have

00:40:13.840 --> 00:40:16.980
passed us something that doesn't match our constraints.

00:40:17.340 --> 00:40:20.540
As soon as you started trusting user input, you're in for a bad time.

00:40:20.540 --> 00:40:21.900
We don't want to arbitrarily be trusting.

00:40:21.900 --> 00:40:24.640
We do validate on JSON decoding.

00:40:24.640 --> 00:40:25.860
We validate on message pack decoding.

00:40:25.860 --> 00:40:28.700
We also have a couple of functions for doing in-memory conversions.

00:40:28.700 --> 00:40:32.920
So there's msgspec convert, msgspec to built-ins for going the other way.

00:40:33.160 --> 00:40:37.240
So that's for doing conversion of runtime data that you got from someone rather than a specific

00:40:37.240 --> 00:40:37.820
format.

00:40:37.820 --> 00:40:42.000
Yeah, because if you're calling this constructor and passing the wrong data, mypy should check

00:40:42.000 --> 00:40:42.300
that.

00:40:42.300 --> 00:40:44.140
PyCharm should check that.

00:40:44.140 --> 00:40:46.380
Maybe Ruff would catch it.

00:40:46.380 --> 00:40:46.860
I'm not sure.

00:40:46.860 --> 00:40:48.140
But there's a bunch of tools.

00:40:48.140 --> 00:40:49.860
Yeah, Ruff doesn't have a type checker yet.

00:40:49.860 --> 00:40:51.960
TBD on that.

00:40:51.960 --> 00:40:52.440
Yeah, OK.

00:40:52.440 --> 00:40:57.700
Yeah, being able to check these statically, it means that we don't have to pay the cost

00:40:57.700 --> 00:40:59.300
every time we're running, which I don't think we should.

00:40:59.300 --> 00:41:02.480
That's extra runtime performance that we don't need to be spending.

00:41:02.660 --> 00:41:03.180
Yeah, definitely.

00:41:03.180 --> 00:41:04.660
Check it on the boundaries, right?

00:41:04.660 --> 00:41:07.420
Check it where it comes into the system, and then it should be good.

00:41:07.420 --> 00:41:11.900
The other reason I was against adding runtime validation to these structs is I want all types

00:41:11.900 --> 00:41:12.620
to be on equal footing.

00:41:12.620 --> 00:41:17.460
And so if I am creating a list, the list isn't going to be doing any validation because it's

00:41:17.460 --> 00:41:18.560
the Python built-in.

00:41:18.560 --> 00:41:21.300
Same with data classes, same with adders, types, whatever.

00:41:21.300 --> 00:41:26.560
And so only doing a validation when you construct some object type that subclasses from a built-in

00:41:26.560 --> 00:41:31.540
that I've defined, or a type I've defined, doesn't give parity across all types and might

00:41:31.540 --> 00:41:35.460
give a user misconceptions about when something is validated and when they can be sure it's

00:41:35.460 --> 00:41:36.960
correct first when it hasn't.

00:41:36.960 --> 00:41:37.160
Yeah.

00:41:37.160 --> 00:41:38.600
Have you seen bear type?

00:41:38.600 --> 00:41:39.140
I have.

00:41:39.140 --> 00:41:39.480
Yeah.

00:41:39.480 --> 00:41:41.100
Bear type's a pretty interesting option.

00:41:41.100 --> 00:41:46.360
If people really want runtime validation, they could go in and throw bear type onto their

00:41:46.360 --> 00:41:48.000
system and let it do its thing.

00:41:48.000 --> 00:41:51.820
Even if you're not doing it, you should read the docs just for the sheer joy that these

00:41:51.820 --> 00:41:52.320
docs are.

00:41:52.320 --> 00:41:53.540
Oh, they are pretty glorious.

00:41:53.540 --> 00:41:54.660
Yeah, I'll do it.

00:41:54.660 --> 00:41:58.920
It's kind of like burying the lead a little down here, but they described themselves as

00:41:58.920 --> 00:42:04.400
bear type brings Rust and C++ inspired zero-cost abstractions into the lawless world of the

00:42:04.400 --> 00:42:09.600
dynamically typed Python by enforcing type safety at the granular level of functions and methods

00:42:09.600 --> 00:42:14.800
against type hints standardized by the Python community of O order one, non-amortized worst

00:42:14.800 --> 00:42:17.980
case time with negligible constant factors.

00:42:17.980 --> 00:42:18.600
Oh my gosh.

00:42:18.600 --> 00:42:20.040
So much fun, right?

00:42:20.040 --> 00:42:22.220
They're just joking around here, but it's a pretty cool library.

00:42:22.220 --> 00:42:25.260
If you want runtime type checking, it's pretty fast.

00:42:25.260 --> 00:42:25.880
Okay.

00:42:25.880 --> 00:42:26.540
Interesting.

00:42:26.540 --> 00:42:28.000
You talked about the pattern matching.

00:42:28.000 --> 00:42:29.100
I'll come back to that.

00:42:29.100 --> 00:42:30.500
One thing I want to talk about.

00:42:30.500 --> 00:42:31.280
Well, okay.

00:42:31.280 --> 00:42:31.860
Frozen.

00:42:31.860 --> 00:42:32.900
Frozen instances.

00:42:32.900 --> 00:42:34.040
This comes from data classes.

00:42:34.040 --> 00:42:37.840
Without the instances being frozen, the structs are mutable.

00:42:37.980 --> 00:42:42.000
Yeah, I can get one, change its value, serialize it back out, things like that.

00:42:42.000 --> 00:42:42.200
Yep.

00:42:42.200 --> 00:42:44.960
But frozen, I suppose, means what you would expect, right?

00:42:44.960 --> 00:42:45.160
Yeah.

00:42:45.160 --> 00:42:47.040
Frozen has the same meaning as a data class equivalent.

00:42:47.040 --> 00:42:49.060
How deep does frozen go?

00:42:49.060 --> 00:42:51.140
So for example, is it frozen all the way down?

00:42:51.140 --> 00:42:57.080
So in the previous example from Itamar, it had the top level class and then other structs

00:42:57.080 --> 00:42:58.820
that were nested in there.

00:42:58.820 --> 00:43:02.660
If I say the top level is frozen, do the nested ones themselves become frozen?

00:43:02.660 --> 00:43:03.220
No.

00:43:03.220 --> 00:43:04.680
So frozen applies to the type.

00:43:05.040 --> 00:43:10.200
So if you define a type as frozen, that means you can't change values that are set as attributes

00:43:10.200 --> 00:43:10.860
on that type.

00:43:10.860 --> 00:43:13.300
But you can still change things that are inside it.

00:43:13.300 --> 00:43:16.800
So if a frozen class contains a list, you can still append stuff to the list.

00:43:16.800 --> 00:43:20.580
There's no way to get around that except if we were to do some deep, deep, deep magic,

00:43:20.580 --> 00:43:21.420
which we shouldn't.

00:43:21.420 --> 00:43:25.980
It would definitely slow it down if you had to go through and recreate frozen lists every

00:43:25.980 --> 00:43:27.300
time you saw a list and stuff like that.

00:43:27.300 --> 00:43:27.440
Yeah.

00:43:27.760 --> 00:43:27.960
Okay.

00:43:27.960 --> 00:43:30.900
And then there's one about garbage collection in here.

00:43:30.900 --> 00:43:31.520
Yeah.

00:43:31.520 --> 00:43:32.780
Which is pretty interesting.

00:43:32.780 --> 00:43:33.680
There we go.

00:43:33.680 --> 00:43:35.100
Disabling garbage collection.

00:43:35.100 --> 00:43:36.940
This is under the advanced category.

00:43:36.940 --> 00:43:39.340
Warning box around this that tells you not to.

00:43:39.340 --> 00:43:40.160
What could go wrong?

00:43:40.160 --> 00:43:40.460
Come on.

00:43:40.460 --> 00:43:46.220
Part of this was experimenting with the DAS distributed scheduler, which is a unique application,

00:43:46.220 --> 00:43:50.260
I think, for people that are writing web stuff in that all of its data is kept in memory.

00:43:50.260 --> 00:43:52.180
There's no backing database that's external.

00:43:52.180 --> 00:43:58.140
And so it is as fast to respond as the bits of in-memory computation it needs to do before

00:43:58.140 --> 00:43:59.460
it sends out a new task to a worker.

00:43:59.460 --> 00:44:03.540
So in this case, their serialization performance matters.

00:44:03.540 --> 00:44:05.680
But also, it's got a lot of in-memory state.

00:44:05.680 --> 00:44:09.680
It's a dict of types of lots of chaining down.

00:44:09.680 --> 00:44:15.900
The way the CPython garbage collector works is that these large dictionaries could add GC overhead.

00:44:15.900 --> 00:44:19.100
Every time a GC thing happens, it has to scan the entire dictionary.

00:44:19.100 --> 00:44:20.960
Any container thing could contain another.

00:44:20.960 --> 00:44:22.500
And once you do that, there could be a cycle.

00:44:22.500 --> 00:44:27.020
And then for very large graphs, GC pauses could become noticeable.

00:44:27.020 --> 00:44:27.320
Yes.

00:44:27.320 --> 00:44:29.080
This is an experiment and seeing ways around that.

00:44:29.080 --> 00:44:34.740
Because we've done some deep magic with how structs work, we can disable GC for subclasses,

00:44:34.740 --> 00:44:40.380
user-defined types, which CPython does not expose normally and really isn't something you

00:44:40.380 --> 00:44:42.540
probably want to be doing in most cases.

00:44:42.540 --> 00:44:44.520
But if you do, you get a couple benefits.

00:44:44.520 --> 00:44:45.500
The types are smaller.

00:44:45.500 --> 00:44:49.920
Every instance needs to include some extra state for tracking GC.

00:44:49.920 --> 00:44:52.720
I believe on recent builds, it's 16 bytes.

00:44:52.720 --> 00:44:53.500
So it's two pointers.

00:44:53.500 --> 00:44:56.180
So that's, you know, you're saving 16 bytes print.

00:44:56.180 --> 00:44:56.900
That's non-trivial.

00:44:56.900 --> 00:44:57.160
Yeah.

00:44:57.160 --> 00:44:59.240
If you got a huge list of them, that could be a lot.

00:44:59.240 --> 00:44:59.480
Yeah.

00:44:59.480 --> 00:45:02.520
And two, they don't, they're not traced.

00:45:02.520 --> 00:45:07.600
And so if you have a lot of them, that's reducing reduction in tracing overhead every time

00:45:07.600 --> 00:45:08.460
a GC pass happens.

00:45:08.460 --> 00:45:12.040
GC puts more overhead on top of stuff than you would think.

00:45:12.040 --> 00:45:16.760
So I did some crazy GC stuff over at Talk Python and training with my courses.

00:45:16.760 --> 00:45:19.460
You go to slash sitemap.xml.

00:45:19.460 --> 00:45:24.820
I don't know how many entries are in the sitemap, but there are 30,000 lines of sitemap.

00:45:24.820 --> 00:45:29.780
Like many, many, many, many, many, many thousands of URLs up to come back with details.

00:45:29.960 --> 00:45:35.580
Just to generate that page in one request with the default Python settings in Python 3.10,

00:45:35.580 --> 00:45:40.840
I think it was, it was doing 77 garbage collections while generating this page.

00:45:40.840 --> 00:45:43.020
That's not ideal.

00:45:43.020 --> 00:45:46.960
I switched it to just change or tweak how frequently the GC runs.

00:45:46.960 --> 00:45:51.580
So like every 70,000, no, every 50,000 allocations instead of every 700.

00:45:51.800 --> 00:45:55.740
And the site runs 20% faster now and uses the same amount of memory, right?

00:45:55.740 --> 00:46:00.400
And so this is not exactly what you're talking about here, but it's in the, it plays in the

00:46:00.400 --> 00:46:05.820
same space as like, you can dramatically change the things that are triggering this and dramatically

00:46:05.820 --> 00:46:08.340
change the performance potentially.

00:46:08.340 --> 00:46:10.780
The caveat is you better not have cycles.

00:46:10.780 --> 00:46:11.120
Yeah.

00:46:11.460 --> 00:46:16.160
So the other thing with these is, as you pointed out, is the indicator of when a GC pass happens

00:46:16.160 --> 00:46:19.060
has to do with how many GC aware types have been allocated.

00:46:19.060 --> 00:46:19.400
Yep.

00:46:19.400 --> 00:46:22.280
And so if your market type is not a GC type, then the counter is an increment.

00:46:22.280 --> 00:46:23.740
You're not paying that cost.

00:46:23.740 --> 00:46:24.000
Right.

00:46:24.000 --> 00:46:26.500
You can allocate all the integers you want all day long.

00:46:26.500 --> 00:46:27.700
It'll never affect the GC.

00:46:27.700 --> 00:46:31.620
But if you start allocating classes, dictionaries, tuples, et cetera, that is like, well, those

00:46:31.620 --> 00:46:32.400
could contain cycles.

00:46:32.400 --> 00:46:35.900
You have 700 more than you've deallocated since last time.

00:46:35.900 --> 00:46:36.840
I'm going to go check it.

00:46:36.840 --> 00:46:40.960
One place this comes up is if you have, say, a really, really large JSON file.

00:46:41.180 --> 00:46:46.640
Because any deserialization is an alien allocation heavy workload, which means that you can have

00:46:46.640 --> 00:46:50.700
a GC pause happen, you know, several times during it because you've allocated, you know,

00:46:50.700 --> 00:46:51.360
that many types.

00:46:51.360 --> 00:46:56.060
Turning up GC for these types lets you avoid those GC pauses, which gives you actual runtime

00:46:56.060 --> 00:46:56.600
benefits.

00:46:56.600 --> 00:47:01.780
A different way of doing this that is less insane is to just disable GC during the decode.

00:47:01.780 --> 00:47:07.620
Do a, you know, GC disable, JSON decode, GC enable, and you only do a GC pass once.

00:47:07.620 --> 00:47:10.960
Especially because JSON as a tree-like structure can never create cycles.

00:47:10.960 --> 00:47:12.700
You're not going to be having an issue there.

00:47:12.700 --> 00:47:15.700
But you're probably allocating a lot of different things that are container types.

00:47:15.700 --> 00:47:20.480
And so it looks to the GC like, oh, this is some really sketchy stuff.

00:47:20.480 --> 00:47:21.800
We better get on the game here.

00:47:21.800 --> 00:47:25.880
But you know, as you said, there's no cycles in JSON.

00:47:25.880 --> 00:47:29.080
So there's a lot of scenarios like that, like database queries.

00:47:29.080 --> 00:47:31.700
You know, I got a thousand records back from a table.

00:47:31.700 --> 00:47:33.440
They're all some kind of container.

00:47:33.640 --> 00:47:36.880
So minimum one GC happens just to read back that data.

00:47:36.880 --> 00:47:38.320
But you know, there's no cycles.

00:47:38.320 --> 00:47:39.620
So why is the GC happening?

00:47:39.620 --> 00:47:41.280
You can kind of control that a little bit.

00:47:41.280 --> 00:47:43.680
Or you just turn the number up to 50,000 like I did.

00:47:43.680 --> 00:47:46.160
It still happens, but less.

00:47:46.160 --> 00:47:46.740
A lot less.

00:47:46.740 --> 00:47:47.420
Yeah.

00:47:47.620 --> 00:47:50.500
So this is pretty interesting, though, that you just set GC equals false.

00:47:50.500 --> 00:47:51.900
Where do you set this?

00:47:51.900 --> 00:47:54.020
Is this like in the derived bit?

00:47:54.020 --> 00:47:56.860
It's part of the class definition.

00:47:56.860 --> 00:48:00.120
So we make use of class definition keyword arguments.

00:48:00.600 --> 00:48:03.180
So it goes after the struct type in the subclass.

00:48:03.180 --> 00:48:08.560
You do, you know, my class, open over at the C, struct, comma, GC equals false, close, comma,

00:48:08.560 --> 00:48:10.040
colon, rest of the class.

00:48:10.040 --> 00:48:11.080
Yeah, that's where I thought.

00:48:11.080 --> 00:48:12.280
But it is a little funky.

00:48:12.280 --> 00:48:13.140
Yeah.

00:48:13.140 --> 00:48:16.100
I mean, it kind of highlights the meta class action going on there, right?

00:48:16.100 --> 00:48:18.860
What else should people know about these structs?

00:48:18.860 --> 00:48:21.620
They're fast and they can be used for not just serialization.

00:48:21.620 --> 00:48:25.240
So if you are just writing a program and you happen to have msgspec on your system,

00:48:25.240 --> 00:48:27.500
it should be faster to use them than data classes.

00:48:27.500 --> 00:48:29.580
Whether that matters is, of course, application dependent.

00:48:30.160 --> 00:48:32.040
But they're like generally a good idea.

00:48:32.040 --> 00:48:35.880
They happen to live in this serialization library, but that's just because that's where I wrote

00:48:35.880 --> 00:48:36.100
them.

00:48:36.100 --> 00:48:37.080
Yeah, that's where they.

00:48:37.080 --> 00:48:39.620
In a future world, we might split them out into a sub package.

00:48:39.620 --> 00:48:40.020
Yeah.

00:48:40.020 --> 00:48:41.200
Fast struck.

00:48:41.200 --> 00:48:42.400
Pippin saw fast struck.

00:48:42.400 --> 00:48:43.340
Who knows?

00:48:43.340 --> 00:48:45.000
Yet to be named.

00:48:45.000 --> 00:48:47.000
So better than data classes.

00:48:47.000 --> 00:48:48.780
I mean, they have the capabilities of data classes.

00:48:48.780 --> 00:48:49.360
So that's cool.

00:48:49.360 --> 00:48:54.640
But better than straight up regular classes, like Bayer classes, you know, class colon name.

00:48:54.640 --> 00:48:56.380
Are opinionated a little bit.

00:48:56.380 --> 00:48:59.200
They're how I think people probably should be writing classes.

00:48:59.600 --> 00:49:00.660
And they're opinionated in a bit.

00:49:00.660 --> 00:49:03.180
That means that you can't write them in ways that I don't want you to.

00:49:03.180 --> 00:49:07.840
So the way a struct works is you define attributes on it using type annotations.

00:49:07.840 --> 00:49:09.860
And we generate a fast init method for you.

00:49:09.860 --> 00:49:11.580
We don't let you write your own init.

00:49:11.580 --> 00:49:13.360
In the subclass, you can't override init.

00:49:13.360 --> 00:49:15.000
The generated one is the one you get.

00:49:15.000 --> 00:49:19.920
That means that like if you're trying to create an instance from something that isn't those field

00:49:19.920 --> 00:49:21.520
names, you can't do that.

00:49:21.520 --> 00:49:23.520
You need to use a new class method for writing those.

00:49:23.780 --> 00:49:27.400
I believe this is how people, at least on projects I work on, generally use classes.

00:49:27.400 --> 00:49:29.660
So I think it's a fine limitation.

00:49:29.660 --> 00:49:35.120
But it is putting some guardrails around how the arbitrariness of how you can define a Python

00:49:35.120 --> 00:49:35.520
class.

00:49:35.520 --> 00:49:40.500
You could have a, you know, a handwritten class that has two attributes, X and Y, and your

00:49:40.500 --> 00:49:43.140
init takes, you know, parameters A and B.

00:49:43.140 --> 00:49:43.460
Sure.

00:49:43.460 --> 00:49:48.380
Or maybe it just takes X and it always defaults Y unless you go and change it after or whatever,

00:49:48.380 --> 00:49:48.640
right?

00:49:48.880 --> 00:49:51.500
I guess you could do sort of do that with default values, right?

00:49:51.500 --> 00:49:53.580
But you couldn't prohibit it from being passed in.

00:49:53.580 --> 00:49:55.580
I'm feeling some factory classes.

00:49:55.580 --> 00:50:01.220
The adders docs have a whole, whole page telling people about why this pattern is, is better

00:50:01.220 --> 00:50:02.360
and nudging them to do this.

00:50:02.360 --> 00:50:03.540
So this isn't a new idea.

00:50:03.540 --> 00:50:03.780
Yeah.

00:50:03.780 --> 00:50:06.540
Go check out adders and see what they're saying as well.

00:50:06.540 --> 00:50:08.920
There's probably a debate in the issues somewhere on GitHub.

00:50:08.920 --> 00:50:10.020
There always is a debate.

00:50:10.020 --> 00:50:10.280
Yeah.

00:50:10.280 --> 00:50:10.800
Let's see.

00:50:10.800 --> 00:50:12.860
Let's go get a bunch of stuff up here I want to talk about.

00:50:12.860 --> 00:50:17.540
I guess really quickly, since there's a lot of like C native platform stuff, right?

00:50:17.640 --> 00:50:21.760
This is available on, you know, pip install msgspec.

00:50:21.760 --> 00:50:22.880
We're getting a wheel.

00:50:22.880 --> 00:50:26.720
It seemed like it worked fine on my M2 MacBook Air.

00:50:26.720 --> 00:50:30.760
Like what are the platforms that I get a wheel that don't have to worry about compiling?

00:50:30.760 --> 00:50:33.480
So we use CI BuildWheel for building everything.

00:50:33.480 --> 00:50:36.380
And I believe I've disabled some of the platforms.

00:50:36.380 --> 00:50:40.800
The ones that are disabled are mostly disabled because CI takes time.

00:50:40.800 --> 00:50:42.220
I need to minimize them.

00:50:42.220 --> 00:50:46.540
But everything common should exist, including Raspberry Pi and various ARM builds.

00:50:46.540 --> 00:50:46.980
Excellent.

00:50:47.300 --> 00:50:47.480
Okay.

00:50:47.480 --> 00:50:47.780
Yeah.

00:50:47.780 --> 00:50:49.020
It seemed like it worked just fine.

00:50:49.020 --> 00:50:53.240
I didn't really know that it was like doing a lot of native code stuff, but it seems like

00:50:53.240 --> 00:50:53.380
it.

00:50:53.380 --> 00:50:55.940
And also available on Conda, Conda Forge.

00:50:55.940 --> 00:50:56.560
So that's cool.

00:50:56.560 --> 00:50:59.200
If you Conda, you can also just Conda install it.

00:50:59.200 --> 00:51:02.100
Kind of promised talking about the benchmarks a little bit, didn't I?

00:51:02.100 --> 00:51:04.140
So benchmarks are always...

00:51:04.140 --> 00:51:06.600
If you click on the graph on the bottom, it'll bring you to it.

00:51:06.600 --> 00:51:06.860
Yeah.

00:51:06.860 --> 00:51:10.120
They're always rife with like, that's not my benchmark.

00:51:10.120 --> 00:51:11.780
I'm doing it different, you know?

00:51:11.900 --> 00:51:13.900
But give us a sense of just...

00:51:13.900 --> 00:51:16.200
It says fast, italicies leaning forward.

00:51:16.200 --> 00:51:18.340
Give us a sense of like, where does this land?

00:51:18.340 --> 00:51:21.160
Is it, you know, 20% faster or is it a lot better?

00:51:21.160 --> 00:51:21.400
Yeah.

00:51:21.540 --> 00:51:22.900
So as you said, benchmarks are a problem.

00:51:22.900 --> 00:51:27.340
The top of this benchmark docs is a whole argument against believing them and telling you to run

00:51:27.340 --> 00:51:27.660
your own.

00:51:27.660 --> 00:51:29.420
So take a grain of salt.

00:51:29.420 --> 00:51:33.440
I started benchmarking this mostly just to know how we stacked up.

00:51:33.440 --> 00:51:35.660
It's important if you're making changes to know if you're getting slower.

00:51:35.660 --> 00:51:38.380
It's also important to know what the actual trade-offs of your library are.

00:51:38.500 --> 00:51:40.040
All software engineering is trade-offs.

00:51:40.040 --> 00:51:43.760
So msgspec is generally fast.

00:51:43.760 --> 00:51:50.080
The JSON parser in it is one of the fastest in Python or the fastest, depending on what

00:51:50.080 --> 00:51:52.580
your message structure is and how you're invoking it.

00:51:52.580 --> 00:51:56.940
It at least is on par with or JSON, which is generally what people consider to be the fast

00:51:56.940 --> 00:51:57.280
parser.

00:51:57.280 --> 00:51:57.560
Right.

00:51:57.560 --> 00:51:59.280
That's where they go when they want fast.

00:51:59.280 --> 00:51:59.500
Yeah.

00:51:59.500 --> 00:51:59.940
Yes.

00:51:59.940 --> 00:52:05.160
If you are specifying types, so if you, you know, add in a type annotation to a JSON decode

00:52:05.160 --> 00:52:08.320
call with message spec, even if you're decoding the whole message, you're not doing

00:52:08.320 --> 00:52:08.760
a subset.

00:52:08.760 --> 00:52:10.820
We're about 2x faster than our JSON.

00:52:10.820 --> 00:52:15.760
You actually get a speed up by defining your types because struct types are so efficient

00:52:15.760 --> 00:52:17.200
to allocate versus a deck.

00:52:17.200 --> 00:52:19.600
That's kind of the opposite of what you might expect, right?

00:52:19.600 --> 00:52:23.080
It seems like we're doing more work, but we're actually able to do less because we can

00:52:23.080 --> 00:52:25.000
take some more, you know, efficient fast paths.

00:52:25.000 --> 00:52:29.380
And then a thousand objects with validation compared to.

00:52:29.380 --> 00:52:29.720
Yeah.

00:52:29.720 --> 00:52:34.020
Mesher, Murrow, Seatters, Pydantic, and so on.

00:52:34.020 --> 00:52:34.980
Probably the last one.

00:52:34.980 --> 00:52:38.140
This was a grab bag of various validation libraries that seemed popular.

00:52:38.920 --> 00:52:40.260
Mashemar is the one that DBT uses.

00:52:40.260 --> 00:52:42.600
I think they're the primary consumer of that.

00:52:42.600 --> 00:52:43.820
Catters is for adders.

00:52:43.820 --> 00:52:45.640
Pydantic is, you know, ubiquitous.

00:52:45.640 --> 00:52:50.520
This right here in this benchmark graph we're looking at is against Pydantic V1.

00:52:50.520 --> 00:52:54.500
I have not had a chance to update our benchmarks to go against V2.

00:52:54.500 --> 00:52:57.720
There's a separate gist somewhere that has got some numbers there.

00:52:57.800 --> 00:53:00.140
Standard number they throw out is like 22 times faster.

00:53:00.140 --> 00:53:02.760
So it still puts you multiples faster.

00:53:02.760 --> 00:53:06.620
In that benchmark, we're averaging 10 to 20x faster than Pydantic V2.

00:53:06.620 --> 00:53:10.880
In numbers I run against V1, we're about 80 to 150x faster.

00:53:11.380 --> 00:53:13.880
So it really is structure dependent.

00:53:13.880 --> 00:53:14.560
Yeah, sure.

00:53:14.560 --> 00:53:17.220
Do you have one field or do you have a whole bunch of stuff?

00:53:17.220 --> 00:53:17.920
Yeah, exactly.

00:53:17.920 --> 00:53:19.380
And what types of fields?

00:53:19.380 --> 00:53:24.200
To be getting more into the weeds here, JSON parsing is not easy.

00:53:24.200 --> 00:53:27.620
Message pack parsing is like the format was designed for computers to handle it.

00:53:27.620 --> 00:53:28.640
It's, you know.

00:53:28.720 --> 00:53:30.820
Seven bytes in there is an integer here.

00:53:30.820 --> 00:53:31.680
Yeah, okay.

00:53:31.680 --> 00:53:35.560
Where JSON is human readable and parsing strings into stuff is slow.

00:53:35.560 --> 00:53:35.840
Right.

00:53:35.840 --> 00:53:37.680
The flexibility equals slowness, yeah.

00:53:37.680 --> 00:53:43.560
Our string parsing routines in msgspec are faster than the ones used by orJSON.

00:53:43.560 --> 00:53:46.300
Our integer parsing routines are slower.

00:53:46.300 --> 00:53:47.760
But there's a different trade-off there.

00:53:47.760 --> 00:53:48.120
Interesting.

00:53:48.120 --> 00:53:48.740
Okay.

00:53:48.740 --> 00:53:50.900
Yeah, I think this is, it just seems so neat.

00:53:50.900 --> 00:53:53.120
There's so much flexibility, right, with all the different formats.

00:53:53.120 --> 00:53:56.240
And the restrictions on the class, they exist, but they're unstruck.

00:53:56.240 --> 00:53:58.140
But they're not insane, right?

00:53:58.240 --> 00:54:02.380
I mean, you build proper OOP type of things.

00:54:02.380 --> 00:54:04.840
You don't need super crazy hierarchies.

00:54:04.840 --> 00:54:06.520
Like, that's where you get in trouble with that stuff anyway.

00:54:06.520 --> 00:54:07.620
So don't do it.

00:54:07.620 --> 00:54:09.480
I guess we don't have much time left.

00:54:09.480 --> 00:54:13.740
One thing I think we could talk about a bit maybe would be, I find it, the extensions.

00:54:13.740 --> 00:54:16.880
Just maybe talk about parsing stuff that is kind of unknown.

00:54:16.880 --> 00:54:17.720
This is pretty interesting.

00:54:17.720 --> 00:54:23.760
So the way we allow extension currently, there's an intention to change this and expand it.

00:54:23.760 --> 00:54:28.180
But currently, extending adding new types is done via a number of different hooks.

00:54:28.180 --> 00:54:30.140
They're called when a new type is encountered.

00:54:30.140 --> 00:54:32.320
So custom user defined type of some form.

00:54:32.320 --> 00:54:38.620
I liked doing this rather than adding it into the annotation because if I have a new type, I want it to exist probably everywhere.

00:54:38.620 --> 00:54:45.580
And I don't want to have to keep adding in and use the serializer and deserializer as part of the type annotations.

00:54:45.960 --> 00:54:54.700
So to define a new type that you want to encode, you can add an encode hook, which takes in the instance and returns something that msgspec knows how to handle.

00:54:54.700 --> 00:54:59.060
This is similar to, you know, if you're coming from standard library JSON, there's a default callback.

00:54:59.060 --> 00:54:59.580
It's the same.

00:54:59.580 --> 00:55:02.740
We renamed it to be a little better name in my mind, but it's the same thing.

00:55:02.740 --> 00:55:02.920
Right.

00:55:03.000 --> 00:55:12.360
So your example here is taking a complex number, but storing it as a tuple of real and imaginary numbers, but then pulling it back into a proper complex number object.

00:55:12.360 --> 00:55:13.340
Super straightforward.

00:55:13.340 --> 00:55:13.640
Yeah.

00:55:13.640 --> 00:55:14.880
But makes it possible.

00:55:14.880 --> 00:55:15.200
Yeah.

00:55:15.200 --> 00:55:15.460
Yeah.

00:55:15.460 --> 00:55:16.140
That's really cool.

00:55:16.140 --> 00:55:17.700
So people can apply this.

00:55:17.700 --> 00:55:21.420
And this, I guess, didn't really matter on the output destination, does it?

00:55:21.420 --> 00:55:31.020
Your job here is to take a type that's not serializable to one that is, and then whether that goes to a message pack or JSON or whatever is kind of not your problem.

00:55:31.020 --> 00:55:31.300
Yeah.

00:55:31.300 --> 00:55:32.960
And then the decode hook is the inverse.

00:55:32.960 --> 00:55:39.360
You get a bunch of stuff that is, you know, core types, ints, strings, whatever, and you compose them up into your new custom type.

00:55:39.360 --> 00:55:41.080
Jim, I think we're getting about out of time here.

00:55:41.080 --> 00:55:44.660
But I just want to point out, like, if people hit the user guide, there's a lot of cool stuff here.

00:55:44.660 --> 00:55:50.100
And there's a whole performance tips section that people can check out.

00:55:50.100 --> 00:55:52.260
You know, if we had more time, maybe we'd go into them.

00:55:52.260 --> 00:55:59.760
But, like, for example, you can call msgspec dot JSON dot encode, or you can create an encoder and say the type and stuff and then reuse that.

00:55:59.760 --> 00:55:59.960
Right.

00:55:59.960 --> 00:56:01.340
Those kinds of things.

00:56:01.340 --> 00:56:01.660
Yeah.

00:56:01.660 --> 00:56:07.040
There's another method that is, again, a terrible internal hack for reusing buffers.

00:56:07.040 --> 00:56:09.460
So you don't have to keep allocating byte buffers every message.

00:56:09.600 --> 00:56:12.340
You can allocate a byte array once and use it for everything.

00:56:12.340 --> 00:56:13.080
Save some memory.

00:56:13.080 --> 00:56:14.740
Let me ask, Ellie's got a question.

00:56:14.740 --> 00:56:17.280
I'm going to read some words that don't mean anything to me, but they've made to you.

00:56:17.280 --> 00:56:25.740
How does the performance of message pack plus msgspec with the array-like equals true optimization compared to flat buffers?

00:56:25.740 --> 00:56:31.560
So by default, objects, so struct types, data classes, whatever, encode as objects in the stream.

00:56:31.560 --> 00:56:34.460
So a JSON object has keys and values, right?

00:56:34.460 --> 00:56:38.620
If you have a point with fields X and Y, it's got X and Y, you know, one, two.

00:56:39.040 --> 00:56:42.100
We have an array-like optimization, which lets you drop the field names.

00:56:42.100 --> 00:56:47.080
And so that would instead include as an array of, you know, one comma two, dropping the X and Y.

00:56:47.080 --> 00:56:48.700
Reduces the message size on the wire.

00:56:48.700 --> 00:56:52.760
If the other side knows what the structure is, it can, you know, pull that back up into a type.

00:56:53.020 --> 00:57:02.940
In terms of message pack as a format, plus with the array-like optimization, the output size should be approximately the same as you would expect it to come out of flat buffers.

00:57:03.680 --> 00:57:09.080
The Python flat buffers library is not efficient for creating objects from the binary.

00:57:09.080 --> 00:57:11.320
So it's going to be a lot faster to pull it in.

00:57:11.320 --> 00:57:13.320
Obviously, this is then a very custom format.

00:57:13.320 --> 00:57:15.180
You're doing a weird thing.

00:57:15.180 --> 00:57:18.320
And so compatibility with other ecosystems will be slower.

00:57:18.320 --> 00:57:21.020
Or it's not slower necessarily, but you'll have to write them yourself.

00:57:21.020 --> 00:57:21.380
Yeah.

00:57:21.380 --> 00:57:23.120
Not everything knows how to read the message pack.

00:57:23.120 --> 00:57:24.440
More brittle, potentially.

00:57:24.440 --> 00:57:25.000
Yeah.

00:57:25.000 --> 00:57:25.380
Yes.

00:57:25.380 --> 00:57:25.760
Yeah.

00:57:25.820 --> 00:57:29.280
But for Python, talking to Python, that's probably the fastest way to go between processes.

00:57:29.280 --> 00:57:33.100
And probably a lot faster than JSON or YAML or something like that.

00:57:33.100 --> 00:57:33.520
Okay.

00:57:33.520 --> 00:57:34.180
Excellent.

00:57:34.180 --> 00:57:38.600
I guess, you know, there's many more things to discuss, but we're going to leave it here.

00:57:38.600 --> 00:57:39.880
Thanks for being on the show.

00:57:39.880 --> 00:57:40.720
Final call to action.

00:57:40.720 --> 00:57:43.000
People want to get started with message pack.

00:57:43.000 --> 00:57:45.480
Are you accepting PRs if they want to contribute?

00:57:45.480 --> 00:57:47.180
What do you tell them?

00:57:47.180 --> 00:57:48.900
First, I encourage people to try it out.

00:57:48.900 --> 00:57:51.880
I'm available, you know, to answer questions on GitHub and stuff.

00:57:51.880 --> 00:57:53.540
It is obviously a hobby project.

00:57:53.540 --> 00:57:58.660
So, you know, if the usage bandwidth increases significantly, we'll have to get some more

00:57:58.660 --> 00:58:01.400
maintainers on and hopefully we can make this more maintainable over time.

00:58:01.400 --> 00:58:06.860
But once the sponsor funds exceed $10,000, $20,000, $30,000 a month, like you'll reevaluate

00:58:06.860 --> 00:58:07.360
your...

00:58:07.360 --> 00:58:07.920
No, just kidding.

00:58:07.920 --> 00:58:08.300
Sure.

00:58:08.300 --> 00:58:08.720
Sure.

00:58:08.720 --> 00:58:10.480
But yeah, please try it out.

00:58:10.480 --> 00:58:15.160
Things work, should be hopefully faster than what you're currently using and hopefully intuitive

00:58:15.160 --> 00:58:15.460
to use.

00:58:15.460 --> 00:58:17.360
We've done a lot of work to make sure the API is friendly.

00:58:17.360 --> 00:58:17.640
Yeah.

00:58:17.640 --> 00:58:19.180
It looks pretty easy to get started with.

00:58:19.180 --> 00:58:20.760
The docs are really good.

00:58:20.760 --> 00:58:21.300
No, thank you.

00:58:21.300 --> 00:58:22.340
Congrats on the cool project.

00:58:22.340 --> 00:58:25.220
Thanks for taking the time to come on the show and tell everyone about it.

00:58:25.220 --> 00:58:25.920
Thanks.

00:58:25.920 --> 00:58:26.260
Yeah.

00:58:26.260 --> 00:58:26.700
See you later.

00:58:26.700 --> 00:58:26.880
Bye.

00:58:27.260 --> 00:58:29.820
This has been another episode of Talk Python to Me.

00:58:29.820 --> 00:58:31.640
Thank you to our sponsors.

00:58:31.640 --> 00:58:33.240
Be sure to check out what they're offering.

00:58:33.240 --> 00:58:34.660
It really helps support the show.

00:58:35.540 --> 00:58:39.520
This episode is sponsored by Posit Connect from the makers of Shiny.

00:58:39.520 --> 00:58:44.040
Publish, share, and deploy all of your data projects that you're creating using Python.

00:58:44.040 --> 00:58:50.520
Streamlit, dash, dash, dash, dash, dash, dashboards, dashboards, dashboards, and APIs.

00:58:50.520 --> 00:58:52.900
Posit Connect supports all of them.

00:58:52.900 --> 00:58:57.260
Try Posit Connect for free by going to talkpython.fm/posit.

00:58:57.600 --> 00:58:58.480
P-O-S-T.

00:58:58.480 --> 00:59:00.600
Want to level up your Python?

00:59:00.600 --> 00:59:04.580
We have one of the largest catalogs of Python video courses over at Talk Python.

00:59:04.580 --> 00:59:09.760
Our content ranges from true beginners to deeply advanced topics like memory and async.

00:59:09.760 --> 00:59:12.420
And best of all, there's not a subscription in sight.

00:59:12.420 --> 00:59:15.340
Check it out for yourself at training.talkpython.fm.

00:59:15.620 --> 00:59:17.400
Be sure to subscribe to the show.

00:59:17.400 --> 00:59:20.180
Open your favorite podcast app and search for Python.

00:59:20.180 --> 00:59:21.480
We should be right at the top.

00:59:21.480 --> 00:59:26.660
You can also find the iTunes feed at /itunes, the Google Play feed at /play,

00:59:26.660 --> 00:59:30.840
and the direct RSS feed at /rss on talkpython.fm.

00:59:30.840 --> 00:59:33.800
We're live streaming most of our recordings these days.

00:59:33.800 --> 00:59:37.220
If you want to be part of the show and have your comments featured on the air,

00:59:37.220 --> 00:59:41.640
be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

00:59:41.640 --> 00:59:43.700
This is your host, Michael Kennedy.

00:59:43.700 --> 00:59:45.000
Thanks so much for listening.

00:59:45.000 --> 00:59:46.160
I really appreciate it.

00:59:46.160 --> 00:59:48.080
Now get out there and write some Python code.

00:59:48.080 --> 01:00:18.060
Thank you.

