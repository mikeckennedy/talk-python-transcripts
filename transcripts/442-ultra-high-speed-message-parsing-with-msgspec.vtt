WEBVTT

00:00:00.320 --> 00:00:05.160
If you're a fan of pedantic or data classes, you'll definitely be interested in this episode.


00:00:05.160 --> 00:00:10.480
We are talking about a super fast data modeling and validation framework called msgspec.


00:00:10.480 --> 00:00:15.000
And some of the types in here might even be better for general purpose use than Python's


00:00:15.000 --> 00:00:20.460
native classes. Join me and Jim Chris Hariff to talk about his framework msgspec.


00:00:20.460 --> 00:00:40.020
This is Talk Python to Me, episode 442, recorded November 2nd, 2023.


00:00:40.020 --> 00:00:45.300
Welcome to Talk Python to Me, a weekly podcast on Python. This is your host, Michael Kennedy.


00:00:45.300 --> 00:00:50.420
Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython.


00:00:50.420 --> 00:00:55.900
Both on mastodon.org. Keep up with the show and listen to over seven years of past episodes


00:00:55.900 --> 00:01:01.840
at talkpython.fm. We've started streaming most of our episodes live on YouTube. Subscribe


00:01:01.840 --> 00:01:07.700
to our YouTube channel over at talkpython.fm/youtube to get notified about upcoming shows and be


00:01:07.700 --> 00:01:10.600
part of that episode.


00:01:10.600 --> 00:01:15.300
This episode is sponsored by Posit Connect from the makers of Shiny. Publish, share,


00:01:15.300 --> 00:01:20.460
and deploy all of your data projects that you're creating using Python. Streamlit, Dash,


00:01:20.460 --> 00:01:27.340
Shiny, Bokeh, FastAPI, Flask, Reports, Dashboards, and APIs. Posit Connect supports all of them.


00:01:27.340 --> 00:01:35.660
Try Posit Connect for free by going to talkpython.fm/posit. P-O-S-I-T. And it's brought to you by us over


00:01:35.660 --> 00:01:41.620
at Talk Python Training. Did you know that we have over 250 hours of Python courses?


00:01:41.620 --> 00:01:45.580
Yeah, that's right. Check them out at talkpython.fm/courses. Jim.


00:01:45.580 --> 00:01:46.580
Hello.


00:01:47.580 --> 00:01:51.180
Welcome to Talk Python. I mean, it's awesome to have you here.


00:01:51.180 --> 00:01:52.180
Yeah. Thanks for having me.


00:01:52.180 --> 00:01:56.540
Yeah, of course. I spoke to the Litestar guys, you know, at lightstar.dev and had them


00:01:56.540 --> 00:02:02.060
on the show. And I was talking about their DTOs, different types of objects they can


00:02:02.060 --> 00:02:06.860
pass around in their APIs and their web apps. And like FastAPI, they've got this concept


00:02:06.860 --> 00:02:13.020
where you kind of bind a type, like a class or something to an input to a web API. And


00:02:13.020 --> 00:02:16.620
it does all that sort of magic like FastAPI. And I said, Oh, so you guys probably work


00:02:16.620 --> 00:02:20.820
with PyData. It's like, yes, but let me tell you about msgspec. Because that's where


00:02:20.820 --> 00:02:24.900
the action is. They were so enamored with your project that I just had to reach out


00:02:24.900 --> 00:02:28.260
and have you on. It looks super cool. I think people are going to really enjoy learning


00:02:28.260 --> 00:02:29.260
about it.


00:02:29.260 --> 00:02:31.340
Yeah, thanks. Yeah. It's nice to hear that.


00:02:31.340 --> 00:02:34.660
Yeah. We're going to dive into the details. It's going to be a lot of fun. Before we get


00:02:34.660 --> 00:02:39.100
to them, though, give us just a quick introduction on who you are. So people, you know, people


00:02:39.100 --> 00:02:40.100
don't know you yet.


00:02:40.100 --> 00:02:44.820
So my name's Jim Christreif. I am currently an engineering manager doing actually mostly


00:02:44.820 --> 00:02:50.940
dev work at Voltron Data, working on the IBIS project, which is a completely different conversation


00:02:50.940 --> 00:02:53.820
than what we're going to have today. Prior to that, I've worked on a couple of startups


00:02:53.820 --> 00:03:00.100
and was most of them doing Dask, was the main thing I've contributed to in the past on an


00:03:00.100 --> 00:03:04.980
open source Python front. For those not aware, Dask is a distributed compute ecosystem. I


00:03:04.980 --> 00:03:09.380
come from the PyData side of the Python ecosystem, not the web dev side.


00:03:09.380 --> 00:03:12.900
Nice. Yeah, I've had Matthew Rocklin on a couple of times, but it's been a while, so


00:03:12.900 --> 00:03:18.700
people don't necessarily know, but it's like super distributed pandas, kind of. Grid computing


00:03:18.700 --> 00:03:21.140
for pandas, sort of.


00:03:21.140 --> 00:03:23.100
Or say like Spark written in Python.


00:03:23.100 --> 00:03:27.380
Sure. You know, another thing that's been on kind of on my radar, but I didn't really


00:03:27.380 --> 00:03:31.620
necessarily realize it was associated with you. Tell people just a bit about IBIS. Like


00:03:31.620 --> 00:03:33.220
IBIS is looking pretty interesting.


00:03:33.220 --> 00:03:37.660
IBIS is, I don't want to say the wrong thing. IBIS is a portable data frame library is the


00:03:37.660 --> 00:03:43.260
current tagline we're using. If you're coming from R, it's dplyr for Python. It's more than


00:03:43.260 --> 00:03:48.500
that and it's not exactly that, but that's a quick mental model. So you write data frame


00:03:48.500 --> 00:03:52.740
like code. We're not pandas compatible. We're pandas like enough that you might find something


00:03:52.740 --> 00:03:57.780
extra familiar and it can compile down to, you know, generate SQL for 18 plus different


00:03:57.780 --> 00:04:01.100
database backends. Also like PySpark and a couple other things.


00:04:01.100 --> 00:04:02.100
Okay.


00:04:02.100 --> 00:04:03.380
So you write your code once and you kind of run it on whatever.


00:04:03.380 --> 00:04:08.500
I see. And you do pandas like things, but it converts those into database queries. Is


00:04:08.500 --> 00:04:09.500
that right?


00:04:09.500 --> 00:04:14.020
Yeah. Yeah. So it's a data frame API. It's not pandas compatible, but if you're familiar


00:04:14.020 --> 00:04:17.460
with pandas, you should be able to pick it up. You know, we cleaned up what we thought


00:04:17.460 --> 00:04:19.300
as a bunch of rough edges with the pandas API.


00:04:19.300 --> 00:04:22.300
Yeah. Were those pandas one or pandas two rough edges?


00:04:22.300 --> 00:04:25.340
Both. It's, I don't know. It's pandas like.


00:04:25.340 --> 00:04:30.060
Sure. Yeah. This looks really cool. That's a topic for another day, but awesome. People


00:04:30.060 --> 00:04:36.860
can check that out. But this time you're here to talk about your personal project, msgspec.


00:04:36.860 --> 00:04:40.980
Am I saying that right? Are you saying MSG or msgspec?


00:04:40.980 --> 00:04:41.980
msgspec.


00:04:41.980 --> 00:04:46.660
Right on. I think a lot of these projects sometimes need a little, like, here's the


00:04:46.660 --> 00:04:52.460
MP3 you can press play on, like how it's meant to be said, you know? And sometimes it's kind


00:04:52.460 --> 00:04:58.420
of obvious like PyPI versus PyPi. Other times it's just like, okay, I know you have a really


00:04:58.420 --> 00:04:59.420
clever name.


00:04:59.420 --> 00:05:00.420
People say numpy.


00:05:00.420 --> 00:05:03.740
Yes, I know. People say numpy all the time. I'm like, I don't want to, I try to not correct


00:05:03.740 --> 00:05:07.580
guess cause it's, it's not kind, but I also feel awkward. They will say numpy and I'll


00:05:07.580 --> 00:05:09.820
say, how do you feel about numpy? They're like, numpy's great. I'm like, okay, we're


00:05:09.820 --> 00:05:15.020
just going back and forth like this for the next hour. It's fine. But yeah, it's, it's


00:05:15.020 --> 00:05:19.180
always, I think some of these could use a little, like a little play on. So msgspec,


00:05:19.180 --> 00:05:20.180
tell people about what it is.


00:05:20.180 --> 00:05:24.660
Yeah. So gone through a couple of different taglines. The current one is a fast serialization


00:05:24.660 --> 00:05:30.100
and validation library with a built-in support for JSON, MessagePack, YAML, and TOML. If


00:05:30.100 --> 00:05:34.460
you are familiar with Pydantic, that's probably one of the closest, you know, most popular


00:05:34.460 --> 00:05:38.380
libraries that does a similar thing. You define kind of a structure of your data using type


00:05:38.380 --> 00:05:43.780
annotations and msgspec will parse your data to ensure it is that structure and does


00:05:43.780 --> 00:05:48.820
so efficiently. It's also compatible with a lot of the other serialization libraries.


00:05:48.820 --> 00:05:53.700
You could also use it as a stand in for JSON, you know, with the JSON dumps, JSON loads,


00:05:53.700 --> 00:05:55.380
you don't need to specify the types.


00:05:55.380 --> 00:06:00.860
Right. It's, I think the mental model of kind of like it swims in the same water, the same


00:06:00.860 --> 00:06:07.660
pond as Pydantic, but it's also fairly distinguished from Pydantic, right? As we're going to explore


00:06:07.660 --> 00:06:09.280
throughout our chat here.


00:06:09.280 --> 00:06:14.740
The goal from my side, one of the goals was to replicate more of the experience writing


00:06:14.740 --> 00:06:20.360
Rust or Go with Rust Serde or Go's JSON, where the serializer kind of stands in the background


00:06:20.360 --> 00:06:24.720
rather than my experience working with Pydantic where it felt like the base model kind of


00:06:24.720 --> 00:06:28.560
stood at the foreground. You're defining the models, serialization kind of comes onto the


00:06:28.560 --> 00:06:31.560
types you've defined, but you're not actually working with the serializers on the types


00:06:31.560 --> 00:06:32.560
themselves.


00:06:32.560 --> 00:06:37.560
Got it. So an example, let me see if I, see if I do have it. An example might be if I


00:06:37.560 --> 00:06:42.560
want to take some message I got from some response I got from an API, I want to turn


00:06:42.560 --> 00:06:45.960
it into a Pydantic model or I'm writing an API. I want to take something from a client,


00:06:45.960 --> 00:06:51.240
whatever. I'll go and create a Pydantic class. And then I, the way I use it is I go to that


00:06:51.240 --> 00:06:57.840
class and I'll say star star dictionary I got. And then it comes to life like that,


00:06:57.840 --> 00:07:02.720
right? Where there's a little more focused on just the, the serialization and has this


00:07:02.720 --> 00:07:06.440
capability. But like, like you said, it's optional in the sense.


00:07:06.440 --> 00:07:14.520
Yeah. I, in message spec, all types are on equal footing. So we use functions, not methods,


00:07:14.520 --> 00:07:18.880
because if you want to decode into a list of ints, I can't add a method to a list. You


00:07:18.880 --> 00:07:25.000
know, it's a Python built in type. So you'd say msgspec dot JSON dot decode your


00:07:25.000 --> 00:07:29.640
message. And then you'd specify the type annotation as part of that function call. So it could


00:07:29.640 --> 00:07:31.080
be, you know, list bracket int.


00:07:31.080 --> 00:07:36.360
Right. So you'll say decode. And then you might say type equals list of, of your type


00:07:36.360 --> 00:07:41.200
or like you say, list of int. And that's hard when you have to have a class that knows how


00:07:41.200 --> 00:07:46.920
to basically become what the model, the data passed in is, even if it's just a list, some


00:07:46.920 --> 00:07:51.080
Pydantic classes, you got to kind of jump through some hoops to say, Hey, Pydantic,


00:07:51.080 --> 00:07:55.320
I don't have a thing to give you. I want a list of those things. And that's the top level


00:07:55.320 --> 00:07:59.640
thing is, you know, bracket bracket. It's not, it's not any one thing I can specify


00:07:59.640 --> 00:08:00.640
in Python easily.


00:08:00.640 --> 00:08:06.360
Yeah. To, to be fair to the Pydantic project, I believe in V2, the type adapter object can


00:08:06.360 --> 00:08:09.960
work with that, but that is, you know, it's a, it's a different way of working with it.


00:08:09.960 --> 00:08:12.320
I wanted to have one API that did it all.


00:08:12.320 --> 00:08:17.040
Sure. And it's awesome. They made it. I mean, I want to put this out front, like I'm a massive


00:08:17.040 --> 00:08:21.840
fan of Pydantic. What Samuel's done there is incredible. And it's just, it's really


00:08:21.840 --> 00:08:25.760
made a big difference in the way that people work with data in Python. It's awesome. But


00:08:25.760 --> 00:08:29.680
it's also awesome that you have this project that is an alternative and it makes different


00:08:29.680 --> 00:08:34.320
assumptions and you can see those really play out in like the performance or the APIs. So


00:08:34.320 --> 00:08:39.800
you know, like Pydantic encourages you to take your classes and then send them the data,


00:08:39.800 --> 00:08:43.600
but you've kind of got to know like, oh, there's this type adapter thing that I can give a


00:08:43.600 --> 00:08:48.280
list of my class and then make it work. Right. But it's not just, oh, you just fall into


00:08:48.280 --> 00:08:51.080
that by trying to play with the API, you know?


00:08:51.080 --> 00:08:54.920
Yeah. Yeah. And I think having, being able to specify any type means we, we work with


00:08:54.920 --> 00:08:59.000
standard library data classes, the same as we work with our, our built-in struct type


00:08:59.000 --> 00:09:02.760
or we also work with the adders types. You know, everything is kind of on equal footing.


00:09:02.760 --> 00:09:09.320
Yeah. And I, what I want to really dig into is your custom struct type that has some really


00:09:09.320 --> 00:09:14.600
cool properties, not class properties, but components, features of the class of the type


00:09:14.600 --> 00:09:20.360
there. Yeah. Let's look at a couple of things here. So as you said, it's a fast and I love


00:09:20.360 --> 00:09:25.240
how somehow italicies on the word fast makes it feel even faster. Like it's leaning forward,


00:09:25.240 --> 00:09:30.000
you know, it's leaning into the speed, a faster realization and validation library. The validation


00:09:30.000 --> 00:09:34.600
is kind of can be, but not required, right? The types can be, but they don't have to be.


00:09:34.600 --> 00:09:39.680
So I think that's one of the ways that really differs from Pydantic. But the other is Pydantic


00:09:39.680 --> 00:09:46.160
is quite focused on JSON, whereas this is JSON, MessagePack, YAML, and TOML. Everyone


00:09:46.160 --> 00:09:50.760
knows what JSON is. I always thought of TOML as kind of like, like YAML or are they really


00:09:50.760 --> 00:09:51.760
different?


00:09:51.760 --> 00:09:57.160
It's another configuration focused language. I think people, some people do JSON for config


00:09:57.160 --> 00:10:03.000
files, but I personally don't like to handwrite JSON. YAML and TOML are like more human friendly


00:10:03.000 --> 00:10:09.480
in quotes forms of that. YAML is a superset of JSON. TOML is its own thing. And then MessagePack


00:10:09.480 --> 00:10:12.640
is a binary JSON like file format.


00:10:12.640 --> 00:10:15.940
Yeah. MessagePack. I don't know how many people work with that. Like where would people run


00:10:15.940 --> 00:10:21.440
into MessagePack if they were like say consuming an API or what API framework would people


00:10:21.440 --> 00:10:23.960
be generating MessagePack in Python typically?


00:10:23.960 --> 00:10:28.920
That's a good question. So going back to the creation of this project, actually, msgspec


00:10:28.920 --> 00:10:32.680
sounds a lot like MessagePack. And that was intentional because that's what I wrote it


00:10:32.680 --> 00:10:37.720
for originally. So as I said at the beginning, I'm one of the original contributors to Dask,


00:10:37.720 --> 00:10:43.640
worked on Dask for forever. And the Dask distributed scheduler uses MessagePack for its RP serialization


00:10:43.640 --> 00:10:47.240
layer that kind of fell out of what was available at the time. We benchmarked a bunch of different


00:10:47.240 --> 00:10:52.760
libraries and that was the fastest way to send bytes between nodes in 2015.


00:10:52.760 --> 00:10:53.760
Sure.


00:10:53.760 --> 00:10:59.240
The distributed schedulers RPC framework has kind of grown haphazardly over time. And there


00:10:59.240 --> 00:11:03.000
were a bunch of bugs due to some hacky things we were doing with it. And also it was a slower


00:11:03.000 --> 00:11:07.640
than we would have wanted. So this was an attempt to write a faster MessagePack library


00:11:07.640 --> 00:11:13.240
for Python that also did fancier things, supported more types, did some schema validation because


00:11:13.240 --> 00:11:17.160
like we wanted to catch the worker is sending this data and the scheduler is getting it


00:11:17.160 --> 00:11:23.520
and saying it's wrong. And we wanted to also add in a way to make schema evolution, meaning


00:11:23.520 --> 00:11:28.280
that I can have different versions of my worker and scheduler and client process and things


00:11:28.280 --> 00:11:33.800
kind of work. If I add new features to the scheduler, they don't break the client. You


00:11:33.800 --> 00:11:37.640
know, we have a nice forward and backward compatibility story. And so that's what kind


00:11:37.640 --> 00:11:38.640
of fell out.


00:11:38.640 --> 00:11:41.760
Yeah, it's a really nice feature. We're going to dive into that. But you know, you might


00:11:41.760 --> 00:11:46.280
think, oh, well, just update your client or update the server. But there's all sorts of


00:11:46.280 --> 00:11:52.280
situations that get really weird. Like if you have Redis as a caching layer and you


00:11:52.280 --> 00:11:56.640
create a MessagePack object and stick it in there and then you deploy a new version of


00:11:56.640 --> 00:12:01.480
the app, it could maybe can't deserialize anything in the cache anymore because it says


00:12:01.480 --> 00:12:06.040
something's missing or something's there that it doesn't expect. Right. And so this evolution


00:12:06.040 --> 00:12:09.960
is important there. If you got long running work and you stash it into a database and


00:12:09.960 --> 00:12:13.480
you pull it back out, like all these things where it kind of lives a little outside the


00:12:13.480 --> 00:12:18.460
process, all of a sudden it starts to matter that before you even consider like clients


00:12:18.460 --> 00:12:22.600
that run separate code, right? Like you could be the client, just different places in time.


00:12:22.600 --> 00:12:26.520
Yeah. Yeah. So adding a little bit more structure to how you define messages in a way to make


00:12:26.520 --> 00:12:31.200
the scheduler more maintainable. That work never landed. It's as it is with open source


00:12:31.200 --> 00:12:35.840
projects. It's a democracy and also a duocracy. And you know, you don't always have this can


00:12:35.840 --> 00:12:39.280
be done at that ends. I still think it'll be valuable in the future. But some stuff


00:12:39.280 --> 00:12:43.960
was changing the scheduler and a serialization is no longer the bottleneck that it was two


00:12:43.960 --> 00:12:46.080
and a half years ago when this originally started.


00:12:46.080 --> 00:12:50.680
So let me put this in context for people, maybe make it relevant. Like maybe right now


00:12:50.680 --> 00:12:56.760
someone's got a FastAPI, API, and they're using Pydantic and obviously it generates


00:12:56.760 --> 00:13:03.080
all the awesome JSON they want. Is there a way to, how would you go about creating say


00:13:03.080 --> 00:13:10.240
a Python server-based system set of APIs that maybe as an option take MessagePack or maybe


00:13:10.240 --> 00:13:14.800
use that as a primary way? Like it could be maybe, you know, passing an accept header.


00:13:14.800 --> 00:13:16.000
To take MessagePack?


00:13:16.000 --> 00:13:19.680
If you want to exchange MessagePack client server Python right now, what do you do?


00:13:19.680 --> 00:13:23.880
That's a good question. To be clear, I am not a web dev. I do not do this for a living.


00:13:23.880 --> 00:13:28.640
I think there is no standard application slash MessagePack. I think people can use it if


00:13:28.640 --> 00:13:32.560
they want, but that's not a, it's a standardized thing the same way that JSON is.


00:13:32.560 --> 00:13:33.560
Yeah.


00:13:33.560 --> 00:13:36.520
I think that Litestar as a framework does support this out of the box. I don't know


00:13:36.520 --> 00:13:40.720
about FastAPI. I'm sure there's a way to hack it in as there is with any ASCII server.


00:13:40.720 --> 00:13:45.760
Yeah. Litestar, like I said, I had Litestar on those guys maybe a month ago and yeah,


00:13:45.760 --> 00:13:50.920
it was super, super cool about that. So yeah, I know that they support msgspec and


00:13:50.920 --> 00:13:54.000
a lot of different options there, but you know, you could just, I imagine you could


00:13:54.000 --> 00:14:01.120
just return binary bits between you and your client. I'm thinking of like latency sensitive


00:14:01.120 --> 00:14:06.400
microservice type things sort of within your data center. How can you lower serialization,


00:14:06.400 --> 00:14:10.960
deserialization, serialization, like all that, that cost that could be the max, you know,


00:14:10.960 --> 00:14:15.800
the biggest part of what's making your app spend time and energy. Michael out there says


00:14:15.800 --> 00:14:19.040
would love PyArrow parquet support for large data.


00:14:19.040 --> 00:14:23.680
There's been a request for arrow integration with MessagePack. I'm not exactly sure what


00:14:23.680 --> 00:14:27.420
that would look like. Arrow containers are pretty efficient on their own. Breaking them


00:14:27.420 --> 00:14:31.840
out into a bunch of objects or stuff to work with MessagePack doesn't necessarily make


00:14:31.840 --> 00:14:35.700
sense in my mind. But anyway, if you have ideas on that, please open an issue or comment


00:14:35.700 --> 00:14:36.860
on the existing issue.


00:14:36.860 --> 00:14:41.840
Yeah, indeed. All right. So let's see. Some of the highlights are high performance encoders


00:14:41.840 --> 00:14:46.560
and decoders across those protocols we talked. You have benchmarks. We'll look at them for


00:14:46.560 --> 00:14:51.000
in a minute. You have a really nice, a lot of support for different types that can go


00:14:51.000 --> 00:14:55.860
in there that can be serialized, but there's also a way to extend it to say, I've got a


00:14:55.860 --> 00:15:01.560
custom type that you don't think is serializable to whatever and thing, a MessagePack, JSON,


00:15:01.560 --> 00:15:06.220
whatever. But I can write a little code that'll take it either way, you know, dates or something


00:15:06.220 --> 00:15:10.400
that drive me crazy, but it could be like an object ID out of MongoDB or other things


00:15:10.400 --> 00:15:14.100
that are seem like they should go back and forth, but don't, you know, right. So that's


00:15:14.100 --> 00:15:20.000
really nice. And then zero cost schema validation, right? It validates decodes and validates


00:15:20.000 --> 00:15:25.240
JSON two times as fast as ORJSON, which is one of the high performance JSON decoders.


00:15:25.240 --> 00:15:28.040
And that's just decoding, right? And then the struct thing that we're going to talk


00:15:28.040 --> 00:15:33.840
about, which the struct type is kind of what brings the parody with Pydantic, right?


00:15:33.840 --> 00:15:38.240
Yeah. You could think of it as Pydantic's base model. It's our built in data class like


00:15:38.240 --> 00:15:43.440
type. So structs are data class like, like everything in msgspec are implemented


00:15:43.440 --> 00:15:49.640
fully as a C extension. Getting these to work required reading a lot of the CPython source


00:15:49.640 --> 00:15:53.160
code because we're doing some things that I don't want to say that they're not, they


00:15:53.160 --> 00:15:57.580
don't want you to do. We're not doing them wrong, but they're not really documented.


00:15:57.580 --> 00:16:02.360
So for example, the, the, when you subclass for msgspec or msgspec.struct, that's


00:16:02.360 --> 00:16:07.160
using a meta class mechanism, which is a way of defining types that define types. And the


00:16:07.160 --> 00:16:14.680
meta class is written in C, which CPython doesn't make easy to do. So it's a C class


00:16:14.680 --> 00:16:21.160
that creates new C types. They're pretty speedy. They are 10 to 100X faster for most operations


00:16:21.160 --> 00:16:24.920
than even handwriting a class that does the same thing, but definitely more than data


00:16:24.920 --> 00:16:25.920
classes or adders.


00:16:25.920 --> 00:16:29.280
Yeah. It's super interesting. And I really want to dive into that. Like I almost can


00:16:29.280 --> 00:16:35.320
see the struct type being relevant even outside of msgspec and in general potentially.


00:16:35.320 --> 00:16:40.000
So yeah, we'll see about that, but it's super cool. And Michael also points out like he's


00:16:40.000 --> 00:16:45.560
the one who made the issue. So sorry about that. He's commented already, I suppose in


00:16:45.560 --> 00:16:50.720
a sense, but yeah. Awesome. Cool. All right. So let's do this. I think probably the best


00:16:50.720 --> 00:16:56.520
way to get started is we could talk through an example and there's a really nice article


00:16:56.520 --> 00:17:01.000
by Itmar Turner-Trauring who's been on the show a couple of times called Faster, More


00:17:01.000 --> 00:17:05.480
Memory Efficient Python, JSON Parsing with msgspec. And just has a couple of examples


00:17:05.480 --> 00:17:09.560
that I thought maybe we could throw up and you could talk to, speak to your thoughts


00:17:09.560 --> 00:17:13.600
of like, why is the API work this way? Here's the advantages and so on. Yeah. So there's


00:17:13.600 --> 00:17:17.840
this big, I believe this is the GitHub API, just returning these giant blobs of stuff


00:17:17.840 --> 00:17:23.840
about users. Okay. And it says, well, if we want to find out what users follow what repos


00:17:23.840 --> 00:17:28.480
or how many, given a user, how many repos do they follow? Right. We could just say with


00:17:28.480 --> 00:17:34.280
open, read this and then just do a JSON load and then do the standard dictionary stuff,


00:17:34.280 --> 00:17:37.560
right? Like for everything, we're going to go, go to the element that we got out and


00:17:37.560 --> 00:17:42.760
say bracket some key, bracket some key. You know, it looks like key not found errors are


00:17:42.760 --> 00:17:47.520
just lurking in here all over the place, but you know, it's, you should know that maybe


00:17:47.520 --> 00:17:51.200
it'll work right. If you know the API, I guess. So it was like, this is the standard way.


00:17:51.200 --> 00:17:56.120
How much memory does this use? How much time does it take? Look, we can basically swap


00:17:56.120 --> 00:18:02.640
out or JSON. I'm not super familiar with or JSON. Are you? Yeah. Or JSON is compatible


00:18:02.640 --> 00:18:06.960
ish with the standard lib JSON, except that it returns bytes rather than strengths. Got


00:18:06.960 --> 00:18:12.040
it. Okay. There's also iJSON, I believe, which makes it streaming. So there's that. And then


00:18:12.040 --> 00:18:16.760
it says, okay, well, how would this look if we're going to use msgspec? And in his


00:18:16.760 --> 00:18:22.280
example, he's using structured data. So the structs, this is like the Pydantic version,


00:18:22.280 --> 00:18:26.360
but it doesn't have to be this way, but it is this way, right? This is the one he chose.


00:18:26.360 --> 00:18:30.880
So maybe just talk us through, like, how would you solve this problem using msgspec and


00:18:30.880 --> 00:18:35.480
classes? Yeah. So as he's done here in this blog post, he's defined a couple of struct


00:18:35.480 --> 00:18:43.280
types for the various levels of this message. So repos, actors, and interactions, and then


00:18:43.280 --> 00:18:48.400
parses the message directly into those types. So the final call there is passing in the


00:18:48.400 --> 00:18:53.220
read message and then specifying the type as a list of interactions, which are tree


00:18:53.220 --> 00:18:57.640
down into actors and repos. Exactly. So this is what you mentioned earlier about having


00:18:57.640 --> 00:19:02.920
more function-based. So you just say decode, give it the string or the bytes, and you say


00:19:02.920 --> 00:19:09.320
type equals list of bracket, up-level class. And just like Pydantic, these can be nested.


00:19:09.320 --> 00:19:13.120
So there's an interaction, which has an actor. There's an actor class, which has a login,


00:19:13.120 --> 00:19:18.000
which has a type. So your Pydantic Mendel model for how those kind of fit together is


00:19:18.000 --> 00:19:21.680
pretty straightforward, right? Pretty similar. Yeah. And then you're just programming with


00:19:21.680 --> 00:19:25.040
classes. Awesome. Yep. And it'll all work well with like mypy or PyWrite or whatever


00:19:25.040 --> 00:19:28.960
you're using if you're doing static analysis tools. Yeah. So you've thought about making


00:19:28.960 --> 00:19:33.840
sure that not just does it work well from a usability perspective, but it like the type


00:19:33.840 --> 00:19:38.600
checkers don't go crazy. Yeah. And any, you know, editor integration you have should just


00:19:38.600 --> 00:19:44.600
work. Nice. Because there's sometimes, oh gosh, I think maybe FastAPI has changed this,


00:19:44.600 --> 00:19:49.560
but you'll have things like you would say the type of an argument being passed in, if


00:19:49.560 --> 00:19:54.080
it's say coming off the query string, you would say it's depend. It's a type depends,


00:19:54.080 --> 00:19:58.680
not a, not an int, for example. It's because it's being pulled out of the query string.


00:19:58.680 --> 00:20:03.880
I think that's FastAPI. And while it makes the runtime happy and the runtime says, oh,


00:20:03.880 --> 00:20:08.560
I see you want to get this int from the query string, the type checkers and stuff are like,


00:20:08.560 --> 00:20:12.240
depends. What is this? Like, this is an int. Why are you trying to use this depends as


00:20:12.240 --> 00:20:15.760
an int? This doesn't make any sense. I think it's a bit of a challenge to have the runtime,


00:20:15.760 --> 00:20:20.720
the types drive the runtime, but still not freak it out. You know? Yeah. I think that


00:20:20.720 --> 00:20:25.920
the Python typing ecosystem, especially with the recent changes in new versions and the


00:20:25.920 --> 00:20:31.440
annotated wrapper are moving towards a system where these kinds of APIs can be spelled natively


00:20:31.440 --> 00:20:35.780
in ways that the type checkers will understand. Right. But if you're a product that existed


00:20:35.780 --> 00:20:40.080
before these changes, you obviously had some preexisting way to make those work that might


00:20:40.080 --> 00:20:44.360
not play as nicely. So there's, there's the upgrade cost of the project. I'm not envious


00:20:44.360 --> 00:20:49.160
of the work that Samuel Covenant team have had to do to upgrade Pydantic to erase some


00:20:49.160 --> 00:20:52.520
old warts in the API that they found. It's nice to see what they've done and it's, it's


00:20:52.520 --> 00:20:56.360
impressive, but it's, I have the benefit of starting this project after those changes


00:20:56.360 --> 00:21:00.520
in typing system existed, you know, can look at hindsight mistakes others have made and


00:21:00.520 --> 00:21:01.520
learn from them.


00:21:01.520 --> 00:21:04.720
Yeah, that's really excellent. They have done, like I said, I'm a big fan of Pydantic and


00:21:04.720 --> 00:21:09.760
it took them almost a year. I interviewed Samuel about that change and it was no joke.


00:21:09.760 --> 00:21:13.960
You know, it was a lot of work, but you know what they came up with pretty compatible,


00:21:13.960 --> 00:21:18.000
pretty, pretty much feels like the same Pydantic, but you know, if you peel back the covers,


00:21:18.000 --> 00:21:21.720
it's definitely not. All right. So the other interesting thing about it, Inmar's article


00:21:21.720 --> 00:21:27.440
here is the performance sides is okay. Do you get fixed memory usage or does it vary


00:21:27.440 --> 00:21:32.360
based on the size of the data and do you get schema validation? Right. So standard lib,


00:21:32.360 --> 00:21:38.560
straight JSON module, 420 milliseconds. OR JSON, the fast one, a little less than twice


00:21:38.560 --> 00:21:45.520
as fast, 280 milliseconds. IJSON for iterable JSON, 300. So a little more than the fast


00:21:45.520 --> 00:21:50.920
one. Message spec, 90 milliseconds. That's awesome. That's like three times as fast as


00:21:50.920 --> 00:21:54.440
the better one over four times as fast as the built-in one.


00:21:54.440 --> 00:21:58.340
It also is doing, you know, quote unquote more work. It's a validating the responses


00:21:58.340 --> 00:21:59.340
it comes in.


00:21:59.340 --> 00:22:00.340
Exactly.


00:22:00.340 --> 00:22:01.340
So you're sure that it's correct then too.


00:22:01.340 --> 00:22:05.220
Yeah. And all those other ones are just giving you dictionaries and YOLO, do what you want


00:22:05.220 --> 00:22:09.100
with them. Right. But here you're actually all those types that you described, right?


00:22:09.100 --> 00:22:13.500
The interaction and the actors and the repos and the class structure, that's all validation.


00:22:13.500 --> 00:22:18.340
So, and on top of that, you've created classes, which are heavier weight than dictionaries


00:22:18.340 --> 00:22:23.100
because general classes are heavier weight than dictionaries because they have the dunder


00:22:23.100 --> 00:22:26.580
dict that has all the fields in there effectively anyway. Right?


00:22:26.580 --> 00:22:29.780
That's not true for, for structs. Structs are slot classes.


00:22:29.780 --> 00:22:30.780
Yes. Structs.


00:22:30.780 --> 00:22:34.380
They are lighter weight to allocate than a dictionary or a standard class. That's one


00:22:34.380 --> 00:22:35.380
of the reasons they're faster.


00:22:35.380 --> 00:22:38.900
Yeah. Structs are awesome. And so the other thing I was pointing out is, you know, you've


00:22:38.900 --> 00:22:44.780
got 40 megabytes of memory usage versus 130. So almost four times less than the standard


00:22:44.780 --> 00:22:49.040
module. And the only thing that beats you is the iterative one, because it literally


00:22:49.040 --> 00:22:52.340
only has one in memory at a time. Right. One element. Yeah.


00:22:52.340 --> 00:22:58.660
So, so this benchmark is kind of hiding two things together. So there, there is the output,


00:22:58.660 --> 00:23:02.620
what you're parsing. Everything here except for IJSON is going to parse the full input


00:23:02.620 --> 00:23:03.620
into something.


00:23:03.620 --> 00:23:04.620
One big batch.


00:23:04.620 --> 00:23:07.500
Message spec is more efficient than orJSON or the standard lib in this respect, because


00:23:07.500 --> 00:23:11.060
we're only extracting the fields we care about, but you're still going to end up with a list


00:23:11.060 --> 00:23:15.220
of a bunch of objects. IJSON is only going to pull one into memory at a time. So it's


00:23:15.220 --> 00:23:19.180
going to have less in memory there. And then you have the, the memory usage of the parsers


00:23:19.180 --> 00:23:25.240
themselves, which can also vary. So orJSON's memory or usage in its parser is a lot higher


00:23:25.240 --> 00:23:30.020
than message specs, regardless of the output size. There's a little more internal state.


00:23:30.020 --> 00:23:35.380
So this is a pretty interesting distinction that you're calling out here. So for example,


00:23:35.380 --> 00:23:39.620
if people check out this article, which I'll link, there's like tons of stuff that people


00:23:39.620 --> 00:23:46.180
don't care about in the JSON, like the avatar URL, the gravatar ID, you know, the reference


00:23:46.180 --> 00:23:49.500
type, whether it's a brand, like this stuff that you just don't care about. Right. But


00:23:49.500 --> 00:23:54.020
the parser then you got to read that. But what's pretty cool. You're saying is like,


00:23:54.020 --> 00:23:59.340
in this case, the class that it Mark came up with is just repo driving from struct.


00:23:59.340 --> 00:24:02.700
It just has name. There's a bunch of other stuff in there, but you don't care about it.


00:24:02.700 --> 00:24:06.700
And so what you're saying is like, if you say that that's the decoder, it looks at that


00:24:06.700 --> 00:24:09.840
and goes, there's a bunch of stuff here. We're not loading that. We're just going to look


00:24:09.840 --> 00:24:13.940
for the things you've explicitly asked us to model. Right. That's all.


00:24:13.940 --> 00:24:16.180
There's no sense in doing the work if you're never going to look at it.


00:24:16.180 --> 00:24:21.580
A lot of different serialization frameworks. Can't remember how Pydantic responds when


00:24:21.580 --> 00:24:26.020
you do this, but you know, the comments beyond Pydantic, so it doesn't really matter is they'll


00:24:26.020 --> 00:24:30.500
freak out to say, Oh, there's extra stuff here. What am I supposed, you know, for example,


00:24:30.500 --> 00:24:35.460
this repo, it just has name, but in the data model, it has way more in the, the JSON data.


00:24:35.460 --> 00:24:39.300
So you try to deserialize it. I'll go, well, I don't have room to put all this other stuff.


00:24:39.300 --> 00:24:41.980
Things are, you know, freak out. And this one is just like, no, we're just going to


00:24:41.980 --> 00:24:46.020
filter down to what you asked for. I really, it's nice in a couple of ways. It's nice from


00:24:46.020 --> 00:24:49.760
performance, nice from clean code. I don't have to put all those other fields I don't


00:24:49.760 --> 00:24:54.900
care about, but also from, you talked about the evolution friendliness, right? Because


00:24:54.900 --> 00:24:59.420
what's way more common is that things get added rather than taken away or change. It's


00:24:59.420 --> 00:25:04.660
like, well, the complexity grows. Now repos also have this, you know, related repos or


00:25:04.660 --> 00:25:08.740
sub repos or whatever the heck they have. Right. And this model here will just let you


00:25:08.740 --> 00:25:10.340
go, whatever. Don't care.


00:25:10.340 --> 00:25:15.340
Yeah. If GitHub updates their API and adds new fields, you're not going to get an error.


00:25:15.340 --> 00:25:19.740
And if they remove a field, you should get a nice error that says expected, you know,


00:25:19.740 --> 00:25:23.840
field name, and now it's missing. You can track that down a lot easier than a random


00:25:23.840 --> 00:25:24.840
key error.


00:25:24.840 --> 00:25:28.240
I agree. I think, okay, let's, let's dive into the struct a little bit because that's


00:25:28.240 --> 00:25:31.800
where we're kind of on that now. And I think this is one of the highlights of what you


00:25:31.800 --> 00:25:36.420
built again. It's kind of the same mental model as people are familiar with some data


00:25:36.420 --> 00:25:41.500
classes with Pytantic and Adders and so on. So when I saw your numbers, I won't come back


00:25:41.500 --> 00:25:46.500
and talk about benchmarks with numbers on, but I just saw like, wow, this is fast. And


00:25:46.500 --> 00:25:50.380
now the memory usage is low. You must be doing something native. You must be doing something


00:25:50.380 --> 00:25:55.960
crazy in here. That's not just Dunder slots. While Dunder slots is awesome. It's there's


00:25:55.960 --> 00:26:02.820
more to it than that. Right. And so the written NC quite speedy and lightweight. So measurably


00:26:02.820 --> 00:26:06.740
faster than data classes, Adders and Pytantic. Like tell us about these classes. Like this


00:26:06.740 --> 00:26:07.740
is, this is pretty interesting.


00:26:07.740 --> 00:26:11.420
As mentioned earlier, they're not exactly, but they're, they're basically slots classes.


00:26:11.420 --> 00:26:16.540
So Python data model actually CPython's data model is either a class is a standard class


00:26:16.540 --> 00:26:21.580
where it stores its attributes in a dict. That's not exactly true. There's been some


00:26:21.580 --> 00:26:25.780
optimizations where the keys are stored separately alongside the class structure and all of the


00:26:25.780 --> 00:26:30.140
values are stored on the object instances. But in model, there's dict classes and there's


00:26:30.140 --> 00:26:34.980
slots classes, which you pre-declare your attributes to be in this, this Dunder slots


00:26:34.980 --> 00:26:40.700
iterable. And those get stored in line in the same allocation as the object instance.


00:26:40.700 --> 00:26:44.340
There's no pointer chasing. What that means is that you can't set extra attributes on


00:26:44.340 --> 00:26:49.180
them that weren't pre-declared, but also things are a little bit more efficient. We create


00:26:49.180 --> 00:26:53.820
those automatically when you subclass from a struct type. And we do a bunch of other


00:26:53.820 --> 00:26:58.020
interesting things that are stored on the type. That is why we had to write a meta class


00:26:58.020 --> 00:26:59.020
and see.


00:26:59.020 --> 00:27:02.300
I went to read it. I'm like, whoa, okay. Well, maybe we'll come back to this. There's a lot


00:27:02.300 --> 00:27:03.300
of stuff going on in that type.


00:27:03.300 --> 00:27:07.820
This is one of the problems with this, this hobby projects is that I wrote this for fun


00:27:07.820 --> 00:27:11.380
and a little bit of work related, but mostly fun. And it's not the easiest code base for


00:27:11.380 --> 00:27:15.380
others to step in to. It fits my mental model. Not necessarily everyone's.


00:27:15.380 --> 00:27:19.540
Yeah. I can tell you weren't looking for VC funding cause you didn't write it in Rust.


00:27:19.540 --> 00:27:22.460
Seems to be the common denominator these days.


00:27:22.460 --> 00:27:23.460
Yeah.


00:27:23.460 --> 00:27:26.740
Why C just because the CPython's already in C and that's the


00:27:26.740 --> 00:27:31.660
And I knew C. I do know Rust, but for what I wanted to do and the use case I had in mind,


00:27:31.660 --> 00:27:37.700
I wanted to be able to touch the C API directly. And that felt like the easiest way to go about


00:27:37.700 --> 00:27:40.260
doing it.


00:27:40.260 --> 00:27:44.660
This portion of talk Python to me is brought to you by Posit, the makers of Shiny, formerly


00:27:44.660 --> 00:27:50.540
R studio and especially shiny for Python. Let me ask you a question. Are you building


00:27:50.540 --> 00:27:54.440
awesome things? Of course you are. You're a developer or a data scientist. That's what


00:27:54.440 --> 00:28:00.220
we do. And you should check out Posit connect. Posit connect is a way for you to publish,


00:28:00.220 --> 00:28:05.040
share and deploy all the data products that you're building using Python.


00:28:05.040 --> 00:28:09.320
People ask me the same question all the time. Michael, I have some cool data science project


00:28:09.320 --> 00:28:14.040
or notebook that I built. How do I share it with my users, stakeholders, teammates? Do


00:28:14.040 --> 00:28:20.240
I need to learn FastAPI or flask or maybe view or react JS? Hold on now. Those are cool


00:28:20.240 --> 00:28:25.080
technologies and I'm sure you'd benefit from them, but maybe stay focused on the data project.


00:28:25.080 --> 00:28:29.480
Let Posit connect handle that side of things. With Posit connect, you can rapidly and securely


00:28:29.480 --> 00:28:36.360
deploy the things you build in Python streamlet dash, shiny, Bokeh, FastAPI flask, quadro


00:28:36.360 --> 00:28:42.200
reports, dashboards and API's. Posit connect supports all of them. And Posit connect comes


00:28:42.200 --> 00:28:46.920
with all the bells and whistles to satisfy it and other enterprise requirements. Make


00:28:46.920 --> 00:28:51.760
deployment the easiest step in your workflow with Posit connect. For a limited time, you


00:28:51.760 --> 00:28:57.320
can try Posit connect for free for three months by going to talkpython.fm/ Posit.


00:28:57.320 --> 00:29:03.040
That's talkpython.fm/ P O S I T. The link is in your podcast player show notes.


00:29:03.040 --> 00:29:05.960
Thank you to the team at Posit for supporting talk Python.


00:29:05.960 --> 00:29:12.600
Okay. So from a consumer of this struct class, I just say class, your examples, user is a


00:29:12.600 --> 00:29:17.080
class user, parentheses, dress from struct in the field, colon type. So like name, colon


00:29:17.080 --> 00:29:23.400
string groups, colon set of stir and so on. It looks like standard data classes type of


00:29:23.400 --> 00:29:27.800
stuff. But what you're saying is your meta class goes through and looks at that and says,


00:29:27.800 --> 00:29:31.400
okay, we're going to create a class called user, but it's going to have slots called


00:29:31.400 --> 00:29:35.320
name, email and groups among other things. Right. Like does that magic for us?


00:29:35.320 --> 00:29:39.600
Yeah. And then it sets up a bunch of internal data structures that are stored on the type.


00:29:39.600 --> 00:29:43.440
Okay. Like give me a sense of like, like what's, what's something, why, why do you got to put


00:29:43.440 --> 00:29:44.520
that in there? What's in there?


00:29:44.520 --> 00:29:48.800
So the way data classes work after they do all the type parsing stuff, which we have


00:29:48.800 --> 00:29:54.800
to do too, they then generate some code and eval it to generate each of the model methods.


00:29:54.800 --> 00:29:59.080
So when you're importing or when you define a new data class, it generates an init method


00:29:59.080 --> 00:30:02.600
and evals it and then stores it on the instance. That means that you have little bits of byte


00:30:02.600 --> 00:30:08.040
code floating around for all of your new methods. Message spec structs instead, each of the


00:30:08.040 --> 00:30:13.480
standard methods that the implementation provides, which would be, you know, init, wrapper, equality


00:30:13.480 --> 00:30:21.640
checks, copies, you know, various things are single C functions. And then the type has


00:30:21.640 --> 00:30:26.920
some data structures on it that we can use to define those. So we have a single init


00:30:26.920 --> 00:30:31.960
method for all struct types that's used everywhere. And as part of the init method, we need to


00:30:31.960 --> 00:30:36.000
know the fields that are defined on the struct. So we have some data stored on there about


00:30:36.000 --> 00:30:38.760
like the field names, default values, various things.


00:30:38.760 --> 00:30:39.760
Nice.


00:30:39.760 --> 00:30:42.240
Because they're written in C rather than, you know, Python byte code, they can be a


00:30:42.240 --> 00:30:47.880
lot faster. And because we're not having to eval a new method every time we define a struct,


00:30:47.880 --> 00:30:51.560
importing structs is a lot faster than data classes. Something I'm not going to guess,


00:30:51.560 --> 00:30:56.760
I have to look up on my benchmarks, but they are basically as efficient to define as a


00:30:56.760 --> 00:31:00.160
handwritten class where data classes have a bunch of overhead. If you've ever written


00:31:00.160 --> 00:31:03.320
a project that has, you know, a hundred of them, importing can slow down.


00:31:03.320 --> 00:31:07.960
Yeah. Okay. Because you basically are dynamically building them up, right? In data class story.


00:31:07.960 --> 00:31:13.000
Yeah. So you've got kind of the data class stuff. You got, as you said, Dunder net, repper,


00:31:13.000 --> 00:31:17.760
copy, et cetera. But you also have Dunder match args for pattern matching. That's pretty


00:31:17.760 --> 00:31:23.880
cool. And Dunder rich repper for pretty printing support with rich. Yeah. If you just rich.print,


00:31:23.880 --> 00:31:25.920
it'll take that, right? What happens then?


00:31:25.920 --> 00:31:28.400
It pre-prints it similar to like how a data class should be rendered.


00:31:28.400 --> 00:31:31.920
Rich is making a pretty big impact. So rich is special.


00:31:31.920 --> 00:31:32.920
I enjoy using it.


00:31:32.920 --> 00:31:37.640
This is excellent. You've got all this stuff generated. So much of it is in C and super


00:31:37.640 --> 00:31:42.400
lightweight and fast. But from the way we think of it, it's just a Python class, even


00:31:42.400 --> 00:31:45.600
little less weird than data classes, right? Because you don't have to put a decorator


00:31:45.600 --> 00:31:50.520
on it. You just derive from, from this thing. So that's super cool. Yeah. Super neat.


00:31:50.520 --> 00:31:54.840
The hope was that these would feel familiar enough to users coming from data classes or


00:31:54.840 --> 00:32:00.880
adders or Pydantic or all the various models that learning a new one wouldn't be necessary.


00:32:00.880 --> 00:32:02.240
They they're the same.


00:32:02.240 --> 00:32:05.440
Excellent. One difference if you're coming from Pydantic is there is no method to define


00:32:05.440 --> 00:32:11.080
done these by default. So you define a struct with fields, A, B, and C only A, B, and C


00:32:11.080 --> 00:32:14.880
exists as attributes on that, that class. You don't have to worry about any conflicting


00:32:14.880 --> 00:32:15.880
names.


00:32:15.880 --> 00:32:22.200
Okay. So for example, like the Pydantic ones have, I can't remember the V1 versus V2. It's


00:32:22.200 --> 00:32:26.040
like, I can't remember like two dictionary effectively, right? Where they'll like dump


00:32:26.040 --> 00:32:28.400
out the JSON or strings or things like that.


00:32:28.400 --> 00:32:33.560
In V1, there's a method dot JSON, which if you have a field name, JSON will conflict.


00:32:33.560 --> 00:32:37.920
They are remedying that by adding a model prefix for everything, which I think is a


00:32:37.920 --> 00:32:40.200
good idea. I think that's a good way of handling it.


00:32:40.200 --> 00:32:44.000
Yeah. Yeah. It's like model underscore JSON or dict or something like that. Yeah. Cool.


00:32:44.000 --> 00:32:47.800
Yeah. That's one of the few breaking changes. They actually, unless you're deep down in


00:32:47.800 --> 00:32:51.200
the guts of Pydantic that you'll, you might encounter. Yeah. You don't have to worry about


00:32:51.200 --> 00:32:57.120
that stuff because you're more function based, right? You would say decode or I guess, yeah,


00:32:57.120 --> 00:33:01.960
decode here's some, some data, some JSON or something. And then the thing you decode it


00:33:01.960 --> 00:33:07.240
into would be your user type. You'd say type equals user rather than go into the user directly.


00:33:07.240 --> 00:33:12.520
Right. Can we put our own, own properties and methods and stuff on these classes and


00:33:12.520 --> 00:33:13.520
that'll work all right?


00:33:13.520 --> 00:33:16.880
Yeah. They, they, this to a user, you should think of this as a data class that doesn't


00:33:16.880 --> 00:33:20.960
use a decorator. They should be identical unless you're ever trying to touch, you know,


00:33:20.960 --> 00:33:25.600
the dunder data class fields attribute that exists on data classes. There should be no


00:33:25.600 --> 00:33:27.600
runtime differences as far as you can tell.


00:33:27.600 --> 00:33:31.520
And when you're doing the schema validation, it sounds like you're basically embracing


00:33:31.520 --> 00:33:36.880
the optional optionality of, of the type system. If you say int, it has to be there. If you


00:33:36.880 --> 00:33:40.720
say optional int or int pipe none, may or may not be there, right?


00:33:40.720 --> 00:33:45.280
No, it's, it's, it's close. I'm going to be pedantic here a little bit. The optional fields


00:33:45.280 --> 00:33:49.600
are ones that have default values set. So optional bracket int without a default is


00:33:49.600 --> 00:33:52.880
still a required field. It's just one that could be an int or none. You'd have to have


00:33:52.880 --> 00:33:56.960
a literal none passed in. Otherwise we'd error. This more matches with how mypy interprets


00:33:56.960 --> 00:33:57.960
the type system.


00:33:57.960 --> 00:34:01.920
Okay. So if I had an optional thing, but it had no value, it would have to explicitly


00:34:01.920 --> 00:34:02.920
set it to none.


00:34:02.920 --> 00:34:03.920
Yes.


00:34:03.920 --> 00:34:07.480
Or would, yeah. Or it'd have to be there in the data every time. Like other things, you


00:34:07.480 --> 00:34:11.720
have default factories, right? Passing a function that gets called if it does, I guess if it


00:34:11.720 --> 00:34:16.760
doesn't exist, right? If the data's in there, it's being deserialized, it won't. Okay. Excellent.


00:34:16.760 --> 00:34:20.440
And I guess your, your decorator creates the initializer. But another thing that I saw


00:34:20.440 --> 00:34:25.640
that you had was you have this post init, which is really nice. Like a way to say like,


00:34:25.640 --> 00:34:29.960
okay, it's been deserialized. Let me try a little further. Tell us about this. This is


00:34:29.960 --> 00:34:30.960
cool.


00:34:30.960 --> 00:34:33.600
Yeah. It's coming from data classes. They have the same method. So if you need to do


00:34:33.600 --> 00:34:38.480
any extra thing after init, you can use it here rather than trying to override the built-in


00:34:38.480 --> 00:34:40.320
init, which we don't let you do.


00:34:40.320 --> 00:34:44.720
Right. Because it has so much magic to do, like let it do it. And yeah, you don't want


00:34:44.720 --> 00:34:48.480
to override that anyway. You don't have to deal with like passing all the arguments.


00:34:48.480 --> 00:34:52.880
Yeah. It's, you know, runs Python instead of maybe C, all these things. Right. So post


00:34:52.880 --> 00:34:57.200
init would exist if you have more complex constraints, right?


00:34:57.200 --> 00:35:01.600
Currently that's one reason to use it. We currently don't support custom validation


00:35:01.600 --> 00:35:06.000
functions. There's no dot validate decorator, various frameworks, different ways of defining


00:35:06.000 --> 00:35:09.320
these. We, we have some constraints that are built in. You can constrain, you know, if


00:35:09.320 --> 00:35:13.800
the number to be greater than some value, but there's no way to specify custom constraints


00:35:13.800 --> 00:35:17.760
currently. It's on the roadmap. It's the thing we want to add. Post init's a way to hack


00:35:17.760 --> 00:35:22.080
around that. So right now you're looking at the screen, you have a post init defined and


00:35:22.080 --> 00:35:27.160
you're checking if low is greater than high, raise an error. And that'll bubble up through


00:35:27.160 --> 00:35:30.880
decodes and, you know, raise an isuser facing validation error. In the long run, we'd like


00:35:30.880 --> 00:35:34.400
that to be done a little bit more field-based, similar to coming from other frameworks.


00:35:34.400 --> 00:35:38.760
It is tricky though, because you know, the validation goes onto one field or the other.


00:35:38.760 --> 00:35:44.240
You don't have like composite validators necessarily. Right. And so there's totally valid values


00:35:44.240 --> 00:35:48.120
of this low, but it long, whatever it is, it has to be lower than high. Right. But how


00:35:48.120 --> 00:35:52.160
do you express that relationship? So I think this is awesome. Other areas where, you know,


00:35:52.160 --> 00:35:55.480
it could be interesting is like under some circumstances, maybe you've got to compute,


00:35:55.480 --> 00:35:59.960
I don't know, compute some field also that's in there. That's not set. I don't know. There's,


00:35:59.960 --> 00:36:02.760
there's some good options in here. I like it a lot. Yeah. I guess the errors just come


00:36:02.760 --> 00:36:06.720
out as just straight out of like something went wrong with under post init, right. Rather


00:36:06.720 --> 00:36:08.720
than field low has this problem.


00:36:08.720 --> 00:36:13.120
It's a little harder to relate an error being raised to a specific field if you raise it


00:36:13.120 --> 00:36:16.960
in the post init. Yeah. Also, since you're looking at this and I'm proud that I got this


00:36:16.960 --> 00:36:21.560
to work, the post errors raised in post init use chained exceptions. So you can see a little


00:36:21.560 --> 00:36:25.120
bit of the cause of where it comes from and getting those to work at the Python C API


00:36:25.120 --> 00:36:29.680
is completely undocumented and a little tricky to figure out. A lot of reading how the interpreter


00:36:29.680 --> 00:36:34.840
does it and making the right, you know, 12 incantations to get them to bubble up. Right.


00:36:34.840 --> 00:36:40.160
Yeah. I do not envy you working on this struct, lower this base class, but that'd be, that's


00:36:40.160 --> 00:36:43.680
where part of the magic is. Right. And that's why I wanted to dive into this because I think


00:36:43.680 --> 00:36:47.960
it's, it behaves like Python classes, but it has this, these really special features


00:36:47.960 --> 00:36:52.200
that we don't normally get, right. Like low memory usage, high performance, accessing


00:36:52.200 --> 00:36:57.840
the fields. Is that any quicker or is it like standard struct level of quick?


00:36:57.840 --> 00:37:01.520
Attribute access and settings should be the same as any other class. Things that are faster


00:37:01.520 --> 00:37:06.200
are init, repper, not that that should matter. If you're looking for a high performance repper,


00:37:06.200 --> 00:37:09.920
that's, you're doing it wrong. Seems like you're doing something wrong. Equality checks,


00:37:09.920 --> 00:37:13.400
comparisons. So sorting, you know, less than greater than, I think that's it. Everything


00:37:13.400 --> 00:37:14.640
else should be about the same.


00:37:14.640 --> 00:37:19.600
So field ordering, you talked about like evolution over time. Does it, does this matter?


00:37:19.600 --> 00:37:22.680
Field ordering is mostly defining how, what happens if you do subclasses and stuff. This


00:37:22.680 --> 00:37:27.320
whole section is, if you're not subclassing, shouldn't hopefully be relevant to you. We


00:37:27.320 --> 00:37:29.880
match how data class handles things for ordering.


00:37:29.880 --> 00:37:34.840
Okay. So I could have my user, but I could have a super user that derives from my user


00:37:34.840 --> 00:37:37.360
that derives from struct and things will still hang together.


00:37:37.360 --> 00:37:41.360
And so figuring out how all the fields order out through that subclassing, this doc is


00:37:41.360 --> 00:37:42.360
about.


00:37:42.360 --> 00:37:46.880
Another type typing system thing you can do a lot is have explicitly claim something


00:37:46.880 --> 00:37:53.440
as a class variable. You know, Python is weird about its classes and what makes a variable


00:37:53.440 --> 00:37:57.400
that's associated with a class and or not. Right. So with these type of classes, you


00:37:57.400 --> 00:38:01.800
would say like class example, colon, and then you have X colon int. Right. And that appears


00:38:01.800 --> 00:38:07.520
will appear on the static type, like example dot X, but it also imbues each object with


00:38:07.520 --> 00:38:13.080
its own copy of that X. Right. Which is like a little bit, is it a static thing or part


00:38:13.080 --> 00:38:17.160
of the type or is it not? It's kind of funky, but you also can say that explicitly from


00:38:17.160 --> 00:38:20.640
the typing, you say this is a class variable. What happens then? Right.


00:38:20.640 --> 00:38:26.920
So standard attributes exist on the instances where a class var exists on the class itself.


00:38:26.920 --> 00:38:31.720
Class vars are accessible on an instance, but the actual data is stored on the class.


00:38:31.720 --> 00:38:33.480
So you're not having an extra copy.


00:38:33.480 --> 00:38:37.760
I see. So if there's some kind of singleton type of thing or just one of them. Yeah.


00:38:37.760 --> 00:38:43.200
Yeah. It has to do with how Python does attribute resolution where it'll check on the instance


00:38:43.200 --> 00:38:47.600
and then on the type. And also there's descriptors in there, you know, somewhere.


00:38:47.600 --> 00:38:51.800
Interesting. Okay. Like other things, I suppose it's pretty straightforward that you take


00:38:51.800 --> 00:38:55.600
these types and you use them to validate them. But one of the big differences with message


00:38:55.600 --> 00:39:02.040
spec dot struct versus pydantic dot base model and others is the validation doesn't happen


00:39:02.040 --> 00:39:08.080
all the time. It just happens on encode decode. Right. Like you could call the constructor


00:39:08.080 --> 00:39:12.360
and pass in bad data or like it just doesn't pay attention. Right. Yeah. Why is it like


00:39:12.360 --> 00:39:13.360
that?


00:39:13.360 --> 00:39:17.360
So this is one of the reasons I wrote my own thing rather than building off of something


00:39:17.360 --> 00:39:21.560
existing like pydantic. Side tangent here just to add history context here. Message


00:39:21.560 --> 00:39:25.280
spec was started about three years ago, the JSON and it kind of fell into its full model


00:39:25.280 --> 00:39:29.760
about two years ago. So this has existed for around two years. Yeah. We're pre the pydantic


00:39:29.760 --> 00:39:35.920
degree. Right. Anyway, the reason I wanted all of this was when you have your own code,


00:39:35.920 --> 00:39:39.320
where bugs can come up are bugs in your own code. I've typed something wrong. I've made


00:39:39.320 --> 00:39:44.040
a mistake and I want that to be checked or it can be user data is coming in or, you know,


00:39:44.040 --> 00:39:47.520
maybe it's distributed system and it's still my own code. It's just a file or database.


00:39:47.520 --> 00:39:52.460
Yeah. Whatever. Yeah. We have many mechanisms of testing our own code. You can write tests.


00:39:52.460 --> 00:39:56.640
You have static analysis tools like my py, pywrite or checking. It's a lot easier for


00:39:56.640 --> 00:40:02.120
me to validate that a function I wrote is correct. There are other tools I believe then


00:40:02.120 --> 00:40:07.320
we should lean on rather than runtime validation in those cases. But when we're reading an


00:40:07.320 --> 00:40:11.120
external data, whether it's coming over the wire or coming from a file, coming from user


00:40:11.120 --> 00:40:15.200
input in some way, we do need to validate because the user could have passed us something


00:40:15.200 --> 00:40:19.240
that doesn't match our constraints. Yeah. As soon as you start a trusting user input,


00:40:19.240 --> 00:40:23.880
you're in for a bad time. We don't want to arbitrarily be trusting. We do validate on


00:40:23.880 --> 00:40:27.840
JSON decoding without a master pack decoding. We also have a couple of functions for doing


00:40:27.840 --> 00:40:32.340
in memory conversions. So there's msgspec convert msgspec to built-ins for


00:40:32.340 --> 00:40:36.500
going the other way. So that's for doing conversion of runtime data that you got from some rather


00:40:36.500 --> 00:40:40.800
than a specific format. Yeah. Because if you're calling this constructor and passing the wrong


00:40:40.800 --> 00:40:46.440
data, my py should check that. Pycharm should check that. Maybe rough would catch it. I'm


00:40:46.440 --> 00:40:50.120
not sure, but like there's a bunch of tools. Yeah. Rough doesn't have a type checker yet.


00:40:50.120 --> 00:40:55.560
Yeah. TBD on that. Yeah. Okay. Yeah. Being able to check these statically, it means that


00:40:55.560 --> 00:41:00.120
we don't have to pay the cost every time we're running, which I don't think we should. That's


00:41:00.120 --> 00:41:03.880
extra runtime performance that we don't need to be spending. Yeah. Definitely. Check it


00:41:03.880 --> 00:41:07.600
on the boundaries, right? Check it where it comes into the system and then should be good.


00:41:07.600 --> 00:41:11.880
The other reason I was against adding runtime validation to these trucks is I want all types


00:41:11.880 --> 00:41:15.960
to be on equal footing. And so if I am creating a list, the list isn't going to be doing any


00:41:15.960 --> 00:41:20.200
validation because it's, you know, the Python built in. Same with data classes, same with


00:41:20.200 --> 00:41:24.680
adders, types, you know, whatever. And so only doing a validation when you construct


00:41:24.680 --> 00:41:29.120
some object type that's from a built in that I've defined or like a type I've defined doesn't


00:41:29.120 --> 00:41:33.660
give parity across all types and might give a user, you know, misconceptions about when


00:41:33.660 --> 00:41:37.000
something is validated and when they can be sure it's correct versus when it hasn't.


00:41:37.000 --> 00:41:41.280
Yeah. Have you seen bear type? I have. Yeah. Bear type's a pretty interesting option. If


00:41:41.280 --> 00:41:45.920
people really want runtime validation, they could, you know, go in and throw bear type


00:41:45.920 --> 00:41:49.960
onto their system and let it do its thing. Even if you're not doing it, you should read


00:41:49.960 --> 00:41:54.160
the docs just for the sheer joy that these docs are. Oh, they are pretty glorious. Yeah,


00:41:54.160 --> 00:41:58.360
I'll do it. You got it. This guy, they bearing the weight a little down here, but they described


00:41:58.360 --> 00:42:04.080
himself as bear type brings rust and C++ inspired zero cost abstractions into the lawless world


00:42:04.080 --> 00:42:09.160
of the dynamically typed Python by enforcing type safety at the granular level of functions


00:42:09.160 --> 00:42:14.080
and methods against type hints standardized by the Python community of O order one non


00:42:14.080 --> 00:42:19.840
amortized worst case time with negligible constant factors. Oh my gosh. So much fun,


00:42:19.840 --> 00:42:23.600
right? They're just joking around here, but it's a pretty cool library. If you want runtime


00:42:23.600 --> 00:42:28.560
type checking pretty fast. Okay. Interesting. You talked about the pattern matching. I'll


00:42:28.560 --> 00:42:33.160
come back to that. One thing I want to talk about, well, okay. Frozen, frozen instances.


00:42:33.160 --> 00:42:38.160
This comes from data classes without the instances being frozen. The structs are mutable. Yeah.


00:42:38.160 --> 00:42:42.800
I can like get one changes value, serialize it back out, things like that. But frozen,


00:42:42.800 --> 00:42:46.560
I suppose means what you would expect, right? Yeah. Frozen is the same meaning as a data


00:42:46.560 --> 00:42:52.840
classic. How deep does frozen go? So for example, is it frozen all the way down? So in the previous


00:42:52.840 --> 00:42:57.760
example from itamar, it had like the top level class and then like other structs that were


00:42:57.760 --> 00:43:02.600
nested in there. Like if I say the top level is frozen to the nested ones themselves become


00:43:02.600 --> 00:43:07.680
frozen? No. So frozen applies to the type. So if you define a type as frozen, that means


00:43:07.680 --> 00:43:12.600
you can't change values that are set as attributes on that type, but you can still change things


00:43:12.600 --> 00:43:16.880
that are inside it. So if a frozen class contains a list, you can still append stuff to the


00:43:16.880 --> 00:43:20.800
list. There's no way to get around that except if we were to do some deep, deep, deep magic,


00:43:20.800 --> 00:43:23.880
which we shouldn't. It would definitely slow it down if you had to go through and like


00:43:23.880 --> 00:43:28.440
re create frozen lists every time you saw a list and stuff like that. Yeah. Okay. And


00:43:28.440 --> 00:43:32.960
then there's one about garbage collection in here. Yeah. Which is pretty interesting.


00:43:32.960 --> 00:43:37.480
There we go. Disabling garbage collection. This is under the advanced category. Warning


00:43:37.480 --> 00:43:42.080
box around this that tells you not to. What could go wrong? Come on. Part of this was


00:43:42.080 --> 00:43:47.120
experimenting with the DAS distributed scheduler, which is a unique application, I think, for


00:43:47.120 --> 00:43:50.560
people that are writing web stuff in that all of its data is kept in memory. There's


00:43:50.560 --> 00:43:55.920
no backing database that's external. And so it is as fast to respond as, you know, the


00:43:55.920 --> 00:43:59.880
bits of in-memory computation that needs to do before it sends out a new task to a worker.


00:43:59.880 --> 00:44:05.560
So in this case, their serialization performance matters. But also it's got a lot of in-memory


00:44:05.560 --> 00:44:10.440
compute. You know, it's a dicts of types of, you know, lots of chaining down. The way the


00:44:10.440 --> 00:44:16.200
CPython garbage collector works is that these large dictionaries could add GC overhead.


00:44:16.200 --> 00:44:19.400
Every time a GC thing happens, it has to scan the entire dictionary.


00:44:19.400 --> 00:44:22.740
Any container thing could contain another. And once you do that, there could be a cycle.


00:44:22.740 --> 00:44:28.080
And then for very large graphs, GC pauses could become noticeable. Yes. This is an experiment


00:44:28.080 --> 00:44:32.180
and seeing ways around that because we've done some deep magic with how structs work.


00:44:32.180 --> 00:44:39.140
We can disable GC for subclasses, user-defined types, which CPython does not expose normally


00:44:39.140 --> 00:44:43.800
and really isn't something you probably want to be doing in most cases. But if you do,


00:44:43.800 --> 00:44:48.520
you get a couple benefits. The types are smaller. Every instance needs to include some extra


00:44:48.520 --> 00:44:54.080
state for tracking GC. I believe on recent builds, it's 16 bytes. So it's two pointers.


00:44:54.080 --> 00:44:56.360
So that's, you know, you're shaving 16 bytes print.


00:44:56.360 --> 00:44:59.600
That's non-trivial. Yeah. If you got a huge list of them, that could be a lot.


00:44:59.600 --> 00:45:05.440
And two, they don't, they're not traced. And so if you have a lot of them, that's a


00:45:05.440 --> 00:45:08.680
reducing reduction in tracing overhead. Every time a GC pass happens.


00:45:08.680 --> 00:45:13.680
GC puts more overhead on, on, on top of stuff than you would think. So I did some crazy


00:45:13.680 --> 00:45:18.920
GC stuff over at talk Python and training of my courses. You go to slash sitemap.xml.


00:45:18.920 --> 00:45:25.360
I don't know how many entries are in the sitemap, but there are 30,000 lines of sitemap, like


00:45:25.360 --> 00:45:30.480
many, many, many, many, many thousands of URLs have to come back with details just to


00:45:30.480 --> 00:45:35.680
generate that page in one request with the default Python settings in Python 3.10. I


00:45:35.680 --> 00:45:43.320
think it was, it was doing 77 garbage collections while generating this page. That's not ideal.


00:45:43.320 --> 00:45:48.960
I switched it to just change or tweak how frequently the GC runs. So like every 70,000,


00:45:48.960 --> 00:45:54.520
no, every 50,000 allocations instead of every 700 and the site runs 20% faster now and uses


00:45:54.520 --> 00:45:59.040
the same amount of memory. Right. And so this is not exactly what you're talking about here,


00:45:59.040 --> 00:46:04.280
but it's in the, it plays in the same space as like you can dramatically change the things


00:46:04.280 --> 00:46:09.280
that are triggering this and dramatically change the performance potentially. The caveat


00:46:09.280 --> 00:46:10.960
is you better not have cycles.


00:46:10.960 --> 00:46:15.320
Yeah. So the, the other thing with these is, as you pointed out is the indicator of when


00:46:15.320 --> 00:46:19.640
a GC pass happens has to do with how many GC aware types have been allocated. And so


00:46:19.640 --> 00:46:23.200
if you mark a type is not a GC type, then the counter is an increment. You're not paying


00:46:23.200 --> 00:46:24.200
that cost.


00:46:24.200 --> 00:46:27.400
So if you're going to allocate all the integers you want all day long, it'll never affect


00:46:27.400 --> 00:46:31.320
the GC. But if you start allocating classes, dictionaries, tuples, et cetera, that is like,


00:46:31.320 --> 00:46:35.640
well, those could contain cycles. You have 700 more than you've deallocated since last


00:46:35.640 --> 00:46:37.040
time. I'm going to go check it.


00:46:37.040 --> 00:46:42.120
One place this comes up is if you have say a really, really large JSON file because any


00:46:42.120 --> 00:46:46.600
deserialization is in early in allocation, heavy workload, which means that you can have


00:46:46.600 --> 00:46:50.840
a GC pause happen, you know, several times during it because you've allocated, you know,


00:46:50.840 --> 00:46:55.120
that many types. Turning off GC for these types lets you avoid those GC pauses, which


00:46:55.120 --> 00:46:59.920
gives you actual runtime benefits. A different way of doing this that is less insane is to


00:46:59.920 --> 00:47:06.240
just disable GC during the decode. Do a, you know, GC disable, JSON decode, GC enable,


00:47:06.240 --> 00:47:10.000
and you only do a GC pass once, especially because JSON as a tree-like structure can


00:47:10.000 --> 00:47:12.880
never create cycles. You're not going to be having an issue there.


00:47:12.880 --> 00:47:16.200
But you're probably allocating a lot of different things that are container types. And so it


00:47:16.200 --> 00:47:22.560
looks to the GC like, oh, this is some really sketchy stuff. We better get on the game here.


00:47:22.560 --> 00:47:28.200
But you know, as you said, there's no cycles in JSON. So there's a lot of scenarios like


00:47:28.200 --> 00:47:32.120
that, like database queries. You know, I got a thousand records back from a table. They're


00:47:32.120 --> 00:47:37.440
all some kind of container. So minimum one GC happens just to read back that data, but


00:47:37.440 --> 00:47:40.720
you know, there's no cycles. So why is the GC happening? Right. You can kind of control


00:47:40.720 --> 00:47:45.880
that a little bit, or you just turn the number up to 50,000 like I did. It still happens,


00:47:45.880 --> 00:47:50.120
but less, a lot less. Yeah. So this is pretty interesting though, that you just set GC equals


00:47:50.120 --> 00:47:54.680
false. Where do you set this? Is this like in the derived bit or?


00:47:54.680 --> 00:48:00.600
It's part of the class definition. So we make use of class definition keyword arguments.


00:48:00.600 --> 00:48:05.560
So it goes after the struct type in the subclass. You do, you know, my class, open a breath


00:48:05.560 --> 00:48:10.160
of sea, struct, comma, GC equals false, close comma, colon, rest of the class.


00:48:10.160 --> 00:48:14.080
Yeah. That's where I thought, but it is a little funky. I mean, it kind of highlights


00:48:14.080 --> 00:48:19.080
the meta class action going on there. Right. What else should people know about these structs?


00:48:19.080 --> 00:48:23.280
They're fast and they can be used for not just a serialization. So if you are just writing


00:48:23.280 --> 00:48:26.600
a program and you happen to have msgspec on your system, it should be faster to use


00:48:26.600 --> 00:48:30.560
them than data classes. Whether that matters is of course, application dependent, but they're


00:48:30.560 --> 00:48:34.680
like generally a good idea. They happen to live in this serialization library, but that's


00:48:34.680 --> 00:48:38.280
just because that's where I wrote them. Yeah. It's where they, in a future world, we might


00:48:38.280 --> 00:48:42.600
split them out into a sub package. Yeah. Fast struct, Pippin's all fast struct.


00:48:42.600 --> 00:48:48.200
Who knows? He had to be named. So better than data classes. I mean, they have the capabilities


00:48:48.200 --> 00:48:53.120
of data classes, so that's cool, but better than straight up regular classes, like bare


00:48:53.120 --> 00:48:54.920
classes, you know, class, colon, name.


00:48:54.920 --> 00:48:58.760
Are opinionated a little bit. They're how I think people probably should be writing


00:48:58.760 --> 00:49:02.440
classes and they're opinionated in a way that means that you can't write them in ways that


00:49:02.440 --> 00:49:07.240
I don't want you to. So the way a struct works is you define attributes on it using type


00:49:07.240 --> 00:49:11.920
annotations and we generate a fast init method for you. We don't let you write your own init.


00:49:11.920 --> 00:49:15.600
In the subclass, you can't override init. The generated one is the one you get. That


00:49:15.600 --> 00:49:19.520
means that like if you're trying to create an instance from something that isn't those


00:49:19.520 --> 00:49:23.680
field names, you can't do that. You need to use a new class method for writing those.


00:49:23.680 --> 00:49:28.080
I believe this is how people, at least on projects I work on, generally use classes.


00:49:28.080 --> 00:49:33.520
So I think it's a fine limitation, but it is putting some guardrails around how the


00:49:33.520 --> 00:49:37.600
arbitrariness of how you can define a Python class. You could have a, you know, a handwritten


00:49:37.600 --> 00:49:42.960
class that has two attributes, X and Y, and your init takes, you know, parameters A and


00:49:42.960 --> 00:49:43.960
B.


00:49:43.960 --> 00:49:47.920
Sure. Or maybe it just takes X and it always defaults Y unless you go and change it after


00:49:47.920 --> 00:49:51.680
or whatever. Right. I guess you could do sort of do that with default values, right? But


00:49:51.680 --> 00:49:56.040
you couldn't prohibit it from being passed in. I'm feeling some factory classes.


00:49:56.040 --> 00:50:01.200
The Adders docs have a whole, whole page telling people about why this pattern is, is better


00:50:01.200 --> 00:50:03.520
and nudging them to do this. So this isn't a new idea.


00:50:03.520 --> 00:50:07.360
Yeah. Go, go check out Adders and see what they're saying as well. Huh? There's probably


00:50:07.360 --> 00:50:10.960
a debate in the issues somewhere on GitHub. There always is a debate. Yeah. Let's see.


00:50:10.960 --> 00:50:14.360
Let's go get a bunch of stuff up here. I want to talk about, I guess really quickly, since


00:50:14.360 --> 00:50:19.760
there's a lot of like C native platform stuff, right? This is available on, you know, pip


00:50:19.760 --> 00:50:24.480
install message, msgspec. We're getting the wheel. It seemed like it worked fine on


00:50:24.480 --> 00:50:30.160
my M2 MacBook Air. Like what are the platforms that I get a wheel that don't have to worry


00:50:30.160 --> 00:50:31.160
about compiling?


00:50:31.160 --> 00:50:37.000
So we use CI build wheel for building everything. And I believe I've disabled some of the platforms.


00:50:37.000 --> 00:50:42.160
The ones that are disabled are mostly disabled because CI takes time and you need to minimize


00:50:42.160 --> 00:50:46.800
them, but everything common should exist, including Raspberry Pi and various ARM builds.


00:50:46.800 --> 00:50:51.080
Excellent. Okay. Yeah. It seemed like it worked just fine. I didn't really know that it was


00:50:51.080 --> 00:50:55.120
like doing a lot of native code stuff, but it seems like it. And also available on Conda,


00:50:55.120 --> 00:50:59.520
Conda Forge. So that's cool. If you Conda, you can also just Conda install it. I kind


00:50:59.520 --> 00:51:04.240
of promised talking about the benchmarks a little bit, didn't I? So benchmarks are always.


00:51:04.240 --> 00:51:06.880
If you click on the graph on the bottom, it'll get, bring you to it.


00:51:06.880 --> 00:51:11.320
Yeah. They're always, always rife with like, that's not my benchmark. I'm doing it different,


00:51:11.320 --> 00:51:16.480
you know, but give us a sense of just, it says fast italicies leaning forward. Give


00:51:16.480 --> 00:51:21.040
us a sense of like, where does this land? Is it, you know, 20% faster or is it a lot


00:51:21.040 --> 00:51:22.040
better?


00:51:22.040 --> 00:51:24.960
Yeah. So as you said, benchmarks are a problem. The top of this benchmark docs has a whole


00:51:24.960 --> 00:51:29.160
argument against believing them and telling you to run your own. So take the grain of


00:51:29.160 --> 00:51:34.040
salt. I started benchmarking this mostly just to know how we stacked up. It's important


00:51:34.040 --> 00:51:36.960
if you're making changes to know if you're getting slower, it's also important to know


00:51:36.960 --> 00:51:40.600
what the actual trade-offs of your library are. All software engineering is trade-offs.


00:51:40.600 --> 00:51:48.240
So msgspec is generally fast. The JSON parser in it is one of the fastest in Python


00:51:48.240 --> 00:51:53.000
or the fastest, depending on what your message structure is and how you're invoking it. It


00:51:53.000 --> 00:51:56.920
at least is on par with, or JSON, which is generally what people consider to be the fast


00:51:56.920 --> 00:51:57.920
parser.


00:51:57.920 --> 00:51:59.960
Right. That's, that's where they go when they want fast. Yeah.


00:51:59.960 --> 00:52:04.880
So if you are specifying types, so if you, you know, add in a type annotation to a JSON


00:52:04.880 --> 00:52:08.320
decode call with message spec, even if you're decoding the whole message, you're not doing


00:52:08.320 --> 00:52:13.040
a subset. We're about two X faster than our JSON. You actually get a speed up by defining


00:52:13.040 --> 00:52:17.320
your types because struct types are so efficient to allocate versus a deck.


00:52:17.320 --> 00:52:19.800
That's kind of the opposite of what you might expect, right?


00:52:19.800 --> 00:52:23.240
It seems like we're doing more work, but we're actually able to do less because we can take


00:52:23.240 --> 00:52:26.080
some more, you know, efficient fast paths.


00:52:26.080 --> 00:52:33.040
And then a thousand objects with validation compared to, yeah, Mesher, Merle, C Adders,


00:52:33.040 --> 00:52:35.160
Pydantic and so on. Probably the last one.


00:52:35.160 --> 00:52:39.280
This was a grab bag of various validation libraries that seemed popular. Mesher Merle


00:52:39.280 --> 00:52:43.640
is the one that dbt uses. I think they're the primary consumer of that. Catters is for


00:52:43.640 --> 00:52:48.160
Adders. Pydantic is, you know, ubiquitous. This right here in this, this benchmark graph


00:52:48.160 --> 00:52:53.760
we're looking at is against Pydantic V1. I have not had a chance to update our benchmarks


00:52:53.760 --> 00:52:57.800
to go against V2. There's a separate gist somewhere that is, got some numbers there.


00:52:57.800 --> 00:53:02.680
The standard number they throw out is like 22 times faster. So it still puts you multiples


00:53:02.680 --> 00:53:03.680
faster.


00:53:03.680 --> 00:53:08.160
In that benchmark, we're averaging 10 to 20 X faster than Pydantic V2. In numbers I run


00:53:08.160 --> 00:53:13.880
against V1, we're about 80 to 150 X faster. So it really is structure dependent.


00:53:13.880 --> 00:53:17.440
Yeah, sure. You have one field or do you have a whole bunch of stuff?


00:53:17.440 --> 00:53:22.560
Yeah, exactly. And what types of fields? To be getting more into the weeds here, JSON


00:53:22.560 --> 00:53:27.080
parsing is not easy. Message pack parsing is like the form that was designed for computers


00:53:27.080 --> 00:53:28.920
to handle it. It's, you know,


00:53:28.920 --> 00:53:31.160
Seven bytes in there is an integer here.


00:53:31.160 --> 00:53:35.880
Yeah. Okay. Where, where JSON is human readable and parsing strings into stuff is slow.


00:53:35.880 --> 00:53:38.040
Right. The flexibility equals slowness. Yeah.


00:53:38.040 --> 00:53:44.120
Our string parsing routines in, in msgspec are faster than the ones used by or JSON.


00:53:44.120 --> 00:53:48.080
Our integer parsing routines are slower, but there's a different trade off there.


00:53:48.080 --> 00:53:51.760
Interesting. Okay. Yeah. I think this is just seems so neat. There's so much flexibility,


00:53:51.760 --> 00:53:55.480
right? With all the different formats and the restrictions on the class, they exist,


00:53:55.480 --> 00:54:00.440
but they're unstruck, but they're, they're not insane. Right? I mean, you build a proper,


00:54:00.440 --> 00:54:05.320
proper OOP type of things. You don't need super crazy hierarchies. Like that's where


00:54:05.320 --> 00:54:09.040
you get in trouble with that stuff anyway. So don't do it. I guess we don't have much


00:54:09.040 --> 00:54:13.160
time left. One thing I think we could talk about a bit, maybe it would be, if I find


00:54:13.160 --> 00:54:16.880
it, the extensions. Just maybe talk about parsing stuff that are, is kind of unknown.


00:54:16.880 --> 00:54:18.200
This is pretty interesting.


00:54:18.200 --> 00:54:23.280
So the way we allow extension currently, this is, there's an intention to change this and


00:54:23.280 --> 00:54:27.880
expand it, but currently extending, adding new types is done via a number of different


00:54:27.880 --> 00:54:32.600
hooks that are called when a new type is encountered. So custom user defined type of some form.


00:54:32.600 --> 00:54:36.920
I liked doing this rather than adding it into the annotation, because if I have a new type,


00:54:36.920 --> 00:54:41.200
I want it to exist probably everywhere. And I don't want to have to keep adding in and


00:54:41.200 --> 00:54:47.400
use the serializer and deserializer and as part of the type annotations. So to define


00:54:47.400 --> 00:54:52.280
a new type that you want to encode, you can add an encode hook, which takes in the instance


00:54:52.280 --> 00:54:56.080
and returns something that msgspec knows how to handle. This is similar to, if you're


00:54:56.080 --> 00:55:00.240
coming from standard library JSON, there's a default callback. It's the same. We renamed


00:55:00.240 --> 00:55:02.840
it to be a little better name in my mind, but it's the same thing.


00:55:02.840 --> 00:55:07.520
Right. So your example here is taking a complex number, but storing it as a tuple of real


00:55:07.520 --> 00:55:13.000
and imaginary numbers, but then pulling it back into a proper complex number object.


00:55:13.000 --> 00:55:16.360
Super straightforward. Yeah. But makes it possible. Yeah. Yeah. That's really cool.


00:55:16.360 --> 00:55:20.240
So people can apply this and this, I guess, would, didn't really matter on the output


00:55:20.240 --> 00:55:25.000
destination, does it? Your job is here is to take a type that's not serializable to


00:55:25.000 --> 00:55:29.680
one that is, and then whether that goes to a message, a message pack or JSON or whatever,


00:55:29.680 --> 00:55:31.080
it's kind of not your problem.


00:55:31.080 --> 00:55:35.360
Yeah. And then the decode hook is the inverse. You get a bunch of stuff that is, you know,


00:55:35.360 --> 00:55:39.360
core types and strings, whatever, and you compose them up into your new custom type.


00:55:39.360 --> 00:55:42.560
Jim, I think we're getting about out of time here, but I just want to point out, like if


00:55:42.560 --> 00:55:46.240
people hit the user guide, there's a lot of cool stuff here and there's a whole performance


00:55:46.240 --> 00:55:52.160
tips section that people can check out. You know, if we had more time, maybe we'd go into


00:55:52.160 --> 00:55:57.240
them, but like, for example, you can call msgspec dot JSON dot encode, or you can


00:55:57.240 --> 00:56:00.600
create an encoder and say the type and stuff, and then reuse that, right? Those, those kinds


00:56:00.600 --> 00:56:01.960
of things. Yeah.


00:56:01.960 --> 00:56:07.480
There's another method that is, again, a terrible internal hack for reusing buffers. So you


00:56:07.480 --> 00:56:10.720
don't have to keep allocating byte buffers every message. You can allocate a byte array


00:56:10.720 --> 00:56:13.240
once and use it for everything. Save some memory.


00:56:13.240 --> 00:56:16.120
Let me ask Ellie's got a question. I'm going to read some words that don't mean anything


00:56:16.120 --> 00:56:22.200
to me, but they've made to you. How does the performance of message pack plus message spec


00:56:22.200 --> 00:56:26.020
with the array-like equals true optimization compared to flat buffers?


00:56:26.020 --> 00:56:31.160
So by default objects, so struct types, data classes, whatever, encode as objects in the


00:56:31.160 --> 00:56:36.320
stream. So a JSON object has keys and values, right? If you have a point with fields X and


00:56:36.320 --> 00:56:41.160
Y, it's got X and Y, you know, one, two. We have an array-like optimization, which lets


00:56:41.160 --> 00:56:45.400
you drop the field names. And so that would instead encode as an array of, you know, one


00:56:45.400 --> 00:56:49.760
comma two, dropping the X and Y reduces the message size on the wire. If the other side


00:56:49.760 --> 00:56:53.080
knows what the structure is, it can, you know, pull that back up into a type.


00:56:53.080 --> 00:56:59.120
In terms of message pack as a format, plus with the array-like optimization, the output


00:56:59.120 --> 00:57:03.880
size should be approximately the same as you would expect it to come out of flat buffers.


00:57:03.880 --> 00:57:09.640
The Python flat buffers library is not efficient for creating objects from the binary. So it's


00:57:09.640 --> 00:57:13.760
going to be a lot faster to pull it in. Obviously, this is then a very custom format. You're


00:57:13.760 --> 00:57:19.440
doing a weird thing. And so compatibility with other ecosystems will be slower or not


00:57:19.440 --> 00:57:22.320
slower necessarily, but you'll have to write them yourself. Not everything knows how to


00:57:22.320 --> 00:57:23.400
read message pack.


00:57:23.400 --> 00:57:24.400
More brittle potentially.


00:57:24.400 --> 00:57:25.400
Yeah.


00:57:25.400 --> 00:57:26.400
Yes.


00:57:26.400 --> 00:57:27.400
Yeah. Yeah.


00:57:27.400 --> 00:57:29.640
But for Python, talking to Python, that's probably the fastest way to go between processes.


00:57:29.640 --> 00:57:33.560
And probably a lot faster than JSON or YAML or something like that.


00:57:33.560 --> 00:57:38.040
Okay. Excellent. I guess, you know, there's many more things to discuss, but we're going


00:57:38.040 --> 00:57:41.320
to leave it here. Thanks for being on the show. Final call to action. People want to


00:57:41.320 --> 00:57:46.360
get started with message back. Are you accepting PRs if they want to contribute and what's,


00:57:46.360 --> 00:57:47.360
what do you tell them?


00:57:47.360 --> 00:57:51.080
First, I encourage people to try it out. I am available, you know, to answer questions


00:57:51.080 --> 00:57:55.960
on GitHub and stuff. It is obviously a hobby project. So, you know, if the, the usage bandwidth


00:57:55.960 --> 00:57:59.720
increases significantly, we'll have to get some more maintainers on and hopefully we


00:57:59.720 --> 00:58:01.600
can make this more maintainable over time.


00:58:01.600 --> 00:58:07.200
But once the sponsor funds exceed a 10,000, 20, 30,000 a month, like it'll revalue your,


00:58:07.200 --> 00:58:08.200
no, just kidding.


00:58:08.200 --> 00:58:12.800
Sure. Sure. But yeah, please try it out. Things work should be hopefully faster than what


00:58:12.800 --> 00:58:16.640
you're currently using and hopefully intuitive to use. We've done a lot of work to make sure


00:58:16.640 --> 00:58:17.640
the API is friendly.


00:58:17.640 --> 00:58:20.800
Yeah. It looks pretty easy to get started with. The docs are really good.


00:58:20.800 --> 00:58:21.800
Oh, thank you.


00:58:21.800 --> 00:58:24.800
Congrats on the cool project. Thanks for taking the time to come on the show and tell everyone


00:58:24.800 --> 00:58:25.800
about it.


00:58:25.800 --> 00:58:26.800
Thanks.


00:58:26.800 --> 00:58:27.800
Yeah. See you later.


00:58:27.800 --> 00:58:28.800
Bye.


00:58:28.800 --> 00:58:32.280
This has been another episode of Talk Python to Me. Thank you to our sponsors. Be sure


00:58:32.280 --> 00:58:37.040
to check out what they're offering. It really helps support the show. This episode is sponsored


00:58:37.040 --> 00:58:41.920
by Posit Connect from the makers of Shiny. Publish, share and deploy all of your data


00:58:41.920 --> 00:58:48.080
projects that you're creating using Python. Streamlit, Dash, Shiny, Bokeh, FastAPI, Flask,


00:58:48.080 --> 00:58:53.880
Quatro, Reports, Dashboards and APIs. Posit Connect supports all of them. Try Posit Connect


00:58:53.880 --> 00:58:57.880
for free by going to talkpython.fm/posit. P-O-S-I-T.


00:58:57.880 --> 00:59:03.780
Want to level up your Python? We have one of the largest catalogs of Python video courses


00:59:03.780 --> 00:59:08.600
over at Talk Python. Our content ranges from true beginners to deeply advanced topics like


00:59:08.600 --> 00:59:13.120
memory and async. And best of all, there's not a subscription in sight. Check it out


00:59:13.120 --> 00:59:18.280
for yourself at training.talkpython.fm. Be sure to subscribe to the show. Open your favorite


00:59:18.280 --> 00:59:22.880
podcast app and search for Python. We should be right at the top. You can also find the


00:59:22.880 --> 00:59:29.800
iTunes feed at /iTunes, the Google Play feed at /play, and the direct RSS feed at /rss


00:59:29.800 --> 00:59:34.360
on talkpython.fm. We're live streaming most of our recordings these days. If you want


00:59:34.360 --> 00:59:38.400
to be part of the show and have your comments featured on the air, be sure to subscribe


00:59:38.400 --> 00:59:44.200
to our YouTube channel at talkpython.fm/youtube. This is your host, Michael Kennedy. Thanks


00:59:44.200 --> 00:59:47.760
so much for listening. I really appreciate it. Now get out there and write some Python


00:59:47.760 --> 00:59:48.120
code.


00:59:49.040 --> 00:59:51.040
[MUSIC]


00:59:51.040 --> 01:00:01.040
[END]


01:00:01.040 --> 01:00:08.040
[MUSIC]


01:00:08.040 --> 01:00:15.000
[BLANK_AUDIO]

