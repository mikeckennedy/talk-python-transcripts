WEBVTT

00:00:00.001 --> 00:00:02.260
Do you have stateless code that needs to run in the cloud?

00:00:02.260 --> 00:00:08.720
The clear answer years ago was to create an HTTP or even GASP, a SOAP service before then.

00:00:08.720 --> 00:00:13.900
While HTTP services are still very important, some of this code can be moved entirely away

00:00:13.900 --> 00:00:18.700
from the framework that runs it and over to serverless programming and hosted functions.

00:00:18.700 --> 00:00:23.060
On this episode, I meet up with Asavri Tayal to discuss serverless programming in the cloud.

00:00:23.060 --> 00:00:29.820
This is Talk Python to Me, episode 218, recorded live at Microsoft Build on May 8, 2019.

00:00:29.820 --> 00:00:47.740
Welcome to Talk Python to Me, a weekly podcast on Python, the language, the libraries, the

00:00:47.740 --> 00:00:49.200
ecosystem, and the personalities.

00:00:49.200 --> 00:00:51.140
This is your host, Michael Kennedy.

00:00:51.140 --> 00:00:53.260
Follow me on Twitter where I'm @mkennedy.

00:00:53.260 --> 00:00:58.400
Keep up with the show and listen to past episodes at talkpython.fm and follow the show on Twitter

00:00:58.400 --> 00:00:59.520
via at talkpython.

00:01:00.220 --> 00:01:04.520
This episode is brought to you by Command Line Heroes from Red Hat and Datadog.

00:01:04.520 --> 00:01:06.680
Please check out what they're offering during their segments.

00:01:06.680 --> 00:01:08.000
It really helps support the show.

00:01:08.000 --> 00:01:09.960
Asavri, welcome to Talk Python.

00:01:09.960 --> 00:01:10.560
Hi, Michael.

00:01:10.560 --> 00:01:11.140
Thank you.

00:01:11.140 --> 00:01:12.100
It's super to have you here.

00:01:12.100 --> 00:01:15.180
It's great to meet you live at Build.

00:01:15.180 --> 00:01:17.580
So it's going to be a lot of fun to talk about serverless.

00:01:17.580 --> 00:01:18.780
Yeah, excited to be here.

00:01:18.780 --> 00:01:19.280
Yeah, it's cool.

00:01:19.280 --> 00:01:23.880
So it feels like everyone I'm having on the show lately is doing this like sort of dual

00:01:23.880 --> 00:01:24.920
life, right?

00:01:24.980 --> 00:01:28.760
Like we're at PyCon and then now are at Microsoft Build.

00:01:28.760 --> 00:01:29.160
Yep.

00:01:29.160 --> 00:01:30.000
Same for you?

00:01:30.000 --> 00:01:30.600
Same for me.

00:01:30.600 --> 00:01:34.140
We were just in Cleveland at PyCon and it was great.

00:01:34.140 --> 00:01:38.460
So now coming back to Seattle back home and enjoying Build.

00:01:38.460 --> 00:01:39.020
Yeah.

00:01:39.240 --> 00:01:40.520
Both of those are great conferences.

00:01:40.520 --> 00:01:41.920
What was your thoughts on PyCon?

00:01:41.920 --> 00:01:42.640
Did you enjoy it?

00:01:42.640 --> 00:01:43.440
You had a good time there?

00:01:43.440 --> 00:01:45.580
Yeah, this was my second PyCon ever.

00:01:45.580 --> 00:01:46.920
Definitely a lot of fun.

00:01:46.920 --> 00:01:50.920
I really like the sense of community that Python developers have.

00:01:50.920 --> 00:01:53.820
It's different from all the marketing conferences we go to.

00:01:53.820 --> 00:01:55.460
You know, Build being one of them.

00:01:55.460 --> 00:01:57.380
So always a pleasant experience.

00:01:57.380 --> 00:01:58.180
Always good.

00:01:58.180 --> 00:01:58.400
Yeah.

00:01:58.480 --> 00:02:04.220
Yeah, I would say one of the biggest differences I felt was when you walk onto the expo floor

00:02:04.220 --> 00:02:09.140
at PyCon, it's like everybody there is kind of doing their own thing, their own separate

00:02:09.140 --> 00:02:09.540
thing.

00:02:09.540 --> 00:02:14.040
It's just a sampling of the community, whereas places like Build, they're great, but they're

00:02:14.040 --> 00:02:16.500
like, it's more like, here's the team for this.

00:02:16.500 --> 00:02:17.680
So here's the team for that, right?

00:02:17.680 --> 00:02:19.320
It's much more sort of focused.

00:02:19.320 --> 00:02:19.720
Great.

00:02:19.800 --> 00:02:23.460
I like the fact that at PyCon, everybody was there to share their stories of, hey,

00:02:23.460 --> 00:02:25.260
this is how we use Python.

00:02:25.260 --> 00:02:26.320
This is what we build.

00:02:26.320 --> 00:02:28.000
And maybe it's useful for you as well.

00:02:28.000 --> 00:02:29.740
So I'm going to share my story with you.

00:02:29.740 --> 00:02:32.420
That was definitely my favorite part of PyCon.

00:02:32.420 --> 00:02:33.140
Yeah, that's awesome.

00:02:33.140 --> 00:02:34.520
I had a great time there as well.

00:02:34.520 --> 00:02:35.820
I always enjoy that conference.

00:02:35.820 --> 00:02:39.280
So let's go ahead and get started with your story, though.

00:02:39.280 --> 00:02:40.540
How did you get into programming in Python?

00:02:40.540 --> 00:02:44.160
Yeah, I started web development when I was 15.

00:02:44.160 --> 00:02:49.200
And definitely my first programming language was JavaScript.

00:02:49.640 --> 00:02:55.660
But eventually, as I got exposed to Python, and I remember, I think my first web application

00:02:55.660 --> 00:03:03.320
that I developed was in Django, was developing like a portal of some kind for farmers to go

00:03:03.320 --> 00:03:08.580
up and look up various great imaging systems that was being developed by a computer vision

00:03:08.580 --> 00:03:08.880
lab.

00:03:08.880 --> 00:03:11.380
So anyway, that's how I started Python.

00:03:11.380 --> 00:03:17.540
And eventually, when there was an opportunity to make Azure great for Python developers, it

00:03:17.540 --> 00:03:23.220
just happened that I was there and I had some experience with Python and was basically asked

00:03:23.220 --> 00:03:25.460
to take over Python for functions and app service.

00:03:25.460 --> 00:03:26.520
Yeah, that's super cool.

00:03:26.520 --> 00:03:27.900
Tell me more about this Django app.

00:03:27.900 --> 00:03:31.540
Yeah, so the Django app I was building was back in college.

00:03:31.700 --> 00:03:34.900
I was working with the University of Illinois computer vision laboratory.

00:03:34.900 --> 00:03:43.120
And they were building a algorithm that could take as input the image of some grain and basically

00:03:43.120 --> 00:03:48.380
identify the quality of that grain based on a machine learning model or a computer vision

00:03:48.380 --> 00:03:48.740
model.

00:03:49.300 --> 00:03:52.440
So I wasn't as involved with the data science part of things.

00:03:52.440 --> 00:03:56.860
But at the end of the day, the algorithm needed to plug into a portal that the farmer could

00:03:56.860 --> 00:04:01.160
go interact with, plug in their grain and get market value for it.

00:04:01.160 --> 00:04:04.380
So that portal was something I built using Django.

00:04:04.380 --> 00:04:06.580
Yeah, that sounds like a really fun first project, actually.

00:04:06.580 --> 00:04:07.020
Yeah.

00:04:07.020 --> 00:04:07.440
Nice.

00:04:07.440 --> 00:04:09.480
Okay, so that was then.

00:04:09.480 --> 00:04:11.920
And did you study computer science at University of Illinois?

00:04:11.920 --> 00:04:12.420
Yes.

00:04:12.420 --> 00:04:13.100
Yeah, cool.

00:04:13.780 --> 00:04:16.940
And now you're at Microsoft working on the Azure team.

00:04:16.940 --> 00:04:17.320
Yes.

00:04:17.320 --> 00:04:18.420
Tell me what you do there.

00:04:18.420 --> 00:04:21.340
Yeah, so I'm a product manager on Azure Functions.

00:04:21.340 --> 00:04:27.420
And currently, I'm leading both the Python experience on Azure Functions and Java experience

00:04:27.420 --> 00:04:27.920
as well.

00:04:27.920 --> 00:04:34.000
So really, what that means is everything that needs to be built, either on the platform tooling

00:04:34.000 --> 00:04:36.840
or the runtime, I go in and build.

00:04:36.840 --> 00:04:43.620
So trying to understand the story of the journey of a Python developer on the Azure Functions

00:04:43.620 --> 00:04:46.200
platform, how they come in, what their problems are.

00:04:46.200 --> 00:04:51.640
So on a daily basis, speaking to lots and lots of customers about why they're trying to use

00:04:51.640 --> 00:04:55.220
serverless, why they're trying to use functions, what are some problems they face today and how

00:04:55.220 --> 00:04:56.020
we can solve them.

00:04:56.020 --> 00:05:01.340
And really taking those requirements back to the engineering team, building POCs and prioritizing

00:05:01.340 --> 00:05:02.280
features on the product.

00:05:02.280 --> 00:05:02.940
Yeah, that's cool.

00:05:02.940 --> 00:05:05.160
Did you have a lot of those conversations at PyCon?

00:05:05.160 --> 00:05:06.300
At PyCon, yes.

00:05:06.300 --> 00:05:11.900
Also at Build the last two days, it's great to see that the number of folks who are developing

00:05:11.900 --> 00:05:14.560
with Python are increasing at Build every year.

00:05:14.560 --> 00:05:15.660
Yeah, it's pretty cool.

00:05:15.660 --> 00:05:18.600
Did you guys have like an expo stand area?

00:05:18.600 --> 00:05:19.280
We do.

00:05:19.280 --> 00:05:23.940
It's a pretty, there's a lot of hustle bustle around the functions stand.

00:05:23.940 --> 00:05:26.500
It's been great to see all the interest, yeah.

00:05:26.500 --> 00:05:28.000
Yeah, I'm sure it's a fun area to work on.

00:05:28.000 --> 00:05:28.900
Yep, for sure.

00:05:28.900 --> 00:05:29.180
Cool.

00:05:29.180 --> 00:05:34.700
So before we get into the details of Azure Functions and the Python story, let's focus

00:05:34.700 --> 00:05:35.800
on just serverless.

00:05:35.800 --> 00:05:36.800
We had servers.

00:05:36.800 --> 00:05:42.120
And then we had the cloud, we had kind of virtual servers, and then we have serverless.

00:05:42.120 --> 00:05:43.220
Walk us through that.

00:05:43.220 --> 00:05:43.600
Right.

00:05:43.600 --> 00:05:49.500
So we almost think of serverless as the evolution of application development on the cloud.

00:05:49.500 --> 00:05:55.560
You know, we used to have on-prem or on-premise, and then we moved on to IaaS, then we moved

00:05:55.560 --> 00:05:56.420
on to PaaS.

00:05:56.580 --> 00:05:59.720
And now serverless is even the next level of PaaS.

00:05:59.720 --> 00:06:05.620
What that means is, hey, PaaS already took away your problems of having to manage physical

00:06:05.620 --> 00:06:07.320
hardware when it comes to servers.

00:06:07.320 --> 00:06:10.680
Now you only need to worry about what's running on that hardware.

00:06:10.920 --> 00:06:16.220
But you still needed to worry about what OS you're using, what packages or dependencies are installed

00:06:16.220 --> 00:06:17.160
on that OS.

00:06:17.480 --> 00:06:21.120
patching those OS is upgrading those machines.

00:06:21.120 --> 00:06:32.180
serverless takes away all of that because at the end of the day, what you need to do as a

00:06:32.180 --> 00:06:37.240
developer is give us your application code, and we'll run it in the cloud for you.

00:06:37.240 --> 00:06:43.120
Very important concept in serverless is that you also no longer need to estimate how many

00:06:43.120 --> 00:06:46.060
app servers do you need to run your application.

00:06:46.060 --> 00:06:49.660
So typically when you were building an app, you would have to think, hey, my app's going

00:06:49.660 --> 00:06:51.260
to get 20,000 requests.

00:06:51.260 --> 00:06:57.700
And so I estimate I'll need about maybe 100 servers to serve those requests coming in.

00:06:57.700 --> 00:07:03.320
You no longer have to do that with serverless because we're going to automatically scale based

00:07:03.320 --> 00:07:05.440
on events that your application is receiving.

00:07:05.440 --> 00:07:05.860
Right.

00:07:05.860 --> 00:07:10.040
You don't have to worry about memory, like your server running out of memory because it's

00:07:10.040 --> 00:07:11.660
doing too much and things like that, right?

00:07:11.660 --> 00:07:11.940
Exactly.

00:07:11.940 --> 00:07:13.160
Just at the individual level, maybe.

00:07:13.160 --> 00:07:13.760
Exactly.

00:07:13.760 --> 00:07:17.500
And that's why when we talk about serverless, we actually also start to talk about event-driven

00:07:17.500 --> 00:07:22.060
serverless, which is really important because if you write an application that's going to

00:07:22.060 --> 00:07:27.540
be triggered off an event in the cloud, something like an HTTP request or an event in some source

00:07:27.540 --> 00:07:31.520
system somewhere, could be storage, could be an eventing mechanism, we are constantly

00:07:31.520 --> 00:07:32.740
watching for those events.

00:07:32.740 --> 00:07:37.120
And when those events occur, we'll go ahead and spin the resources you need to run your

00:07:37.120 --> 00:07:37.700
application.

00:07:37.700 --> 00:07:41.160
And when more of those events happen, we go spin more resources.

00:07:41.160 --> 00:07:45.240
So you're really never out of compute or memory in that matter.

00:07:45.240 --> 00:07:46.020
Yeah, that's pretty cool.

00:07:46.140 --> 00:07:48.840
So maybe some scenario like that fits in my world.

00:07:48.840 --> 00:07:52.940
Maybe I have some videos from an online course, but they're in a raw form.

00:07:52.940 --> 00:07:55.600
I want to get them transcoded for the right kind of streaming.

00:07:55.600 --> 00:08:00.560
I could upload them to like a storage that could trigger an event, a serverless function, which

00:08:00.560 --> 00:08:04.660
grabs it, runs it through a transcoder service, and then drops it back maybe like in a file

00:08:04.660 --> 00:08:05.900
in a location and something like this.

00:08:05.900 --> 00:08:06.040
Yeah.

00:08:06.260 --> 00:08:06.620
Exactly.

00:08:06.620 --> 00:08:07.100
Both.

00:08:07.100 --> 00:08:10.260
I think in the scenario that you described, there were two important factors.

00:08:10.260 --> 00:08:12.000
One, your scenario was event-driven.

00:08:12.000 --> 00:08:16.980
You were going to kick it off and then process something and drop it in a queue somewhere.

00:08:16.980 --> 00:08:22.420
And the other one that it seemed like it was a lightweight job that you maybe didn't need

00:08:22.420 --> 00:08:24.360
to stand dedicated servers up for.

00:08:24.360 --> 00:08:24.620
Right.

00:08:24.700 --> 00:08:25.820
Can you do like timing?

00:08:25.820 --> 00:08:28.160
Like, could I say run this function every hour?

00:08:28.160 --> 00:08:29.020
Yes, absolutely.

00:08:29.020 --> 00:08:30.340
We've got the timer trigger.

00:08:30.340 --> 00:08:34.700
So if you go ahead and set the time that you'd like your function to run on, we can always

00:08:34.700 --> 00:08:34.980
do that.

00:08:34.980 --> 00:08:35.220
Cool.

00:08:35.220 --> 00:08:39.060
So I guess maybe I would put that into like an automation category.

00:08:39.060 --> 00:08:40.980
Like what are some of the other main scenarios?

00:08:40.980 --> 00:08:42.360
I guess this was automation.

00:08:42.360 --> 00:08:48.080
The other two scenarios that we talk about with Python, the first one is web scripting or APIs.

00:08:48.080 --> 00:08:53.920
If you, for instance, had a Cosmos DB instance and you wanted to go ahead and stand up

00:08:53.920 --> 00:08:58.540
a REST API in front of that Cosmos DB, you could do that using functions.

00:08:58.540 --> 00:08:58.820
Okay.

00:08:58.820 --> 00:09:02.720
So it might be like an easy way to build an API endpoint that serves up JSON or something

00:09:02.720 --> 00:09:03.020
like that.

00:09:03.020 --> 00:09:03.460
Exactly.

00:09:03.460 --> 00:09:06.940
How would I, I guess, in the Python space, what technology would I use that?

00:09:06.940 --> 00:09:12.660
Like if I were building an API and I was hosting it in uWSGI, I would do that with Flask

00:09:12.660 --> 00:09:16.000
or Pyramid or API star or something like that.

00:09:16.000 --> 00:09:20.080
Like, do I work at that level in these functions or like what's the story there?

00:09:20.080 --> 00:09:20.460
Yeah.

00:09:20.580 --> 00:09:25.840
So when you develop a function, you would have to pick a trigger for your function, right?

00:09:25.840 --> 00:09:27.920
That's the event that we were talking about earlier.

00:09:28.120 --> 00:09:33.060
In this case, as long as you tell us, hey, it's going to be an HTTP trigger, you configure

00:09:33.060 --> 00:09:35.080
the parameters you want for that request.

00:09:35.080 --> 00:09:41.260
Things like, are we accepting a get post request or are we authenticating against an API key

00:09:41.260 --> 00:09:43.200
or an AED or no auth to mechanism?

00:09:43.200 --> 00:09:47.820
As long as you give us that configuration and give us the Python script that you'd like to

00:09:47.820 --> 00:09:52.380
run when that trigger occurs, we'd be able to run it as a function for you.

00:09:52.380 --> 00:09:55.800
You no longer have to go in and define droughts in a Flask app.

00:09:55.860 --> 00:09:56.020
Right.

00:09:56.020 --> 00:09:57.480
Do I return a string?

00:09:57.480 --> 00:09:58.400
Do I return a dictionary?

00:09:58.400 --> 00:09:59.380
Like, what do I return?

00:09:59.380 --> 00:10:00.100
It's up to you.

00:10:00.100 --> 00:10:03.720
We have this concept called bindings and functions.

00:10:03.720 --> 00:10:10.100
So you can actually return or you can bind your return value to a data source.

00:10:10.100 --> 00:10:14.720
So for instance, if you were to say, hey, I'd like an output binding for my function app,

00:10:14.720 --> 00:10:17.840
which was Azure Q Storage, for instance, right?

00:10:17.940 --> 00:10:20.280
And I'd like to bind that to the return value of function.

00:10:20.280 --> 00:10:26.680
Now, whatever you return, whether it's a string value or a byte array or a JSON string or a stream,

00:10:26.680 --> 00:10:31.400
we'd go ahead and use that to create a message in Azure Storage Q.

00:10:31.400 --> 00:10:31.720
Okay.

00:10:31.720 --> 00:10:32.420
That's pretty cool.

00:10:32.420 --> 00:10:34.400
That's the website.

00:10:34.400 --> 00:10:35.460
And what's the third one?

00:10:35.540 --> 00:10:39.640
The third one that we're actually seeing very often is ML inferencing.

00:10:39.640 --> 00:10:47.340
So I've mentioned I've been talking to a lot of customers for serverless, specifically Python applications and serverless.

00:10:47.700 --> 00:10:50.340
And really seeing two personas.

00:10:50.340 --> 00:10:56.140
One, a lot of data scientists who are coming to us and telling us, hey, I trained a machine learning model.

00:10:56.140 --> 00:10:58.820
Now I'd like to deploy it as a web service.

00:10:58.820 --> 00:11:05.200
Serverless sounds like I no longer need to worry about all those things that typically I don't care about.

00:11:05.200 --> 00:11:08.200
And I would have to go spend time on figuring out.

00:11:08.200 --> 00:11:08.480
Right.

00:11:08.480 --> 00:11:14.080
As a data scientist, you don't care about like configuring micro WSGI for high performance or whatever.

00:11:14.080 --> 00:11:16.840
I'm not a data scientist, but I imagine for a data scientist,

00:11:16.840 --> 00:11:18.140
those problems are not even fun.

00:11:18.140 --> 00:11:22.800
You like thinking about the algorithm you want to use for training your model or the libraries you want to use.

00:11:22.800 --> 00:11:27.920
Probably not about what packages to install or when to security patch your OS, right?

00:11:27.920 --> 00:11:28.540
Yeah, yeah.

00:11:28.540 --> 00:11:33.140
Serverless is a great scenario there because you give us your model, you give us your scoring script,

00:11:33.140 --> 00:11:38.660
tell us what event to use, and we'll go ahead and provision the application for you in the cloud, right?

00:11:38.660 --> 00:11:44.300
And then there are also second kind of category of the people or customers that we speak to who are,

00:11:44.520 --> 00:11:47.420
saying maybe not mainstream Python developers.

00:11:47.420 --> 00:11:52.500
They might say, hey, we work at a large enterprise and we're using Java for our mainframe application.

00:11:52.500 --> 00:11:56.520
But our data science team gave us this thing called a model,

00:11:56.520 --> 00:12:01.740
and we need to kind of run it in our application or use it in our application to make some predictions.

00:12:01.740 --> 00:12:04.080
Doesn't look like I can do that with Java.

00:12:04.220 --> 00:12:05.760
I'd like to do that with Python.

00:12:05.760 --> 00:12:11.980
Serverless is a great way to create a microservice that you can invoke via an HTTP endpoint,

00:12:11.980 --> 00:12:17.240
all written in Python, that you can call from your mainframe Java application if you want to do it.

00:12:17.240 --> 00:12:17.960
That sounds pretty cool.

00:12:17.960 --> 00:12:22.880
Do you find students using it a lot for like testing out their code or creating little projects?

00:12:22.880 --> 00:12:23.720
That's a good one.

00:12:23.980 --> 00:12:27.220
I still think serverless is very new to the student community.

00:12:27.220 --> 00:12:30.460
We're trying to create more awareness in that space.

00:12:30.460 --> 00:12:38.180
But definitely spoken to a couple of people who've taken an interest and started to use serverless more on the cognitive services side of things.

00:12:38.180 --> 00:12:40.060
But I think we still have a long way to go there.

00:12:40.060 --> 00:12:44.560
There's always kind of a lag between academic courses and stuff like that, right?

00:12:44.560 --> 00:12:47.100
They don't seem to change technology super fast.

00:12:47.100 --> 00:12:48.660
So I can imagine that.

00:12:48.660 --> 00:12:54.540
So you're talking about Azure Functions, and obviously it supports Python, and that's great.

00:12:54.540 --> 00:12:58.540
But just to kind of round out the story, like what other languages does it support?

00:12:58.540 --> 00:13:07.740
Yeah, Functions supports JavaScript, TypeScript, .NET, Java, and we recently announced PowerShell as of a week ago.

00:13:07.740 --> 00:13:08.480
That sounds pretty cool.

00:13:09.060 --> 00:13:12.940
What does a workflow look like if I write regular functions in a Python file?

00:13:12.940 --> 00:13:20.480
I know what that is, but I don't edit this like on Azure or AWS or wherever I'm working on with my functions.

00:13:20.480 --> 00:13:22.760
Like what does the workflow kind of look like there?

00:13:22.760 --> 00:13:23.340
Give me a sense.

00:13:23.340 --> 00:13:27.880
To help you develop functions, we basically have two tooling options.

00:13:27.880 --> 00:13:31.900
And they sort of walk you through the process of building a function.

00:13:31.900 --> 00:13:37.560
The first one is we've got an extension with Visual Studio Code where you can go in and say,

00:13:37.680 --> 00:13:40.340
hey, I'd like to create a project, which is a function project.

00:13:40.340 --> 00:13:42.140
Pick a language for your project.

00:13:42.140 --> 00:13:43.060
That would be Python.

00:13:43.060 --> 00:13:45.560
Pick a trigger for your very first function.

00:13:45.560 --> 00:13:47.940
It can be an HTTP trigger.

00:13:47.940 --> 00:13:51.500
And then it'll populate a template for you that you can start modifying.

00:13:52.140 --> 00:13:56.000
So when I say a template, it's basically a combination of two files.

00:13:56.160 --> 00:14:00.260
One's a template Python script, which pretty much has a single function.

00:14:00.260 --> 00:14:01.420
It's a hello world function.

00:14:01.420 --> 00:14:04.860
Takes an HTTP request, returns hello name as the response.

00:14:04.860 --> 00:14:05.140
Right.

00:14:05.140 --> 00:14:06.900
So that's where you write your normal code, right?

00:14:06.900 --> 00:14:07.440
Exactly.

00:14:07.440 --> 00:14:11.580
You can do pretty much what you want between the beginning and end there, right?

00:14:11.580 --> 00:14:13.800
Like long as it conforms to the shape.

00:14:13.900 --> 00:14:14.260
Exactly.

00:14:14.260 --> 00:14:15.620
You can do pretty much what you want.

00:14:15.620 --> 00:14:19.360
You can also load up additional modules within that folder itself.

00:14:19.360 --> 00:14:22.700
So you don't have to have all your Python code living in that file.

00:14:22.700 --> 00:14:27.360
If you had dependencies or in our ML case, if you wanted to throw in a model as a dependency

00:14:27.360 --> 00:14:30.820
in your function, you can go ahead and add all of the data within your function project.

00:14:30.820 --> 00:14:32.480
And what about packages I'm using?

00:14:32.480 --> 00:14:35.580
Like suppose I'm using requests or NumPy or whatever.

00:14:35.780 --> 00:14:36.000
Right.

00:14:36.000 --> 00:14:39.360
So we've got a requirements TXT format that we support.

00:14:39.360 --> 00:14:42.860
You automatically get that when you generate a new function project.

00:14:42.860 --> 00:14:47.240
Just go and type in your packages and the versions you need and we'll go ahead and install them

00:14:47.240 --> 00:14:48.240
for you when you deploy.

00:14:48.240 --> 00:14:48.880
Yeah, that's cool.

00:14:48.880 --> 00:14:52.760
So basically it creates a little virtual environment for the thing.

00:14:52.760 --> 00:14:53.160
Right.

00:14:53.160 --> 00:14:57.000
When you're developing locally, we actually leave it up to you for how you'd like to manage

00:14:57.000 --> 00:14:57.700
your dependencies.

00:14:57.700 --> 00:15:03.500
As long as you're running in some environment where the Python path does have the packages you

00:15:03.500 --> 00:15:06.940
need, we'll be able to identify the packages on that path and run.

00:15:06.940 --> 00:15:12.600
So we leave it up to you whether using VN, FIPN, the global Python installed in your machine

00:15:12.600 --> 00:15:16.420
definitely feels like Python developers are passionate about how they manage their virtual

00:15:16.420 --> 00:15:16.900
environments.

00:15:16.900 --> 00:15:17.300
For sure.

00:15:17.300 --> 00:15:21.820
In the cloud, the only contract is that you give us a requirements TXT file and we'll use

00:15:21.820 --> 00:15:22.880
that to install the packages.

00:15:22.880 --> 00:15:28.500
This portion of Talk Python to Me is brought to you by Command Line Heroes.

00:15:28.500 --> 00:15:33.420
Command Line Heroes is an original podcast from Red Hat and it's back for its third

00:15:33.420 --> 00:15:33.760
season.

00:15:33.760 --> 00:15:37.260
This one is all about the epic history of programming languages.

00:15:37.260 --> 00:15:41.640
The very first episode explores the origin and evolution of Python.

00:15:41.640 --> 00:15:44.680
Let me tell you, this show is really good.

00:15:44.680 --> 00:15:46.020
It has a great host.

00:15:46.020 --> 00:15:47.400
It's highly produced and edited.

00:15:47.400 --> 00:15:50.160
Imagine if Radiolab made a tech podcast.

00:15:50.160 --> 00:15:51.340
Yeah, it'd be like that.

00:15:51.340 --> 00:15:55.920
Even better, this particular episode has a bunch of cool Python personalities such as Guido

00:15:55.920 --> 00:15:58.200
van Rossum and I even make an appearance in there.

00:15:58.900 --> 00:16:04.180
Listen and subscribe wherever you get your podcasts or just visit talkpython.fm/heroes.

00:16:04.180 --> 00:16:06.880
That's talkpython.fm/heroes.

00:16:08.240 --> 00:16:11.180
Do you like pre-compute that little environment or something?

00:16:11.180 --> 00:16:14.480
So, because you don't want to obviously reinstall it per request.

00:16:14.480 --> 00:16:14.780
Yeah.

00:16:14.780 --> 00:16:18.700
So, it basically gets installed during deployment time.

00:16:18.700 --> 00:16:18.980
Yeah.

00:16:19.080 --> 00:16:23.560
When you deploy your function, we take your code, install the dependencies that it needs, package

00:16:23.560 --> 00:16:26.740
it up as a zip package, store it in Azure storage.

00:16:26.740 --> 00:16:31.820
And when your applications are invoked or the first call comes in, that's when we go ahead

00:16:31.820 --> 00:16:34.340
and run that zip package on the platform.

00:16:34.480 --> 00:16:35.000
How's it run?

00:16:35.000 --> 00:16:36.820
Does it run like in a Docker container?

00:16:36.820 --> 00:16:38.540
Does it run in like a VM?

00:16:38.540 --> 00:16:39.860
How is it shared?

00:16:39.860 --> 00:16:41.780
Like, what's that story look like?

00:16:41.780 --> 00:16:42.440
Good question.

00:16:42.440 --> 00:16:44.640
You shouldn't worry about it because it's serverless.

00:16:44.640 --> 00:16:47.280
But since you asked nicely, I'll share it with you.

00:16:47.280 --> 00:16:50.140
We do run in a container underneath the covers.

00:16:50.140 --> 00:16:55.620
Good thing is our Python offering is actually running on Linux, which is something that's brand

00:16:55.620 --> 00:16:56.740
new to Azure Functions.

00:16:56.740 --> 00:16:59.920
Traditionally, we only supported Windows hosting.

00:17:00.420 --> 00:17:05.640
Just given the kinds of scenarios Python developers are bringing in, we decided to go ahead and

00:17:05.640 --> 00:17:07.100
host Python on Linux.

00:17:07.100 --> 00:17:12.040
So, underneath the covers, things are running in a Docker container on Linux.

00:17:12.040 --> 00:17:15.100
We actually go ahead and even open source that image.

00:17:15.100 --> 00:17:19.040
It's called the Azure Functions Python image on our Docker Hub profile.

00:17:19.040 --> 00:17:22.380
And you can use that to build your own custom container.

00:17:22.380 --> 00:17:27.020
Or if you're just giving us code, we'll go ahead and run that code in that container for you.

00:17:27.020 --> 00:17:27.400
That's cool.

00:17:27.400 --> 00:17:29.160
Could I use it to like test locally?

00:17:29.360 --> 00:17:33.900
Could I, you know, Docker pull, Docker run that image locally and play with it?

00:17:33.900 --> 00:17:35.440
Yeah, you can absolutely do that.

00:17:35.440 --> 00:17:39.560
If you wanted to have the container approach, the cool thing is you don't need to.

00:17:39.560 --> 00:17:39.820
Right.

00:17:39.820 --> 00:17:42.980
We've got something called the Azure Functions Core Tools.

00:17:42.980 --> 00:17:44.260
It's a CLI experience.

00:17:44.260 --> 00:17:49.200
As long as you install the Azure Functions Core Tools on your local machine and VS Code, in

00:17:49.200 --> 00:17:50.280
fact, does that for you.

00:17:50.280 --> 00:17:55.940
You can test your functions locally simply by hitting F5 in VS Code.

00:17:55.940 --> 00:17:58.340
It automatically starts the function process.

00:17:58.340 --> 00:18:04.320
In fact, it'll even identify that it's a function process and go ahead and attach the Python debugger to the functions process.

00:18:04.720 --> 00:18:11.880
So you can start serving your function on the local host endpoint and start testing against real Azure events.

00:18:11.880 --> 00:18:12.820
That sounds pretty cool.

00:18:12.820 --> 00:18:14.900
I saw that it had a command line.

00:18:14.900 --> 00:18:20.780
So you can do things like go in and say like it's the funk command, right?

00:18:20.780 --> 00:18:22.220
So you can go and say funk new.

00:18:22.760 --> 00:18:24.640
And then you can say, what is it?

00:18:24.640 --> 00:18:26.580
Funk run or something like this to like.

00:18:26.580 --> 00:18:26.860
Yep.

00:18:26.860 --> 00:18:27.680
Funk host start.

00:18:27.680 --> 00:18:27.960
Yeah.

00:18:27.960 --> 00:18:28.800
Funk host start.

00:18:28.800 --> 00:18:29.160
That's right.

00:18:29.160 --> 00:18:29.780
That sounds funky.

00:18:29.780 --> 00:18:30.480
Yeah.

00:18:30.480 --> 00:18:31.520
So yeah.

00:18:31.680 --> 00:18:33.700
So you can do that in the command line as well.

00:18:33.700 --> 00:18:35.300
Like if you don't use Jules Chico.

00:18:35.300 --> 00:18:36.200
That sounds pretty cool.

00:18:36.200 --> 00:18:36.720
Absolutely.

00:18:36.720 --> 00:18:41.000
So you can use the command line in conjunction with your favorite editor.

00:18:41.000 --> 00:18:44.600
So if you were using PyCharm, that's what some of the users are using as well.

00:18:44.600 --> 00:18:44.860
Yeah.

00:18:44.860 --> 00:18:46.240
You can definitely go ahead and attach.

00:18:46.240 --> 00:18:46.560
Yeah.

00:18:46.560 --> 00:18:46.980
That's cool.

00:18:47.060 --> 00:18:51.040
Like one of the things that drives me crazy about working with the cloud, any cloud, right?

00:18:51.040 --> 00:18:55.660
Whatever is the sort of airplane scenario as I see it.

00:18:55.660 --> 00:18:56.240
All right.

00:18:56.240 --> 00:19:02.320
Like I'm working on this project and now I'm somewhere without internet or I'm in a hotel

00:19:02.320 --> 00:19:03.840
with bad internet or whatever.

00:19:03.840 --> 00:19:06.020
And it's just like, well, now you're done.

00:19:06.020 --> 00:19:07.780
You know, it doesn't work anymore.

00:19:07.780 --> 00:19:11.500
And so it's cool that you have this way to kind of like run it locally, like regardless

00:19:11.500 --> 00:19:12.400
of that.

00:19:12.400 --> 00:19:12.660
Yep.

00:19:12.660 --> 00:19:15.260
And it's the same runtime that's running in the cloud as well.

00:19:15.260 --> 00:19:16.880
So there's nothing different about it.

00:19:16.960 --> 00:19:17.200
Okay.

00:19:17.200 --> 00:19:18.980
Well, that seems pretty cool.

00:19:18.980 --> 00:19:20.980
We talked about some of the events, right?

00:19:20.980 --> 00:19:21.800
We talked about HTTP.

00:19:21.800 --> 00:19:25.980
We talked about, I asked if I could do it on a timer, like a cron job equivalent, which

00:19:25.980 --> 00:19:26.600
is pretty cool.

00:19:26.600 --> 00:19:27.420
What are some of the other ones?

00:19:27.420 --> 00:19:28.440
All the storage events.

00:19:28.440 --> 00:19:35.200
So Azure Storage, QBlob, Cosmos DB events, also the eventing and messaging systems.

00:19:35.200 --> 00:19:41.540
So Service Bus Queues, Service Bus Topics, Event Hubs, any IoT Hub messages coming to an

00:19:41.540 --> 00:19:43.080
Event Hub, Event Grid.

00:19:43.640 --> 00:19:47.620
And the cool thing is, as of two days ago, we also announced Kafka.

00:19:47.620 --> 00:19:50.000
Kafka triggers are supported as well.

00:19:50.000 --> 00:19:54.320
Going back to functions being open source, I feel like sometimes I can't say that enough.

00:19:54.320 --> 00:19:59.760
We've got an extensibility model where we do support any trigger or binding that you want

00:19:59.760 --> 00:20:00.400
to bring in.

00:20:00.400 --> 00:20:04.260
An example would be somebody recently wrote a SignalR binding for Python.

00:20:04.880 --> 00:20:09.920
And so pretty much any binding that you bring can plug into our extensibility model as well.

00:20:09.920 --> 00:20:10.400
That's pretty cool.

00:20:10.400 --> 00:20:11.420
Could I build a Slack one?

00:20:11.420 --> 00:20:13.740
So anytime a certain Slack message comes in?

00:20:13.740 --> 00:20:14.240
Yes.

00:20:14.240 --> 00:20:16.940
I actually just built a demo for that yesterday.

00:20:16.940 --> 00:20:17.380
Oh, really?

00:20:17.380 --> 00:20:19.080
We should talk about that.

00:20:19.080 --> 00:20:19.620
That's pretty awesome.

00:20:19.620 --> 00:20:22.120
Like, what was the scenario you were like laying out there?

00:20:22.220 --> 00:20:22.500
Yeah.

00:20:22.500 --> 00:20:26.800
So we wanted to build a demo when a GitHub issue is filed.

00:20:26.800 --> 00:20:31.540
I'd like to go ahead and process that issue, do some sentiment analysis on it, and go ahead

00:20:31.540 --> 00:20:33.080
and post to our Slack channel.

00:20:33.080 --> 00:20:35.880
So that was the simple scenario I was building.

00:20:35.880 --> 00:20:42.900
But definitely felt like just having those bindings to these data sources or just being

00:20:42.900 --> 00:20:46.580
able to write my Python code and not focus on anything else made things really easy for

00:20:46.580 --> 00:20:46.960
my demo.

00:20:47.060 --> 00:20:48.020
Of course, that's super cool.

00:20:48.020 --> 00:20:53.000
You mentioned the IoT stuff, and that seems like a really interesting angle, right?

00:20:53.000 --> 00:20:59.960
People create these little IoT devices, and they've got to be sitting back like tons of data in

00:20:59.960 --> 00:21:00.640
some cases.

00:21:00.640 --> 00:21:06.360
So I can just somehow subscribe to that in particular and not necessarily have to write an HTTP endpoint.

00:21:06.360 --> 00:21:07.680
What was the story there?

00:21:07.680 --> 00:21:08.120
Right.

00:21:08.120 --> 00:21:13.400
So the IoT events are generally routed through IoT Hub to Event Hub.

00:21:13.820 --> 00:21:17.560
So as long as these events are coming into Event Hub, we've got an Event Hub triggered.

00:21:17.560 --> 00:21:21.740
So every time a message is received on Event Hub, we'll go ahead and trigger or invoke your

00:21:21.740 --> 00:21:26.500
function script and pass it the message coming from or the data coming from your IoT device

00:21:26.500 --> 00:21:28.120
for the function to process.

00:21:28.120 --> 00:21:35.400
So that actually makes a very popular scenario for serverless as well, because IoT scenarios are

00:21:35.400 --> 00:21:37.020
generally burst nature.

00:21:37.360 --> 00:21:42.580
So you'll suddenly get 1,000 events that you need to process and spin up functions for,

00:21:42.580 --> 00:21:45.060
and then it might just go down to zero.

00:21:45.060 --> 00:21:50.140
So really, it doesn't probably make sense for you to be paying for VMs that are running 24

00:21:50.140 --> 00:21:53.080
hours to serve that one-time burst activity.

00:21:53.080 --> 00:21:53.420
Yeah, sure.

00:21:53.420 --> 00:21:56.080
Well, you talk about paying and pricing, right?

00:21:56.080 --> 00:22:02.020
Like, if I go and spin up a virtual machine, Linode, or somewhere like that, I know that

00:22:02.020 --> 00:22:06.140
it's $5 or $10 a month flat, right?

00:22:06.140 --> 00:22:07.560
Like, there's no surprises.

00:22:07.560 --> 00:22:10.720
If I leave it on, it's $10, let's say.

00:22:10.720 --> 00:22:12.780
What's the story with functions?

00:22:12.780 --> 00:22:18.040
I feel like of so many of the cloud resources, it's not always sure, but with functions, it's

00:22:18.040 --> 00:22:22.240
like really not clear all the time, like what is this going to cost?

00:22:22.240 --> 00:22:23.120
Right.

00:22:23.120 --> 00:22:24.780
There's a lot of factors, right?

00:22:24.780 --> 00:22:28.460
Not just how many times it's called, but how much work did each function do, things like

00:22:28.460 --> 00:22:28.740
this.

00:22:28.740 --> 00:22:31.020
How do I know, like, what this might cost?

00:22:31.020 --> 00:22:32.400
Or is there a free tier, things like that?

00:22:32.400 --> 00:22:32.660
Yeah.

00:22:32.660 --> 00:22:37.920
With functions, we actually support a few different hosting plans, and the price is typically tied

00:22:37.920 --> 00:22:39.440
to what hosting plan you pick.

00:22:39.440 --> 00:22:44.560
So the first and the most popular one is the serverless, or the truly serverless, or what

00:22:44.560 --> 00:22:45.920
we call the consumption plan.

00:22:46.340 --> 00:22:52.120
What that means is that when a function execution occurs, there's a flat rate for that function

00:22:52.120 --> 00:22:59.340
execution, and you only get charged for the compute that you use GB per second to run that

00:22:59.340 --> 00:23:00.420
function, right?

00:23:00.420 --> 00:23:03.500
So your actual compute consumption determines the cost.

00:23:03.500 --> 00:23:07.740
And of course, any other resources that you might be using during that consumption, memory,

00:23:07.740 --> 00:23:08.240
et cetera.

00:23:08.240 --> 00:23:10.560
That's the truly serverless plan.

00:23:10.560 --> 00:23:16.140
On that, I believe we give you, and I do need to go back and double check this once, but

00:23:16.140 --> 00:23:19.580
I know we give you a million executions free to get started.

00:23:19.580 --> 00:23:24.040
I forget the other thing that we also give you free with the memory.

00:23:24.040 --> 00:23:25.780
I should go back, double check that.

00:23:25.780 --> 00:23:29.040
The other hosting plan that we have is called an app service plan.

00:23:29.040 --> 00:23:33.640
And this is different from what a lot of the other serverless vendors actually provide, because

00:23:33.640 --> 00:23:38.340
we had a bunch of customers who told us, hey, I love the functions programming model,

00:23:38.920 --> 00:23:43.960
but you know, I already have the set of VMs that I'm using to run a web application.

00:23:43.960 --> 00:23:49.560
Maybe can I just use the extra compute available to me on those VMs to run my function as well,

00:23:49.560 --> 00:23:52.200
and scale within those set of VMs.

00:23:52.620 --> 00:23:54.520
And so that's what we call the app service plan.

00:23:54.520 --> 00:24:01.860
Hey, it's not truly serverless, but if you already have a set of N VMs, tell us about those VMs and what plan they're running on,

00:24:01.860 --> 00:24:05.800
call the app service plan, and we'll go ahead and run the function on that for you.

00:24:05.800 --> 00:24:07.800
It's not serverless scaling.

00:24:07.800 --> 00:24:09.240
It's CPU-based scaling.

00:24:09.400 --> 00:24:14.920
So it's scaling based on CPU metrics, how much CPU is being used or how much memory is being used.

00:24:14.920 --> 00:24:17.800
But it's kind of a determined set.

00:24:17.800 --> 00:24:21.920
It does give you advantages like now you can put your functions within a VNet.

00:24:21.920 --> 00:24:23.560
So there's networking options.

00:24:23.560 --> 00:24:26.820
You can connect to data in a VNet, you know, those kind of capabilities.

00:24:26.820 --> 00:24:35.120
So we more or less had these two groups of people, the serverless plan people, really happy about their true serverless event-based scaling.

00:24:35.120 --> 00:24:36.360
They just don't have to think about it.

00:24:36.360 --> 00:24:37.720
It does its magic.

00:24:37.720 --> 00:24:38.320
Exactly.

00:24:38.320 --> 00:24:39.040
It's the cloud.

00:24:39.040 --> 00:24:47.980
And then we had the folks who were super happy with the app service plan because the performance is great because things are already provisioned for you and all the networking capabilities.

00:24:47.980 --> 00:24:55.600
So in the last few months, we actually went and did some work to enable a third hosting plan, which we're very, very excited about, actually.

00:24:55.600 --> 00:24:58.120
It's called the Elastic Premium Plan.

00:24:58.120 --> 00:25:06.780
And in the Elastic Premium Plan, what you can essentially do is you can tell us how many reserved VMs you want for your application.

00:25:07.040 --> 00:25:11.980
So if you're going to have a third one, which is the first time, which is the first time you want for your application.

00:25:11.980 --> 00:25:11.980
So if you're going to have a third of the application, you can tell us how many of the applications are going for your application.

00:25:11.980 --> 00:25:16.600
So if you're going to have a third of the application, you can tell us how many of the applications are going for your application.

00:25:16.600 --> 00:25:20.600
So if you're going to have a third of the application, you can tell us how many of the applications are going for your application.

00:25:20.600 --> 00:25:24.600
I always want to have my application warm and running on that one instance.

00:25:24.600 --> 00:25:34.440
But if I receive traffic that's more than that instance can handle, I'd like to scale out to meet that demand and scale back down to one when the demand's gone.

00:25:34.540 --> 00:25:47.760
Which is cool because now you get the benefits of really good performance because reserved instance, networking options because you already have something that's provisioned there, and the serverless scale.

00:25:47.880 --> 00:25:48.900
Yeah, that sounds pretty cool.

00:25:48.900 --> 00:25:53.460
Like most of the time, probably just one VM may do it.

00:25:53.460 --> 00:26:02.120
And one of the things that I think that I'm always a little suspicious of in the serverless world is there's how long your code takes to run,

00:26:02.120 --> 00:26:12.680
and then how long it takes end to end for the request to come in, for the temporary Docker image to spin up and get created, and all that kind of stuff.

00:26:12.680 --> 00:26:17.640
And it seems to me like you might be able to avoid a lot of it in this scenario you're talking about here.

00:26:17.640 --> 00:26:20.040
If you have your own VM that's dedicated to it.

00:26:20.040 --> 00:26:24.080
In fact, what you explained right now, we're already optimizing for that.

00:26:24.080 --> 00:26:32.420
So, you know, the whole point of us running in a Docker container, which we are certain what container it is, is the fact that it's already warm and running.

00:26:32.420 --> 00:26:37.160
Even when a first call comes to your application, we're actually running a dummy Python site.

00:26:37.300 --> 00:26:45.940
And when we invoke your application, we go ahead and specialize that site with your code and with your application settings in order to invoke your code.

00:26:45.940 --> 00:26:47.040
A lot of that's already been done.

00:26:47.040 --> 00:26:48.600
A lot of that's already been done.

00:26:48.600 --> 00:26:51.520
So we're optimizing for performance even on the serverless plan.

00:26:51.520 --> 00:26:57.620
That said, though, even that doing that job takes a little bit of time because, again, it is serverless.

00:26:57.620 --> 00:26:59.420
We're not provisioning anything for you.

00:26:59.420 --> 00:27:02.580
So when you do specialize something, it takes a little bit of time.

00:27:02.740 --> 00:27:10.660
If you absolutely had a workload that could not function on that kind of latency either, premium plan is a great option.

00:27:10.660 --> 00:27:11.060
Interesting.

00:27:11.060 --> 00:27:14.520
There's a lot of talk about Docker and Docker images and stuff in here.

00:27:14.520 --> 00:27:15.220
And it's really cool.

00:27:15.220 --> 00:27:20.300
And that obviously gives you like a decent level of isolation in your environment and stuff like that.

00:27:20.300 --> 00:27:24.200
But would I be able to like create my own Docker container?

00:27:24.340 --> 00:27:34.080
Say instead of taking the one that we talked about that we could get Docker pull and mess with, could I base a custom image on that and like get it exactly the way I want?

00:27:34.080 --> 00:27:35.340
And then run that?

00:27:35.340 --> 00:27:36.760
Yeah, you can absolutely do that.

00:27:36.760 --> 00:27:39.900
So like I said, our Docker image is open source.

00:27:39.900 --> 00:27:44.540
You could go ahead and extend that image to add whatever requirements you want to add.

00:27:45.100 --> 00:27:54.140
And the typical reasons why we see somebody would do that is if you had dependencies that weren't essentially pip installable, but something that was an OS level dependency.

00:27:54.140 --> 00:28:00.880
Maybe you wanted to install the PyTorch package via apt-get on your container, right?

00:28:00.880 --> 00:28:03.600
In that case, custom container is a great scenario.

00:28:03.600 --> 00:28:10.020
The other one is if you were using a distribution of Python that we didn't support out of the box, something like the Anaconda distribution.

00:28:10.480 --> 00:28:16.180
Again, go ahead and customize the container with the Anaconda distribution and you can still use functions with it.

00:28:16.180 --> 00:28:20.980
Okay, then I put like blob storage and point to it in my container or my function concentrate.

00:28:20.980 --> 00:28:21.820
How do I do it?

00:28:21.820 --> 00:28:28.060
You can simply push your Docker image to either Docker Hub or ACR and we also support private repositories.

00:28:28.060 --> 00:28:34.020
But as long as you bring us the image itself, we'd be able to use that to create a function app.

00:28:34.020 --> 00:28:37.620
This is mixing a lot of these cloud and networking ideas together, right?

00:28:37.720 --> 00:28:46.140
Right, and that's why the only caveat I will mention though with custom containers is we will not be offering the consumption or the serverless plan with custom containers.

00:28:46.140 --> 00:28:52.740
We are sticking to the premium plan and the app service plan to optimize for performance benefits.

00:28:52.740 --> 00:28:53.180
Sure.

00:28:53.700 --> 00:29:02.440
So we talked a little bit about being able to run with that func host start command, run locally our code.

00:29:02.440 --> 00:29:12.120
But, you know, if you're going to do like a production release of your code, you probably want to have continuous integration, maybe continuous deployment.

00:29:12.120 --> 00:29:14.540
Like how do I test this stuff?

00:29:14.540 --> 00:29:18.300
Can I test it in like some kind of CI system or something like that?

00:29:18.340 --> 00:29:19.800
We integrate with Azure DevOps.

00:29:19.800 --> 00:29:23.980
Azure DevOps is basically a CI CD tool.

00:29:23.980 --> 00:29:27.920
Specifically, Azure Pipelines is the CI CD component of DevOps.

00:29:27.920 --> 00:29:31.660
And it provides you with multiple client touch points.

00:29:31.660 --> 00:29:38.460
You can go to this portal where you'd say, hey, I'd like to set up a DevOps pipeline or a CI CD pipeline for my function app.

00:29:38.700 --> 00:29:42.120
We're already populating functions and Python templates in there.

00:29:42.120 --> 00:29:48.720
So it'll be able to generate the structure of the various steps that you need to take in order to do CD.

00:29:48.720 --> 00:29:51.600
And you can go ahead and integrate your own CI in there as well.

00:29:51.600 --> 00:29:58.780
We've also got the command line experience, which is starting from if you've already developed a function app,

00:29:59.200 --> 00:30:04.140
you can go ahead and run the az function app devops create command.

00:30:04.140 --> 00:30:07.020
It's documented in the functions documentation.

00:30:07.020 --> 00:30:11.860
But that'll automatically identify the app that you're using is a Python app.

00:30:11.860 --> 00:30:14.340
It contains requirements txt, which are certain packages.

00:30:14.340 --> 00:30:23.240
And go ahead and generate what's called an Azure Pipelines YAML file that contains exactly the script that needs to run for your CD step.

00:30:23.240 --> 00:30:23.500
Okay.

00:30:23.500 --> 00:30:24.740
And then how do I test it?

00:30:24.740 --> 00:30:28.060
Can I test it with like pytest or like what is the story?

00:30:28.060 --> 00:30:30.820
You can definitely write unit tests against pytest.

00:30:30.820 --> 00:30:37.780
For each of the triggers and bindings that we integrate with, we're including in the library mock interfaces.

00:30:37.780 --> 00:30:38.260
Right.

00:30:38.260 --> 00:30:38.560
Okay.

00:30:38.560 --> 00:30:44.080
So if I want to have like say a CosmoDB or like a blob storage one, I guess that would be hard to test.

00:30:44.080 --> 00:30:48.340
Like it would be hard to simulate something going into blob storage, right?

00:30:48.340 --> 00:30:48.880
Exactly.

00:30:48.880 --> 00:30:57.860
So in that case, you'd use that mock rich type that we're providing to create an object that you can test against in a CI CD kind of setting.

00:30:57.860 --> 00:31:04.160
If you were to be testing locally, however, we do let you run against real time Azure events.

00:31:04.160 --> 00:31:16.680
So if I started a function that's going to be triggered by a message being added to queue, when I'm running it locally on my local host endpoint, I can go to Azure, add a message to queue, and it will trigger off the function.

00:31:16.680 --> 00:31:18.060
Yeah, that sounds like a pretty cool way.

00:31:18.060 --> 00:31:20.220
So that's more of a development side of things.

00:31:20.220 --> 00:31:22.440
It's more like the inner loop, outer loop story.

00:31:22.440 --> 00:31:22.700
Okay.

00:31:22.700 --> 00:31:23.020
Right.

00:31:23.020 --> 00:31:29.920
So we think of like, hey, when you're developing locally, just trying to get your function to work in the way you want it to.

00:31:29.920 --> 00:31:31.160
Inner loop is great.

00:31:31.160 --> 00:31:32.100
You develop with a CLI.

00:31:32.100 --> 00:31:33.740
You test against real time Azure events.

00:31:33.980 --> 00:31:42.560
Now, when you think you're ready with your function app and you'd like to move to the cloud, that's when you go ahead and initiate the CI CD or DevOps create command.

00:31:42.560 --> 00:31:42.920
Okay.

00:31:42.920 --> 00:31:43.860
I'm happy with it.

00:31:43.860 --> 00:31:44.820
I want to put it out there.

00:31:44.820 --> 00:31:46.500
How do I do that without?

00:31:46.500 --> 00:31:48.440
Let's suppose I have a function.

00:31:48.940 --> 00:31:51.540
It's getting 100 requests per second, right?

00:31:51.540 --> 00:31:56.460
Even if it only takes, you know, half a second to deploy this thing, that could be a problem.

00:31:56.460 --> 00:32:02.900
Is there a way to roll it out so that it kind of upgrades without breaking inbound requests?

00:32:02.900 --> 00:32:05.800
We've got this feature called deployment slots.

00:32:05.800 --> 00:32:06.240
Okay.

00:32:06.240 --> 00:32:14.300
And essentially what that means is, hey, you can have different kind of canary environments that you're publishing your application to.

00:32:14.400 --> 00:32:20.420
For instance, I could have an app that's running in a production slot and 100% of my traffic is coming to that slot.

00:32:20.420 --> 00:32:24.340
Now I've got an updated version of that app, a V2 version, say.

00:32:24.340 --> 00:32:31.820
I'll go ahead and create a separate slot, which is, you know, my canary slot, say, and publish my application to that slot.

00:32:31.820 --> 00:32:44.320
And I can start navigating some of my traffic, the incoming traffic to that slot so that, you know, maybe 80% of my traffic is coming to production, 20% is going to canary, and so on and so forth.

00:32:44.380 --> 00:32:57.280
I start to navigate the traffic until I feel very comfortable that my staging slot or my canary slot is doing what I expected to do and bring my number of requests on the production slot down to zero before I swap.

00:32:57.280 --> 00:32:59.800
Yeah, that's actually cooler than I expected.

00:32:59.800 --> 00:33:05.520
I figured, like, there might be some kind of, like, Kubernetes sort of upgrade story type of thing.

00:33:05.640 --> 00:33:13.000
But the ability to, like, send, like, 1% of your traffic at it and just go, well, let's just make sure that this is going to hang together.

00:33:13.000 --> 00:33:13.300
Yep.

00:33:13.300 --> 00:33:14.500
Yeah, that's pretty cool.

00:33:16.420 --> 00:33:19.560
This portion of Talk Python To Me was brought to you by Datadog.

00:33:19.560 --> 00:33:25.280
Get insights into your Python applications and infrastructures with Datadog's fully integrated platform.

00:33:25.280 --> 00:33:31.660
Debug and optimize your code by tracing requests across web servers, databases, and services in your environment.

00:33:32.060 --> 00:33:39.520
Then correlate and pivot between distributed request traces, metrics, and logs to troubleshoot issues without switching tools or contexts.

00:33:39.520 --> 00:33:44.060
Get started today with a 14-day trial, and Datadog will send you a free t-shirt.

00:33:44.060 --> 00:33:47.740
Visit talkpython.fm/Datadog for more details.

00:33:50.060 --> 00:33:54.620
So, Bill, there's always a lot of announcements from folks at Microsoft.

00:33:54.620 --> 00:34:01.320
Like, this is one of the conferences that you hold back all of your big news for, right, until you, like, let it out here.

00:34:01.320 --> 00:34:08.080
So, I guess, you know, maybe give us a rundown on, like, what's the big news for you in the serverless and Python?

00:34:08.080 --> 00:34:17.720
Yeah, it's interesting because we've actually been releasing pretty exciting announcements over the last month, month and a half almost.

00:34:17.720 --> 00:34:22.600
So, the Elastic Premium plan that I spoke about went out about a month ago.

00:34:22.600 --> 00:34:28.140
And that's the biggest thing, even at Build, we're talking about it, and a lot of people already knew.

00:34:28.140 --> 00:34:31.360
So, I would say that's the biggest announcement of it all.

00:34:31.360 --> 00:34:35.400
The second one that's also super exciting is support for Kubernetes.

00:34:35.400 --> 00:34:42.480
So, Functions was already open source, the runtime and the programming model and the framework itself.

00:34:42.760 --> 00:34:49.000
But there was still a component of the platform that was still novel to what Azure was doing.

00:34:49.000 --> 00:34:50.800
That was called the Scale Controller.

00:34:50.800 --> 00:34:57.840
And just to give you a little rundown, Scale Controller is the component that's responsible for event-based scaling.

00:34:57.840 --> 00:35:05.000
So, for instance, if your function was triggering off messages in a queue, are Scale Controllers always watching that queue?

00:35:05.000 --> 00:35:06.480
How many messages are on that queue?

00:35:06.580 --> 00:35:07.780
What's the size of each message?

00:35:07.780 --> 00:35:09.400
How long is it taking to process?

00:35:09.400 --> 00:35:12.360
And depending on that, actually scaling your infrastructure.

00:35:12.360 --> 00:35:20.160
Now, we actually went in and rewrote that Scale Controller in Golang and made it open source.

00:35:20.160 --> 00:35:27.800
So, you can now deploy your functions containers or function apps to Kubernetes and also take advantage of the event-based scaling.

00:35:27.800 --> 00:35:29.740
It's a project we're calling KEDA.

00:35:29.740 --> 00:35:40.160
And it's an attempt to really open source a lot of what we're doing in functions and try to reduce the vendor lock-in that may still be remaining.

00:35:40.160 --> 00:35:44.840
For instance, I mentioned earlier in the podcast, we added support for the Kafka trigger.

00:35:44.840 --> 00:35:47.540
That was motivated by the KEDA project.

00:35:47.540 --> 00:35:51.560
We're also looking at adding support for things like RabbitMQ as triggers.

00:35:51.560 --> 00:35:56.660
That sounds super cool because, you know, to me, I feel like the cloud is the new lock-in, right?

00:35:56.660 --> 00:36:01.500
For both good and bad, like, you know, not just Azure, but AWS and the other ones.

00:36:01.500 --> 00:36:07.200
Like, the more you take advantage of these really cool features, the more you are kind of committed, right?

00:36:07.200 --> 00:36:13.720
But it sounds like with this, as long as you're willing to run a Kubernetes cluster, you can kind of bring it wherever you want it to go.

00:36:13.720 --> 00:36:18.400
Exactly. And it's more for the audience, I'd say, who are already bought into Kubernetes.

00:36:18.400 --> 00:36:22.880
You know, you don't have to do that if you really, truly are serverless.

00:36:22.880 --> 00:36:27.420
Just leave the infrastructure to us and let us do what we do best.

00:36:27.420 --> 00:36:34.120
But if you're a big enterprise and everything you have is going to be running on Kubernetes and you want functions to be a part of that,

00:36:34.260 --> 00:36:38.520
it's a great way to still be able to take advantage of the event-driven serverless functions.

00:36:38.520 --> 00:36:40.380
Okay, so I already have a Kubernetes cluster.

00:36:40.380 --> 00:36:47.500
Can I just take this and, like, run it on the cluster as one of the things and then start calling the endpoints there?

00:36:47.500 --> 00:36:48.060
Exactly.

00:36:48.060 --> 00:36:49.580
Where's the rest of the infrastructure?

00:36:49.580 --> 00:36:54.800
Like, where are the HTTP endpoints that tie it back to the functions and, you know, stuff like that?

00:36:54.800 --> 00:37:00.040
Is that enough to just sort of have my own little private serverless thing in some cluster or what?

00:37:00.040 --> 00:37:07.300
Yeah, absolutely. I'd actually, without going too much in detail on KEDA, I would encourage you to look at the documentation.

00:37:07.300 --> 00:37:12.580
It's got some great references on how things are actually working in the Kubernetes space.

00:37:12.580 --> 00:37:15.940
There's a lot going on in terms of your cluster, your pods.

00:37:15.940 --> 00:37:18.700
I'm still learning about Kubernetes, to be honest.

00:37:18.700 --> 00:37:20.840
There is great documentation online.

00:37:20.840 --> 00:37:23.780
I'd encourage you to go look up KEDA, KEDA project.

00:37:23.780 --> 00:37:25.280
Okay, cool. What else?

00:37:25.280 --> 00:37:28.600
We introduced this concept called extension bundles.

00:37:28.820 --> 00:37:32.040
It's a little underneath the cover, so you don't really need to know about it.

00:37:32.040 --> 00:37:37.460
But really, historically, what happened was because our core runtimes were in .NET.

00:37:37.460 --> 00:37:43.480
There's a lot of .NET that kind of flew out or snuck up on you when you were developing Python.

00:37:43.480 --> 00:37:46.620
And really, we don't want that for our Python developers.

00:37:46.620 --> 00:37:53.300
You know, we want, if you're a Python developer, let us come and plug into your ecosystem rather than trying to drive you to .NET.

00:37:53.760 --> 00:37:57.220
Extension bundles abstracts away a lot of that stuff.

00:37:57.220 --> 00:38:05.620
So if you're using an extension or a binding that's written in .NET, you no longer have to have the .NET SDK installed.

00:38:05.620 --> 00:38:08.960
All you need is Python if you're developing in Python.

00:38:08.960 --> 00:38:11.220
And we'll take care of all of that for you.

00:38:11.220 --> 00:38:13.940
You're kind of what you would expect, but I guess it's nice that it's there, right?

00:38:14.000 --> 00:38:15.420
It's an evolution of the platform.

00:38:15.420 --> 00:38:32.460
I think from functions we want to where you could only develop on Windows and host on Windows to v2, where you can now develop on any platform, be it macOS, Windows, Linux machine, and host on Linux, to finally being able to abstract the few fritters that were still left.

00:38:32.680 --> 00:38:37.100
I'm really excited and looking forward to a place that we're at right now.

00:38:37.100 --> 00:38:38.340
I think we're getting pretty close.

00:38:38.340 --> 00:38:39.280
Yeah, that's cool.

00:38:39.280 --> 00:38:44.000
So speaking of environments, what versions of Python are supported?

00:38:44.000 --> 00:38:46.760
Today we support 3.6, Python 3.6.

00:38:46.760 --> 00:38:49.580
We're looking to add 3.7 support in the future as well.

00:38:49.580 --> 00:38:52.320
We've already got a container image available if that's of interest.

00:38:52.320 --> 00:38:56.000
And I guess if you can do your own container, then put whatever else you want out there, right?

00:38:56.000 --> 00:38:56.400
Exactly.

00:38:56.820 --> 00:39:01.960
Our core runtime is actually already compatible with 3.6 and 3.7.

00:39:01.960 --> 00:39:13.960
We heard feedback from specifically some of the ML users that there were packages like TensorFlow that weren't yet compatible with 3.7 or were still working on the wheels for 3.7 and such.

00:39:13.960 --> 00:39:19.100
So we just decided to delay that by a few months before we go ahead and make a big release.

00:39:19.100 --> 00:39:19.440
Sure.

00:39:19.440 --> 00:39:19.980
Okay, cool.

00:39:20.300 --> 00:39:32.560
One of the things that I saw people doing on AWS Lambda is using sort of abstracting away the deployment of something like Flask, right?

00:39:32.560 --> 00:39:44.600
So there's a project called Zappa, and Zappa will let you take what looks like a Flask function and turn it into a bunch of serverless functions, which is an interesting idea.

00:39:44.600 --> 00:39:45.460
I don't know.

00:39:45.460 --> 00:39:52.820
I saw that in the docs you guys have something kind of like that, but it's much more manual than working directly with Flask.

00:39:52.820 --> 00:39:54.900
Do you have any plans for something like that?

00:39:54.900 --> 00:39:55.540
Not yet.

00:39:55.540 --> 00:39:57.580
And honestly, we're still thinking about it.

00:39:57.580 --> 00:40:01.720
We don't know if that's a direction that most users want to go in.

00:40:01.720 --> 00:40:05.920
Just speaking to customers, and we've got a very good community of preview users.

00:40:05.920 --> 00:40:07.380
They've given a lot of feedback.

00:40:07.380 --> 00:40:09.640
This hasn't come out that much.

00:40:09.640 --> 00:40:15.280
So really, I'd encourage you and everybody who's listening to give us feedback on that.

00:40:15.280 --> 00:40:23.220
Tell us if that scenario is something that's important for you, and help us just understand how we should think about these things.

00:40:23.220 --> 00:40:27.780
Yeah, honestly, I look at that, and it's super interesting, and it's kind of cool to think about it.

00:40:27.780 --> 00:40:33.180
But in practice, I'm like, do I really want to maintain my web app as like 100 separate little things?

00:40:33.180 --> 00:40:42.260
Or does it make more sense to maybe build like 10 specialized serverless bits and then host the rest of it as like a regular web app or something?

00:40:42.260 --> 00:40:42.680
I don't know.

00:40:42.680 --> 00:40:44.700
It's an interesting idea, though.

00:40:44.700 --> 00:40:45.400
Right, right.

00:40:45.400 --> 00:40:45.900
Yeah, yeah.

00:40:45.900 --> 00:40:46.720
Cool.

00:40:46.720 --> 00:40:50.120
Well, I guess that's probably about it.

00:40:50.120 --> 00:40:53.480
Anything else you want to share about the service that you guys got going on right now?

00:40:53.480 --> 00:40:55.760
We've got a few things in motion.

00:40:55.760 --> 00:41:03.760
I would say the biggest one, definitely Python is something that is our biggest focus right now for the Azure Functions team.

00:41:03.760 --> 00:41:12.560
So just keep your eyes and ears open for what we have coming next in terms of improvements for the preview itself and our geo offering.

00:41:12.560 --> 00:41:15.220
That's really my biggest call to action here.

00:41:15.220 --> 00:41:15.740
Yeah, cool.

00:41:16.020 --> 00:41:24.460
One of the things I thought was really nice is I went to the Azure serverless functions bit for Python and said to subscribe to our announcements.

00:41:24.460 --> 00:41:26.320
Just go watch the GitHub repository.

00:41:26.320 --> 00:41:31.140
That's a pretty cool way to like, instead of like, yeah, we're going to shoot you a newsletter or something.

00:41:31.140 --> 00:41:32.860
No, just it'll be in GitHub.

00:41:32.860 --> 00:41:33.580
Subscribe to it.

00:41:33.580 --> 00:41:38.260
Yeah, we definitely spend a lot of time and lives on GitHub these days.

00:41:38.260 --> 00:41:39.640
Open source is great.

00:41:39.640 --> 00:41:43.480
Anytime we want to speak to the community, we pretty much post an announcement.

00:41:43.480 --> 00:41:47.480
Even something like, I'm not sure if you've heard of durable functions in Python.

00:41:47.480 --> 00:41:51.860
We're thinking about, hey, it doesn't make sense to have durable functions for Python.

00:41:51.860 --> 00:41:55.040
Do Python users have scenarios that they need durable for?

00:41:55.460 --> 00:42:00.400
Something I posted the announcement about a GitHub issue about, and there was a ton of interest.

00:42:00.400 --> 00:42:07.480
So it's a great way to just go watch our repository, see what we're doing every day, the questions we're asking, and participate in the discussion.

00:42:07.480 --> 00:42:08.140
Yeah, that's cool.

00:42:08.140 --> 00:42:09.600
What do you mean by durable functions?

00:42:09.600 --> 00:42:13.880
Like functions, if they fail, they'll like restart, kind of like a transaction, or what is this?

00:42:13.880 --> 00:42:14.620
No, actually.

00:42:14.620 --> 00:42:24.760
So going back to a little bit of principles of serverless per se, serverless functions are typically supposed to be short-lived and stateless, right?

00:42:24.760 --> 00:42:25.120
Okay.

00:42:25.260 --> 00:42:31.280
But what we heard was there can be scenarios that are long-running because maybe you're running some kind of an orchestration.

00:42:31.280 --> 00:42:36.660
Maybe you're doing a function, like a map-reduced scenario, a fan-in, fan-out scenario, right?

00:42:36.660 --> 00:42:36.980
I see.

00:42:36.980 --> 00:42:44.420
And that can be a problem because a lot of the server infrastructure, serverless infrastructure will stop your function if it takes too long, right?

00:42:44.420 --> 00:42:44.740
Exactly.

00:42:44.740 --> 00:42:52.600
In fact, on our infrastructure, on our platform, it's a 10-minute default timeout, but you can tweak it to be a 20-minute at the max.

00:42:53.220 --> 00:42:55.680
The new Elastic Premium plan actually can run forever.

00:42:55.680 --> 00:42:58.480
That's a premium benefit of the Premium plan.

00:42:58.480 --> 00:43:07.220
But with durable functions, what we do is we are basically providing a framework for you to be able to orchestrate your function executions.

00:43:07.220 --> 00:43:09.500
There are two concepts in durable.

00:43:09.500 --> 00:43:12.600
There is something called an orchestrator function, which can run forever.

00:43:12.600 --> 00:43:21.640
And orchestrator function basically calls into what are called activity functions, which are basically just regular functions and have the 20-minute timeout.

00:43:21.640 --> 00:43:32.980
So if I were doing a scenario like function chaining, I could have my orchestrator function kind of coordinate calling function A, taking the response from function A, and using it to invoke B.

00:43:32.980 --> 00:43:35.060
Similarly, B to C, and so on.

00:43:35.060 --> 00:43:39.900
Simple function chaining scenario or something where I was actually doing a fan-in, fan-out would be more complicated.

00:43:39.900 --> 00:43:44.140
I could orchestrate various functions with the long-running orchestrator function.

00:43:44.140 --> 00:43:48.440
Or even the other day, somebody built a scenario which required an OTP pin, for instance.

00:43:48.440 --> 00:43:53.980
Human interaction with a function, which we humans take time sometimes to respond.

00:43:53.980 --> 00:43:54.660
Yeah, for sure.

00:43:54.660 --> 00:43:57.540
So those kind of scenarios are possible with durable functions.

00:43:57.540 --> 00:44:02.780
It kind of solves the problem of being able to maintain state and run for a longer duration.

00:44:02.780 --> 00:44:06.680
Does it like serialize a state while it's waiting and like bring it back?

00:44:06.680 --> 00:44:08.040
It basically does.

00:44:08.040 --> 00:44:11.400
Somebody on Twitter today just posted about the black magic of durable.

00:44:11.400 --> 00:44:14.400
So it's a little bit complex and I can go into the details.

00:44:14.400 --> 00:44:17.180
But yes, it's basically using storage to maintain state.

00:44:17.180 --> 00:44:17.760
Okay, cool.

00:44:17.760 --> 00:44:18.420
That's exciting.

00:44:18.420 --> 00:44:19.400
That's a cool idea.

00:44:19.400 --> 00:44:19.660
Yeah.

00:44:19.660 --> 00:44:19.980
All right.

00:44:19.980 --> 00:44:21.380
Well, thanks for sharing all these ideas.

00:44:21.380 --> 00:44:25.100
There's just a couple of questions I'll ask you before I'll let you go.

00:44:25.100 --> 00:44:29.080
One is if you're going to write some Python code, what editor do you use?

00:44:29.080 --> 00:44:34.880
Yeah, so good question because I was traditionally using PyCharm and recently, when I say recently,

00:44:34.880 --> 00:44:38.040
since about the last year or so, starting using VS Code.

00:44:38.040 --> 00:44:44.100
And I'm really appreciating right now the VS Code ability to be able to work with Azure

00:44:44.100 --> 00:44:45.860
because I work a lot with the Azure cloud.

00:44:45.860 --> 00:44:47.880
The extensions do a great job at it.

00:44:47.880 --> 00:44:48.720
Yeah, that's cool.

00:44:48.720 --> 00:44:50.100
There's definitely a lot of integration there.

00:44:50.100 --> 00:44:53.060
Do you have plugins for PyCharm as well?

00:44:53.060 --> 00:44:55.540
I was using a few plugins for PyCharm.

00:44:55.540 --> 00:44:58.720
I honestly don't even remember what plugins there were right now.

00:44:58.720 --> 00:44:59.200
Yeah, sure.

00:44:59.200 --> 00:44:59.780
It's been a while.

00:44:59.780 --> 00:45:00.180
All right.

00:45:00.180 --> 00:45:03.800
And then a notable PyPI package that maybe you ran across.

00:45:03.800 --> 00:45:04.780
You're like, oh, that's cool.

00:45:04.780 --> 00:45:05.760
People should know about this.

00:45:05.760 --> 00:45:10.040
So yesterday, I ran into something called Flask Dance.

00:45:10.040 --> 00:45:11.080
Flask Dance.

00:45:11.080 --> 00:45:15.980
And really, what it helps you do is a lot of the OAuth dance that you would do manually,

00:45:15.980 --> 00:45:18.760
typically, I mentioned the Slack application to you, right?

00:45:18.760 --> 00:45:25.120
A lot of the OAuth dance that you would do typically yourself, it makes it really, really easy to be able to do that.

00:45:25.420 --> 00:45:29.560
And I was super thankful for that package yesterday when I had to do this in a short time.

00:45:29.560 --> 00:45:30.320
Yeah, that's awesome.

00:45:30.320 --> 00:45:32.620
Because OAuth can be kind of annoying.

00:45:32.620 --> 00:45:35.980
You do this, and then it calls back this function, and then it gets to be...

00:45:35.980 --> 00:45:38.880
And of course, every API provider does it differently.

00:45:38.880 --> 00:45:39.880
Yeah, of course.

00:45:39.880 --> 00:45:41.020
And yeah, that's no fun.

00:45:41.020 --> 00:45:41.620
So awesome.

00:45:41.620 --> 00:45:42.160
Flask Dance.

00:45:42.160 --> 00:45:42.480
That's cool.

00:45:42.480 --> 00:45:43.380
I hadn't heard about that one.

00:45:43.380 --> 00:45:43.980
All right.

00:45:43.980 --> 00:45:44.660
Final call to action.

00:45:44.660 --> 00:45:46.040
People are excited about serverless.

00:45:46.040 --> 00:45:46.820
What should they do?

00:45:46.880 --> 00:45:49.060
I think you should go check out Azure Functions.

00:45:49.060 --> 00:45:51.360
We've got some great getting started material.

00:45:51.360 --> 00:45:55.420
Like I said, if you're a CLI person, go ahead, download a CLI tool, start developing.

00:45:55.420 --> 00:45:57.680
Or you can also use VS Code.

00:45:57.680 --> 00:46:02.700
We are all eyes and ears right now for any feedback that you can give us.

00:46:02.700 --> 00:46:08.160
And given that we're out there in the open, all of our discussions, ideas, feature requests,

00:46:08.520 --> 00:46:13.600
you really have a great chance to influence the Azure product, I'd encourage you to come

00:46:13.600 --> 00:46:14.960
participate and talk to us.

00:46:14.960 --> 00:46:15.280
Yeah.

00:46:15.280 --> 00:46:15.540
All right.

00:46:15.540 --> 00:46:15.700
Cool.

00:46:15.700 --> 00:46:18.880
Well, thank you so much for being on the show and sharing all of what you're up to here.

00:46:18.880 --> 00:46:19.120
Yeah.

00:46:19.120 --> 00:46:20.340
Thank you so much for having me.

00:46:20.340 --> 00:46:20.540
Yeah.

00:46:20.540 --> 00:46:20.740
Bye.

00:46:20.740 --> 00:46:24.060
This has been another episode of Talk Python to Me.

00:46:24.060 --> 00:46:28.900
Our guest on this episode was Asavri Tayal, and it's been brought to you by Command Line Heroes

00:46:28.900 --> 00:46:29.760
and Datadog.

00:46:29.760 --> 00:46:33.600
Command Line Heroes is a podcast telling the story of developers.

00:46:33.600 --> 00:46:38.480
This season is all about programming languages and starts off with Python, of course.

00:46:38.940 --> 00:46:41.820
Subscribe at talkpython.fm/heroes.

00:46:41.820 --> 00:46:46.220
Datadog gives you visibility into the whole system running your code.

00:46:46.220 --> 00:46:50.540
Visit talkpython.fm/datadog and see what you've been missing.

00:46:50.540 --> 00:46:52.740
Don't even throw in a free t-shirt for doing the tutorial.

00:46:52.740 --> 00:46:55.120
Want to level up your Python?

00:46:55.120 --> 00:46:59.980
If you're just getting started, try my Python Jumpstart by Building 10 Apps course.

00:46:59.980 --> 00:47:05.080
Or if you're looking for something more advanced, check out our new async course that digs into

00:47:05.080 --> 00:47:08.200
all the different types of async programming you can do in Python.

00:47:08.400 --> 00:47:12.100
And of course, if you're interested in more than one of these, be sure to check out our

00:47:12.100 --> 00:47:12.800
Everything Bundle.

00:47:12.800 --> 00:47:14.680
It's like a subscription that never expires.

00:47:14.680 --> 00:47:16.980
Be sure to subscribe to the show.

00:47:16.980 --> 00:47:19.400
Open your favorite podcatcher and search for Python.

00:47:19.400 --> 00:47:20.620
We should be right at the top.

00:47:20.620 --> 00:47:25.460
You can also find the iTunes feed at /itunes, the Google Play feed at /play,

00:47:25.460 --> 00:47:29.620
and the direct RSS feed at /rss on talkpython.fm.

00:47:30.140 --> 00:47:31.720
This is your host, Michael Kennedy.

00:47:31.720 --> 00:47:33.220
Thanks so much for listening.

00:47:33.220 --> 00:47:34.280
I really appreciate it.

00:47:34.280 --> 00:47:36.040
Now get out there and write some Python code.

00:47:36.040 --> 00:47:40.460
We'll see you next time.

00:47:40.460 --> 00:48:10.440
We'll see you next time.

