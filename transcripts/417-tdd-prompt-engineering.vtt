WEBVTT

00:00:00.000 --> 00:00:04.320
Large language models and chat-based AIs are kind of mind-blowing at the moment.


00:00:04.320 --> 00:00:10.640
Many of us are playing with them for working on code or just as a fun alternative to search,


00:00:10.640 --> 00:00:14.320
but others of us are building applications with AI at the core.


00:00:14.320 --> 00:00:21.200
And when doing that, the slight unpredictable nature and probabilistic style of LLMs makes


00:00:21.200 --> 00:00:23.840
writing and testing Python code very tricky.


00:00:23.840 --> 00:00:28.080
Interpromptimize, from Maxine Bocheman, and Preset.


00:00:28.080 --> 00:00:33.040
It's a framework for non-deterministic testing of LLMs inside of our applications.


00:00:33.040 --> 00:00:41.040
Let's dive inside the AIs with Max. This is Talk Python To Me, episode 417, recorded May 22nd, 2023.


00:00:54.160 --> 00:00:59.360
Welcome to Talk Python.me, a weekly podcast on Python. This is your host, Michael Kennedy.


00:00:59.360 --> 00:01:04.560
Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython,


00:01:04.560 --> 00:01:09.360
both on fosstodon.org. Be careful with impersonating accounts on other instances,


00:01:09.360 --> 00:01:14.000
there are many. Keep up with the show and listen to over seven years of past episodes at


00:01:14.000 --> 00:01:20.160
talkpython.fm. We've started streaming most of our episodes live on YouTube. Subscribe to our


00:01:20.160 --> 00:01:25.840
YouTube channel over at talkpython.fm/youtube to get notified about upcoming shows and be


00:01:25.840 --> 00:01:28.160
part of that episode.


00:01:28.160 --> 00:01:34.400
This episode is brought to you by JetBrains, who encourage you to get work done with PyCharm.


00:01:34.400 --> 00:01:41.320
Download your free trial of PyCharm Professional at talkpython.fm/done-with-pycharm.


00:01:41.320 --> 00:01:45.080
And it's brought to you by The Compiler Podcast from Red Hat.


00:01:45.080 --> 00:01:53.200
to an episode of their podcast to demystify the tech industry over at talkbython.fm/compiler.


00:01:53.200 --> 00:01:54.520
Max welcome to Talk Python to Me.


00:01:54.520 --> 00:01:58.840
>> Well it's good to be back on the show and now I know it's live too so no mistakes.


00:01:58.840 --> 00:02:01.560
I'm going to try to not say anything outrageous.


00:02:01.560 --> 00:02:05.440
>> People get the unfiltered version so absolutely.


00:02:05.440 --> 00:02:08.320
I love it when people come check out the live show.


00:02:08.320 --> 00:02:09.320
Welcome back.


00:02:09.320 --> 00:02:11.800
It's been a little while since you were on the show.


00:02:11.800 --> 00:02:13.080
About since September.


00:02:13.080 --> 00:02:15.000
We talked about supersets.


00:02:15.000 --> 00:02:17.060
We also talked a little bit about Airflow,


00:02:17.060 --> 00:02:19.300
some of the stuff that you've been working on.


00:02:19.300 --> 00:02:21.740
And now we're kind of circling back


00:02:21.740 --> 00:02:23.840
through this data side of things,


00:02:23.840 --> 00:02:26.680
but trying to bring AI into the whole story.


00:02:26.680 --> 00:02:28.320
So pretty cool project


00:02:28.320 --> 00:02:29.720
I'm looking forward to talking to you about.


00:02:29.720 --> 00:02:31.480
- Awesome, excited to talk about it too.


00:02:31.480 --> 00:02:34.700
And these things are related in many ways.


00:02:34.700 --> 00:02:37.920
Like one common foundation is Python.


00:02:37.920 --> 00:02:39.880
Like a lot of these projects are in Python.


00:02:39.880 --> 00:02:40.980
They're data related.


00:02:40.980 --> 00:02:43.380
And here, Proptimize and Propt Engineering


00:02:43.380 --> 00:02:46.840
and integrating AI is related in a way


00:02:46.840 --> 00:02:49.660
that we're building some AI features


00:02:49.660 --> 00:02:52.540
into Superset right now and into Preset,


00:02:52.540 --> 00:02:55.260
so it ties things together in some way.


00:02:55.260 --> 00:02:57.700
- Yeah, I can certainly see a lot of synergy here.


00:02:57.700 --> 00:02:59.900
Before we dive into it, it hasn't been that long


00:02:59.900 --> 00:03:02.620
since you were on the show, but give us a quick update,


00:03:02.620 --> 00:03:03.820
just a bit about your background


00:03:03.820 --> 00:03:05.260
for people who don't know you.


00:03:05.260 --> 00:03:10.260
- Yeah, so my career is a career of maybe 20 or so years


00:03:10.260 --> 00:03:17.020
years in data building, doing data engineering, doing, trying to make useful, data useful


00:03:17.020 --> 00:03:19.460
for organizations.


00:03:19.460 --> 00:03:22.820
Over the past decade or so, I've been very involved in open source.


00:03:22.820 --> 00:03:25.880
I started Apache Airflow in 2014.


00:03:25.880 --> 00:03:29.380
So for those not familiar with Airflow, though, it's pretty well known now.


00:03:29.380 --> 00:03:35.500
It's used at, I heard, like, I think it's like tens of thousands, I think above 100,000


00:03:35.500 --> 00:03:38.740
companies are using Apache Airflow, which is kind of insane to think about.


00:03:38.740 --> 00:03:40.060
It's like you started a little project.


00:03:40.060 --> 00:03:42.720
So for me, I started this project at Airbnb


00:03:42.720 --> 00:03:44.640
and it really took off.


00:03:44.640 --> 00:03:48.220
And I think it's just like great project community fit.


00:03:48.220 --> 00:03:50.140
Like people really needed that.


00:03:50.140 --> 00:03:52.720
It was the right abstraction for people at the time


00:03:52.720 --> 00:03:53.900
and still today.


00:03:53.900 --> 00:03:56.100
And it just really took off.


00:03:56.100 --> 00:03:57.680
So I was working on orchestration


00:03:57.680 --> 00:03:59.400
and then I was like,


00:03:59.400 --> 00:04:02.080
I just love things that are visual and interactive.


00:04:02.080 --> 00:04:05.880
So there was no great open source BI tool


00:04:05.880 --> 00:04:07.040
out there, business intelligence.


00:04:07.040 --> 00:04:11.680
So this whole data dashboarding, exploration, SQL IDE.


00:04:11.680 --> 00:04:14.600
So it's a playground for people trying to understand


00:04:14.600 --> 00:04:16.200
and visualize and explore data.


00:04:16.200 --> 00:04:18.960
So I started working on Apache SuperSend in 2000,


00:04:18.960 --> 00:04:21.680
I think it was like 15 or 16 at Airbnb too.


00:04:21.680 --> 00:04:24.280
And we also brought that to the Apache Software Foundation.


00:04:24.280 --> 00:04:28.200
So again, like a very, very popular open source project


00:04:28.200 --> 00:04:30.480
that's used in like tens of thousands,


00:04:30.480 --> 00:04:33.080
a hundred thousand organizations or so.


00:04:33.080 --> 00:04:37.880
And today, it has become a super great open source


00:04:37.880 --> 00:04:40.200
or alternative to Tableau, Looker,


00:04:40.200 --> 00:04:41.920
all those business intelligence tool


00:04:41.920 --> 00:04:45.120
is very viable for organizations.


00:04:45.120 --> 00:04:48.400
And then quick plugs, I pre-set I/O a company I started.


00:04:48.400 --> 00:04:51.400
I'm also an entrepreneur, I started a company


00:04:51.400 --> 00:04:52.640
a little bit more than four years ago


00:04:52.640 --> 00:04:55.160
around Apache Superset, and the idea is to


00:04:55.160 --> 00:04:58.180
bring Superset to the masses.


00:04:58.180 --> 00:05:01.120
So it's really like hosted, managed,


00:05:01.120 --> 00:05:03.460
state of the art Apache Super Set for everyone


00:05:03.460 --> 00:05:04.680
with some bells and whistles.


00:05:04.680 --> 00:05:07.060
So the best Super Set you can run,


00:05:07.060 --> 00:05:08.600
there's a free version too.


00:05:08.600 --> 00:05:11.100
So you can go and play and try it,


00:05:11.100 --> 00:05:12.480
today gets started in five minutes.


00:05:12.480 --> 00:05:15.740
So it's a little bit of a commercial pointer,


00:05:15.740 --> 00:05:18.380
but also very relevant to what I've been doing,


00:05:18.380 --> 00:05:20.940
personally over the past like three or four years.


00:05:20.940 --> 00:05:22.580
- Some of the inspiration for some of the things


00:05:22.580 --> 00:05:23.700
we're gonna talk about as well


00:05:23.700 --> 00:05:26.740
and trying to bring some of the AI craziness


00:05:26.740 --> 00:05:28.340
back to products, right?


00:05:28.340 --> 00:05:30.700
From an engineering perspective, not just a,


00:05:30.700 --> 00:05:34.740
"Hey, look, I asked what basketball team was gonna,


00:05:34.740 --> 00:05:37.820
"you know, win this year and it gave me this answer, right?"


00:05:37.820 --> 00:05:39.420
- It's like, and caveats,


00:05:39.420 --> 00:05:42.340
I don't know anything that happened since 2021.


00:05:42.340 --> 00:05:44.820
So AI, or AI specifically,


00:05:44.820 --> 00:05:46.180
Bart is a little bit better at that,


00:05:46.180 --> 00:05:47.700
but it's like, you know,


00:05:47.700 --> 00:05:49.620
the last thing I read off of the internet


00:05:49.620 --> 00:05:51.340
was in fall 2021.


00:05:51.340 --> 00:05:54.140
Makes some things a little bit challenging.


00:05:54.140 --> 00:05:56.020
But yeah, so we're building, you know,


00:05:56.020 --> 00:05:58.260
AI features into preset, you know,


00:05:58.260 --> 00:05:59.600
as a commercial open source company,


00:05:59.600 --> 00:06:02.320
We need to build some differentiators too from supersets.


00:06:02.320 --> 00:06:05.520
We contribute a huge amount, like maybe 50, 80%


00:06:05.520 --> 00:06:08.240
of the work we do at Preset is contributed back to Superset,


00:06:08.240 --> 00:06:10.640
but we're looking to build differentiators.


00:06:10.640 --> 00:06:14.200
And we feel like AI is a great kind of commercial


00:06:14.200 --> 00:06:16.880
differentiator too on top of Superset


00:06:16.880 --> 00:06:19.080
that makes people even more interested to come


00:06:19.080 --> 00:06:22.240
and run Preset too.


00:06:22.240 --> 00:06:23.240
- Yeah, excellent.


00:06:23.240 --> 00:06:27.160
And people, you say they were popular projects,


00:06:27.160 --> 00:06:30.280
Like Airflow has 30,000 stars.


00:06:30.280 --> 00:06:32.980
Apache SuperSet has 50,000 stars,


00:06:32.980 --> 00:06:35.720
which puts it on par with Django and Flask


00:06:35.720 --> 00:06:37.680
for people sort of mental models out there,


00:06:37.680 --> 00:06:39.480
which is, I would say it's pretty well known.


00:06:39.480 --> 00:06:41.060
So awesome.


00:06:41.060 --> 00:06:44.200
- Yeah, stars is kind of vanity metric in some ways, right?


00:06:44.200 --> 00:06:48.040
So it's not necessarily usefulness or value delivered,


00:06:48.040 --> 00:06:50.880
but it's a proxy for popularity and hype, you know?


00:06:50.880 --> 00:06:52.560
So it gives a good sense.


00:06:52.560 --> 00:06:55.680
And I think like at 50,000 stars,


00:06:55.680 --> 00:06:58.560
if you look at, it's probably in the top 100


00:06:58.560 --> 00:06:59.480
of GitHub projects.


00:06:59.480 --> 00:07:02.760
If you remove the, in the top 100,


00:07:02.760 --> 00:07:05.280
there's a lot of documentations and guides


00:07:05.280 --> 00:07:08.040
and things that are not really open source projects.


00:07:08.040 --> 00:07:12.080
So it's probably like top 100 open source project-ish,


00:07:12.080 --> 00:07:13.640
in both cases, which is--


00:07:13.640 --> 00:07:14.920
- Right, I'm so cool.


00:07:14.920 --> 00:07:15.960
Like, it's like you start a project


00:07:15.960 --> 00:07:19.440
and you don't know whether it's gonna take off and how,


00:07:19.440 --> 00:07:22.040
and it's like, wow, it's just nice to see that.


00:07:22.040 --> 00:07:23.000
- Yeah, absolutely.


00:07:23.000 --> 00:07:24.840
I mean, on one hand, it's nice,


00:07:24.840 --> 00:07:27.080
but it doesn't necessarily make it better.


00:07:27.080 --> 00:07:28.780
But it does mean there's a lot of people using it.


00:07:28.780 --> 00:07:29.900
There's a lot of polish.


00:07:29.900 --> 00:07:33.940
There's a lot of PRs and stuff that have been submitted


00:07:33.940 --> 00:07:35.000
to make it work.


00:07:35.000 --> 00:07:36.540
A lot of things that you can plug into, right?


00:07:36.540 --> 00:07:37.940
So there's certainly a value


00:07:37.940 --> 00:07:40.700
for having a project popular versus unpopular.


00:07:40.700 --> 00:07:41.540
- Oh my God, yes.


00:07:41.540 --> 00:07:42.900
And I would say one thing is,


00:07:42.900 --> 00:07:45.860
all the dark, call it like secondary assets


00:07:45.860 --> 00:07:49.440
outside of the core projects documentation.


00:07:49.440 --> 00:07:51.380
And there will be a lot of like use cases


00:07:51.380 --> 00:07:53.660
and testimonials and reviews


00:07:53.660 --> 00:07:56.820
and people bending the framework in various ways


00:07:56.820 --> 00:07:58.660
and forks and plugins.


00:07:58.660 --> 00:08:00.980
Another thing too, that people I think


00:08:00.980 --> 00:08:02.900
don't really understand the value of


00:08:02.900 --> 00:08:05.020
in software and open source is,


00:08:05.020 --> 00:08:07.380
or I'm sure people understand the value


00:08:07.380 --> 00:08:08.500
but it's not talked about,


00:08:08.500 --> 00:08:10.300
it's just a whole battle tested thing.


00:08:10.300 --> 00:08:14.160
Like when something is run at thousands of organizations


00:08:14.160 --> 00:08:16.580
in production for a long time,


00:08:16.580 --> 00:08:20.140
there's a lot of things that happen in a software


00:08:20.140 --> 00:08:21.620
that are very, very valuable


00:08:21.620 --> 00:08:23.860
for the criminal organization adopting it.


00:08:23.860 --> 00:08:28.020
- Well, let's talk large language models for a second.


00:08:28.020 --> 00:08:31.820
So AI means different things to different people, right?


00:08:31.820 --> 00:08:33.780
They kind of get carved off


00:08:33.780 --> 00:08:37.820
as they find some kind of productive productized use, right?


00:08:37.820 --> 00:08:39.300
AI is this general term and like,


00:08:39.300 --> 00:08:41.840
oh, machine learning is now a thing that we've done,


00:08:41.840 --> 00:08:43.740
or computer vision is a thing we've done.


00:08:43.740 --> 00:08:45.020
And the large language models


00:08:45.020 --> 00:08:48.240
are starting to find their own special space.


00:08:48.240 --> 00:08:52.080
So maybe we could talk a bit about a couple of examples,


00:08:52.080 --> 00:08:54.240
just so people get a sense.


00:08:54.240 --> 00:08:57.800
To me, ChatGPT seems like the most well-known.


00:08:57.800 --> 00:08:58.640
What do you think?


00:08:58.640 --> 00:09:01.280
- Yeah, I mean, well, I'll say if you think about


00:09:01.280 --> 00:09:03.760
what is a large language model,


00:09:03.760 --> 00:09:05.520
what are some of the leaps there?


00:09:05.520 --> 00:09:06.640
And I'm not an expert,


00:09:06.640 --> 00:09:09.720
so I'm gonna try to not put my foot in my mouth,


00:09:09.720 --> 00:09:12.000
but some things that I think are interesting.


00:09:12.000 --> 00:09:15.500
A large language model is a big neural network


00:09:15.500 --> 00:09:18.820
that is trained on a big corpus of text.


00:09:18.820 --> 00:09:20.700
I think one of the big leaps we've seen


00:09:20.700 --> 00:09:22.620
is unsupervised learning.


00:09:22.620 --> 00:09:26.020
So like really often like in machine learning in the past


00:09:26.020 --> 00:09:30.560
or pre-LLMs, we would have very specific


00:09:30.560 --> 00:09:33.940
like training set and outcomes we were looking for.


00:09:33.940 --> 00:09:37.060
And then the training data would have to be really structure.


00:09:37.060 --> 00:09:38.900
Here what we're doing with large language models


00:09:38.900 --> 00:09:42.300
is feeding a lot of huge corpus of text


00:09:42.300 --> 00:09:44.600
and what the large language model is trying to do


00:09:44.600 --> 00:09:46.760
or resolve is to chain words, right?


00:09:46.760 --> 00:09:49.320
So he's trying to predict the next word,


00:09:49.320 --> 00:09:53.040
which seems like you would be able to put words together


00:09:53.040 --> 00:09:55.320
that kind of makes sense,


00:09:55.320 --> 00:09:57.780
but like you wouldn't think that consciousness,


00:09:57.780 --> 00:09:58.660
not just like consciousness,


00:09:58.660 --> 00:10:00.160
but intelligence would come out of that,


00:10:00.160 --> 00:10:01.440
but somehow it does, right?


00:10:01.440 --> 00:10:04.280
Like if you chain, it's like, if you say, you know,


00:10:04.280 --> 00:10:06.520
Hupty Dutty sits on A,


00:10:06.520 --> 00:10:09.020
it's really clear it's gonna be wall, you know,


00:10:09.020 --> 00:10:13.520
the next word, but if you push this idea much further


00:10:13.520 --> 00:10:16.160
with a very large corpus of human knowledge,


00:10:16.160 --> 00:10:18.420
somehow there's some really amazing stuff


00:10:18.420 --> 00:10:21.400
that does happen on these large language models.


00:10:21.400 --> 00:10:24.840
And I think that realization happened around,


00:10:24.840 --> 00:10:29.420
Chad, at GPT-3, 3.5 getting pretty good,


00:10:29.420 --> 00:10:30.860
and then at 4, we're like, oh my god,


00:10:30.860 --> 00:10:34.520
this stuff can really, seems like it can think


00:10:34.520 --> 00:10:36.880
or be smart or be very helpful.


00:10:36.880 --> 00:10:39.520
- Yeah, the thing that I think impresses me the most


00:10:39.520 --> 00:10:41.800
about these is they seem,


00:10:41.800 --> 00:10:45.060
People can tell me it's statistics and I'll believe them,


00:10:45.060 --> 00:10:47.020
but it seems like they have an understanding


00:10:47.020 --> 00:10:49.440
of the context of what they're talking about


00:10:49.440 --> 00:10:51.180
more than just predicting like,


00:10:51.180 --> 00:10:52.580
Humpty Dumpty sat on the what?


00:10:52.580 --> 00:10:53.760
It sat on the wall, right?


00:10:53.760 --> 00:10:57.040
Obviously that's what was likely to come next


00:10:57.040 --> 00:10:58.620
when you see that set of words.


00:10:58.620 --> 00:11:01.700
But there's an example that I like to play with,


00:11:01.700 --> 00:11:02.660
which I copied out.


00:11:02.660 --> 00:11:03.620
I'll give it a little thing.


00:11:03.620 --> 00:11:07.660
I'll say, "Hey, here's a program, Python program.


00:11:07.660 --> 00:11:08.920
"I'm gonna ask you questions about it.


00:11:08.920 --> 00:11:09.760
"Let's call it Arrow."


00:11:09.760 --> 00:11:12.800
and it's like this is a highly nested program


00:11:12.800 --> 00:11:15.720
that function that tests whether something's a platypus.


00:11:15.720 --> 00:11:17.240
I saw this example somewhere and I thought,


00:11:17.240 --> 00:11:19.800
okay, this is pretty cool, but it has this.


00:11:19.800 --> 00:11:22.880
If it's a mammal, then if it has fur,


00:11:22.880 --> 00:11:25.280
then if it has a beak, then if it has a tail,


00:11:25.280 --> 00:11:27.560
and you can just do stuff that's really interesting.


00:11:27.560 --> 00:11:29.600
Like I'll ask it to--


00:11:29.600 --> 00:11:32.000
- Is bird or something like that or--


00:11:32.000 --> 00:11:33.560
- Yeah, yeah, yeah, rewrite it,


00:11:33.560 --> 00:11:38.560
write it using guardian clauses to be not nested, right?


00:11:39.640 --> 00:11:40.540
- Oh yeah.


00:11:40.540 --> 00:11:41.380
- Right?


00:11:41.380 --> 00:11:43.160
And it'll say, sure, here we go.


00:11:43.160 --> 00:11:45.200
And instead of being, if this, then nest,


00:11:45.200 --> 00:11:47.160
if this, then if this, it'll do, if not,


00:11:47.160 --> 00:11:48.220
if not return false, right?


00:11:48.220 --> 00:11:49.800
Which is really cool.


00:11:49.800 --> 00:11:51.840
And that's kind of a pretty interesting one.


00:11:51.840 --> 00:11:54.140
But like, this is the one,


00:11:54.140 --> 00:11:56.040
this is the example that I think is crazy.


00:11:56.040 --> 00:12:00.920
It's rewrite arrow to test for crocodiles.


00:12:00.920 --> 00:12:01.760
- Yeah, using the,


00:12:01.760 --> 00:12:04.360
it's like what people would call a one shot,


00:12:04.360 --> 00:12:05.880
a few shot example of like,


00:12:05.880 --> 00:12:09.680
hey, here's an example of the kind of stuff I might want.


00:12:09.680 --> 00:12:12.000
There's some different ways to do that,


00:12:12.000 --> 00:12:13.560
but it's a pattern in prompt engineering


00:12:13.560 --> 00:12:15.880
where you'll say you have a zero shot, one shot,


00:12:15.880 --> 00:12:18.000
few shot examples we can get into.


00:12:18.000 --> 00:12:21.040
But it does feel like it understands the code, right?


00:12:21.040 --> 00:12:22.840
Like what you're trying to do.


00:12:22.840 --> 00:12:24.840
- Right, just for people listening, it said,


00:12:24.840 --> 00:12:26.600
okay, here's a function isCrocodile.


00:12:26.600 --> 00:12:30.040
If not self.isReptile, if not self.hasScales,


00:12:30.040 --> 00:12:31.240
these are false examples, right?


00:12:31.240 --> 00:12:33.120
And if it has four legs and a long snout,


00:12:33.120 --> 00:12:34.040
it can swim, right?


00:12:34.040 --> 00:12:37.080
Like it rewrote the little tests and stuff, right?


00:12:37.080 --> 00:12:39.780
In a way that seems really unlikely


00:12:39.780 --> 00:12:42.540
that it's just predicting likelihood


00:12:42.540 --> 00:12:44.360
'cause it's never seen anything like this really,


00:12:44.360 --> 00:12:47.560
which is really, it's pretty mind blowing I think.


00:12:47.560 --> 00:12:49.900
- Or it had, like it read the entire internet


00:12:49.900 --> 00:12:51.480
and all of GitHub and that kind of stuff.


00:12:51.480 --> 00:12:53.980
So say it has seen some other things.


00:12:53.980 --> 00:12:54.820
I think that's mind boggling.


00:12:54.820 --> 00:12:58.120
It's just like, when you think about what it did there


00:12:58.120 --> 00:13:02.140
is it read the entire conversation so far,


00:13:02.140 --> 00:13:05.180
your input prompt and it has like a system prompt


00:13:05.180 --> 00:13:07.660
ahead of time that says, you know, your ChatGPT,


00:13:07.660 --> 00:13:11.340
try to be helpful to people and here's a bunch of things


00:13:11.340 --> 00:13:13.460
you should or should not say and non-biased,


00:13:13.460 --> 00:13:17.380
you try to be concise and good and virtuous.


00:13:17.380 --> 00:13:19.820
And people have found all sorts of jailbreaks out of that.


00:13:19.820 --> 00:13:22.180
But like, all it does from that point on


00:13:22.180 --> 00:13:24.820
is like try to predict the next word,


00:13:24.820 --> 00:13:27.980
which is kind of insane that it gets to, you know,


00:13:27.980 --> 00:13:29.740
the amount of structure that we see.


00:13:29.740 --> 00:13:30.620
- Right, right.


00:13:30.620 --> 00:13:32.060
That's a lot of structure there, right?


00:13:32.060 --> 00:13:33.780
So pretty impressive.


00:13:33.780 --> 00:13:35.740
And ChatGPT is starting to grow.


00:13:35.740 --> 00:13:36.900
You know, if you've got version four


00:13:36.900 --> 00:13:38.260
and you can start using some of the plugins,


00:13:38.260 --> 00:13:39.620
it's gonna keep going crazy there.


00:13:39.620 --> 00:13:42.980
Other examples are simply AI just released Lemur,


00:13:42.980 --> 00:13:44.740
which is a large language model,


00:13:44.740 --> 00:13:47.580
but really focused on transcribing speech,


00:13:47.580 --> 00:13:49.180
which I think is kind of cool.


00:13:49.180 --> 00:13:52.140
Microsoft reduced Microsoft security,


00:13:52.140 --> 00:13:54.180
released Microsoft security copilot,


00:13:54.180 --> 00:13:57.060
which is a large language model to talk about things


00:13:57.060 --> 00:14:00.420
like Nginx misconfiguration and stuff like that.


00:14:00.420 --> 00:14:03.300
There's just a lot of stuff out there


00:14:03.300 --> 00:14:05.420
that's coming along here, right?


00:14:05.420 --> 00:14:07.700
A lot of thousands of models coming in type of thing.


00:14:07.700 --> 00:14:10.060
- On the open source front too,


00:14:10.060 --> 00:14:11.540
there's this whole ethical thing,


00:14:11.540 --> 00:14:15.100
like should everyone and anyone have access


00:14:15.100 --> 00:14:16.820
to open source models doing that?


00:14:16.820 --> 00:14:18.620
Well, we don't really understand.


00:14:18.620 --> 00:14:21.820
We probably shouldn't get into the ethics part


00:14:21.820 --> 00:14:24.540
of the debate here, 'cause that's a whole series


00:14:24.540 --> 00:14:26.740
of episodes we probably won't wanna get into.


00:14:26.740 --> 00:14:29.260
But what's interesting is Databricks came up


00:14:29.260 --> 00:14:31.380
a model for what's called Facebook,


00:14:31.380 --> 00:14:32.740
came up with one called Lama,


00:14:32.740 --> 00:14:35.300
and they open sourced and/or leaked the weights,


00:14:35.300 --> 00:14:39.500
so you have the model topology with the pre-trained weights.


00:14:39.500 --> 00:14:41.740
In some cases, there's open source corpus of training


00:14:41.740 --> 00:14:46.140
that are also coming out and are also open sourced.


00:14:46.140 --> 00:14:49.180
I mean, it's like, and these open source models


00:14:49.180 --> 00:14:53.060
are somewhat competitive or increasingly competitive


00:14:53.060 --> 00:14:57.980
with GPT-4, yeah, which is kind of crazy.


00:14:57.980 --> 00:15:02.180
And some of them I don't, or where GPT-4 has limitations,


00:15:02.180 --> 00:15:03.680
they break through these limitations.


00:15:03.680 --> 00:15:05.940
So one thing that's really important


00:15:05.940 --> 00:15:09.940
as a current limitation of the GPT models and LLMs


00:15:09.940 --> 00:15:14.380
is the prompt window, the token prompt window.


00:15:14.380 --> 00:15:16.240
So basically when you ask a question,


00:15:16.240 --> 00:15:20.280
you know, it's been trained and has machine learned


00:15:20.280 --> 00:15:24.700
with data up to, I think in the case of GPT-3, 5, or 4,


00:15:24.700 --> 00:15:26.860
it's the corpus of training goes all the way


00:15:26.860 --> 00:15:29.460
to fall 2021.


00:15:29.460 --> 00:15:30.300
So if you ask like,


00:15:30.300 --> 00:15:31.740
who is the current president of the United States,


00:15:31.740 --> 00:15:33.060
it just doesn't know,


00:15:33.060 --> 00:15:36.500
or it will tell you as of 2021, it is this person.


00:15:36.500 --> 00:15:39.940
But so if you're trying to do tasks,


00:15:39.940 --> 00:15:41.060
like what I've been working on,


00:15:41.060 --> 00:15:43.140
we'll probably get into later in the conversation


00:15:43.140 --> 00:15:45.580
is trying to generate SQL.


00:15:45.580 --> 00:15:46.600
It doesn't know your table.


00:15:46.600 --> 00:15:47.440
So you have to say like,


00:15:47.440 --> 00:15:49.120
hey, here's all the tables in my database.


00:15:49.120 --> 00:15:52.060
Now, can you generate SQL that does X on top of it?


00:15:52.060 --> 00:15:56.840
And that context window is limited


00:15:56.840 --> 00:15:58.960
and increasing, but some of these open source models


00:15:58.960 --> 00:16:00.760
have different types of limitations.


00:16:00.760 --> 00:16:04.120
- This portion of Talk Python to Me


00:16:04.120 --> 00:16:05.880
is brought to you by JetBrains,


00:16:05.880 --> 00:16:09.400
who encourage you to get work done with PyCharm.


00:16:09.400 --> 00:16:11.560
PyCharm Professional is the complete IDE


00:16:11.560 --> 00:16:14.120
that supports all major Python workflows,


00:16:14.120 --> 00:16:16.300
including full stack development.


00:16:16.300 --> 00:16:18.520
That's front-end JavaScript, Python backend,


00:16:18.520 --> 00:16:19.880
and data support,


00:16:19.880 --> 00:16:23.200
as well as data science workflows with Jupyter.


00:16:23.200 --> 00:16:25.120
PyCharm just works out of the box.


00:16:25.120 --> 00:16:30.520
Some editors provide their functionality through piecemeal add-ins that you put together from


00:16:30.520 --> 00:16:32.320
a variety of sources.


00:16:32.320 --> 00:16:35.320
PyCharm is ready to go from minute one.


00:16:35.320 --> 00:16:37.780
And PyCharm thrives on complexity.


00:16:37.780 --> 00:16:42.200
The biggest selling point for me personally is that PyCharm understands the code structure


00:16:42.200 --> 00:16:48.720
of my entire project, even across languages such as Python and SQL and HTML.


00:16:48.720 --> 00:16:52.920
If you see your editor completing statements just because the word appears elsewhere in


00:16:52.920 --> 00:16:57.720
the file, but it's not actually relevant to that code block, that should make you really nervous.


00:16:57.720 --> 00:17:04.280
I've been a happy paying customer of PyCharm for years. Hardly a workday passes that I'm not deep


00:17:04.280 --> 00:17:10.040
inside PyCharm working on projects here at Talk Python. What tool is more important to your


00:17:10.040 --> 00:17:15.560
productivity than your code editor? You deserve one that works the best. So download your free


00:17:15.560 --> 00:17:24.000
trial of PyCharm professional today at talkpython.fm/donewithpycharm and get work done. That link is in your podcast


00:17:24.000 --> 00:17:29.300
player show notes. Thank you to PyCharm from JetBrains for sponsoring the show and keeping


00:17:29.300 --> 00:17:31.720
Talk Python going strong.


00:17:31.720 --> 00:17:38.040
Right. It's interesting to ask questions, right? But it's more interesting from a software


00:17:38.040 --> 00:17:43.680
developer perspective of can I teach it a little bit more about what my app needs to


00:17:43.680 --> 00:17:46.400
know or what my app structure is, right?


00:17:46.400 --> 00:17:49.280
In your case, I want to use


00:17:49.280 --> 00:17:52.040
SuperSet to ask the database questions.


00:17:52.040 --> 00:17:54.280
But if I'm going to bring in AI,


00:17:54.280 --> 00:17:57.860
it needs to understand the database structure so that when I say,


00:17:57.860 --> 00:18:00.220
"Help me do a query to do this thing,"


00:18:00.220 --> 00:18:02.560
it needs to know what the heck to do, right?


00:18:02.560 --> 00:18:07.600
>> The table, so there's this stuff it knows and the stuff it can't know.


00:18:07.600 --> 00:18:10.560
Some of it goes is really to the fact that whether


00:18:10.560 --> 00:18:13.040
this information is going to public on the Internet,


00:18:13.040 --> 00:18:15.760
whether it has happened to be trained against it.


00:18:15.760 --> 00:18:16.920
And then if it's in private,


00:18:16.920 --> 00:18:19.400
there's just no hope that it would know about,


00:18:19.400 --> 00:18:21.300
you know, your own internal documents


00:18:21.300 --> 00:18:22.220
or your database structure.


00:18:22.220 --> 00:18:26.000
So in our case, it speaks SQLs very, very well.


00:18:26.000 --> 00:18:27.280
So as we get into this example,


00:18:27.280 --> 00:18:31.480
like how to get GPT to generate good SQL


00:18:31.480 --> 00:18:34.200
in the context of a tool like SuperSET or SQL lab,


00:18:34.200 --> 00:18:36.360
which is our SQL IDE.


00:18:36.360 --> 00:18:38.120
So it knows how to speak SQL super well.


00:18:38.120 --> 00:18:41.280
It knows the different dialects of SQL very, very well.


00:18:41.280 --> 00:18:43.780
It knows its functions, its dates functions,


00:18:43.780 --> 00:18:45.980
which a lot of the SQL,


00:18:45.980 --> 00:18:47.540
and like the engineers only call it like,


00:18:47.540 --> 00:18:50.100
yeah, I can never remember like what Postgres


00:18:50.100 --> 00:18:51.860
date diff function is,


00:18:51.860 --> 00:18:54.020
but at GPT or GPT models,


00:18:54.020 --> 00:18:56.280
just it knows SQL and knows the dialects


00:18:56.280 --> 00:18:57.940
and knows the mechanical SQL.


00:18:57.940 --> 00:19:00.060
It understands data modeling, foreign keys,


00:19:00.060 --> 00:19:02.900
joins, primary, all this stuff it understands.


00:19:02.900 --> 00:19:06.580
It knows nothing about your specific database,


00:19:06.580 --> 00:19:09.500
the schema names and the table names,


00:19:09.500 --> 00:19:11.340
in the column names that I might be able to use.


00:19:11.340 --> 00:19:14.380
So that's where we need to start providing some context


00:19:14.380 --> 00:19:15.940
and this context window is limited.


00:19:15.940 --> 00:19:20.020
So it's like, how do you use that context well


00:19:20.020 --> 00:19:22.220
or as well as possible?


00:19:22.220 --> 00:19:24.980
And that's the field and some of the ideas behind it


00:19:24.980 --> 00:19:27.060
is prompt crafting and prompt engineering,


00:19:27.060 --> 00:19:30.540
which we can get into once we get there,


00:19:30.540 --> 00:19:31.940
maybe we're there already.


00:19:31.940 --> 00:19:34.940
- Yeah, yeah, well, yeah, I think where I see


00:19:34.940 --> 00:19:38.780
this stuff going is from this general purpose knowledge


00:19:38.780 --> 00:19:42.580
starting to bring in more private or personal


00:19:42.580 --> 00:19:44.660
or internal type of information, right?


00:19:44.660 --> 00:19:47.540
Like our data about our customers is like


00:19:47.540 --> 00:19:48.700
structured like this in a table


00:19:48.700 --> 00:19:49.800
and here's what we know about them.


00:19:49.800 --> 00:19:52.340
Now let us ask questions about our company


00:19:52.340 --> 00:19:53.740
and our thing, right?


00:19:53.740 --> 00:19:55.680
And it's like starting to make inroads


00:19:55.680 --> 00:19:56.700
in that direction, I think.


00:19:56.700 --> 00:19:59.860
- Yeah, and one thing to know about is


00:19:59.860 --> 00:20:03.780
that there's different approaches to teach


00:20:03.780 --> 00:20:04.980
or provide that context.


00:20:04.980 --> 00:20:09.220
So one would be to build your own model from scratch, right?


00:20:09.220 --> 00:20:11.140
And that's pretty prohibitive.


00:20:11.140 --> 00:20:13.220
So you'd have to find the right corpus.


00:20:13.220 --> 00:20:15.860
And instead of starting with a model that knows SQL


00:20:15.860 --> 00:20:17.940
and needs to know your table and context,


00:20:17.940 --> 00:20:21.380
you have to start from zero and very prohibitive.


00:20:21.380 --> 00:20:23.380
Another one is you start from a base model


00:20:23.380 --> 00:20:25.260
at some point of some kind.


00:20:25.260 --> 00:20:28.380
There's a topology, so there's different layers


00:20:28.380 --> 00:20:30.440
and number of neurons and it knows some things.


00:20:30.440 --> 00:20:32.500
And then you load up some weights that are open source.


00:20:32.500 --> 00:20:35.200
And then you say, I'm gonna tune this model


00:20:35.200 --> 00:20:38.420
to teach it my database schemas


00:20:38.420 --> 00:20:40.180
and basically my own corpus of data.


00:20:40.180 --> 00:20:42.180
So it could be your data dictionaries,


00:20:42.180 --> 00:20:43.860
could be your internal documents,


00:20:43.860 --> 00:20:47.380
it could be your GitHub code, your dbt projects.


00:20:47.380 --> 00:20:48.860
If you have one of your Airflow DAGs,


00:20:48.860 --> 00:20:51.300
be like, I'm gonna dump all this stuff in the model


00:20:51.300 --> 00:20:55.340
and that will get baked into the neural network itself.


00:20:55.340 --> 00:20:59.180
That's doable, pretty primitive in this era.


00:20:59.180 --> 00:21:01.960
If you have the challenge that we have at Preset,


00:21:01.960 --> 00:21:04.560
which is we have multiple customers with different schemas.


00:21:04.560 --> 00:21:06.040
We can't have spillover.


00:21:06.040 --> 00:21:09.200
So you have to train a model for each one of our customers


00:21:09.200 --> 00:21:11.360
and serve a model for each one of our customers.


00:21:11.360 --> 00:21:13.360
So still pretty prohibitive.


00:21:13.360 --> 00:21:16.000
And a lot of people fall back on this third or fourth method


00:21:16.000 --> 00:21:18.800
that I would call prompt engineering,


00:21:18.800 --> 00:21:21.120
which is I'm gonna use the base model,


00:21:21.120 --> 00:21:25.420
the open AI API, or just an API on LLM.


00:21:25.420 --> 00:21:27.460
And then I will, if no SQL already,


00:21:27.460 --> 00:21:29.680
I'll just say, hey, here's a bunch of tables


00:21:29.680 --> 00:21:30.520
that you might wanna use.


00:21:30.520 --> 00:21:32.300
can you generate SQL on top of it?


00:21:32.300 --> 00:21:36.540
So then that's just a big request with a lot of context.


00:21:36.540 --> 00:21:39.720
Then we have to start thinking about maximizing


00:21:39.720 --> 00:21:42.240
the use of that context window to pass the information


00:21:42.240 --> 00:21:45.300
that's most relevant within the limits


00:21:45.300 --> 00:21:47.060
allowed by the specific model.


00:21:47.060 --> 00:21:50.600
- Right, and that starts to get into reproducibility,


00:21:50.600 --> 00:21:54.360
accuracy, and just those limitations,


00:21:54.360 --> 00:21:56.940
which is kind of an engineering type of thing, right?


00:21:56.940 --> 00:21:59.460
- Yeah, and then, you know, maybe a topic too,


00:21:59.460 --> 00:22:02.580
And this conversation is based on a recent blog post


00:22:02.580 --> 00:22:05.540
and the flow, just going back to the flow of that blog post.


00:22:05.540 --> 00:22:07.940
So we started by establishing the premise


00:22:07.940 --> 00:22:12.380
that everyone is trying to bring AI into their product today.


00:22:12.380 --> 00:22:14.780
Thousands of product builders are currently exploring ways


00:22:14.780 --> 00:22:17.420
to harness the power of AI in the products


00:22:17.420 --> 00:22:19.500
and experiences they create.


00:22:19.500 --> 00:22:21.860
That's the premise for us with text to SQL


00:22:21.860 --> 00:22:24.300
and SQL lab as part of superset and preset.


00:22:24.300 --> 00:22:27.840
But I don't know, like if you think of any product,


00:22:27.840 --> 00:22:30.120
any startup, any SaaS product you use.


00:22:30.120 --> 00:22:32.500
You work at HubSpot today, you're trying to figure out


00:22:32.500 --> 00:22:37.080
how to leverage AI to build sales chatbots


00:22:37.080 --> 00:22:40.040
or SDR chatbots, so everyone everywhere


00:22:40.040 --> 00:22:41.360
is trying to figure that out.


00:22:41.360 --> 00:22:44.840
The challenge is, I guess, very probabilistic


00:22:44.840 --> 00:22:47.880
in a different interface to anything we know.


00:22:47.880 --> 00:22:50.760
Engineers would be like, oh, let's look at an API


00:22:50.760 --> 00:22:54.560
and leverage it, and APIs are very, very deterministic


00:22:54.560 --> 00:22:59.560
in general, AI is kind of wild beast to tame.


00:22:59.560 --> 00:23:03.840
You ask, first the interface is language not code,


00:23:03.840 --> 00:23:05.800
and then what comes back is like


00:23:05.800 --> 00:23:08.320
semi-probabilistic in nature.


00:23:08.320 --> 00:23:09.680
- And it could change underneath you.


00:23:09.680 --> 00:23:12.040
It's a little bit like web scraping in that regard.


00:23:12.040 --> 00:23:13.960
That like, it does the same, it does the same,


00:23:13.960 --> 00:23:17.240
and then something out there changed, not your code,


00:23:17.240 --> 00:23:20.960
and then a potentially different behavior comes back, right?


00:23:20.960 --> 00:23:23.000
'Cause they may have trained another couple of years,


00:23:23.000 --> 00:23:24.440
refine the model, switch the model,


00:23:24.440 --> 00:23:27.040
change the default temperature, all these things.


00:23:27.040 --> 00:23:29.200
- Yeah, there's a lot that can happen there.


00:23:29.200 --> 00:23:31.080
One thing I noticed, like starting to work


00:23:31.080 --> 00:23:33.280
with what I would call prompt crafting,


00:23:33.280 --> 00:23:35.640
which is, you know, you work with ChatGPT


00:23:35.640 --> 00:23:38.160
and you craft different prompt


00:23:38.160 --> 00:23:41.680
with putting emphasis in a place or another


00:23:41.680 --> 00:23:43.280
or changing the order of things


00:23:43.280 --> 00:23:44.640
or just changing a word, right?


00:23:44.640 --> 00:23:47.760
Just say like important exclamation point,


00:23:47.760 --> 00:23:49.840
capitalize the words, you know,


00:23:49.840 --> 00:23:52.200
the reserve words in SQL,


00:23:52.200 --> 00:23:54.560
And then just the fact that you put important exclamation


00:23:54.560 --> 00:23:58.040
point will make it do it or not do it,


00:23:58.040 --> 00:23:59.680
changing from a model to another.


00:23:59.680 --> 00:24:01.520
So one thing that's great is the model,


00:24:01.520 --> 00:24:06.520
at least at OpenAI, they are immutable as far as I know.


00:24:06.520 --> 00:24:11.040
But like if you use GPT-3.5 Turbo, for instance,


00:24:11.040 --> 00:24:12.920
that's just one train model.


00:24:12.920 --> 00:24:16.160
I believe that that is immutable.


00:24:16.160 --> 00:24:19.360
The chatbot on top of it might get fine tuned


00:24:19.360 --> 00:24:23.060
and change over time, but the model is supposed to be static.


00:24:23.060 --> 00:24:25.140
You mentioned temperature, it'd be kind of interesting


00:24:25.140 --> 00:24:27.800
to just mention for those who are not familiar with that.


00:24:27.800 --> 00:24:29.380
So when you interact with AI,


00:24:29.380 --> 00:24:32.100
one of the core parameters is temperature,


00:24:32.100 --> 00:24:35.760
and I think it's a value from zero to one,


00:24:35.760 --> 00:24:39.760
or I'm not sure how exactly you pass it,


00:24:39.760 --> 00:24:44.520
but it basically defines how creative you want,


00:24:44.520 --> 00:24:46.360
you want to let the AI be.


00:24:46.360 --> 00:24:48.700
Like if you put it to zero,


00:24:48.700 --> 00:24:50.380
you're gonna have something more deterministic.


00:24:50.380 --> 00:24:53.280
So asking the same question should lead to a similar


00:24:53.280 --> 00:24:56.780
or the same answer, though not in my experience.


00:24:56.780 --> 00:24:58.480
It feels like it should, but it doesn't.


00:24:58.480 --> 00:25:01.620
But then if you put a higher, it will get more creative.


00:25:01.620 --> 00:25:04.560
Talk more about like how that actually


00:25:04.560 --> 00:25:06.080
seemed to work behind the scenes.


00:25:06.080 --> 00:25:09.600
- Yeah, well, that variability seems to show up more


00:25:09.600 --> 00:25:11.360
in the image-based ones.


00:25:11.360 --> 00:25:14.320
So for example, this article, this blog post that you wrote,


00:25:14.320 --> 00:25:15.800
you have this image here and you said,


00:25:15.800 --> 00:25:18.640
oh, and I made this image from mid-journey.


00:25:18.640 --> 00:25:22.280
I've also got some examples of a couple that I did.


00:25:22.280 --> 00:25:23.480
Where did I stick them?


00:25:23.480 --> 00:25:24.680
Somewhere, here we go.


00:25:24.680 --> 00:25:27.200
Where I asked, just for YouTube thumbnails,


00:25:27.200 --> 00:25:30.320
I asked Midjourney for a radio astronomy example


00:25:30.320 --> 00:25:33.160
that I can use, 'cause here's one that's not encumbered


00:25:33.160 --> 00:25:36.200
by some sort of licensing, but still looks kinda cool


00:25:36.200 --> 00:25:37.800
and is representative, right?


00:25:37.800 --> 00:25:41.320
And there, it's like massive difference.


00:25:41.320 --> 00:25:43.960
I'm not sure how much difference I've seen.


00:25:43.960 --> 00:25:46.040
I know it will make some, but I haven't seen


00:25:46.040 --> 00:25:48.600
as dramatic of a difference on ChatGPT.


00:25:48.600 --> 00:25:50.840
- Oh, ChatGPT, yeah.


00:25:50.840 --> 00:25:53.160
Yeah, I'm not sure exactly how they introduced


00:25:53.160 --> 00:25:57.800
the variability on the generative images AI.


00:25:57.800 --> 00:26:00.080
I know it's like this multi-dimensional space


00:26:00.080 --> 00:26:03.180
with a lot of words and a lot of images in there.


00:26:03.180 --> 00:26:06.560
And then it's probably like where the location point


00:26:06.560 --> 00:26:10.700
of that, they randomized that point


00:26:10.700 --> 00:26:12.720
in that multi-dimensional space.


00:26:12.720 --> 00:26:14.820
For ChatGPT, it's pretty easy to reason about,


00:26:14.820 --> 00:26:16.840
and I might be wrong on this, again, I'm not an expert,


00:26:16.840 --> 00:26:19.900
But you know how the way it works is it writes,


00:26:19.900 --> 00:26:22.760
it takes the prompt and then it comes up


00:26:22.760 --> 00:26:24.620
with the next word sequentially.


00:26:24.620 --> 00:26:27.300
So for each word for the next word,


00:26:27.300 --> 00:26:32.160
so Humpty Dumpty sat on A, it might be wall at 99%,


00:26:32.160 --> 00:26:36.780
but like there might be 1% of fence or something like that.


00:26:36.780 --> 00:26:40.780
And if you up the temperature,


00:26:40.780 --> 00:26:44.980
it's more likely to pick the non-first word


00:26:44.980 --> 00:26:46.700
and that probably less,


00:26:46.700 --> 00:26:48.300
so they probably do in a weighted way,


00:26:48.300 --> 00:26:50.400
like it's possible that I take a second


00:26:50.400 --> 00:26:52.240
or the third word randomly,


00:26:52.240 --> 00:26:54.160
and then of course it's gonna get a tree


00:26:54.160 --> 00:26:55.720
or decision tree once it picks the words,


00:26:55.720 --> 00:26:57.780
the next word is also changes.


00:26:57.780 --> 00:27:01.780
So as you up that, it goes down path


00:27:01.780 --> 00:27:04.900
that sends it into more creative or different.


00:27:04.900 --> 00:27:05.900
- Right, right.


00:27:05.900 --> 00:27:07.300
Yeah, a little butterfly effect,


00:27:07.300 --> 00:27:08.700
it makes a different choice here,


00:27:08.700 --> 00:27:09.540
and then it sends it,


00:27:09.540 --> 00:27:11.820
you know, sends it down through the graph.


00:27:11.820 --> 00:27:12.660
Interesting.


00:27:12.660 --> 00:27:14.820
So one thing that you really pointed out here,


00:27:14.820 --> 00:27:17.220
and I think it's maybe worth touching on a bit,


00:27:17.220 --> 00:27:19.420
is this idea of prompt engineering.


00:27:19.420 --> 00:27:22.140
There's even places like learnprompting.org


00:27:22.140 --> 00:27:25.060
that try to teach you how to talk to these things.


00:27:25.060 --> 00:27:28.580
And you make a strong distinction between prompt crafting


00:27:28.580 --> 00:27:30.140
or just talking to the AI


00:27:30.140 --> 00:27:33.820
versus really trying to put an engineering focus on it.


00:27:33.820 --> 00:27:35.940
Do you wanna talk about the differentiation?


00:27:35.940 --> 00:27:38.500
- Yeah, I think it's a super important differentiation,


00:27:38.500 --> 00:27:40.660
but one that I'm proposing, right?


00:27:40.660 --> 00:27:43.060
So I don't think that people have settled


00:27:43.060 --> 00:27:45.540
as to what is one or what is the other.


00:27:45.540 --> 00:27:48.220
I think I saw a Reddit post recently that was like,


00:27:48.220 --> 00:27:50.660
prompt engineering is just a load of crap.


00:27:50.660 --> 00:27:52.660
Like, you know, anyone can go,


00:27:52.660 --> 00:27:55.020
'cause they thought their understanding


00:27:55.020 --> 00:27:56.860
of prompt engineering was like,


00:27:56.860 --> 00:28:00.180
oh, you know, you fine tune or you craft your prompt


00:28:00.180 --> 00:28:02.580
and you say like, you are an expert AI


00:28:02.580 --> 00:28:06.080
working on, you know, creating molecules.


00:28:06.080 --> 00:28:07.280
Now can you do this?


00:28:07.280 --> 00:28:08.620
And then, you know, by doing that,


00:28:08.620 --> 00:28:10.780
you might get a better outcome.


00:28:10.780 --> 00:28:12.540
Or one really interesting thing


00:28:12.540 --> 00:28:15.460
that people have been doing in prompt crafting


00:28:15.460 --> 00:28:17.420
that seem to have huge impact on,


00:28:17.420 --> 00:28:20.940
there's been paper written on this specific


00:28:20.940 --> 00:28:25.940
just hint or craft tweak is let's proceed step by step.


00:28:25.940 --> 00:28:30.460
So basically whatever the question is that you are asking,


00:28:30.460 --> 00:28:32.560
specifically around more mathematicals


00:28:32.560 --> 00:28:35.560
or things that require more systematic


00:28:35.560 --> 00:28:38.100
step by step thinking, the whole just like


00:28:38.100 --> 00:28:40.320
let's think, let's expose this


00:28:40.320 --> 00:28:41.980
or let's go about it step by step


00:28:41.980 --> 00:28:43.620
makes it much better.


00:28:43.620 --> 00:28:45.540
So here you might be able to,


00:28:45.540 --> 00:28:49.860
well, so, you know, if you had an example


00:28:49.860 --> 00:28:54.220
where ChatGPT-3 failed or ChatGPT-4 failed,


00:28:54.220 --> 00:28:56.900
you could just say, colon, let's go step by step,


00:28:56.900 --> 00:28:59.940
and it might succeed that time around.


00:28:59.940 --> 00:29:02.740
- Maybe you can get it to help you understand


00:29:02.740 --> 00:29:04.580
instead of just get the answer, right?


00:29:04.580 --> 00:29:09.380
Like, factor this polynomial into its primary,


00:29:09.380 --> 00:29:10.900
you know, solutions or roots or whatever,


00:29:10.900 --> 00:29:12.100
and you're like, okay, show me,


00:29:12.100 --> 00:29:14.340
don't just show me the answer, show me step by step


00:29:14.340 --> 00:29:16.420
so I could understand and try to learn


00:29:16.420 --> 00:29:17.660
from what you've done, right?


00:29:17.660 --> 00:29:19.500
- Yeah, I mean, if you think about how the way


00:29:19.500 --> 00:29:21.700
that it's trying to come up with a new word,


00:29:21.700 --> 00:29:24.700
if all it does is a language-based answer


00:29:24.700 --> 00:29:27.860
to a mathematical question, like how many days


00:29:27.860 --> 00:29:30.380
are there between this date and that date?


00:29:30.380 --> 00:29:34.460
There's no, that specific example might not exist


00:29:34.460 --> 00:29:36.460
or it's kind of complicated for it to go about it,


00:29:36.460 --> 00:29:38.380
but if you say, let's think step by step,


00:29:38.380 --> 00:29:40.500
okay, there's this many months,


00:29:40.500 --> 00:29:42.980
this month's duration is this long,


00:29:42.980 --> 00:29:46.060
there's this many days since the beginning of that month,


00:29:46.060 --> 00:29:48.660
and might get it right that time around.


00:29:48.660 --> 00:29:51.500
- Right, or if it fails, you could pick up part way along


00:29:51.500 --> 00:29:52.780
where it has some more--


00:29:52.780 --> 00:29:54.140
- Yeah, you know, and then you can trace,


00:29:54.140 --> 00:29:56.860
I mean, just you too, I think one thing is


00:29:56.860 --> 00:29:59.500
you should be extremely careful as taking for granted


00:29:59.500 --> 00:30:01.060
that it's right all the time, you know?


00:30:01.060 --> 00:30:04.640
So that means it also helps you review its process


00:30:04.640 --> 00:30:06.780
and where it might be wrong.


00:30:06.780 --> 00:30:08.940
But back to crafting versus engineering.


00:30:08.940 --> 00:30:11.020
So crafting would be the process


00:30:11.020 --> 00:30:15.740
that I think is more attached to a use ChatGPT every day,


00:30:15.740 --> 00:30:19.100
the same way that we've been trained at Googling


00:30:19.100 --> 00:30:20.720
over the past two decades.


00:30:20.720 --> 00:30:24.580
You use quotes, you use plus and minus,


00:30:24.580 --> 00:30:28.660
and you know which keywords to use intuitively,


00:30:28.660 --> 00:30:30.020
where it's gonna get confused or not.


00:30:30.020 --> 00:30:33.540
So I think prompt crafting is a different version of that


00:30:33.540 --> 00:30:35.780
that's just more worthy.


00:30:35.780 --> 00:30:39.140
And if you're working with the AI to try to assist you,


00:30:39.140 --> 00:30:41.260
write your blog post, or to try to assist you


00:30:41.260 --> 00:30:43.660
in any task really, just to be smart about


00:30:43.660 --> 00:30:47.900
how you bring the context, how you tell it to proceed,


00:30:47.900 --> 00:30:49.380
goes a very, very long way.


00:30:49.380 --> 00:30:51.380
So that's what I call prompt crafting,


00:30:51.380 --> 00:30:54.140
call it like one-off cases.


00:30:54.140 --> 00:30:56.460
- Kind of what people do when they're interacting


00:30:56.460 --> 00:30:58.300
with the large language model.


00:30:58.300 --> 00:30:59.140
- I think so, right?


00:30:59.140 --> 00:31:01.260
Like it's not evident for a lot of people


00:31:01.260 --> 00:31:03.800
who are exploring the edge of where it fails,


00:31:03.800 --> 00:31:05.920
and they love to see it fail.


00:31:05.920 --> 00:31:08.200
And then they don't think about like,


00:31:08.200 --> 00:31:11.040
oh, what could I have told it to get the answer


00:31:11.040 --> 00:31:12.000
I was actually looking for?


00:31:12.000 --> 00:31:13.320
Like, ah, I got you wrong.


00:31:13.320 --> 00:31:16.440
You know, it's as if I had that actor in a conversation


00:31:16.440 --> 00:31:18.880
of like, ah, you're wrong and I told you so.


00:31:18.880 --> 00:31:21.240
You know, so I think there's a lot of that online.


00:31:21.240 --> 00:31:23.560
But I think for all these examples that I've seen,


00:31:23.560 --> 00:31:25.920
I'm really tempted to take the prompt that they had


00:31:25.920 --> 00:31:28.560
and then give it an instruction or two or more


00:31:28.560 --> 00:31:30.760
and then figure out how to get it to come up


00:31:30.760 --> 00:31:31.600
with the right thing.


00:31:31.600 --> 00:31:33.600
super important skill.


00:31:33.600 --> 00:31:35.600
You know, you could probably get a boost


00:31:35.600 --> 00:31:37.600
of for most knowledge information


00:31:37.600 --> 00:31:39.600
workers, you'll get a boost of 50%


00:31:39.600 --> 00:31:41.600
to 10x for a lot of the tasks you do


00:31:41.600 --> 00:31:43.600
every day if you use AI well. So it's


00:31:43.600 --> 00:31:45.600
great personal skill to have


00:31:45.600 --> 00:31:47.600
go and develop that skill if you don't.


00:31:47.600 --> 00:31:49.600
This portion of Talk Python to Me


00:31:49.600 --> 00:31:51.600
is sponsored by the Compiler Podcast


00:31:51.600 --> 00:31:53.600
from Red Hat. Just like you, I'm a


00:31:53.600 --> 00:31:55.600
big fan of podcasts and I'm happy


00:31:55.600 --> 00:31:57.600
to share a new one from a highly respected


00:31:57.600 --> 00:31:59.600
open source company. Compiler,


00:31:59.600 --> 00:32:02.000
an original podcast from Red Hat.


00:32:02.000 --> 00:32:05.880
Do you want to stay on top of tech without dedicating tons of time to it?


00:32:05.880 --> 00:32:09.960
Compiler presents perspectives, topics, and insights from the tech industry, free from


00:32:09.960 --> 00:32:11.560
jargon and judgment.


00:32:11.560 --> 00:32:14.840
They want to discover where technology is headed beyond the headlines and create a place


00:32:14.840 --> 00:32:18.560
for new IT professionals to learn, grow, and thrive.


00:32:18.560 --> 00:32:22.280
Compiler helps people break through the barriers and challenges turning code into community


00:32:22.280 --> 00:32:25.120
at all levels of the enterprise.


00:32:25.120 --> 00:32:28.560
One recent and interesting episode is their "The Great Stack Debate."


00:32:28.560 --> 00:32:32.520
I love love love talking to people about how they architect their code, the trade-offs


00:32:32.520 --> 00:32:37.860
and conventions they chose, and the costs, challenges, and smiles that result.


00:32:37.860 --> 00:32:40.300
This Great Stack Debate episode is like that.


00:32:40.300 --> 00:32:44.860
Check it out and see if software is more like an onion, or more like lasagna, or maybe even


00:32:44.860 --> 00:32:46.500
more complicated than that.


00:32:46.500 --> 00:32:50.440
It's the first episode in Compiler's series on software stacks.


00:32:50.440 --> 00:32:54.360
Learn more about Compiler at talkpython.fm/compiler.


00:32:54.360 --> 00:32:56.480
The link is in your podcast player show notes.


00:32:56.480 --> 00:33:00.080
And yes, you could just go search for a compiler and subscribe to it.


00:33:00.080 --> 00:33:05.340
But follow that link and click on your players icon to add it that way they know you came


00:33:05.340 --> 00:33:06.340
from us.


00:33:06.340 --> 00:33:11.440
Our thanks to the compiler podcast for keeping this podcast going strong.


00:33:11.440 --> 00:33:17.680
From engineering, in my case, I'm like, you're building something, you're using an AI as


00:33:17.680 --> 00:33:24.760
an API behind the scene, you want to pass it a bunch of relevant contexts, really specify


00:33:24.760 --> 00:33:26.160
what you want to get out of it.


00:33:26.160 --> 00:33:28.660
Maybe you even want to get a structured output, right?


00:33:28.660 --> 00:33:31.060
You might want to get a JSON blob out of it.


00:33:31.060 --> 00:33:34.260
You say, "Return a JSON blob with the following format,"


00:33:34.260 --> 00:33:36.260
so it's more structured.


00:33:36.260 --> 00:33:38.860
So then to give all these instructions,


00:33:38.860 --> 00:33:41.160
there's this idea of providing few shots too.


00:33:41.160 --> 00:33:43.760
You might be storing context in a vector database.


00:33:43.760 --> 00:33:45.660
I don't know if we're getting ahead to that today,


00:33:45.660 --> 00:33:49.360
but there are ways to kind of structure


00:33:49.360 --> 00:33:51.260
and organize your potential embeddings


00:33:51.260 --> 00:33:52.860
or the things you want to pass as context.


00:33:52.860 --> 00:33:53.960
So there's a lot here.


00:33:53.960 --> 00:33:58.780
I think somewhere to I talked about prompt engineering if we'd scroll in the blog post like what is


00:33:58.780 --> 00:34:07.060
And prompt engineering it will list the kind of things it might be higher in the post. I was scrolling for people


00:34:07.060 --> 00:34:11.580
When I introduce what is what is prompt engineering? Yeah


00:34:11.580 --> 00:34:20.480
Above this section about Mickey you scroll at the big end like what is prompt engineering?


00:34:21.480 --> 00:34:22.320
>> Yeah, here.


00:34:22.320 --> 00:34:23.240
>> Yeah, right here.


00:34:23.240 --> 00:34:26.480
The definition of this is Chad's GPT's version of it.


00:34:26.480 --> 00:34:27.800
When you do prompt engineering,


00:34:27.800 --> 00:34:29.200
you can add context,


00:34:29.200 --> 00:34:30.480
which that means that you're going to have to


00:34:30.480 --> 00:34:32.760
retrieve context maybe from a database,


00:34:32.760 --> 00:34:34.460
from a user session,


00:34:34.460 --> 00:34:37.440
from your Redux store if you're in the front end.


00:34:37.440 --> 00:34:39.720
You're going to go and fetch the context


00:34:39.720 --> 00:34:41.880
that's relevant in the context of the application,


00:34:41.880 --> 00:34:43.380
at least while building products.


00:34:43.380 --> 00:34:44.740
Specify an answer format.


00:34:44.740 --> 00:34:46.720
You could just say, yes, I just want a yes or no,


00:34:46.720 --> 00:34:50.520
a Boolean, I want a JSON blob with not only the answer,


00:34:50.520 --> 00:34:53.880
but your confidence on that answer or something like that.


00:34:53.880 --> 00:34:56.840
Limiting scope, asking for pros and cons,


00:34:56.840 --> 00:34:59.820
incorporating verification or sourcing.


00:34:59.820 --> 00:35:02.320
So that's more, you know, if you iterate on a prompt,


00:35:02.320 --> 00:35:03.500
you're gonna be rigorous about,


00:35:03.500 --> 00:35:06.680
is this prompt better than the previous prompt I had?


00:35:06.680 --> 00:35:10.400
Like if I pass five rows of sample data


00:35:10.400 --> 00:35:14.280
while doing text to SQL, does it do better than if I,


00:35:14.280 --> 00:35:17.100
or does it do more poorly than if I pass 10 rows


00:35:17.100 --> 00:35:19.360
of sample data or provide a certain amount


00:35:19.360 --> 00:35:21.240
of column level statistics.


00:35:21.240 --> 00:35:23.580
So, prompt engineering is not just prompt crafting.


00:35:23.580 --> 00:35:27.000
It is like bringing maybe the scientific method to it,


00:35:27.000 --> 00:35:30.400
bring some engineering of like fetching the right context


00:35:30.400 --> 00:35:33.440
and organizing it well and then measuring the outcome.


00:35:33.440 --> 00:35:34.260
- Right, exactly.


00:35:34.260 --> 00:35:35.840
Something that comes out, you can measure and say,


00:35:35.840 --> 00:35:40.220
this is 10% better by my metric than it was before


00:35:40.220 --> 00:35:41.700
with this additional data, right?


00:35:41.700 --> 00:35:43.000
That's a big difference.


00:35:43.000 --> 00:35:45.720
- Right, and then there's so many things moving, right?


00:35:45.720 --> 00:35:48.960
Like, and everything is changing so fast in the space


00:35:48.960 --> 00:35:51.960
So you're like, oh, well, ChatGPT5 is out,


00:35:51.960 --> 00:35:55.280
or GPT4 Turbo is half the price and then just came out.


00:35:55.280 --> 00:35:56.560
Now I'm just gonna move to that.


00:35:56.560 --> 00:35:59.080
They're like, wait, is that performing better?


00:35:59.080 --> 00:36:01.860
Or what are the trade-off?


00:36:01.860 --> 00:36:05.160
Or even I'm gonna add, I'm gonna move this section,


00:36:05.160 --> 00:36:08.720
asking for a certain JSON format above this other section.


00:36:08.720 --> 00:36:12.480
I'm gonna write important exclamation point, do X.


00:36:12.480 --> 00:36:14.600
Does that improve my results?


00:36:14.600 --> 00:36:19.340
as I mess it up and which one of my test case perhaps


00:36:19.340 --> 00:36:21.280
that succeeded before fails now


00:36:21.280 --> 00:36:22.940
and which one failed before succeeds now.


00:36:22.940 --> 00:36:24.860
So it can be like, is that a better


00:36:24.860 --> 00:36:28.680
or worse iteration towards my goal?


00:36:28.680 --> 00:36:29.520
- Right, right.


00:36:29.520 --> 00:36:34.240
Kind of bringing this unit testing TDD mindset.


00:36:34.240 --> 00:36:35.080
- Yes, yeah.


00:36:35.080 --> 00:36:37.740
So that's what we're getting deeper into the blog post.


00:36:37.740 --> 00:36:41.620
Right, so the blog post is talking about bringing this TDD,


00:36:41.620 --> 00:36:46.620
the test-driven development mindset to prompt engineering.


00:36:46.620 --> 00:36:49.860
Right, and there's a lot of things that are in common.


00:36:49.860 --> 00:36:53.780
You can take and apply and kind of transfer just over.


00:36:53.780 --> 00:36:55.540
There are some things to that breakdown


00:36:55.540 --> 00:36:56.820
that are fundamentally different


00:36:56.820 --> 00:36:59.260
between testing a prompt or working with AI


00:36:59.260 --> 00:37:04.100
and working with just a bit more deterministic


00:37:04.100 --> 00:37:05.900
code testing type framework.


00:37:05.900 --> 00:37:08.580
- Yeah, yeah, for sure.


00:37:08.580 --> 00:37:11.500
So you called out a couple of reasons


00:37:11.500 --> 00:37:13.860
of why TDD is important for prompt engineering.


00:37:13.860 --> 00:37:15.380
Maybe we could run through those.


00:37:15.380 --> 00:37:20.340
- Yeah, so the first thing is the AI model


00:37:20.340 --> 00:37:22.300
is not a deterministic things,


00:37:22.300 --> 00:37:27.300
or you use a modern API or a GraphQL REST API.


00:37:27.300 --> 00:37:30.820
The format of what you ask is extremely clear,


00:37:30.820 --> 00:37:32.540
and then the format of what you get back


00:37:32.540 --> 00:37:34.740
is usually defined by a schema.


00:37:34.740 --> 00:37:36.180
It's very deterministic.


00:37:36.180 --> 00:37:38.340
Pretty guaranteed that you do the same request


00:37:38.340 --> 00:37:41.840
to get the same output-ish or at least format.


00:37:41.840 --> 00:37:43.600
With AI, that's not the case, right?


00:37:43.600 --> 00:37:48.600
So it's much more unpredictable and probabilistic by nature.


00:37:48.600 --> 00:37:50.460
Second one is handling complexity.


00:37:50.460 --> 00:37:53.440
So AI systems are complex, black boxy,


00:37:53.440 --> 00:37:55.080
kind of unpredictable too.


00:37:55.080 --> 00:37:58.820
So embrace that and assume that you might get something


00:37:58.820 --> 00:38:02.180
really creative coming out of there for better or for worse.


00:38:02.180 --> 00:38:06.520
And then reducing risk, like you're shipping product.


00:38:06.520 --> 00:38:09.520
If you're shipping product, writing product,


00:38:09.520 --> 00:38:13.220
you don't want necessarily any sort of like bias


00:38:13.220 --> 00:38:17.080
or weird thing like the AI could go crazy.


00:38:17.080 --> 00:38:20.040
- Yeah, there are examples of AIs going crazy before


00:38:20.040 --> 00:38:22.920
like Tay, do you remember Microsoft Tay?


00:38:22.920 --> 00:38:25.680
- I don't know that one, but I know of other examples.


00:38:25.680 --> 00:38:28.200
- Yeah, I mean, it came out and it was like this sort of


00:38:28.200 --> 00:38:30.560
just, I'm here to learn from you internet


00:38:30.560 --> 00:38:33.040
and people just turned it into a racist


00:38:33.040 --> 00:38:34.640
and made it do all sorts of horrible things.


00:38:34.640 --> 00:38:36.240
And they had to shut it down a couple of days later


00:38:36.240 --> 00:38:39.800
because it just, it's like, whoa, it met the internet


00:38:39.800 --> 00:38:41.100
and the internet is mean.


00:38:41.100 --> 00:38:42.920
So that's not great.


00:38:42.920 --> 00:38:45.520
- Yeah, train it on 4chan or let it,


00:38:45.520 --> 00:38:48.440
you know, go crawl 4chan and read it.


00:38:48.440 --> 00:38:49.880
It's not always gonna be nice.


00:38:49.880 --> 00:38:51.320
- So bad, right?


00:38:51.320 --> 00:38:53.320
I mean, you don't entirely control


00:38:53.320 --> 00:38:54.800
what's gonna come out of those things.


00:38:54.800 --> 00:38:57.920
And so, you're a little more predictable, right?


00:38:57.920 --> 00:39:00.600
- And it's not even like you don't entirely control.


00:39:00.600 --> 00:39:02.880
Like I think, yeah, like basically, you know,


00:39:02.880 --> 00:39:04.840
control might be a complete illusion.


00:39:04.840 --> 00:39:07.000
Like even the people working at OpenAI


00:39:07.000 --> 00:39:09.940
don't fully understand what's happening in there.


00:39:09.940 --> 00:39:11.640
(laughs)


00:39:11.640 --> 00:39:13.720
Well, it read a bunch of stuff


00:39:13.720 --> 00:39:15.780
and it's predicting the next word


00:39:15.780 --> 00:39:18.420
and it gets most things right.


00:39:18.420 --> 00:39:20.480
By the way, they do a lot around this idea


00:39:20.480 --> 00:39:21.580
of like not necessarily TDD,


00:39:21.580 --> 00:39:23.080
but there's a whole eval framework


00:39:23.080 --> 00:39:26.720
so you can submit your evaluation functions to OpenAI.


00:39:26.720 --> 00:39:29.240
And as they train the next version of things,


00:39:29.240 --> 00:39:33.560
they include that in what their evaluation system


00:39:33.560 --> 00:39:34.400
for the new thing.


00:39:34.400 --> 00:39:36.720
Say, if I wanted to go and contribute back a bunch of like


00:39:36.720 --> 00:39:41.000
text to SQL type use cases as they call evals,


00:39:41.000 --> 00:39:43.480
then they would take that into consideration


00:39:43.480 --> 00:39:45.840
when they train their next models.


00:39:45.840 --> 00:39:48.080
All right, so going down the list, reducing risk, right?


00:39:48.080 --> 00:39:50.120
So you're integrating a beast


00:39:50.120 --> 00:39:52.640
that's not fully tamed into your product.


00:39:52.640 --> 00:39:54.640
You probably wanna make sure it's tamed enough


00:39:54.640 --> 00:39:57.060
to live inside your product.


00:39:57.060 --> 00:39:58.680
Continuous improvements,


00:39:58.680 --> 00:40:01.200
that should have been maybe the first one in the list


00:40:01.200 --> 00:40:03.160
is you're iterating on your prompts,


00:40:03.160 --> 00:40:05.460
you're trying to figure out a past context,


00:40:05.460 --> 00:40:09.080
you're trying different model versions,


00:40:09.080 --> 00:40:11.800
maybe you're trying some open source models


00:40:11.800 --> 00:40:15.440
or the latest GPT cheaper, greater thing.


00:40:15.440 --> 00:40:18.320
So you wanna make sure that as you iterate,


00:40:18.320 --> 00:40:20.080
you're getting to the actual outcomes


00:40:20.080 --> 00:40:21.720
that you want systematically.


00:40:21.720 --> 00:40:23.200
And performance measurement too,


00:40:23.200 --> 00:40:24.360
of like how long does it take?


00:40:24.360 --> 00:40:26.000
How much does it cost?


00:40:26.000 --> 00:40:30.280
You kind of need to have a handle on that.


00:40:30.280 --> 00:40:34.440
The new model might be 3% better on your corpus of tests,


00:40:34.440 --> 00:40:36.000
but it might be six times the price.


00:40:36.000 --> 00:40:38.000
Like, do you want, are you okay with that?


00:40:38.000 --> 00:40:41.040
- Right, right, or just from a user perspective, yeah.


00:40:41.040 --> 00:40:42.260
- Time to interaction, you know,


00:40:42.260 --> 00:40:44.480
that's one thing with AI we're realizing now


00:40:44.480 --> 00:40:47.200
is a lot of the prompts on four will be like,


00:40:47.200 --> 00:40:50.920
you know, two to, one to seven seconds,


00:40:50.920 --> 00:40:52.600
which in the Google era, you know,


00:40:52.600 --> 00:40:55.720
there's been some really great papers out of Google


00:40:55.720 --> 00:40:58.520
early on that prove that, you know,


00:40:58.520 --> 00:41:02.280
it's like 100 milliseconds as an impact on user behaviors


00:41:02.280 --> 00:41:03.120
and how long they stay.


00:41:03.120 --> 00:41:04.440
- Right.


00:41:04.440 --> 00:41:07.520
Yeah, people give up on checkout flows or whatever


00:41:07.520 --> 00:41:09.120
going to the next part of your site


00:41:09.120 --> 00:41:12.100
on a, measurably on 100 millisecond blocks, right?


00:41:12.100 --> 00:41:14.280
When you're talking, well, here's 7,000,


00:41:14.280 --> 00:41:15.520
here's 70 of those.


00:41:15.520 --> 00:41:17.000
That's gonna have an effect, potentially.


00:41:17.000 --> 00:41:20.360
- Oh, it has, it has been proven and very intricate


00:41:20.360 --> 00:41:24.200
in usage patterns, session duration, session outcomes,


00:41:24.200 --> 00:41:27.520
right, and a second is a mountain.


00:41:27.520 --> 00:41:30.080
If today, like we were to AB test Google


00:41:30.080 --> 00:41:33.000
between like whatever millisecond it's at now,


00:41:33.000 --> 00:41:35.440
for like just one second or half a second,


00:41:35.440 --> 00:41:37.800
that the results coming out of that AB test


00:41:37.800 --> 00:41:39.840
would show very, very different behaviors.


00:41:39.840 --> 00:41:40.680
- Wow.


00:41:40.680 --> 00:41:41.840
- I think there's some, don't quote me on it,


00:41:41.840 --> 00:41:43.280
there's some really great papers, you know,


00:41:43.280 --> 00:41:46.080
written on TTI and just time to interaction


00:41:46.080 --> 00:41:48.320
and the way it influences user behavior.


00:41:48.320 --> 00:41:50.560
So we're still, you know, in the AI world,


00:41:50.560 --> 00:41:53.040
it has to, if you're gonna wait two to seven seconds


00:41:53.040 --> 00:41:54.500
for your prompt to come back,


00:41:54.500 --> 00:41:58.300
it's got to add some real important value to what's happening.


00:41:58.300 --> 00:41:59.140
- Yeah, it does.


00:41:59.140 --> 00:42:01.860
I think it's interesting that it's presented as a chat.


00:42:01.860 --> 00:42:04.060
I think that gives people a little bit of a pause.


00:42:04.060 --> 00:42:05.180
Like, oh, it's talking to me.


00:42:05.180 --> 00:42:07.180
So let's let it think for a second,


00:42:07.180 --> 00:42:08.300
rather than it's a website


00:42:08.300 --> 00:42:09.940
that's supposed to give me an answer.


00:42:09.940 --> 00:42:11.340
- Yeah, compared to then, I guess,


00:42:11.340 --> 00:42:13.700
your basis for comparison is a human,


00:42:13.700 --> 00:42:17.700
not a website or comparing against Google.


00:42:17.700 --> 00:42:18.540
So that's great.


00:42:18.540 --> 00:42:20.140
- Yeah, I ask it a really hard question.


00:42:20.140 --> 00:42:21.220
Give it some time, right?


00:42:21.220 --> 00:42:22.980
Like that's not normally how we think about these things.


00:42:22.980 --> 00:42:25.040
Okay, so you have a kind of a workflow


00:42:25.040 --> 00:42:27.300
from this engineering, building a product,


00:42:27.300 --> 00:42:30.760
testing, like an AI inside of your product.


00:42:30.760 --> 00:42:32.340
You wanna walk us through your workflow here?


00:42:32.340 --> 00:42:34.140
- Yeah, and you know, if you,


00:42:34.140 --> 00:42:36.460
I think I looked at TDD, you know,


00:42:36.460 --> 00:42:40.580
and originally, what is the normal TDD type workflow?


00:42:40.580 --> 00:42:44.380
And I just adapted this little diagram


00:42:44.380 --> 00:42:46.700
to prompt engineering, right?


00:42:46.700 --> 00:42:48.020
'Cause the whole idea of the blog post


00:42:48.020 --> 00:42:49.940
is to bring prompt engine,


00:42:49.940 --> 00:42:51.820
like TDD mindset to prompt engineering.


00:42:51.820 --> 00:42:55.620
So this is where I went, but yeah, the workflow is like,


00:42:55.620 --> 00:42:58.940
okay, define the use case and desired AI behavior.


00:42:58.940 --> 00:43:00.540
What are you trying to solve with AI?


00:43:00.540 --> 00:43:03.380
In my case, the example that I'll use


00:43:03.380 --> 00:43:06.700
and try to reuse throughout this presentation is,


00:43:06.700 --> 00:43:11.000
throughout this conversation is, you know, text to SQL.


00:43:11.000 --> 00:43:13.300
So like we're trying to, what a user prompt,


00:43:13.300 --> 00:43:16.220
what a database schema, get the AI to generate


00:43:16.220 --> 00:43:19.540
good, useful SQL, find the right tables and columns to use,


00:43:19.540 --> 00:43:20.740
that kind of stuff.


00:43:20.740 --> 00:43:22.160
I create test cases.


00:43:22.160 --> 00:43:24.780
So it's like, okay, if I have this database


00:43:24.780 --> 00:43:26.500
and I have this prompt,


00:43:26.500 --> 00:43:29.100
give me my top five salary per department


00:43:29.100 --> 00:43:31.060
on this HR dataset,


00:43:31.060 --> 00:43:34.180
there's a fairly deterministic output to that.


00:43:34.180 --> 00:43:36.740
You could say the SQL is not necessarily deterministic.


00:43:36.740 --> 00:43:38.580
There's different ways to write that SQL.


00:43:38.580 --> 00:43:41.500
There's a deterministic data frame or results set


00:43:41.500 --> 00:43:42.340
that might come up.


00:43:42.340 --> 00:43:45.060
- There is a right answer of the top five salaries.


00:43:45.060 --> 00:43:45.900
- That's right.


00:43:45.900 --> 00:43:47.860
- You're not getting, ultimately get that.


00:43:47.860 --> 00:43:50.900
And it's great if it is deterministic


00:43:50.900 --> 00:43:52.180
'cause you can test it.


00:43:52.180 --> 00:43:55.680
If you're trying to use AI to say write an essay


00:43:55.680 --> 00:44:00.420
about Napoleon Bonaparte's second conquest,


00:44:00.420 --> 00:44:05.260
in less than 500 words, it's not as deterministic


00:44:05.260 --> 00:44:08.780
and it's hard to test whether the AI is doing good or not.


00:44:08.780 --> 00:44:10.740
So you might need human evaluators.


00:44:10.740 --> 00:44:14.340
But I would say in most AI product,


00:44:14.340 --> 00:44:16.780
or people are trying to bring AI into their product,


00:44:16.780 --> 00:44:18.900
in many cases more deterministic.


00:44:18.900 --> 00:44:20.540
So another example of like more deterministic


00:44:20.540 --> 00:44:22.940
would say like, oh, getting,


00:44:22.940 --> 00:44:26.380
if you say getting AI to write Python functions,


00:44:26.380 --> 00:44:28.540
it's like, oh, write a function that, you know,


00:44:28.540 --> 00:44:33.540
returns if a number is prime, yes or no,


00:44:33.540 --> 00:44:36.740
like that you can get the function


00:44:36.740 --> 00:44:39.500
and test it in a deterministic kind of way.


00:44:39.500 --> 00:44:42.020
So anyways, just pointing out, it's better,


00:44:42.020 --> 00:44:44.180
you're only gonna be able to have a TDD mindset


00:44:44.180 --> 00:44:47.380
if you have a somewhat deterministic outcome


00:44:47.380 --> 00:44:49.980
to the one I use the AI for.


00:44:49.980 --> 00:44:51.300
Then create a PROM generator.


00:44:51.300 --> 00:44:52.540
So that would be your first version


00:44:52.540 --> 00:44:54.780
or in the text to SQL example,


00:44:54.780 --> 00:44:58.380
it's given the 20 tables in this database


00:44:58.380 --> 00:45:00.580
and this columns and table names


00:45:00.580 --> 00:45:02.940
and data types and sample data,


00:45:02.940 --> 00:45:05.540
generate SQL that answers the following user prompt.


00:45:05.540 --> 00:45:07.500
And then the user prompt would say something like


00:45:07.500 --> 00:45:10.960
department by top five salary per department.


00:45:12.020 --> 00:45:14.000
And then we're getting for people


00:45:14.000 --> 00:45:16.340
that are not on the visual stream,


00:45:16.340 --> 00:45:18.220
not YouTube, but on just audio,


00:45:18.220 --> 00:45:19.480
we're getting into the loop here


00:45:19.480 --> 00:45:21.600
where it's like run the test, evaluate the results,


00:45:21.600 --> 00:45:24.420
refine the test, refine the prompts, and then start over.


00:45:24.420 --> 00:45:27.540
Right, and probably compile the results,


00:45:27.540 --> 00:45:30.160
keep track of the results so that you can compare,


00:45:30.160 --> 00:45:34.140
not just like are you 3% better on your test cases,


00:45:34.140 --> 00:45:38.500
but also did you, which tests that used to fail succeed now,


00:45:38.500 --> 00:45:41.580
which tests that used to fail now succeed, fail now.


00:45:41.580 --> 00:45:44.360
And then once you're happy with the level of success


00:45:44.360 --> 00:45:48.320
you have, you can integrate the prompt into the product


00:45:48.320 --> 00:45:49.820
or maybe upgrade.


00:45:49.820 --> 00:45:50.820
- Ship it.


00:45:50.820 --> 00:45:52.240
- Yeah, ship it.


00:45:52.240 --> 00:45:53.200
- Ship it.


00:45:53.200 --> 00:45:56.660
So I think it's probably a good time to jump over


00:45:56.660 --> 00:46:00.100
to your framework for this because pytest


00:46:00.100 --> 00:46:02.420
and other testing frameworks in Python are great,


00:46:02.420 --> 00:46:05.260
but they're pretty low level compared to these types


00:46:05.260 --> 00:46:06.980
of questions you're trying to answer, right?


00:46:06.980 --> 00:46:10.900
Like how has this improved over time for,


00:46:10.900 --> 00:46:13.100
I was doing 83% right, right?


00:46:13.100 --> 00:46:14.980
pytest asserts a true or a false.


00:46:14.980 --> 00:46:17.820
It doesn't assert that 83% is--


00:46:17.820 --> 00:46:19.500
- Yeah, and it's a part of CI.


00:46:19.500 --> 00:46:23.980
Like, if any of your pytest fail,


00:46:23.980 --> 00:46:26.900
you're probably gonna not CI, not allow CI,


00:46:26.900 --> 00:46:28.700
not even merge the PR, right?


00:46:28.700 --> 00:46:30.820
So one thing that's different between


00:46:30.820 --> 00:46:33.320
test-driven development and unit testing


00:46:33.320 --> 00:46:37.340
and prompt engineering is that the outcome is probabilistic.


00:46:37.340 --> 00:46:38.380
It's not true or false.


00:46:38.380 --> 00:46:40.620
It might just be like zero or one, right?


00:46:40.620 --> 00:46:45.620
where our spectrum fails for a specific test.


00:46:45.620 --> 00:46:48.180
You're like, oh, if it gets this column,


00:46:48.180 --> 00:46:51.480
but not this other column, you succeed at 50%.


00:46:51.480 --> 00:46:53.380
So it's non-binary.


00:46:53.380 --> 00:46:55.800
It's also, you don't need perfection to ship.


00:46:55.800 --> 00:46:58.060
You just need better than the previous version


00:46:58.060 --> 00:46:59.980
or good enough to start with.


00:46:59.980 --> 00:47:03.340
So the mindset is, so there's a bunch of differences.


00:47:03.340 --> 00:47:06.780
And for those interested, we won't get into the blog post.


00:47:06.780 --> 00:47:09.520
I think I list out the things that are different


00:47:09.520 --> 00:47:12.420
between the two, I think it's a little bit above this.


00:47:12.420 --> 00:47:14.440
But, you know, the first thing I want to say


00:47:14.440 --> 00:47:16.680
is like the level of ambition of this project


00:47:16.680 --> 00:47:19.320
versus say an Airflow is super set is like very low, right?


00:47:19.320 --> 00:47:24.080
So it's maybe more similar to a test,


00:47:24.080 --> 00:47:27.680
a unit test library and no discredit to the great,


00:47:27.680 --> 00:47:29.680
awesome like unit test libraries out there,


00:47:29.680 --> 00:47:31.720
but you would think those are fairly simple


00:47:31.720 --> 00:47:34.680
and straightforward, which is the information architecture


00:47:34.680 --> 00:47:37.200
of a pytest is probably simpler


00:47:37.200 --> 00:47:39.240
than the information architecture of a Django


00:47:39.240 --> 00:47:40.240
for instance, right?


00:47:40.240 --> 00:47:41.840
It's just like a different thing.


00:47:41.840 --> 00:47:44.520
And here, the level of ambition is much low,


00:47:44.520 --> 00:47:47.880
and much, you know, for this is fairly simple.


00:47:47.880 --> 00:47:50.680
So Promptimize is something that I created,


00:47:50.680 --> 00:47:55.400
which is a toolkit to help people write,


00:47:55.400 --> 00:48:00.040
to evaluate and score and understand


00:48:00.040 --> 00:48:02.560
while they iterate on their,


00:48:02.560 --> 00:48:04.120
while doing prompt engineering.


00:48:04.120 --> 00:48:04.960
But so in this case,


00:48:04.960 --> 00:48:07.400
I think I talk about the use case I preset,


00:48:07.400 --> 00:48:11.600
which is we have a big corpus that luckily was contributed


00:48:11.600 --> 00:48:14.440
by, I forgot which university, but a bunch of PhD people


00:48:14.440 --> 00:48:17.640
did a text-to-SQL contest.


00:48:17.640 --> 00:48:18.880
- I think it was Yale.


00:48:18.880 --> 00:48:19.720
- Yale, yeah.


00:48:19.720 --> 00:48:21.280
- I think it was Yale, yeah.


00:48:21.280 --> 00:48:22.640
- So great people at Yale were like,


00:48:22.640 --> 00:48:27.560
hey, we're gonna generate 3,000 prompts on 200 databases


00:48:27.560 --> 00:48:31.920
with the SQL that should be the outcome of that.


00:48:31.920 --> 00:48:34.680
It's a big test set so that different researchers


00:48:34.680 --> 00:48:37.800
working on Text to SQL and compare their results.


00:48:37.800 --> 00:48:40.200
So for us, we're able to take that test set


00:48:40.200 --> 00:48:44.040
and some of our own test sets and run it at scale


00:48:44.040 --> 00:48:49.040
against OpenAI or against LLAMA or against different models.


00:48:49.040 --> 00:48:52.240
And by doing that, we're able to evaluate


00:48:52.240 --> 00:48:56.760
this particular combo of this prompt engineering methodology


00:48:56.760 --> 00:49:01.080
with this model generates 73% accuracy.


00:49:01.080 --> 00:49:04.560
And we have these reports we can compare fairly easily


00:49:04.560 --> 00:49:06.520
which prompts that, as I said before,


00:49:06.520 --> 00:49:09.400
were failing before are succeeding now and vice versa.


00:49:09.400 --> 00:49:12.820
So you're like, am I actually making progress here


00:49:12.820 --> 00:49:14.300
or going backwards?


00:49:14.300 --> 00:49:15.620
And if you try to do that on your own,


00:49:15.620 --> 00:49:18.080
like if you're crafting your prompt just anecdotally


00:49:18.080 --> 00:49:19.580
and try it on five or six things,


00:49:19.580 --> 00:49:21.320
like you quickly realize like,


00:49:21.320 --> 00:49:23.580
oh shit, I'm gonna need to really test


00:49:23.580 --> 00:49:25.640
out of a much broader range of tests


00:49:25.640 --> 00:49:28.380
and then some rigor and methodology around that.


00:49:28.380 --> 00:49:29.940
- So right, and try, how do you remember


00:49:29.940 --> 00:49:32.460
and go back and go, this actually made it better, right?


00:49:32.460 --> 00:49:35.060
'cause it's hard to keep all that in your mind, yeah.


00:49:35.060 --> 00:49:38.500
- Yeah, and something interesting that I'm realizing


00:49:38.500 --> 00:49:40.580
to work on this stuff is like,


00:49:40.580 --> 00:49:41.940
everything is changing so fast, right?


00:49:41.940 --> 00:49:43.260
The models are changing fast,


00:49:43.260 --> 00:49:45.260
the prompting windows are changing fast,


00:49:45.260 --> 00:49:47.260
the vector databases, which is a way


00:49:47.260 --> 00:49:50.660
that organize and structure a context for your prompts,


00:49:50.660 --> 00:49:52.300
evolving extremely fast.


00:49:52.300 --> 00:49:55.220
It feels like you're working on unsettled ground


00:49:55.220 --> 00:49:57.900
in a lot of ways, like a lot of stuff you're doing


00:49:57.900 --> 00:49:59.820
might be challenged by, you know,


00:49:59.820 --> 00:50:01.540
the BART API came out last week


00:50:01.540 --> 00:50:03.500
Maybe it's better at SQL generation,


00:50:03.500 --> 00:50:06.980
and then I got to throw everything that I did on OpenAI.


00:50:06.980 --> 00:50:08.700
But here's something you don't throw away,


00:50:08.700 --> 00:50:11.340
your test library and your use cases.


00:50:11.340 --> 00:50:12.060
>> Right.


00:50:12.060 --> 00:50:15.100
>> Maybe is the real asset here.


00:50:15.100 --> 00:50:16.180
The rest of the stuff is like,


00:50:16.180 --> 00:50:20.380
I was moving so fast that all the mechanics of


00:50:20.380 --> 00:50:23.060
the prompt engineering itself and


00:50:23.060 --> 00:50:27.240
the interface with whatever model is the best at the time,


00:50:27.240 --> 00:50:28.500
you're probably going to have to throw


00:50:28.500 --> 00:50:29.880
away as this evolves quickly.


00:50:29.880 --> 00:50:33.320
but your test library is something really, really solid


00:50:33.320 --> 00:50:36.740
that you can perpetuate or keep improving


00:50:36.740 --> 00:50:39.620
and bringing along with you along the way.


00:50:39.620 --> 00:50:41.820
It's an interesting thought around that.


00:50:41.820 --> 00:50:44.540
>> Let's talk through this example you have on


00:50:44.540 --> 00:50:47.980
Promptimize's GitHub read me here.


00:50:47.980 --> 00:50:49.860
To make it a little concrete for people,


00:50:49.860 --> 00:50:51.980
how do you actually write one of these tests?


00:50:51.980 --> 00:50:54.740
>> Yeah. There's different types of prompts.


00:50:54.740 --> 00:50:58.420
But what I wanted to get to was just like,


00:50:58.420 --> 00:51:01.920
what is the prompt and how do you evaluate it, right?


00:51:01.920 --> 00:51:04.660
And then behind the scene, we're going to be, you know,


00:51:04.660 --> 00:51:08.820
discovering all your prompts and running them and compiling results and reports,


00:51:08.820 --> 00:51:11.860
right, and doing analytics and making it easy to do analytics on it.


00:51:11.860 --> 00:51:16.520
The examples that we have here, and I'll try to be conscious of both the people who can read the code


00:51:16.520 --> 00:51:19.720
and people who don't, like the people who are just on audio.


00:51:19.720 --> 00:51:23.960
But here, from Proptimize.prompt, we import a simple prompt,


00:51:23.960 --> 00:51:27.320
And then we bring some evals that are just like utility functions around


00:51:27.320 --> 00:51:32.200
evaluating the output of what comes back from the AI.


00:51:32.200 --> 00:51:34.560
And here the first prompt case in the model,


00:51:34.560 --> 00:51:38.840
here I just create an array or a list of prompt cases.


00:51:38.840 --> 00:51:41.480
And it's a prompt case like a test case.


00:51:41.480 --> 00:51:45.520
And with this prompt case, this very simple one, I say,


00:51:45.520 --> 00:51:47.040
"Hello there!"


00:51:47.040 --> 00:51:49.680
And then I evaluate that as says, you know,


00:51:49.680 --> 00:51:53.280
either "Hi" or "Hello" in the output, right?


00:51:53.280 --> 00:51:57.040
So if any of the words exist and what comes back,


00:51:57.040 --> 00:51:59.080
I give it a one or a zero.


00:51:59.080 --> 00:52:00.520
Framework allows you to, you could say,


00:52:00.520 --> 00:52:02.060
oh, it has to have both these words


00:52:02.060 --> 00:52:04.760
or give the percentage of success


00:52:04.760 --> 00:52:08.080
based on the number of words from this list that it has.


00:52:08.080 --> 00:52:11.120
But that's the first case.


00:52:11.120 --> 00:52:13.060
The second one is a little bit more complicated,


00:52:13.060 --> 00:52:17.600
but name the top 50 guitar players of all time, I guess.


00:52:17.600 --> 00:52:19.600
And I make sure that Frank Zappa is in the list


00:52:19.600 --> 00:52:21.960
'cause I'm a Frank Zappa fan here.


00:52:21.960 --> 00:52:25.000
But you could say,


00:52:25.000 --> 00:52:29.940
I want to make sure that at least three out of five of these are in the list.


00:52:29.940 --> 00:52:32.040
Those are very more like natural language,


00:52:32.040 --> 00:52:35.160
very simple tests too.


00:52:35.160 --> 00:52:37.800
That's the Hello World essentially.


00:52:37.800 --> 00:52:40.960
Then we're showing some examples of what's happening behind the scene.


00:52:40.960 --> 00:52:44.800
Well, it will actually call the underlying API,


00:52:44.800 --> 00:52:48.520
get the results, run your eval function and compile a report.


00:52:48.520 --> 00:52:49.840
What was the prompt?


00:52:49.840 --> 00:52:52.800
What was oh a bird just flew into my room?


00:52:52.800 --> 00:53:01.200
That's gonna make that's gonna make the podcast interesting. Oh my goodness. Okay, that might be that might be a first here


00:53:01.200 --> 00:53:08.960
That is nuts. Oh, well, it's out of my room. Guess what? There's other people in that house. I'm just gonna close the door to my room


00:53:08.960 --> 00:53:13.840
And deal with it later. All right. Well, that's that's a first


00:53:15.360 --> 00:53:17.600
- I've had a bat fly into my house once,


00:53:17.600 --> 00:53:21.000
but never a bird, so both are crazy.


00:53:21.000 --> 00:53:23.420
How interesting, this is the first on the podcast


00:53:23.420 --> 00:53:25.180
out of eight years we've never had a bird,


00:53:25.180 --> 00:53:28.320
wild animal enter the studio of the guests.


00:53:28.320 --> 00:53:31.180
- Yes, well, welcome to my room.


00:53:31.180 --> 00:53:33.460
I live in Tahoe, so I guess that's something,


00:53:33.460 --> 00:53:35.100
it's better than a bear, you know?


00:53:35.100 --> 00:53:36.140
It could have been better.


00:53:36.140 --> 00:53:37.220
- It is better than a bear.


00:53:37.220 --> 00:53:40.500
- All right, but yeah, so just keep enumerating


00:53:40.500 --> 00:53:43.980
kind of what we're seeing visually here.


00:53:43.980 --> 00:53:48.660
We'll keep a YAML file as the report output.


00:53:48.660 --> 00:53:52.880
In Promptimize, you have your test case or your prompt cases,


00:53:52.880 --> 00:53:56.100
like test cases, you have an output report that says,


00:53:56.100 --> 00:53:58.200
for this prompt case,


00:53:58.200 --> 00:54:00.780
here's the key, here's what the prompt that was actually,


00:54:00.780 --> 00:54:02.840
the user input that came in,


00:54:02.840 --> 00:54:05.300
here's what the prompt look like,


00:54:05.300 --> 00:54:07.140
what was the response,


00:54:07.140 --> 00:54:09.260
the raw response from the API,


00:54:09.260 --> 00:54:11.020
what are all the tasks, how long did it run?


00:54:11.020 --> 00:54:14.080
So a bunch of metadata and relevant information


00:54:14.080 --> 00:54:16.840
that we can use later to create these reports.


00:54:16.840 --> 00:54:19.680
So you're like, was the score zero or one?


00:54:19.680 --> 00:54:22.000
So you get the whole output report.


00:54:22.000 --> 00:54:22.840
- Yeah, okay.


00:54:22.840 --> 00:54:26.000
And then you also have a way to get like a report.


00:54:26.000 --> 00:54:27.320
I'm not sure, maybe I scrolled past it.


00:54:27.320 --> 00:54:28.160
- Yeah, I think it's--


00:54:28.160 --> 00:54:30.460
- Where it shows you how it did, right?


00:54:30.460 --> 00:54:31.920
I think that was in your--


00:54:31.920 --> 00:54:32.920
- I think at the blog post,


00:54:32.920 --> 00:54:33.760
do you see a much more--


00:54:33.760 --> 00:54:35.680
- Oh, there it is.


00:54:35.680 --> 00:54:38.480
- So this one, we're running the spider dataset


00:54:38.480 --> 00:54:40.080
that I just, that I talked about.


00:54:40.080 --> 00:54:44.680
Remember, it's like the Yale generated text to SQL competition corpus.


00:54:44.680 --> 00:54:49.720
So here we looked at my percentage of success is 70%.


00:54:49.720 --> 00:54:52.160
So here you say weight and score.


00:54:52.160 --> 00:54:57.400
So there's a way to say, "Oh, this particular prompt case is 10 times more important than another one."


00:54:57.400 --> 00:55:02.520
Right? So you can do a relative importance of weight of your different text cases.


00:55:02.520 --> 00:55:07.680
Now, one thing we didn't mention too is like all these tests are generated programmatically too.


00:55:07.920 --> 00:55:10.280
So that it's the same philosophy behind, you know,


00:55:10.280 --> 00:55:11.680
Airflow of like, you know,


00:55:11.680 --> 00:55:15.120
it's almost like a little DSL to write your test case.


00:55:15.120 --> 00:55:16.080
So you could, you know,


00:55:16.080 --> 00:55:18.400
it could read from a YAML file for instance,


00:55:18.400 --> 00:55:20.760
in the case of what we do with spider SQL,


00:55:20.760 --> 00:55:22.440
there's a big Jason file of all the prompts


00:55:22.440 --> 00:55:23.880
and all the databases.


00:55:23.880 --> 00:55:25.640
And then we dynamically generate, you know,


00:55:25.640 --> 00:55:28.520
a thousand tests based on that.


00:55:28.520 --> 00:55:31.520
So you can do programmatic test definition,


00:55:31.520 --> 00:55:33.800
so more and more dynamic if you want it to be,


00:55:33.800 --> 00:55:35.560
or you could do more static if you prefer that.


00:55:35.560 --> 00:55:37.620
So in this case, we're doing,


00:55:37.620 --> 00:55:39.740
We introduced this idea of a category too.


00:55:39.740 --> 00:55:41.500
So I mentioned like there's some features


00:55:41.500 --> 00:55:44.800
in Promptomize like categorizing your tests


00:55:44.800 --> 00:55:47.840
or weights, you know, and things like that.


00:55:47.840 --> 00:55:51.460
So here we'll do some reporting on per category.


00:55:51.460 --> 00:55:53.260
What is the score per category?


00:55:53.260 --> 00:55:56.380
You can see which database is performing well


00:55:56.380 --> 00:55:57.500
or poorly again.


00:55:57.500 --> 00:56:00.340
So I could have another category that is large database,


00:56:00.340 --> 00:56:02.540
small databases and see how that,


00:56:02.540 --> 00:56:05.740
what the score is and compare reports.


00:56:05.740 --> 00:56:09.180
It's pretty cool that it saves the test run to a file


00:56:09.180 --> 00:56:11.140
that then you can ask questions about


00:56:11.140 --> 00:56:13.100
and write and generate this report on


00:56:13.100 --> 00:56:14.940
and rather than just running it


00:56:14.940 --> 00:56:16.420
and passing or failing, right?


00:56:16.420 --> 00:56:17.840
- Yeah, or like giving the output


00:56:17.840 --> 00:56:19.400
and then having to run it again.


00:56:19.400 --> 00:56:22.460
Yeah, there's some other features around if,


00:56:22.460 --> 00:56:24.500
so you can memoize the test.


00:56:24.500 --> 00:56:25.780
So because it has a reports,


00:56:25.780 --> 00:56:29.780
if you like, you know, exit off of it or restart it later,


00:56:29.780 --> 00:56:35.340
it won't rerun the same tests if it's the same hash input,


00:56:35.340 --> 00:56:37.340
Even though with AI, you might get a different answer


00:56:37.340 --> 00:56:38.460
with the same input.


00:56:38.460 --> 00:56:40.140
But at least in this case, it will say like,


00:56:40.140 --> 00:56:43.940
hey, I'm rerunning the same prompt


00:56:43.940 --> 00:56:46.180
instead of like waiting five seconds for open AI


00:56:46.180 --> 00:56:48.540
and then paying the tokens and paying the piper.


00:56:48.540 --> 00:56:51.160
You know, I'm just gonna skip that.


00:56:51.160 --> 00:56:52.820
So there's some logic around


00:56:52.820 --> 00:56:54.500
skipping what's been done already.


00:56:54.500 --> 00:56:56.700
- It's not just a couple of milliseconds to run it.


00:56:56.700 --> 00:56:58.740
It could be a while to get the answers.


00:56:58.740 --> 00:57:00.700
- Yeah, also like early libraries,


00:57:00.700 --> 00:57:03.620
I haven't written the sub, the threading for it


00:57:03.620 --> 00:57:06.780
where you can say like, oh, run it on eight threads.


00:57:06.780 --> 00:57:11.180
So with Promptimize, I think, and the blog post


00:57:11.180 --> 00:57:15.460
is probably more impactful than the Python project itself.


00:57:15.460 --> 00:57:16.880
If the Python project takes off


00:57:16.880 --> 00:57:18.860
and a bunch of people are using it to test prompts


00:57:18.860 --> 00:57:21.500
and contribute to it, it's great.


00:57:21.500 --> 00:57:22.780
But I think it's more like, okay,


00:57:22.780 --> 00:57:25.040
this is uncharted territory,


00:57:25.040 --> 00:57:29.140
working with an AI type interface.


00:57:29.140 --> 00:57:33.420
And then it's more like, how do we best do that


00:57:33.420 --> 00:57:36.500
as practitioners or as people building products.


00:57:36.500 --> 00:57:39.080
I think that's the big idea there.


00:57:39.080 --> 00:57:41.660
Then the test library, you could probably write your own.


00:57:41.660 --> 00:57:44.540
I think for me that was a one or two week project.


00:57:44.540 --> 00:57:47.460
The one I would like to say is normally,


00:57:47.460 --> 00:57:51.780
if it wasn't for getting all the help from ChatGPT on,


00:57:51.780 --> 00:57:53.260
it's like, "I'm creating a project.


00:57:53.260 --> 00:57:55.660
"I'm setting up my setup.py."


00:57:55.660 --> 00:57:59.040
Setup tools is always a little bit of pain in the ass.


00:57:59.040 --> 00:58:02.500
Then I'm like, "Can you help me create my setup.py?"


00:58:02.500 --> 00:58:04.340
and then generate some code.


00:58:04.340 --> 00:58:07.660
And I'm like, "Oh, I wanna make sure that PyPI


00:58:07.660 --> 00:58:09.980
is gonna get my readme from GitHub.


00:58:09.980 --> 00:58:13.540
I forgot how to read the markdown and pass the stuff.


00:58:13.540 --> 00:58:14.500
Can you do that for me?"


00:58:14.500 --> 00:58:17.580
And then Chedjabint generates this stuff very nicely.


00:58:17.580 --> 00:58:20.340
Or, "I wanna make sure I use my request


00:58:20.340 --> 00:58:24.540
that requirements of TXT inside my dynamically building


00:58:24.540 --> 00:58:26.180
my setup tools integration.


00:58:26.180 --> 00:58:27.100
Can you do that for me?"


00:58:27.100 --> 00:58:28.900
And it's just like, bam, bam, bam.


00:58:28.900 --> 00:58:30.740
Like all the repetitive stuff.


00:58:30.740 --> 00:58:31.580
I need a function.


00:58:31.580 --> 00:58:32.980
- This is incredible, right?


00:58:32.980 --> 00:58:33.820
- Go ahead.


00:58:33.820 --> 00:58:35.380
- Yeah, I kind of want to close out


00:58:35.380 --> 00:58:36.220
the conversation with that.


00:58:36.220 --> 00:58:39.100
I do agree that the blog post is super powerful


00:58:39.100 --> 00:58:41.780
in how it kind of teaches you to think


00:58:41.780 --> 00:58:44.100
about how might you go about testing,


00:58:44.100 --> 00:58:46.740
integrating with an AI and these types of products, right?


00:58:46.740 --> 00:58:50.260
Much like TDD brought a way to think about


00:58:50.260 --> 00:58:52.220
how do we actually apply the concept of just,


00:58:52.220 --> 00:58:53.460
well, I have things and I can test them


00:58:53.460 --> 00:58:55.140
with this assert thing.


00:58:55.140 --> 00:58:56.760
How should I actually go about building software, right?


00:58:56.760 --> 00:59:00.020
So this is kind of that for AI integrated software.


00:59:00.020 --> 00:59:02.220
So it's certainly worth people watching.


00:59:02.220 --> 00:59:03.540
Let's just close it out with,


00:59:03.540 --> 00:59:05.380
you kind of touched on some of those things there.


00:59:05.380 --> 00:59:09.660
Like, how do you recommend that people leverage things


00:59:09.660 --> 00:59:13.700
like ChatGPT to help them build their apps


00:59:13.700 --> 00:59:16.620
or how to use AI, this kind of


00:59:16.620 --> 00:59:17.460
- Oh my God, yeah.


00:59:17.460 --> 00:59:20.380
- To like amp up your software development?


00:59:20.380 --> 00:59:21.200
- 100%.


00:59:21.200 --> 00:59:25.420
I mean, it's been a lot of people report on Twitter,


00:59:25.420 --> 00:59:27.880
people used to like Google,


00:59:28.820 --> 00:59:32.900
all the problems that they had while writing code


00:59:32.900 --> 00:59:34.920
and using a lot of Stack Overflow.


00:59:34.920 --> 00:59:37.480
I don't know what the stats on Stack Overflow traffic,


00:59:37.480 --> 00:59:41.740
but once you try working with Chat GPT to do coding,


00:59:41.740 --> 00:59:45.580
you probably don't go back to those other flows of,


00:59:45.580 --> 00:59:47.820
I don't know, it's like putting your error message


00:59:47.820 --> 00:59:49.420
or stack trace into Google


00:59:49.420 --> 00:59:51.700
and then going to a bunch of Stack Overflow link


00:59:51.700 --> 00:59:54.720
and try to make sense of what comes out.


00:59:54.720 --> 00:59:57.260
To me, it's been so much better


00:59:57.260 --> 01:00:00.660
to go just with ChatGPT and there's a conversation there too.


01:00:00.660 --> 01:00:03.580
So say for instance, if I'm in pro optimize, I needed a function to say,


01:00:03.580 --> 01:00:06.700
can you write, I wrote that function before, you know,


01:00:06.700 --> 01:00:07.140
but it's a,


01:00:07.140 --> 01:00:12.140
can you crawl a certain given folder and look for modules that contain objects


01:00:12.140 --> 01:00:16.820
of a certain class and then bring that back and you know,


01:00:16.820 --> 01:00:20.300
you have to use the import lib and she's a little bit of pain in the ass to


01:00:20.300 --> 01:00:23.740
write this. So it writes, you know, a function that works pretty well.


01:00:23.740 --> 01:00:25.400
and I'm like, "Oh, I forgot to ask you


01:00:25.400 --> 01:00:27.320
"to look into lists and dictionaries.


01:00:27.320 --> 01:00:28.720
"Can you do that too?"


01:00:28.720 --> 01:00:30.200
Then it does that in a second.


01:00:30.200 --> 01:00:32.380
It's like, "Oh, you didn't add type hints


01:00:32.380 --> 01:00:34.400
"and duck string and duck test.


01:00:34.400 --> 01:00:36.880
"Can you write that too?"


01:00:36.880 --> 01:00:38.240
And it's a bang, bang, bang,


01:00:38.240 --> 01:00:40.520
and just like copy paste in your utils file


01:00:40.520 --> 01:00:43.240
and it works and you save like two hours.


01:00:43.240 --> 01:00:46.120
- I think it would be really good at those things


01:00:46.120 --> 01:00:48.040
that are kind of algorithmic.


01:00:48.040 --> 01:00:50.200
Now, it might be the kind of thing


01:00:50.200 --> 01:00:53.680
that you would do on a whiteboard job interview test, right?


01:00:53.680 --> 01:00:56.700
it's just gonna know that really, really solid.


01:00:56.700 --> 01:00:59.420
It actually, but it knows quite a bit


01:00:59.420 --> 01:01:01.860
about the other libraries and stuff that are out there.


01:01:01.860 --> 01:01:02.700
- It's insane, yeah.


01:01:02.700 --> 01:01:05.020
So one thing that I came across is,


01:01:05.020 --> 01:01:06.980
I leverage something called LinkChain,


01:01:06.980 --> 01:01:09.600
which pointed to people getting interested


01:01:09.600 --> 01:01:11.020
in prompt engineering.


01:01:11.020 --> 01:01:12.860
There's a really good,


01:01:12.860 --> 01:01:15.380
well, the library LinkChain is really interesting.


01:01:15.380 --> 01:01:17.440
That's not perfect, it's new, it's moving fast,


01:01:17.440 --> 01:01:20.020
but push people to check it out.


01:01:20.020 --> 01:01:22.140
Also like 41,000 stars, so very--


01:01:22.140 --> 01:01:23.940
I know that's nice, right?


01:01:23.940 --> 01:01:25.180
It's written in Python.


01:01:25.180 --> 01:01:27.420
- Yes, you can do like, yeah, it's in Python too.


01:01:27.420 --> 01:01:32.420
You should talk to whoever is writing this or started this.


01:01:32.420 --> 01:01:35.180
But yeah, you can change some prompt to say like,


01:01:35.180 --> 01:01:37.320
the output of a prompt will generate the next one.


01:01:37.320 --> 01:01:39.460
There's this idea of agents.


01:01:39.460 --> 01:01:41.760
There's this idea of estimating tokens


01:01:41.760 --> 01:01:44.380
before doing the request.


01:01:44.380 --> 01:01:47.980
There's a bunch of really cool things that it does.


01:01:47.980 --> 01:01:49.660
To me, the docs are not that comprehensive.


01:01:49.660 --> 01:01:51.500
There's someone else that created,


01:01:51.500 --> 01:01:56.500
if you Google "Lang chain cookbook,"


01:01:56.500 --> 01:01:58.540
you'll find someone else that wrote


01:01:58.540 --> 01:02:03.460
what I thought was more comprehensive way to start.


01:02:03.460 --> 01:02:06.640
This one has a YouTube video and an IP1B file


01:02:06.640 --> 01:02:09.680
and introduces you to the concept in an interactive way.


01:02:09.680 --> 01:02:12.620
I thought that was really good.


01:02:12.620 --> 01:02:14.740
But yeah, so we were trying to, I was trying to use this,


01:02:14.740 --> 01:02:16.380
and I was like, "Oh, Chad, can you generate


01:02:16.380 --> 01:02:17.820
"a bunch of like, Lang chain related stuff?"


01:02:17.820 --> 01:02:20.140
I was like, I don't know of a project called LinkChain.


01:02:20.140 --> 01:02:22.780
It was created after 2021.


01:02:22.780 --> 01:02:24.580
So I was like, I wish I could just say,


01:02:24.580 --> 01:02:27.940
just go read the GitHub, just read it all, read the docs,


01:02:27.940 --> 01:02:30.300
and then I'll ask you questions.


01:02:30.300 --> 01:02:34.320
And then Chajupiti's not that great at that currently,


01:02:34.320 --> 01:02:36.400
at learning things it doesn't know,


01:02:36.400 --> 01:02:38.340
for reasons we talked about.


01:02:38.340 --> 01:02:40.460
BART is much more up to date,


01:02:40.460 --> 01:02:43.300
so you can always, for those projects,


01:02:43.300 --> 01:02:45.140
Chajupiti might be better at Django,


01:02:45.140 --> 01:02:47.300
'cause it's old and settled,


01:02:47.300 --> 01:02:48.980
and it's better at writing code overall,


01:02:48.980 --> 01:02:52.180
but Bard might be decent and pretty good for--


01:02:52.180 --> 01:02:54.780
- Right, if you ask advice on how to do promptimize stuff,


01:02:54.780 --> 01:02:55.980
it's like, I don't know what that is.


01:02:55.980 --> 01:02:57.220
- Yeah, it's like, I've never heard of,


01:02:57.220 --> 01:02:58.380
it might elucidate too,


01:02:58.380 --> 01:03:00.060
I think if you go and make shit up,


01:03:00.060 --> 01:03:01.420
like I've seen it,


01:03:01.420 --> 01:03:03.480
like promptimize sounds like it would be this


01:03:03.480 --> 01:03:06.340
and it just makes up stuff, so,


01:03:06.340 --> 01:03:07.660
so not that great.


01:03:07.660 --> 01:03:10.780
But yeah, absolutely, I encourage people to try,


01:03:10.780 --> 01:03:13.500
you know, for any subtasks that you're trying to do


01:03:13.500 --> 01:03:15.580
to see if it can help you at it


01:03:15.580 --> 01:03:18.260
and maybe try a variation on the prompt.


01:03:18.260 --> 01:03:20.300
And then, you know, if it's not good at it,


01:03:20.300 --> 01:03:21.740
do it the old way.


01:03:21.740 --> 01:03:23.820
But yeah, it might be better too


01:03:23.820 --> 01:03:26.460
for those familiar with the idea of functional programming,


01:03:26.460 --> 01:03:28.580
where each function is more deterministic


01:03:28.580 --> 01:03:32.700
and can be reasoned about and unit test in an isolation.


01:03:32.700 --> 01:03:34.660
Chai GPT is gonna be better at that


01:03:34.660 --> 01:03:36.860
'cause it doesn't know about all your other packages


01:03:36.860 --> 01:03:37.700
and modules.


01:03:37.700 --> 01:03:39.920
So really great for the utils functions


01:03:39.920 --> 01:03:44.260
are very deterministic, functional, super great at that.


01:03:44.260 --> 01:03:48.220
Another thing is, and you tell me when we run out of time,


01:03:48.220 --> 01:03:51.300
but another thing that was really interesting too,


01:03:51.300 --> 01:03:54.180
of bringing some of the concept and promptimize


01:03:54.180 --> 01:03:56.140
and writing the blog post itself.


01:03:56.140 --> 01:03:56.980
- Right.


01:03:56.980 --> 01:03:58.860
- And things like, hey, I'm thinking about the difference


01:03:58.860 --> 01:04:01.520
of the properties of test-driven development


01:04:01.520 --> 01:04:04.320
as it applies for prompt engineering.


01:04:04.320 --> 01:04:08.180
Here's my blog post, but can you think of other differences


01:04:08.180 --> 01:04:10.500
between the two that are very core?


01:04:10.500 --> 01:04:14.180
And can you talk about the similarities and the differences


01:04:14.180 --> 01:04:18.260
and I would come up with just really, really great ideas,


01:04:18.260 --> 01:04:22.580
brainstorming and just very smart at mixing concepts.


01:04:22.580 --> 01:04:24.780
- I do think one thing that's not a great idea


01:04:24.780 --> 01:04:26.220
is just say, "Write this for me."


01:04:26.220 --> 01:04:28.220
But if you've got something in mind


01:04:28.220 --> 01:04:30.100
and you're gonna say, "Give me some ideas,"


01:04:30.100 --> 01:04:32.540
or, "Where should I go deeper into this?"


01:04:32.540 --> 01:04:36.020
And then you use your own creativity to create that,


01:04:36.020 --> 01:04:37.340
that's a totally valid use.


01:04:37.340 --> 01:04:40.540
I wouldn't feel like, "Oh, I'm reading this AI crap."


01:04:40.540 --> 01:04:41.780
It brought out some insights


01:04:41.780 --> 01:04:43.140
that you had forgot to think about


01:04:43.140 --> 01:04:45.140
and now you are, right?


01:04:45.140 --> 01:04:46.340
- Or when it fails, instead of saying,


01:04:46.340 --> 01:04:49.220
like, I got it to fail, AI is wrong, I'm smarter than it,


01:04:49.220 --> 01:04:52.140
you're like, wait, is there something, can I try to,


01:04:52.140 --> 01:04:54.540
you know, here's what it didn't get right,


01:04:54.540 --> 01:04:56.860
and why, like, what did I need to tell it?


01:04:56.860 --> 01:04:59.860
So you can go and edit your prompt, or ask a follow-up,


01:04:59.860 --> 01:05:03.060
and generally it will do better and well.


01:05:03.060 --> 01:05:04.980
- Yeah, I think also you can ask it to find bugs,


01:05:04.980 --> 01:05:06.540
or security vulnerabilities.


01:05:06.540 --> 01:05:07.380
- Yeah.


01:05:07.380 --> 01:05:10.180
- Right, you're like, here's my 30-line function,


01:05:10.180 --> 01:05:11.140
do you see any bugs?


01:05:11.140 --> 01:05:12.420
Do you-- - Yeah.


01:05:12.420 --> 01:05:14.620
- Do you see any security vulnerabilities?


01:05:14.620 --> 01:05:18.340
Like yeah, you're passing this straight to,


01:05:18.340 --> 01:05:20.180
you're concatenating the string in the SQL


01:05:20.180 --> 01:05:21.020
or something like that.


01:05:21.020 --> 01:05:24.220
- Yeah, the rigor stuff too,


01:05:24.220 --> 01:05:27.360
or like, you know, I would say writing a good doc string,


01:05:27.360 --> 01:05:30.740
writing doc tests, writing unit tests,


01:05:30.740 --> 01:05:33.060
reviewing the logic, that kind of stuff.


01:05:33.060 --> 01:05:34.980
It does, type hints, right?


01:05:34.980 --> 01:05:37.300
If you're like me, like I don't really like


01:05:37.300 --> 01:05:39.260
to write type hints up front,


01:05:40.220 --> 01:05:42.260
but I'm like, can you just like sprinkle some type


01:05:42.260 --> 01:05:43.100
in on top of that?


01:05:43.100 --> 01:05:44.860
- Retrofit this thing for me.


01:05:44.860 --> 01:05:45.700
- Yeah, that's it.


01:05:45.700 --> 01:05:46.980
Just make it that production grade.


01:05:46.980 --> 01:05:48.260
You know, one thing that's interesting too,


01:05:48.260 --> 01:05:50.780
of like, you know, you would think I'm a big TDD guy.


01:05:50.780 --> 01:05:52.660
Like I don't do tests.


01:05:52.660 --> 01:05:54.220
(laughs)


01:05:54.220 --> 01:05:55.060
It's just not my thing.


01:05:55.060 --> 01:05:56.540
I like to write code.


01:05:56.540 --> 01:05:59.620
I don't think of like what I'm gonna use the function for


01:05:59.620 --> 01:06:03.200
and before I write it, but like generating,


01:06:03.200 --> 01:06:07.220
it's good at generating unit tests for a function too.


01:06:07.220 --> 01:06:09.260
And then I think what's interesting


01:06:09.260 --> 01:06:11.140
with Promptimize too is you might,


01:06:11.140 --> 01:06:13.900
you want deterministic, what I call prompt cases


01:06:13.900 --> 01:06:16.180
or test cases, but you can say,


01:06:16.180 --> 01:06:19.940
I've written five or six of these,


01:06:19.940 --> 01:06:22.620
can you write variations on that theme too?


01:06:22.620 --> 01:06:26.460
So you can use it to generate test cases


01:06:26.460 --> 01:06:29.340
in the case of like TDD, but also the opposite,


01:06:29.340 --> 01:06:31.000
like for Promptimize, you can get it


01:06:31.000 --> 01:06:32.860
to generate stuff dynamically too.


01:06:32.860 --> 01:06:33.940
- Yeah. - By itself.


01:06:33.940 --> 01:06:35.620
- Yeah, it's pretty amazing.


01:06:35.620 --> 01:06:36.780
It is pretty neat.


01:06:36.780 --> 01:06:37.940
Let's maybe close this out,


01:06:37.940 --> 01:06:39.660
Well, I'll ask you one more question.


01:06:39.660 --> 01:06:40.900
- Okay, can I do one more?


01:06:40.900 --> 01:06:41.740
Can I show one more thing?


01:06:41.740 --> 01:06:44.100
Since it's a Python podcast,


01:06:44.100 --> 01:06:48.140
if you go on that repo for Promptomize under examples,


01:06:48.140 --> 01:06:50.300
there's one called Python example.


01:06:50.300 --> 01:06:51.540
- Here we go, something like this.


01:06:51.540 --> 01:06:52.360
- Yeah, something like this.


01:06:52.360 --> 01:06:53.980
So this stuff right here.


01:06:53.980 --> 01:06:55.700
So say here it says,


01:06:55.700 --> 01:06:59.460
so here I wrote a prompt that asks the bot


01:06:59.460 --> 01:07:01.700
to generate Python function.


01:07:01.700 --> 01:07:04.660
Then I sandbox it and bring the function I wrote


01:07:04.660 --> 01:07:06.740
into the interpreter, and then I test it.


01:07:06.740 --> 01:07:09.340
So I say, write a function that tests if a number


01:07:09.340 --> 01:07:12.540
is a prime number and returns a Boolean.


01:07:12.540 --> 01:07:16.660
And then I test, I have six state test cases for it.


01:07:16.660 --> 01:07:19.300
So write a function that finds the greatest common denominator


01:07:19.300 --> 01:07:21.420
of two numbers, right?


01:07:21.420 --> 01:07:24.700
Then behind the scene, we won't get into the class above.


01:07:24.700 --> 01:07:27.540
The class above basically interacts with it,


01:07:27.540 --> 01:07:29.940
gets the input, then runs the test,


01:07:29.940 --> 01:07:31.380
then compiles the results, right?


01:07:31.380 --> 01:07:36.060
So we could test how well 3.5 compares to four.


01:07:36.060 --> 01:07:39.140
but I thought it was relevant for the Python folks


01:07:39.140 --> 01:07:40.140
on the line.


01:07:40.140 --> 01:07:43.220
So we're testing out what it is that writing Python function.


01:07:43.220 --> 01:07:45.920
- Write a function that generates the Fibonacci sequence.


01:07:45.920 --> 01:07:46.760
Yeah.


01:07:46.760 --> 01:07:48.340
- Up to a certain number of terms, right?


01:07:48.340 --> 01:07:50.100
So it's easy to test.


01:07:50.100 --> 01:07:51.180
So it's cool stuff.


01:07:51.180 --> 01:07:53.100
What was your last question?


01:07:53.100 --> 01:07:55.380
- Oh, I was gonna say something like,


01:07:55.380 --> 01:07:57.280
see how far we can push it.


01:07:57.280 --> 01:08:01.020
Write a Python function to use requests


01:08:01.020 --> 01:08:06.020
and beautiful soup to scrape the titles of the request.


01:08:06.020 --> 01:08:09.860
of episodes of Talk Python to me.


01:08:09.860 --> 01:08:12.320
- Oh yeah, and then, yeah, it is.


01:08:12.320 --> 01:08:13.620
And if you don't have a, you know,


01:08:13.620 --> 01:08:15.920
one thing that's a pain in the butt for podcast people


01:08:15.920 --> 01:08:18.480
is to write the, like, what all we talk about.


01:08:18.480 --> 01:08:21.500
So you use another AI to get the transcripts.


01:08:21.500 --> 01:08:22.700
It's like, can you write something


01:08:22.700 --> 01:08:24.300
that's gonna leverage this library


01:08:24.300 --> 01:08:25.500
to transcript the library,


01:08:25.500 --> 01:08:28.660
summarize it and publish it back on the,


01:08:28.660 --> 01:08:32.380
with SEO in mind.


01:08:32.380 --> 01:08:34.060
- Yeah, it's really quite amazing.


01:08:34.060 --> 01:08:35.540
It went through and said, okay, here's a function,


01:08:35.540 --> 01:08:39.260
and it knows talkbython.evm/episodes/all.


01:08:39.260 --> 01:08:43.020
Use h, get the title, and let's just finish this out, Max.


01:08:43.020 --> 01:08:44.460
I'll throw this into--


01:08:44.460 --> 01:08:45.780
- An interpreter, see if it runs.


01:08:45.780 --> 01:08:48.500
- Interpreter, and I'll see if I can get it to run.


01:08:48.500 --> 01:08:50.300
- Hey, you know what's really interesting too,


01:08:50.300 --> 01:08:52.220
is you can give it a random function,


01:08:52.220 --> 01:08:53.660
like you can write a function,


01:08:53.660 --> 01:08:58.440
and say, write a certain function that does certain things,


01:08:58.440 --> 01:09:01.860
and you say, if I give this input to this function,


01:09:01.860 --> 01:09:03.580
what is it gonna come out of?


01:09:03.580 --> 01:09:05.420
And it doesn't have an interpreter,


01:09:05.420 --> 01:09:08.980
but it can interpret code like you and I do, right?


01:09:08.980 --> 01:09:11.260
Like an interview question of like,


01:09:11.260 --> 01:09:13.880
hey, here's a function if I input a three as a value,


01:09:13.880 --> 01:09:15.500
what's gonna come, what's gonna return?


01:09:15.500 --> 01:09:17.780
So it's able to do the follow the loops,


01:09:17.780 --> 01:09:19.320
you know, follow the if statements


01:09:19.320 --> 01:09:21.780
and basically just do a logical.


01:09:21.780 --> 01:09:23.980
- Yeah, another thing I think would be really good


01:09:23.980 --> 01:09:27.180
is to say, here's a function, explain to me what it does.


01:09:27.180 --> 01:09:28.340
- Oh yeah, it's super great at that.


01:09:28.340 --> 01:09:29.820
It's great at that for SQL too.


01:09:29.820 --> 01:09:33.580
Here's a stupid long SQL query, can you explain to me?


01:09:33.580 --> 01:09:35.780
No, it's like, the explanation is on,


01:09:35.780 --> 01:09:38.380
can you just summarize that in a hundred words?


01:09:38.380 --> 01:09:39.820
- Yeah, let's go step by step.


01:09:39.820 --> 01:09:41.260
Let's go step by step.


01:09:41.260 --> 01:09:42.660
What's this do?


01:09:42.660 --> 01:09:44.740
- Well, yeah, I mean, maybe a closing statement


01:09:44.740 --> 01:09:47.260
is like this stuff is changing our world.


01:09:47.260 --> 01:09:49.260
Like for me, I'm interested in how it's changing,


01:09:49.260 --> 01:09:51.220
how we're building products, you know?


01:09:51.220 --> 01:09:55.500
But the core things as a data practitioner,


01:09:55.500 --> 01:09:57.980
as a Python expert, as a programmer,


01:09:57.980 --> 01:10:01.700
it's really changing the way people work day after day


01:10:01.700 --> 01:10:04.940
faster than we all think.


01:10:04.940 --> 01:10:07.740
And across a broad, you might understand pretty well,


01:10:07.740 --> 01:10:11.700
it's changing your daily workflow as a software engineer,


01:10:11.700 --> 01:10:15.100
but it's changing people's workflow to chemistry


01:10:15.100 --> 01:10:20.100
or like in every field, there's a lot we can leverage here


01:10:20.100 --> 01:10:21.580
if you use it well.


01:10:21.580 --> 01:10:23.900
- Right, take this idea and apply it to


01:10:23.900 --> 01:10:27.100
whatever vertical you wanna think of,


01:10:27.100 --> 01:10:28.980
it's doing the same thing there, right?


01:10:28.980 --> 01:10:30.140
- 100%. - Medicine, all over.


01:10:30.140 --> 01:10:32.180
- Yeah, 100%, 100%.


01:10:32.180 --> 01:10:35.660
All right, well, let's call it a, call it a wrap.


01:10:35.660 --> 01:10:38.020
I think we're out of time here.


01:10:38.020 --> 01:10:41.800
So really quick before we quit,


01:10:41.800 --> 01:10:43.180
PyPI package to recommend,


01:10:43.180 --> 01:10:45.580
maybe something AI related that you found recently,


01:10:45.580 --> 01:10:47.660
like all these things cool, people should check it out.


01:10:47.660 --> 01:10:50.500
- Promptimize, I think it would be something to check out.


01:10:50.500 --> 01:10:52.140
I think there's something called Future Tools


01:10:52.140 --> 01:10:53.740
that you could try to navigate there,


01:10:53.740 --> 01:10:57.540
but it shows all of the AI-powered tools


01:10:57.540 --> 01:10:58.900
that are coming out,


01:10:58.900 --> 01:11:00.300
and it's hard to keep up.


01:11:00.300 --> 01:11:02.100
- Yeah, I think I have seen that, yeah.


01:11:02.100 --> 01:11:05.220
- And then if you wanna keep up on a daily,


01:11:05.220 --> 01:11:09.060
what's happening in AI, there's TLDR AI,


01:11:09.060 --> 01:11:12.940
they have like a DL with their relevant list for the day.


01:11:12.940 --> 01:11:14.180
I think that's a...


01:11:14.180 --> 01:11:18.740
- It's hard to stay on, I prefer like their weekly digest


01:11:18.740 --> 01:11:20.060
of what's going on in AI.


01:11:20.060 --> 01:11:24.260
- It's more of a, just a stream of information.


01:11:24.260 --> 01:11:25.860
- Yeah, it's just kind of dizzying,


01:11:25.860 --> 01:11:27.940
and it's like, oh, this new model does this,


01:11:27.940 --> 01:11:29.300
I gotta change everything to that.


01:11:29.300 --> 01:11:33.900
And then something else, if you update the correct course


01:11:33.900 --> 01:11:36.580
too often, it's just like, you know, do nothing.


01:11:36.580 --> 01:11:40.700
'Cause you're like, the foundation's shifting too fast


01:11:40.700 --> 01:11:41.860
under you, so.


01:11:41.860 --> 01:11:42.940
- Yeah, absolutely.


01:11:42.940 --> 01:11:43.780
Well, very cool.


01:11:43.780 --> 01:11:45.940
All right, and then final question.


01:11:45.940 --> 01:11:47.080
You're gonna write some Python code.


01:11:47.080 --> 01:11:48.980
What editor are you using these days?


01:11:48.980 --> 01:11:50.620
- I'm a Vim user, yeah.


01:11:50.620 --> 01:11:52.460
I know it's not the best.


01:11:52.460 --> 01:11:55.380
I know all the limitation, but it's like muscle memory.


01:11:55.380 --> 01:11:59.380
And I'm a UX guy now working on supersets.


01:11:59.380 --> 01:12:03.740
I do appreciate the development of all the new IDEs


01:12:03.740 --> 01:12:05.620
and the functionality that they have.


01:12:05.620 --> 01:12:07.380
I think it's amazing.


01:12:07.380 --> 01:12:09.580
It's just like, for me, it's all,


01:12:09.580 --> 01:12:12.220
like I know all my bash commands and big commands.


01:12:12.220 --> 01:12:13.060
- Absolutely.


01:12:13.060 --> 01:12:15.300
All right, well, Max, thanks for coming on the show,


01:12:15.300 --> 01:12:18.540
helping everyone explore this wild new frontier


01:12:18.540 --> 01:12:19.980
of AI and large language models.


01:12:19.980 --> 01:12:20.900
And for James,


01:12:20.900 --> 01:12:21.740
- Yeah, well, you know,


01:12:21.740 --> 01:12:23.780
exploring it while we're still relevant,


01:12:23.780 --> 01:12:27.220
because I don't know how long we're gonna be relevant for.


01:12:27.220 --> 01:12:28.060
So yeah.


01:12:28.060 --> 01:12:30.060
- Yeah, enjoy it while we can, right?


01:12:30.060 --> 01:12:30.900
Get out there.


01:12:30.900 --> 01:12:35.020
Either control the robots or be controlled by them.


01:12:35.020 --> 01:12:36.540
So get on the right side of that.


01:12:36.540 --> 01:12:38.220
All right, thanks again.


01:12:38.220 --> 01:12:39.060
- Thank you.


01:12:39.060 --> 01:12:43.460
- This has been another episode of Talk Python to Me.


01:12:43.460 --> 01:12:44.900
Thank you to our sponsors.


01:12:44.900 --> 01:12:46.260
Be sure to check out what they're offering.


01:12:46.260 --> 01:12:48.300
It really helps support the show.


01:12:48.300 --> 01:12:50.100
The folks over at JetBrains encourage you


01:12:50.100 --> 01:12:52.860
to get work done with PyCharm.


01:12:52.860 --> 01:12:58.580
PyCharm Professional understands complex projects across multiple languages and technologies,


01:12:58.580 --> 01:13:04.340
so you can stay productive while you're writing Python code and other code like HTML or SQL.


01:13:04.340 --> 01:13:09.980
Download your free trial at talkpython.fm/donewithpycharm.


01:13:09.980 --> 01:13:14.220
Listen to an episode of Compiler, an original podcast from Red Hat.


01:13:14.220 --> 01:13:18.100
Compiler unravels industry topics, trends, and things you've always wanted to know about


01:13:18.100 --> 01:13:21.380
tech through interviews with the people who know it best.


01:13:21.380 --> 01:13:25.820
Subscribe today by following talkpython.fm/compiler.


01:13:25.820 --> 01:13:27.140
Want to level up your Python?


01:13:27.140 --> 01:13:28.920
We have one of the largest catalogs


01:13:28.920 --> 01:13:31.280
of Python video courses over at Talk Python.


01:13:31.280 --> 01:13:33.340
Our content ranges from true beginners


01:13:33.340 --> 01:13:36.320
to deeply advanced topics like memory and async.


01:13:36.320 --> 01:13:38.980
And best of all, there's not a subscription in sight.


01:13:38.980 --> 01:13:42.020
Check it out for yourself at training.talkpython.fm.


01:13:42.020 --> 01:13:43.620
Be sure to subscribe to the show,


01:13:43.620 --> 01:13:46.620
open your favorite podcast app, and search for Python.


01:13:46.620 --> 01:13:48.000
We should be right at the top.


01:13:48.000 --> 01:13:50.900
You can also find the iTunes feed at /iTunes,


01:13:50.900 --> 01:13:53.060
the Google Play feed at /play,


01:13:53.060 --> 01:13:57.080
and the Direct RSS feed at /rss on talkpython.fm.


01:13:57.080 --> 01:14:00.620
We're live streaming most of our recordings these days.


01:14:00.620 --> 01:14:01.740
If you want to be part of the show


01:14:01.740 --> 01:14:04.020
and have your comments featured on the air,


01:14:04.020 --> 01:14:05.900
be sure to subscribe to our YouTube channel


01:14:05.900 --> 01:14:08.900
at talkpython.fm/youtube.


01:14:08.900 --> 01:14:10.300
This is your host, Michael Kennedy.


01:14:10.300 --> 01:14:11.460
Thanks so much for listening.


01:14:11.460 --> 01:14:12.700
I really appreciate it.


01:14:12.700 --> 01:14:14.940
Now get out there and write some Python code.


01:14:14.940 --> 01:14:17.520
(upbeat music)


01:14:17.520 --> 01:14:32.520
[Music]


01:14:32.520 --> 01:14:35.100
(upbeat music)


01:14:35.100 --> 01:14:45.100
[BLANK_AUDIO]

