WEBVTT

00:00:00.001 --> 00:00:03.580
You likely know that Python is one of the fastest growing languages for data science.

00:00:03.580 --> 00:00:07.780
This is a discipline that combines the scientific inquiry of hypotheses and tests,

00:00:07.780 --> 00:00:12.480
the mathematical intuition of probability and statistics, the AI foundations of machine learning,

00:00:12.480 --> 00:00:18.500
affluency in big data processing, and the Python language itself. That's a very broad set of skills

00:00:18.500 --> 00:00:23.280
you'll need to be a good data scientist, and yet each one is deep and often hard to understand.

00:00:23.280 --> 00:00:28.120
That's why I'm excited to speak with Joel Gruse, a data scientist from Seattle. He wrote a book to

00:00:28.120 --> 00:00:32.500
help us all understand what's actually happening when we employ libraries such as scikit-learn or

00:00:32.500 --> 00:00:36.840
numpy. It's called data science from scratch, and that's the topic of this week's episode.

00:00:36.840 --> 00:00:42.860
This is Talk Python To Me, episode number 56, recorded April 19th, 2016.

00:00:56.240 --> 00:01:11.740
Welcome to Talk Python To Me, a weekly podcast on Python, the language, the libraries, the

00:01:11.740 --> 00:01:16.920
ecosystem, and the personalities. This is your host, Michael Kennedy. Follow me on Twitter where I'm at

00:01:16.920 --> 00:01:22.460
mkennedy. Keep up with the show and listen to past episodes at talkpython.fm, and follow the show on

00:01:22.460 --> 00:01:28.980
Twitter via at talkpython. This episode is brought to you by SnapCI and Hired. Thank them for supporting

00:01:28.980 --> 00:01:36.740
the show on Twitter via at snap underscore CI and at Hired underscore HQ. Hey, everyone. It's great to be

00:01:36.740 --> 00:01:40.940
back with you. I have something to share before we get to the main part of the show. I had a chance to be on the

00:01:40.940 --> 00:01:46.220
Partially Derivative podcast this week, where we did a short segment on programming tips for data scientists,

00:01:46.540 --> 00:01:51.060
relevant to this episode, wouldn't you say? I talked about using generator methods and the yield

00:01:51.060 --> 00:01:55.280
keyword to dramatically improve performance while processing lots of data in a pipeline.

00:01:55.280 --> 00:01:59.640
If you want to hear more about that kind of stuff, check out the link to the Partially Derivative episode

00:01:59.640 --> 00:02:04.860
in the show notes. Now, let's talk to Joel. Joel, welcome to the show. Thanks. I'm glad to be here.

00:02:04.860 --> 00:02:09.080
Yeah, I'm really looking forward to doing data science, but from scratch today.

00:02:09.080 --> 00:02:12.160
I know. That's my book. I'm the right person to be here for that topic.

00:02:12.540 --> 00:02:16.680
Fantastic. So we're going to dig into data science. And I think your idea of

00:02:16.680 --> 00:02:22.620
taking it from the fundamentals and building something that's not so complicated or so well

00:02:22.620 --> 00:02:26.560
polished and optimized that you can really understand what you're doing is great. But before we get into

00:02:26.560 --> 00:02:31.560
that, let's just sort of start from the beginning. How did you get into programming in Python?

00:02:31.560 --> 00:02:39.040
So if you go back about 10-ish years or so, I was in grad school for economics. I took a class called

00:02:39.040 --> 00:02:45.220
probability modeling, which was a lot of simulating Markov chains and doing Monte Carlo simulations and

00:02:45.220 --> 00:02:50.580
things like that. And the class was actually taught in MATLAB. School had a site license for MATLAB that

00:02:50.580 --> 00:02:55.500
only worked on campus. And so that meant that if I went home and worked at home, which I like to do,

00:02:55.500 --> 00:03:02.680
I couldn't use MATLAB. And so as an alternative, I found that I could use Python and NumPy to basically

00:03:02.680 --> 00:03:08.160
do all the MATLAB-y things without that site license. So that's actually why I started being

00:03:08.160 --> 00:03:10.340
into Python. And then I just kind of liked it and stuck with it.

00:03:10.340 --> 00:03:14.620
Yeah, I can definitely see how you would like working in Python more than MATLAB. I've

00:03:14.620 --> 00:03:20.960
spent my fair share of time in the .im files and it's all right, but it's not Python.

00:03:20.960 --> 00:03:25.560
You were studying economics and that's kind of how you got interested in data science,

00:03:25.560 --> 00:03:30.620
trying to answer these big questions. The subject of economics or some other way?

00:03:30.960 --> 00:03:34.200
I followed sort of a convoluted path. My background was math and economics.

00:03:34.200 --> 00:03:39.820
And I actually started off doing quantitative finance, options, pricing, mathematics of financial

00:03:39.820 --> 00:03:43.880
risk. I worked at a hedge fund. And then when the hedge fund went out of business, I just kind of

00:03:43.880 --> 00:03:50.980
lucked into this job at a startup called Faircast, where I was doing a lot of kind of BI work,

00:03:50.980 --> 00:03:58.600
writing SQL queries, building dashboards. And over the years, I just sort of moved more and more in a

00:03:58.600 --> 00:04:02.740
data science kind of direction and eventually became a data scientist and software developer.

00:04:02.740 --> 00:04:06.940
And certainly the math and economics training was helpful for that. But it was

00:04:06.940 --> 00:04:11.520
it was a career that I sort of ended up in by accident rather than through a deliberate plan.

00:04:11.520 --> 00:04:17.360
That's a little bit like my my career path as well. Studied math and just sort of had to learn the

00:04:17.360 --> 00:04:21.320
programming and stumbled into it. That's cool. What do you do day to day now?

00:04:21.840 --> 00:04:27.020
So I just started a new job. I work at a nonprofit. It's called the Allen Institute for Artificial

00:04:27.020 --> 00:04:33.160
Intelligence. It was founded by Paul Allen, who was one of the Microsoft founders. And we're

00:04:33.160 --> 00:04:38.660
basically doing kind of fundamental AI research. I can't tell you that much about what I do. So

00:04:38.660 --> 00:04:43.400
it's really my second week and I'm still kind of just learning what goes on. I'm learning my way

00:04:43.400 --> 00:04:51.480
around. But I work on a team called Aristo, which is basically building AI to take science quizzes and

00:04:51.480 --> 00:04:56.600
to understand science. Oh, very interesting. Yeah, that sounds like a fascinating place to work.

00:04:56.600 --> 00:05:00.000
It's really neat. Yeah. A lot of smart people, interesting problems.

00:05:00.000 --> 00:05:03.760
Would you say it's different than working in a hedge fund or is it surprisingly similar?

00:05:03.760 --> 00:05:07.740
No, it's totally different. I mean, it's similar in that there's a lot of smart people,

00:05:07.740 --> 00:05:11.640
but it's not really similar in any other way. Yeah. The goals are not necessarily the same,

00:05:11.640 --> 00:05:16.020
are they? The goals, the incentives, the sort of day-to-day stress levels, it's all different.

00:05:16.020 --> 00:05:21.480
Yeah. Yeah. Sounds great. So let's talk about your book a little bit. It's called Data Science

00:05:21.480 --> 00:05:28.060
from Scratch. And I think that's a really cool way to approach it. We have all these super polished

00:05:28.060 --> 00:05:34.240
data science tools. You know, we have them in Python, we have them in R and other languages as well.

00:05:34.560 --> 00:05:38.940
Why from scratch, rather than just grabbing one of these libraries or a set of these libraries and

00:05:38.940 --> 00:05:42.580
talking about how to use them? So a couple of reasons. One is, as I mentioned,

00:05:42.580 --> 00:05:49.760
I come from a math background and the math way of doing things is you can't really use something

00:05:49.760 --> 00:05:58.320
until you can prove it. I once had a teacher who came in and he looked at the syllabus for the

00:05:58.320 --> 00:06:02.700
previous semester and he said, oh, good, you proved this theorem. That means I can use it in this class.

00:06:03.040 --> 00:06:06.700
And so there's this real rigor around, you can't use things unless you understand them.

00:06:06.700 --> 00:06:11.760
And that approach always kind of resonated with me. And so that, to a large degree,

00:06:11.760 --> 00:06:17.760
that's the approach I took in the book. At the same time, you also have all these really powerful,

00:06:17.760 --> 00:06:24.740
really easy to use libraries like Psych and Learn, where you can go in and basically copy and paste,

00:06:24.740 --> 00:06:29.320
you know, five lines of code. And you've built a decision tree or you've built a regression model.

00:06:29.820 --> 00:06:34.860
And it's very easy to not know what you're doing. And you can pick up books and they'll tell you what

00:06:34.860 --> 00:06:39.340
commands to type. But you can also type those commands and again, not know what you're doing.

00:06:39.340 --> 00:06:44.660
And so I thought the book I would like, the book that I would have found valuable would be,

00:06:44.660 --> 00:06:50.060
here's what these models are actually doing behind the scenes. And so then when it's time you apply

00:06:50.060 --> 00:06:54.140
them, you kind of understand the principles, you understand where they go wrong, where they go right,

00:06:54.200 --> 00:06:56.380
what they're good for, and what they're actually doing.

00:06:56.380 --> 00:07:01.320
Yeah, I think that's, that's really interesting. And that is very much the mathematical way of,

00:07:01.320 --> 00:07:07.580
if you can't prove it, you can't use it, sort of a way of thinking, which is not that common

00:07:07.580 --> 00:07:12.120
in programming, you know, if there's an API, hey, just grab it and use it. But I think it's really

00:07:12.120 --> 00:07:18.660
important in data science, because you have all these different disciplines and backgrounds coming

00:07:18.660 --> 00:07:24.660
together to make it work, right? You're not just a pure programmer, and you're not, you know, a

00:07:24.660 --> 00:07:28.820
mathematician or some sort of domain expert, right? You kind of have to blend these together.

00:07:28.820 --> 00:07:34.300
Yep. And the other thing that I think ended up working pretty nicely about this approach was,

00:07:34.300 --> 00:07:38.660
there's a lot of really mathy books about data science and machine learning, that's like,

00:07:38.660 --> 00:07:43.740
here's an equation, here's an equation, here's an equation. But what I ended up doing was a little bit

00:07:43.740 --> 00:07:49.560
of math. But then when it came time to, here's how something works, do it in working Python code.

00:07:49.560 --> 00:07:54.880
So that it's rigorous in terms of here's all the steps laid out. But it's also, that's the code you

00:07:54.880 --> 00:07:57.020
can run and sort of follow along.

00:07:57.020 --> 00:08:02.020
Yeah, absolutely. Why Python and not something like R or some other language?

00:08:02.020 --> 00:08:08.120
So the short answer is that, for whatever reason, R is not sympathetic with the way my brain works.

00:08:08.120 --> 00:08:13.480
And whenever I try to sit down and start writing things in R, I just find that it doesn't work the way

00:08:13.480 --> 00:08:18.880
I expect it to. The joke is that R is a language designed by statisticians for statisticians.

00:08:18.880 --> 00:08:24.680
There's some truth to that, some unfairness to it. But for whatever reason, Python is much

00:08:24.680 --> 00:08:29.620
friendlier to the way that my brain works and the way that I solve problems. Actually, the first draft

00:08:29.620 --> 00:08:35.240
of my book was a lot harsher against R. And then some of the earlier reviewers didn't like that.

00:08:35.240 --> 00:08:38.920
They really don't like that. So I kind of revised it to be more gentle.

00:08:39.260 --> 00:08:44.320
Yeah, I think one of the things that's nice about Python is if you invest to learn data science

00:08:44.320 --> 00:08:50.560
through Python, or the Python data science tools, rather, and then you want to build sort of a more

00:08:50.560 --> 00:08:55.540
working application, you don't have to, you know, convert that to some other language, right? You're

00:08:55.540 --> 00:08:59.080
already in Python. It's sort of a full stack thing you keep working with, right?

00:08:59.080 --> 00:09:03.880
That's one important aspect. I think another important aspect is that from my perspective,

00:09:03.880 --> 00:09:11.020
Python code, if written well, is super readable. And so you don't necessarily have to be a Python

00:09:11.020 --> 00:09:16.520
expert to take a well written piece of Python code and understand what it's doing. Whereas other

00:09:16.520 --> 00:09:21.140
languages can be a lot more impenetrable. And so I find it a nice teaching language from that

00:09:21.140 --> 00:09:21.840
perspective as well.

00:09:22.080 --> 00:09:27.560
Yeah, I would say, you know, most of the universities in the US came to that same conclusion,

00:09:27.560 --> 00:09:33.800
right? With Python being the most common CompSci 101 course these days, as opposed to like Java or

00:09:33.800 --> 00:09:34.700
Scheme, which I took.

00:09:34.700 --> 00:09:40.660
Yeah, I mean, when I took, I also did Scheme when I took CompSci 101 way back in the day. And I actually

00:09:40.660 --> 00:09:46.040
thought that was a really nice language for learning computer science. But I can see why Python would be

00:09:46.040 --> 00:09:50.460
a better language for, you know, learning programming as opposed to computer science.

00:09:50.460 --> 00:09:55.200
So when you start off your book, you actually have a few sections that are just about,

00:09:55.200 --> 00:10:02.340
I'm going to teach you just enough Python to get started so that you can understand the data

00:10:02.340 --> 00:10:06.620
science tools and what we're doing. What kind of stuff do you put in there? Like, what do you

00:10:06.620 --> 00:10:10.640
consider fundamental for doing data science? And what can be sort of learned later?

00:10:10.640 --> 00:10:16.300
The most important things I put in were obviously functions, you want to do any kind of

00:10:16.300 --> 00:10:20.260
analytic work or Python functions. The other thing I really tried to emphasize,

00:10:20.260 --> 00:10:27.440
was writing kind of clean Pythonic code. So I also did a lot of list comprehensions,

00:10:27.440 --> 00:10:34.400
as well as understanding all the basic data structures. So lots of dicks and sets and some

00:10:34.400 --> 00:10:40.120
of the less common ones like default dicks and counters, I also make quite heavy use of.

00:10:40.420 --> 00:10:46.940
So basically, anything you would need to understand, here's how to write out an algorithm in Python,

00:10:46.940 --> 00:10:52.400
here's how to use the correct data structures in Python, I kind of baked into that initial

00:10:52.400 --> 00:10:58.240
introduction. Whereas more advanced things like really specialized data structures like queues,

00:10:58.240 --> 00:11:03.120
I just kind of introduced as needed later in the book, some of the more esoteric math functions,

00:11:03.320 --> 00:11:05.080
I just kind of introduced when they came up.

00:11:05.080 --> 00:11:09.680
I think obviously, nicer dictionaries, things like default dict and so on,

00:11:09.680 --> 00:11:15.460
played a really important role in the subsequent pieces where you're trying to make concise,

00:11:15.460 --> 00:11:23.080
clean statements. I thought your use of sort of list comprehensions and generator expressions,

00:11:23.080 --> 00:11:29.820
along with the other functional pieces like zip and counter and so on, were really,

00:11:30.080 --> 00:11:35.640
really interesting. You write some very concise code without being unreadable. I thought that was

00:11:35.640 --> 00:11:35.900
nice.

00:11:35.900 --> 00:11:41.740
That's one of the things I aim for. And as much time as you spend sort of revising the text of the book,

00:11:41.740 --> 00:11:46.460
I also spent probably just as much time revising the code of the book. I went back over it many,

00:11:46.460 --> 00:11:50.380
many times thinking, can I make this simpler? Can I make it cleaner? Can I make it easier to

00:11:50.380 --> 00:11:57.360
understand? And I try and do that. I'm kind of a stickler for code elegance. It's kind of a

00:11:57.360 --> 00:11:58.480
personal fault, maybe.

00:11:58.480 --> 00:12:04.080
But also, you come out with very polished codes. I found it very easy to follow. And I thought the

00:12:04.080 --> 00:12:10.140
focus on this functional style programming was really neat. So one of the things you talk about

00:12:10.140 --> 00:12:15.440
in the beginning is about visualizing data, because you have all these numbers. And obviously,

00:12:15.440 --> 00:12:19.520
to understand them, putting a picture to it is very nice. And what is that? Matt Plotlib?

00:12:19.920 --> 00:12:25.400
Yes. In the book, I only use Matt Plotlib. I was just having a conversation with some of my data

00:12:25.400 --> 00:12:31.300
science buddies about a week ago, where they were, as always, complaining that the data visualization

00:12:31.300 --> 00:12:33.360
story in Python is still not very good.

00:12:33.360 --> 00:12:38.960
Yeah. So you said that Matt Plotlib was starting to show its age in your book. What did you mean by

00:12:38.960 --> 00:12:39.160
that?

00:12:39.160 --> 00:12:44.840
When I first started learning Python, what I said, more than 10 years ago, as a Matlab replacement,

00:12:44.840 --> 00:12:51.720
like Matt Plotlib was around back then, and it had the same features and the same interface, and it

00:12:51.720 --> 00:12:57.640
produced the same not particularly attractive plots. So it hasn't kind of evolved as the rest of the

00:12:57.640 --> 00:13:03.480
language has evolved, say. And there's been some projects like Seaborn that try and put some prettiness

00:13:03.480 --> 00:13:09.720
on top of it. And there's been some other attempts to kind of bring the R-style GG plot into Python.

00:13:09.720 --> 00:13:14.680
But from my perspective, none of them has really like won the mindshare and become the solution.

00:13:14.680 --> 00:13:16.620
So everyone kind of does something different.

00:13:16.620 --> 00:13:23.560
Yeah. You also talked about Boku, which is from the Continuum guys. I had them on show 34. Can you

00:13:23.560 --> 00:13:28.120
maybe talk about that really quickly? Just what is it? What's it used for? That sort of thing.

00:13:28.280 --> 00:13:32.680
It is another visualization library. And I have to be honest with you, I haven't really

00:13:32.680 --> 00:13:38.120
checked it out since I was writing the book like more than a year ago. So I seem to remember that it

00:13:38.120 --> 00:13:42.360
seemed to have some facility for building in interactive plots and things like that.

00:13:42.360 --> 00:13:47.160
It's kind of D3, fancy graphics in the browser type stuff, right?

00:13:47.160 --> 00:13:49.400
I believe so. But I haven't spent much time playing with it.

00:13:49.400 --> 00:13:49.800
Yeah.

00:13:49.800 --> 00:13:55.240
I haven't been doing a lot of, probably doing actually more D3 than Python visualization recently.

00:13:55.240 --> 00:13:59.000
Right, right. Tell everyone what D3 is. I don't think we've talked too much about that on the show.

00:13:59.000 --> 00:14:06.360
Oh, sure. So D3 is actually a JavaScript library for building interactive data visualizations.

00:14:06.360 --> 00:14:12.520
It brings sort of an interesting model where you bind your data to DOM elements, which are quite often

00:14:12.520 --> 00:14:21.560
SVG elements. And so what that makes it easy to do is to have a data set. And when you add new data to it,

00:14:21.560 --> 00:14:26.360
your plot updates, and it allows a lot of really interesting interactivity. If you check out the

00:14:26.360 --> 00:14:32.760
D3 website, they have a gallery of all sorts of amazing visualizations where you look at and think,

00:14:32.760 --> 00:14:33.960
how on earth did they do that?

00:14:33.960 --> 00:14:39.080
Yeah. Whenever I go to the D3 website, I'm like, wow, I want to use this. I don't have a use for it,

00:14:39.080 --> 00:14:42.680
but it's fantastic. How can I just build stuff that looks like this?

00:14:42.680 --> 00:14:48.520
Well, I mean, so I have a joke, which is that good data scientists copy from the D3 gallery and great

00:14:48.520 --> 00:14:53.960
data scientists steal from the D3 gallery. And they give you the code for all of them. So I would say

00:14:53.960 --> 00:15:00.040
probably close to 100% of the D3 visualizations I've ever built have been something I found in the D3

00:15:00.040 --> 00:15:02.760
gallery and kind of tweaked until it fit my data.

00:15:02.760 --> 00:15:08.840
Yeah. Yeah. Nice. So the next section you talked about was like the fundamentals of the math and

00:15:08.840 --> 00:15:14.200
science that you need to know in order to be a data scientist. And I like the way that you put it.

00:15:14.200 --> 00:15:19.640
You said these are like the cartoon versions of big mathematical ideas. How much math do you need to

00:15:19.640 --> 00:15:25.160
know to call yourself a data scientist? Like if I studied just programming, I'm not a data scientist.

00:15:25.160 --> 00:15:28.680
So I studied just math. I'm not a data scientist. Like what's the story there?

00:15:28.680 --> 00:15:34.440
Data science is a funny thing in that there are as many different jobs called data science as there

00:15:34.440 --> 00:15:39.160
are data scientists. So there are people who will call themselves data scientists and they write SQL

00:15:39.160 --> 00:15:44.680
queries all day. There are people who call themselves data scientists and they do cutting edge machine

00:15:44.680 --> 00:15:49.080
learning research all day. There are people who do data scientists who convince people to click it at.

00:15:49.080 --> 00:15:53.800
Like you could know almost no math and still get a job where you call yourself a data scientist.

00:15:53.800 --> 00:15:58.760
You could know very little programming and get a job where you call yourself a data scientist.

00:15:58.760 --> 00:16:03.800
It's a field that's still kind of figuring itself out. And there's just such a breadth of the different

00:16:03.800 --> 00:16:08.120
roles that are all calling themselves data science. In an ideal world, you would know, you know,

00:16:08.120 --> 00:16:14.200
linear algebra, you would know probability, you would know statistics. But among people who are data

00:16:14.200 --> 00:16:17.640
scientists, some people know that stuff really well. Some people know that stuff not so well.

00:16:17.640 --> 00:16:23.000
Yeah, I can imagine. It's really about you've got a lot of data or some specific type of data and you

00:16:23.000 --> 00:16:28.200
want to answer questions about it or even discover the questions you could ask that nobody's asked, right?

00:16:29.960 --> 00:16:48.920
SnapCI is a continuous delivery tool from ThoughtWorks that lets you reliably test and deploy your code

00:16:48.920 --> 00:16:55.000
through multi-stage pipelines in the cloud without the hassle of managing hardware. Automate and visualize

00:16:55.000 --> 00:16:59.880
your deployments with ease and make pushing to production an effortless item on your to-do list.

00:16:59.880 --> 00:17:05.880
Snap also supports Docker and M browser debugging, and they integrate with AWS and Heroku.

00:17:05.880 --> 00:17:12.280
Thanks SnapCI for sponsoring this episode by trying them with no obligation for 30 days by going to

00:17:12.280 --> 00:17:21.800
snap.com/talkpython

00:17:21.800 --> 00:17:26.200
Sometimes it's actually the opposite, which is, I have a question I want to ask. Where can I find some

00:17:26.200 --> 00:17:31.240
data that will allow me to answer that question? So, you know, sometimes you start with the data and

00:17:31.240 --> 00:17:34.440
go to the questions. Sometimes you start with the questions and then you got to find the data.

00:17:34.440 --> 00:17:39.640
Right. Okay. Very interesting. So that actually brought you to your next section that you talked

00:17:39.640 --> 00:17:45.240
about in your book, which was getting data. And so what are the common ways that you talk about there?

00:17:45.240 --> 00:17:50.600
Nowadays, there's a lot of people who just like post interesting data sets. So if you go to like

00:17:50.600 --> 00:17:55.080
Kaggle, which is a site that does data science competitions, every one of their competitions has

00:17:55.080 --> 00:18:01.640
a data set, which might be interesting in ways that are not related to the competition. The government websites

00:18:01.640 --> 00:18:05.400
publish all sorts of data, some of which is potentially interesting. If you like weather,

00:18:05.400 --> 00:18:10.040
they publish weather data. If you like economics, they publish economic data. There's a lot of

00:18:10.040 --> 00:18:15.880
Python libraries for scraping websites. So even if a data set is not available, you can always go out and

00:18:15.880 --> 00:18:21.880
try and scrape it and collect it yourself and clean it. My go-to source is always Twitter. I always build

00:18:21.880 --> 00:18:27.320
things using Twitter data. So Twitter and all the other sites will have APIs where you can just make

00:18:27.320 --> 00:18:33.400
restful calls or even use libraries. Python has some Twitter libraries that abstracts all that away. And you can,

00:18:33.400 --> 00:18:37.720
you know, collect tweets on a given topic or collect tweets from certain users and

00:18:37.720 --> 00:18:41.080
collect your own tweets and do analysis on those.

00:18:41.080 --> 00:18:45.240
Yeah. And you had a section about that in the book. What was the package you were using for that?

00:18:45.240 --> 00:18:50.920
In Python, the package I usually use is called Twython. There's a bunch of them. That's the one that I

00:18:50.920 --> 00:18:53.880
got to work the easiest, but I think highly of it.

00:18:53.880 --> 00:18:56.360
I don't think I believe any other ones.

00:18:56.360 --> 00:19:01.880
Yeah. Yeah. Okay, cool. Yeah. I think Twitter is fairly special among the social networks.

00:19:01.880 --> 00:19:08.520
To me, Twitter is the social network of ideas more than it is of friends or family or whatever. So

00:19:08.520 --> 00:19:11.160
yeah, pretty cool. I love to get data from Twitter as well.

00:19:11.160 --> 00:19:15.560
Yeah. It's just that they won't give you the fire hose, but for most things, they'll give you

00:19:15.560 --> 00:19:19.960
more tweets than you can handle anyway. So if you want to find out what people are saying on a certain

00:19:19.960 --> 00:19:24.200
topic or just what people are talking about in general, it's an awesome source for that.

00:19:24.200 --> 00:19:30.680
Yeah. I saw some study or heard about some study a few years ago where people were studying, they were

00:19:30.680 --> 00:19:38.520
trying to do sentiment, mood analysis on Twitter, and then trying to tie that back to the stock market

00:19:38.520 --> 00:19:45.080
and predict how people were feeling on Twitter to near-term changes like in the next hour in the

00:19:45.080 --> 00:19:48.920
stock market, which I thought was a pretty interesting project.

00:19:48.920 --> 00:19:53.400
So ideas you can come up with using that data are pretty much endless. Every month, I think I'm done

00:19:53.400 --> 00:19:54.840
with it and then I think of something else to do.

00:19:54.840 --> 00:20:01.800
Nice. So then you kind of get into the topics that I think of as traditional data science,

00:20:01.800 --> 00:20:07.080
machine learning, neural networks, network analysis, that kind of stuff. And I thought maybe we could go

00:20:07.080 --> 00:20:13.240
through each section and you could tell me what it is that you need to sort of fundamentally

00:20:13.240 --> 00:20:20.200
understand what are the from scratch basic pieces of each part of this data science and then maybe

00:20:20.200 --> 00:20:22.360
some examples of problems you might solve with it.

00:20:22.360 --> 00:20:22.760
Sure.

00:20:22.760 --> 00:20:27.880
Yeah. So the first one you talk about is machine learning. And when you said machine learning

00:20:27.880 --> 00:20:33.240
and later neural networks from scratch, I'm like, wow, that's a pretty big thing to take on from

00:20:33.240 --> 00:20:34.760
scratch. What's the story there?

00:20:34.760 --> 00:20:41.400
Machine learning at a high level is just learning some kind of model from data rather than sitting

00:20:41.400 --> 00:20:47.320
down and writing out the model yourself by hand. And so if you have a small amount of data, then

00:20:47.320 --> 00:20:51.320
you know, coming up with an algorithm that's going to learn a model from it is actually a pretty

00:20:51.320 --> 00:20:56.120
reasonable thing to do if you go with a simple model and don't add too many bells and whistles.

00:20:56.120 --> 00:21:01.080
It's only when you want to, you know, start producing recommendations at Netflix scale

00:21:01.080 --> 00:21:06.280
or when you want to start, you know, building something to recognize speech patterns from audio files

00:21:06.280 --> 00:21:10.440
that your beautiful handcrafted Python is probably not going to be up to the task.

00:21:10.440 --> 00:21:19.000
Yeah. So I suppose the types of things you solve with machine learning is fairly unbounded. There's

00:21:19.000 --> 00:21:22.680
a lot of problems that machine learning answers. What are some of your favorite examples?

00:21:22.680 --> 00:21:29.160
Everything out there is machine learning these days. But if you want to talk about like projects that

00:21:29.160 --> 00:21:37.720
I've worked on for fun, one time I built a classifier to predict or to identify hacker news articles that I would be interested in or not

00:21:37.720 --> 00:21:49.720
interested in. So it would take kind of the feed of new hacker news stories and come up with a score between zero and one about how interested, you know, it thought I would be based on some initial seed values that I leave.

00:21:49.720 --> 00:21:58.280
How did you teach it?

00:21:58.280 --> 00:22:06.840
How did you teach it?

00:22:06.840 --> 00:22:15.400
I was for a few other kind of idiosyncratic features I threw in, like, you know, is it an ask hacker news? Is it a show hacker news? Does it have a dollar

00:22:15.400 --> 00:22:23.080
amount in there? Because there are a lot of kind of bad stories that's like, here's how to make $5,000, I think. So that was a good negative signal.

00:22:23.080 --> 00:22:24.920
It's sort of a spammy signal.

00:22:24.920 --> 00:22:31.640
Yeah. So, and then it turned out I wrote a blog post about it and someone posted the story to hacker news and then

00:22:31.640 --> 00:22:36.120
the hacker news community got very angry that someone would think that not every story there was worth

00:22:36.120 --> 00:22:39.480
reading. Because of course every story there is worth reading. So why would I want to filter some of them out?

00:22:39.480 --> 00:22:41.960
They accused me of wanting to live in a bubble.

00:22:41.960 --> 00:22:49.640
And yeah, they can be fairly critical on there. But that's funny. So the big question is, did your system

00:22:49.640 --> 00:22:51.320
like your article, your blog post?

00:22:51.320 --> 00:22:55.800
That's a good question. I don't know that I actually checked. I should go back and try and find that.

00:22:55.800 --> 00:22:59.800
It'd be funny if it recommended it to you. Or not. Either way, it would be funny to know.

00:22:59.800 --> 00:23:03.480
Well, you know, it's funny speaking of this. So, you know, I have an Android phone and so

00:23:03.480 --> 00:23:07.880
Google now will sometimes recommend me articles of things I'll be interested in.

00:23:07.880 --> 00:23:13.080
And occasionally it will recommend to me like my own blog posts, which I guess means doing a good job.

00:23:13.080 --> 00:23:19.240
You know, one of the things I did with machine learning was I have a five year old daughter and I do her clothes shopping.

00:23:19.560 --> 00:23:23.880
And I noticed that little boys clothes were very interesting and little girls clothes tend to be

00:23:23.880 --> 00:23:28.120
kind of like really boring. The boys clothes have like dinosaurs and rockets and robots and

00:23:28.120 --> 00:23:33.800
girls clothes have like hearts and flowers. And so I built a machine learning model to take an image of a

00:23:33.800 --> 00:23:37.240
children's t-shirt and predict whether it was a boy shirt or a girl shirt.

00:23:38.040 --> 00:23:44.440
Awesome. Yeah. And I agree with the classification there as well. I have three daughters and we still

00:23:44.440 --> 00:23:46.040
bought a fair number of boy clothes for them.

00:23:46.040 --> 00:23:47.880
Yeah, I did the same.

00:23:47.880 --> 00:23:53.480
Nice. So the next topic that you covered was nearest neighbors. And I mean, conceptually,

00:23:53.480 --> 00:23:58.360
I kind of know what nearest neighbors are pretty easily, but it's a computation hard problem,

00:23:58.360 --> 00:24:03.160
especially in higher dimensions. And so what kind of stuff do you do? What kind of problems do you solve with

00:24:03.160 --> 00:24:03.640
this?

00:24:03.640 --> 00:24:08.520
One thing you can do is when you don't have like a great parametric model for what's going on.

00:24:08.520 --> 00:24:12.760
So for instance, one place where I've seen this applied is when I have some kind of like

00:24:12.760 --> 00:24:19.000
time series type signal, and I want to know if it represents some kind of bad anomaly. And it might be,

00:24:19.000 --> 00:24:22.840
you know, like thousands of points in some weird shape. And I don't have a great way to classify it.

00:24:22.840 --> 00:24:28.520
But one thing you can do is if you have a bunch of labeled signals, you know, some are good or some

00:24:28.520 --> 00:24:33.880
are bad, you just say, I'm just going to take my set of reference signals and figure out which are

00:24:33.880 --> 00:24:38.360
the ones it's closest to. And that way, I don't need to necessarily have a model of what's a good

00:24:38.360 --> 00:24:43.320
signal or what's a bad signal. I can just say, I have some labeled data, that's enough. And so I

00:24:43.320 --> 00:24:46.200
think that's kind of the situation where it can be useful.

00:24:46.200 --> 00:24:51.080
Yeah, nice. When you're still trying to explore things, and maybe you don't, you don't have a model

00:24:51.080 --> 00:24:53.400
you're trying to fit it to, you're just trying to understand it, right?

00:24:53.400 --> 00:24:59.560
Yeah. Or think about where like, you have some weird kind of shapes, or weird kind of patterns

00:24:59.560 --> 00:25:04.600
that you can't really put math behind. What's a good pattern? What's a bad pattern? But you have

00:25:04.600 --> 00:25:10.040
some labels, then you could use nearest neighbors to basically classify without having to have a

00:25:10.040 --> 00:25:11.240
mathematical model behind it.

00:25:11.240 --> 00:25:15.960
Okay, yeah, very cool. So then you get to another topic called Bayesian

00:25:15.960 --> 00:25:21.880
Bayesian analysis, which the context I know this is around kind of determining spam and

00:25:21.880 --> 00:25:26.120
filtering and stuff like that. But what's, what's the story of Bayesian analysis?

00:25:26.120 --> 00:25:33.240
So it's named after Bayes' rule, which is just a theorem in statistics, having to do with ways of

00:25:33.240 --> 00:25:38.760
reversing conditional probabilities. So at a high level, if you have some knowledge about

00:25:38.760 --> 00:25:45.480
what is the probability of seeing certain features in email, given that an email is spam, you can use that

00:25:45.480 --> 00:25:50.360
data to produce estimates of what is the probability that an email is spam, given that you see those

00:25:50.360 --> 00:25:56.120
features. And so typically, if you have a lot of, you know, spam and non-spam, you can make estimates

00:25:56.120 --> 00:26:02.120
of, okay, given that an email is spam, it's likely that I'll see Viagra, and it's likely that I'll see

00:26:02.120 --> 00:26:07.240
Gary Twick. And then basically just a classifier that turns that around and allows you to now look for

00:26:07.240 --> 00:26:11.560
these features. And in a math-manipede rigorous way, come up with the probability, okay, here's the

00:26:11.560 --> 00:26:15.800
probability that this is spam. Okay. Yeah. Very interesting. What other types of problems?

00:26:15.800 --> 00:26:22.920
Spam is certainly easy to understand, but you know, hopefully that's solved by Gmail and other people

00:26:22.920 --> 00:26:27.960
for us, right? I think it was Paul Graham who wrote a pretty influential article about this approach,

00:26:27.960 --> 00:26:33.000
probably about 15 years ago now. And so I think a lot of the tools that, I don't know if it's what

00:26:33.000 --> 00:26:37.560
Gmail uses, but that a lot of these BAM assassin and some of the other mail providers used are still

00:26:37.560 --> 00:26:44.440
based on this principle of naive base. But basically the, anything where you have kind of big clumps of

00:26:44.440 --> 00:26:52.120
say text, and you want to classify it into one or more classes, like two or more classes, and you're

00:26:52.120 --> 00:26:58.680
willing to make this really big kind of technical assumption that the words in it are kind of independent

00:26:58.680 --> 00:27:02.600
of each other. So you're treating them kind of like a bag of words rather than as a sequence of words,

00:27:02.600 --> 00:27:08.360
then that's where you would use this kind of model. Okay. Yeah, cool. So maybe the opposite of that,

00:27:08.360 --> 00:27:14.120
where you treat words sort of as, as having more meaning is in natural language processing, which is

00:27:14.120 --> 00:27:18.680
another thing you talk about, right? Natural language processing is, is a huge field. Like people,

00:27:18.680 --> 00:27:25.080
there are textbooks about it. So in fact, there are Python books about it. So cram it into a chapter of a

00:27:25.080 --> 00:27:31.800
book is really just kind of, here's a really high level overview. And here's, you know, a couple of

00:27:31.800 --> 00:27:37.080
examples to give you a flavor. Natural language processing is, it's actually relevant to this sort of

00:27:37.080 --> 00:27:43.000
problems that I'm getting into in artificial intelligence. It gets used a lot by all the voice recognition in

00:27:43.000 --> 00:27:52.680
your phone, and a lot of these new, basically reading texts, and having a computer extract information from it. So it's a

00:27:52.680 --> 00:27:57.880
pretty important, pretty, pretty rich field. Yeah. And there's some pretty decent Python libraries for

00:27:57.880 --> 00:28:04.600
doing that, right? Yeah. So there's a, so the big one is called NLTK, the natural language toolkit,

00:28:04.600 --> 00:28:10.920
I think it stands for. And so it has its own book just about natural language processing in that library.

00:28:10.920 --> 00:28:15.400
And I think that's actually free on the web. But yeah, that would be a great place to start it. If you

00:28:15.400 --> 00:28:20.600
want to come to understand more deeply, there's also a nice Coursera course on natural language processing.

00:28:20.600 --> 00:28:25.560
It took several years back that I liked. Oh, okay. Yeah, excellent. So let's see. Another topic you

00:28:25.560 --> 00:28:31.880
covered were decision trees. And what's the story of these? A decision tree is kind of what it sounds

00:28:31.880 --> 00:28:39.000
like. Yeah. You know, intuitively, you might have a model of making decisions based on a tree. So I have,

00:28:39.000 --> 00:28:44.840
I want to know whether to buy a certain car or not. So, you know, I might start asking a question. Okay,

00:28:44.840 --> 00:28:49.800
is it an American car or a Japanese car? American car. Okay. Go to the next question. Does it have

00:28:49.800 --> 00:28:55.800
four doors or two doors? It has two doors. Okay. Is it gas or diesel? Diesel. And then you just have a

00:28:55.800 --> 00:29:00.600
sequence of questions to ask and you classify it based on those questions. Okay. It's diesel. That

00:29:00.600 --> 00:29:06.440
means, yes, I want to buy it. And so conceptually, you know, a tree like that is not that complicated.

00:29:06.440 --> 00:29:13.160
Interesting part is given some set of data, how do I build such a tree? And there, there's a variety of

00:29:13.160 --> 00:29:18.360
algorithms, but the one that we talk about in the book, it is a pretty simple one, which is basically

00:29:18.360 --> 00:29:24.040
around, okay, I have a bunch of data. It has a bunch of attributes. I can split my data on each

00:29:24.040 --> 00:29:28.440
attribute. So I could say, okay, at this stage, I want to look at two door versus four door. I want to

00:29:28.440 --> 00:29:35.240
look at Japanese versus American, or I want to look at gas versus diesel and which of those choices is

00:29:35.240 --> 00:29:43.640
going to allow me to kind of really separate the good buys from the bad buys the most. So if gas versus

00:29:43.640 --> 00:29:50.120
diesel totally splits where diesel is the buys and gas is the don't buys, that's a great thing to choose.

00:29:50.120 --> 00:29:56.040
If gas versus diesel splits where, you know, on gas, I want to buy half and not by half and on diesel,

00:29:56.040 --> 00:29:59.960
diesel and by half and up by half. That's not a good split to choose because it doesn't really

00:29:59.960 --> 00:30:05.320
help me at all. And so the mathematics here are just around kind of making this precise with this

00:30:05.320 --> 00:30:11.960
entropy and building these trees from the data. Okay. Yeah, that's very cool. The way where you

00:30:11.960 --> 00:30:17.640
build them knowing the data intimately, you know, that, that makes a lot of sense. And you don't really

00:30:17.640 --> 00:30:23.240
necessarily need data science to do that, right? That's just helping, you know, asking a few questions

00:30:23.240 --> 00:30:27.000
and making a decision. But the reverse, I think is pretty interesting.

00:30:27.000 --> 00:30:31.560
They build some of these expert systems in this kind of way. I think they apply them sometimes in like,

00:30:31.560 --> 00:30:35.240
uh, medical diagnosis and sometimes they do better than doctors.

00:30:35.240 --> 00:30:40.120
Wow. Okay. Sometimes just let the data talk, huh? Rather than intuition.

00:30:40.120 --> 00:30:56.600
This episode is brought to you by hired hired is a two sided curated marketplace that connects the

00:30:56.600 --> 00:31:01.960
world's knowledge workers to the best opportunities. Each offer you receive has salary and equity presented

00:31:01.960 --> 00:31:06.520
right up front, and you can view the offers to accept or reject them before you even talk to the company.

00:31:07.320 --> 00:31:11.960
Typically candidates receive five or more offers within the first week, and there are no obligations

00:31:11.960 --> 00:31:16.680
ever. Sounds awesome, doesn't it? Well, did I mention the signing bonus? Everyone who accepts a job

00:31:16.680 --> 00:31:22.040
from hired gets a thousand dollars signing bonus. And as talk Python listeners, it gets way sweeter.

00:31:22.040 --> 00:31:27.640
Use the link hired.com/talkpythontome and hired will double the signing bonus to $2,000.

00:31:29.080 --> 00:31:32.920
Is knocking. Visit hire.com/talkpythontome and answer the call.

00:31:32.920 --> 00:31:42.200
Somewhat related to that maybe is, you talked about neural networks as well.

00:31:42.200 --> 00:31:47.640
Yeah. So neural networks are a super hot topic right now, because I'm sure you've probably heard

00:31:47.640 --> 00:31:54.280
that all the buzz about, or about deep learning, right? And deep learning tends to have neural networks

00:31:54.280 --> 00:32:01.960
at the root of it. So neural networks are basically a way of building up kind of layers and layers of

00:32:01.960 --> 00:32:06.680
representations. And the deep learning, this book doesn't really go into deep learning because it's,

00:32:06.680 --> 00:32:12.920
the single neural network is hard enough to do by hand or from scratch. But basically it's a way to

00:32:12.920 --> 00:32:20.680
kind of build a classifier that works similar to how a toy model of a brain might work. So you have

00:32:20.680 --> 00:32:26.280
artificial neurons, and each neuron has a bunch of inputs that go into it with weights. If the weighted

00:32:26.280 --> 00:32:30.920
sum of the inputs exceeds some certain threshold, the neuron fires. And if it doesn't exceed that

00:32:30.920 --> 00:32:36.760
threshold, the neuron doesn't fire. So you present an input, which could be like a, an image. So basically

00:32:36.760 --> 00:32:42.040
a bunch of zeros and ones, and that causes some neurons to fire. And that propagates through this

00:32:42.040 --> 00:32:46.840
network. And in the end it will spit out, you know, I think you should be an image of a cat,

00:32:46.840 --> 00:32:52.520
or I think you should be an image of a dog. And you train it by showing that a lot of labeled images

00:32:52.520 --> 00:32:55.640
and adjusting the weights based on how you got them wrong.

00:32:55.640 --> 00:33:02.120
Yeah. Interesting. So you basically just say, these are the inputs and the decisions that you feed it

00:33:02.120 --> 00:33:07.160
known data. You say, you know, like, here's a cat, here's a cat, that's a dog. It's just a cat or a dog,

00:33:07.160 --> 00:33:11.400
right? And then you ask that question. I was gonna say, well, one of the problems that I sort

00:33:11.400 --> 00:33:15.080
of remember from neural networks is that they, as you try to get them more accurate,

00:33:15.080 --> 00:33:21.320
they get over-trained to just only do little bits of stuff. And so how does things like decision

00:33:21.320 --> 00:33:26.200
forests or these things and this deep learning, how does it deal with that?

00:33:26.200 --> 00:33:33.560
So random forests are basically taking multiple decision trees and combining them. So there's this

00:33:33.560 --> 00:33:40.360
kind of general principle where a lot of times rather than building one model, it's like super predictive.

00:33:40.760 --> 00:33:47.160
I take a lot of much less predictive models and just kind of let them vote on things or average

00:33:47.160 --> 00:33:52.600
the results or whatever. So you're right that it's really a decision tree. And I included like all

00:33:52.600 --> 00:33:57.560
hundred features in my data set that there's a good chance I'm going to like overfit and

00:33:57.560 --> 00:34:04.680
overlearn my training data and not generalize outside of that. So one thing that happens with decision

00:34:04.680 --> 00:34:10.600
trees is people often don't use the bare decision trees. They use the random forest where they'll build

00:34:10.600 --> 00:34:16.520
a bunch of smaller decision trees, each of which is really restricted to a small subset of features

00:34:16.520 --> 00:34:22.840
so that each one is individually less powerful. But then when you combine them, they do well in the aggregate

00:34:22.840 --> 00:34:28.440
and they don't have necessarily the same overfitting problem that single decision tree with lots of features would.

00:34:28.440 --> 00:34:34.600
So in neural networks, especially in deep learning, they're not exactly the same techniques, but other

00:34:34.600 --> 00:34:40.040
techniques where you will zero out some of your weight sometimes and train without them to make

00:34:40.040 --> 00:34:44.040
sure that they don't learn too much. And there's a lot of other techniques that get used in order to

00:34:44.040 --> 00:34:47.560
make sure you're not overfitting. There's something that, you know, data scientists and machine learning

00:34:47.560 --> 00:34:48.840
people worry about a lot.

00:34:48.840 --> 00:34:54.760
It's a really hot field, like you said right now. That's awesome. So one more section I'd like to touch

00:34:54.760 --> 00:34:58.120
on is you talked about this thing called recommender systems.

00:34:58.120 --> 00:35:02.920
So mobile, they're just what they sound like. Anyone who's used the internet has a pretty good experience

00:35:02.920 --> 00:35:09.000
where you go to Netflix and it says movies for Joel and it's trying to predict what it thinks I'll like.

00:35:09.000 --> 00:35:16.680
Or you go to Amazon and it will recommend items for me. And you can go a little further where you have

00:35:16.680 --> 00:35:22.200
all these startups like Stitch Fix is a very hot data science startup where you tell what kind of clothes

00:35:22.200 --> 00:35:28.200
you like and they'll send you a box of clothes every month that they think you'll like a lot.

00:35:28.200 --> 00:35:36.280
And so generating these kind of recommendations is a pretty popular task within data science. A lot of data scientists

00:35:36.280 --> 00:35:41.560
work on these kind of problems. There's a lot of data scientists work in this job. So they have to sell you stuff

00:35:41.560 --> 00:35:44.200
and they always want to sell you stuff that they think you'll like.

00:35:44.200 --> 00:35:47.400
You convert better if you figure out what people actually might want, right?

00:35:47.400 --> 00:35:54.200
Yeah, I mean, well, if Amazon sends me an email, "Hey, Joel, you know, these five things are on sale

00:35:54.200 --> 00:35:58.760
and they're five things that I really want to buy," then that's much better for them and potentially

00:35:58.760 --> 00:36:03.080
for me than if they send me a random email that's like, "These are the five most popular things on

00:36:03.080 --> 00:36:08.280
Amazon today." Yeah, it definitely is better. I really like going to my Kindle and looking at

00:36:08.280 --> 00:36:14.040
the recommended things based on what I've been reading. But the Netflix recommendations, I know they

00:36:14.040 --> 00:36:19.880
do really great work, but it just doesn't work for me because my seven-year-old daughter watches

00:36:19.880 --> 00:36:25.000
Strawberry Shortcake and other random things. I get a lot of kid shows recommended to me.

00:36:25.000 --> 00:36:30.440
They have a "Who's watching" button that you can click and say "A kid is watching."

00:36:30.440 --> 00:36:33.640
I know, but my daughter won't use it. She'll just randomly pick one.

00:36:33.640 --> 00:36:40.040
Yeah, so you know, it's funny. I use that button pretty well, but my Netflix recommendations are also

00:36:40.040 --> 00:36:45.320
not that good. But I think it's just because I don't like anything on there. So no matter what they're

00:36:45.320 --> 00:36:49.720
telling me, I'm not going to like it. Yeah, I hear you. I feel like you covered a pretty

00:36:49.720 --> 00:36:57.960
wide swath of data science from scratch. Some of the topics were really accessible. Some of them

00:36:57.960 --> 00:37:04.680
required more math and more thinking, but it was all a really nice presentation. What do you feel like

00:37:04.680 --> 00:37:11.240
you left out? There aren't any topics that, if anything, I kept adding stuff while I was writing it.

00:37:11.240 --> 00:37:17.800
So I don't feel like there were any huge topics that I necessarily left out. But in terms of coverage,

00:37:17.800 --> 00:37:20.840
probably my biggest regret is that I used Python 2 instead of Python 3.

00:37:20.840 --> 00:37:27.640
Yeah. Okay. And I saw on your GitHub repo, you had something about Python 3 in there. Is that right?

00:37:27.640 --> 00:37:32.520
Yes. So pretty much as soon as the book came out, one, I got a lot of emails from people saying,

00:37:32.520 --> 00:37:37.240
"Hey, why do you not use Python 3?" And then I also got a number of emails saying,

00:37:37.240 --> 00:37:41.320
"I would like to use Python 3. Will the code work?" And so I wrote them back and I said,

00:37:41.320 --> 00:37:46.760
"Yeah, you know, I don't feel like the code shouldn't work. Give it a try. I bet it works

00:37:46.760 --> 00:37:51.400
with probably a few changes, add some parentheses, print statements and so on." And then eventually one

00:37:51.400 --> 00:37:57.000
guy wrote me back and he said, "It doesn't work." I said, "Okay." So I sat down and I said, "I'm going to

00:37:57.000 --> 00:38:04.840
convert the code to Python 3." And it took me about, I'd say four to five hours. And that's with me

00:38:04.840 --> 00:38:09.640
knowing the code intimately. So it would have taken someone who hadn't written the code in the first

00:38:09.640 --> 00:38:12.920
place a lot longer than that. And then I felt kind of guilty that I've been telling all these people that

00:38:12.920 --> 00:38:14.840
it was so easy to do when it wasn't.

00:38:14.840 --> 00:38:16.200
Just spend a week. It'll be fine.

00:38:16.200 --> 00:38:23.080
Yeah. But so yeah, I have the Python 3 versions of the code up on the GitHub. I sort of regret not

00:38:23.080 --> 00:38:25.000
having just done it that way in the first place.

00:38:25.000 --> 00:38:31.720
Yeah, sure. So at the end, you talked about data science, not from scratch, and you pointed out

00:38:31.720 --> 00:38:37.320
a lot of the libraries you might actually use, like NumPy and so on. Do you want to talk about that

00:38:37.320 --> 00:38:43.480
really quick? Like what's the real data science versus the from scratch data science comparison?

00:38:43.480 --> 00:38:51.240
Yeah. So I would say that NumPy is pretty fundamental. That's basically the linear algebra

00:38:51.240 --> 00:38:58.600
library for Python. So it provides you matrices, matrix algebra, high performance arrays, things

00:38:58.600 --> 00:39:05.080
like that, you don't get just built in. And you might not use it directly, but a lot of the other

00:39:05.080 --> 00:39:11.320
libraries are really built on top of it. So kind of the most broadly accessible machine learning

00:39:11.320 --> 00:39:16.520
library for Python is called Scikit Learn. And it has really nice documentation and really nice tutorials

00:39:16.520 --> 00:39:21.320
and a fairly standard API for building machine learning models. Anything you want to build a

00:39:21.320 --> 00:39:27.720
regression model or a random forest model or any kind of classifier, that's probably the place to go.

00:39:27.720 --> 00:39:34.840
There's also Pandas, which is the data frame library, which is good if you're working with tabular data. So

00:39:34.840 --> 00:39:39.640
not necessarily the machine learning side of data science, but more of the, I have kind of a

00:39:39.640 --> 00:39:48.760
spreadsheet data set and now I want to clean it and aggregate it and pivot it and look for kind of data

00:39:48.760 --> 00:39:49.400
analysis type insights.

00:39:49.400 --> 00:39:56.040
Yeah. If you're exploring, like you said, tabular data and you kind of kind of load it up and clean it,

00:39:56.040 --> 00:39:57.800
Pandas seems really fantastic for that.

00:39:57.800 --> 00:40:03.800
It's a really nice library. And then kind of the new kid on the block is a TensorFlow, which is

00:40:03.800 --> 00:40:09.960
Google's deep learning library. And it's only was released, you know, a few months ago and it's gone

00:40:09.960 --> 00:40:15.800
through. It's not 1.0 yet, but it seems like people are sort of converging around it as that's how they're

00:40:15.800 --> 00:40:20.920
going to do deep learning in Python. Now there are other sort of previous libraries that some people

00:40:20.920 --> 00:40:24.840
used and still use, but TensorFlow seems to be gaining a lot of mindshare.

00:40:24.840 --> 00:40:30.280
Okay. Yeah, that's really cool. And I've definitely seen TensorFlow talked about a lot in this context,

00:40:30.280 --> 00:40:35.080
but it's just a library. You can download that and run it locally. It's not like a cloud type thing,

00:40:35.080 --> 00:40:35.560
right?

00:40:35.560 --> 00:40:42.520
Yes. So currently it's a local library and I'm not sure if they've come out with a version that you can

00:40:42.520 --> 00:40:47.880
kind of do your own cloud, like on various AWS instances. But, but I know they're definitely going

00:40:47.880 --> 00:40:52.040
in that direction because a lot of this stuff, these deep learning models take a really long time to

00:40:52.040 --> 00:40:56.840
train. And so you want to go to use them for anything serious. You want to distribute them and

00:40:56.840 --> 00:40:59.240
throw them in high-powered machinery and not just run them on your laptop.

00:40:59.240 --> 00:41:03.160
Yeah, of course. If you have tons of data, maybe it's better to get a bunch of machines

00:41:03.160 --> 00:41:09.160
for an hour. So what do you think about these cloud learning or data science platforms? I'm thinking like

00:41:09.160 --> 00:41:15.640
Azure machine learning, or, you know, I just had SigOpt on the show a few shows ago. I'm not really sure

00:41:15.640 --> 00:41:20.680
what else is out there in terms of like go out to the cloud and grab some data science stuff.

00:41:20.680 --> 00:41:21.800
What do you think about that?

00:41:21.800 --> 00:41:31.080
I haven't spent much time looking at any of those. I think they can add value in terms of either one,

00:41:31.080 --> 00:41:37.720
if you don't have the data science or machine learning expertise in house to do whatever you need to do.

00:41:37.720 --> 00:41:43.800
Or two, if you have some kind of model that you've built, but you need help either putting it into

00:41:43.800 --> 00:41:50.440
production or operationalizing it somehow, that they can build a pretty good role there. But my sense is

00:41:50.440 --> 00:41:56.600
that most people doing data science are aligned more on running the libraries and running the models

00:41:56.600 --> 00:42:00.520
themselves. But that could be just my bias sample of the people who I talk to.

00:42:00.520 --> 00:42:07.000
Yeah, of course, of course. Cool. So another thing that you said you're into is taking some of the

00:42:07.000 --> 00:42:12.920
ideas from Haskell and thinking about how those might manifest in Python. What do you went up to there?

00:42:12.920 --> 00:42:19.800
Haskell, if you're not familiar with it, is kind of the purest of the pure functionally typed

00:42:19.800 --> 00:42:28.440
languages with strong types and lazy evaluation and things like that. And so once I spent some

00:42:28.440 --> 00:42:33.080
time in that world, I spent a lot of time thinking about how can I bring some of these concepts back

00:42:33.080 --> 00:42:42.200
into Python. And in Python 3, lazy evaluation plays a much bigger role in the sense that like range

00:42:42.200 --> 00:42:48.360
is a generator instead of a list and all the map and filter and things like that also are generators

00:42:48.360 --> 00:42:55.320
instead of lists. But I started getting into the iter tools library, which starts giving you tools for

00:42:55.320 --> 00:43:01.640
generating basically infinite sequences and just trying to see how far I could go using

00:43:01.640 --> 00:43:07.960
pure functions and infinite sequences and avoiding mutable variables and other things that you try not

00:43:07.960 --> 00:43:10.920
to do when you're working in a Haskell-like language.

00:43:10.920 --> 00:43:14.760
Yeah. And how did you feel like it came out in the end? Do you feel like you were able to bring a lot

00:43:14.760 --> 00:43:15.880
of those ideas over?

00:43:15.880 --> 00:43:23.640
I was, and I ended up producing code that was really neat and really impenetrable. The lazy

00:43:23.640 --> 00:43:29.720
infinite sequences stuff, that was more almost academic in terms of like, yes, I managed to do it. Like,

00:43:29.720 --> 00:43:34.920
this is mathematically interesting and it works well, but it's not readable at all.

00:43:34.920 --> 00:43:41.480
So imagine you wanted to represent, let's say a binary tree in Python. Kind of the two,

00:43:41.480 --> 00:43:47.800
I would say obvious approaches would be one to make some kind of like class where it has a,

00:43:47.800 --> 00:43:53.400
you know, a value element and a left element and a right element. And then you also might just use

00:43:53.400 --> 00:43:57.800
a dictionary to represent it where it had those keys. In a language with algebraic data types,

00:43:57.800 --> 00:44:04.280
like Haskell, you would just basically represent that kind of tree as a product type where it just

00:44:04.280 --> 00:44:09.240
has like three elements. And so I said, you know, what if I just represented a tree as a tupple with

00:44:09.240 --> 00:44:13.320
three elements where first element is the left subtree, the second element is the value,

00:44:13.320 --> 00:44:17.480
third element is the right subtree. And similarly, if you want to do like linked lists in Python,

00:44:17.480 --> 00:44:21.640
which you probably don't want to do, but if you did want to do linked lists in Python, you just treat

00:44:21.640 --> 00:44:27.640
them as a tupple. First element is the element and the second element is the tail linked list. And so,

00:44:28.040 --> 00:44:32.120
I actually found that I was able to write some pretty nice code using those kinds of ideas.

00:44:32.120 --> 00:44:38.520
And I did some coding interviews that way. I'm not sure the interviewers appreciated it.

00:44:38.520 --> 00:44:47.880
What is this guy talking about? Yeah, cool. So what question given your work at the AI Institute and

00:44:47.880 --> 00:44:54.600
your background in data science and so on, I wanted to ask is the last year or so, there's been a lot of

00:44:54.600 --> 00:45:01.640
news items and people coming out saying that artificial intelligence is a danger to humanity.

00:45:01.640 --> 00:45:07.240
What's your thought? Is like AI something we should be super excited about? Or is it something we should

00:45:07.240 --> 00:45:12.440
be maybe cautious about? I would say I probably fall in the middle. I mean, I'm excited about it because

00:45:12.440 --> 00:45:18.360
this is my job, but I don't go around encouraging everyone else that they have to be excited about it

00:45:18.360 --> 00:45:23.000
because I don't know that that's necessarily warranted. But at the same token, I don't spend a lot of time

00:45:23.000 --> 00:45:29.960
worrying about how dangerous it is. I think we're pretty far off from the time when we have to worry

00:45:29.960 --> 00:45:35.480
about that. And I do have some friends who think we should worry about it now before it's too late. But

00:45:35.480 --> 00:45:39.160
I think there's a lot more important things to worry about in the world when I read the news.

00:45:39.160 --> 00:45:47.160
Yeah, I kind of agree with you on that. And I think there's two, certainly two ways to look at that. On

00:45:47.160 --> 00:45:53.000
one hand, if you think about things like self-driving cars, let's just take that as an example. Like,

00:45:53.000 --> 00:46:02.920
I believe one of the biggest job categories for men in the United States is some form of driving,

00:46:02.920 --> 00:46:08.520
like driving a truck, driving a taxi, those types of things, delivery vehicles and so on. And if self-driving

00:46:08.520 --> 00:46:13.800
cars were to like remove all that, like that would have large social effects. But I think, you know,

00:46:13.800 --> 00:46:19.400
that's not so much the way that people, at least recently in the news, were talking about it. It's

00:46:19.400 --> 00:46:26.200
more like Terminator style, right? And so that I, I'm not too worried about this personally. Who knows?

00:46:26.200 --> 00:46:30.360
Yeah. I mean, I personally would pay a lot of money for a self-driving car because I,

00:46:30.360 --> 00:46:35.080
I don't like driving that much. And I'd much rather be able to read while I'm going somewhere.

00:46:35.080 --> 00:46:40.040
Yeah. Driving is fun until you get stuck on i5 for half an hour inching along. Then you know,

00:46:40.360 --> 00:46:42.760
I don't like driving anymore. Exactly.

00:46:42.760 --> 00:46:46.040
Awesome. Well, we're kind of coming up near the end of the show. Let me ask you just a

00:46:46.040 --> 00:46:50.040
few closing questions. I always ask on my guests, if you're going to write some Python code,

00:46:50.040 --> 00:46:55.000
what editor do you open up? So these days I tend to use Adam for pretty much everything,

00:46:55.000 --> 00:46:58.440
memory leaks and all. Nice. Yeah. That's from a GitHub, right?

00:46:58.440 --> 00:47:03.080
Yeah. It's pretty similar to sublime, but it doesn't nag you for money. So yeah, it's,

00:47:03.080 --> 00:47:08.520
it's pretty nice. And I think it's Adam.io. They have a really cool little video about how

00:47:08.520 --> 00:47:10.920
it's the editor of the future. It's nice.

00:47:10.920 --> 00:47:14.680
Yeah. I don't know if I go that far, but it's the editor of the present at least.

00:47:14.680 --> 00:47:18.920
Yeah. It's like a George Jetson sort of like a promo video. It's, it's pretty funny.

00:47:18.920 --> 00:47:21.400
It is pretty nice. Isn't it? Does it have good Python support?

00:47:21.400 --> 00:47:28.200
You know, I'm never, I'm not someone who leans on like ID functionality a lot. So if good Python

00:47:28.200 --> 00:47:32.840
support counts as syntax highlighting, yes, but that's all I tend to use it for. So yeah.

00:47:32.840 --> 00:47:38.200
Yeah. Yeah. Okay. Cool. And if you look at on the Python package index, there's,

00:47:38.200 --> 00:47:46.360
you know, 75 plus thousand packages and we all have experience with, you know, different parts

00:47:46.360 --> 00:47:49.720
of it and there's things that we love and would recommend, like, what is your favorite one you

00:47:49.720 --> 00:47:51.640
might recommend that people maybe don't know about?

00:47:51.640 --> 00:47:57.720
So the one I recommend that people don't necessarily know about is called Beautiful Soup. It's,

00:47:57.720 --> 00:48:04.040
basically a HTML parsing library. And so if you start scraping data from webpages, you're going to get

00:48:04.040 --> 00:48:10.600
a big mess of ugly HTML. That's probably not even well formed. most of the time,

00:48:10.600 --> 00:48:12.600
most people don't bother to well form their HTML.

00:48:12.600 --> 00:48:17.800
Are you telling me that I can't just like load that up as an XML document or something like this?

00:48:17.800 --> 00:48:21.800
No, I'm just kidding. Of course it's, it's terrible trying to work directly on the web,

00:48:21.800 --> 00:48:25.000
right? And Beautiful Soup is really, I really like it as well.

00:48:25.000 --> 00:48:28.360
It's really nice. I mean, you have to spend a little bit of time getting used to

00:48:28.360 --> 00:48:34.920
it's API and interface and everything, but it's super handy for getting data out of webpages and

00:48:34.920 --> 00:48:37.240
doing anything where you have to do a bunch of scraping.

00:48:37.240 --> 00:48:39.880
And you cover that in your book, right? In the getting data,

00:48:39.880 --> 00:48:42.200
I use Beautiful Soup a little bit and show people how to use it.

00:48:42.200 --> 00:48:46.280
Yeah, I cover it a little bit. It's a nice addition to the data scientist toolkit.

00:48:46.280 --> 00:48:52.600
Nice. So Joel, how do people find your book? Amazon? Just a Google search for data science

00:48:52.600 --> 00:48:57.000
from scratch? Yeah, it's on Amazon and you can buy it from O'Reilly.com. But yeah,

00:48:57.000 --> 00:48:59.000
if you Google data science from scratch, you'll find it.

00:48:59.000 --> 00:49:04.360
Cool. And I'll put a link to the GitHub repo where you have all the code examples and so on as well.

00:49:04.360 --> 00:49:10.280
It's been really fun to talk about data science. And I think you have a really interesting way of teaching

00:49:10.280 --> 00:49:16.920
people to appreciate the tools that we're all fairly familiar with by showing you how to build it from

00:49:16.920 --> 00:49:19.000
scratch. So thanks for that.

00:49:19.000 --> 00:49:19.320
My pleasure.

00:49:21.000 --> 00:49:25.080
This has been another episode of Talk Python to Me. Today's guest was Joel Gruse and this episode

00:49:25.080 --> 00:49:30.360
has been sponsored by SnapCI and Hired. Thank you guys for supporting the show. SnapCI is modern,

00:49:30.360 --> 00:49:34.360
continuous integration and delivery. Build, test and deploy your code directly from GitHub,

00:49:34.360 --> 00:49:39.000
all in your browser with debugging, Docker and parallelism included. Try them for free at snap.ci

00:49:39.080 --> 00:49:45.720
slash talk.com. Hired wants to help you find your next big thing. Visit Hired.com slash talkpython to me to get five or

00:49:45.720 --> 00:49:50.280
more offers with salary and equity presented right up front and a special listener signing bonus of $2,000.

00:49:50.280 --> 00:49:56.440
Are you or a colleague trying to learn Python? Have you tried books or videos that left you bored by just

00:49:56.440 --> 00:50:01.800
covering the topics point by point? Check out my new online course, Python Jumpstart by building 10 apps at

00:50:01.800 --> 00:50:07.240
 talkpython.fm/course to experience a more engaging way to learn Python. You can find links

00:50:07.240 --> 00:50:14.360
from this show at talkpython.fm/episodes slash show slash 56. Be sure to subscribe to the show. Open

00:50:14.360 --> 00:50:18.440
your favorite podcatcher and search for Python. We should be right at the top. You can also find the

00:50:18.440 --> 00:50:25.560
iTunes feed at /itunes, the Google Play feed at /play and the direct RSS feed at /rss on

00:50:25.560 --> 00:50:31.480
talkpython.fm. Our theme music is Developers, Developers, Developers by Corey Smith, who goes by Smix.

00:50:31.480 --> 00:50:36.280
You can hear his entire song at talkpython.fm/music. This is your host, Michael Kennedy.

00:50:36.280 --> 00:50:39.560
Thank you so much for listening. Smix, take us out of here.

00:50:39.560 --> 00:50:49.400
Stating with my voice. There's no norm that I can feel within. Haven't been sleeping. I've been using lots of rest. I'll pass the mic back to who rocked it best.

00:50:49.400 --> 00:50:51.120
I'm first developers.

00:50:51.120 --> 00:50:58.120
I'm first developers.

00:50:58.120 --> 00:51:01.380
Developers, developers, developers, developers.

00:51:01.380 --> 00:51:01.640
.

00:51:01.640 --> 00:51:01.760
Thank you.

00:51:01.760 --> 00:51:31.740
Thank you.

