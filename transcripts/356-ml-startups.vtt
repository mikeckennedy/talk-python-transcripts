WEBVTT

00:00:00.001 --> 00:00:06.080
Have you been considering launching a product or even a business based on Python's AI ML stack?

00:00:06.080 --> 00:00:12.440
We have a great guest on this episode, Dylan Fox, who is the founder of Assembly AI and has been

00:00:12.440 --> 00:00:18.340
building his startup successfully over the past few years. He has interesting stories of hundreds

00:00:18.340 --> 00:00:23.960
of GPUs in the cloud, evolving ML models, and much more that I know you're going to enjoy hearing.

00:00:24.300 --> 00:00:30.820
This is Talk Python to Me, episode 356, recorded February 17th, 2022.

00:00:30.820 --> 00:00:48.680
Welcome to Talk Python to Me, a weekly podcast on Python. This is your host, Michael Kennedy.

00:00:48.680 --> 00:00:53.320
Follow me on Twitter where I'm @mkennedy and keep up with the show and listen to past episodes

00:00:53.320 --> 00:01:00.000
at talkpython.fm and follow the show on Twitter via at Talk Python. We've started streaming most of our

00:01:00.000 --> 00:01:05.840
episodes live on YouTube. Subscribe to our YouTube channel over at talkpython.fm/youtube to get

00:01:05.840 --> 00:01:12.140
notified about upcoming shows and be part of that episode. This episode is brought to you by the

00:01:12.140 --> 00:01:18.040
Stack Overflow podcast. Join them to hear about programming stories and about how software is made

00:01:18.040 --> 00:01:25.160
in century. Find out about errors as soon as they happen. Transcripts for this and all of our episodes

00:01:25.160 --> 00:01:30.420
are brought to you by Assembly AI. Do you need a great automatic speech to text API? Get human level

00:01:30.420 --> 00:01:37.720
accuracy in just a few lines of code. Visit talkpython.fm/assembly AI. Dylan, welcome to Talk Python to

00:01:37.720 --> 00:01:44.620
me. Yes. Thank you. I am a fan. I've listened to a lot of episodes and big podcast fan. So I'm happy

00:01:44.620 --> 00:01:51.040
to be on here. Thanks. Yeah, I'm sure you are. Your specialty is in the realm of turning voice into

00:01:51.040 --> 00:01:58.400
words. Yeah, that's right. I bet you do a lot of studying of media like podcasts or videos and so on,

00:01:58.400 --> 00:02:04.120
right? Yeah, it's actually funny. So I started the company. I started Assembly like four years ago. And

00:02:04.120 --> 00:02:08.800
there was this one audio file that I always used to test our speech recognition models on. It was this

00:02:08.800 --> 00:02:14.660
Al Gore TED Talk from like 2007, I think. And I've almost memorized like parts of that TED Talk because

00:02:14.660 --> 00:02:19.880
I've just tested it so many times. It's actually still part of our end-to-end test suite. It's in

00:02:19.880 --> 00:02:24.440
there. It's like a legacy kind of founder thing that's like in the current still. Yeah, how cool.

00:02:24.860 --> 00:02:31.240
Yeah, it is kind of funny, especially now that we're like 30 people at the company and I'll see

00:02:31.240 --> 00:02:37.120
some of the newer engineers writing tests around like that Al Gore file still. And it makes me laugh

00:02:37.120 --> 00:02:41.800
because there's no real reason I picked that. It was just something easy that came to me.

00:02:41.800 --> 00:02:43.540
Yeah. Yeah. You can start. I just got to grab

00:02:43.540 --> 00:02:48.360
some audio. Here's something, right? Yeah, exactly. Exactly. So yeah, definitely. I've also listened to

00:02:48.360 --> 00:02:53.760
like a ton of podcasts and I was with, we just started like releasing models for different

00:02:53.760 --> 00:02:58.600
languages. And I was with someone from our team last week and I heard this like phone call and it's

00:02:58.600 --> 00:03:02.140
like, it's like foreign language. People like screaming on this. I was like, what are you listening

00:03:02.140 --> 00:03:08.080
to? And, it is, but it is an audio company. You get sometimes data from customers and it's like,

00:03:08.080 --> 00:03:11.580
you have to like listen to it. Yeah. I bet there's some interesting stories in there.

00:03:11.660 --> 00:03:16.860
Yeah. For sure. Yeah. Well, we're very privacy conscious. So not too, not too many, but yeah.

00:03:16.860 --> 00:03:23.800
Yeah. There was just, on the verge, there was just a article about a different speech to text

00:03:23.800 --> 00:03:29.920
company. I don't know. Have you seen this, that there was some suspicious stuff going on? Let me see

00:03:29.920 --> 00:03:34.720
if I can find it. I think, what was it called? It was Otter. Interesting.

00:03:34.940 --> 00:03:41.900
Otter.ai. I'm not asking you to speak on them, but this, a journalist, Otter.ai scares a reminder

00:03:41.900 --> 00:03:48.780
that cloud transcription isn't completely private. Basically there was a conversation about Uyghurs

00:03:48.780 --> 00:03:54.840
in China or something like that. Yeah. And then they unprompted reached out to the person who was

00:03:54.840 --> 00:03:59.360
in the conversation and said, could you tell us the nature of why you had to speak about this?

00:03:59.360 --> 00:04:01.020
No way. They're like, what?

00:04:01.140 --> 00:04:03.120
That is crazy. That is crazy.

00:04:03.120 --> 00:04:07.320
Yeah. They're like, we're a little concerned about why, you know, it's, it's kind of like

00:04:07.320 --> 00:04:08.820
interested in the content of our conversation.

00:04:08.820 --> 00:04:13.580
Yeah. There's a lot of that like suspicion around, you know, there's some like, like conspiracy

00:04:13.580 --> 00:04:17.920
theories right around like, oh, does your phone like listen to you while you're around and does

00:04:17.920 --> 00:04:22.820
it evidently listen to you and then use that data to remarket to you? And I was talking to someone

00:04:22.820 --> 00:04:27.460
about this recently. I did nothing about the location-based advertising world, but sometimes I'll

00:04:27.460 --> 00:04:33.360
be talking about something and then I'll see ads for it on my phone or if I'm on Instagram or something.

00:04:33.360 --> 00:04:38.200
And someone told me it's probably more based on like the location that you're in and the other data

00:04:38.200 --> 00:04:39.520
that they have about you.

00:04:39.520 --> 00:04:44.460
Yeah. You were at your friend's house. Your friend just searched for that, then told you about it.

00:04:44.460 --> 00:04:45.500
Yeah, exactly.

00:04:45.500 --> 00:04:51.200
I think the reality of that is that it's actually more terrifying than if they were just listening to you.

00:04:51.200 --> 00:04:51.820
Yeah, it is.

00:04:51.820 --> 00:04:56.120
That they can piece together this shadow reality of you that matches reality so well.

00:04:56.160 --> 00:05:00.620
Yeah. Like your friend just bought this thing and you went over and then, so maybe you're

00:05:00.620 --> 00:05:02.060
interested in this thing because you're probably-

00:05:02.060 --> 00:05:04.160
Yeah, they probably told you about it or something, right?

00:05:04.160 --> 00:05:08.360
Yeah. It is really crazy. It is really crazy. I haven't paid too much attention to all the

00:05:08.360 --> 00:05:12.760
changes that are happening around like, I listened to some podcast, I think on the Wall Street Journal

00:05:12.760 --> 00:05:17.200
about like the big change that Google's making around that tracking and how a lot of people are up

00:05:17.200 --> 00:05:20.480
in arms about that. And it's, it was saying something how like they're going to have,

00:05:20.480 --> 00:05:23.840
and sorry if I'm derailing whatever plan we had for a conversation here.

00:05:24.060 --> 00:05:28.200
You're derailing in a way that I'm like super passionate about because it's so crazy,

00:05:28.200 --> 00:05:28.700
but yeah.

00:05:28.700 --> 00:05:29.660
Yeah, yeah.

00:05:29.660 --> 00:05:31.340
We'll go too deep, but yeah, it's so interesting.

00:05:31.340 --> 00:05:36.600
They said that there's, I'm probably butchering this, but something how like for each user,

00:05:36.600 --> 00:05:40.620
they're just going to have like six like categories about you. And then one of them is going to be

00:05:40.620 --> 00:05:46.860
randomly inserted as to somehow anonymize your profile. And I just thought, yeah, it was super weird

00:05:46.860 --> 00:05:48.140
to hear about how they're doing it.

00:05:48.140 --> 00:05:48.200
Yeah.

00:05:48.200 --> 00:05:52.420
And like what the meeting was internally that came up with that idea, you know? So I'm like,

00:05:52.420 --> 00:05:55.360
well, let's just throw a random category on there. I don't know.

00:05:55.360 --> 00:06:02.160
My thoughts are we're faced or we're presented with a false dichotomy. Either you can have horrible,

00:06:02.160 --> 00:06:09.560
creepy, tracking, advertising, shadow things like we talked about. Or you can let the creators and

00:06:09.560 --> 00:06:15.040
the sites that make the services you love die. There are more choices than two in this world,

00:06:15.040 --> 00:06:15.360
right?

00:06:15.640 --> 00:06:22.460
For example, you could have some kind of ad that is related to what is on the page rather than who

00:06:22.460 --> 00:06:26.460
is coming to the page, right? You don't necessarily have to retarget me. Like for example, right here

00:06:26.460 --> 00:06:31.120
on this podcast, I'm going to tell people about, I'm not sure without looking at the schedule, what the

00:06:31.120 --> 00:06:37.880
advertisers are for this episode, but I am going to tell people about that. And it is exactly related

00:06:37.880 --> 00:06:45.420
to the content of the show. It's not that I found out that, you know, Sarah from Illinois did this Google

00:06:45.420 --> 00:06:50.160
search and visit this page. So now we're going to show it like, no, it's, there's so many sites like this,

00:06:50.160 --> 00:06:56.600
this one here on the verge, you could have ads for assembly AI and it'll be, maybe you don't actually want

00:06:56.600 --> 00:07:02.220
on this one, but you know, like things like this, it would be totally reasonable to put an ad for speech

00:07:02.220 --> 00:07:09.340
to text companies on there that requires no targeting and no evil shadow companies. And there's,

00:07:09.340 --> 00:07:14.240
you know, like we go on and on, but there are many ways like that, right? That don't require this false

00:07:14.240 --> 00:07:19.060
dichotomy that is being presented to us. So hopefully we don't end up with either of those. Cause I don't

00:07:19.060 --> 00:07:23.260
think those are the best options or the only options. Yeah. It's weird how that's kind of how

00:07:23.260 --> 00:07:28.980
things developed, you know, to where we are now. Yeah. But I agree with you. There's probably a lot we

00:07:28.980 --> 00:07:34.700
can everyone's looking for like, okay, well, if we do retargeting, we can get 2% better returns.

00:07:34.700 --> 00:07:38.940
And like, you know, the, no one's worried about, well, what happens to society?

00:07:38.940 --> 00:07:42.600
Yeah. That's actually what I was going to say. It's all about the kind of high growth,

00:07:42.600 --> 00:07:47.200
like society that we have where we need to maximize growth and maximize returns. And I mean,

00:07:47.200 --> 00:07:52.860
I understand this acutely. Like I'm, you know, the CEO of a startup, so I get it. But yeah,

00:07:52.860 --> 00:07:57.600
it's when it's like growth over everything, you end up with things like what you said,

00:07:57.600 --> 00:08:02.300
like, Oh, it proves it to our returns 2%. So let's do this. But you don't think about what

00:08:02.300 --> 00:08:06.300
the trade-offs will be. Yeah, absolutely. All right. Well, thanks for that diversion.

00:08:06.300 --> 00:08:12.500
That was great. Yeah. But let's, before we get beyond it, let me just, get your background

00:08:12.500 --> 00:08:15.720
real quick. How do you get into programming? And I'm going to mix it up a little and machine

00:08:15.720 --> 00:08:19.860
learning. Yeah. Yeah, definitely. Do you want the long, the long story or the short story?

00:08:19.860 --> 00:08:21.720
How many minutes do I have? Intermediate. Intermediate.

00:08:22.720 --> 00:08:28.360
So the intermediate story is that I started a company when I was in college, just like a

00:08:28.360 --> 00:08:35.240
college startup thing. And at the time was very limited in my programming knowledge. I had done

00:08:35.240 --> 00:08:41.620
some basic like HTML when I was a kid, I was really into like, like Counter-Strike and Call of Duty and,

00:08:41.620 --> 00:08:48.060
Oh yeah. Yeah. I would like sell up like private servers. I don't, I don't know how I got into this,

00:08:48.060 --> 00:08:53.100
but would, would, I like rented these servers and I would like windowed remote windows desktop into

00:08:53.100 --> 00:08:58.640
them and set up like private Counter-Strike servers and then sell those and set up like a basic website

00:08:58.640 --> 00:09:04.080
for it with HTML and CSS. And, and my brother was super into computers. So it was always kind of

00:09:04.080 --> 00:09:09.900
into computers. And then in college, got into startups. And I think like programming and startups are like

00:09:09.900 --> 00:09:15.120
really connected. So through that, learned how to code, learned how to program, started attending

00:09:15.120 --> 00:09:21.580
Python meetups in Washington, DC, where I went to school. And that's how I met Matt Mackay, who we,

00:09:21.580 --> 00:09:23.720
it was a mutual, mutual connection. Yeah.

00:09:23.720 --> 00:09:29.220
So attended a bunch of meetups, learned how to program and then got really into it. But I think

00:09:29.220 --> 00:09:35.500
what I found myself more interested in was the like more like meaty programming problems, more like,

00:09:35.500 --> 00:09:42.040
I guess like algorithm type problems. And that kind of naturally led me to like machine learning and NLP.

00:09:42.040 --> 00:09:46.480
And then kind of just like took off from there. Cause I found that I was really interested in,

00:09:46.480 --> 00:09:50.920
in machine learning and, and like different NLP problems.

00:09:50.920 --> 00:09:54.760
Those are obviously the really hard ones that, you know, it's, yeah.

00:09:54.760 --> 00:09:58.920
Especially probably when was this that you were doing it or the,

00:09:58.920 --> 00:10:02.700
this is maybe like 2013, 2014.

00:10:02.700 --> 00:10:05.800
Yeah. So, so kind of the early days of when that,

00:10:05.800 --> 00:10:11.900
that was becoming real. Right. I remember feeling like all this AI and text to speech

00:10:11.900 --> 00:10:18.520
or speech to text rather type of stuff was very much like fusion, like 30 years out, always 30.

00:10:18.520 --> 00:10:22.500
Like it's, it's going to come eventually, but you know, people are doing weird stuff in Lisp

00:10:22.500 --> 00:10:24.780
and it doesn't seem to be doing much of anything at all.

00:10:24.780 --> 00:10:28.760
Like some like Pearl script seat out. Yeah. Yeah.

00:10:28.760 --> 00:10:32.780
And then all of a sudden we end up with like amazing speech attacks. We ended up with

00:10:32.780 --> 00:10:36.860
self-driving cars, like something clicked and it all came, came to life.

00:10:36.860 --> 00:10:39.880
Yeah. It's kind of crazy, especially over the last couple of years.

00:10:39.880 --> 00:10:44.900
I think what's really interesting is that a lot of the advances in like self-driving cars and NLP

00:10:44.900 --> 00:10:49.500
and speech to text, they're all based on similar machine learning algorithms. So,

00:10:49.900 --> 00:10:54.900
you know, like the transformer, right. Which is like really popular type of neural network

00:10:54.900 --> 00:11:01.520
that came out was initially applied towards like just NLP, like text language modeling related tasks.

00:11:01.520 --> 00:11:06.420
Now that's shown to be super powerful for speech as well. Whereas classical machine learning,

00:11:06.420 --> 00:11:11.460
there were still these underlying algorithms like support vector machines or, you know, other,

00:11:11.460 --> 00:11:15.240
other types of underlying algorithms, but a lot of the work was around like the data.

00:11:15.380 --> 00:11:19.460
And so how can you extract better features for this type of data? And you had to be like,

00:11:19.460 --> 00:11:24.760
I remember when I was getting into speech recognition, I bought this speech recognition,

00:11:24.760 --> 00:11:29.400
like textbook. And this is, yeah, a while ago. And it was around like really understanding like

00:11:29.400 --> 00:11:35.160
phonemes and how different things are spoken and how the human speech is spoken. And now you don't

00:11:35.160 --> 00:11:38.460
need to know about that. You just get a bunch of audio data and you train these like big neural

00:11:38.460 --> 00:11:42.360
networks. They figure that out. Right. You wanted to understand British accents and American

00:11:42.360 --> 00:11:47.180
accents. You just give it a bunch of more data. Yeah, yeah, exactly. So, but it is,

00:11:47.180 --> 00:11:52.640
it is crazy to see where things have gotten over the last couple of years in particular. Yeah. So

00:11:52.640 --> 00:11:58.240
when I was starting out, neural networks were there, but they're a lot more basic and you didn't have,

00:11:58.240 --> 00:12:02.200
like, there's a lot more compute resources down, more mature libraries like TensorFlow and

00:12:02.200 --> 00:12:07.300
PyTorch. I think I went to like one of the first TensorFlow meetups that they had,

00:12:07.300 --> 00:12:12.240
or not meetups, like developer days or whatever down at the Google conference. So it's,

00:12:12.240 --> 00:12:16.440
it's like so new still. Yeah. It's so new. Yeah. It's easy to forget. It is a while ago.

00:12:16.440 --> 00:12:19.980
It is. Yeah. That all the stuff didn't even exist, right? Yeah, absolutely.

00:12:19.980 --> 00:12:24.400
So you mentioned assembly, assembly AI. Yes. That's what you're doing these days, right?

00:12:24.400 --> 00:12:31.340
Yeah. So I am the founder of a company called assembly AI. We create APIs that can automatically

00:12:31.340 --> 00:12:39.700
transcribe and understand audio data. So we had APIs for automatic speech to text of audio files,

00:12:39.700 --> 00:12:44.840
live audio streams, live audio streams, and then APIs that can also summarize audio content,

00:12:44.840 --> 00:12:50.940
do content moderation on top of audio content, detect topics, what we call like audio intelligence

00:12:50.940 --> 00:12:57.740
APIs. And so we have a lot of startups and enterprises using our APIs to build the way we call it like

00:12:57.740 --> 00:13:03.940
applications on top of audio data, whether it's like content moderation of a social platform or

00:13:03.940 --> 00:13:09.560
speeding up workflows like I'm sure you have where you take a podcast recording and transcribe it so

00:13:09.560 --> 00:13:14.720
you can make it more shareable or extract pieces of it to make it more shareable. Yeah, exactly. Yeah.

00:13:14.720 --> 00:13:24.260
Basically for me, it's a CLI command that runs Python against your API, against a remote MP3 file. And then,

00:13:24.260 --> 00:13:30.920
you know, magic. That's the great thing about podcast hosts that are also programmers. Like I've talked

00:13:30.920 --> 00:13:34.640
to a few and they're all like, there's a bunch that are non-programmers and they use these different

00:13:34.640 --> 00:13:38.940
services. But every podcast host that I've talked to that's a programmer, they have their own like

00:13:38.940 --> 00:13:42.520
CLIs and Python scripts that they're running. Yeah.

00:13:42.520 --> 00:13:48.500
Yeah. Yeah. Yeah. There's a whole series of just, you know, CLIs and other commands to do the workflow.

00:13:48.500 --> 00:13:53.540
Yeah. I do want to just give a quick statement, disclaimer. Yes. So if you go over to the transcripts

00:13:53.540 --> 00:13:57.220
or possibly, I suspect if you've listened to the beginning of this episode, it'll say that it's

00:13:57.220 --> 00:14:04.780
sponsored by Assembly AI. This episode is not part of that sponsorship. This is just, you and I got to

00:14:04.780 --> 00:14:08.100
know each other. You're doing interesting stuff. You've been on some other shows that I've heard that

00:14:08.100 --> 00:14:13.480
the conversation was interesting. So invited you on. Thank you for sponsoring the show. But just to

00:14:13.480 --> 00:14:18.720
point out, this is not actually part of that. But with the transcripts that we do have on the show,

00:14:18.720 --> 00:14:23.000
the last year or so are basically generated from you guys, which is pretty cool.

00:14:23.000 --> 00:14:27.220
Yep. Yep. And we don't even need to talk about Assembly that much on this podcast. We can talk

00:14:27.220 --> 00:14:34.020
about other things. Yeah. So one of the things I want to talk about, and maybe what's on the screen

00:14:34.020 --> 00:14:39.380
here gives a little bit of a hint of being TensorFlow, is why do you think Python is popular

00:14:39.380 --> 00:14:44.960
for machine learning startups in general? I feel that I'm not as deep in that space as you, but

00:14:44.960 --> 00:14:50.580
looking in from the outside, I guess I would say it feels very much like Python is the primary way

00:14:50.580 --> 00:14:52.800
which a lot of this machine learning stuff is done.

00:14:52.800 --> 00:14:57.900
Yeah. Yeah. That's a good point. So why that is, outside of machine learning even, I think Python's

00:14:57.900 --> 00:15:05.860
just such a popular language because it's so easy to build with compared to PHP or C# and even

00:15:05.860 --> 00:15:11.040
JavaScript. When I learned to code, I started with Python because the syntax was easy to understand.

00:15:11.040 --> 00:15:16.260
There were a lot of good resources. And then there's this snowball effect where more people know Python,

00:15:16.260 --> 00:15:19.580
so there's more tutorials about Python. There's more libraries about Python. And

00:15:19.580 --> 00:15:23.620
it's just more popular of a language. Yeah. This insights to be pulling this up.

00:15:23.620 --> 00:15:28.140
Yeah. If people have talked about this a lot, right? But if you pull up the Stack Overflow

00:15:28.140 --> 00:15:35.160
trends for the most popular programming languages, there's only one that is going dramatically up out

00:15:35.160 --> 00:15:38.880
of 10 languages or something. It's just so much more popular. Yeah.

00:15:38.880 --> 00:15:46.200
It is. It's so interesting how it's really sort of taken off. And it wasn't back in when you got

00:15:46.200 --> 00:15:50.460
started and when I got started back in this general area in 2012. Interesting. What was the number one

00:15:50.460 --> 00:15:54.760
language then? The number one then was, what is that? C#.

00:15:54.760 --> 00:16:00.440
C#. But you got to keep in mind, this is a little bit of a historical bias of Stack Overflow.

00:16:00.440 --> 00:16:05.100
Stack Overflow was started by Jeff Atwood and Joel Spolsky, who came out of the .NET space.

00:16:05.100 --> 00:16:11.040
So when they created it, its initial traction was in C# and VB. But over time, clearly,

00:16:11.040 --> 00:16:16.000
it's become where programmers go, obviously. So take that a bit with a grain of salt. But that was the

00:16:16.000 --> 00:16:19.320
number one back in the early days. Another founder legacy decision.

00:16:19.320 --> 00:16:25.960
Yeah. Exactly. Yeah. Yeah. So I agree that it's absolutely generally popular. And I think there's

00:16:25.960 --> 00:16:30.920
some interesting reasons for that. Yeah. It's just so approachable, but it's not a toy, right? A lot

00:16:30.920 --> 00:16:35.480
of approachable languages are toy languages and a lot of non-toy languages are hard to approach.

00:16:35.480 --> 00:16:49.080
So I'll see you next time.

00:16:49.080 --> 00:16:53.800
For a dozen years, the Stack Overflow podcast has been exploring what it means to be a developer

00:16:53.800 --> 00:17:00.280
and how the art and practice of software programming is changing our world. Are you wondering which skills

00:17:00.280 --> 00:17:04.840
you need to break into the world of technology or level up as a developer? Curious how the tools

00:17:04.840 --> 00:17:10.760
and frameworks you use every day were created? The Stack Overflow podcast is your resource for tough

00:17:10.760 --> 00:17:16.280
coding questions and your home for candid conversations with guests from leading tech companies about the

00:17:16.280 --> 00:17:22.360
art and practice of programming. From Rails to React, from Java to Python, the Stack Overflow podcast will

00:17:22.360 --> 00:17:25.880
help you understand how technology is made and where it's headed.

00:17:25.880 --> 00:17:31.560
Hosted by Ben Popper, Cassidy Williams, Matt Kierninder, and Sierra Ford, the Stack Overflow

00:17:31.560 --> 00:17:36.600
podcast is your home for all things code. You'll find new episodes twice a week, wherever you get

00:17:36.600 --> 00:17:43.640
your podcasts. Just visit talkpython.fm/stackoverflow and click your podcast player icon to subscribe.

00:17:43.640 --> 00:17:48.280
And one more thing. I know you're a podcast veteran and you could just open up your favorite podcast app

00:17:48.280 --> 00:17:53.320
and search for the Stack Overflow podcast and subscribe there. But our sponsors continue to support us when

00:17:53.320 --> 00:17:57.800
they see results and they'll only know you're interested from Talk Python if you use our link.

00:17:57.800 --> 00:18:03.560
So if you plan on listening, do use our link, talkpython.fm/stackoverflow to get started.

00:18:03.560 --> 00:18:05.640
Thank you to Stack Overflow for sponsoring the show.

00:18:05.640 --> 00:18:12.360
Yeah, like for me, it was very easy to get started with Python. And I actually had,

00:18:12.360 --> 00:18:19.160
so I taught myself how to program. I went to college, I studied economics. So I did not study

00:18:19.160 --> 00:18:24.520
programming in college computer science. And the first language I started to try to learn was PHP.

00:18:24.520 --> 00:18:29.240
And I bought this huge PHP textbook and made it halfway through. And I was like, what is going on?

00:18:29.240 --> 00:18:33.480
I gave up and then tried again with Python later and it was so much easier.

00:18:33.480 --> 00:18:38.120
And then I also wonder how much of this is for the machine learning libraries in specific.

00:18:38.120 --> 00:18:43.160
Like you have these macro trends where a lot of the data science boot camps that have been so popular.

00:18:43.160 --> 00:18:50.120
There's scikit-learn. I know we have a tab up there. There's NumPy and I dream of what NLTK is one of the

00:18:50.120 --> 00:18:56.920
popular NLP libraries. So there are a lot of libraries in Python in the early, like when I was getting into NLP,

00:18:56.920 --> 00:19:02.760
I worked a lot with NLTK and like SciPy and scikit-learn and NumPy. And I think a lot of

00:19:02.760 --> 00:19:07.240
work was done around there. And so people that were doing data science or doing some type of machine

00:19:07.240 --> 00:19:12.040
learning were already in Python. And then now you have like PyTorch and TensorFlow and it's just like

00:19:12.040 --> 00:19:17.080
kind of cemented, like, okay, the machine learning libraries today, the popular ones that you work

00:19:17.080 --> 00:19:17.880
with them in Python.

00:19:17.880 --> 00:19:22.280
Yeah. You want to give us your thoughts on those? We've got TensorFlow and PyTorch and

00:19:22.280 --> 00:19:26.440
you know, probably scikit-learn as well. Those are the traditional ones. We've got some newer ones like

00:19:26.440 --> 00:19:27.320
Hugging Face.

00:19:27.320 --> 00:19:28.840
Yeah. Yeah. They're a cool company.

00:19:28.840 --> 00:19:32.120
Yeah. Maybe give us a survey of how you see the...

00:19:32.120 --> 00:19:33.080
The different libraries.

00:19:33.080 --> 00:19:33.160
The different libraries.

00:19:33.160 --> 00:19:33.880
The different libraries.

00:19:33.880 --> 00:19:35.480
The libraries that people might choose from.

00:19:35.480 --> 00:19:38.360
So when we started the company, everything was in TensorFlow.

00:19:38.360 --> 00:19:39.400
When was that?

00:19:39.400 --> 00:19:41.560
Back in like late 2017.

00:19:41.560 --> 00:19:41.960
Okay.

00:19:41.960 --> 00:19:47.720
Yeah. Late 2017. Everything was in TensorFlow. And actually, I don't know what year

00:19:47.720 --> 00:19:52.840
PyTorch came out. I don't even know if it was out back then. Or maybe it was like just internally at Facebook.

00:19:52.840 --> 00:19:53.720
Yeah. It's pretty new. Yeah.

00:19:53.720 --> 00:19:58.680
Yeah. So TensorFlow was definitely, they got started early. I think their docs and

00:19:58.680 --> 00:20:05.240
the framework just got complicated over the years. And then they sort of rebooted with like TensorFlow 2.0.

00:20:05.240 --> 00:20:11.000
And then there was like Keras that was popular. It kind of got pulled in. Now, I think,

00:20:11.000 --> 00:20:16.600
so we switched everything over to PyTorch in the last year or two. A big reason for that was that,

00:20:16.600 --> 00:20:21.080
and we actually put out this article on our blog comparing like PyTorch and TensorFlow. And we have

00:20:21.080 --> 00:20:27.560
this chart where we show like the percentage of papers that are released where the code for the

00:20:27.560 --> 00:20:34.840
paper is in PyTorch versus TensorFlow. And it's a huge, huge difference. Like most of the latest

00:20:34.840 --> 00:20:39.960
research gets implemented. Yeah, here it is. If you go down to one of, so this is, yeah,

00:20:39.960 --> 00:20:44.520
hugging face. Can you go keep going? Yeah, research papers. Yeah. Go up to that one. Yeah. Okay.

00:20:44.520 --> 00:20:48.840
So it shows like the fraction of papers. And so what we're showing here for the people that are

00:20:48.840 --> 00:20:55.960
listening is like a graph that shows the percentage of papers that are used, built using PyTorch

00:20:55.960 --> 00:21:00.600
versus TensorFlow over time. Yeah. When you started, it was, what is this? Six, seven percent?

00:21:00.600 --> 00:21:04.280
Yeah, probably 10 percent. And the balance being TensorFlow,

00:21:04.280 --> 00:21:05.800
Right. When you started your company. Right.

00:21:05.800 --> 00:21:10.520
And now it's 75 percent PyTorch. That's a huge, very large change.

00:21:10.520 --> 00:21:13.400
It's a dramatic change. You know, if PyTorch was a company, it'd be like

00:21:13.400 --> 00:21:19.000
probably raising a lot of money. I think one of the reasons we picked PyTorch is because a lot of the

00:21:19.000 --> 00:21:23.800
newer research was being implemented in PyTorch first. There were examples in PyTorch. And so it's

00:21:23.800 --> 00:21:28.600
easier to get, they have it on their, it's their tagline, but to quote them, like from research to

00:21:28.600 --> 00:21:35.800
production, right? Like it was easier to get more exotic, advanced neural networks into production and

00:21:35.800 --> 00:21:39.800
like actually start training models with those different types of layers or operations or loss

00:21:39.800 --> 00:21:44.840
functions that were released in these different papers. So we started using PyTorch and we kind of

00:21:44.840 --> 00:21:46.200
haven't looked back. Yeah.

00:21:46.200 --> 00:21:52.360
Well, if you're tracking all the research and trying to build a cutting edge startup around ML,

00:21:52.360 --> 00:21:57.720
you don't want to wait for this to make its way to other frameworks. You want to just grab it and go.

00:21:57.720 --> 00:22:00.520
So that's where the research is being done. That helps a lot, right?

00:22:00.520 --> 00:22:05.720
Right. Exactly. Yeah. You can get, just get up and running a lot faster with the newer research. And so

00:22:05.720 --> 00:22:11.720
most companies that I talk to now, they're all using PyTorch. I think PyTorch is definitely like

00:22:11.720 --> 00:22:15.960
the more popular framework. There's some new ones coming out that have people excited, but still,

00:22:15.960 --> 00:22:21.400
like from what I can sense, PyTorch is, if someone was going to get started today, I would tell them

00:22:21.400 --> 00:22:22.760
to start with PyTorch. Yeah.

00:22:22.760 --> 00:22:26.440
And I think TensorFlow is also- Who runs PyTorch? I think it's-

00:22:26.440 --> 00:22:28.360
Sorry, who runs PyTorch? It's released by Facebook, right?

00:22:28.360 --> 00:22:30.120
Yeah. And then TensorFlow, that's Google, right?

00:22:30.120 --> 00:22:35.720
Google, yeah. Yeah. And I think Google's tried to tie TensorFlow into their cloud ML products,

00:22:35.720 --> 00:22:41.560
so train your models on Google Cloud and use their TPUs in the cloud. And there's probably some business

00:22:41.560 --> 00:22:46.040
in this cases behind that, but I feel like it may have made the developer experience worse because

00:22:46.040 --> 00:22:51.720
it's trying to get back to Google. Whereas PyTorch isn't trying to get you to train your models on

00:22:51.720 --> 00:22:54.760
Facebook Cloud or something. Yeah. What's the story with hugging face?

00:22:54.760 --> 00:22:58.840
This is how- People probably wouldn't use Facebook Cloud if that existed nowadays.

00:22:58.840 --> 00:23:04.200
Yeah. I don't know if you'd want to host your data interface. Meta Cloud. I mean, Meta Cloud now.

00:23:04.200 --> 00:23:06.520
Yeah. Meta Cloud. You can only do it in VR. Yeah.

00:23:06.520 --> 00:23:10.680
What's the story with Hugging Face? So Hugging Face is a cool- So this is a company, actually.

00:23:10.680 --> 00:23:16.040
And they have, it's kind of hard to even explain. It's like, you can basically get access to a bunch

00:23:16.040 --> 00:23:20.520
of different pre-trained models really quickly through Hugging Face. And so if you want to,

00:23:20.520 --> 00:23:25.720
a lot of work around NLP now is like how familiar you are with like self-supervised learning or

00:23:26.440 --> 00:23:31.560
base models for NLP. I'm familiar with that. Somewhat. So the idea is to have a,

00:23:31.560 --> 00:23:37.640
like a general model and then apply some sort of transfer learning to build up a more specialized

00:23:37.640 --> 00:23:41.640
one without training from scratch. Is that- Exactly. Yeah. And then that general model is,

00:23:41.640 --> 00:23:47.400
is really just trained to like learn representations of the data. It's not even really trained like with us,

00:23:47.400 --> 00:23:52.760
our particular like NLP task. It's just like trained to learn representations of data. And then

00:23:52.760 --> 00:23:55.960
with those representations that it learns, you can then say like, okay,

00:23:55.960 --> 00:24:01.400
you know, I'm going to train you towards this specific task with some labeled data in a

00:24:01.400 --> 00:24:07.160
supervised manner. And so there are some really popular open source, like base models, foundation

00:24:07.160 --> 00:24:13.560
models, like BERT is one, there's a bunch of others, but you can easily get like, like load up BERT basically,

00:24:13.560 --> 00:24:19.800
and fine tune it on your data with Hugging Face. So if you're trying to get a model, the model up and

00:24:19.800 --> 00:24:24.680
running quickly in like the NLP, like the text domain, you can do that pretty easily with Hugging

00:24:24.680 --> 00:24:25.240
Face. And- Okay.

00:24:25.240 --> 00:24:29.800
Yeah. So it's less like, if you want to like build your own neural network from scratch,

00:24:29.800 --> 00:24:35.400
like inputs to outputs, implement your own loss function, all that, you would do that in PyTorch.

00:24:35.400 --> 00:24:40.840
If you want to try to just like quickly fine tune BERT for a specific task that you're trying to solve,

00:24:40.840 --> 00:24:52.840
you could still go like the PyTorch out, but it would just be faster to go with Hugging Face. So they've seen a lot of adoption there. And then scikit-learn is kind of like the old school library that's been around forever with like-

00:24:52.840 --> 00:24:54.840
The OG, yeah.

00:24:54.840 --> 00:25:05.640
The OG, yeah. Like if you want to do stuff with like support vector machines or random forest or like KDR's neighbors, you know, this scikit-learn is probably still really popular in that for those different use cases.

00:25:05.640 --> 00:25:09.480
I do think that I hear scikit-learn being used quite a bit still.

00:25:09.480 --> 00:25:09.840
Yeah.

00:25:09.840 --> 00:25:17.240
Maybe in the research, the academic, if you go take a course on it, you know, probably there's a lot of stuff on this, I would guess.

00:25:17.240 --> 00:25:22.820
Yeah. Like there's a lot of times where, I mean, you don't really need to build a neural network. I mean, there's parts of our stack that are really important.

00:25:22.820 --> 00:25:41.860
Like basic machine learning, like statistical models. And if you can get away with it, it's a lot easier to train and you don't need as much data and it's easier to deploy. So like a lot of like recommendation type models or, and sometimes SVMs are just like good enough. SVMs, support vector machines are just good enough for, for a task that you might want to have.

00:25:41.860 --> 00:25:49.300
So for a lightweight Netflix recommendation or YouTube recommendation, not like the high end stuff that I'm sure they're actually doing.

00:25:49.300 --> 00:25:49.700
Yeah.

00:25:49.700 --> 00:25:50.400
Something like that.

00:25:50.400 --> 00:25:51.400
Yeah. Yeah, exactly.

00:25:51.400 --> 00:25:52.800
That kind of recommendation engine. Yeah.

00:25:52.800 --> 00:25:59.560
Something, yeah. Something basic. Yeah. Although I actually am kind of underwhelmed with like the Netflix and YouTube recommendations are very good.

00:25:59.560 --> 00:26:04.960
Netflix recommendations and like prime recommendations are kind of underwhelmed by. You would think that you watch.

00:26:04.960 --> 00:26:05.540
I agree.

00:26:05.540 --> 00:26:09.740
Yeah. Yeah. It's still so hard to find things to watch sometimes on those platforms.

00:26:09.740 --> 00:26:22.540
It is. And YouTube interestingly seems to have an end. So if you scroll down through YouTube, like 10 pages, it'll start showing you like, well, it seems like we're out of options. Here, we'll show you 10 from this one channel. And then we'll just kind of stop.

00:26:22.780 --> 00:26:30.900
I know you got a lot of videos. You could just keep recommending stuff. I'm pretty sure if you would keep recommending it. There's stuff down here. But yeah, I agree. It's interesting.

00:26:30.900 --> 00:26:58.960
I feel like it's gotten better too. Like my YouTube consumption has really picked up over the last year, I would say. The recommendation algorithms, and I don't know if it's just more content being created or maybe it's just like a personal thing for me. And there was some thing on Hacker News too about like YouTube comments that like one of the founders of Stripe posted are like generally very positive. And like there's really good comments on YouTube too. So they've definitely also come up with ways to classify comments as being high value or not.

00:26:58.960 --> 00:27:07.200
And then put those up top. And nowadays, those models are definitely used with something like for some big neural networks and transformer.

00:27:07.200 --> 00:27:07.640
Yeah.

00:27:07.640 --> 00:27:24.260
Because those neural networks, they're so much better at understanding context. And like SVMs, you have to still, for a lot of these classical machine learning approaches, like feed it, hand-labeled data. But the neural networks, yeah, they're really good for those language tasks now.

00:27:24.400 --> 00:27:37.320
Yeah, absolutely. Christopher out in the audience has a question. That's kind of interesting. Does it make sense to start with scikit-learn if, for example, you're trying to predict when a production machine is not out of tolerance yet is trending to be?

00:27:37.320 --> 00:27:38.740
Is that like, God...

00:27:38.740 --> 00:27:42.460
Like if you were like monitoring like a data center for maybe VMs, I'm guessing.

00:27:42.460 --> 00:27:50.540
Like your RAM or like memory is going high or some statistic is like predictive that this VM will probably go down.

00:27:50.540 --> 00:27:51.420
Failure is coming.

00:27:51.420 --> 00:27:52.600
Failure is coming. Yeah.

00:27:53.040 --> 00:28:00.780
And the question was, is it SBM or scikit-learn good to start with? Yeah, I would actually probably say that's where you want to go with something like scikit-learn.

00:28:00.780 --> 00:28:03.880
Because there's probably very clear-cut patterns.

00:28:03.880 --> 00:28:12.900
I would say if you're unsure of what the pattern is, then a neural network is good because a neural network can, in theory, like you're feeding it raw data and it's learning the pattern.

00:28:12.900 --> 00:28:23.260
But if you know what the pattern is, like, okay, like there's probably like these signals that if a human was just sitting there looking at it all day, would be able to tell this system is probably going to go down.

00:28:23.260 --> 00:28:31.640
Then you just can train an SVM or some type of classical machine learning model with scikit-learn to be able to do those predictions with pretty high accuracy.

00:28:31.640 --> 00:28:33.560
And then you've got a super lightweight model.

00:28:33.560 --> 00:28:41.220
You don't need much training data to train it because you're not trying to build something that's like super generalizable to like all systems or like all AWS instances.

00:28:41.220 --> 00:28:43.540
It's probably something unique to your system.

00:28:43.540 --> 00:28:45.920
But I would say that's kind of where the difference is.

00:28:45.920 --> 00:28:56.780
And then it's a lot easier too because if you're trying to build like a neural net, it's like, well, what type, how many layers, what, you know, kind of like optimization schedule, like learning rate.

00:28:56.780 --> 00:28:59.180
There's all these hyper parameters and things you have to figure out.

00:28:59.180 --> 00:29:02.620
You still have to do that too for classical machine learning to a degree.

00:29:02.620 --> 00:29:10.060
But if your problem is not that difficult, it's not as, you know, like fancy nowadays, but it gets the job done.

00:29:10.060 --> 00:29:10.320
Yeah.

00:29:10.320 --> 00:29:18.920
I suspect you could come up with some predictors and then like monitor them for in this model, whereas opposed to here's an image that is a breast scan.

00:29:18.920 --> 00:29:20.380
Does it have cancer or not?

00:29:20.380 --> 00:29:20.520
Right.

00:29:20.520 --> 00:29:21.140
Like exactly.

00:29:21.140 --> 00:29:26.860
We don't even really know what we're looking for, but there probably is a pattern that could be pulled out by a neural network.

00:29:26.860 --> 00:29:27.340
Exactly.

00:29:27.340 --> 00:29:27.660
Yeah.

00:29:27.660 --> 00:29:28.440
That's a great point.

00:29:28.440 --> 00:29:45.300
And, you know, like we're trying to build some predictive scaling for our API right now, because, you know, one of the problems with the challenges of a startup that's doing machine learning in production is, you know, we deploy like hundreds of GPUs and thousands of CPU cores into production every day at peak load.

00:29:45.300 --> 00:29:50.300
And then there's just huge costs that come from a scale.

00:29:50.300 --> 00:29:52.440
And then there's huge costs that come with that.

00:29:52.440 --> 00:30:01.160
And so we've done a ton of work around like auto scaling and trying to optimize models and production and things like that.

00:30:01.160 --> 00:30:04.060
And now we're trying to do some predictive scaling.

00:30:04.060 --> 00:30:08.300
And for that, for example, we'd probably do something super simple with like scikit-learn.

00:30:08.580 --> 00:30:10.420
We wouldn't do a neural net for that.

00:30:10.420 --> 00:30:10.600
Yeah.

00:30:10.600 --> 00:30:13.600
The scaling sounds like solving a basically a similar issue.

00:30:13.600 --> 00:30:14.000
Yeah.

00:30:14.000 --> 00:30:14.440
Yeah.

00:30:14.440 --> 00:30:15.740
As understanding failure, right?

00:30:15.740 --> 00:30:16.460
Yeah, exactly.

00:30:16.460 --> 00:30:17.160
Exactly.

00:30:17.160 --> 00:30:20.620
The lack of scaling sometimes is kind of the result is failure.

00:30:20.620 --> 00:30:23.820
So yeah, they're somewhat related together.

00:30:23.820 --> 00:30:24.180
Yeah.

00:30:24.180 --> 00:30:26.540
You talked about like running stuff in production.

00:30:26.540 --> 00:30:36.440
And there's obviously two aspects for machine learning companies and startups and teams and products that are very different than say the kind of stuff I do, right?

00:30:36.440 --> 00:30:38.260
Like I've got APIs that are running.

00:30:38.260 --> 00:30:39.360
We've got mobile apps.

00:30:39.360 --> 00:30:40.800
We've got people taking the courses.

00:30:40.800 --> 00:30:43.080
But all of that stuff, there is like one.

00:30:43.080 --> 00:30:45.000
It's always the same, right?

00:30:45.000 --> 00:30:48.660
We put stuff up and people will use it and consume it and so on.

00:30:48.720 --> 00:30:55.460
But for you all, you've got the training and almost the R&D side of things that you've got to worry about working on and scaling.

00:30:55.460 --> 00:30:55.860
Right.

00:30:55.860 --> 00:30:57.460
And then you've got the productionizing.

00:30:57.460 --> 00:31:02.300
So maybe tell us a little bit about how you, what do you guys use for-

00:31:02.300 --> 00:31:02.940
For both parts.

00:31:02.940 --> 00:31:03.360
For training.

00:31:03.360 --> 00:31:03.880
Yeah.

00:31:03.880 --> 00:31:05.620
Maybe start with the training side.

00:31:05.840 --> 00:31:13.680
Yeah, the training side, it's basically like impossible to use the big clouds for that because it would just be prohibitively expensive, at least for what we do.

00:31:13.680 --> 00:31:19.020
So we train like these huge neural nets for speech recognition and different NLP tasks.

00:31:19.020 --> 00:31:24.680
And, you know, we're training them across like 48, 64 GPUs, like really powerful GPUs.

00:31:24.680 --> 00:31:28.120
I've got the GeForce 3090, which is a beast up here.

00:31:28.120 --> 00:31:29.960
Do you know what kind you're using?

00:31:30.120 --> 00:31:34.180
Yeah, so we use a lot of V100s, like A100s.

00:31:34.180 --> 00:31:40.700
And we rent, basically what we do is we rent dedicated machines from provider.

00:31:40.700 --> 00:31:44.420
And each machine, we're able to like pick the specs that we want.

00:31:44.420 --> 00:31:48.840
Like how many GPUs, what cards, how much RAM, what kind of CPU we want on there.

00:31:48.840 --> 00:31:51.360
So we're able to pick the specs that we want.

00:31:51.360 --> 00:32:06.420
And we found that that's been the best way to do it because the big clouds, yeah, if you're running like a dozen, dozens of GPU, like of the most expensive types of GPUs for like weeks on end, you could do that if you had like one training run you wanted to do.

00:32:06.420 --> 00:32:09.280
But a lot of times you have to train a model halfway through.

00:32:09.280 --> 00:32:10.540
It doesn't work well.

00:32:10.540 --> 00:32:14.600
You have to restart or finish this training and the results are not that good.

00:32:14.600 --> 00:32:15.260
And you learn something.

00:32:15.260 --> 00:32:16.620
So you have to go back and start over.

00:32:16.960 --> 00:32:19.940
And now what we're doing is buying a bunch of our own compute.

00:32:19.940 --> 00:32:26.720
Like my dream is to have some closet somewhere with just like, you know, tons of GPUs and like our own like mini data center for the R&D.

00:32:26.720 --> 00:32:31.120
Because if things go down, you know, like when you're training a model, you checkpoint it as you go.

00:32:31.120 --> 00:32:35.680
So if your program crashes or your server crashes, like you can resume training.

00:32:35.680 --> 00:32:40.540
Whereas like for production workloads, we use AWS for that because things can't go down.

00:32:40.540 --> 00:32:44.880
And I don't think we'd want to take on our own competency of like hosting our own production infrastructure.

00:32:45.340 --> 00:32:53.500
But for the R&D stuff, you know, we are looking into just buying a ton versus renting because it'd be a lot more cost efficient.

00:32:53.500 --> 00:32:58.760
And you can, instead of basically like paying each year for the same compute, you just like buy it once.

00:32:58.760 --> 00:33:03.580
And then you just pay for the electricity and server hosting costs and maintenance costs that come with that.

00:33:03.580 --> 00:33:04.040
Yeah.

00:33:04.040 --> 00:33:09.120
Maybe find a big office building and offer to heat it for free in the winter by just running on the inside.

00:33:09.120 --> 00:33:11.920
There's this like, you know, you can run like NVIDIA SMI.

00:33:11.920 --> 00:33:13.640
I don't even play around with GPUs at all.

00:33:13.640 --> 00:33:16.040
But like, you can see what the temperature is of the GPU.

00:33:16.040 --> 00:33:24.020
And like sometimes, you know, if I'm, I remember a while ago when I was training some of these models, I would just like look at what the temperature is during training.

00:33:24.020 --> 00:33:25.180
And yeah, they get so hot.

00:33:25.180 --> 00:33:30.720
And these data centers have to have all this, all these special cooling infrastructure to keep the machines down.

00:33:30.720 --> 00:33:32.580
It's pretty environmentally unfriendly.

00:33:32.600 --> 00:33:32.820
Yeah.

00:33:32.820 --> 00:33:44.520
To the extent that some of them, yeah, to the extent that people are creating underwater data center, like nodes and putting them down there and just letting the ocean be the heat sink.

00:33:44.520 --> 00:33:45.040
Yeah.

00:33:45.040 --> 00:33:45.540
That's crazy.

00:33:45.540 --> 00:33:50.360
You can buy some land and like, you know, in Arctica and put our stuff there.

00:33:50.360 --> 00:33:53.560
That's where like the GitHub, like the Arctic code thing.

00:33:53.560 --> 00:33:54.460
I forget what it's called.

00:33:54.460 --> 00:33:54.840
Yeah.

00:33:54.840 --> 00:33:55.440
Yeah.

00:33:55.440 --> 00:33:58.000
The Arctic code vault.

00:33:58.000 --> 00:33:58.340
Yeah.

00:33:58.340 --> 00:33:58.680
Yes.

00:33:58.680 --> 00:33:58.940
Yeah.

00:33:58.940 --> 00:34:00.760
So we could do something like that for our GPUs.

00:34:00.760 --> 00:34:02.240
When we get bigger, that's, that's the dream.

00:34:02.340 --> 00:34:03.200
That's where it might nerd out.

00:34:03.200 --> 00:34:03.600
There you go.

00:34:03.600 --> 00:34:13.020
So, yeah, so we train, I think we have like, I think somewhere like maybe like 200 like GPUs that we use just for R&D and training.

00:34:13.020 --> 00:34:18.200
And we're getting a lot more because you don't want to be, a lot of times there's like scheduling bottlenecks.

00:34:18.200 --> 00:34:22.920
So two researchers want to run a model and need a bunch of compute to be able to do that.

00:34:22.920 --> 00:34:23.840
And they're both good ideas.

00:34:23.840 --> 00:34:30.300
You don't want to have to like wait four weeks for someone to run their, their model because compute is taken.

00:34:30.500 --> 00:34:34.740
So we're trying to unblock those scheduling conflicts by just getting more compute.

00:34:34.740 --> 00:34:34.980
Yeah.

00:34:34.980 --> 00:34:41.800
And for the production side, yeah, we deploy everything in AWS right now and onto like smaller GPUs.

00:34:41.800 --> 00:34:44.420
Because a lot of our models do inference on GPU still.

00:34:44.420 --> 00:34:47.180
Some of our models do inference on CPU.

00:34:47.180 --> 00:34:47.940
Oh, interesting.

00:34:47.940 --> 00:34:48.180
Yeah.

00:34:48.180 --> 00:34:51.520
To, to evaluate the stuff, it still uses GPUs.

00:34:51.520 --> 00:34:51.780
Yeah.

00:34:51.780 --> 00:34:52.040
Correct.

00:34:52.040 --> 00:34:52.760
Even after the models are created.

00:34:52.760 --> 00:34:53.140
Correct.

00:34:53.240 --> 00:34:53.420
Yeah.

00:34:53.420 --> 00:34:58.080
I mean, there's, we could run it on CPU, but it's just not as parallelizable as running

00:34:58.080 --> 00:34:58.800
it on GPUs.

00:34:58.800 --> 00:35:04.540
There's a lot of work that we could probably do to get it really efficient so that, you know,

00:35:04.540 --> 00:35:06.880
we're running it on like as few CPU cores as possible.

00:35:06.880 --> 00:35:11.520
But one of the problems is like almost like every like three to four months, we're like throwing

00:35:11.520 --> 00:35:15.120
out the current neural network architecture and using a different one that is giving us better

00:35:15.120 --> 00:35:15.520
results.

00:35:15.520 --> 00:35:19.560
Like sometimes we'll make the model bigger or there'll be a small tweak in the model

00:35:19.560 --> 00:35:21.500
architecture that yields better results.

00:35:21.500 --> 00:35:25.800
But a lot of times it's like, okay, we've kind of iterated within this architecture as

00:35:25.800 --> 00:35:26.340
much as we can.

00:35:26.340 --> 00:35:29.580
And now to get the next accuracy bump, we have to go to a new architecture.

00:35:29.580 --> 00:35:31.440
We're undergoing that right now.

00:35:31.440 --> 00:35:36.100
We've released our, one of our like newer speech recognition models we released, I think

00:35:36.100 --> 00:35:40.480
like three months ago and the results are really good, but now we have one that is looking

00:35:40.480 --> 00:35:43.140
a lot better and it'd be like a completely different architecture.

00:35:43.140 --> 00:35:48.520
And so it's just that trade off of, do you spend a bunch of time optimizing the current

00:35:48.520 --> 00:35:54.020
model that you have and trying to like prune the neural network and do all these optimizations

00:35:54.020 --> 00:35:55.520
to get it really small?

00:35:55.520 --> 00:36:00.880
Or do you just spend that research effort and that energy focused on finding the next accuracy

00:36:00.880 --> 00:36:01.280
game?

00:36:01.280 --> 00:36:06.020
And because we're trying to win customers and grow our revenue, it's just, all right, let's

00:36:06.020 --> 00:36:07.260
just focus on the next model.

00:36:07.260 --> 00:36:11.920
And when we have a big enough team or when we can focus on it, we'll work on making the

00:36:11.920 --> 00:36:16.520
models smaller and more compute efficient and less costly to run.

00:36:16.520 --> 00:36:17.620
But right now, yeah.

00:36:17.620 --> 00:36:21.300
Like our speech recognition model that does inference on a GPU.

00:36:21.300 --> 00:36:26.700
There's a couple of our like NLP related models, like our content moderation model that does

00:36:26.700 --> 00:36:28.000
inference on a GPU.

00:36:28.000 --> 00:36:31.840
And then there's like our automatic punctuation and casing restoration model.

00:36:31.840 --> 00:36:35.280
Like that runs on a CPU because that's not as compute intense.

00:36:35.280 --> 00:36:36.780
And so it really varies.

00:36:37.080 --> 00:36:37.180
Yeah.

00:36:37.180 --> 00:36:37.480
Yeah.

00:36:37.480 --> 00:36:37.520
Yeah.

00:36:37.520 --> 00:36:41.640
As you say, it's pretty interesting to think about how you're optimizing the software

00:36:41.640 --> 00:36:44.980
stack and the algorithms and the libraries and whatnot.

00:36:44.980 --> 00:36:51.160
You know, when you're not doing something that's changing so quickly, you know, if it's working,

00:36:51.160 --> 00:36:53.340
you can kind of just leave it alone.

00:36:53.340 --> 00:36:53.640
Right.

00:36:53.640 --> 00:36:54.300
Like, right.

00:36:54.340 --> 00:36:58.060
I've got some APIs, I think they're built either in Pyramid or Flask.

00:36:58.060 --> 00:36:58.860
Sure.

00:36:58.860 --> 00:37:02.180
It'd be nicer to rebuild them in FastAPI, but they're working fine.

00:37:02.180 --> 00:37:04.220
I'm just like, I have no reason to touch them.

00:37:04.220 --> 00:37:04.440
Right.

00:37:04.440 --> 00:37:08.000
So there's not a, like a huge step jump I'm going to take.

00:37:08.000 --> 00:37:11.080
They're not under extreme load or anything.

00:37:11.080 --> 00:37:11.380
Right.

00:37:13.940 --> 00:37:17.040
This portion of Talk Python To Me is brought to you by Sentry.

00:37:17.040 --> 00:37:19.920
How would you like to remove a little stress from your life?

00:37:19.920 --> 00:37:25.600
Do you worry that users may be encountering errors, slowdowns, or crashes with your app right

00:37:25.600 --> 00:37:25.920
now?

00:37:25.920 --> 00:37:28.960
Would you even know it until they sent you that support email?

00:37:28.960 --> 00:37:33.720
How much better would it be to have the error or performance details immediately sent to you,

00:37:33.720 --> 00:37:39.040
including the call stack and values of local variables and the active user recorded in the

00:37:39.040 --> 00:37:39.360
report?

00:37:39.360 --> 00:37:42.780
With Sentry, this is not only possible, it's simple.

00:37:42.780 --> 00:37:46.340
In fact, we use Sentry on all the Talk Python web properties.

00:37:46.340 --> 00:37:51.880
We've actually fixed a bug triggered by a user and had the upgrade ready to roll out as we

00:37:51.880 --> 00:37:52.900
got the support email.

00:37:52.900 --> 00:37:54.880
That was a great email to write back.

00:37:54.880 --> 00:37:58.260
Hey, we already saw your error and have already rolled out the fix.

00:37:58.260 --> 00:37:59.680
Imagine their surprise.

00:37:59.680 --> 00:38:01.900
Surprise and delight your users.

00:38:01.900 --> 00:38:05.960
Create your Sentry account at talkpython.fm/sentry.

00:38:05.960 --> 00:38:11.640
And if you sign up with the code talkpython, all one word, it's good for two free months of

00:38:11.640 --> 00:38:16.900
Sentry's business plan, which will give you up to 20 times as many monthly events as well

00:38:16.900 --> 00:38:17.640
as other features.

00:38:17.640 --> 00:38:22.040
Create better software, delight your users, and support the podcast.

00:38:22.040 --> 00:38:27.020
Visit talkpython.fm/sentry and use the coupon code talkpython.

00:38:29.420 --> 00:38:36.560
But in your world, there's so much innovation happening around the models that you do have

00:38:36.560 --> 00:38:37.260
to think about that.

00:38:37.260 --> 00:38:38.920
So how do you work that tradeoff?

00:38:38.920 --> 00:38:42.900
How do you like, well, could we get more out of what we've got or should we abandon it and

00:38:42.900 --> 00:38:43.440
start over?

00:38:43.440 --> 00:38:43.700
Right?

00:38:43.760 --> 00:38:47.780
Because it is nice to have a very polished and well-known thing as well.

00:38:47.780 --> 00:38:48.140
Definitely.

00:38:48.140 --> 00:38:52.280
And every time you throw out our architecture to implement a new architecture, you've now

00:38:52.280 --> 00:38:54.280
got to figure out how to run that architecture at scale.

00:38:54.280 --> 00:38:59.380
And you don't want to have any hiccups for your current customers or users of your API, which

00:38:59.380 --> 00:39:03.400
sometimes happens because these models are so big that you can't just write this model

00:39:03.400 --> 00:39:06.340
that service that sits on a GPU and does everything.

00:39:06.340 --> 00:39:10.740
You have to break it up into a bunch of component parts so that you can run it efficiently at

00:39:10.740 --> 00:39:11.020
scale.

00:39:11.020 --> 00:39:17.020
So there's like eight, nine microservices for a single model because you break out all

00:39:17.020 --> 00:39:20.220
these different parts and try to get it running really efficiently in parallel.

00:39:20.220 --> 00:39:25.180
But it does beg the question of how do you build good CICD workflows and good DevOps workflows

00:39:25.180 --> 00:39:26.960
to get models into production quickly?

00:39:26.960 --> 00:39:30.480
And this is something that we're working on right now and trying to solve.

00:39:31.040 --> 00:39:34.740
Because a lot of times we have better models and we sit on them for like two, three weeks

00:39:34.740 --> 00:39:39.480
because to get them into staging, we have to do low testing, see does anything with scaling

00:39:39.480 --> 00:39:42.280
have to change because the model profile is different?

00:39:42.280 --> 00:39:46.940
Are there any weird edge cases that we didn't check or see during testing?

00:39:46.940 --> 00:39:53.000
So it slows down the rate of development because you have, it's hard to do CICD.

00:39:53.000 --> 00:39:57.280
It's not like you just, okay, run these tests, the code works, go.

00:39:57.280 --> 00:39:59.780
There's like compute profile changes that happen.

00:39:59.900 --> 00:40:02.600
So maybe you need a different instance type or you need to...

00:40:02.600 --> 00:40:03.160
Right.

00:40:03.160 --> 00:40:05.220
Uses less CPU, but way more RAM.

00:40:05.220 --> 00:40:07.360
So if you actually deploy, it's going to crash or something.

00:40:07.360 --> 00:40:07.640
Okay.

00:40:07.640 --> 00:40:08.000
Exactly.

00:40:08.000 --> 00:40:12.340
And then doing that at scale, you have to profile out and do low testing.

00:40:12.340 --> 00:40:15.320
And so really, we're trying to figure out how to get these models into production faster.

00:40:15.320 --> 00:40:21.000
And I think the whole ML ops world is so in its infancy around things like that.

00:40:21.000 --> 00:40:23.120
And it's a lot of work.

00:40:23.120 --> 00:40:23.400
Yeah.

00:40:23.400 --> 00:40:24.180
It's a lot of work.

00:40:24.340 --> 00:40:28.700
So for us, the trade-off though is always like, you know, our customers and developers,

00:40:28.700 --> 00:40:31.860
they just want better results and always more accurate results.

00:40:31.860 --> 00:40:36.680
And so we just always are working on pushing our models, making them more accurate.

00:40:36.680 --> 00:40:39.260
If we can iterate within a current architecture, great.

00:40:39.260 --> 00:40:43.580
Like sometimes you can just make the model bigger or make a small change and then you get

00:40:43.580 --> 00:40:44.880
a lot of accuracy improvements.

00:40:44.880 --> 00:40:48.400
And it's just like what we call it a drop-in update where no code changes.

00:40:48.400 --> 00:40:51.620
It's just literally like the model that you're loading is just different.

00:40:51.620 --> 00:40:53.160
And then it's just more accurate.

00:40:53.160 --> 00:40:53.440
Right.

00:40:53.440 --> 00:40:53.960
That's easy.

00:40:53.960 --> 00:40:54.180
Yeah.

00:40:54.180 --> 00:40:54.680
That's the dream.

00:40:54.680 --> 00:40:59.360
You know, it's just a drop-in, but that's maybe like 30% of updates.

00:40:59.360 --> 00:41:03.880
Like the other 70% are, okay, you've got a new architecture or it's got a pretty different

00:41:03.880 --> 00:41:04.720
compute profile.

00:41:04.720 --> 00:41:08.820
So it uses a lot more RAM or it's a lot slower to load in the beginning.

00:41:08.820 --> 00:41:14.660
So we need to scale earlier because instances come online later and become healthy later.

00:41:14.660 --> 00:41:17.020
So there's all these like things you have to think about.

00:41:17.020 --> 00:41:17.280
Yeah.

00:41:17.280 --> 00:41:22.020
The whole DevOps side of this sounds way more interesting and involved than I first thought.

00:41:22.020 --> 00:41:22.040
Yeah.

00:41:22.040 --> 00:41:23.040
It's painful too.

00:41:23.040 --> 00:41:27.900
I mean, we're like, I can't explain how many like graphs we have in Datadog, just like monitoring

00:41:27.900 --> 00:41:28.900
things all day.

00:41:28.900 --> 00:41:31.740
Luckily, I don't have to work on that anymore.

00:41:31.740 --> 00:41:33.320
That was very stressful.

00:41:33.320 --> 00:41:35.040
And I was like owning the infrastructure.

00:41:35.040 --> 00:41:37.220
Now we have people that are better at it than me.

00:41:37.220 --> 00:41:41.820
We had like two DevOps people start on Monday, but yeah, like DevOps is a huge, huge piece

00:41:41.820 --> 00:41:42.140
of this.

00:41:42.140 --> 00:41:42.380
Yeah.

00:41:42.380 --> 00:41:43.000
That's quite interesting.

00:41:43.000 --> 00:41:43.220
Yeah.

00:41:43.220 --> 00:41:45.040
I do want to just circle back to one real quick thing.

00:41:45.040 --> 00:41:48.660
You talked about buying your own GPUs for training and people might out there be thinking

00:41:48.660 --> 00:41:54.900
like, who would want to go and get their own hardware in the day of AWS, Node, whatever,

00:41:54.900 --> 00:41:55.280
right?

00:41:55.280 --> 00:41:56.400
Like it just seems crazy.

00:41:56.400 --> 00:41:58.040
But there's certainly circumstances.

00:41:58.040 --> 00:42:00.980
Like here's an example that I recently thought about.

00:42:00.980 --> 00:42:03.720
So there's a place called Mac Stadium where you can get Macs in the cloud.

00:42:03.720 --> 00:42:04.740
Hey, how cool, right?

00:42:04.740 --> 00:42:07.480
So maybe you want to have like something you could do with extra things.

00:42:07.480 --> 00:42:08.920
And well, what does it cost?

00:42:08.920 --> 00:42:14.020
Well, for a Mac mini M1, it's $132 a month.

00:42:14.020 --> 00:42:15.500
You think that's, is that high or low?

00:42:15.500 --> 00:42:19.920
Well, the whole device, if you were to buy it costs $700.

00:42:19.920 --> 00:42:20.680
Yeah.

00:42:20.680 --> 00:42:26.060
You know, that's, and I suspect that even though the GPUs are expensive, there's probably something

00:42:26.060 --> 00:42:29.900
where like, if you really utilize it extensively, it actually makes.

00:42:29.900 --> 00:42:30.420
To buy it.

00:42:30.420 --> 00:42:33.320
It stops making sense in ways that people might not expect.

00:42:33.320 --> 00:42:33.580
Yeah.

00:42:33.580 --> 00:42:35.180
That it's a buy it, you mean, right?

00:42:35.180 --> 00:42:36.420
Like it stops making sense to rent it.

00:42:36.420 --> 00:42:36.540
Yeah.

00:42:36.540 --> 00:42:37.520
That's what we're facing.

00:42:37.520 --> 00:42:38.780
It stops making sense to rent it in the cloud.

00:42:38.780 --> 00:42:38.960
Yeah.

00:42:38.960 --> 00:42:39.260
Yeah.

00:42:39.260 --> 00:42:45.760
I mean, we spent a crazy amount of money renting GPUs in the cloud and it's like, okay, you

00:42:45.760 --> 00:42:49.620
know, if we had a bunch of money to make a, you know, CapEx purchase, right?

00:42:49.620 --> 00:42:53.360
Like just shell out a bunch of money to buy a bunch of hardware up front, it'd be so much

00:42:53.360 --> 00:42:54.240
better in the long run.

00:42:54.240 --> 00:42:59.420
Cause it is similar to the example you made about like, if you don't have a lot of cash, then you're

00:42:59.420 --> 00:43:01.300
only going to use a Mac for a couple months.

00:43:01.300 --> 00:43:01.560
Right.

00:43:01.560 --> 00:43:02.480
You need it for two weeks.

00:43:02.480 --> 00:43:03.820
Then it doesn't make sense to buy it.

00:43:03.820 --> 00:43:04.020
Great.

00:43:04.020 --> 00:43:05.780
You just, you pay the a hundred dollars and you're good.

00:43:05.780 --> 00:43:06.080
Right.

00:43:06.080 --> 00:43:06.360
Right.

00:43:06.360 --> 00:43:11.300
Or if you don't have like 2k and you know, then, then you just rent in and it's like,

00:43:11.300 --> 00:43:14.620
you know, if you don't have the money to buy a house, you rent an apartment, right?

00:43:14.620 --> 00:43:15.700
Like things like that.

00:43:15.760 --> 00:43:17.320
So there are definitely benefits.

00:43:17.320 --> 00:43:22.240
And I think for a lot of, I think for most models, you don't need crazy compute.

00:43:22.240 --> 00:43:28.020
Like you could get away with, like, you could buy a desktop device that has like two GPUs

00:43:28.020 --> 00:43:32.060
or you could rent a dedicated machine or still do it on AWS if you're using like one or two

00:43:32.060 --> 00:43:32.600
GPUs.

00:43:32.600 --> 00:43:33.880
And it wouldn't be insane.

00:43:34.400 --> 00:43:37.160
So if you're just starting out, all those options are fine.

00:43:37.160 --> 00:43:42.860
But if you're trying to do like big models and, or train a bunch of parallel, you need

00:43:42.860 --> 00:43:43.540
more compute.

00:43:43.540 --> 00:43:47.280
And definitely doesn't make sense to use the big clouds for that.

00:43:47.400 --> 00:43:51.820
There's a bunch of dedicated providers that you can rent, like dedicated machines from

00:43:51.820 --> 00:43:55.000
and just pay a monthly fee regardless of how much you use it.

00:43:55.000 --> 00:43:59.520
And, it's a lot more, it's a lot more efficient for like companies to do that.

00:43:59.520 --> 00:44:06.660
Give me your thoughts on sort of CapEx versus OpEx for ML startups rather than, I don't know,

00:44:06.660 --> 00:44:11.060
it's some other SaaS service that doesn't have such computational stuff, you know, being

00:44:11.060 --> 00:44:16.040
CapEx being, you got to buy a whole bunch of machines and GPUs and stuff versus OpEx.

00:44:16.040 --> 00:44:18.840
Like, well, it's going to cost this much to rent in the cloud.

00:44:18.840 --> 00:44:24.060
Like I feel like things are more possible because you can get the stuff in the cloud,

00:44:24.060 --> 00:44:29.360
prove an idea and then get investors without going, well, you know, let's go to friends

00:44:29.360 --> 00:44:31.460
and family and get 250,000 for GPUs.

00:44:31.460 --> 00:44:32.800
And if it doesn't work, we'll just do Bitcoin.

00:44:32.800 --> 00:44:33.120
Yeah.

00:44:33.120 --> 00:44:34.100
Yeah, yeah, yeah.

00:44:34.100 --> 00:44:34.720
Definitely.

00:44:34.720 --> 00:44:36.340
I mean, we started in the cloud, right?

00:44:36.340 --> 00:44:43.500
So like first models we trained were K80s on K80s and AWS took like a month to train.

00:44:43.500 --> 00:44:43.840
Wow.

00:44:43.840 --> 00:44:44.560
Yeah, it was terrible.

00:44:44.720 --> 00:44:49.700
So we started in the cloud and then now that we're fortunate to have like more investment

00:44:49.700 --> 00:44:51.800
in the company, we can make these CapEx purchases.

00:44:51.800 --> 00:44:56.880
But yeah, I mean, the operating expenses of running an ML startup are also like crazy, like

00:44:56.880 --> 00:45:02.980
payroll and GP and payroll and like AWS are our biggest expenses because you run so much

00:45:02.980 --> 00:45:04.700
compute and it's super expensive.

00:45:04.700 --> 00:45:09.900
And what I talk about and what we talk about is like, there's nothing fundamental about what

00:45:09.900 --> 00:45:11.780
we're doing that makes that the case.

00:45:11.780 --> 00:45:16.720
It's just goes back to that point of like, do you spend a couple months optimizing your models,

00:45:16.720 --> 00:45:18.660
bringing compute costs down?

00:45:18.660 --> 00:45:23.680
Or do you just focus on the new architecture and kind of pay your way to get to the future?

00:45:23.680 --> 00:45:25.960
Like this growth versus, yeah.

00:45:26.180 --> 00:45:28.060
And then we're like a venture-backed company.

00:45:28.060 --> 00:45:31.560
So like there's expectations around our growth and, you know, all that.

00:45:31.560 --> 00:45:36.440
So we just focus on like, okay, let's just get to the next milestone and not focus too much

00:45:36.440 --> 00:45:40.500
on like bringing those costs down because there's the opportunity cost of doing that.

00:45:40.500 --> 00:45:42.260
But eventually we'll have to.

00:45:42.260 --> 00:45:42.760
Yeah.

00:45:43.000 --> 00:45:48.720
It's a little bit of the ML equivalent of sort of the growth.

00:45:48.720 --> 00:45:51.360
You can lose money to just gather users.

00:45:51.360 --> 00:45:52.100
Yeah.

00:45:52.100 --> 00:45:55.720
But this is the sort of gain capabilities, right?

00:45:55.720 --> 00:45:55.940
It is.

00:45:55.940 --> 00:45:56.960
Yeah, it is 100%.

00:45:56.960 --> 00:46:00.400
And then you'll figure out how to do it efficiently once you kind of find your way.

00:46:00.400 --> 00:46:00.940
Okay.

00:46:00.940 --> 00:46:02.900
And I'll give you like a tangible example.

00:46:02.900 --> 00:46:08.180
I mean, like we've been adding a lot of customers and developers on the API and there's always like

00:46:08.180 --> 00:46:09.620
new scaling problems that come up.

00:46:09.620 --> 00:46:13.020
And sometimes we're just like, look, let's just scale the whole system up.

00:46:13.020 --> 00:46:14.120
It's going to be inefficient.

00:46:14.120 --> 00:46:18.940
There's going to be waste, but like let's scale it up and then we'll like fine tune the auto

00:46:18.940 --> 00:46:26.040
scaling to bring it down over time versus like having to step into like a more perfect auto

00:46:26.040 --> 00:46:30.260
scaling scenario that wouldn't cost as much, but there'd be bumps along the way.

00:46:30.260 --> 00:46:35.940
And so we just like scaled everything up recently to buy us time to go work on figuring out how

00:46:35.940 --> 00:46:37.840
to improve some of these like auto scaling.

00:46:37.840 --> 00:46:38.580
Yeah.

00:46:38.580 --> 00:46:38.860
Yeah.

00:46:38.860 --> 00:46:42.440
You could spend two weeks trying to figure out the right way to go to production or you

00:46:42.440 --> 00:46:44.100
could spend just more money.

00:46:44.100 --> 00:46:45.020
Exactly.

00:46:45.020 --> 00:46:51.780
And then, cause you, you might not be sure with the, like the multiple month life cycle,

00:46:51.780 --> 00:46:52.880
some of these things.

00:46:52.880 --> 00:46:53.160
Right.

00:46:53.160 --> 00:46:55.080
Is this actually going to be the way we want to stick with?

00:46:55.080 --> 00:46:57.540
So let's not spend two weeks optimizing it first.

00:46:57.540 --> 00:46:57.840
Right.

00:46:57.840 --> 00:46:58.380
Very interesting.

00:46:58.380 --> 00:47:01.320
And I mean, like, look, not every company can make that decision.

00:47:01.320 --> 00:47:05.420
Like if you are bootstrapped or you're trying to get off the ground, which like a lot of companies

00:47:05.420 --> 00:47:09.280
are, you do have to make those, you can't just pay your way to the future.

00:47:09.280 --> 00:47:09.560
Yeah.

00:47:09.560 --> 00:47:14.300
And I'm a big fan of bootstrapped companies and finding your way.

00:47:14.300 --> 00:47:17.580
I don't think that necessarily just, you know, set a ton of money on fire.

00:47:17.580 --> 00:47:18.320
Right.

00:47:18.440 --> 00:47:19.640
It's the only way forward.

00:47:19.640 --> 00:47:24.780
But if you have backers already, then they would prefer you to move faster.

00:47:24.780 --> 00:47:25.260
I suspect.

00:47:25.260 --> 00:47:25.920
Correct.

00:47:25.920 --> 00:47:26.320
Yeah.

00:47:26.320 --> 00:47:26.780
Correct.

00:47:26.780 --> 00:47:27.100
Correct.

00:47:27.100 --> 00:47:31.700
Like I always was self-conscious about our, you know, operating costs as an ML company,

00:47:31.700 --> 00:47:36.120
cause they're high compared to other SaaS companies where you don't have heavy compute,

00:47:36.120 --> 00:47:41.420
but you know, the investors we work with, they get that like, okay, this isn't, there's

00:47:41.420 --> 00:47:45.640
nothing like that fundamental about this that requires those costs to be high.

00:47:45.720 --> 00:47:49.920
You just have to spend time on bringing them down and it's, there's like a clear path.

00:47:49.920 --> 00:47:54.400
It's not like Uber where it's like the path to bring costs down or like self-driving cars

00:47:54.400 --> 00:47:56.220
because it's expensive to employ humans.

00:47:56.220 --> 00:47:59.200
That's like, you know, so far down the road.

00:47:59.200 --> 00:47:59.480
Yeah.

00:47:59.480 --> 00:48:03.960
But for us, it's like, okay, we need to just spend three months making these models more

00:48:03.960 --> 00:48:08.260
efficient and they'll run a lot cheaper, but it's that trade-off.

00:48:08.260 --> 00:48:10.320
But I love bootstrap companies too.

00:48:10.420 --> 00:48:11.980
I mean, it's just a different way to do it.

00:48:11.980 --> 00:48:16.480
Something special about like, you're actually making a profit and you're actually, you have

00:48:16.480 --> 00:48:18.720
customers and people paying for stuff.

00:48:18.720 --> 00:48:19.180
Yeah.

00:48:19.180 --> 00:48:19.900
Yeah.

00:48:19.900 --> 00:48:20.180
Yeah.

00:48:20.180 --> 00:48:21.600
And the, and the freedom for sure.

00:48:21.600 --> 00:48:21.860
Yeah.

00:48:21.860 --> 00:48:25.220
So you probably saw me messing around with the screen here to pull up this Raspberry Pi

00:48:25.220 --> 00:48:25.520
thing.

00:48:25.520 --> 00:48:29.940
There's a question out in the audience says, could you do this kind of stuff on a Raspberry

00:48:29.940 --> 00:48:30.380
Pi?

00:48:30.380 --> 00:48:33.500
And like a standard Raspberry Pi, I suspect absolutely no.

00:48:33.500 --> 00:48:34.040
Yeah.

00:48:34.040 --> 00:48:37.080
Have you ever seen that there are water-cooled Raspberry Pi clusters?

00:48:37.080 --> 00:48:37.540
Whoa.

00:48:37.660 --> 00:48:39.000
I have not seen that.

00:48:39.000 --> 00:48:40.080
That is crazy.

00:48:40.080 --> 00:48:40.980
Is that insane?

00:48:40.980 --> 00:48:41.740
That's insane.

00:48:41.740 --> 00:48:44.460
So what kind of computer are they getting on that?

00:48:44.460 --> 00:48:47.540
It's pretty comparable to a MacBook Pro on this.

00:48:47.540 --> 00:48:48.000
That's crazy.

00:48:48.000 --> 00:48:51.320
They've got what, eight water-cooled Raspberry Pis in a cluster.

00:48:51.320 --> 00:48:53.860
And it's really an amazing device.

00:48:53.860 --> 00:49:00.780
But if you look back at a, you know, you sort of consider it like a single PC with a, you

00:49:00.780 --> 00:49:04.740
know, a basic Nvidia card or a MacBook Pro or something like that.

00:49:04.800 --> 00:49:06.980
Like that's still pretty far from what you guys need.

00:49:06.980 --> 00:49:09.680
Like how many GPUs did you say you were using to train your models?

00:49:09.680 --> 00:49:12.620
It's like 64 for the bigger ones.

00:49:12.620 --> 00:49:12.900
Yeah.

00:49:12.900 --> 00:49:13.780
In parallel.

00:49:13.780 --> 00:49:14.380
Yeah.

00:49:14.380 --> 00:49:14.780
Yeah.

00:49:14.780 --> 00:49:16.680
These are not small GPUs.

00:49:16.680 --> 00:49:21.580
So I suspect I'm going to maybe throw it out there for you and say probably no, maybe

00:49:21.580 --> 00:49:23.840
for the scikit learn type stuff, but not for what you're doing.

00:49:23.840 --> 00:49:25.540
Not the TensorFlow PyTorch.

00:49:25.540 --> 00:49:25.800
Yeah.

00:49:25.800 --> 00:49:30.020
Not for, not for training, but you could do inference on a Raspberry Pi.

00:49:30.220 --> 00:49:34.500
Like you could squeeze a model down super tiny, like what they do to get some models

00:49:34.500 --> 00:49:35.240
onto your phones.

00:49:35.240 --> 00:49:37.900
And you're on that on a Raspberry Pi.

00:49:37.900 --> 00:49:39.260
You get the models small enough.

00:49:39.260 --> 00:49:41.560
The accuracy might not be great, but like you could do it.

00:49:41.560 --> 00:49:41.760
Yeah.

00:49:41.760 --> 00:49:43.640
Oh, there's a lot of stuff happening around the edge.

00:49:43.640 --> 00:49:45.300
Like I think a lot of that Siri.

00:49:45.300 --> 00:49:45.620
Yeah.

00:49:45.620 --> 00:49:48.920
The edge compute, the sort of ML on device type stuff.

00:49:48.920 --> 00:49:52.020
Like a lot of the speech recognition on your phone now happens on device.

00:49:52.020 --> 00:49:52.440
Yeah.

00:49:52.600 --> 00:49:52.880
Yeah.

00:49:52.880 --> 00:49:53.720
And not in the cloud.

00:49:53.720 --> 00:49:53.880
Yeah.

00:49:53.880 --> 00:50:00.720
Sort of related to this, like the new M1 chips and even the chips in the Apple phones before

00:50:00.720 --> 00:50:04.200
then come with like neural engines built in, like multi-core neural engines.

00:50:04.200 --> 00:50:04.440
Right.

00:50:04.440 --> 00:50:09.660
Interesting for edge stuff again, but not really going to, not really going to let you do like

00:50:09.660 --> 00:50:11.400
the training and stuff like that.

00:50:11.400 --> 00:50:11.540
Right.

00:50:11.540 --> 00:50:16.180
I haven't done much iOS development, but I know there's like SDKs now to kind of like get your

00:50:16.180 --> 00:50:20.640
neural networks like on device and make use of these, like the hardware on the phone.

00:50:20.640 --> 00:50:24.380
And definitely if you're trying to deploy your stuff on the edge, there's a lot more

00:50:24.380 --> 00:50:26.180
resources available to you.

00:50:26.180 --> 00:50:26.380
Yeah.

00:50:26.380 --> 00:50:26.820
Yeah.

00:50:26.820 --> 00:50:31.280
And it's a really good experience because having, you know, you speak to your assistant

00:50:31.280 --> 00:50:35.360
or you do something and it says thinking, thinking like, okay, well that I don't want

00:50:35.360 --> 00:50:35.500
that.

00:50:35.500 --> 00:50:37.500
Like, I'll just go do it if I got to wait 10 seconds.

00:50:37.500 --> 00:50:37.780
Right.

00:50:37.780 --> 00:50:38.000
Yeah.

00:50:38.000 --> 00:50:38.800
But it happens immediately.

00:50:38.800 --> 00:50:40.420
And there's the privacy aspect too.

00:50:40.420 --> 00:50:41.840
Yeah, absolutely.

00:50:41.840 --> 00:50:42.840
The privacy is great.

00:50:42.840 --> 00:50:43.020
Yeah.

00:50:43.020 --> 00:50:46.020
Like the wake word on the, like, I don't know if you know this, but like the wake

00:50:46.020 --> 00:50:49.860
words, like on the Alexa device, like they happen local, that runs locally.

00:50:49.860 --> 00:50:55.020
Although I've heard, I've heard that when you say Alexa, they verify it in the cloud

00:50:55.020 --> 00:50:56.460
with a more powerful model.

00:50:56.460 --> 00:50:57.020
Interesting.

00:50:57.020 --> 00:50:58.860
Because sometimes it'll trigger and then shut off.

00:50:58.860 --> 00:51:00.340
I don't know if you've ever seen that happen.

00:51:00.340 --> 00:51:00.660
Yeah.

00:51:00.660 --> 00:51:03.380
It's, it'll spin around and go, ah, no, that wasn't right.

00:51:03.380 --> 00:51:04.020
Yeah, exactly.

00:51:04.020 --> 00:51:07.280
I think what's happening is that they're sending what they're sending like the wake

00:51:07.280 --> 00:51:08.320
word to the cloud to verify.

00:51:08.320 --> 00:51:09.620
Like, did you actually say Alexa?

00:51:09.620 --> 00:51:15.460
Probably the local models below some certain confidence level, it sends it up to the cloud and then

00:51:15.460 --> 00:51:17.780
the cloud verifies like, yeah, start, start processing.

00:51:17.780 --> 00:51:20.620
But it is much faster from a latency perspective.

00:51:20.620 --> 00:51:25.600
Although with, with 5G, I don't know, like mobile internet is so much, it's faster now.

00:51:25.600 --> 00:51:26.480
It's getting pretty crazy.

00:51:26.480 --> 00:51:26.780
Yeah.

00:51:26.780 --> 00:51:27.560
Yeah, absolutely.

00:51:27.560 --> 00:51:27.960
Yeah.

00:51:27.960 --> 00:51:32.200
Sometimes I'll be somewhere my wifi is slow and I'll just tether my phone and it's like

00:51:32.200 --> 00:51:32.580
faster.

00:51:32.580 --> 00:51:32.760
Yeah.

00:51:32.760 --> 00:51:33.080
Yeah.

00:51:33.080 --> 00:51:35.540
If I'm not at my house, I usually do that.

00:51:35.540 --> 00:51:39.540
If I go to a coffee shop or an airport, I'm like, there's a very low chance that the wifi

00:51:39.540 --> 00:51:41.280
here is better than my 5G tethered.

00:51:41.280 --> 00:51:41.480
Yeah.

00:51:41.480 --> 00:51:41.580
Yeah.

00:51:41.580 --> 00:51:42.060
Exactly.

00:51:42.060 --> 00:51:42.220
Exactly.

00:51:42.220 --> 00:51:43.660
Exactly.

00:51:45.020 --> 00:51:47.740
Jack Woody out in the audience has a real interesting question.

00:51:47.740 --> 00:51:54.040
I think that you can speak to because you're in this space right now, living it.

00:51:54.040 --> 00:51:59.780
What do investors look at when considering an AI startup or maybe AI startup, not just specifically

00:51:59.780 --> 00:52:01.080
speech to text?

00:52:01.240 --> 00:52:01.440
Yeah.

00:52:01.440 --> 00:52:02.120
It's a good question.

00:52:02.120 --> 00:52:07.020
I think it really depends on like, are you building like a vertical application that makes

00:52:07.020 --> 00:52:07.640
use of the AI?

00:52:07.640 --> 00:52:13.480
So you're building some like call center optimization software where there's like AI under the hood,

00:52:13.480 --> 00:52:18.540
but you're, you're using it to power this like business use case versus are you building

00:52:18.540 --> 00:52:20.720
some like, like infrastructure AI company?

00:52:20.720 --> 00:52:25.920
Like we're us, like we're building APIs for speech to text, or if you're building a company

00:52:25.920 --> 00:52:30.840
that's exposing like APIs for NLP or different types of tasks, I think it varies what they

00:52:30.840 --> 00:52:31.240
look at.

00:52:31.240 --> 00:52:35.020
I am not an expert in like fundraising or AI startups.

00:52:35.020 --> 00:52:36.260
I want to make that very clear.

00:52:36.260 --> 00:52:39.940
Like, so, so maybe don't take my advice too, too seriously.

00:52:39.940 --> 00:52:44.280
Yeah, but you've done it successfully, which is, I mean, there are people who claim to

00:52:44.280 --> 00:52:48.960
be experts, but are not currently running, you know, a successful backed company.

00:52:48.960 --> 00:52:51.580
So I wouldn't put too much of a caveat there.

00:52:51.580 --> 00:52:51.740
Yeah.

00:52:51.740 --> 00:52:55.560
I think we just got lucky with, you know, meeting some of the right people that have helped us.

00:52:55.560 --> 00:52:59.880
But I think it's like, yeah, you know, are you, are you doing something innovative on the

00:52:59.880 --> 00:53:00.560
model side?

00:53:00.560 --> 00:53:03.440
Do you have some innovation on the architecture side?

00:53:03.440 --> 00:53:08.340
I actually don't really think the whole like data vote is that strong of an argument personally,

00:53:08.520 --> 00:53:10.880
because there's just so much data on the internet now.

00:53:10.880 --> 00:53:14.700
And did a moat being like, we run Gmail so we can scan everybody's email.

00:53:14.700 --> 00:53:16.600
That gives us a competitive advantage.

00:53:16.600 --> 00:53:16.940
Yeah.

00:53:16.940 --> 00:53:17.540
Yeah.

00:53:17.540 --> 00:53:18.160
Something like that.

00:53:18.160 --> 00:53:18.580
Exactly.

00:53:18.580 --> 00:53:18.840
Yeah.

00:53:18.840 --> 00:53:19.980
I don't know.

00:53:19.980 --> 00:53:23.440
Like you might get like a slight advantage, but there's so much data on the internet and

00:53:23.440 --> 00:53:26.160
there's so many, there's so many innovations happening around.

00:53:26.160 --> 00:53:29.560
Like, look at, look at GPT-3 that OpenAI put out, right?

00:53:29.560 --> 00:53:34.660
That was just trained on like crazy amount, a huge model trained on crazy amounts of public

00:53:34.660 --> 00:53:36.500
domain data on the internet.

00:53:36.500 --> 00:53:38.980
That works so well across so many different tasks.

00:53:38.980 --> 00:53:45.020
So even if you had a data mode for a specific task, like it's arguable that GPT-3 could beat

00:53:45.020 --> 00:53:46.740
you at that task.

00:53:46.740 --> 00:53:51.520
So I think it depends what you're doing, but I don't personally buy into the whole data mode

00:53:51.520 --> 00:53:52.360
thing that much.

00:53:52.360 --> 00:53:56.600
You know, cause like even for us, we're able to build some of the best speech to text models

00:53:56.600 --> 00:53:57.880
in the world.

00:53:57.880 --> 00:54:01.360
And we don't have this like secret source of data.

00:54:01.360 --> 00:54:05.880
You know, we, we just have a lot of innovation on the model side and there's tons of domain

00:54:05.880 --> 00:54:08.580
data in a public domain that you can access now.

00:54:08.580 --> 00:54:13.040
So I think it's really about like, are you building some type of application that is making

00:54:13.040 --> 00:54:19.000
the lives of like a customer developer, some startup, like easier leveraging AI?

00:54:19.000 --> 00:54:19.160
Right.

00:54:19.160 --> 00:54:22.800
Are you solving a problem that people will pay money to solve?

00:54:22.800 --> 00:54:23.000
Yeah.

00:54:23.000 --> 00:54:23.620
Yeah, exactly.

00:54:23.620 --> 00:54:24.200
Exactly.

00:54:24.200 --> 00:54:28.120
Cause I actually think it's more about like the distribution of the tech you're building

00:54:28.120 --> 00:54:29.300
versus the tech itself.

00:54:29.520 --> 00:54:36.040
So like, are you packaging it up in an easy to use API or is like the, imagine you're

00:54:36.040 --> 00:54:39.540
selling something to like podcast hosts that uses AI.

00:54:39.540 --> 00:54:44.120
I mean, AI could be amazing, but if like the user interface sucks, you know, like you're,

00:54:44.120 --> 00:54:45.180
you're not going to use it.

00:54:45.180 --> 00:54:45.360
Here's what you do.

00:54:45.360 --> 00:54:49.140
You're going to make a post request over to this and you put this header in and like, it's

00:54:49.140 --> 00:54:53.140
going to like, here's how you do paging and you're like, no, no, here's the library

00:54:53.140 --> 00:54:53.720
in your language.

00:54:53.720 --> 00:54:55.800
You call the one function, things happen, right?

00:54:55.800 --> 00:54:58.740
Like how presentable or straightforward do you make it right?

00:54:58.740 --> 00:54:59.000
Right.

00:54:59.000 --> 00:55:01.180
Cause I actually think that's a huge piece of it.

00:55:01.180 --> 00:55:02.340
Are you, are you making it easier?

00:55:02.340 --> 00:55:06.660
Are you making, is the distribution around the technology you're creating like really powerful

00:55:06.660 --> 00:55:09.260
and, and like, do you have good ideas around that?

00:55:09.260 --> 00:55:13.100
So I think it's a combination of those things, but to be honest, I think really depends on

00:55:13.100 --> 00:55:16.220
what you're building and what the product is or what you're doing.

00:55:16.220 --> 00:55:18.640
Cause it varies, like really it varies a lot.

00:55:18.640 --> 00:55:18.880
Yeah.

00:55:18.880 --> 00:55:25.000
There's also the part that we as developers don't love to think about, but the marketing

00:55:25.000 --> 00:55:28.960
and awareness and growth and traction, right?

00:55:28.960 --> 00:55:29.220
Yeah.

00:55:29.220 --> 00:55:32.120
It's, you could say, look, here's the most amazing model we have.

00:55:32.120 --> 00:55:36.700
Well, we haven't actually got any users yet, but that is a really hard sell for investors

00:55:36.700 --> 00:55:40.440
unless they absolutely see, you know, this has huge potential.

00:55:40.440 --> 00:55:40.720
Right.

00:55:40.820 --> 00:55:46.000
But if you're like, look, we've got this much monthly number of users and here's the

00:55:46.000 --> 00:55:49.040
way we're going to start to up, you know, create a premium offering.

00:55:49.040 --> 00:55:49.520
And yeah.

00:55:49.520 --> 00:55:50.020
Yeah.

00:55:50.020 --> 00:55:50.240
Right.

00:55:50.240 --> 00:55:56.380
That that's something we're not particularly skilled at as developers, but that's a non-trivial

00:55:56.380 --> 00:55:57.520
part of any tech startup.

00:55:57.520 --> 00:55:57.820
Right.

00:55:57.980 --> 00:55:58.360
Oh yeah.

00:55:58.360 --> 00:56:01.620
And I think as a developer too, you kind of like shy away from wanting to work on that

00:56:01.620 --> 00:56:05.980
because it's so much easier to just write code or build a new feature versus like go solve

00:56:05.980 --> 00:56:08.120
this hard marketing problem or go like.

00:56:08.120 --> 00:56:09.160
Marketing sales.

00:56:09.160 --> 00:56:12.300
Like you gotta have them, even if you're bad at them and you don't like it.

00:56:12.300 --> 00:56:12.740
Yeah.

00:56:12.740 --> 00:56:14.900
And we're fortunate that we get to market it to developers.

00:56:14.900 --> 00:56:20.600
So like I enjoy it, you know, and cause you get like to talk to developers all the time,

00:56:20.600 --> 00:56:22.220
but yeah, that's a huge piece of it too.

00:56:22.220 --> 00:56:22.680
Definitely.

00:56:22.680 --> 00:56:23.260
Definitely.

00:56:23.260 --> 00:56:24.740
It's, it's kind of all come together.

00:56:24.740 --> 00:56:25.360
Yeah.

00:56:25.500 --> 00:56:28.820
This up a little bit, we're getting sort of near the end, but let's talk about,

00:56:28.820 --> 00:56:32.640
you've got this idea of you've got your models, you've got your libraries, you've trained them

00:56:32.640 --> 00:56:33.800
up using your GPUs.

00:56:33.800 --> 00:56:36.160
Now you want to offer it as an API.

00:56:36.160 --> 00:56:41.980
Like how do you go to production with a machine learning model and do something interesting?

00:56:41.980 --> 00:56:44.580
You want to talk about how that's where I know you talked a little bit about running

00:56:44.580 --> 00:56:46.100
the cloud and whatnot, but yeah.

00:56:46.100 --> 00:56:50.820
You know, do you offer as an API over Flask or run it in a cloud?

00:56:50.820 --> 00:56:51.560
Like what are you doing there?

00:56:51.560 --> 00:56:52.600
Are they Lambda functions?

00:56:52.600 --> 00:56:53.780
Yeah, that's a good, that's a good question.

00:56:53.780 --> 00:56:54.700
What's your world look like?

00:56:54.800 --> 00:57:00.000
So we have asynchronous APIs where you send in an audio file and then we send you a

00:57:00.000 --> 00:57:01.420
webhook when it's done processing.

00:57:01.420 --> 00:57:05.440
And then we have real-time APIs over WebSocket where you're streaming audio and you're getting

00:57:05.440 --> 00:57:07.620
stuff back over a WebSocket in real time.

00:57:07.620 --> 00:57:10.620
The real-time stuff's a lot more challenging to build.

00:57:10.620 --> 00:57:11.620
I'm sure it is.

00:57:11.800 --> 00:57:11.940
Yeah.

00:57:11.940 --> 00:57:12.000
Yeah.

00:57:12.000 --> 00:57:13.160
The async stuff.

00:57:13.160 --> 00:57:18.080
Really what happens is we have like, so one of our main APIs was built in Tornado.

00:57:18.080 --> 00:57:19.680
I don't know if you, yeah.

00:57:19.680 --> 00:57:20.160
Legacy.

00:57:20.160 --> 00:57:26.940
The early, early async enabled Python web framework before asyncio was officially a thing.

00:57:26.940 --> 00:57:27.220
Yep.

00:57:27.540 --> 00:57:30.140
So I built the first version of the API in Tornado.

00:57:30.140 --> 00:57:33.500
So it's kind of like still in Tornado for that reason.

00:57:33.500 --> 00:57:38.520
A lot of the newer things or newer microservices are built by FastAPI or Flask.

00:57:38.980 --> 00:57:43.020
And so for the asynchronous API, what happens is like you're making a post request.

00:57:43.020 --> 00:57:44.920
The API is really just like a CRUD app.

00:57:44.920 --> 00:57:49.140
It's storing a record of the request that you made with all the parameters that you turned

00:57:49.140 --> 00:57:50.000
on or turn off.

00:57:50.000 --> 00:57:52.020
And then that goes into a database.

00:57:52.020 --> 00:57:57.460
Some worker that's like the orchestrator is constantly looking at that database and it's

00:57:57.460 --> 00:57:59.460
like, okay, there's some new work to be done.

00:57:59.460 --> 00:58:04.320
And then kicks off all these different jobs to all these different microservices, some over

00:58:04.320 --> 00:58:09.700
queues, some over HTTP, collects everything back, orchestrates like what could be done in

00:58:09.700 --> 00:58:12.100
parallel, what depends on what to be done first.

00:58:12.100 --> 00:58:18.220
When that's all done, all the kind of asynchronous like background jobs, the orchestrator pushes

00:58:18.220 --> 00:58:21.280
the final result back into our primary database.

00:58:21.280 --> 00:58:25.240
And then that triggers you getting a webhook with the final result.

00:58:25.240 --> 00:58:29.420
So that's like in a nutshell, kind of what the architecture looks like for the asynchronous

00:58:29.420 --> 00:58:30.040
workloads.

00:58:30.040 --> 00:58:34.260
There's like tons of different microservices, all with different instance types.

00:58:34.260 --> 00:58:40.400
different like compute requirements, some GPU, some CPU, some, you know, like all different

00:58:40.400 --> 00:58:41.400
scaling policies.

00:58:41.400 --> 00:58:43.480
And that's really where the hard part is.

00:58:43.480 --> 00:58:47.640
That's kind of like the basic overview of how the asynchronous stuff works in production.

00:58:47.640 --> 00:58:48.080
Yeah.

00:58:48.080 --> 00:58:48.420
Yeah.

00:58:48.420 --> 00:58:48.820
Very cool.

00:58:48.820 --> 00:58:49.120
Yeah.

00:58:49.120 --> 00:58:52.840
Are you seeing Postgres or MySQL or something like that?

00:58:52.840 --> 00:58:54.660
Postgres for the primary DB.

00:58:54.660 --> 00:58:59.600
Because we're on AWS, we use DynamoDB for a couple of things like ephemeral records we need

00:58:59.600 --> 00:59:02.880
to keep around for when you send something in, it goes to DynamoDB.

00:59:02.880 --> 00:59:08.360
And that's where we like keep track of basically like your request and what parameters you add

00:59:08.360 --> 00:59:08.840
on and off.

00:59:08.840 --> 00:59:10.240
And that kicks off a bunch of things.

00:59:10.240 --> 00:59:11.920
But the primary DB is Postgres.

00:59:11.920 --> 00:59:12.400
Yeah.

00:59:12.400 --> 00:59:15.000
I think there's like, at this point, like it's getting pretty large.

00:59:15.000 --> 00:59:18.880
There's like a few billion records in there.

00:59:18.880 --> 00:59:22.960
Because we process like a couple million audio files a day with the API.

00:59:22.960 --> 00:59:27.340
Sometimes I'll read on Hacker News like these, I think like GitHub went down at one point

00:59:27.340 --> 00:59:31.460
because they couldn't increment the primary key values any higher.

00:59:31.460 --> 00:59:34.580
It's, int 64 is overflowing.

00:59:34.580 --> 00:59:34.980
We're done.

00:59:34.980 --> 00:59:35.440
Yeah, yeah.

00:59:35.440 --> 00:59:36.280
Something like that.

00:59:36.280 --> 00:59:36.460
Yeah.

00:59:36.460 --> 00:59:39.400
I mean, in the back of my mind, like I hope we're thinking about something like that because

00:59:39.400 --> 00:59:42.960
that would be really bad if we came up against something like that.

00:59:42.960 --> 00:59:48.140
Do you store the audio content in the database or are they going like some kind of bucket,

00:59:48.140 --> 00:59:49.440
some object storage thing?

00:59:49.440 --> 00:59:52.820
So we're unique in that we don't store a copy of your audio data.

00:59:52.820 --> 00:59:53.320
Okay.

00:59:53.320 --> 00:59:55.480
For privacy reasons for you.

00:59:55.480 --> 01:00:01.020
So you send something in, it's stored ephemerally like in the memory of the machine that's processing

01:00:01.020 --> 01:00:01.540
your file.

01:00:01.540 --> 01:00:05.960
And then what's stored is the transcription text encrypted at rest because you need to be able

01:00:05.960 --> 01:00:07.860
to make a GET request for the API to fetch it.

01:00:07.860 --> 01:00:12.400
But then you can follow up with a delete request to permanently delete the transcription text from

01:00:12.400 --> 01:00:12.880
our database.

01:00:12.880 --> 01:00:18.540
So we try to like keep no record of the data that you're processing because like we want

01:00:18.540 --> 01:00:22.000
to be really privacy focused and sensitive.

01:00:22.000 --> 01:00:25.860
You can, like some customers will toggle on that.

01:00:25.860 --> 01:00:31.400
We keep some of their data to continuously improve the models, but by default, we don't store anything.

01:00:31.400 --> 01:00:32.360
Yeah, that's really cool.

01:00:32.360 --> 01:00:32.700
Yeah.

01:00:32.700 --> 01:00:34.620
That's good for privacy.

01:00:34.620 --> 01:00:38.980
It's also good for you all because there's just less stuff that you have to be nervous about

01:00:38.980 --> 01:00:40.200
when you're trying to fall asleep.

01:00:40.200 --> 01:00:42.020
You're like, what if somebody broke in and got all the audio?

01:00:42.120 --> 01:00:43.180
Oh wait, we don't have the audio.

01:00:43.180 --> 01:00:43.480
Okay.

01:00:43.480 --> 01:00:44.880
So that's not a thing they could get.

01:00:44.880 --> 01:00:45.180
Yeah.

01:00:45.180 --> 01:00:46.180
Like things like that, right?

01:00:46.180 --> 01:00:46.520
Yeah.

01:00:46.520 --> 01:00:47.120
Yeah.

01:00:47.120 --> 01:00:50.140
It's definitely, definitely.

01:00:50.140 --> 01:00:53.840
I hadn't thought about that before, but I'm imagining now what that would be like.

01:00:53.840 --> 01:00:56.400
Well, now I'm making, now you're going to be nervous because there's probably other stuff,

01:00:56.400 --> 01:00:56.740
but that's all right.

01:00:56.740 --> 01:00:56.880
Yeah.

01:00:56.880 --> 01:01:00.420
Now you got me thinking about in that space.

01:01:00.420 --> 01:01:02.160
Like what are those things we need to lock up?

01:01:02.240 --> 01:01:05.200
No, we have like, we're mostly a team of engineers.

01:01:05.200 --> 01:01:11.560
So I think of the 30 people, like 70% are engineers with a lot more experience than me.

01:01:11.560 --> 01:01:15.140
So we're doing everything like by the book, especially with the business that we're in.

01:01:15.140 --> 01:01:15.480
Yeah.

01:01:15.480 --> 01:01:15.800
Yeah.

01:01:15.800 --> 01:01:16.140
Of course.

01:01:16.140 --> 01:01:16.460
Yeah.

01:01:16.460 --> 01:01:19.860
All right, Dylan, I think we're out of time, if not out of topic.

01:01:19.860 --> 01:01:25.700
So let's maybe wrap this up a little bit with the final two questions and some packages and

01:01:25.700 --> 01:01:25.860
stuff.

01:01:25.860 --> 01:01:29.580
So if you're going to work on some Python code, what editor are you using these days?

01:01:29.720 --> 01:01:31.260
I'm still using Sublime.

01:01:31.260 --> 01:01:31.800
Right on.

01:01:31.800 --> 01:01:32.600
What do you use?

01:01:32.600 --> 01:01:34.340
The OG easy ones.

01:01:34.340 --> 01:01:35.940
I'm mostly PyCharm.

01:01:35.940 --> 01:01:40.260
If I want to just open a single file and look at it, I'll probably use VS Code for that.

01:01:40.260 --> 01:01:42.900
That's probably just, you know, I want to open that thing.

01:01:42.900 --> 01:01:46.420
Not have all the project ideas around it, but I'm doing proper work.

01:01:46.420 --> 01:01:47.620
Probably PyCharm these days.

01:01:47.620 --> 01:01:47.900
Yeah.

01:01:47.900 --> 01:01:48.340
Yeah.

01:01:48.340 --> 01:01:48.940
That makes sense.

01:01:48.940 --> 01:01:49.180
Yeah.

01:01:49.180 --> 01:01:52.220
And then a notable PyPI project, some library out there.

01:01:52.220 --> 01:01:56.400
I mean, you've already talked about it, like TensorFlow and some others, but anything out

01:01:56.400 --> 01:01:58.580
there you're like, oh, we should, you should definitely check this out.

01:01:58.580 --> 01:02:00.440
I would check out Hugging Face if you haven't yet.

01:02:00.440 --> 01:02:01.740
It's a pretty cool library.

01:02:01.740 --> 01:02:02.200
Yeah.

01:02:02.200 --> 01:02:03.140
A pretty cool library.

01:02:03.140 --> 01:02:03.520
Yeah.

01:02:03.520 --> 01:02:05.380
Hugging Face seems like a really interesting idea.

01:02:05.380 --> 01:02:05.740
Yeah.

01:02:05.740 --> 01:02:09.400
I want to give a quick shout out to one as well that I don't know if you've seen this.

01:02:09.400 --> 01:02:13.400
Have you seen TLS, please, as an LS replacement?

01:02:13.400 --> 01:02:14.120
No.

01:02:14.120 --> 01:02:16.020
Chris May told me about this yesterday.

01:02:16.020 --> 01:02:18.180
Told me and Brian for Python Bytes.

01:02:18.180 --> 01:02:19.440
Check this out.

01:02:19.440 --> 01:02:24.200
So it's a new LS that has like icons and it's all developer focused.

01:02:24.200 --> 01:02:27.680
So like if you've got a virtual environment, it'll show that separately.

01:02:27.680 --> 01:02:30.700
If you've got a Python file, it has a Python icon.

01:02:30.700 --> 01:02:34.860
The things that appear in the list are controlled somewhat by the git ignore file and other things

01:02:34.860 --> 01:02:35.400
like that.

01:02:35.400 --> 01:02:40.720
And you can even do like a more detailed listing where it'll show like the git status of the

01:02:40.720 --> 01:02:41.240
various files.

01:02:41.240 --> 01:02:41.740
Isn't that crazy?

01:02:41.740 --> 01:02:42.440
That's really cool.

01:02:42.440 --> 01:02:42.840
Yeah.

01:02:42.840 --> 01:02:43.600
That's really cool.

01:02:43.600 --> 01:02:44.900
That's a Python library, PLS.

01:02:44.900 --> 01:02:45.500
PLS.

01:02:45.500 --> 01:02:46.100
That's awesome.

01:02:46.100 --> 01:02:46.940
I'll check that one out.

01:02:46.940 --> 01:02:47.140
Yeah.

01:02:47.140 --> 01:02:47.460
Yeah.

01:02:47.460 --> 01:02:48.620
People can check that out.

01:02:48.620 --> 01:02:48.860
Yeah.

01:02:48.860 --> 01:02:49.240
All right.

01:02:49.240 --> 01:02:51.440
Dylan, thank you so much for being on the show.

01:02:51.440 --> 01:02:55.640
It's really cool to get this look into running ML stuff.

01:02:55.640 --> 01:02:55.940
Yeah.

01:02:55.940 --> 01:02:57.080
And production and whatnot.

01:02:57.080 --> 01:02:58.480
Thanks for having me on.

01:02:58.480 --> 01:02:59.320
Yeah.

01:02:59.320 --> 01:02:59.620
You bet.

01:02:59.620 --> 01:03:00.880
You want to give us a final call to action?

01:03:00.880 --> 01:03:06.360
People interested in sort of maybe doing an ML startup or even if they want to do things

01:03:06.360 --> 01:03:07.080
with Assembly AI?

01:03:07.080 --> 01:03:11.200
If you want to check out our APIs for automatic speech and text, you can go to our website,

01:03:11.200 --> 01:03:12.780
assemblyai.com.

01:03:13.200 --> 01:03:14.220
Get a free API token.

01:03:14.220 --> 01:03:15.600
You don't have to talk to anyone.

01:03:15.600 --> 01:03:17.060
You can start playing around.

01:03:17.060 --> 01:03:20.900
There's a lot of Python code samples that you can grab to get up and running pretty quickly.

01:03:20.900 --> 01:03:25.940
And then, yeah, if you're interested in ML startups, I think that one of the things that

01:03:25.940 --> 01:03:31.960
I always recommend is if you want to go the funding route, definitely check out Y Combinator

01:03:31.960 --> 01:03:35.420
as a place to apply because that really helped us get off the ground.

01:03:35.420 --> 01:03:39.540
They help you out with a lot of credits around GPUs and resources.

01:03:39.940 --> 01:03:40.800
And it helps a lot.

01:03:40.800 --> 01:03:41.580
That helped us a lot.

01:03:41.580 --> 01:03:43.680
Were you in the 2017 cohort?

01:03:43.680 --> 01:03:43.960
Yeah.

01:03:43.960 --> 01:03:44.840
Something like that.

01:03:44.840 --> 01:03:45.000
Yeah.

01:03:45.000 --> 01:03:45.380
2017.

01:03:45.380 --> 01:03:47.140
So it was super helpful.

01:03:47.140 --> 01:03:48.860
And I would highly recommend that.

01:03:48.860 --> 01:03:53.600
There's also just a big community of other ML people that you can get access to through

01:03:53.600 --> 01:03:53.820
that.

01:03:53.820 --> 01:03:55.180
So that really helped.

01:03:55.180 --> 01:03:57.680
And I would recommend people check that out.

01:03:57.680 --> 01:04:00.160
How about if I don't want to go PC funded?

01:04:00.160 --> 01:04:00.720
Yeah.

01:04:00.720 --> 01:04:01.000
Go ahead.

01:04:01.000 --> 01:04:01.280
Yeah.

01:04:01.280 --> 01:04:05.300
So one more is there's also an online accelerator called Pioneer.

01:04:05.300 --> 01:04:08.900
I don't know if you've heard of this, but that's also a good one to check out too.

01:04:09.220 --> 01:04:15.080
If you don't want to go the accelerator route, then I would say like, yeah, really it's just

01:04:15.080 --> 01:04:19.340
about getting a model working good enough to like close your first customer and then just

01:04:19.340 --> 01:04:20.440
like keep iterating, you know?

01:04:20.440 --> 01:04:23.920
So like don't get caught up in like reaching state of the art or yeah.

01:04:23.920 --> 01:04:27.800
Like in the research, just like kind of think of like the MVP model that you need to build.

01:04:27.800 --> 01:04:30.880
They go win your first customer and they kind of keep going from there.

01:04:30.880 --> 01:04:31.120
Yeah.

01:04:31.120 --> 01:04:31.620
Awesome.

01:04:31.620 --> 01:04:32.380
All right.

01:04:32.380 --> 01:04:34.780
Well, thanks for sharing all your experience and for being here.

01:04:34.780 --> 01:04:35.140
Yeah.

01:04:35.140 --> 01:04:35.420
Yeah.

01:04:35.420 --> 01:04:36.120
Thanks for having me on.

01:04:36.120 --> 01:04:36.600
This was fun.

01:04:36.600 --> 01:04:37.520
Yeah, you bet it was.

01:04:37.520 --> 01:04:37.940
All right.

01:04:37.940 --> 01:04:38.140
Bye.

01:04:38.140 --> 01:04:41.980
This has been another episode of Talk Python to Me.

01:04:41.980 --> 01:04:43.800
Thank you to our sponsors.

01:04:43.800 --> 01:04:45.420
Be sure to check out what they're offering.

01:04:45.420 --> 01:04:46.840
It really helps support the show.

01:04:46.840 --> 01:04:51.220
For over a dozen years, the Stack Overflow podcast has been exploring what it means to

01:04:51.220 --> 01:04:55.520
be a developer and how the art and practice of software programming is changing the world.

01:04:55.520 --> 01:04:59.880
Join them on that adventure at talkpython.fm/stack overflow.

01:04:59.880 --> 01:05:01.800
Take some stress out of your life.

01:05:02.140 --> 01:05:07.260
Get notified immediately about errors and performance issues in your web or mobile applications with

01:05:07.260 --> 01:05:07.600
Sentry.

01:05:07.600 --> 01:05:12.580
Just visit talkpython.fm/sentry and get started for free.

01:05:12.580 --> 01:05:16.180
And be sure to use the promo code talkpython, all one word.

01:05:16.180 --> 01:05:17.620
Want to level up your Python?

01:05:17.880 --> 01:05:21.660
We have one of the largest catalogs of Python video courses over at Talk Python.

01:05:21.660 --> 01:05:26.840
Our content ranges from true beginners to deeply advanced topics like memory and async.

01:05:26.840 --> 01:05:29.520
And best of all, there's not a subscription in sight.

01:05:29.520 --> 01:05:32.420
Check it out for yourself at training.talkpython.fm.

01:05:32.540 --> 01:05:34.320
Be sure to subscribe to the show.

01:05:34.320 --> 01:05:37.100
Open your favorite podcast app and search for Python.

01:05:37.100 --> 01:05:38.420
We should be right at the top.

01:05:38.420 --> 01:05:43.560
You can also find the iTunes feed at /itunes, the Google Play feed at /play,

01:05:43.560 --> 01:05:47.780
and the direct RSS feed at /rss on talkpython.fm.

01:05:47.780 --> 01:05:51.200
We're live streaming most of our recordings these days.

01:05:51.200 --> 01:05:54.620
If you want to be part of the show and have your comments featured on the air,

01:05:54.620 --> 01:05:59.040
be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

01:05:59.040 --> 01:06:00.900
This is your host, Michael Kennedy.

01:06:00.900 --> 01:06:02.180
Thanks so much for listening.

01:06:02.180 --> 01:06:03.360
I really appreciate it.

01:06:03.360 --> 01:06:05.260
Now get out there and write some Python code.

01:06:05.260 --> 01:06:35.240
Thank you.

