WEBVTT

00:00:00.001 --> 00:00:04.840
Do you love Python's async and await, but feel that you could use more flexibility or

00:00:04.840 --> 00:00:09.860
higher order constructs, like running a group of tasks and child tasks as a single operation,

00:00:09.860 --> 00:00:15.360
or streaming data between tasks, combining tasks with multi-processing or threads,

00:00:15.360 --> 00:00:21.560
or even async file support? You should check out AnyIO. On this episode, we have Alex Granholm,

00:00:21.560 --> 00:00:25.600
the creator of AnyIO, here to give us the whole story. This is Talk Python To Me,

00:00:25.600 --> 00:00:29.580
episode 385, recorded September 29th, 2022.

00:00:29.580 --> 00:00:47.740
Welcome to Talk Python To Me, a weekly podcast on Python. This is your host, Michael Kennedy.

00:00:47.740 --> 00:00:52.380
Follow me on Twitter where I'm @mkennedy and keep up with the show and listen to past episodes

00:00:52.380 --> 00:00:59.060
at talkpython.fm and follow the show on Twitter via at talkpython. We've started streaming most of our

00:00:59.060 --> 00:01:04.720
episodes live on YouTube. Subscribe to our YouTube channel over at talkpython.fm/youtube to

00:01:04.720 --> 00:01:10.620
get notified about upcoming shows and be part of that episode. This episode of Talk Python To Me is

00:01:10.620 --> 00:01:17.200
brought to you by Compiler from Red Hat, an original podcast. Listen to an episode of their show as they

00:01:17.200 --> 00:01:24.540
demystify the tech industry over at talkpython.fm/compiler. It's also brought to you by us over

00:01:24.540 --> 00:01:30.600
at Talk Python training where we have over 240 hours of Python courses. Please visit talkpython.fm

00:01:30.600 --> 00:01:36.060
and click on courses in the nav bar. Transcripts for this and all of our episodes are brought to you by

00:01:36.100 --> 00:01:41.520
assembly. Do you need a great automatic speech to text API? Get human level accuracy in just a few

00:01:41.520 --> 00:01:47.580
lines of code. Visit talkpython.fm/assembly AI. Alex, welcome to Talk Python To Me.

00:01:47.580 --> 00:01:48.260
Thank you.

00:01:48.260 --> 00:01:53.700
Yeah, it's fantastic to have you here. You have so many cool open source projects out there. We're here to

00:01:53.700 --> 00:02:00.680
talk about any IO, but actually several of them I've covered on Python bytes on the other podcasts that

00:02:00.680 --> 00:02:06.720
I run. And we've talked about SQL code gen and typeguard. And I didn't associate that with you

00:02:06.720 --> 00:02:10.820
specifically and back over to any IO. So yeah, a lot of cool projects you got going on there.

00:02:10.820 --> 00:02:16.160
Yeah, too many actually. I managed to hand over a couple of them to other people where

00:02:16.160 --> 00:02:23.780
Seaboard 2 and Sphinx Autodoc type hints, because I'm really the stretched thing at the moment. I barely

00:02:23.780 --> 00:02:26.360
have time for all of the projects that I'm maintaining.

00:02:26.500 --> 00:02:32.220
I can imagine. That's, you know, how do you juggle all that? You know, it's, I'm sure you have a full

00:02:32.220 --> 00:02:35.280
time job and you have all these different projects, right? How do you prioritize?

00:02:35.280 --> 00:02:41.840
Yes. Yes. Basically, I get the equivalent of a writer's block from time to time. So when that

00:02:41.840 --> 00:02:47.500
happens, I just either don't try to code up at all, or I just switch to another project.

00:02:47.500 --> 00:02:52.460
Yeah, true. If you're talking about type hints versus async programming, like if you're stuck on one,

00:02:52.460 --> 00:02:54.020
you probably are not stuck on the other, right?

00:02:54.020 --> 00:02:54.300
Yeah.

00:02:54.300 --> 00:02:56.340
Yeah. Interesting. Well, we're going to have a lot of fun

00:02:56.340 --> 00:03:01.280
talking about all of them. I doubt there's going to be any writer's block or speaker's block here,

00:03:01.280 --> 00:03:05.400
podcaster's block. It'll be good. We'll have a good time chatting about it and sharing with everyone.

00:03:05.400 --> 00:03:09.620
Before we get to that, though, let's hear your story. How'd you get into programming in Python?

00:03:09.620 --> 00:03:18.240
I got into programming at the age of eight. It was on an MSX compatible machine. I started with BASIC,

00:03:18.240 --> 00:03:27.620
as so many others did. I did some simple text-based games at first, just playing around. At some point,

00:03:27.660 --> 00:03:38.720
I got Commodore's Commodore's 128 and I did some simple graphical demos with it. Then I got an Amiga 500.

00:03:38.720 --> 00:03:41.040
Oh yeah, the Amiga's were cool. They were special.

00:03:41.040 --> 00:04:07.100
Mm-hmm. Yeah. I dabbled in almost BASIC, then other kinds of tools also. I don't really remember that much of it. Then at some point, I did something with Mac. That is macOS Classic. There was this tool called HyperCard. It's precursor for Flash, basically. So that's something I did some things with. Simple games and whatnot.

00:04:07.100 --> 00:04:33.260
Yeah. Okay. Skipping forward a bit. I got into PC programming like C++, mostly C. Then I think it was in the latter half of 2005. No, actually it was much earlier in 1999. I started with Perl. Hated it. Then I think the next step was in 2005 when I got to learn PHP. Hated that too.

00:04:34.120 --> 00:04:35.120
Kept searching.

00:04:35.120 --> 00:04:48.780
Then finally, finally in 2007, I got to know Python. Then that was Lava's first site, really. At that point, it was, I think, Python 2.5. And of course, I stuck with it.

00:04:49.120 --> 00:04:58.380
I did some Java professionally for a while, but I never really, really got to love it. It had this corporate industrial feeling to it.

00:04:59.520 --> 00:05:04.220
Sorry, I'm not dressed up enough to program my Java today. Let me go get my tie. I'll be right back.

00:05:04.220 --> 00:05:18.580
Yeah. Python was really cool. When I started learning it, my first practical application I made in what, 30 minutes after starting to learn it. It's really staggeringly easy to learn.

00:05:18.700 --> 00:05:19.000
It is.

00:05:19.000 --> 00:05:20.560
That's one thing I love about it.

00:05:20.560 --> 00:05:34.740
It really is. It's one of the few languages you can be really successful with, with a partial understanding of what's going on. Right. You don't even have to know what a class is or what modules are. You can just write a few functions in a file and you're good to go.

00:05:34.800 --> 00:05:36.560
It's almost like it's English.

00:05:36.560 --> 00:05:44.780
Yeah. Very cool. Raul out in the audience says, third time's the charm. The third language, you found the one you like there. Excellent. And how about now? What are you doing these days?

00:05:44.780 --> 00:06:01.820
I've been working for several years on a project, a very complicated project where, okay, this is always a hard part to describe it. It's a sort of working well as application. I'm part of a bigger team. I'm the lead backend developer.

00:06:01.820 --> 00:06:14.580
It collects IoT data and visualizes it and it provides all sorts of peripheral services to it. This is the first time I really had to spread my wings with databases.

00:06:14.580 --> 00:06:17.580
Oh yeah. Okay. What technologies are you using there?

00:06:17.580 --> 00:06:22.740
On the backend, we use TimescaleDB, which is a PostgreSQL extension.

00:06:22.740 --> 00:06:23.180
Okay.

00:06:23.180 --> 00:06:30.600
This is for storing that time series data. Then on the backend, we use my framework called Asphalt.

00:06:30.960 --> 00:06:48.960
I don't know if you've encountered that one. I think it's really cool, but it's not in widespread use. It's not a web framework per se. It's more like a generic framework where you can compose applications from a mix of free-made components and custom-made components.

00:06:48.960 --> 00:06:49.460
What's it called?

00:06:49.460 --> 00:06:49.780
Sorry?

00:06:49.780 --> 00:06:50.420
What was it called?

00:06:50.420 --> 00:06:50.980
Asphalt.

00:06:50.980 --> 00:06:52.300
Asphalt. Like the road?

00:06:52.300 --> 00:06:52.740
Yes.

00:06:52.740 --> 00:06:53.860
I do not spell it.

00:06:53.860 --> 00:06:54.620
There we go.

00:06:54.620 --> 00:06:54.980
Yeah.

00:06:55.180 --> 00:06:59.600
No. I did a search and I just found a snake and a python snake.

00:06:59.600 --> 00:07:01.300
Yeah, that's it. That's a good thing.

00:07:01.300 --> 00:07:09.900
That python snake on some asphalt road. Yeah. You got to be careful here. Okay. Is it a little bit like Flask or what makes it special?

00:07:09.900 --> 00:07:17.480
Well, Flask is a web framework. This is a generic framework. You can build any kind of applications with it. It doesn't have to be involved with web.

00:07:17.560 --> 00:07:23.160
I see any network thing, not necessarily HTTP, so it could be UDP or it could be just...

00:07:23.160 --> 00:07:39.460
It doesn't even have to do any networking at all. You can build command line tools with it. You can just have this mix of components and the YAML configuration to give settings to them all. I think this really would require a whole different session.

00:07:39.460 --> 00:07:44.500
It does sound like it would be a whole different session, but this is news to me and very interesting.

00:07:44.500 --> 00:07:57.160
Yeah, I haven't really advertised much. I'm working on version 5 at the moment, which does incorporate any I/O support and it brings the tech up to date with the current standards.

00:07:57.160 --> 00:08:05.420
Okay. Yeah, this looks very asynchronous based. It's an async I/O based micro framework for network oriented applications, it says.

00:08:05.420 --> 00:08:06.420
Yeah.

00:08:06.420 --> 00:08:12.780
And built upon uv loop, which is how all the good Python async things seem to be back these days.

00:08:12.780 --> 00:08:12.780
Mm-hmm.

00:08:12.780 --> 00:08:25.020
So it has a lot of sort of modern Python features. It's got async I/O, it's got uv loop, it's got type hints, those sorts of things. When you started in Python in 2007, none of those existed.

00:08:25.020 --> 00:08:26.020
Yeah.

00:08:26.020 --> 00:08:29.580
How do you see the recent changes to Python in the last five years or so?

00:08:29.580 --> 00:08:36.940
I would say that Python has been developing at an incredible speed. I really love it. So many useful stuff coming out with every release.

00:08:36.940 --> 00:08:37.940
I agree.

00:08:37.940 --> 00:08:47.300
Yeah, basically from 3.5 to 3.8 or something. There were just so many amazing features that came out then. And now we're seeing these libraries built upon it, right?

00:08:47.300 --> 00:08:48.300
Right.

00:08:48.300 --> 00:08:48.300
Right.

00:08:48.300 --> 00:09:01.300
All right. Well, let's transition over to our main topic that we're going to talk about, which is what I reached out to you for, not realizing the other two interesting projects that I already gave a shout out to are also yours. We'll get to those if we got time.

00:09:01.300 --> 00:09:16.660
So with Python in 3.4, we had async I/O introduced the actual frameworks that supported that. And then when it really came into its own was Python 3.5 when the async and await keywords were added to the language.

00:09:16.660 --> 00:09:33.620
Python 3.5. And Python came out of the box with some support for great async programming. But then there are these other libraries that developed on top of that to make certain use cases easier or add new capabilities. And any I/O falls into that realm, right?

00:09:33.620 --> 00:09:37.620
Yeah. So before we talk about any I/O, we should talk about Trio.

00:09:37.620 --> 00:09:38.020
Yep.

00:09:38.020 --> 00:09:39.140
Have you heard about Trio?

00:09:39.140 --> 00:09:45.620
Yes, I have heard about Trio. I even had Nathaniel on the show, but it's been a little while.

00:09:45.620 --> 00:09:53.540
That was back in 2018, I talked to Nathaniel. So Nathaniel Smith. So there's probably quite a few changes since then, actually.

00:09:53.540 --> 00:09:54.660
Yeah, let's talk about Trio.

00:09:54.660 --> 00:10:07.300
Yeah, actually, the last version of Trio was released just yesterday. The thing about any I/O is that it's an effort to basically bring the Trio features to async I/O land.

00:10:07.300 --> 00:10:15.060
So Trio is fundamentally incompatible with async I/O. There is a compatibility layer called Trio async I/O,

00:10:15.060 --> 00:10:32.980
async I/O, but it's far from perfect. So what any I/O does really is allow developers to add these features from Trio to their async I/O applications and libraries, one by one, without making a commitment.

00:10:32.980 --> 00:10:40.900
For example, at my work, I use any I/O for just a handful of tasks. I think we should talk about the features.

00:10:40.900 --> 00:10:48.820
Mm-hmm.

00:10:48.820 --> 00:10:56.740
Async I/O for just a few months, but the other thing about Trio is that they're not going to be able to add them to the same thing.

00:10:56.740 --> 00:11:00.660
Async I/O does not. So it's kind of a meta-async framework.

00:11:00.660 --> 00:11:04.660
I see. So it builds on top of these different frameworks, right?

00:11:04.660 --> 00:11:04.660
Yeah.

00:11:04.660 --> 00:11:08.660
Yeah. So it builds on top of these frameworks and their underlying primitives.

00:11:08.660 --> 00:11:16.260
Yeah. So for people who are not familiar, Trio adds things like this concept of grouped tasks.

00:11:16.260 --> 00:11:22.420
So normally in async I/O, you start one task, you start another. They're kind of unrelated, but

00:11:22.420 --> 00:11:27.940
even if they're conceptually solving parts of the same problem. And then with Trio, you can do things

00:11:27.940 --> 00:11:32.900
like create what's called a nursery and then you can have them all run or you could potentially cancel

00:11:32.900 --> 00:11:39.620
unstarted tasks. And there's other coordination type of operations as well, right? That's the kind of stuff

00:11:39.620 --> 00:11:40.500
that Trio adds.

00:11:40.500 --> 00:11:47.140
Yeah. So the point of any I/O is, as I said, to bring these Trio features to async I/O.

00:11:47.140 --> 00:11:51.860
Right. And because when you do Trio, it's an end-to-end async stack.

00:11:51.860 --> 00:11:56.900
Yeah. Which means things have to be built for Trio, right? It's like if I have, let's say,

00:11:56.900 --> 00:12:03.620
HTTPX. I don't know how easy it is to integrate those kinds of things that are expecting an async I/O

00:12:03.620 --> 00:12:09.620
event loop over into Trio. Well, I already have my own event loop running. It's hard to coordinate the

00:12:09.620 --> 00:12:10.340
tasks, right?

00:12:10.340 --> 00:12:16.980
If you're talking about HTTPX, it had a Trio and async I/O backends. Now it defaults to the

00:12:16.980 --> 00:12:20.260
any I/O backends. So it runs by default on both.

00:12:20.260 --> 00:12:20.820
Okay.

00:12:20.820 --> 00:12:27.380
About any I/O features, it provides Trio-like task groups on top of async I/O.

00:12:27.380 --> 00:12:27.860
Uh-huh.

00:12:27.860 --> 00:12:36.100
In here, we mentioned that Python 3.11 has its own concept of a task group, but the mechanics

00:12:36.100 --> 00:12:40.020
are quite a bit different. That requires a bit of explaining.

00:12:40.020 --> 00:12:41.860
Yeah. How does it work here?

00:12:41.860 --> 00:12:49.060
The thing is that async I/O task group, so not this one, but the standard library task groups,

00:12:49.060 --> 00:12:57.140
which are in Python 3.11, they basically just start normal async I/O tasks and you can cancel

00:12:57.140 --> 00:13:03.700
use the task objects with using the task objects that come out of the create task method.

00:13:03.700 --> 00:13:10.260
So what sets any I/O task groups apart from async I/O task groups is the way cancellation is done.

00:13:10.260 --> 00:13:20.260
And since any I/O was designed based on Trio, when you do start soon, it doesn't return any task

00:13:20.260 --> 00:13:26.820
objects that you can cancel. Instead, cancellation is done via so-called cancel scopes. So each task group

00:13:26.820 --> 00:13:36.020
has its own cancel scope. If you cancel that, you basically cancel all the underlying tasks, but it goes even deeper.

00:13:36.020 --> 00:13:43.140
Because this is a bit complicated, so bear with me. Cancellation is not done on a per-task basis,

00:13:43.140 --> 00:13:51.060
but on a per-cancel scope basis. You can have cancel scopes nested so that if you start a task and it starts

00:13:51.060 --> 00:13:57.780
a cancel scope, you can just cancel that scope and it cancels everything up to that point.

00:13:57.780 --> 00:14:04.180
Okay. So like if I call a task, if I create a task and then somewhere inside for it to do its job,

00:14:04.180 --> 00:14:10.900
it also creates a task. Those can be grouped into the same basic scope, right? So there's not these

00:14:10.900 --> 00:14:16.580
like children tasks running around. Yeah. You don't even have to start another task. If you cancel a

00:14:16.580 --> 00:14:23.620
cancel scope, then anything you basically wait on gets automatically canceled. Bam. This is called

00:14:23.620 --> 00:14:30.580
level cancellation in contrast to the edge cancellation mechanism employed by any SEKIO.

00:14:30.580 --> 00:14:41.300
In edge cancellation, you just cancel the task once and it gets canceled error raised in the task. So you can

00:14:42.100 --> 00:14:48.820
ignore it, which is by the way, bad thing to do, but then the task won't be canceled again. Usually there

00:14:48.820 --> 00:14:56.180
are exceptions to this, which are a topic of debate in the community, but the cancel scope basically,

00:14:56.180 --> 00:15:04.180
they define boundaries for cancellation. So if you say you cancel a task group's canceled scope, only the tasks

00:15:04.180 --> 00:15:11.700
started from that task group are canceled. So when those tasks return back, so all the tasks are done,

00:15:11.700 --> 00:15:18.820
then the code just goes forward from this async context manager. Basically when all the tasks are done,

00:15:18.820 --> 00:15:26.260
then however they end, unless some raised exceptions, that's a different situation. If they were either

00:15:26.260 --> 00:15:32.580
canceled or successful, then the code just goes forward to the all tasks finished part.

00:15:32.580 --> 00:15:38.340
This is really neat. The other thing that's standing out here as I think about these task groups. So for

00:15:38.340 --> 00:15:42.420
those of you listening, you can just create an async with block to create the task group. And then

00:15:42.420 --> 00:15:47.220
in there you can just say task group dot start soon and give it a bunch of async methods to start

00:15:47.220 --> 00:15:52.740
running. One of the things that's cool about this is it automatically waits for them all to be finished

00:15:52.740 --> 00:15:55.460
at the end of that context manager, the with block, right?

00:15:55.460 --> 00:15:58.260
The standard library task groups work the same way actually.

00:15:58.260 --> 00:15:59.460
Okay. And those are in 3.11?

00:15:59.460 --> 00:16:06.580
3.11? Yes. Yep. We'll see how the mechanism will work. There's a new mechanism for cancellation

00:16:06.580 --> 00:16:14.660
on cancellation of tasks. It's not really battle tested. It's something that was added fairly late

00:16:14.660 --> 00:16:23.780
in the game to 3.11. So it's not yet clear if there are edge cases where it fails totally. This is also

00:16:23.780 --> 00:16:28.900
a debated topic in the community. Sure. The other thing here that I wanted to ask you about is you don't

00:16:28.900 --> 00:16:35.780
say like create task and you don't say start, you say start soon. Why do you say what's this like

00:16:35.780 --> 00:16:42.500
uncertainty about? Tell us about that. Okay. So start soon. It's actually does the same thing as create

00:16:42.500 --> 00:16:48.180
the task because creating task doesn't start running it right away. It starts only running it on the

00:16:48.180 --> 00:16:53.220
perhaps over the next iteration of the event. Right. Or maybe not. Maybe the event loops all backed up.

00:16:53.220 --> 00:16:59.140
Maybe it's the, you know, it takes a while, right? Yeah. So it's basically the same as a loop that

00:16:59.140 --> 00:17:05.380
calls soon. So you schedule a callback. That's all that tasks are. They are callbacks with bells and

00:17:05.380 --> 00:17:12.820
whistles. Start soon is modeled based on trial, but it's, I should mention that there's also a method called

00:17:12.820 --> 00:17:20.500
start, which works a bit differently. This showcase is the start method. So this is very, very useful.

00:17:20.500 --> 00:17:27.220
This feature is not present in the standard library task groups. So basically it's very useful starting

00:17:27.220 --> 00:17:33.220
a background service that you need to know that the task has actually started before you move on.

00:17:33.220 --> 00:17:37.940
So in the example that you have on the docs here is you create a task group. And the first thing is to

00:17:37.940 --> 00:17:42.980
start a service that's listening on a port. The next thing is to talk to that service on the port.

00:17:42.980 --> 00:17:46.420
Right. And if you just say, kick them both off, who knows if that thing is actually

00:17:46.420 --> 00:17:51.140
going to be ready by the time you try to talk to it. Exactly. This is something I use in practice all

00:17:51.140 --> 00:17:56.100
the time. And this is different than what you would get just with the asyncio create task or whatever, right?

00:17:56.100 --> 00:18:02.820
Yeah. And even the new task group feature doesn't have this.

00:18:02.820 --> 00:18:08.180
This portion of talk Python is sponsored by the compiler podcast from Red Hat. Just like you,

00:18:08.180 --> 00:18:13.460
I'm a big fan of podcasts, and I'm happy to share a new one from a highly respected open source company,

00:18:13.460 --> 00:18:19.220
compiler and original podcast from Red Hat. Do you want to stay on top of tech without dedicating tons

00:18:19.220 --> 00:18:24.020
of time to it? Compiler presents perspectives, topics and insights from the tech industry,

00:18:24.020 --> 00:18:28.900
free from jargon and judgment. They want to discover where technology is headed beyond the headlines and

00:18:28.900 --> 00:18:34.100
create a place for new IT professionals to learn, grow and thrive. Compiler helps people break through

00:18:34.100 --> 00:18:38.580
the barriers and challenges turning code into community at all levels of the enterprise.

00:18:38.580 --> 00:18:44.420
One recent and interesting episode is there, the great stack debate. I love, love, love talking to

00:18:44.420 --> 00:18:49.620
people about how they architect their code, the tradeoffs and conventions they chose, and the costs,

00:18:49.620 --> 00:18:55.860
challenges and smiles that result. This great stack debate episode is like that. Check it out and see if

00:18:55.860 --> 00:19:02.020
software is more like an onion or more like lasagna or maybe even more complicated than that. It's the first episode

00:19:02.020 --> 00:19:09.540
in compiler series on software stacks. Learn more about compiler at talkpython.fm/compiler. The link is in your

00:19:09.540 --> 00:19:16.420
podcast player show notes. And yes, you could just go search for compiler and subscribe to it, but follow that link and

00:19:16.420 --> 00:19:23.060
click on your players icon to add it. That way they know you came from us. Our thanks to the compiler podcast for keeping

00:19:23.300 --> 00:19:27.140
this podcast going strong.

00:19:27.140 --> 00:19:32.740
So I guess you could in the standard library, you could start it and then you would have to just

00:19:32.740 --> 00:19:37.300
wait for it to finish and then you would carry on. But it's like two steps, right?

00:19:37.300 --> 00:19:45.300
The workaround would be to create a future, then pass that to the task and then wait on that future. So it's a

00:19:45.300 --> 00:19:55.460
bit cumbersome. And then you have to remember to use a try, accept in case that that task happens to fail. Otherwise, you end up waiting on the future forever.

00:19:55.460 --> 00:20:02.340
Mm hmm. Another, I really like this idea. Now, the other thing that I don't see in your examples here,

00:20:02.340 --> 00:20:06.260
where I'm creating a task group and starting these tasks and waiting for them to finish is

00:20:06.260 --> 00:20:12.020
management of the event loop. If I was doing that with my code, I'd probably have to create a group

00:20:12.020 --> 00:20:20.900
or a loop and then like, you know, call some functions on it. And, and here you just use any IO. What, where's the event loop being managed?

00:20:20.900 --> 00:20:26.660
I'm not sure. What do you mean? Well, like a lot of times when you're doing async stuff,

00:20:26.660 --> 00:20:31.300
you have to go and actually create an async event loop and then use the loop directly.

00:20:31.300 --> 00:20:34.740
And, you know, you're working with a loop for various things.

00:20:34.740 --> 00:20:40.020
And well, there is that a run command at the bottom. Yeah. Yeah. Okay. So basically the,

00:20:40.020 --> 00:20:45.780
you just say any IO dot run or asyncio dot run. Okay. And I can, if, even though I'm using the

00:20:45.780 --> 00:20:52.660
any IO task groups, I can still just, I can mix and match this with like more standard asyncio event

00:20:52.660 --> 00:20:57.540
loops. That's the premise. So you can just, like, ease into it. Nice.

00:20:57.540 --> 00:21:05.540
So for example, if I have a FastAPI web app and you know, FastAPI is in charge of manning the event,

00:21:05.540 --> 00:21:10.660
managing the event loop. And if I've got like an async API endpoint, I could still go and use an

00:21:10.660 --> 00:21:15.140
any IO task group and get all the benefits in there. Yep. Okay. That's beautiful.

00:21:15.140 --> 00:21:19.780
I should mention that FastAPI also depends on any IO. Oh really? Okay. Yes.

00:21:19.780 --> 00:21:24.340
How interesting. Yeah. I've seen Sebastian Ramirez talking about some, some little functions that he

00:21:24.340 --> 00:21:28.340
wrote and he's like, I would love to see these just get back into any IO. I didn't realize that

00:21:28.340 --> 00:21:36.420
FastAPI itself was using them. Yeah. Okay. So very useful. We've got these task groups. We've got the

00:21:36.420 --> 00:21:43.140
concept of cancellation. Another one that's not exactly cancellation, but as, as sort of cancellation is

00:21:43.140 --> 00:21:49.380
timeouts. Do you want to talk about how you do timeouts? Yeah. I meant to talk about that. As I recall,

00:21:50.100 --> 00:21:57.620
in Python 3.11, there is a, it's a similar construct. I think it was a with timeout or some asyncio.timeout or

00:21:57.620 --> 00:22:05.940
something similar. I don't remember really, but what this move on after does is it creates a cancel scope

00:22:05.940 --> 00:22:12.500
with a timeout. Basically, this is a very, very practical use of cancel scopes. What it does is it

00:22:12.500 --> 00:22:20.580
start a timer and after one second, it cancels this scope. So anything under that gets canceled.

00:22:20.580 --> 00:22:27.140
So in this case, just at the sleep command gets canceled and then the task just keeps going.

00:22:27.140 --> 00:22:30.980
So the way you described it before, it sounds like if there was a bunch of tasks running,

00:22:30.980 --> 00:22:35.700
if any of them try to await something, they're also going to get canceled. Is that right?

00:22:35.700 --> 00:22:40.500
In this case, you mean? Yeah. No, only the part that is within the with block.

00:22:40.500 --> 00:22:44.820
Right. Well, that's what I mean. But if you had done multiple tasks within like the move on after,

00:22:44.820 --> 00:22:49.620
right? Like I say, I try to talk to the database to insert a record and I try to call an API and

00:22:49.620 --> 00:22:55.460
the database times out. Within a single move on after block, you can only have one thing going on,

00:22:55.460 --> 00:23:01.700
which is the await here. So even if you start multiple tasks from that task group, they are not

00:23:01.700 --> 00:23:07.620
enclosed within that cancel scope. I realized that castle scopes are complex and difficult concept.

00:23:07.620 --> 00:23:13.700
And I don't think I can adequately explain them, but I hope that this will at least

00:23:13.700 --> 00:23:20.180
give some shit some light into that. Yeah. It's a really cool idea because your code could,

00:23:20.180 --> 00:23:27.300
it's async. So it's not as bad as if you were to like lock up waiting for an API call or something

00:23:27.300 --> 00:23:32.260
that's going to time out on the network eventually after a really long time. But it's still a

00:23:32.260 --> 00:23:35.620
cancellation. Still, you don't want it to clog up your code, right? You want to just say,

00:23:35.620 --> 00:23:42.020
sorry, this isn't working. Yeah. One place where I often use a construct like this is finalization.

00:23:42.020 --> 00:23:49.220
So when you are closing up things, then you can use this, this move on after to the timeout for closing

00:23:49.220 --> 00:23:54.100
resources. Yeah, that makes sense. Because you want to be a good citizen and in terms of your app and

00:23:54.100 --> 00:23:59.940
release the resources as soon as possible, like a database connection or a file handle. But if it's,

00:23:59.940 --> 00:24:04.580
if it's not working, you know, like, oh, I made it. I made a try at it after a second. We're done.

00:24:04.580 --> 00:24:11.460
Exactly. And also, I should mention that this is where any of your biggest caveat lies. It is in

00:24:11.460 --> 00:24:20.660
in finalization. I often run into problems with the cancel scopes because the thing with cancel scopes is that

00:24:20.660 --> 00:24:28.900
that when you run code within a cancel scope and that scope gets canceled, then anything awaiting on

00:24:28.900 --> 00:24:35.540
anything within that cancel scope is always canceled time after time. So you cannot wait on anything as

00:24:35.540 --> 00:24:43.060
long as you are within the cancel scope. And asyncIO code is not expecting that. So it might have a final

00:24:43.060 --> 00:24:49.940
clause where it does await, say, connection.close. But that also gets canceled if you are within an

00:24:49.940 --> 00:24:56.260
NEIO cancel scope. And it's one of the biggest practical issues with NEIO right now. And we are

00:24:56.260 --> 00:25:03.460
trying to figure out the solution for that. Just something to keep in mind when you are writing

00:25:03.460 --> 00:25:06.340
NEIO stuff. Yeah, that is tricky, right? But help us on the way.

00:25:06.340 --> 00:25:11.220
You say, well, I'm going to try to call this web service and I'm going to wait it. And if it fails,

00:25:11.220 --> 00:25:15.300
you know, probably internally, what you want to do is close the network connection as well.

00:25:15.940 --> 00:25:21.460
Right. But if you try to wait closing the network connection. Yeah. So what happens there? Does it

00:25:21.460 --> 00:25:25.540
eventually just get cleaned up by the garbage collector or dereferenced?

00:25:25.540 --> 00:25:31.220
Well, garbage collector doesn't work that well with async stuff because the destructors could be called

00:25:31.220 --> 00:25:39.300
in any thread. So you can't rely on that. You can't do any async callbacks in the destructor. So it's a

00:25:39.300 --> 00:25:45.140
better, it's a good idea not to try any of that and instead just raise a resource warning. If you're

00:25:45.140 --> 00:25:52.020
you're writing any I/O where code, you would have either this shielded cancel scope or better yet,

00:25:52.020 --> 00:25:57.380
a move on after which shield true. What does that do? At least temporarily protects the

00:25:57.380 --> 00:26:06.100
the kind of test from cancellation. So let's say you have move on after say five and with shield true.

00:26:06.100 --> 00:26:12.340
It means that even if the outer cancel scope is canceled, your actual task will start running until

00:26:12.340 --> 00:26:19.380
it exists as cancel scope or if the timeout expires. So you have a five second window to close any

00:26:19.380 --> 00:26:24.740
resources that need closing. Got it. And so you just create a, you could do that say within your exception

00:26:24.740 --> 00:26:29.380
handler or something, right? Yeah. Okay. Or actually, I think finally,

00:26:29.380 --> 00:26:32.100
a finally block might be the best place to do that. Sure.

00:26:32.100 --> 00:26:37.620
But depending on the audio use case, of course. Yeah, of course. Okay. Very interesting. So all

00:26:37.620 --> 00:26:43.540
this stuff about task groups and scheduling tasks and canceling them, that's very trio-esque, but it's

00:26:43.540 --> 00:26:51.140
also just a small part of any I/O. There's a bunch of other features and capabilities here that are

00:26:51.140 --> 00:26:57.380
probably worth going into. Some cool stuff about taking async I/O code and converting it to threads or

00:26:57.380 --> 00:27:02.020
converting threads to async I/O and similarly for sub processes. But let's maybe just

00:27:02.020 --> 00:27:08.260
talk real quick about the synchronization primitives. These are things like events, semaphores. Maybe

00:27:08.260 --> 00:27:13.140
not everyone knows what events and semaphores are in this context. Give us a quick rundown of that.

00:27:13.140 --> 00:27:19.540
Yeah. Well, these are pretty much the same as they are on async I/O. Many of them use just the async I/O

00:27:19.540 --> 00:27:28.020
counterfeits straight up. So events are a mechanism for telling another task that something happened,

00:27:28.020 --> 00:27:32.580
something significant, a significant happened, and they need to react to it. It's often used to

00:27:32.580 --> 00:27:38.260
coordinate the tasks. So one thing doesn't happen before something else has happened in another task.

00:27:38.260 --> 00:27:43.380
Yeah, there might be two tasks running and one says, I'm going to wait until this file appears.

00:27:43.380 --> 00:27:47.380
And the other one's going to eventually create the file, right? But you don't know the order.

00:27:47.380 --> 00:27:52.740
Yeah. So one option is to just do polling. Like, well, I'm going to async I/O.sleep for

00:27:52.740 --> 00:27:58.820
a little while and then see if the file's there. Try to access it, you know, and do that over and over. A much more

00:27:58.820 --> 00:28:06.260
responsive way and deterministic way would be to say, I'm going to wait on an event to be set. And the thing

00:28:06.260 --> 00:28:11.380
that creates the file will create the file and then set the event, which will kind of release that other task to carry on. Right?

00:28:11.380 --> 00:28:21.620
Right. Moving on, semaphores are a mechanism for saying that you have this limited, you have a number of limit,

00:28:21.620 --> 00:28:28.580
let's say a connection pool or something. And you want to specify that whenever some part of the code needs

00:28:28.580 --> 00:28:36.740
to access to this resource, it needs to acquire the semaphore. So you set a limit, and then each time

00:28:36.740 --> 00:28:43.460
a task enter, that's a semaphore, that decrements the counter. And when you hit the limit, then it

00:28:43.460 --> 00:28:49.060
starts blocking until something else releases it. Right. So people might be familiar with thread locks

00:28:49.060 --> 00:28:55.540
or async I/O's equivalent, where you say only one thing can access this at a time. So you don't end up

00:28:55.540 --> 00:29:01.140
with deadlocks or race conditions and so on. But semaphores are kind of like that, but they allow

00:29:01.140 --> 00:29:06.580
multiple things to happen. Say maybe your database only allows 10 connections, or you don't want to have

00:29:06.580 --> 00:29:12.580
more than 10 connections. So you could have a semaphore that says it has a limit of 10 and you have to

00:29:12.580 --> 00:29:17.220
acquire it to talk to the database. That doesn't mean it stops multiple things from happening at once. It

00:29:17.220 --> 00:29:22.660
just doesn't let it become a thousand at once. Right? Exactly. I really like this idea. And I was

00:29:22.660 --> 00:29:29.700
showing some people some web scraping work with async I/O where it's like, oh, let's go create a whole bunch of

00:29:29.700 --> 00:29:36.420
ACPX requests or whatever type of requests, you know, something asynchronous talking to some

00:29:36.420 --> 00:29:41.220
servers to download some code. And if it's a limited set, you know, no big deal. But if you have

00:29:41.220 --> 00:29:47.700
thousands of URLs to go hit, well, then how do you manage not killing your network or overloading that?

00:29:47.700 --> 00:29:52.020
And the semaphore actually would be perfect. So for people listening, the way that you do it is you

00:29:52.020 --> 00:29:57.700
create a task group and then just you pass the semaphore to start soon. That's really clean. And then

00:29:57.700 --> 00:30:02.900
any I/O takes care of just making sure it gets access and then runs and then gives it back. How's that work?

00:30:02.900 --> 00:30:09.460
I'm not sure what sort of answer you are expecting, but as I recall, this current implementation is

00:30:09.460 --> 00:30:16.500
actually using the underlying async libraries events. Okay. So there are actually methods to

00:30:16.500 --> 00:30:23.300
acquire and release the semaphores. It just implements an async context manager that acquires it at the

00:30:23.300 --> 00:30:31.780
beginning and releases it at the end. There's an event involved for notifying the any awaiting task that

00:30:31.780 --> 00:30:37.300
it has a semaphore slot available. It's super clean. And the fact that you don't have to write that code,

00:30:37.300 --> 00:30:42.100
you just say the semaphore is associated with this task through your task group. I really like it.

00:30:42.100 --> 00:30:47.460
Actually, semaphores are not associated with a particular task. That's what capacity limiters are for.

00:30:47.460 --> 00:30:55.060
Okay. So you can release a setup from another task while capacity limiters are bound to the specific

00:30:55.060 --> 00:31:00.500
task that you acquired them in. Okay. Yeah. And the semaphore example, was it being passed? Oh, yeah.

00:31:00.500 --> 00:31:06.580
It's just being passed as an argument, isn't it, to the task. And it's up to the task to use it. I see. Okay.

00:31:06.580 --> 00:31:34.580
So this other concept, the capacity limiter. Yeah. It sort of does that. Yeah. So this is from Trio. It's very similar to the semaphore. So you can set, you can actually set the borrower. But in most circumstances, you want the current task to be the borrower. And limiters are actually used in other parts of any IO as they are in Trio. For example, to limit the number of threads that you allocate,

00:31:34.580 --> 00:32:04.460
or limit the number of sub processes that you spawn. Sure. You don't have too many sub processes, right? You've got a thousand jobs and you just for each thing a job, start it in a sub process. You're going to have a bad time. Right. So as documentation says, they are quite like semaphores, but they have additional safeguards. Such as? Well, they check that the borrower is the same. Okay. Yeah, that makes sense. So by default, they check that the task they are used for both acquiring or releasing on the

00:32:04.460 --> 00:32:23.600
Yeah, nice. Okay. Well, this I didn't know about capacitor, capacity limiters. That's fantastic. I love the idea. Okay. Let's jump over to you talked about the threads, and the sub processes. Let's talk about this thread capability that you have here. This is very nice. Any IO dot two thread. What is this?

00:32:23.600 --> 00:32:45.200
Yeah, so this is also modeled based on trio. It's basically any IO's way of doing worker threads. So in asyncio, you have these thread pool executors that do the same as run sync. Async IO's API is somewhat problematic because you have basically two methods.

00:32:45.200 --> 00:33:09.820
So you have, I forget the older one, the newer one is called two thread. The first one was what it run in executor, whatever, whatever it was. They both have their own issues. Two thread doesn't allow you to specify any thread pool. So it always uses the default thread pool. And there is no way to add that to the API because it was done in such a manner.

00:33:09.820 --> 00:33:26.800
Then the older function does have this parameter at the front. But the problem is that it doesn't propagate context variables, unlike the newer two thread function. So context variables, if you don't know about them, they are a fairly recent addition to Python.

00:33:26.800 --> 00:33:27.580
Yeah, what are those?

00:33:27.580 --> 00:33:31.700
They are basically thread locals. Are you familiar with thread locals?

00:33:31.700 --> 00:33:39.700
Make a comment for people who don't know, like thread local variables, which also exist in Python, allow you to say, I'm going to have a...

00:33:39.700 --> 00:33:51.400
variable, maybe it's even a global variable, and it's thread local, which means every thread that sees it gets its own copy of the variable and where it points to and what value it is.

00:33:51.400 --> 00:33:59.640
And so that way you can initialize something at the start of a thread. If you have multiple threads, they can all kind of have their own copy so they don't have to share it.

00:33:59.820 --> 00:34:12.700
But that falls down because asyncio event loops, when you await those, all that stuff is running on one thread, just the one that's running the loop. That's what you're talking about is that equivalent, but for asyncio, right?

00:34:12.700 --> 00:34:21.080
Yeah, so context variables are much more advanced concept. They basically, yeah, as you said, thread locals for async tasks.

00:34:21.080 --> 00:34:26.780
That sounds very tricky. I've thought about that. I have no idea how to implement that. So that's pretty cool. I guess Python would know.

00:34:26.780 --> 00:34:39.380
Yeah, the thing is, when it starts running a task, when it switches between tasks, it runs that callback within that context that the task is tied to.

00:34:39.380 --> 00:35:02.120
Right. So this has been somewhat of a problem because that older method in asyncio for running worker threads, it doesn't propagate these variables, but the newer function to thread does.

00:35:02.120 --> 00:35:07.320
But then you can specify which thread pool you want to use.

00:35:07.320 --> 00:35:13.080
Right. Okay, so it's similar to the built-in one, but it gives you more capability to determine where you might run it.

00:35:13.080 --> 00:35:23.000
Yeah. When you call run sync, it allows you to specify a limiter, and it uses the default limiter, which has a capacity of 40 threads.

00:35:23.000 --> 00:35:30.900
40? That seems like a pretty good default. Much more than that, and you end up with memory and context switching issues.

00:35:30.900 --> 00:35:37.380
Yeah. It was arbitrarily set to 40, but then Nathaniel and Trio, so I just followed suit.

00:35:37.380 --> 00:35:54.080
Sure. Yeah, so basically, if you've got some function that is not async, but you want to be able to await it so that basically run it on a background thread, here you just say any IO dot to thread, and then dot run sync, and you give it the function to call.

00:35:54.740 --> 00:35:59.460
And now you can await it, and it runs on a background thread in this thread pool, which is really nice.

00:35:59.460 --> 00:36:00.640
Yeah, that's how it works.

00:36:00.640 --> 00:36:29.860
So one thing that I'm noticing a lot in the API here for any IO is often, now for the synchronous functions, it's completely obvious why you wouldn't do it, but even in the sort of creating tasks ones, what I'm noticing is that even the async functions, you pass the function name and then the arguments to like start soon, as opposed to saying, call the function, passing the arguments, and getting a coroutine back.

00:36:29.860 --> 00:36:30.860
Why does it work that way?

00:36:30.860 --> 00:36:38.620
It seems like it would make it a little less easy to use, like, say, type hands and autocomplete and various niceties of calling functions and editors.

00:36:38.620 --> 00:36:40.780
Yeah, there was a good reason for that.

00:36:40.780 --> 00:36:48.340
I can't remember that offhand, but at the very least, it's consistent with the synchronous counterforce, like run sync.

00:36:48.340 --> 00:36:49.060
Yeah, cool.

00:36:49.060 --> 00:36:55.600
All right, so we have this stuff about threads, and you have the to thread, also from thread, which is nice.

00:36:55.600 --> 00:36:56.740
What's from thread to?

00:36:56.740 --> 00:37:08.480
Yeah, so when you are in a worker thread, and you occasionally need to call something in the event loop thread, then you need to use this from thread to run stuff on event loop thread.

00:37:08.480 --> 00:37:12.260
Right, because the worker method is not async.

00:37:12.260 --> 00:37:13.640
Otherwise, you would just await it, right?

00:37:13.700 --> 00:37:15.200
It's a regular function.

00:37:15.200 --> 00:37:20.140
But if in that regular function, you want to be able to await a thing, you can kind of reverse back.

00:37:20.140 --> 00:37:20.740
I see.

00:37:20.740 --> 00:37:22.600
That's an interesting bi-directional aspect.

00:37:22.600 --> 00:37:24.460
All right, subprocesses.

00:37:24.600 --> 00:37:31.140
We all know about the GIL, how Python doesn't necessarily love doing computational work across processes.

00:37:31.140 --> 00:37:34.180
Tell us about the subprocess equivalent.

00:37:34.180 --> 00:37:46.680
This is a relatively easy way to both run tasks in a subprocess and then opening arbitrary executables for running asynchronously.

00:37:46.680 --> 00:38:05.320
AsyncIO has similar facilities for running async processes, but these async subprocess facilities are not really up to par with, say, multiprocessing, which has some additional nice things like async shared queues and other synchronization primitives.

00:38:05.320 --> 00:38:09.620
But they are still pretty useful as they are.

00:38:09.620 --> 00:38:09.900
Yeah.

00:38:09.900 --> 00:38:20.040
So basically, you can just say anyIO.toProcess run sync and you give it a function and now you can await that subprocess multiprocessing.

00:38:20.040 --> 00:38:20.360
Yeah.

00:38:20.360 --> 00:38:30.460
There are the usual caveats like because they don't share memory, then you have to serialize the arguments and that could be a problem in some cases.

00:38:30.460 --> 00:38:30.700
Sure.

00:38:30.700 --> 00:38:34.580
So basically, it pickles the arguments and the return values.

00:38:35.040 --> 00:38:35.840
It sends them over.

00:38:35.840 --> 00:38:36.260
Yeah.

00:38:36.260 --> 00:38:44.060
It could be even so that the arguments are pickable, but the return value is not, which obviously causes some confusion.

00:38:44.060 --> 00:38:44.380
Yeah.

00:38:44.380 --> 00:38:44.740
Yeah.

00:38:44.740 --> 00:38:45.180
Absolutely.

00:38:45.180 --> 00:38:54.740
People maybe hear that pickling is bad and you can have all sorts of challenges like code injection and whatnot from pickling through security.

00:38:54.740 --> 00:39:00.680
So this, I would think, is not really subject to that because it's you calling the function directly.

00:39:00.680 --> 00:39:01.980
Right.

00:39:01.980 --> 00:39:03.720
It's like completely just inside.

00:39:03.720 --> 00:39:05.860
There's no way to inject anything bad.

00:39:05.860 --> 00:39:06.460
Yeah, exactly.

00:39:06.460 --> 00:39:09.040
It's all the multiprocessing that's handling it anyway.

00:39:09.040 --> 00:39:09.920
So it should be okay.

00:39:09.920 --> 00:39:10.200
Yeah.

00:39:10.200 --> 00:39:10.420
Yeah.

00:39:10.420 --> 00:39:10.620
Cool.

00:39:10.620 --> 00:39:14.880
Another one that is pretty exciting has to do with file support.

00:39:15.120 --> 00:39:18.220
So we have open in Python.

00:39:18.220 --> 00:39:23.000
And you would say with open something as F, right?

00:39:23.000 --> 00:39:25.620
But that's, there's no async equivalent, right?

00:39:25.620 --> 00:39:25.960
Yeah.

00:39:25.960 --> 00:39:33.200
These file facilities are really just a convenience that wraps pictures around these file objects.

00:39:33.200 --> 00:39:33.600
Yeah.

00:39:33.660 --> 00:39:40.660
So if somebody wants to know, there is no actual async file IO happening because that's not really a thing on Linux.

00:39:40.660 --> 00:39:44.140
And even on Windows, it has terrible problems.

00:39:44.140 --> 00:39:44.500
Okay.

00:39:44.500 --> 00:39:47.280
So what's happening with this any IO dot open file?

00:39:47.280 --> 00:39:55.680
It opens a file in a thread and then this opens, it starts an async context manager that on exit closes the file.

00:39:55.680 --> 00:39:57.660
And it also closes the thread, I guess, right?

00:39:57.780 --> 00:39:59.520
Well, it uses throwaway threads.

00:39:59.520 --> 00:39:59.820
Okay.

00:39:59.820 --> 00:40:01.200
Basically from the thread pool.

00:40:01.200 --> 00:40:01.540
All right.

00:40:01.540 --> 00:40:01.820
I see.

00:40:01.820 --> 00:40:07.140
So it'll use a thread to open the file and get the file handle and then throw away the thread.

00:40:07.140 --> 00:40:09.200
And then does it do something similar?

00:40:09.200 --> 00:40:10.600
Well, return the thread to the pool.

00:40:10.600 --> 00:40:10.960
Yeah.

00:40:10.960 --> 00:40:11.180
Yeah.

00:40:11.180 --> 00:40:17.120
Return to the pool, which is much better than creating it and throwing it away completely.

00:40:17.120 --> 00:40:19.920
Also that read call is done in the shed.

00:40:19.920 --> 00:40:20.300
Okay.

00:40:20.300 --> 00:40:23.940
So it just is sort of a fancy layer over top of thread pool.

00:40:23.940 --> 00:40:25.460
But it's really, really nice.

00:40:25.460 --> 00:40:33.060
You write basically exactly the same code that you would write with regular open and a context manager, but the async version.

00:40:33.060 --> 00:40:40.380
Although I got to say that the opening part of creating a context manager here, you say async with, which people are probably used to.

00:40:40.380 --> 00:40:45.520
And then async with await open file, which is, it's a bit of a mouthful.

00:40:45.520 --> 00:40:53.420
The reason for that is because you can just do await open file and then go about your business and then manually close the file.

00:40:53.420 --> 00:40:53.800
Got it.

00:40:53.800 --> 00:40:57.000
I'm not sure if there's a more convenient way to do this.

00:40:57.000 --> 00:41:01.680
I might be open to adding that to any IO, but for the moment, this is how you do it.

00:41:01.800 --> 00:41:06.480
Yeah, you could probably wrap it in some kind of class or something that's synchronous, but then has an A enter.

00:41:06.480 --> 00:41:09.640
But yeah, I'm not sure if it's totally worth it.

00:41:09.640 --> 00:41:11.740
But yeah, that's quite the statement there.

00:41:11.740 --> 00:41:25.180
And the other area that's interesting about this is if you want to loop over it line for line, instead of doing a regular for loop, you can do an async for line in file and then read it asynchronously line by line, right?

00:41:25.300 --> 00:41:25.540
Yeah.

00:41:25.540 --> 00:41:30.640
So the A next method just gets the next line in the worker side.

00:41:30.640 --> 00:41:31.900
Yeah, this is fantastic.

00:41:31.900 --> 00:41:33.420
So people are working with files.

00:41:33.420 --> 00:41:34.920
They definitely can check this out.

00:41:34.920 --> 00:41:40.380
And also related to that is you have an asynchronous pathlib path.

00:41:40.380 --> 00:41:42.280
Yeah, that's a fairly recent addition.

00:41:42.280 --> 00:41:42.980
Yeah, it looks great.

00:41:43.320 --> 00:41:46.180
So you have like an async iterator.

00:41:46.180 --> 00:41:53.580
You have an async, is it a file, async read text, all built into the path, which, you know, is like what's built into a regular path, but not asynchronous.

00:41:53.580 --> 00:41:55.340
Yeah, quite nice.

00:41:55.340 --> 00:41:57.200
We're getting kind of short on time here.

00:41:57.200 --> 00:41:58.880
What else would you like to highlight here?

00:41:58.880 --> 00:41:59.880
That's really important.

00:41:59.880 --> 00:42:05.660
I would like to highlight the streaming framework here because it's one of the unique things in any IO.

00:42:05.660 --> 00:42:05.980
Okay.

00:42:05.980 --> 00:42:06.900
Yeah, let's talk about it.

00:42:06.900 --> 00:42:10.820
Trio has its channels and then it has Socas and whatnot.

00:42:11.260 --> 00:42:14.380
But any IO has something that Trio doesn't have.

00:42:14.380 --> 00:42:21.280
And really, asyncio has some kind of streaming abstraction, but not quite on this level.

00:42:21.280 --> 00:42:25.800
In any IO, we have a streaming abstract base class hierarchy.

00:42:25.800 --> 00:42:29.800
We have object streams and byte streams.

00:42:29.800 --> 00:42:38.720
The difference between these are that object streams can have anything like the integers, strings, any arbitrary objects.

00:42:39.240 --> 00:42:44.440
And byte streams have only bytes and they are modeled according to TCP.

00:42:44.440 --> 00:42:51.120
With byte streams, you can send a number of bytes or receive bytes, but they might be chugged differently.

00:42:51.120 --> 00:42:53.060
These are abstract streams.

00:42:53.060 --> 00:42:57.600
So you can, say, build a library that wraps another stream.

00:42:57.600 --> 00:43:03.880
Say you build an SSH client that creates a tunnel and it exposes that as a stream.

00:43:04.260 --> 00:43:14.440
So long as you are able to consume a stream, you don't have to care how the stream works internally or what other streams it wraps.

00:43:14.640 --> 00:43:19.540
A good example of a stream wrapper is the TLS support.

00:43:19.540 --> 00:43:20.900
That's right there.

00:43:20.900 --> 00:43:32.220
So TLS support in any IO can wrap any existing stream that gives you bytes, even if it's an object, whether it's an object on or byte stream.

00:43:32.640 --> 00:43:35.520
So it does the handshake using the standard library.

00:43:35.520 --> 00:43:40.980
Actually, standard library contains this sense IO protocol for TLS.

00:43:40.980 --> 00:43:41.400
Okay.

00:43:41.400 --> 00:43:48.760
Sense IO protocol, if you are not aware of it, it's basically a state machine without any actual IO calls.

00:43:48.900 --> 00:43:54.600
It's a very neat protocol implementation that lets you add whatever kind of IO layer you want on top of that.

00:43:54.600 --> 00:43:57.820
So this is what any IO uses to implement TLS.

00:43:57.820 --> 00:44:02.940
So you have both a listener and a connect and a TLS wrapper.

00:44:02.940 --> 00:44:07.480
So any kind of TLS, you can even do TLS on top of TLS if you like.

00:44:07.480 --> 00:44:10.340
I'm not sure that's useful, but you can do it.

00:44:10.340 --> 00:44:11.500
It's super encrypted.

00:44:12.540 --> 00:44:14.640
Yeah, this is very flexible.

00:44:14.640 --> 00:44:21.520
We have all sorts of streams that even these unreliable streams, which are modeled based on UDP.

00:44:21.520 --> 00:44:22.040
Oh, really?

00:44:22.040 --> 00:44:22.360
Okay.

00:44:22.360 --> 00:44:28.040
So UDP is implemented by using these unreliable streams.

00:44:28.040 --> 00:44:32.600
I don't think there are any more unreliable streams implementations than UDP.

00:44:32.600 --> 00:44:36.920
But yeah, I'm really part of this streaming class hierarchy.

00:44:37.220 --> 00:44:42.580
And it's too bad that there are no cool projects to show off this system.

00:44:42.580 --> 00:44:44.640
But maybe in the future we will have.

00:44:44.640 --> 00:44:50.040
Another thing that I would like to highlight is a system of type attributes.

00:44:50.040 --> 00:44:52.140
That has been really useful in practice.

00:44:52.140 --> 00:44:55.360
Yeah, before we move on to the type attributes, let me just ask you real quickly.

00:44:55.360 --> 00:45:02.180
Can I use these streams and these bidirectional streams where you create like a send stream and receive stream?

00:45:02.180 --> 00:45:08.300
Can we use those across like multiprocessing or to coordinate threads?

00:45:08.300 --> 00:45:09.820
I'm sure we could use it for threads, right?

00:45:09.820 --> 00:45:11.060
Sadly, no.

00:45:11.060 --> 00:45:12.040
Sadly, no.

00:45:12.040 --> 00:45:16.720
This memory object stream is, as they call it in TRIO channel.

00:45:16.720 --> 00:45:20.840
This is one of the most useful pieces of NEI, really.

00:45:20.840 --> 00:45:22.980
I use this every day at work.

00:45:22.980 --> 00:45:25.540
So these are basically like cues on steroids.

00:45:25.540 --> 00:45:26.400
Yeah, exactly.

00:45:26.400 --> 00:45:27.240
That's what I was thinking.

00:45:27.240 --> 00:45:30.880
Unlike ASYNCIO cues, you can actually close these.

00:45:30.880 --> 00:45:32.000
You can clone them.

00:45:32.000 --> 00:45:36.940
So you can have multiple tasks waiting to receive something like workers.

00:45:36.940 --> 00:45:39.160
And we have multiple senders.

00:45:39.160 --> 00:45:39.560
I see.

00:45:39.560 --> 00:45:45.140
Like a producer consumer where some things are put in, but there's like five workers who might grab a job and work on it.

00:45:45.140 --> 00:45:48.660
You can have five consumers and five producers all talking to each other.

00:45:48.660 --> 00:45:49.140
Yeah.

00:45:49.140 --> 00:45:51.840
And then you can just iterate over them.

00:45:51.840 --> 00:45:53.700
Also not possible with cues.

00:45:53.700 --> 00:46:03.240
You can close the cues, sorry, streams, so that when you iterate on them, if all the other other ints are closed, then the iterator adjusts the ints.

00:46:03.240 --> 00:46:03.700
Oh, wow.

00:46:03.700 --> 00:46:04.060
Okay.

00:46:04.440 --> 00:46:08.700
So if the send stream goes away, then the receive stream is done.

00:46:08.700 --> 00:46:09.600
It's at the end.

00:46:09.600 --> 00:46:09.880
Yeah.

00:46:09.880 --> 00:46:11.340
That's fantastic, actually.

00:46:11.340 --> 00:46:12.320
I really like that.

00:46:12.320 --> 00:46:12.620
Yeah.

00:46:12.620 --> 00:46:14.420
I think this is also coming to...

00:46:14.420 --> 00:46:14.640
Go ahead.

00:46:14.640 --> 00:46:14.800
Sorry.

00:46:14.800 --> 00:46:15.440
...asyncIO.

00:46:15.640 --> 00:46:15.860
Okay.

00:46:15.860 --> 00:46:18.080
I think this is also coming to the standard library.

00:46:18.080 --> 00:46:19.080
Nice.

00:46:19.080 --> 00:46:19.740
All right.

00:46:19.740 --> 00:46:20.140
At some point.

00:46:20.140 --> 00:46:25.240
Last thing we probably have time for here that you wanted to highlight is typed attributes.

00:46:25.240 --> 00:46:26.580
What's the story of typed attributes?

00:46:26.580 --> 00:46:30.480
If you knew about the asyncIO extras in...

00:46:30.480 --> 00:46:33.220
I think they have both in protocols and streams.

00:46:33.220 --> 00:46:39.100
For example, with TLS, you want to get the certificate from the stream.

00:46:39.100 --> 00:46:44.580
Like if you have negotiated a TLS stream, you want to get the client certificate, right?

00:46:44.580 --> 00:46:47.280
So this is a type of way to do it.

00:46:47.280 --> 00:46:50.600
You can add any arbitrary extra attributes to a stream.

00:46:50.600 --> 00:46:54.180
Just declare it in the extra attributes method.

00:46:54.180 --> 00:46:58.660
But the niftiest part here is that it can work across wrapped streams.

00:46:58.660 --> 00:47:03.900
A very good example is that say you have a stream that is based on HTTP.

00:47:03.900 --> 00:47:08.880
You have an HTTP server and you have access to a stream, let's say, WebSockets.

00:47:08.880 --> 00:47:12.800
Then you want to get the client IP address.

00:47:12.800 --> 00:47:18.740
Well, usually you may have a front-end web server like Nginx at the front.

00:47:18.740 --> 00:47:24.320
Normally, what you would get when you ask for an IP address, you actually get the IP address of the server.

00:47:24.320 --> 00:47:27.300
What you need to do is look at the headers.

00:47:27.460 --> 00:47:30.760
This is something you can do transparently with these type attributes.

00:47:30.760 --> 00:47:40.020
So basically, wrap a stream that understands HTTP, you can have that handle the request for the IP address, the remote IP address,

00:47:40.020 --> 00:47:45.100
and have it look at the headers and look for a forwarded header and return that instead.

00:47:45.100 --> 00:47:45.660
Nice.

00:47:45.660 --> 00:47:47.260
Let me see if I got this right here.

00:47:47.260 --> 00:47:49.940
So people are probably familiar with Pydantic.

00:47:49.940 --> 00:47:56.120
And Pydantic allows you to create a class and it says what types are in the class and the names and so on.

00:47:56.720 --> 00:48:00.020
And those serialize out a JSON message.

00:48:00.180 --> 00:48:14.580
It sounds to me like what this is built for is when I'm talking binary messages over a stream like a TCP stream, I can create a similar class that says, well, I expect a message that is a string and then a float.

00:48:14.580 --> 00:48:16.280
Read that out of the stream.

00:48:16.280 --> 00:48:17.100
Is that right?

00:48:17.320 --> 00:48:17.920
Yeah.

00:48:17.920 --> 00:48:21.620
A good example here is if you go back to the streams part.

00:48:21.620 --> 00:48:22.580
Text streams.

00:48:22.580 --> 00:48:22.900
Okay.

00:48:23.060 --> 00:48:28.780
So this is something that translates between bytes and streams on the fly.

00:48:28.780 --> 00:48:32.740
So this is a perfect trivial example of a stream prepper.

00:48:32.740 --> 00:48:33.100
Okay.

00:48:33.100 --> 00:48:33.760
Yeah.

00:48:33.760 --> 00:48:37.240
So you have a text received stream that will do the byte decoding.

00:48:37.240 --> 00:48:37.660
Yeah.

00:48:37.660 --> 00:48:37.860
Yeah.

00:48:37.860 --> 00:48:38.360
Very nice.

00:48:38.360 --> 00:48:41.580
And you don't have to care like what's downstream of that.

00:48:41.580 --> 00:48:49.720
And even if you have three layers on top, you can just still ask for, say, client remote IP address.

00:48:49.720 --> 00:48:54.940
If there's a network stream somewhere downstream, that's the stream that will give you your answer.

00:48:55.100 --> 00:48:55.260
Cool.

00:48:55.260 --> 00:48:55.900
Yeah.

00:48:55.900 --> 00:48:58.100
The stream work here is really nice.

00:48:58.100 --> 00:48:59.440
There's a lot of things that are nice.

00:48:59.440 --> 00:49:08.380
The coordinating, the task groups, the coordinating limitations, like with a limiter, capacity limiter.

00:49:08.380 --> 00:49:11.020
A lot of cool building blocks on top of it.

00:49:11.020 --> 00:49:20.820
And also the fact that this runs against or integrates with regular asyncio means you don't have to completely change your whole system in order to use it.

00:49:20.820 --> 00:49:21.020
Right?

00:49:21.020 --> 00:49:21.240
Yeah.

00:49:21.240 --> 00:49:21.820
Very cool.

00:49:21.820 --> 00:49:22.320
All right.

00:49:22.320 --> 00:49:24.820
Well, I think we're about out of time to talk about any IO.

00:49:24.820 --> 00:49:31.480
Do you want to take 30 seconds and just give the elevator pitch for SQL A code gen?

00:49:31.480 --> 00:49:34.040
This is a really exciting project that you created here.

00:49:34.040 --> 00:49:34.480
Yeah.

00:49:34.480 --> 00:49:38.740
This is one of those side projects that are on the verge of a major release.

00:49:38.740 --> 00:49:44.140
So what this does is it takes an existing database.

00:49:44.140 --> 00:49:51.440
It connects to an existing database, reflects the schema from that, and then writes modal code for you.

00:49:51.640 --> 00:49:57.040
The next major version even supports data classes and other kinds of formats.

00:49:57.040 --> 00:50:05.500
The SQL model one is the one that's most exciting for me because that'll give you Pydantic models that use a SQL model, which is very exciting.

00:50:05.500 --> 00:50:06.300
Yeah.

00:50:06.300 --> 00:50:06.960
So nice.

00:50:06.960 --> 00:50:07.540
What else?

00:50:07.700 --> 00:50:12.540
Think if you're a consultant or you talked about Java earlier, right?

00:50:12.540 --> 00:50:18.900
Imagine you've got like a Java code base and you want to do a proof of concept in Python and SQL model or SQLAlchemy.

00:50:18.900 --> 00:50:24.280
And somebody says, well, why don't you try building a simple version that talks to our database?

00:50:24.540 --> 00:50:31.500
And if that thing has like a hundred tables and complicated relationships, it's no fun to sit down.

00:50:31.500 --> 00:50:34.800
Like a big portion of that project might be just modeling the database.

00:50:34.800 --> 00:50:38.200
And here I can just say SQL A code gen, connect to Postgres.

00:50:38.200 --> 00:50:39.060
Boom.

00:50:39.060 --> 00:50:40.920
Out comes SQLAlchemy classes.

00:50:40.920 --> 00:50:43.880
That's a huge jumpstart for getting started.

00:50:43.880 --> 00:50:45.960
Or if you're a consultant jumping into a new project.

00:50:45.960 --> 00:50:46.500
Yeah, exactly.

00:50:46.500 --> 00:50:51.760
If you have a really large database in this, we will save little hours, you'll feel time.

00:50:51.980 --> 00:50:53.400
At least hours, yes.

00:50:53.400 --> 00:50:56.020
And a lot of frustration, right?

00:50:56.020 --> 00:51:01.400
Because with like SQLAlchemy, you've got to have the model match the database just right.

00:51:01.400 --> 00:51:03.020
And this will do that for you.

00:51:03.020 --> 00:51:03.580
Yeah.

00:51:03.580 --> 00:51:04.380
Okay.

00:51:04.380 --> 00:51:05.240
Super cool project.

00:51:05.240 --> 00:51:06.700
Typeguard is another one you have.

00:51:06.700 --> 00:51:10.640
Not super complicated, but an interesting capability to grab on.

00:51:10.640 --> 00:51:11.000
Yeah.

00:51:11.000 --> 00:51:16.740
Also one of those that are on major of a version of a major release.

00:51:16.740 --> 00:51:21.740
I sadly have not had enough time to finish the next major version.

00:51:21.740 --> 00:51:31.520
And then there's of course, Python 3.11, which brings a whole bunch of new features that I have not been able to yet incorporate into Typeguard.

00:51:31.520 --> 00:51:37.100
And sadly, I also have not started using it myself.

00:51:37.100 --> 00:51:40.540
It's a sad story really.

00:51:40.540 --> 00:51:40.760
Yeah.

00:51:40.760 --> 00:51:41.080
Okay.

00:51:41.300 --> 00:51:45.120
But the premise is that you have this pytest plugin.

00:51:45.120 --> 00:51:47.780
You activate it during the test run.

00:51:47.780 --> 00:51:57.000
And then in addition to static type checking your application, which you usually do with mypy, PyRite or what have you.

00:51:57.000 --> 00:52:04.780
You can also do a runtime type checking because the static tools don't always see the correct types.

00:52:05.000 --> 00:52:06.840
You might not be in control of that, right?

00:52:06.840 --> 00:52:08.300
You might write a library.

00:52:08.300 --> 00:52:11.820
Your library might have types declared on what it's supposed to take.

00:52:11.820 --> 00:52:16.540
The person consuming your library has no requirement to run mypy.

00:52:16.840 --> 00:52:21.380
And they have no requirement to make sure what they typed matches what you said you expect.

00:52:21.380 --> 00:52:25.720
And because they're hints, they're not compiled options in Python.

00:52:25.720 --> 00:52:30.800
It might at runtime, you might get something you don't expect, even though you put a type there.

00:52:30.800 --> 00:52:31.780
Yeah, exactly.

00:52:31.780 --> 00:52:37.100
So this is how you get the runtime assurance that you have the right type there.

00:52:37.100 --> 00:52:37.380
Nice.

00:52:37.480 --> 00:52:42.900
So all you do with this type guard library is you put an at type checked decorator on a function?

00:52:42.900 --> 00:52:46.060
The best way would be to use the import hook.

00:52:46.060 --> 00:52:46.400
Okay.

00:52:46.400 --> 00:52:52.700
So there's an import hook that will automatically add these decorators while doing the import.

00:52:52.700 --> 00:52:55.060
So you don't have to alter your code at all.

00:52:55.060 --> 00:52:58.100
There are some open issues with that import hook.

00:52:58.100 --> 00:53:02.700
Like somebody reported that this import hook is installed too late.

00:53:02.700 --> 00:53:06.360
The modules in question were already imported.

00:53:06.760 --> 00:53:10.740
So that's something I have yet to fix or find a workaround for.

00:53:10.740 --> 00:53:10.980
Sure.

00:53:10.980 --> 00:53:11.980
That's the idea.

00:53:11.980 --> 00:53:12.280
Right.

00:53:12.280 --> 00:53:18.160
So you can either use the decorator and be somewhat guarded about how you're doing it and only apply

00:53:18.160 --> 00:53:20.040
to certain parts, like say your public API.

00:53:20.040 --> 00:53:25.260
Or you could just say install import hook and then everything that gets imported gets wrapped

00:53:25.260 --> 00:53:26.660
in the type checked decorator.

00:53:26.660 --> 00:53:32.120
And what that does is it looks at the type hints and the declared return value and will raise

00:53:32.120 --> 00:53:36.740
an exception if say you say that your function takes an integer and it's passed a string.

00:53:36.740 --> 00:53:38.320
That becomes a runtime error.

00:53:38.320 --> 00:53:38.680
Right.

00:53:38.680 --> 00:53:41.340
Or you can just issue a warning.

00:53:41.340 --> 00:53:41.620
Sure.

00:53:41.620 --> 00:53:44.960
The warning may be nice, but it's still, I think it's pretty cool.

00:53:44.960 --> 00:53:49.960
You can opt into having Python type hints become enforced basically.

00:53:49.960 --> 00:53:50.360
Yeah.

00:53:50.360 --> 00:53:53.460
What I use this for is in the asphalt.

00:53:53.460 --> 00:54:01.220
When I accept the configuration for a component, I use this decorator or rather an assert to

00:54:01.220 --> 00:54:03.460
check that the types are correct.

00:54:03.460 --> 00:54:09.180
So I don't, I don't raise any mysterious warning type errors or value errors further down the

00:54:09.180 --> 00:54:09.680
line.

00:54:09.680 --> 00:54:11.180
Or even worse at runtime.

00:54:11.180 --> 00:54:11.580
Sure.

00:54:11.580 --> 00:54:12.280
Okay.

00:54:12.280 --> 00:54:13.020
That's, that's cool.

00:54:13.020 --> 00:54:19.900
There is one of the features of asphalt or highlights is runtime type checking for development

00:54:19.900 --> 00:54:24.420
and testing to fail early when functions are called with incompatible arguments and can

00:54:24.420 --> 00:54:26.080
be disabled for zero overhead.

00:54:26.280 --> 00:54:31.800
So it sounds like, you know, you're maybe doing an import hook in development mode.

00:54:31.800 --> 00:54:33.440
That's handling all this for you.

00:54:33.440 --> 00:54:33.860
Is that right?

00:54:33.860 --> 00:54:37.000
Well, actually in this current version, I'm using the assert.

00:54:37.000 --> 00:54:42.800
So in case you didn't know when you have asserts, they are normally run without any switches

00:54:42.800 --> 00:54:43.320
to Python.

00:54:43.320 --> 00:54:48.960
But if you run Python without the debug mode, then asserts are not compiled in the bytecode.

00:54:49.460 --> 00:54:54.600
So just by using this switch, you can disable these potentially expensive asserts.

00:54:54.600 --> 00:54:54.800
Yeah.

00:54:54.800 --> 00:54:55.060
Okay.

00:54:55.060 --> 00:54:55.980
I didn't know that.

00:54:55.980 --> 00:55:00.840
I'm familiar with that from C and C# and other compiled languages with their pragmas

00:55:00.840 --> 00:55:02.200
and that type of thing.

00:55:02.200 --> 00:55:04.400
But I didn't realize that about Python asserts.

00:55:04.400 --> 00:55:04.680
Yeah.

00:55:04.680 --> 00:55:11.400
There's this one thing that actually, if you have this code, if under debug, and there's a

00:55:11.400 --> 00:55:16.700
bus of code under that block, that whole block gets omitted from the compiled code.

00:55:16.780 --> 00:55:20.100
If you run Python with the debug mode disabled.

00:55:20.100 --> 00:55:20.620
Okay.

00:55:20.620 --> 00:55:21.260
Yeah.

00:55:21.260 --> 00:55:21.620
Very cool.

00:55:21.620 --> 00:55:22.660
All right, Alex.

00:55:22.660 --> 00:55:24.780
Well, those are some cool additional projects.

00:55:24.780 --> 00:55:28.960
You know, I feel like the SQL A code gen, we almost could spend a whole bunch of other

00:55:28.960 --> 00:55:29.400
time on it.

00:55:29.400 --> 00:55:31.260
Another one is the AP scheduler.

00:55:31.260 --> 00:55:35.520
Again, could almost be its own show, but we're out of time for this one.

00:55:35.520 --> 00:55:36.920
So thanks so much for being here.

00:55:36.920 --> 00:55:39.820
Now, before you get out of here, I've got the two final questions to ask you.

00:55:39.820 --> 00:55:43.100
If you're going to write some Python code, what editor do you use?

00:55:43.100 --> 00:55:46.320
I've been looking at the different editors available.

00:55:46.320 --> 00:55:50.020
And so far, PyCharm wins, hands down.

00:55:50.020 --> 00:55:50.480
Right on.

00:55:50.480 --> 00:55:51.320
I'm with you there.

00:55:51.320 --> 00:55:58.960
So it has so many of these intelligent features and what have you that, for example, I use its

00:55:58.960 --> 00:56:01.460
database features to browse through my database.

00:56:01.800 --> 00:56:06.600
I use its refactoring features to change my code relatively safely.

00:56:06.600 --> 00:56:10.540
And its Docker support gives me auto-completion.

00:56:10.540 --> 00:56:12.880
The list goes on and on and on.

00:56:12.880 --> 00:56:17.440
And most of these IDs are not nearly as sophisticated.

00:56:17.440 --> 00:56:18.020
I agree.

00:56:18.020 --> 00:56:18.940
Excellent one.

00:56:18.940 --> 00:56:21.140
Now, notable PyPI package.

00:56:21.140 --> 00:56:22.740
I mean, we talked about a bunch.

00:56:22.740 --> 00:56:24.420
You can recommend any of these we talked about.

00:56:24.420 --> 00:56:27.060
You can say something else you found interesting.

00:56:27.060 --> 00:56:33.140
Well, I think I already mentioned Trio, but this is a difficult question, really.

00:56:33.140 --> 00:56:34.040
Maybe poetry.

00:56:34.040 --> 00:56:34.460
Okay.

00:56:34.460 --> 00:56:34.820
Yeah.

00:56:34.820 --> 00:56:35.180
Poetry.

00:56:35.180 --> 00:56:35.640
Yeah.

00:56:35.640 --> 00:56:38.940
Poetry is something that I use for my application at work.

00:56:38.940 --> 00:56:42.220
It's the closest thing in Fison to, say, Yarn.

00:56:42.220 --> 00:56:47.800
So I manage the dependencies and lock down the dependencies using poetry.

00:56:47.800 --> 00:56:50.120
It's quite handy for that.

00:56:50.120 --> 00:56:56.140
There are some issues with poetry, like when I just need to update one dependency,

00:56:56.140 --> 00:56:59.960
update them all, and small issues like that.

00:56:59.960 --> 00:57:01.820
But other than that, it's great.

00:57:01.820 --> 00:57:02.080
Yeah.

00:57:02.080 --> 00:57:03.020
It looks really great.

00:57:03.020 --> 00:57:04.400
I know a lot of people are loving poetry.

00:57:04.400 --> 00:57:06.400
It's a good recommendation there.

00:57:06.400 --> 00:57:07.200
All right.

00:57:07.200 --> 00:57:07.980
Final call to action.

00:57:07.980 --> 00:57:09.980
People are interested in any I.O.

00:57:09.980 --> 00:57:11.180
How do they get started?

00:57:11.180 --> 00:57:15.360
Well, there's somewhat of a tutorial there.

00:57:15.360 --> 00:57:18.560
I really don't have a really long tutorial on it.

00:57:18.560 --> 00:57:22.700
I like Trio, so I'm heavily leaning on Trio's documentation here.

00:57:22.700 --> 00:57:28.520
Because any I.O. has such a similar design to Trio, then a lot of Trio's manual can be

00:57:28.520 --> 00:57:31.300
used to draw parallels to any I.O.

00:57:31.300 --> 00:57:37.180
You can almost use Trio's documentation, the tutorial, to learn how any I.O. works.

00:57:37.300 --> 00:57:39.340
Yeah, it's highly inspired, right?

00:57:39.340 --> 00:57:43.020
Anything else, then you should just come to Gitter.

00:57:43.020 --> 00:57:46.160
I think there's a link getting help at the bottom.

00:57:46.160 --> 00:57:46.420
Okay.

00:57:46.420 --> 00:57:46.820
Yeah.

00:57:46.820 --> 00:57:50.540
So there's a Gitter link, and I'm usually available there.

00:57:50.540 --> 00:57:50.820
Great.

00:57:50.820 --> 00:57:51.560
Okay.

00:57:51.560 --> 00:57:51.940
Yeah.

00:57:51.940 --> 00:57:52.580
Very, very nice.

00:57:52.580 --> 00:57:54.840
And I'm guessing you accept contributions?

00:57:54.840 --> 00:57:55.460
Sure.

00:57:55.460 --> 00:57:55.780
Yeah.

00:57:55.780 --> 00:57:56.640
So, yeah.

00:57:56.640 --> 00:57:57.160
Let's see.

00:57:57.160 --> 00:58:00.020
Over here, we've got, what is that?

00:58:00.020 --> 00:58:00.900
33 contributors?

00:58:00.900 --> 00:58:01.460
So, yeah.

00:58:01.460 --> 00:58:01.940
Excellent.

00:58:01.940 --> 00:58:06.820
If people want to contribute to the project, and maybe that's code, or maybe even they

00:58:06.820 --> 00:58:09.860
could put together a tutorial or something like that if they're interested.

00:58:09.860 --> 00:58:10.200
Maybe.

00:58:10.200 --> 00:58:10.440
Yeah.

00:58:10.440 --> 00:58:10.880
Perhaps.

00:58:10.880 --> 00:58:11.120
Okay.

00:58:11.120 --> 00:58:11.900
Excellent.

00:58:11.900 --> 00:58:16.040
Well, thank you for all the cool libraries, and take the time to come share them with us.

00:58:16.040 --> 00:58:16.800
Thanks for having me.

00:58:16.800 --> 00:58:17.060
Yeah.

00:58:17.060 --> 00:58:17.560
You bet.

00:58:17.560 --> 00:58:18.140
Bye.

00:58:18.140 --> 00:58:19.080
Thanks, everyone, for listening.

00:58:19.080 --> 00:58:19.420
Bye.

00:58:19.420 --> 00:58:23.220
This has been another episode of Talk Python To Me.

00:58:23.220 --> 00:58:25.040
Thank you to our sponsors.

00:58:25.040 --> 00:58:26.640
Be sure to check out what they're offering.

00:58:26.640 --> 00:58:28.060
It really helps support the show.

00:58:29.020 --> 00:58:32.680
Listen to an episode of Compiler, an original podcast from Red Hat.

00:58:32.680 --> 00:58:37.440
Compiler unravels industry topics, trends, and things you've always wanted to know about

00:58:37.440 --> 00:58:40.120
tech through interviews with the people who know it best.

00:58:40.120 --> 00:58:44.240
Subscribe today by following talkpython.fm/compiler.

00:58:44.240 --> 00:58:45.960
Want to level up your Python?

00:58:45.960 --> 00:58:50.020
We have one of the largest catalogs of Python video courses over at Talk Python.

00:58:50.020 --> 00:58:55.180
Our content ranges from true beginners to deeply advanced topics like memory and async.

00:58:55.180 --> 00:58:57.860
And best of all, there's not a subscription in sight.

00:58:57.860 --> 00:58:58.980
Check it out for yourself.

00:58:58.980 --> 00:59:02.660
Be sure to subscribe to the show.

00:59:02.660 --> 00:59:05.440
Open your favorite podcast app and search for Python.

00:59:05.440 --> 00:59:06.760
We should be right at the top.

00:59:06.760 --> 00:59:11.900
You can also find the iTunes feed at /itunes, the Google Play feed at /play,

00:59:11.900 --> 00:59:16.120
and the direct RSS feed at /rss on talkpython.fm.

00:59:16.120 --> 00:59:19.540
We're live streaming most of our recordings these days.

00:59:19.540 --> 00:59:22.960
If you want to be part of the show and have your comments featured on the air,

00:59:22.960 --> 00:59:27.380
be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

00:59:28.060 --> 00:59:29.240
This is your host, Michael Kennedy.

00:59:29.240 --> 00:59:30.520
Thanks so much for listening.

00:59:30.520 --> 00:59:31.680
I really appreciate it.

00:59:31.680 --> 00:59:33.620
Now get out there and write some Python code.

00:59:33.620 --> 00:59:53.640
Thank you.

00:59:53.640 --> 01:00:23.620
Thank you.

