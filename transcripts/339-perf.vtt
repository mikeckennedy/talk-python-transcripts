WEBVTT

00:00:00.001 --> 00:00:04.440
There's been a bunch of renewed interest in making Python faster. While for some of us,

00:00:04.440 --> 00:00:09.520
Python is already plenty fast, for others, such as those in data science, scientific computing,

00:00:09.520 --> 00:00:13.980
and even large tech companies, making Python even a little faster would be a big deal.

00:00:13.980 --> 00:00:20.120
This episode is the first of several that dive into some of the active efforts to increase the

00:00:20.120 --> 00:00:25.660
speed of Python while maintaining compatibility with existing code and packages. And who better

00:00:25.660 --> 00:00:30.740
to help kick this off than Guido van Rossum and Mark Shannon. They both joined us to share their

00:00:30.740 --> 00:00:34.720
project to make Python faster. I'm sure you'll love hearing about what they're up to.

00:00:34.720 --> 00:00:40.840
This is Talk Python to Me, episode 339, recorded November 1st, 2021.

00:00:54.320 --> 00:00:59.280
Welcome to Talk Python to Me, a weekly podcast on Python. This is your host, Michael Kennedy.

00:00:59.280 --> 00:01:03.500
Follow me on Twitter where I'm @mkennedy, and keep up with the show and listen to past

00:01:03.500 --> 00:01:09.920
episodes at talkpython.fm. And follow the show on Twitter via at Talk Python. We've started streaming

00:01:09.920 --> 00:01:15.600
most of our episodes live on YouTube. Subscribe to our YouTube channel over at talkpython.fm slash

00:01:15.600 --> 00:01:21.880
YouTube to get notified about upcoming shows and be part of that episode. This episode is brought to

00:01:21.880 --> 00:01:26.920
you by Shortcut and Linode, and the transcripts are sponsored by Assembly AI.

00:01:26.920 --> 00:01:35.000
Mark Guido, welcome to Talk Python to Me. Fantastic to have you here. I'm so excited about all the

00:01:35.000 --> 00:01:40.380
things that are happening around Python performance. I feel like there's just a bunch of new ideas

00:01:40.380 --> 00:01:43.840
springing up and people working on it, and it's exciting times.

00:01:43.840 --> 00:01:48.700
Definitely. You two are, of course, right at the center of it. But before we talk

00:01:48.700 --> 00:01:53.900
about the performance work that you are all doing, as well as some of the other initiatives going along,

00:01:53.900 --> 00:01:58.680
maybe in parallel there, let's just get started with a little bit of background on you. Guido,

00:01:58.680 --> 00:02:03.180
you've been on the show before, creator of Python. You hardly need an introduction to most people out

00:02:03.180 --> 00:02:08.920
there. But you have recently made a couple of big changes in your life. I thought I'd just ask you how

00:02:08.920 --> 00:02:13.480
that's going. You retired, and we were all super happy for you on that. And then you said, you know

00:02:13.480 --> 00:02:17.420
what, I kind of want to play with code some more. And now you're at Microsoft. What's the story there?

00:02:17.420 --> 00:02:23.840
Oh, I just like the idea of retiring. So I try to see how many times in a lifetime I can retire.

00:02:23.840 --> 00:02:30.760
And starting with my retirement from BDFL didn't stop me from staying super active in the community.

00:02:30.800 --> 00:02:36.980
But when I retired from Dropbox a little over two years ago, I really thought that that was it,

00:02:36.980 --> 00:02:43.380
that I believed it. And everybody else believed it too. Dropbox certainly believed it. They were very

00:02:43.380 --> 00:02:52.060
sad to see me go. I was sad to go, but I thought there was time. And I had a few great months decompressing,

00:02:52.060 --> 00:02:58.860
going on bike rides with my wife and family, fun stuff. And then the pandemic hit.

00:02:59.360 --> 00:02:59.600
Yeah.

00:02:59.600 --> 00:03:05.700
And a bunch of things got harder. Fortunately, the bike rides eventually got restored. But

00:03:05.700 --> 00:03:11.660
other activities like eating out was a lot more stressful. Basically, just life was a lot more

00:03:11.660 --> 00:03:12.640
stressful in general.

00:03:12.640 --> 00:03:16.520
Right. And the human interaction was definitely shrunken down to a kernel.

00:03:16.520 --> 00:03:25.520
Yeah. And somehow I thought, well, I want to have something to do. I want to do more sort of

00:03:25.520 --> 00:03:30.640
software development in a team. And the Python core development team didn't really cut it for me,

00:03:30.640 --> 00:03:37.280
because it's sort of diffuse and volunteer based. And sometimes you get stuck waiting for months for

00:03:37.280 --> 00:03:45.480
the steering council to sort of approve of or reject a certain idea that you've worked on.

00:03:45.480 --> 00:03:52.400
So I asked around and I found that Microsoft was super interested in hiring me. And that was now,

00:03:52.400 --> 00:03:58.780
well, tomorrow, exactly a month, a year, tomorrow, a year ago, I started at Microsoft officially.

00:03:58.780 --> 00:03:59.020
Yeah.

00:03:59.020 --> 00:04:06.580
In the beginning, I just had to find my way around at Microsoft. Eventually, I figured I should pick a

00:04:06.580 --> 00:04:13.220
project. And after looking around and realizing I couldn't really sort of turn the world of machine

00:04:13.220 --> 00:04:19.680
learning upside down, I figured I'd stay closer to home and see if Microsoft was interested in funding

00:04:19.680 --> 00:04:27.140
a team working on speeding up CPython. And I was actually inspired by Mark's proposals that were

00:04:27.140 --> 00:04:35.180
going around at the time. So I convinced people, Microsoft to sort of start a small team and get Mark on board.

00:04:35.180 --> 00:04:41.380
Yeah, that's fantastic. I also feel a little bit like machine learning is amazing, but I don't have a lot of

00:04:41.380 --> 00:04:47.760
experience with it. And whenever I work with it, I always kind of feel on the outside of it. But this core

00:04:47.760 --> 00:04:53.680
performance of Python, that helps everybody, right? Including even Microsoft, right? It maybe saves them

00:04:53.680 --> 00:04:54.340
Oh, absolutely.

00:04:54.340 --> 00:05:00.420
energy on Azure when they're running Python workloads or whatever. So you're enjoying your time? You're happy you're there?

00:05:00.420 --> 00:05:00.840
I'm very happy.

00:05:00.840 --> 00:05:05.220
Yeah, yeah, a lot of freedom to basically pursue what you are, right?

00:05:05.220 --> 00:05:10.800
Yeah, it's nice that the new Microsoft is very open source friendly, at least in many cases, obviously,

00:05:10.800 --> 00:05:33.380
not everywhere. But our department is very open source friendly. Things like Visual Studio Code are all open source. And so there was great support with management for sort of the way I said, I wanted to do this project, which is completely out in the open. Everything we do is sort of just merged into main as soon as we can.

00:05:33.380 --> 00:05:47.720
Yeah, we work with the core developers. We don't have like a private fork of Python, where we do amazing stuff. And then we knock on the steering council door and say, Hey, we'd like to merge this.

00:05:47.720 --> 00:05:53.580
Yeah, you're not going to drop six months of work just in one block, right? It's there for everyone to see.

00:05:53.580 --> 00:05:54.060
Exactly.

00:05:54.060 --> 00:06:03.680
I think that's really, really positive. And wow, what a change, not just for Microsoft, but so many companies to work that way compared to 10, 15 years ago.

00:06:03.680 --> 00:06:17.080
Yeah, absolutely. Now, before I get to Mark, I just want to, you know, some bunch of people are excited that you're here. And Luis out in the audience said, Wow, it's Guido. I can't thank you enough for your amazing Python and all the community.

00:06:17.080 --> 00:06:17.700
Great to hear.

00:06:17.700 --> 00:06:24.560
Mark, how about you? How'd you get into this Python performance thing? I know you did some stuff with Hot Pie back in the day.

00:06:24.680 --> 00:06:52.360
Yeah, that was sort of my PhD work. So I guess I kind of go into the performance almost before the Python. So I was doing sort of compiler work, masters. And obviously, just, you know, you need to write scripts and just get stuff done. And, you know, just Python is just a language to get stuff done. And then it's that, I think Armin Rigo, sort of, I think one of his sort of credits in one of his papers or something says,

00:06:52.360 --> 00:07:07.580
Thank you for Python for being such a great language to use and such a challenge to optimize. So it's doubly good if you're coming at it from a sort of it. So it provides this great intellectual challenge when you're actually trying to optimize it. And it's a really nice language to use as well. So it's doubly good.

00:07:07.580 --> 00:07:16.640
It is doubly good. It's doubly good. Yeah. And before we move on really quick, Paul Everett says, It's really impressive how the in the open work has been done. Yeah, totally agree.

00:07:16.640 --> 00:07:17.100
Hi, Paul.

00:07:17.100 --> 00:07:19.260
Yeah, keep that going. Hey, Paul, happy to see you here.

00:07:19.380 --> 00:07:30.840
We're going to talk about making Python faster. But I want to start this conversation, a bit of a hypothetical question, but sort of set the stage and ask, how much does Python really need to be faster?

00:07:31.240 --> 00:07:44.740
Because on one hand, sure, there's a lot more performance we can do if you're going to say, well, we're going to solve the in-body problem using C++ or C# versus Python. It's going to be faster with the native value types and whatnot.

00:07:44.980 --> 00:08:11.300
On the other, people are building amazing software that runs really fast with Python already. We've got the C optimizations for things like NumPy and SQLAlchemy's transformation layer, serialization layer, and so on. So a lot of times that kind of brings it back to C performance. So how much do you think Python really needs to be optimized already? Not that more is always better, faster is always better. But I just kind of want to set the stage and get your two thoughts on that.

00:08:11.300 --> 00:08:19.320
I always think back to my experience at Dropbox, where there was a large server called the Meta

00:08:19.320 --> 00:08:27.080
Server, which did sort of all the server side work, like anything that hits www.dropbox.com

00:08:27.080 --> 00:08:34.580
hits that server. And that server was initially a small prototype written in Python, the client was

00:08:34.580 --> 00:08:40.920
actually also a small prototype written in Python. And to this day, both the server and the client

00:08:40.920 --> 00:08:45.440
at Dropbox, as far as I know, and unless in the last two years, they totally ripped it apart,

00:08:45.440 --> 00:08:51.080
but I don't think they did. They tweaked it, but it's still all now very large Python applications.

00:08:51.080 --> 00:09:02.220
And so Dropbox really sort of feels the speed of Python in its budget, because they have thousands,

00:09:02.220 --> 00:09:08.200
I don't know how many thousands of machines that all run this enormous Python application.

00:09:08.200 --> 00:09:10.540
Right. And if it was four times faster, that's

00:09:10.540 --> 00:09:15.680
not just for, you know, a quarter of the machines, that's less DevOps, less admin,

00:09:15.680 --> 00:09:16.520
all sorts of stuff, right?

00:09:16.520 --> 00:09:19.800
Oh, even if it was 4% faster, they would notice.

00:09:19.800 --> 00:09:26.040
Yeah. The other area where I think it's really relevant has to do with the multi-core side of

00:09:26.040 --> 00:09:32.260
things. I have a PC over there, 16 cores. My new laptop has 10 cores. Although with Python,

00:09:32.260 --> 00:09:38.420
it's hard to take true advantage of that side of modern CPU performance if it's not IO bound,

00:09:38.480 --> 00:09:44.360
right? Yeah. I don't know how deep you want me to go into that and Mark can stop me if I'm going too deep

00:09:44.360 --> 00:09:52.120
too, but there are existing patterns that work reasonably well. If you have a server application

00:09:52.120 --> 00:10:00.660
that handles multiple fairly independent requests. Like if you're building a multi-core web application,

00:10:00.660 --> 00:10:08.460
you can use multi-processing or pre-forking or a variety of ways of running a Python interpreter on

00:10:08.460 --> 00:10:15.720
each core that you have, each independently handling requests. And you can do that if you have 64 cores,

00:10:15.720 --> 00:10:18.060
you run 64 Python processes.

00:10:18.060 --> 00:10:21.720
Right. That's just a number in a microwave config file. It's nothing.

00:10:21.920 --> 00:10:31.040
Yeah. It works for applications that are designed to sort of handle multiple independent requests in a scalable fashion.

00:10:31.040 --> 00:10:38.420
There are other things that the other algorithms that you would want to execute where it's much more complicated to,

00:10:38.420 --> 00:10:42.080
to sort of employ all your cores efficiently.

00:10:42.080 --> 00:10:43.180
Yeah, absolutely.

00:10:43.400 --> 00:10:50.480
That's still a nut that Python hasn't cracked. And I'm assuming you're asking this question because Sam Gross,

00:10:50.480 --> 00:10:54.220
a very smart developer at Facebook, claims that he has cracked it.

00:10:54.220 --> 00:10:58.420
Perhaps he has. It's an interesting idea. We'll dive into that a little bit later.

00:10:58.420 --> 00:11:04.860
I'm more asking it just because I see a lot of people say that Python is too slow. And then I also see

00:11:04.860 --> 00:11:11.560
a lot of people being very successful with it and it not being slow in practice or not being much slower

00:11:11.560 --> 00:11:17.420
than other things. And so I'm more or less at the stage of like the context matters, right? This Dropbox example

00:11:17.420 --> 00:11:22.740
you have, it really matters to them. You know, my course website where people take courses,

00:11:22.740 --> 00:11:28.300
the response time of the pages is 40 milliseconds. If it was 38, it doesn't matter. It's really fast.

00:11:28.300 --> 00:11:33.560
It's fine. So I think, but if I was trying to do computational biology in Python, we really want

00:11:33.560 --> 00:11:39.220
to be able to take advantage of those 16 cores, right? So there's just such a variety of perspectives

00:11:39.220 --> 00:11:41.780
where it matters. Mark, what are your thoughts on all this?

00:11:41.780 --> 00:11:47.440
Well, it's just a case of saving energy, saving time. It just makes the whole thing nicer to use.

00:11:47.440 --> 00:11:52.320
So, I mean, there's a lot of, you know, just iterative development in data science

00:11:52.320 --> 00:11:57.780
and it's that responsiveness, the whole, you know, just breaking your train of thought

00:11:57.780 --> 00:12:01.800
because things take too long versus just keeping in the flow and all that sort of stuff.

00:12:01.800 --> 00:12:06.320
It's just nice to have something that's faster. I mean, it's not just the big companies saving

00:12:06.320 --> 00:12:09.800
money as well. I mean, it's just, you know, just keeps everyone's server budgets down. I mean,

00:12:09.800 --> 00:12:14.900
if you just need a smaller virtual instance, because you can serve the requests up fast enough

00:12:14.900 --> 00:12:20.580
because Python's faster. So I think it's just generally a sort of responsible thing to do.

00:12:20.580 --> 00:12:25.060
I mean, it's also just, you know, people expect technology to move forwards and there's this

00:12:25.060 --> 00:12:30.040
feeling of, you know, falling behind or, you know, people wanting to move other languages because of

00:12:30.040 --> 00:12:31.200
the perceived performance.

00:12:31.520 --> 00:12:35.280
I do think that that's an issue. You know, I'm moving to go because it has better async support,

00:12:35.280 --> 00:12:39.540
rewriting this in Rust for whatever reason. Sometimes that might make sense, but other times

00:12:39.540 --> 00:12:43.860
I feel like that's just a shame and it could be used better. A couple of questions from the audience

00:12:43.860 --> 00:12:50.320
just want to throw out there. Let's see. One was Guido, especially, you must be really proud to hear

00:12:50.320 --> 00:12:56.480
about the Mars helicopter and the lander and Python in space. You know, how did you feel when you heard

00:12:56.480 --> 00:13:01.660
about the helicopter using Python and the lander using Python and Flask and things like that?

00:13:01.660 --> 00:13:09.300
It wasn't really a surprise given how popular Python is amongst scientists. So I didn't throw a

00:13:09.300 --> 00:13:14.960
party, but it made me feel good. I mean, it's definitely sort of one of those accomplishments

00:13:14.960 --> 00:13:21.300
for a piece of technology. If it's actually shot into space, you know, you've made a difference.

00:13:21.300 --> 00:13:29.440
Yeah. I remember like 30 years ago or more when I helped some coding on European project called

00:13:29.440 --> 00:13:34.720
Amoeba, which was like a little distributed operating system. And one of the things that

00:13:34.720 --> 00:13:40.560
they always boasted was that our software runs on the European space station. And that was very important.

00:13:40.560 --> 00:13:45.420
Yeah. So yeah, I totally get the feeling. And then I hope that everyone who contributed to Python

00:13:45.420 --> 00:13:50.360
also sort of feels that their contribution has made it.

00:13:50.360 --> 00:13:53.420
Yeah. And that sense of awe, if you look up in the night sky, it's that little,

00:13:53.420 --> 00:13:58.220
that bright star that's actually Mars. And you think, yeah, it's up there. Yeah. Fantastic. All right.

00:13:58.220 --> 00:14:04.440
Let's dive into some of the performance stuff that you all have been doing. So maybe Guido starts out

00:14:04.440 --> 00:14:09.940
with the team. So you've, you've got a group of folks working together. It's not just you. And also now

00:14:09.940 --> 00:14:15.940
Mark Shannon is working with you as well, right? That's correct. In March or so, the initial team

00:14:15.940 --> 00:14:23.840
was Eric Snow, Mark and myself. And since I think since early October, we've got fourth team member,

00:14:23.840 --> 00:14:30.520
Brent Booker, who is also a Python core dev since, I think since about a year and a half. He's a really

00:14:30.520 --> 00:14:36.820
smart guy. So now we have four people, except you should really discount me as a team member because I

00:14:36.820 --> 00:14:44.720
spend most of my time in meetings, either with a team or with other things going on at Microsoft in practice.

00:14:44.720 --> 00:14:50.320
Sure. How closely do you work with, say, the VS Code Python plugin team and other parts? Or is this more

00:14:50.320 --> 00:14:51.440
a focused effort?

00:14:51.440 --> 00:14:57.360
This is more focused. I know those people. I've not met anyone in person, of course. I've not met,

00:14:57.360 --> 00:15:04.400
I've not been to a Microsoft office since I started there, which is really crazy. But what we're doing

00:15:04.400 --> 00:15:11.040
is really quite separate from other sort of Python related projects at Microsoft. But I sort of,

00:15:11.040 --> 00:15:17.600
I do get called into meetings to give my opinion or sort of what I know about how the community is

00:15:17.600 --> 00:15:24.040
feeling or how the core dev team is feeling about various things that are interesting to Microsoft or

00:15:24.040 --> 00:15:27.560
sometimes things that management is concerned about.

00:15:27.560 --> 00:15:28.520
Yeah. Excellent.

00:15:28.520 --> 00:15:32.120
I'd be worth saying this, not just Microsoft as well. We've contributed from,

00:15:32.120 --> 00:15:37.000
there's quite a few other core developers are helping out. So it's a broader effort.

00:15:38.040 --> 00:15:44.600
This portion of Talk Python to Me is brought to you by Shortcut, formerly known as clubhouse.io. Happy

00:15:44.600 --> 00:15:49.240
with your project management tool? Most tools are either too simple for a growing engineering team

00:15:49.240 --> 00:15:54.200
to manage everything, or way too complex for anyone to want to use them without constant prodding.

00:15:54.200 --> 00:15:58.920
Shortcut is different though, because it's worse. No, wait, no, I mean, it's better.

00:15:58.920 --> 00:16:04.520
Shortcut is project management built specifically for software teams. It's fast, intuitive, flexible,

00:16:04.520 --> 00:16:10.200
powerful, and many other nice positive adjectives. Key features include team-based workflows.

00:16:10.200 --> 00:16:15.080
Individual teams can use default workflows or customize them to match the way they work.

00:16:15.080 --> 00:16:20.600
Org-wide goals and roadmaps. The work in these workflows is automatically tied into larger company

00:16:20.600 --> 00:16:26.440
goals. It takes one click to move from a roadmap to a team's work to individual updates and back.

00:16:26.440 --> 00:16:31.800
Tight version control integration. Whether you use GitHub, GitLab, or Bitbucket, clubhouse ties

00:16:31.800 --> 00:16:35.240
directly into them so you can update progress from the command line.

00:16:35.240 --> 00:16:40.440
Keyboard-friendly interface. The rest of Shortcut is just as friendly as their power bar,

00:16:40.440 --> 00:16:45.240
allowing you to do virtually anything without touching your mouse. Throw that thing in the trash.

00:16:45.240 --> 00:16:50.920
Iteration planning. Set weekly priorities and let Shortcut run the schedule for you with

00:16:50.920 --> 00:16:58.440
accompanying burndown charts and other reporting. Give it a try over at talkpython.fm/shortcut. Again,

00:16:58.440 --> 00:17:05.240
that's talkpython.fm/shortcut. Choose shortcut because you shouldn't have to project manage your

00:17:05.240 --> 00:17:09.000
project management. Mark, what's your role on the team?

00:17:09.000 --> 00:17:15.160
I know we already have sort of official roles, but I guess I'm sort of doing a fair bit of

00:17:15.160 --> 00:17:19.480
sort of technical, sort of architectural side of stuff, obviously, because this is like my field.

00:17:19.480 --> 00:17:26.520
So right. Optimizer in chief. Yeah, I guess so. All right. Guido, you gave a talk at the Python

00:17:26.520 --> 00:17:31.160
Language Summit in May this year, talking about faster Python, this team, some of the work that

00:17:31.160 --> 00:17:35.000
you're doing. So I thought that might be a good place to start the conversation.

00:17:35.000 --> 00:17:37.560
Yeah. Some of the content there is a little outdated, but...

00:17:37.560 --> 00:17:44.200
Well, you just have to let me know when things have changed. So one of the questions you ask is,

00:17:44.200 --> 00:17:49.480
can we make CPython specifically faster? And I think that's also worth pointing out, right? There's many

00:17:49.480 --> 00:17:55.720
runtimes. Often they're called interpreters. I prefer to the runtime word because sometimes they compile and

00:17:55.720 --> 00:17:56.600
they don't interpret. So...

00:17:56.600 --> 00:17:58.520
Sometimes they're called virtual machines.

00:17:58.520 --> 00:18:06.200
Yeah. There's many Python virtual machines, PyPy, CPython. Traditionally, there's been Jython and

00:18:06.200 --> 00:18:11.000
Iron Python, although I don't know if they're doing anything. But your focus and your energy is about

00:18:11.000 --> 00:18:16.520
how do we make the Python people get if they just go to their terminal and type Python, the main Python

00:18:16.520 --> 00:18:19.320
faster? Because that's what people are using, right? For the most part.

00:18:19.320 --> 00:18:27.080
I don't have specific numbers or sources, but I believe that like between 95 and 99% of people using

00:18:27.080 --> 00:18:32.280
Python are using some version of CPython. Hopefully not too many of them are still using Python too.

00:18:32.280 --> 00:18:37.240
Yeah. I would totally agree with that. And I would think it would trend more towards the 99 and less

00:18:37.240 --> 00:18:42.440
towards the 95 for sure. Maybe a fork of CPython that they've done something weird too. But yeah,

00:18:42.440 --> 00:18:48.360
I would say CPython. So you asked the question, can we speed up CPython? And Teddy out in the live

00:18:48.360 --> 00:18:52.120
stream, I don't know if I'll be able to catch his comment exactly how there is. He says, you know,

00:18:52.120 --> 00:18:56.920
what will we lose in making Python faster if anything? For example, what are the trade-offs?

00:18:56.920 --> 00:19:01.240
So you point out, well, can we make it two times faster, 10 times faster, and then without breaking

00:19:01.240 --> 00:19:06.760
anybody's code, right? Because I think we just went through a two to three type of thing that was way

00:19:06.760 --> 00:19:11.160
more drawn out than I feel like it should have been. We don't want to reset that again, do we?

00:19:11.160 --> 00:19:15.080
No. Well, obviously the numbers on this slide are just teasers.

00:19:15.080 --> 00:19:21.000
Of course. I don't know how to do it. I think Mark has a plan, but that doesn't necessarily mean he knows

00:19:21.000 --> 00:19:28.280
how to do it exactly either. The key thing is, and sort of to answer your audience question without

00:19:28.280 --> 00:19:36.920
breaking on anybody's code. So we're really trying to sort of not have there be any downsides to adopting

00:19:36.920 --> 00:19:45.240
this new version of Python, which is unusual because definitely if you use PyPy, which is,

00:19:45.240 --> 00:19:52.360
I think, the only sort of competitor that competes on speed that is still alive, and in some use,

00:19:52.360 --> 00:20:00.600
you pay in terms of how well does it work with extension modules. It doesn't work with all extension modules.

00:20:00.600 --> 00:20:08.360
And with some extension modules, it works, but it's slower. There are various limitations. And that in particular

00:20:08.360 --> 00:20:13.240
is something that has kept many similar attempts back.

00:20:13.240 --> 00:20:19.480
If we just give this up, we can have X, Y, and Z, right? But that those turn out to be pretty big compromises.

00:20:19.480 --> 00:20:25.960
Absolutely. And sometimes, I mean, quite often extension modules are the issue. Sometimes there are also

00:20:25.960 --> 00:20:33.080
things where Python's runtime semantics are not fully specified. Like, it's not defined by the language

00:20:33.080 --> 00:20:41.640
when exactly objects are finalized when they go out of scope. In practice, there's a lot of code around

00:20:41.640 --> 00:20:48.680
there that in very subtle ways depends on CPython's finalization semantics based on reference counting.

00:20:48.680 --> 00:20:53.640
And so anything, and this is also something that PyPy learned, and I think,

00:20:53.640 --> 00:20:59.640
oh, Piston, which is definitely alive and open source. You should talk to the Piston guys if you

00:20:59.640 --> 00:21:06.680
haven't already. But their first version, which they developed many years ago at Dropbox, suffered from

00:21:06.680 --> 00:21:15.480
sort of imprecise finalization semantics. And they found with sort of early tests on the Dropbox server code

00:21:15.480 --> 00:21:23.160
that there was too much behavior that didn't work right because objects weren't always finalized at the

00:21:23.160 --> 00:21:28.280
same time or sometimes in the same order as they were in standard CPython.

00:21:28.280 --> 00:21:33.240
Oh, interesting. So there's no promises about that, right? It just says, well, when you're done with it,

00:21:33.240 --> 00:21:38.760
it goes away pretty much eventually. If it's a reference count, it might go away quickly. If it's a cycle,

00:21:38.760 --> 00:21:39.720
it might go away slower.

00:21:39.720 --> 00:21:46.120
That's correct. And unfortunately, this is one of those unspecified parts of the language where people

00:21:46.120 --> 00:21:54.040
in practice all depend on, not everybody, obviously, but many large production code bases do end up

00:21:54.040 --> 00:22:01.000
depending on that. Not sort of intentionally. It's not that a bunch of application architects got together

00:22:01.000 --> 00:22:08.840
and said, we're going to depend on precise finalization based on reference counting. It's more that those

00:22:08.840 --> 00:22:15.480
servers, like the 5 million lines of server code that Dropbox had when I left, were written by hundreds of

00:22:15.480 --> 00:22:23.400
different engineers, some of whom wrote only one function or two lines of code, some of whom sort of maintained

00:22:23.400 --> 00:22:29.560
several entire subsystems for years. But collectively, it's a very large number of people who don't

00:22:29.560 --> 00:22:35.640
all have the same understanding of how Python works and which part is part of the sort of the promises

00:22:35.640 --> 00:22:42.440
of the language and which is just sort of how the implementation happens to work. And some of those

00:22:42.440 --> 00:22:47.000
are pretty obvious. I mean, sometimes there are functions where the documentation says,

00:22:47.000 --> 00:22:53.000
well, you can use this, but it's not guaranteed that this function exists or that it always behaves the

00:22:53.000 --> 00:22:57.320
same way. But the sort of the finalization behavior is pretty implicit.

00:22:57.320 --> 00:22:58.840
Yeah, Mark, what are your thoughts here?

00:22:58.840 --> 00:23:03.000
People just expectations is derived from what they use. The problem with documentation is like

00:23:03.000 --> 00:23:08.920
instructions. They don't always get read. And also, it's not just finalization. It's also reclaiming

00:23:08.920 --> 00:23:15.080
memory. So anything that has a different memory management system might just need more memory.

00:23:15.080 --> 00:23:20.200
Reference counting is pretty good at reclaiming memory quickly and will run near the limit of what you

00:23:20.200 --> 00:23:25.800
have available. Whereas a sort of more tracing garbage collector like PyPy doesn't always work so well like

00:23:25.800 --> 00:23:29.480
that. I mean, one thing we are going to change is the performance characteristics. Now, that should

00:23:29.480 --> 00:23:35.240
generally be a good thing, but there may be people who rely on more consistent performance.

00:23:35.240 --> 00:23:40.920
You may end up unearthing race conditions, potentially that no one really knew was there. I mean,

00:23:40.920 --> 00:23:46.920
but I would not blame you for making Python faster and people who write bad, poorly threads of code

00:23:46.920 --> 00:23:51.800
fall into some trap there. But I guess that there's even those kinds of unintended consequences, I guess.

00:23:51.800 --> 00:23:54.920
That one sounds like pretty low risk, to be honest. Yeah.

00:23:54.920 --> 00:23:59.560
Yeah. Also, this sort of the warm up time, we'll get a warm up time. Now, what will happen is,

00:23:59.560 --> 00:24:04.120
of course, it's just getting faster. So it's no slower to start with. But it still has the perception

00:24:04.120 --> 00:24:08.200
that that now takes a while to get up to speed, whereas previously, it used to get up to speed very

00:24:08.200 --> 00:24:13.320
quickly, because it didn't really get up to speed. It just started. It stays around. It stayed at the same

00:24:13.320 --> 00:24:18.200
speeds. But these are subtle things, but they're detectable changes that people may notice.

00:24:18.200 --> 00:24:25.400
Yeah. Also, like any optimizer, there are certain situations where the optimization doesn't really

00:24:25.400 --> 00:24:32.280
work. It's not necessarily a pessimization, but somehow it's not any faster than previous versions.

00:24:32.280 --> 00:24:40.680
Well, other similar code may run much faster. And so you have this strange effect that you make a small

00:24:40.680 --> 00:24:47.480
tweak to your code, which you think should not affect performance at all. Or you're not aware that

00:24:47.480 --> 00:24:50.920
suddenly you've made that part of your code 20% slower.

00:24:50.920 --> 00:24:56.600
Yeah. It is one of our design goals not to have these surprising sort of performance edges. But

00:24:56.600 --> 00:24:59.800
but yeah, there's a little cases where it might definitely make a difference. Things will get a

00:24:59.800 --> 00:25:05.960
bit slower. Yeah. There are very subtle things that can have huge performance differences that

00:25:05.960 --> 00:25:11.800
I think people who are newer to Python run into like, oh, I see you can do this comprehension. And I had

00:25:11.800 --> 00:25:15.320
square brackets, but I saw they had parentheses. So that's the same thing, right? Well,

00:25:16.760 --> 00:25:22.200
not so much, not so much. Not if it's a million lines of code or a million lines of data. All right.

00:25:22.200 --> 00:25:28.280
So that's a great way to think about it. Not making it break a lot of code is I think as much as it's

00:25:28.280 --> 00:25:32.920
exciting to think about completely reinventing it, it's super important that we just have a lot of

00:25:32.920 --> 00:25:37.720
consistency now that we've kind of just moved beyond the Python two versus three type of thing.

00:25:37.720 --> 00:25:43.160
I think also it's worth mentioning, Guido, you gave a shout out to Sam Gross's proposal. The stuff you're

00:25:43.160 --> 00:25:48.600
doing is not Sam Gross's proposal. It's not about even from what I can see from the outside that much

00:25:48.600 --> 00:25:54.520
about threading. It's more about how do I make just the fundamental stuff of Python go faster. Is that right?

00:25:54.520 --> 00:25:59.800
That's right. These are like completely different developments. When we started this, we didn't

00:25:59.800 --> 00:26:06.120
actually know Sam or that there was anyone who was working on something like that. But there had

00:26:06.120 --> 00:26:13.480
been previous attempts to remove the gill, which is what Sam has done. And like the most recent one of

00:26:13.480 --> 00:26:19.640
those was by Larry Hastings, who came up with the great name, the gillectomy. That's a fantastic name.

00:26:19.640 --> 00:26:27.000
Yeah. He put a lot of time in it, but in the end he had to give up because the sort of the baseline

00:26:27.000 --> 00:26:35.480
performance was just significantly slower than vanilla interpreter. And I believe it also didn't scale all

00:26:35.480 --> 00:26:43.000
that well. Although I don't remember whether it sort of stopped scaling at five or 10 or 20 cores,

00:26:43.000 --> 00:26:43.800
but right. Yeah.

00:26:43.800 --> 00:26:51.720
Sam claims that he's sort of got the baseline performance, I think within 10% or so of vanilla

00:26:51.720 --> 00:26:54.280
3.9, which is what he's worked off. Right.

00:26:54.280 --> 00:27:00.120
And he also claims that he has a very scalable solution and he obviously put much more effort in it,

00:27:00.120 --> 00:27:03.160
much more time in it than Larry ever had.

00:27:03.160 --> 00:27:04.840
Yeah. And it sounds like Facebook

00:27:04.840 --> 00:27:08.360
is putting some effort into funding his work on that, which is great.

00:27:08.360 --> 00:27:12.520
Yeah. But it feels like a very sort of bottom up project. It feels like

00:27:12.520 --> 00:27:13.160
Yeah.

00:27:13.160 --> 00:27:19.720
Sam thought that that this was an interesting challenge and he sort of convinced himself that he could do it.

00:27:19.720 --> 00:27:25.640
And he sort of gradually worked on all the different problems that he encountered on the way.

00:27:25.640 --> 00:27:43.160
And he convinced his manager that this was a good use of his time. It's my theory, because that's usually how these projects go. But you almost never have management say, Oh, we got to fund an engineer to make faster or make a multi-core or whatever.

00:27:43.160 --> 00:27:48.680
Find a good engineer.

00:27:48.680 --> 00:28:16.200
Yeah. So you all are adopting what I see is going as the Shannon plan. As in Mark Shannon, the guests in the top left here. That's fantastic. I remember talking about this as well, that you had hosted this thing. When was this back? A little over a year ago. So interesting time in there, right? You had talked about making Python faster.

00:28:16.200 --> 00:28:31.960
You had talked about Python faster over the next four releases by a factor of five, which is pretty awesome. And you have a concrete plan to sort of make changes along each yearly release to add a little bit of performance because the geometric growth may get quite a bit faster over time.

00:28:31.960 --> 00:28:33.800
Yeah. Do you want me to run through these?

00:28:33.800 --> 00:28:39.720
Yeah. Yeah. Tell us about your plan. You've got four stages and maybe we could talk through each stage and focus in on some of the tech there.

00:28:39.720 --> 00:28:57.160
The way we're implementing is now kind of a bit of a jumble of stage one and two, but the basic idea is that, dynamic languages, the key performance improvement is always based on specialization. So obviously, you know, it's the most of the time the code does mostly the same thing as it did last time.

00:28:57.720 --> 00:29:15.720
Yeah. And even in like non loopy code, you know, who are web server, there's still like a big loop level at sort of like requests response on a level. So you're still hitting the same sort of code. And those codes are doing much the same sort of thing. And the idea is that you, you transmit, you know, multiply the code. So it sort of works for those particular cases.

00:29:15.720 --> 00:29:30.720
You should specialize it. So the obvious sort of simple stuff is, you know, like binary arithmetic. I have a special version of adding integers, special version floats. Obviously Python, it's much more to special versions for different calling, different things and different attributes and all this sort of stuff.

00:29:30.720 --> 00:29:54.720
That's sort of the key first stage. I mean, that's mixed in with the second stage, which is really much more to just doing lots and lots of little bits and tweaks memory layout. So that's to do better, better memory layout. You know, modern CPUs are, you know, extremely efficient, but they still have to fetch from, you know, speed light issues with fetching stuff from memory. So, you know, how things are laid out in memory is key performance.

00:29:54.720 --> 00:30:20.720
And it's sort of just those sort of little bits and tweaks here and just kind of writing the code as we would if it had been written for speed in the first place. So a lot of, you know, CPython is old and it's just sort of evolved. And a lot of it has, there's lots of potential for just sort of rearranging data structures and rearranging the code and so on. And these all add up, you know, a few percent here, a few percent there. And it doesn't take many of those to get a decent speed up.

00:30:20.720 --> 00:30:25.680
So that's the sort of first two stages. And those are the ones where we have some pretty concrete idea what we're doing.

00:30:25.680 --> 00:30:33.340
Right. And this is the kind of stuff that will benefit everybody, right? We all use numbers. We all do comparisons. We all do addition call functions and so on.

00:30:33.340 --> 00:30:40.900
Yeah. I mean, the way we're sort of trending with performance in the moment is that sort of, you know, sort of webby type code, web backend sort of code.

00:30:41.200 --> 00:30:45.760
You'd be looking at kind of where we are now, I don't know, it's a 25, 30% speed up.

00:30:45.760 --> 00:30:51.700
Whereas if it's a machine learning, a sort of numerical code, it's more likely to be sort of 10% region.

00:30:51.700 --> 00:30:59.080
Obviously we'd hope to push both up and by more, I don't think we're particularly focused on either.

00:30:59.280 --> 00:31:04.800
It's just often the case where, you know, the next sort of obvious sort of convenient speed up lies.

00:31:04.800 --> 00:31:11.440
And although everyone talks about speed ups and I've been doing the same myself, I mean, it's best to think of really at the time something takes to execute.

00:31:11.600 --> 00:31:16.320
So it's often just shaving off 1% of the type rather than speed up by 1%.

00:31:16.320 --> 00:31:26.880
And because, you know, obviously as the overall runtime shrinks, what were marginal improvements become more valuable, you know, shaving off 0.2% might be not worth it now.

00:31:26.880 --> 00:31:32.240
But once you've sped something up by a factor of three or four, then that suddenly becomes, you know, a percent and it's worth the effort.

00:31:32.240 --> 00:31:37.760
This portion of Talk Python To Me is sponsored by Linode.

00:31:38.220 --> 00:31:41.700
Cut your cloud bills in half with Linode's Linux virtual machines.

00:31:41.700 --> 00:31:45.840
Develop, deploy, and scale your modern applications faster and easier.

00:31:45.840 --> 00:31:53.380
Whether you're developing a personal project or managing larger workloads, you deserve simple, affordable, and accessible cloud computing solutions.

00:31:53.380 --> 00:31:58.640
Get started on Linode today with $100 in free credit for listeners of Talk Python.

00:31:58.640 --> 00:32:02.980
You can find all the details over at talkpython.fm/Linode.

00:32:03.680 --> 00:32:09.560
Linode has data centers around the world with the same simple and consistent pricing regardless of location.

00:32:09.560 --> 00:32:12.040
Choose the data center that's nearest to you.

00:32:12.040 --> 00:32:20.000
You also receive 24, 7, 365 human support with no tiers or handoffs regardless of your plan size.

00:32:20.000 --> 00:32:22.880
Imagine that, real human support for everyone.

00:32:22.880 --> 00:32:33.060
You can choose shared or dedicated compute instances, or you can use your $100 in credit on S3 compatible object storage, managed Kubernetes clusters, and more.

00:32:33.400 --> 00:32:35.680
If it runs on Linux, it runs on Linode.

00:32:35.680 --> 00:32:40.280
Visit talkpython.fm and click the create free account button to get started.

00:32:40.280 --> 00:32:43.160
You can also find the link right in your podcast player show notes.

00:32:43.160 --> 00:32:46.100
Thank you to Linode for supporting Talk Python.

00:32:48.260 --> 00:32:50.420
Yeah, which leads on to stages three and four.

00:32:50.420 --> 00:32:55.520
So, you know, just-in-time compilation is always hailed as the sort of the way to speed up interpreted languages.

00:32:55.520 --> 00:33:01.000
Now, before you move on, let me just like sort of list out what you have on stage two for people who haven't dove into this.

00:33:01.000 --> 00:33:05.560
Because I think some of the concrete details, you know, people hear this in the abstract.

00:33:05.700 --> 00:33:09.500
They kind of want to know, like, okay, well, what actually are some of the things you all are considering?

00:33:09.500 --> 00:33:13.780
So, improved performance for integers less than one machine word.

00:33:13.780 --> 00:33:16.020
It's been a long time since I've done C++.

00:33:16.020 --> 00:33:17.200
Is a word two bytes?

00:33:17.200 --> 00:33:17.920
How big is a word?

00:33:17.920 --> 00:33:19.860
Well, a word is how big depends on the machine.

00:33:19.860 --> 00:33:22.220
So, that would be 64 bits for pretty much anything now.

00:33:22.700 --> 00:33:26.740
Apart from like a little tiny embedded systems, which is 32 still.

00:33:26.740 --> 00:33:28.800
So, that's a lot of numbers, right?

00:33:28.800 --> 00:33:33.120
That's many of the numbers you work with are less than 2 billion or whatever that is.

00:33:33.120 --> 00:33:33.400
Yeah.

00:33:33.400 --> 00:33:35.180
I mean, basically, there are two types of integers.

00:33:35.180 --> 00:33:44.760
There's big ones that are used for cryptography and other such things where, you know, it's a number in a sort of mathematical sense, but it's really sort of some elaborate code.

00:33:44.760 --> 00:33:49.340
And then there's numbers that actually represent the number of things or the number of times you're going to do something.

00:33:49.700 --> 00:33:52.600
And those are all relatively tiny and they'll all fit.

00:33:52.600 --> 00:33:56.760
So, the long ones used for cryptography and so on are relatively rare and they're quite expensive.

00:33:56.760 --> 00:34:02.160
So, it's the other ones we want to optimize for because when you see an integer, that's the integers you get.

00:34:02.160 --> 00:34:04.420
You know, they aren't in the quadrillion range.

00:34:04.420 --> 00:34:05.640
They're in the thousands.

00:34:05.640 --> 00:34:06.380
Right, right.

00:34:06.380 --> 00:34:06.700
Exactly.

00:34:06.700 --> 00:34:10.240
A loop index or an array index or something.

00:34:10.240 --> 00:34:19.000
Some languages, one that I'm thinking of that also maybe is kind of close to where Guido is right now, also in Microsoft space, is C#,

00:34:19.200 --> 00:34:24.560
which treats integers sometimes as value types and sometimes as reference types.

00:34:24.720 --> 00:34:34.480
So, that when you're doing like loops and other stuff, they operate more like C++ numbers and less like pi, you know, pointers to pi long objects.

00:34:34.480 --> 00:34:36.540
Have you considered any of that kind of stuff?

00:34:36.540 --> 00:34:37.260
Is that what you're thinking?

00:34:37.460 --> 00:34:42.720
An obvious thing is an old thing as well is to have tagged integers.

00:34:42.720 --> 00:34:47.840
So, basically, you know, where we would normally have a pointer, we've got a whole bunch of zeros at the end.

00:34:47.840 --> 00:34:48.960
There's 64 bits.

00:34:48.960 --> 00:34:50.560
A machine is three.

00:34:50.560 --> 00:34:54.180
And then for alignment, there's effectively four zeros at the end.

00:34:54.180 --> 00:35:03.280
So, we're using a sixteenth of the sort of the possible numbers that a pointer could hold, four pointers, which means leaves a bunch for integers and floating point numbers.

00:35:03.760 --> 00:35:06.400
So, there's a number of what's called tagging schemes.

00:35:06.400 --> 00:35:19.700
For example, LuaJit, which is a very fast implementation of Lua, uses what's called NAND boxing, which is everything's a floating point, but there is sufficiently something like two to the 53, which is a huge number of not a numbers in the floating point range.

00:35:19.800 --> 00:35:22.740
So, you could use a lot of those for integers or pointers.

00:35:22.740 --> 00:35:27.940
Now, that's a little problematic with 64-bit pointers because, obviously, 64 bits is bigger than 53.

00:35:27.940 --> 00:35:30.340
But there are other schemes where you...

00:35:30.340 --> 00:35:38.300
So, again, a simple scheme is that, basically, the least significant bit is one for pointers and zero for integers or vice versa.

00:35:38.860 --> 00:35:49.800
And basically, it just gives you full machine performance for integers because you just basically, anything up to 63 bits fits in a 64-bit integer and has basically all of your numbers.

00:35:49.800 --> 00:35:50.260
Interesting.

00:35:50.260 --> 00:35:50.620
Okay.

00:35:50.620 --> 00:35:55.540
Because it's shifted across all the machine arithmetic works as normal and overflows.

00:35:55.540 --> 00:35:59.680
You just overflow checks a machine, a single machine instruction and things like this.

00:36:00.040 --> 00:36:01.920
And that's, again, pretty standard.

00:36:01.920 --> 00:36:10.020
And they need sort of like fast Lisp implementation and older small talk and other sort of historical languages.

00:36:10.020 --> 00:36:17.420
JavaScript tends to use things like this NAND boxing I was talking about because all of the numbers are floating point numbers.

00:36:17.420 --> 00:36:21.860
So, another one that stands out to me here is zero overhead exception handling.

00:36:21.860 --> 00:36:24.460
Guido, that's making it into 3.11 already, right?

00:36:24.580 --> 00:36:34.620
That's basically just what we used to have is we'd have a little setup and sort of tear down instruction for every time we wanted to sort of control the block of code inside a try.

00:36:34.620 --> 00:36:37.060
As a try finally, but also with statements.

00:36:37.060 --> 00:36:40.080
But we've just ditched those in favor of just a table lookup.

00:36:40.080 --> 00:36:44.960
So, if there's an exception now, it's just looked up in a table, which is what the JVM Java virtual machine does.

00:36:44.960 --> 00:36:45.600
Yeah, excellent.

00:36:45.600 --> 00:36:48.820
ZeroVerhead is a slightly optimistic term.

00:36:48.820 --> 00:36:51.560
It's obviously not zero overhead, but it is less.

00:36:51.560 --> 00:36:53.820
You'll have a harder time finding it in the profiler.

00:36:53.820 --> 00:36:56.900
There's a little bit of memory that you didn't have before.

00:36:56.900 --> 00:37:02.640
That's a lookup table, but sort of it really is zero overhead if no exceptions happen, right?

00:37:02.640 --> 00:37:03.300
Not quite.

00:37:03.300 --> 00:37:06.680
Just because there is extra memories it causes.

00:37:06.680 --> 00:37:15.300
But also, you know, because of like tracing guarantees, sometimes we have to insert a knob where the try was.

00:37:15.300 --> 00:37:17.460
So, there's still some slight overhead.

00:37:17.460 --> 00:37:21.540
And then potentially in future when we compile code, that should effectively become zero.

00:37:21.540 --> 00:37:23.020
But it is definitely reduced.

00:37:23.060 --> 00:37:28.620
Mark, Apple surprised the world and they took their phone chips and turned them into desktop chips.

00:37:28.620 --> 00:37:32.180
And that seemed to actually work pretty well with their ARM stuff.

00:37:32.180 --> 00:37:39.460
There's a switch not just having basically just x86 and 64-bit stuff to think about, but now you also have this ARM stuff.

00:37:39.460 --> 00:37:41.060
Does that make life harder or easier?

00:37:41.060 --> 00:37:44.500
Does it open up possibilities or is it another thing to deal with?

00:37:44.560 --> 00:37:49.080
It's just harder because it's a bit harder.

00:37:49.080 --> 00:37:52.640
And we may want to look to the future of RISC-V.

00:37:52.640 --> 00:37:56.200
So, currently, CPython makes net is portable.

00:37:56.200 --> 00:37:58.020
That's a key thing.

00:37:58.020 --> 00:38:02.920
It's portability is, yeah, it rather depends on testing.

00:38:02.920 --> 00:38:04.980
You know, it's all very well saying it's perfectly portable.

00:38:04.980 --> 00:38:08.400
But if you have never tested on a platform, you may have surprises.

00:38:08.900 --> 00:38:09.880
But it's all written in C.

00:38:09.880 --> 00:38:12.900
And portability is a sort of serious consideration.

00:38:12.900 --> 00:38:18.420
So, I mean, things like those tagging I was just talking about, that's technically not portable C.

00:38:18.420 --> 00:38:23.180
But it's certainly, I mean, a lot of things aren't technically portable C, but in effect are.

00:38:23.640 --> 00:38:34.080
I mean, technically, it's impossible to write a memory allocator in C because the specification says once you've called free, you can't access the memory, which makes it kind of difficult to write something that handles the memory.

00:38:34.080 --> 00:38:36.840
But, you know, these are oddities.

00:38:36.840 --> 00:38:42.460
But in practice, you know, if you write sensible C code, you should expect to be portable.

00:38:42.460 --> 00:38:45.180
So, we are kind of basing around that.

00:38:45.180 --> 00:38:50.140
I mean, like some other virtual machines, you know, particularly JavaScript ones are effectively written.

00:38:50.140 --> 00:38:53.420
They're interpreted often written in Assembler or some variant of it.

00:38:53.420 --> 00:39:01.080
There's definitely a performance advantage in that, but I'm not convinced it's great enough to lose the portability and the maintenance overhead.

00:39:01.080 --> 00:39:01.320
Yeah.

00:39:01.320 --> 00:39:10.020
And one of the things that you focused on, Guido, was that you wanted this to be, to keep, one of the constraints is you said you want to keep the code maintainable, right?

00:39:10.020 --> 00:39:10.940
This is important.

00:39:10.940 --> 00:39:11.420
Absolutely.

00:39:11.420 --> 00:39:17.660
Why does that matter so much rather than if we can get 20% speed up if Mark refreshes his assembly language skills?

00:39:17.660 --> 00:39:21.960
Well, it would leave most of the core development team behind.

00:39:22.460 --> 00:39:31.960
And so, suddenly, Mark would be a very, very valuable contributor because he's the only one who understands that assembly code.

00:39:31.960 --> 00:39:33.160
That's just how it goes.

00:39:33.160 --> 00:39:33.460
Yeah.

00:39:33.560 --> 00:39:37.540
And I don't think that that would be healthy for the Python ecosystem.

00:39:37.580 --> 00:39:50.360
If the technology we used was so hard to understand and so hard to learn, making it so hard to maintain, then as an open source project, we'd lose velocity.

00:39:50.360 --> 00:39:50.700
Right.

00:39:50.820 --> 00:40:02.340
The only thing that would sort of cause to happen in the core team might be people decide to move more code to Python code because now the interpreter is faster anyway.

00:40:02.520 --> 00:40:06.000
So they don't have to write so much in C code.

00:40:06.000 --> 00:40:12.020
But then, of course, likely it's actually going to be slower, at least that particular bit of code.

00:40:12.020 --> 00:40:13.840
That's an interesting intention to think about.

00:40:13.840 --> 00:40:20.080
If you could make the interpreter dramatically faster, you could actually do more Python and less C.

00:40:20.080 --> 00:40:20.920
I don't know.

00:40:20.920 --> 00:40:21.740
It would have to be.

00:40:21.740 --> 00:40:23.780
There's some big number where that happens, right?

00:40:23.780 --> 00:40:25.800
It's not just a 10%, but maybe.

00:40:25.980 --> 00:40:27.560
That could be in the distant future.

00:40:27.560 --> 00:40:34.160
But nevertheless, I wouldn't want the C code to be unreadable for most of the core developers.

00:40:34.160 --> 00:40:35.100
Yeah, I agree.

00:40:35.100 --> 00:40:36.180
That makes a lot of sense.

00:40:36.180 --> 00:40:39.960
Being a C expert is not a requirement for being a core developer.

00:40:39.960 --> 00:40:44.720
In practice, quite a few of the core developers are really good C coders.

00:40:44.720 --> 00:40:47.620
And we support each other in that.

00:40:47.620 --> 00:40:52.300
We take pride in it and we help each other out.

00:40:52.300 --> 00:40:55.020
I mean, code reviews are incredibly important.

00:40:55.600 --> 00:41:00.960
And we will happily help newbies to sort of get up to speed with C.

00:41:00.960 --> 00:41:04.540
If we had a considerable portion that was written in assembler.

00:41:04.540 --> 00:41:04.800
Yeah.

00:41:04.800 --> 00:41:09.020
And then it would have to be written in sort of multiple assemblers.

00:41:09.020 --> 00:41:16.200
Or there would also have to be a C version for platforms where we don't have access to the assembler.

00:41:16.200 --> 00:41:19.240
Nobody has bothered to write that assembler code yet.

00:41:19.240 --> 00:41:24.000
All these things make things even more complicated than they already are.

00:41:24.000 --> 00:41:24.280
Right.

00:41:24.380 --> 00:41:29.160
And the portability and the approachability of it is certainly a huge benefit.

00:41:29.160 --> 00:41:38.300
Two other constraints that you had here, maybe you could just elaborate on real quick, is don't break stable ABI compatibility and don't break limited API compatibility.

00:41:38.300 --> 00:41:43.080
Yeah. So the ABI is the application binary interface.

00:41:43.080 --> 00:41:56.140
And that guarantees that extension modules that use a limited set of C API functions don't have to be recompiled for each new Python version.

00:41:56.140 --> 00:42:00.420
And so you can, in theory, you can have a wheel containing binary code.

00:42:00.420 --> 00:42:06.280
And that binary code will still be platform specific, but it won't be Python version specific.

00:42:06.280 --> 00:42:07.140
Yeah, that's very nice.

00:42:07.140 --> 00:42:09.920
That sort of that we don't want to break that.

00:42:10.060 --> 00:42:18.340
It is a terrible constraint because it means we can't move fields like the reference count or the type field around in the object.

00:42:18.340 --> 00:42:19.940
Many other things as well.

00:42:19.940 --> 00:42:24.700
But nevertheless, it is an important property because people depend on that.

00:42:24.700 --> 00:42:25.040
Sure.

00:42:25.200 --> 00:42:27.360
And the API compatibility, well, that's pretty clear.

00:42:27.360 --> 00:42:28.880
You don't want people to have to rewrite code.

00:42:28.880 --> 00:42:33.600
The limited API is sort of the compile time version of the stable ABI.

00:42:33.600 --> 00:42:42.360
I think it's the same set of functions, except the stable ABI actually means that you don't have to recompile.

00:42:42.480 --> 00:42:54.920
The limited API offers the same and I think a slightly larger set of API functions where if you do recompile, you're guaranteed to get the same behavior.

00:42:54.920 --> 00:43:10.740
And again, there are sort of our API is pretty large and a few things have snuck into the limited API and the stable ABI that sort of are actually difficult to support with changes that we want to make.

00:43:10.740 --> 00:43:13.320
And so sometimes this holds us back.

00:43:13.320 --> 00:43:20.680
But at the same time, we don't want to break the promises that were made to the Python community about API compatibility.

00:43:20.680 --> 00:43:25.800
We don't want to say, oh, sorry, folks, we made everything 20% faster.

00:43:25.800 --> 00:43:30.660
But alas, you're going to have to use a new API and all your extensions.

00:43:30.660 --> 00:43:33.500
Just recompiling isn't going to be good enough.

00:43:33.500 --> 00:43:38.940
Some functions suddenly have three arguments instead of two or no longer exists.

00:43:39.220 --> 00:43:44.380
Or return memory that you own instead of returning a borrowed reference.

00:43:44.380 --> 00:43:53.520
And we don't want to do any of those things because that just would break the entire ecosystem in a way that would be as bad as the Python 3 transition.

00:43:53.520 --> 00:43:53.860
Right.

00:43:53.860 --> 00:43:55.980
And it's, yeah, just not just not worth it.

00:43:55.980 --> 00:43:56.460
All right.

00:43:56.760 --> 00:43:58.520
Let's go back to the Shannon plan.

00:43:58.520 --> 00:44:01.340
So we talked about stage one and stage two.

00:44:01.340 --> 00:44:05.160
And Mark, I see here this is Python 3.10 and Python 3.11.

00:44:05.160 --> 00:44:07.400
Are those the numbers where they're actually going to make it in?

00:44:07.400 --> 00:44:10.720
Or is it, do we have to do like a plus plus or plus equals on them?

00:44:10.720 --> 00:44:13.340
I think a plus one would be appropriate.

00:44:13.340 --> 00:44:13.740
All right.

00:44:13.740 --> 00:44:14.380
Plus equals one.

00:44:14.600 --> 00:44:14.760
Yeah.

00:44:14.760 --> 00:44:21.220
So maybe we're a bit faster because obviously I envisioned this was basically me and one other developer.

00:44:21.220 --> 00:44:27.300
Plus maybe sort of some sort of reasonable buy-in from the wider core development team.

00:44:27.680 --> 00:44:35.200
So I wasn't sort of doing the work sort of entirely in isolation or, but yeah, I was still having extra hands will definitely help things.

00:44:35.200 --> 00:44:35.520
Sure.

00:44:35.520 --> 00:44:36.060
Yeah.

00:44:36.060 --> 00:44:39.960
So back when you were thinking you were, this was written at 3.9 timeframe, right?

00:44:39.960 --> 00:44:42.780
And you're like, okay, well the next version, maybe we can do this, the version after that.

00:44:42.780 --> 00:44:46.620
And by the time it really got going, it's more like 3.11, 3.12 and so on, right?

00:44:46.620 --> 00:44:46.820
Yeah.

00:44:46.820 --> 00:44:47.760
It's just around the time.

00:44:47.760 --> 00:44:49.980
I think we switched from 3.9 to 3.10 development.

00:44:49.980 --> 00:44:51.060
I think I was sort of thinking.

00:44:51.260 --> 00:44:51.620
Okay.

00:44:51.620 --> 00:45:01.980
So stage three out of the four stages you have is, I guess, Python 3.13 now, which is a miniature JIT compiler.

00:45:01.980 --> 00:45:03.620
Is that a right characterization?

00:45:03.620 --> 00:45:05.360
I think that's not the compiler.

00:45:05.360 --> 00:45:07.400
Well, I suppose it would be smaller.

00:45:07.400 --> 00:45:09.960
Maybe the parts it applies to, the parts that get compiled.

00:45:09.960 --> 00:45:10.080
Yeah.

00:45:10.080 --> 00:45:19.240
So I think the idea is that you want to compile all of the code where it sort of forms as bad as it's sort of hot code.

00:45:19.240 --> 00:45:27.120
But it makes life easier if you just compile little chunks of code and sort of stitch them together afterwards.

00:45:27.120 --> 00:45:32.460
Because it's very easy to fall back into the interpreter and for the interpreter to jump into sort of compiled code.

00:45:32.460 --> 00:45:38.820
And you can sort of just hang these bits of compiled code off by individual bytecodes where they sort of start from.

00:45:38.820 --> 00:45:43.540
Obviously, that's not fantastic for performance because you're having to fall back into the interpreter,

00:45:43.540 --> 00:45:48.340
which limits your ability to infer things about the state of things.

00:45:48.340 --> 00:45:54.040
So obviously, if you've said earlier, a specialization, you have to do some type checks and other sort of checks.

00:45:54.040 --> 00:45:59.160
If you've done a whole bunch of checks, if you then fall back into the interpreter, you have to throw away all that information.

00:45:59.160 --> 00:46:08.820
If you compile a bigger region of code, which is of the stage four, then you already know something about the code and you can sort of apply those compilations.

00:46:08.940 --> 00:46:16.180
The problem with trying to do big regions upfront is that if you choose poorly, you can make performance worse.

00:46:16.180 --> 00:46:20.400
And this is a real issue for, well, the exact existing ones.

00:46:20.400 --> 00:46:24.180
I think we're going to talk about some of the other historical sort of compilers in the past.

00:46:24.180 --> 00:46:31.560
And this is a real issue for those that they're just trying to compile a method at a time, regardless of whether that is a sensible unit to compile.

00:46:31.560 --> 00:46:35.780
Right. It's sometimes hard to do optimizations when it's too small, right?

00:46:35.780 --> 00:46:41.220
Yeah. And also it's very expensive to do regions that are too big or just in the bounded in the wrong places.

00:46:41.220 --> 00:46:43.320
Okay. Yeah. That definitely sounds tricky.

00:46:43.320 --> 00:46:47.580
Guido, there was a question earlier about, you know, mypyC work and the mypy stuff.

00:46:47.580 --> 00:46:51.500
And you are really central to that, right? Doing a lot of work there.

00:46:51.740 --> 00:46:59.600
How do you, both of you, either of you, feel about using type annotations as some sort of guide to this compiler?

00:46:59.600 --> 00:47:07.820
For example, Cython lets you say, you know, x colon int as a parameter, and it will take that as meaning something when you compile it with Cython.

00:47:07.820 --> 00:47:14.440
It seems like, you know, Mark is talking about knowing the types and guessing them correctly matters in terms of what's fast here.

00:47:14.440 --> 00:47:20.400
Is there any thought or appetite for using type annotations to mean more than static analysis?

00:47:20.640 --> 00:47:27.840
It's a great idea. And I think for smaller code bases, something like mypyC will prove to be viable.

00:47:27.840 --> 00:47:34.700
Or for code bases where there is an incredible motivation to make this happen.

00:47:34.700 --> 00:47:37.280
I could see that happen at Instagram, for example.

00:47:37.280 --> 00:47:44.120
But in general, most people haven't annotated their code completely and correctly.

00:47:44.120 --> 00:47:49.160
And so if you were to switch to using something like mypyC,

00:47:49.540 --> 00:47:52.540
you'd find that basically it wouldn't work.

00:47:52.540 --> 00:47:54.580
A large number of cases.

00:47:54.580 --> 00:48:02.780
And it would basically sort of, it's a different language, and it has different semantics, and it has sort of different rules.

00:48:02.780 --> 00:48:04.800
And so you have to write to that.

00:48:04.800 --> 00:48:09.460
I can see there's a big challenge to say, hey, everybody, we can do this great stuff if you type annotated.

00:48:09.820 --> 00:48:14.260
And only 4% of people have properly annotated their code.

00:48:14.260 --> 00:48:23.140
And then there's also the possibility that it's incorrectly annotated, in which case it probably makes it worse in some way of a crash or something.

00:48:23.140 --> 00:48:29.640
mypyC will generally crash if a type is detected that doesn't match the annotation.

00:48:29.640 --> 00:48:35.140
And if you annotate stuff with simple types, you can get quite good speedup.

00:48:35.240 --> 00:48:38.220
So number is generally designed for numerical stuff.

00:48:38.220 --> 00:48:40.240
But again, it's the simple types, integers, floats.

00:48:40.240 --> 00:48:41.960
Cython, obviously, will do this.

00:48:41.960 --> 00:48:44.480
Number does it dynamically, cython, statically.

00:48:44.480 --> 00:48:49.600
And the number model, for example, is similar to the model that Julia language uses.

00:48:49.980 --> 00:48:56.720
Essentially, you compile method at a time, but you make as many specializations as you need for the particular types.

00:48:56.720 --> 00:48:59.780
And that can give very good performance for that sort of numerical code.

00:48:59.780 --> 00:49:05.860
But the problem is that saying something is a particular type doesn't tell you very much about it.

00:49:05.860 --> 00:49:09.360
It doesn't tell you what attributes an instance of it may or may not have.

00:49:09.360 --> 00:49:22.260
It depends, you know, because you can, it's not like Java or C++ where having a particular class means it has those instance attributes and they will exist or at least there exist in a particular place and they can be checked very efficiently.

00:49:22.260 --> 00:49:26.360
Because if dictionary lookup and so on, these things get a bit fuzzy.

00:49:26.360 --> 00:49:30.940
72 bytes into this C object is where you find the name or something like that, right?

00:49:30.940 --> 00:49:31.120
Yeah.

00:49:31.120 --> 00:49:40.580
So because we basically, because anything might not be as the annotations say effectively at the virtual machine level, we have to check everything.

00:49:40.580 --> 00:49:50.180
And if we're going to check it anyway, we may as well just check it once up ahead as we first do the compilation, whatever specialization, and then assume it's going to be like that.

00:49:50.180 --> 00:49:54.320
Because if the annotations are correct, then that's just as efficient.

00:49:54.320 --> 00:49:59.460
And if the annotations are wrong, we still get some performance benefit and it's robust as well.

00:49:59.460 --> 00:50:16.020
So there's really no, the only advantage of the annotations is for this sort of like very sort of loopy code where we can do things like, you know, loop transformations and so on, because we can infer the types from the arguments of enough of the function to do that stuff.

00:50:16.020 --> 00:50:17.920
And that works great for numerical stuff.

00:50:17.920 --> 00:50:20.140
But for more general code is problematic.

00:50:20.140 --> 00:50:21.100
What about slots?

00:50:21.300 --> 00:50:29.480
Slots are an interesting, not frequently used aspect of Python types that seem to change how things are laid out a little bit.

00:50:29.480 --> 00:50:29.800
Yeah.

00:50:29.800 --> 00:50:30.320
Is that?

00:50:30.320 --> 00:50:39.000
Well, mypyC actually, one of mypyC's main tricks is that it turns every class into a class with slots.

00:50:39.000 --> 00:50:39.420
Okay.

00:50:39.420 --> 00:50:47.780
If you know how slots work, you will immediately see the limitation because it means there are no dynamic attributes at all.

00:50:47.860 --> 00:50:48.000
Yeah.

00:50:48.000 --> 00:50:50.940
These are what you get for your fields and that's it.

00:50:50.940 --> 00:50:51.140
Yeah.

00:50:51.140 --> 00:50:55.540
I mean, if you don't have dynamic attributes, though, it gives you pretty efficient memory use.

00:50:55.540 --> 00:50:57.740
I mean, it's a little too far up Java.

00:50:57.740 --> 00:50:58.480
So it's...

00:50:58.480 --> 00:51:02.460
And more predictability about what's there and what's not, which is why it came to mind.

00:51:02.460 --> 00:51:02.620
Yeah.

00:51:02.620 --> 00:51:02.920
Yeah.

00:51:02.920 --> 00:51:04.180
I mean, they definitely have their use.

00:51:04.180 --> 00:51:04.460
Yeah.

00:51:04.460 --> 00:51:05.100
All right.

00:51:05.380 --> 00:51:17.020
Mark, that was your four-stage plan, hoping to make 1.5 times as fast as before each time, which you do that over four releases, you end up with five times faster, right?

00:51:17.020 --> 00:51:18.220
That's the standard plan.

00:51:18.220 --> 00:51:19.180
Where are we on this?

00:51:19.180 --> 00:51:22.120
How's it going for you and everyone on the team?

00:51:22.120 --> 00:51:30.660
I say it's a bit of a jumble of stages one and two that we're implementing, largely because it's a larger, more diverse team than I was expecting.

00:51:30.660 --> 00:51:33.420
So it makes sense to just sort of spread things.

00:51:33.420 --> 00:51:33.680
Yeah.

00:51:33.680 --> 00:51:33.940
Yeah.

00:51:34.020 --> 00:51:34.160
Yeah.

00:51:34.160 --> 00:51:38.820
You'll work on operators, you go work on zero overhead exception handling and so on.

00:51:38.820 --> 00:51:38.860
Yeah.

00:51:38.860 --> 00:51:49.080
So I would say from where we are now, I was probably a bit optimistic with stage one, but stage two seems to have a lot of potential still.

00:51:49.080 --> 00:51:52.520
There's always little bits of the interpreter we can tweak and improve.

00:51:52.520 --> 00:51:58.220
So between the two of them, I'm confident we'll get this projected over twice the speed.

00:51:58.220 --> 00:51:59.260
That's fantastic.

00:51:59.420 --> 00:52:06.880
So the course you're on right now, if let's just say stage one and two happen, and for some reason the JET stuff doesn't, that's still a big contribution.

00:52:06.880 --> 00:52:09.200
What do you think in terms of speed up for that?

00:52:09.200 --> 00:52:11.240
Well, again, it's going to depend a lot.

00:52:11.240 --> 00:52:13.880
I know it matters so much, but like, you know.

00:52:13.880 --> 00:52:19.900
I mean, I just want to, because like currently we have a sort of set of benchmarks that we're using.

00:52:19.900 --> 00:52:23.080
I mean, not possibly the, I mean, the more benchmarks is always better.

00:52:23.080 --> 00:52:24.680
So it's a broad set.

00:52:24.840 --> 00:52:30.340
Individually, the benchmarks, some of them aren't great, but collectively a form a sort of useful data set.

00:52:30.340 --> 00:52:33.940
But I mean, we speed up from up like up to 60% down to zero.

00:52:33.940 --> 00:52:36.160
So it's definitely a spread.

00:52:36.160 --> 00:52:39.460
So it can, you know, try it out would be the thing.

00:52:39.460 --> 00:52:45.460
I mean, you can download 3.11 Alpha 1 and Alpha 2 should be out a few days at all time now.

00:52:45.460 --> 00:52:48.420
It's presumably before you publish a podcast.

00:52:48.420 --> 00:52:48.760
Yeah.

00:52:48.760 --> 00:52:49.380
Fantastic.

00:52:49.520 --> 00:52:51.260
So people can download it, play it with it.

00:52:51.260 --> 00:52:52.080
Yeah, that's fantastic.

00:52:52.080 --> 00:52:53.200
You know, thank you for this.

00:52:53.200 --> 00:52:58.560
I think even 50, 60%, if it stayed there, that's pretty incredible.

00:52:58.560 --> 00:53:01.000
I mean, this language has been around for 30 years.

00:53:01.000 --> 00:53:03.240
People have been trying to optimize it for a long time.

00:53:03.240 --> 00:53:04.100
It's incredible, right?

00:53:04.100 --> 00:53:08.460
And then, you know, to do this sort of change now, that would be really significant.

00:53:08.460 --> 00:53:08.780
Yeah.

00:53:08.780 --> 00:53:14.740
This is an area that we haven't spent much time on previously for various reasons.

00:53:15.020 --> 00:53:21.520
I mean, people have spent a lot of time on sort of making the string of the objects fast,

00:53:21.520 --> 00:53:27.620
making dictionary operations fast, making the memory efficient, adding functionality that

00:53:27.620 --> 00:53:34.020
the sort of Python has generally, I think, had more of a focus on functionality than on speed.

00:53:34.020 --> 00:53:36.860
And so for me, this is also a change in mindset.

00:53:36.860 --> 00:53:38.240
I'm still learning a lot.

00:53:38.240 --> 00:53:41.880
Mark actually teaches me a lot about how to think about this stuff.

00:53:41.940 --> 00:53:44.840
And I decided to buy this horrible book.

00:53:44.840 --> 00:53:46.320
Well, it's a great book.

00:53:46.320 --> 00:53:47.220
Computer architecture.

00:53:47.220 --> 00:53:52.820
But it's also like it weighs more than a 17-inch laptop.

00:53:52.820 --> 00:53:53.960
Wow.

00:53:53.960 --> 00:53:56.260
Classic text, but not a light read.

00:53:56.260 --> 00:53:56.560
Yeah.

00:53:56.560 --> 00:54:00.340
Down into beyond the software layer, into the hardware bits.

00:54:00.340 --> 00:54:06.540
It makes me amazed that we have any performance at all and that any performance is predictable

00:54:06.540 --> 00:54:13.900
because we're doing everything wrong from the perspective of giving the CPU something to work with.

00:54:13.900 --> 00:54:18.900
I mean, all the algorithms described in there, branch prediction, speculative execution,

00:54:18.900 --> 00:54:25.420
caching of instructions, all that is aimed at small loops of numerical code.

00:54:25.420 --> 00:54:26.620
And we have none of that.

00:54:26.620 --> 00:54:27.140
Yeah.

00:54:27.140 --> 00:54:28.100
Exactly.

00:54:28.100 --> 00:54:30.660
C of LC is not a numerical loop.

00:54:30.660 --> 00:54:31.340
Definitely not.

00:54:31.340 --> 00:54:31.860
All right.

00:54:31.860 --> 00:54:34.200
Well, I think that might be it for the time we have.

00:54:34.200 --> 00:54:36.920
I got a couple of questions from the audience out there.

00:54:36.920 --> 00:54:44.480
Toon Army Captain says, I'm interested in Guido's thoughts about the Microsoft funded effort

00:54:44.480 --> 00:54:49.200
versus the developer in residence, particularly in terms of the major work of the language and

00:54:49.200 --> 00:54:50.660
the CPython runtime going forward.

00:54:50.980 --> 00:54:53.660
I think these are both good things, both really good things.

00:54:53.660 --> 00:54:55.080
They seem super different to me.

00:54:55.080 --> 00:54:57.700
I think it's great that we have a developer in residence.

00:54:57.700 --> 00:55:01.280
It's a very different role than what we're doing here.

00:55:01.280 --> 00:55:07.400
The team at Microsoft is at least we're trying to be super focused on performance to the exclusion

00:55:07.400 --> 00:55:11.620
of almost everything else, except all those constraints I mentioned, of course.

00:55:11.820 --> 00:55:20.520
The developer in residence is focused on sort of the community, other core developers, but

00:55:20.520 --> 00:55:21.700
also contributors.

00:55:21.700 --> 00:55:23.480
Lukasz is great.

00:55:23.480 --> 00:55:25.260
He's the perfect guy for that role.

00:55:25.260 --> 00:55:29.340
And his work is completely orthogonal to what we're doing.

00:55:29.340 --> 00:55:36.860
I hope that somehow the PSF finds funds for keeping the developer in residence role and

00:55:36.860 --> 00:55:39.480
maybe even expanding it for many years.

00:55:39.800 --> 00:55:45.880
seems to me like a really important role to smooth the edges of people contributing to

00:55:45.880 --> 00:55:46.420
CPython.

00:55:46.420 --> 00:55:52.020
And the difference of what Mark and you all are doing is heads down, focused on writing

00:55:52.020 --> 00:55:57.920
one type of code, whereas Lukasz is there to make it easier for everyone else to do whatever

00:55:57.920 --> 00:55:58.760
they were going to do.

00:55:58.760 --> 00:55:59.320
Right.

00:55:59.320 --> 00:56:05.140
And I think, you know, one sort of a horizontal scale of the CPython team and the other is very

00:56:05.140 --> 00:56:06.680
focused, which is also needed.

00:56:06.680 --> 00:56:13.500
It's actually amazing that we've been able to do all the work that we've been doing over

00:56:13.500 --> 00:56:17.460
the past 30 years on Python without a developer in residence.

00:56:17.460 --> 00:56:21.900
I think in the early years, I was probably taking up that role.

00:56:21.900 --> 00:56:28.340
But the last decade or two, there just have been too many issues, too many peps for me to

00:56:28.340 --> 00:56:31.680
sort of get everything going and sort of having...

00:56:31.840 --> 00:56:36.040
I was always working part time on Python and part time working on my day job.

00:56:36.040 --> 00:56:36.280
Right.

00:56:36.280 --> 00:56:37.020
Absolutely.

00:56:37.020 --> 00:56:39.920
Lukasz is working full time on Python.

00:56:39.920 --> 00:56:48.800
And he has a somewhat specific mandate to sort of help people, help sort of contributions

00:56:48.800 --> 00:56:53.320
go smoother, make working with the issue tracker easier.

00:56:53.320 --> 00:56:58.480
So and that sort of developer contributors must be encouraged and rewarded.

00:56:58.480 --> 00:57:06.480
And currently, often the way the bugs.python.org experience is it's a very old web app and it

00:57:06.480 --> 00:57:07.460
looks that way.

00:57:07.460 --> 00:57:12.520
And it's difficult to learn how to do various things with that thing.

00:57:12.520 --> 00:57:15.240
And so Lukasz is really helping people.

00:57:15.240 --> 00:57:16.140
Yeah, it's fantastic.

00:57:16.140 --> 00:57:16.720
With the edges.

00:57:16.720 --> 00:57:22.620
Of course, there's also the somewhat separate project of switching from bugs.python.org to

00:57:22.620 --> 00:57:25.060
a purely GitHub-based tracker.

00:57:25.060 --> 00:57:27.800
Yeah, I was just thinking of that as you were speaking there.

00:57:27.800 --> 00:57:28.620
Do you think that'll help?

00:57:28.620 --> 00:57:31.320
I feel like people are more familiar with that workflow.

00:57:31.320 --> 00:57:32.540
People are more familiar.

00:57:32.540 --> 00:57:37.840
It's more integrated with the pool request flow that we already have on GitHub.

00:57:37.840 --> 00:57:39.400
I think it will be great.

00:57:39.400 --> 00:57:44.820
Expectations is that I think it will be actually happening before the end of this year or very

00:57:44.820 --> 00:57:45.640
early next year.

00:57:45.640 --> 00:57:46.420
That'd be fantastic.

00:57:46.420 --> 00:57:47.660
The code's already there.

00:57:47.660 --> 00:57:48.660
The work's already there.

00:57:48.660 --> 00:57:52.180
Might as well have the conversations and the issues and whatnot there.

00:57:52.180 --> 00:57:52.880
All right, guys.

00:57:52.880 --> 00:57:57.660
I think we are definitely over time, but I really appreciate, first of all, the work

00:57:57.660 --> 00:58:02.020
that you're doing, Mark, on this project and Guido on the last 30 years.

00:58:02.020 --> 00:58:02.620
This is amazing.

00:58:02.620 --> 00:58:06.500
You can see out in the comments how appreciative folks are for all the work you've done.

00:58:06.500 --> 00:58:07.500
So thank you for that.

00:58:07.980 --> 00:58:10.120
Let's close with a final call to action.

00:58:10.120 --> 00:58:12.040
You have the small team working on it.

00:58:12.040 --> 00:58:14.180
I'm sure the community can help in some way.

00:58:14.180 --> 00:58:15.460
What do you want from people?

00:58:15.460 --> 00:58:18.180
How can they help you either now or in the future?

00:58:18.360 --> 00:58:19.780
I mean, it's just contribute to CPython.

00:58:19.780 --> 00:58:23.740
So, I mean, I don't think it's specifically performance.

00:58:23.740 --> 00:58:30.320
I mean, like all the contributions help improve, you know, co-quality and reliability are still

00:58:30.320 --> 00:58:31.380
very important.

00:58:31.380 --> 00:58:35.880
So I don't think particularly people can do.

00:58:35.880 --> 00:58:41.140
But we do have a sort of ideas repo if people do have sort of things they want to suggest

00:58:41.140 --> 00:58:43.420
or bounce ideas around, whatever.

00:58:43.420 --> 00:58:48.300
Maybe they could test their workloads on alpha versions of things like that.

00:58:48.300 --> 00:58:49.800
Yeah, I mean, that would be fantastic.

00:58:49.800 --> 00:58:53.400
I mean, we don't really have a set for where people can put that information.

00:58:53.400 --> 00:58:57.600
But if just open an issue on the ideas thing and post some data, it'd be fantastic.

00:58:57.600 --> 00:59:03.040
We'd love it for people to try to use the new code and see how it works out for them.

00:59:03.040 --> 00:59:04.060
Yeah, fantastic.

00:59:04.060 --> 00:59:04.740
All right.

00:59:04.740 --> 00:59:06.420
Well, thank you both for being here.

00:59:06.420 --> 00:59:07.380
It's been great.

00:59:07.380 --> 00:59:07.780
Our pleasure.

00:59:07.780 --> 00:59:08.580
Thank you.

00:59:08.580 --> 00:59:12.040
This has been another episode of Talk Python to Me.

00:59:12.720 --> 00:59:13.860
Thank you to our sponsors.

00:59:13.860 --> 00:59:15.460
Be sure to check out what they're offering.

00:59:15.460 --> 00:59:16.880
It really helps support the show.

00:59:16.880 --> 00:59:21.760
Choose Shortcut, formerly Clubhouse.io, for tracking all of your project's work.

00:59:21.760 --> 00:59:25.140
Because you shouldn't have to project manage your project management.

00:59:25.140 --> 00:59:28.000
Visit talkpython.fm/shortcut.

00:59:28.000 --> 00:59:33.000
Simplify your infrastructure and cut your cloud bills in half with Linode's Linux virtual machines.

00:59:33.000 --> 00:59:36.340
Develop, deploy, and scale your modern applications faster and easier.

00:59:36.340 --> 00:59:41.300
Visit talkpython.fm/linode and click the Create Free Account button to get started.

00:59:42.020 --> 00:59:45.020
Do you need a great automatic speech-to-text API?

00:59:45.020 --> 00:59:47.560
Get human-level accuracy in just a few lines of code.

00:59:47.560 --> 00:59:50.320
Visit talkpython.fm/assemblyai.

00:59:50.320 --> 00:59:52.180
Want to level up your Python?

00:59:52.180 --> 00:59:56.240
We have one of the largest catalogs of Python video courses over at Talk Python.

00:59:56.240 --> 01:00:01.420
Our content ranges from true beginners to deeply advanced topics like memory and async.

01:00:01.420 --> 01:00:04.080
And best of all, there's not a subscription in sight.

01:00:04.080 --> 01:00:06.980
Check it out for yourself at training.talkpython.fm.

01:00:06.980 --> 01:00:08.900
Be sure to subscribe to the show.

01:00:08.900 --> 01:00:11.660
Open your favorite podcast app and search for Python.

01:00:11.660 --> 01:00:12.980
We should be right at the top.

01:00:12.980 --> 01:00:19.060
You can also find the iTunes feed at /itunes, the Google Play feed at /play, and the direct

01:00:19.060 --> 01:00:22.360
RSS feed at /rss on talkpython.fm.

01:00:23.360 --> 01:00:25.780
We're live streaming most of our recordings these days.

01:00:25.780 --> 01:00:30.240
If you want to be part of the show and have your comments featured on the air, be sure to subscribe

01:00:30.240 --> 01:00:33.560
to our YouTube channel at talkpython.fm/youtube.

01:00:33.560 --> 01:00:35.460
This is your host, Michael Kennedy.

01:00:35.460 --> 01:00:36.760
Thanks so much for listening.

01:00:36.760 --> 01:00:37.920
I really appreciate it.

01:00:38.160 --> 01:00:39.820
Now get out there and write some Python code.

01:00:39.820 --> 01:01:00.420
I'll see you next time.

01:01:00.420 --> 01:01:30.400
Thank you.

