WEBVTT

00:00:00.001 --> 00:00:04.740
When you think about processing tabular data in Python, what library comes to mind?

00:00:04.740 --> 00:00:05.940
Pandas, I'd guess.

00:00:05.940 --> 00:00:10.720
But there are other libraries out there, and Polar's is one of the more exciting new ones.

00:00:10.720 --> 00:00:17.080
It's built in Rust, embraces parallelism, and can be 10 to 20 times faster than Pandas out of the box.

00:00:17.080 --> 00:00:23.060
We have Polar's creator, Richie Vink, here to give us a look at this exciting new data frame library.

00:00:23.060 --> 00:00:29.500
This is Talk Python to Me, episode 402, recorded January 29, 2023.

00:00:29.500 --> 00:00:46.580
Welcome to Talk Python to Me, a weekly podcast on Python.

00:00:46.580 --> 00:00:48.320
This is your host, Michael Kennedy.

00:00:48.320 --> 00:00:55.800
Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython, both on fosstodon.org.

00:00:55.800 --> 00:00:58.400
Be careful with impersonating accounts on other instances.

00:00:58.400 --> 00:00:59.380
There are many.

00:00:59.820 --> 00:01:04.420
Keep up with the show and listen to over seven years of past episodes at talkpython.fm.

00:01:04.420 --> 00:01:08.460
We've started streaming most of our episodes live on YouTube.

00:01:08.460 --> 00:01:16.000
Subscribe to our YouTube channel over at talkpython.fm/youtube to get notified about upcoming shows and be part of that episode.

00:01:17.140 --> 00:01:19.220
This episode is brought to you by Typy.

00:01:19.220 --> 00:01:26.980
Typy is here to take on the challenge of rapidly transforming a bare algorithm in Python into a full-fledged decision support system for end users.

00:01:26.980 --> 00:01:31.620
Check them out at talkpython.fm/Typy, T-A-I-P-Y.

00:01:32.300 --> 00:01:34.640
And it's also brought to you by User Interviews.

00:01:34.640 --> 00:01:38.300
Earn extra income for sharing your software developer opinion.

00:01:38.300 --> 00:01:42.840
Head over to talkpython.fm/userinterviews to participate today.

00:01:43.780 --> 00:01:44.460
Hey, Richie.

00:01:44.460 --> 00:01:46.060
Welcome to Talk Python To Me.

00:01:46.060 --> 00:01:46.580
Hello, Argo.

00:01:46.580 --> 00:01:47.420
Thanks for having me.

00:01:47.420 --> 00:01:48.180
Great to be here.

00:01:48.180 --> 00:01:52.180
I feel like maybe I should rename my podcast TalkRust to me or something.

00:01:52.180 --> 00:01:52.660
I don't know.

00:01:52.660 --> 00:01:53.920
Rust is taking over.

00:01:54.720 --> 00:01:59.380
As the low-level part of how do we make Python go fast?

00:01:59.380 --> 00:02:01.460
There's some kind of synergy with Rust.

00:02:01.460 --> 00:02:02.380
What's going on there?

00:02:02.380 --> 00:02:02.960
Yeah, there is.

00:02:02.960 --> 00:02:09.460
I'd say Python already was low-level languages that succeeded that made Python a success.

00:02:09.460 --> 00:02:16.420
I mean, like NumPy, Pandas, everything that was reasonable fast was so because of C or Cytan, which is also C.

00:02:16.420 --> 00:02:23.140
But Rust, different from C, Rust has made low-level programming a lot more fun to use and a lot more safe.

00:02:23.140 --> 00:02:31.600
And especially if you regard multi-threaded programming, parallel programming, computer programming, it is a lot easier in Rust.

00:02:31.600 --> 00:02:33.060
It opens a lot of possibilities.

00:02:33.060 --> 00:02:34.120
Yeah, my understanding.

00:02:34.120 --> 00:02:39.100
I've only given a cursory look to Rust, just sort of scan some examples.

00:02:39.100 --> 00:02:43.740
And we're going to see some examples of code in a little bit, actually, related to pullers.

00:02:43.740 --> 00:02:45.920
But it's kind of a low-level language.

00:02:45.920 --> 00:02:47.460
It's not as simple as Python.

00:02:47.460 --> 00:02:48.120
No.

00:02:48.120 --> 00:02:59.220
Maybe a JavaScript, but it is easier than C, C++, not just in the syntax, but it does better memory tracking for you and the concurrency especially.

00:02:59.220 --> 00:03:05.760
Yeah, well, so Rust has got a, brings a whole new thing to the table, which is called ownership and a bower checker.

00:03:05.760 --> 00:03:07.140
And Rust is really strict.

00:03:07.140 --> 00:03:10.040
There are things in Rust, you cannot do a C or C++.

00:03:10.040 --> 00:03:14.040
Because at the time, there can only be one owner of a piece of memory.

00:03:14.040 --> 00:03:19.380
And other people can, you can lend out this piece of memory to other users, but then they cannot mutate.

00:03:19.600 --> 00:03:22.600
So it can be only one owner, which is able to mutate something.

00:03:22.600 --> 00:03:27.280
And this restriction makes Rust a really hard language to learn.

00:03:27.280 --> 00:03:37.540
But once you, once it clicked, once you went over that, that steep learning curve, it becomes a lot easier because it doesn't allow you things that you could do in C and C++.

00:03:37.540 --> 00:03:44.540
But those things were also things you shouldn't do in C and C++ because they probably led to sack all the central memory issues.

00:03:44.880 --> 00:03:49.260
And this bower checker also makes writing concurrent programming safe.

00:03:49.260 --> 00:03:53.160
You can have many threads reading a variable all they want.

00:03:53.160 --> 00:03:54.260
They can read concurrently.

00:03:54.260 --> 00:04:01.600
It's when you have writers and readers that this whole thread safety, critical section, take your locks or the locks re-entering.

00:04:01.600 --> 00:04:04.240
All of that really difficult stuff comes in.

00:04:04.240 --> 00:04:08.600
And so, yeah, it sounds like an important key to making that.

00:04:08.600 --> 00:04:13.660
And the same bower checker also knows when memory has to be freed and not.

00:04:13.900 --> 00:04:22.080
But it doesn't have to, unlike in Go or Java, where you have a garbage collector, it doesn't have to do garbage connection and it doesn't have to do reference counting by Python.

00:04:22.080 --> 00:04:24.500
It does so by just statically.

00:04:24.500 --> 00:04:28.280
So at compile time, it knows when something is out of scope and not used anymore.

00:04:28.280 --> 00:04:29.340
And this is real power.

00:04:29.340 --> 00:04:38.200
I guess the takeaway for listeners who are wondering, you know, why is Rust seemingly taking over so much of the job that C and variations of C, right?

00:04:38.260 --> 00:04:41.060
Like you said, Cython have traditionally played in Python.

00:04:41.060 --> 00:04:45.040
It's easier to write modern, faster, safer code.

00:04:45.040 --> 00:04:45.300
Yeah.

00:04:45.300 --> 00:04:45.780
Yeah.

00:04:45.780 --> 00:04:47.280
Probably more fun too, right?

00:04:47.280 --> 00:04:47.940
Yeah, definitely.

00:04:47.940 --> 00:04:51.320
And it's a language which has got its tools, right?

00:04:51.440 --> 00:04:54.200
So it's got a package manager, which is really great to use.

00:04:54.200 --> 00:04:58.120
It's got a real great.co, which is similar to the PyPy index.

00:04:58.120 --> 00:04:59.740
It feels like a modern language.

00:04:59.740 --> 00:05:00.040
Yeah.

00:05:00.040 --> 00:05:02.800
Builds low-level, more low-level code.

00:05:02.800 --> 00:05:12.900
You can also write high-level stuff like REST APIs, which is, I will say, also for high-level stuff, I like to write it in Rust because of the safety guarantees.

00:05:13.280 --> 00:05:23.880
And also the correctness guarantees if my program compiles REST, I'm much more certain it is correct than when I write my Python program, which is dynamic and types can, are not enforced.

00:05:23.880 --> 00:05:26.460
So it's always been preying on that side.

00:05:26.460 --> 00:05:26.840
Yeah.

00:05:26.840 --> 00:05:30.220
Python is great to use, but it's harder to write correct code in Python.

00:05:30.720 --> 00:05:41.180
Yeah, and you can optionally write very loose code, or you could opt in to things like type hints and even mypy, and then you get closer to the static languages, right?

00:05:41.180 --> 00:05:43.460
Are you a fan of Python typing?

00:05:43.460 --> 00:05:44.060
Definitely.

00:05:44.060 --> 00:05:47.940
But because they're optional, they are as strong as the weakest link.

00:05:47.940 --> 00:05:54.400
So one library which you use, if it doesn't do the static correct or doesn't do it, it breaks.

00:05:54.400 --> 00:05:56.460
It's quite brittle because it's optional.

00:05:56.980 --> 00:06:00.700
I hope we get something really enforcative and really can check it.

00:06:00.700 --> 00:06:04.300
I'd love to know if it's possible because of the dynamic nature of Python.

00:06:04.300 --> 00:06:07.480
Python can do so many things, jump dynamically.

00:06:07.480 --> 00:06:12.600
And technically, we just cannot know from, I don't know how far it can go.

00:06:12.600 --> 00:06:13.460
But yeah.

00:06:13.460 --> 00:06:19.700
In PowerRest as well, we use mypy type hints, which prevent us from having a nutbox.

00:06:19.700 --> 00:06:22.580
Most of the way, the IDE experience much nicer.

00:06:22.580 --> 00:06:23.260
Yeah.

00:06:23.260 --> 00:06:24.080
Pypins are great.

00:06:24.080 --> 00:06:26.480
They really help you also think about your library.

00:06:26.860 --> 00:06:33.000
I think you really see a shift in modern Python and Python 10 years ago where it was more dynamic.

00:06:33.000 --> 00:06:39.920
And then the dynamic, I remember, I thought it was more seen as a strength than currently I've made.

00:06:39.920 --> 00:06:41.140
Yeah, I totally agree.

00:06:41.140 --> 00:06:47.380
And I feel like when type hints first came out, you know, this was, yes, wow, at this point, kind of early Python 3.

00:06:47.380 --> 00:06:49.800
But it didn't feel like it at the time, you know.

00:06:49.800 --> 00:06:51.740
Python 3 had been out for quite a while.

00:06:51.740 --> 00:06:55.440
When type hints were introduced, I feel like that was Python 3, 4.

00:06:55.620 --> 00:06:59.580
But anyway, that was, put it maybe six years into the life cycle of Python 3.

00:06:59.580 --> 00:07:03.820
But still, I feel like a lot of people were suspicious of that at the moment.

00:07:03.820 --> 00:07:03.980
Yeah.

00:07:03.980 --> 00:07:06.200
You know, they're like, oh, what is this weird thing?

00:07:06.200 --> 00:07:09.280
We're not really sure we want to put these types into our Python.

00:07:09.280 --> 00:07:10.800
And now a lot less.

00:07:11.180 --> 00:07:13.160
There's a lot less of those reactions.

00:07:13.160 --> 00:07:13.360
Yeah.

00:07:13.360 --> 00:07:13.640
I see.

00:07:13.640 --> 00:07:14.180
Yeah.

00:07:14.180 --> 00:07:14.520
Yeah.

00:07:14.520 --> 00:07:17.240
I see Python having two, probably more.

00:07:17.240 --> 00:07:28.000
But I often see Python as the really fun, nice to own duct tape language where I can, in my, for instance, in Jupyter notebook, I can just hack away and try interactively what happens.

00:07:28.000 --> 00:07:30.440
And for such code, type hints don't matter.

00:07:30.440 --> 00:07:36.100
But once I write more of a library or product or tool, then type hints were really great.

00:07:36.100 --> 00:07:39.240
I believe they came about that Dropbox really needed them.

00:07:39.240 --> 00:07:40.980
They have a huge Python project.

00:07:40.980 --> 00:07:42.240
It's a really tough one.

00:07:42.240 --> 00:07:44.600
I'm not really sure.

00:07:44.660 --> 00:07:44.820
Yeah.

00:07:44.820 --> 00:07:47.280
And I heard some guy who has something to do with Python used to work there.

00:07:47.280 --> 00:07:47.660
Yeah.

00:07:47.660 --> 00:07:47.860
Yeah.

00:07:47.860 --> 00:07:48.260
Yeah.

00:07:48.260 --> 00:07:48.980
Guido used to work there.

00:07:48.980 --> 00:07:50.580
I think even at that time.

00:07:50.580 --> 00:07:51.060
All right.

00:07:51.060 --> 00:07:54.180
So a bit of a diversion from how I often start the show.

00:07:54.180 --> 00:07:56.560
So let's just circle back real quick and get your story.

00:07:56.560 --> 00:08:00.000
How did you get into programming and Python and Rust as well, I suppose?

00:08:00.000 --> 00:08:01.080
I got into programming.

00:08:01.080 --> 00:08:02.880
I just wanted to learn programming.

00:08:02.880 --> 00:08:07.280
A friend of mine who did program a lot of PHP said, learn Python.

00:08:07.280 --> 00:08:07.880
Like that.

00:08:07.880 --> 00:08:12.460
He gave me an interactive website where I could do some puzzles.

00:08:12.460 --> 00:08:14.360
And I really got hooked to it.

00:08:14.580 --> 00:08:15.980
It was a fun summer.

00:08:15.980 --> 00:08:16.880
Yeah.

00:08:16.880 --> 00:08:18.260
I was programming a lot.

00:08:18.260 --> 00:08:19.400
I started automating.

00:08:19.400 --> 00:08:22.220
My job was a civil engineer at the moment.

00:08:22.220 --> 00:08:22.900
And I started.

00:08:22.900 --> 00:08:24.500
It was a lot of mundane tasks.

00:08:24.500 --> 00:08:25.240
It's been repetitive.

00:08:25.240 --> 00:08:27.840
And I just found ways to automate my job.

00:08:27.840 --> 00:08:31.480
And eventually I was doing that for a year or three or four.

00:08:31.480 --> 00:08:34.420
And then I got into data science and I switched jobs.

00:08:34.420 --> 00:08:37.620
I became a data scientist and later a data engineer.

00:08:37.620 --> 00:08:38.200
Yeah.

00:08:38.200 --> 00:08:39.680
So that was Python mostly.

00:08:39.680 --> 00:08:44.280
I've always been looking for more languages, playing with Hasbro,

00:08:44.500 --> 00:08:50.000
playing with Go, playing with Dubstrip, or just playing with Scala.

00:08:50.000 --> 00:08:51.180
And then I found Rust.

00:08:51.180 --> 00:08:53.140
Rust really, really.

00:08:53.140 --> 00:08:56.980
And you had, like, you learn a lot about how computers work.

00:08:56.980 --> 00:08:57.340
Yeah.

00:08:57.380 --> 00:09:00.780
So I had a new renaissance of the first experience with Python.

00:09:00.780 --> 00:09:02.340
And another summer with graphs.

00:09:02.340 --> 00:09:04.060
And I'm doing a lot of the project.

00:09:04.060 --> 00:09:06.440
But right there.

00:09:06.440 --> 00:09:07.100
I don't know.

00:09:07.100 --> 00:09:08.140
A lot of projects.

00:09:08.140 --> 00:09:12.680
And all this game, one of those hobby projects just to use last month.

00:09:12.680 --> 00:09:14.800
Now it's got quite the following.

00:09:14.800 --> 00:09:16.960
And we're going to definitely dive into that.

00:09:16.960 --> 00:09:18.440
But let me pull it up.

00:09:18.440 --> 00:09:19.220
It does right here.

00:09:19.220 --> 00:09:21.060
13,000 GitHub stars.

00:09:21.060 --> 00:09:25.120
That's a good number of people using that project.

00:09:25.120 --> 00:09:25.440
Yeah.

00:09:25.440 --> 00:09:25.660
Yeah.

00:09:25.660 --> 00:09:26.340
Crazy, isn't it?

00:09:26.340 --> 00:09:27.120
Yeah, it is.

00:09:27.120 --> 00:09:29.180
And it's on GitHub stars.

00:09:29.180 --> 00:09:30.760
It's the process where I will get it to.

00:09:31.020 --> 00:09:32.020
Wow.

00:09:32.020 --> 00:09:32.800
Incredible.

00:09:32.800 --> 00:09:35.080
You must be really proud of that.

00:09:35.080 --> 00:09:35.560
Yeah.

00:09:35.560 --> 00:09:36.060
Yeah.

00:09:36.060 --> 00:09:39.540
If you would have told me this two years ago, I would never be.

00:09:39.540 --> 00:09:43.540
But it happens slow enough so you can get accustomed to that.

00:09:43.540 --> 00:09:43.820
Yeah.

00:09:43.820 --> 00:09:44.780
That's cool.

00:09:44.780 --> 00:09:46.280
Kind of like being a parent.

00:09:46.280 --> 00:09:48.900
The challenges of the kids are small.

00:09:48.900 --> 00:09:52.140
They're intense, but there are only a few things they need when they're small.

00:09:52.140 --> 00:09:53.260
And you kind of grow with it.

00:09:53.260 --> 00:09:53.440
Yeah.

00:09:53.440 --> 00:09:54.900
So a couple of thoughts.

00:09:54.900 --> 00:10:00.900
One, you had the inverse style of learning to program that I think a lot of computer science

00:10:00.900 --> 00:10:02.700
people do and certainly that I did.

00:10:02.700 --> 00:10:05.440
It could also just be that I learned it a long time ago.

00:10:05.440 --> 00:10:09.660
But when I learned programming, it was, I'm going to learn C and C++.

00:10:09.660 --> 00:10:13.520
And then you're kind of allowed to learn the easier languages.

00:10:13.520 --> 00:10:15.620
But you will learn your pointers.

00:10:15.620 --> 00:10:18.360
You'll have your void star, star, and you're going to like it.

00:10:18.360 --> 00:10:20.600
You're going to understand what a pointer to a pointer means.

00:10:20.600 --> 00:10:26.320
And we're going to get, I mean, you start inside of the most complex,

00:10:26.320 --> 00:10:27.120
closest to the machine.

00:10:27.120 --> 00:10:27.900
You work your way out.

00:10:27.900 --> 00:10:29.300
You kind of took this opposite.

00:10:29.300 --> 00:10:32.140
Like, let me learn Python where it's much more high level.

00:10:32.140 --> 00:10:37.920
It's much, you know, if you choose to be often say very much more away from the hardware and

00:10:37.920 --> 00:10:40.340
the ideas of memories and threads and all that.

00:10:40.340 --> 00:10:41.660
And then you went to Rust.

00:10:41.660 --> 00:10:43.880
So was it kind of an intense experience?

00:10:43.880 --> 00:10:46.100
You're like, oh my gosh, this is intense.

00:10:46.100 --> 00:10:49.060
Or had you studied enough languages by then to become comfortable?

00:10:49.260 --> 00:10:50.520
Well, yeah, yeah, no.

00:10:50.520 --> 00:10:55.500
So the going from high level to low language, I think it makes natural sense.

00:10:55.500 --> 00:10:56.720
You're learning it yourself.

00:10:56.720 --> 00:10:59.700
There's no professor telling me you learn your pointers.

00:10:59.700 --> 00:11:00.180
Yeah.

00:11:00.180 --> 00:11:06.260
I think this also helped a lot because at that point you're really custom programming to algorithms.

00:11:06.420 --> 00:11:10.500
Yeah, but you can, I believe you should learn one thing, one new thing at a time.

00:11:10.500 --> 00:11:13.660
And that you can really own that knowledge that you're on.

00:11:13.660 --> 00:11:17.660
But for us, I wouldn't say you should learn Rust as in first language.

00:11:17.660 --> 00:11:21.520
It'd be really terrible because you meet, terrible.

00:11:21.680 --> 00:11:26.840
But other languages also don't help you much because the power checker is quite unique.

00:11:26.840 --> 00:11:29.240
It doesn't let you do things you can do in other languages.

00:11:29.240 --> 00:11:34.260
So what you, what you learn there, the languages that allow you to do that, they just encourage

00:11:34.260 --> 00:11:38.320
you because you were, they encourage the wrong behavior, right?

00:11:38.320 --> 00:11:39.120
Well, yeah.

00:11:39.120 --> 00:11:45.140
So nine out of, nine out of 10 times, it turns out by compiling, not letting you do that one

00:11:45.140 --> 00:11:48.460
thing, that one thing you wanted was probably really bad to begin with.

00:11:48.720 --> 00:11:52.340
that to really, so in Rust, your code is always a lot flatter.

00:11:52.340 --> 00:11:56.640
It's always really clear who owns the memory, how deep your nesting is.

00:11:56.640 --> 00:12:01.720
It's always one degree or most of the times it's, it's not that complicated.

00:12:01.720 --> 00:12:05.680
You, you make things really flat and really easy to reason about.

00:12:05.680 --> 00:12:11.400
And in the beginning of the project, it seems okay, a bit over constraining, but when, I mean,

00:12:11.400 --> 00:12:16.760
software will become complex and complicated and then you're happy to compile a notch to this.

00:12:16.760 --> 00:12:17.720
Yeah, absolutely.

00:12:17.720 --> 00:12:18.400
In this direction.

00:12:18.400 --> 00:12:19.560
It seems like a better way.

00:12:19.560 --> 00:12:26.080
Honestly, you know, you get a sense of programming in a more simple language that doesn't ask so many

00:12:26.080 --> 00:12:27.840
low level concepts of you.

00:12:27.840 --> 00:12:28.840
And then you're ready.

00:12:28.840 --> 00:12:30.560
You, you can add on these new ones.

00:12:30.560 --> 00:12:34.640
so I feel like a lot of how we teach programming and how people learn programming is

00:12:34.640 --> 00:12:36.080
a little bit backwards, to be honest.

00:12:36.080 --> 00:12:36.480
Yeah.

00:12:36.480 --> 00:12:37.440
Anyway, enough on that.

00:12:37.440 --> 00:12:42.160
So you were a civil engineer for a while and then you became a data scientist and now you've

00:12:42.160 --> 00:12:43.120
created this library.

00:12:43.120 --> 00:12:44.960
Still working as a data scientist now?

00:12:44.960 --> 00:12:45.400
No, no.

00:12:45.400 --> 00:12:48.880
I got sponsored two years ago for two days a week.

00:12:49.120 --> 00:12:53.280
And yeah, just to use the time to get a polar.

00:12:53.280 --> 00:12:56.440
And currently I stopped all my day jobs.

00:12:56.440 --> 00:13:01.240
I'm going full time on all the other side and trying to live on sponsorships, which is not

00:13:01.240 --> 00:13:02.600
really working.

00:13:02.600 --> 00:13:03.880
It's not enough at this time.

00:13:03.880 --> 00:13:06.880
I hope to start a foundation and get some proper sponsors.

00:13:06.880 --> 00:13:07.520
Yeah.

00:13:07.520 --> 00:13:08.240
That'd be great.

00:13:08.240 --> 00:13:08.800
Yeah.

00:13:08.800 --> 00:13:08.960
Yeah.

00:13:08.960 --> 00:13:10.080
That's awesome.

00:13:10.080 --> 00:13:10.480
Yeah.

00:13:10.480 --> 00:13:14.400
It's still awesome that you're able to do that, even if, you know, you still needed to grow

00:13:14.400 --> 00:13:14.960
a little bit.

00:13:14.960 --> 00:13:15.280
Yeah.

00:13:15.280 --> 00:13:19.520
We'll have you on a podcast and let other people know out there who, who may be using your

00:13:19.520 --> 00:13:20.080
library.

00:13:20.080 --> 00:13:20.080
Yeah.

00:13:20.080 --> 00:13:23.760
Maybe they can, you know, put a little sponsorship and get up sponsors.

00:13:23.760 --> 00:13:29.280
I feel like get up sponsors really made it a lot easier for people to, to support.

00:13:29.280 --> 00:13:35.120
Cause there used to be like PayPal donate buttons and other, other things like that.

00:13:35.120 --> 00:13:37.520
And one, those are not really recurring.

00:13:37.520 --> 00:13:40.720
And two, you've got to go find someplace and put your credit card.

00:13:40.720 --> 00:13:44.160
You know, many of us already have a credit card registered at GitHub.

00:13:44.160 --> 00:13:48.000
It's just a matter of checking a box and monthly it'll just go, you know, it's kind of like the

00:13:48.000 --> 00:13:49.920
app store versus buying independent apps.

00:13:49.920 --> 00:13:51.680
It just cuts down a lot of the friction.

00:13:51.680 --> 00:13:55.120
I feel like it's been really positive mostly for open source.

00:13:55.120 --> 00:13:55.280
Yeah.

00:13:55.280 --> 00:13:58.480
I think it's good to, as a way to say, thank you.

00:13:58.480 --> 00:14:00.800
With, it isn't enough to pay the bills.

00:14:00.800 --> 00:14:04.240
I think for most developers, it isn't, but, I hope we get there.

00:14:04.240 --> 00:14:08.480
The companies, who use it to give a bit more, bit more back.

00:14:08.480 --> 00:14:09.520
I mean, we have a lot of money.

00:14:09.520 --> 00:14:10.240
I agree.

00:14:10.240 --> 00:14:16.560
It's really, really ridiculous that there are banks and VC funded companies and things like

00:14:16.560 --> 00:14:21.200
that, that have not necessarily in terms of the VC ones, but definitely in terms of

00:14:21.200 --> 00:14:26.480
financial and other large companies that make billions and billions of dollars in profit on top of

00:14:26.480 --> 00:14:27.680
open source technology.

00:14:27.680 --> 00:14:27.920
Yeah.

00:14:27.920 --> 00:14:33.120
And many of them don't give anything back, which is, it's not criminal because the licenses allow it,

00:14:33.120 --> 00:14:39.680
but it's, it's certainly borders on immoral to say, we're making all this money and not at all support.

00:14:39.680 --> 00:14:42.800
The people who are really building the foundations that we build upon.

00:14:42.800 --> 00:14:44.560
Most of my sponsors are developers.

00:14:44.560 --> 00:14:44.880
Yeah.

00:14:44.880 --> 00:14:45.440
Yeah.

00:14:45.440 --> 00:14:45.760
Yeah.

00:14:45.760 --> 00:14:48.560
So, yeah, we'll let's hope it changes.

00:14:48.560 --> 00:14:49.280
I don't know.

00:14:49.280 --> 00:14:49.520
Yeah.

00:14:49.520 --> 00:14:52.000
Well, I'll continue to beat that drum.

00:14:52.000 --> 00:14:57.680
This portion of talk Python to me is brought to you by type.

00:14:57.680 --> 00:14:58.560
I type.

00:14:58.560 --> 00:14:58.720
I type.

00:14:58.720 --> 00:15:02.560
I is the next generation open source Python application builder with type.

00:15:02.560 --> 00:15:06.880
I, you can turn data and AI algorithms into full web apps in no time.

00:15:06.880 --> 00:15:07.840
Here's how it works.

00:15:07.840 --> 00:15:10.960
You start with a bear algorithm written in Python.

00:15:10.960 --> 00:15:12.160
You then use type.

00:15:12.160 --> 00:15:17.840
I is innovative tool set that enables Python developers to build interactive end user applications quickly.

00:15:17.840 --> 00:15:22.480
There's a visual designer to develop highly interactive GUIs ready for production.

00:15:22.480 --> 00:15:27.040
And for inbound data streams, you can program against the type I core layer as well.

00:15:27.040 --> 00:15:33.200
I buy core provides intelligent pipeline management, data caching, and scenario and cycle management facilities.

00:15:33.200 --> 00:15:34.240
That's it.

00:15:34.240 --> 00:15:39.840
You'll have transformed a bear algorithm into a full fledged decision support system for end users.

00:15:39.840 --> 00:15:44.960
I buy is pure Python and open source and you install with a simple pip install type.

00:15:44.960 --> 00:15:49.840
For large organizations that need fine grained control and authorization around their data.

00:15:49.840 --> 00:15:56.000
There is a paid type I enterprise edition, but the type I core and GUI described above is completely free to use.

00:15:56.000 --> 00:16:03.040
Learn more and get started by visiting talkpython.fm/type I that's T-A-I-P-Y.

00:16:03.040 --> 00:16:04.080
The links in your show notes.

00:16:04.080 --> 00:16:06.880
Thank you to type I for sponsoring the show.

00:16:06.880 --> 00:16:10.640
Let's talk about your project.

00:16:10.640 --> 00:16:15.440
So Polars and the RS is for rust, I imagine at the end.

00:16:15.440 --> 00:16:15.760
Yeah.

00:16:15.760 --> 00:16:19.040
But tell us about the name Polars, like polar bear, but Polars.

00:16:19.040 --> 00:16:19.520
Yeah.

00:16:19.520 --> 00:16:26.000
So I started writing a data from library and initially it was only for for us, my ID or two.

00:16:26.000 --> 00:16:29.360
You can tell you, so all the people doing data science and Python, you're like, wow.

00:16:29.360 --> 00:16:29.840
Yeah.

00:16:29.840 --> 00:16:29.840
Yeah.

00:16:29.840 --> 00:16:30.080
Yeah.

00:16:30.080 --> 00:16:31.200
What can I do for these people?

00:16:31.200 --> 00:16:31.440
Right?

00:16:31.440 --> 00:16:31.840
Yeah.

00:16:31.840 --> 00:16:32.320
Yeah.

00:16:32.320 --> 00:16:37.280
And I wanted to give a week to the ponders project, but I wanted a bear that was better,

00:16:37.280 --> 00:16:38.960
faster, I don't know, stronger.

00:16:38.960 --> 00:16:43.680
So luckily a panda bear is in the most practical bear.

00:16:43.680 --> 00:16:49.600
So I had a few to choose, but the Grizzly, yeah, the polar has the RS.

00:16:49.600 --> 00:16:51.200
So that's a lot of the incentives.

00:16:51.200 --> 00:16:51.600
Yeah.

00:16:51.600 --> 00:16:52.400
So, yeah.

00:16:52.400 --> 00:16:57.680
So the subtitle here is lightning fast data frame library for rust and Python.

00:16:57.680 --> 00:17:00.160
And you have two APIs that people can use.

00:17:00.160 --> 00:17:01.520
We'll get to dive into those.

00:17:01.520 --> 00:17:01.920
Yeah.

00:17:01.920 --> 00:17:04.160
Because we read an angle in rust.

00:17:04.160 --> 00:17:06.560
It's a complete data frame library in rust.

00:17:06.560 --> 00:17:10.160
You can expose it to many fun bits, but it is already front-end thing.

00:17:10.160 --> 00:17:15.440
Well, Python, Node.js, R is coming up and normal JavaScript is coming up.

00:17:15.440 --> 00:17:18.320
And Ruby, there is also a polarity group.

00:17:18.320 --> 00:17:20.080
So, interesting.

00:17:20.080 --> 00:17:23.280
So for the JavaScript one, are you going to use WebAssembly?

00:17:23.280 --> 00:17:23.520
Yeah.

00:17:23.520 --> 00:17:23.840
Right.

00:17:23.840 --> 00:17:27.120
Which is pretty straightforward because rust comes from Mozilla.

00:17:27.120 --> 00:17:31.680
WebAssembly, I believe also originated, they kind of originated as a, somewhat tied

00:17:31.680 --> 00:17:32.160
together.

00:17:32.160 --> 00:17:32.560
Yeah.

00:17:32.560 --> 00:17:32.960
Story.

00:17:32.960 --> 00:17:36.400
So, Rust C++ C can compile to WebAssembly.

00:17:36.400 --> 00:17:41.040
It's not really straightforward because the WebAssembly virtual machine isn't like your normal

00:17:41.040 --> 00:17:41.440
OS.

00:17:41.440 --> 00:17:45.520
So there are a lot of things harder, but we're, we are working on the challenges.

00:17:45.520 --> 00:17:45.920
Okay.

00:17:45.920 --> 00:17:47.120
Well, that's pretty interesting.

00:17:47.120 --> 00:17:50.560
But for now you've got Python and you've got Rust and that's great.

00:17:50.560 --> 00:17:55.840
Let's, I think a lot of people listening, myself included, when I started looking into this,

00:17:55.840 --> 00:18:02.320
immediately go to, it's like pandas, but rust, you know, it's like pandas, but instead of C at

00:18:02.320 --> 00:18:04.480
the bottom, it's, it's rust at the bottom.

00:18:04.480 --> 00:18:07.760
And that's somewhat true, but mostly not true.

00:18:07.760 --> 00:18:12.720
So let's start with you telling us, you know, how is this like pandas and how is it different

00:18:12.720 --> 00:18:13.360
from pandas?

00:18:13.360 --> 00:18:13.920
Yeah.

00:18:13.920 --> 00:18:15.920
So it's not like pandas.

00:18:15.920 --> 00:18:18.480
I think it's different on two ways.

00:18:18.480 --> 00:18:23.280
So we have the API and we have the application and which one should I start with?

00:18:23.280 --> 00:18:24.640
I think.

00:18:24.640 --> 00:18:25.440
Yeah.

00:18:25.440 --> 00:18:25.440
Yeah.

00:18:25.440 --> 00:18:25.920
Bottom up.

00:18:25.920 --> 00:18:26.160
Sure.

00:18:26.160 --> 00:18:26.560
All right.

00:18:26.560 --> 00:18:29.440
So that was my critique from pandas.

00:18:29.440 --> 00:18:30.960
And they didn't start by the library.

00:18:30.960 --> 00:18:32.000
And they didn't start by the library.

00:18:32.000 --> 00:18:36.800
They do whatever was there already with work, do it for that purpose.

00:18:36.800 --> 00:18:39.120
And pandas built on NumPy.

00:18:39.120 --> 00:18:40.960
And NumPy is a great library.

00:18:40.960 --> 00:18:44.000
It's built from numerical process and not from relational.

00:18:44.000 --> 00:18:47.200
Versus in relational data is completely different.

00:18:47.200 --> 00:18:49.040
You have string data, nested data.

00:18:49.040 --> 00:18:54.400
And this data has probably put as Python object in those NumPy lines.

00:18:54.400 --> 00:19:00.240
And if you know anything about memory, then in this array, you have pointer with where each

00:19:00.240 --> 00:19:01.600
Python object is somewhere else.

00:19:01.600 --> 00:19:05.600
So if you traverse this memory, every point you must look it up somewhere else.

00:19:05.600 --> 00:19:11.520
But memory is not a cache where the cache means, which is a 200x slowdown per element to traverse.

00:19:11.520 --> 00:19:12.080
Yeah.

00:19:12.080 --> 00:19:18.160
So for people listening, what you're saying, the 200x slowdown is the L1, L2, L3 caches,

00:19:18.160 --> 00:19:23.200
which all have different speeds and stuff, but the caches that are near the CPU versus

00:19:23.200 --> 00:19:28.480
main memory, it's like two to 400 times slower, not aging off a disk or something.

00:19:28.480 --> 00:19:29.520
It's really different, right?

00:19:29.520 --> 00:19:30.400
It's really a big deal.

00:19:30.400 --> 00:19:31.120
It's a big deal.

00:19:31.120 --> 00:19:32.320
It's terribly slow.

00:19:32.320 --> 00:19:34.240
It also, Python has a GIL.

00:19:34.240 --> 00:19:36.560
It also blocks multi-treading.

00:19:36.560 --> 00:19:40.160
If you want to read the string, you cannot do this from different threads.

00:19:40.160 --> 00:19:43.680
If you want to modify the string, there's only one thread that can access Python.

00:19:43.680 --> 00:19:49.520
They also didn't take into account anything from databases.

00:19:49.520 --> 00:19:53.120
So databases are based in from the 1950s.

00:19:53.120 --> 00:19:59.120
There's been a lot of research in databases, how we do things fast, write a query, and then

00:19:59.120 --> 00:20:03.360
optimize this query because the user that uses your library is not the expert.

00:20:03.360 --> 00:20:04.880
It doesn't write optimized query.

00:20:04.880 --> 00:20:09.680
No, but we have a lot of information, so we can optimize this query and execute this in

00:20:09.680 --> 00:20:11.440
the most, in a very efficient way.

00:20:11.440 --> 00:20:12.800
Well, that's an interesting idea.

00:20:12.800 --> 00:20:13.200
Yeah.

00:20:13.200 --> 00:20:16.640
And Pandas just executes it and gives you what you ask.

00:20:16.640 --> 00:20:18.480
And what you ask is probably not.

00:20:18.480 --> 00:20:24.400
Yeah, that's interesting because as programmers, when I have my Python hat on, I want my code

00:20:24.400 --> 00:20:26.240
to run exactly as I wrote it.

00:20:26.240 --> 00:20:29.520
I don't want it to get clever and change it.

00:20:29.520 --> 00:20:31.760
I, you know, if I said do a loop, do a loop.

00:20:31.760 --> 00:20:34.880
If I, if I said put it in a dictionary, put it in a dictionary.

00:20:34.880 --> 00:20:41.360
But when I write a database query, be that against Postgres with relational or MongoDB,

00:20:41.360 --> 00:20:45.840
there's a query planner and the query planner looks at all the different steps.

00:20:45.840 --> 00:20:47.280
Should we do the filter first?

00:20:47.280 --> 00:20:48.400
Can we use an index?

00:20:48.400 --> 00:20:49.760
Can we use a compo?

00:20:49.760 --> 00:20:51.520
Which index should we choose?

00:20:51.520 --> 00:20:52.880
All of those things, right?

00:20:52.880 --> 00:20:58.240
And so what you tell it and what happens, you don't tell it how to do finding the data,

00:20:58.240 --> 00:20:58.800
the database.

00:20:58.800 --> 00:21:04.240
You just give it, here's kind of the expressions that I need, the predicates that I need you

00:21:04.240 --> 00:21:04.800
to work with.

00:21:04.800 --> 00:21:06.080
And then you figure it out.

00:21:06.080 --> 00:21:06.560
You're smart.

00:21:06.560 --> 00:21:07.600
You're the database.

00:21:07.600 --> 00:21:12.880
So one of the differences I got from reading what, what you've got here so far is it looks

00:21:12.880 --> 00:21:16.640
like, I don't know if it goes as far as this database stuff that we're talking about,

00:21:16.640 --> 00:21:20.960
but there's a way for it to build up the code it's supposed to run.

00:21:20.960 --> 00:21:25.360
And it can decide things like, you know, these two things could go in parallel or things along

00:21:25.360 --> 00:21:25.920
those lines.

00:21:25.920 --> 00:21:26.160
Right?

00:21:26.160 --> 00:21:26.560
Yeah.

00:21:26.560 --> 00:21:26.800
Yeah.

00:21:26.800 --> 00:21:29.040
Well, it is actually very similar.

00:21:29.040 --> 00:21:30.160
It is a vectorized query.

00:21:30.160 --> 00:21:35.200
And you can, the only thing that doesn't make us a database is that we don't have any,

00:21:35.200 --> 00:21:40.880
we don't bother with the, with file structures that we write, like the persistence and transactions

00:21:40.880 --> 00:21:41.360
and all that.

00:21:41.360 --> 00:21:41.360
Yeah.

00:21:41.360 --> 00:21:46.400
So we have different kinds of databases, you have OLAP and OLTP, transactional

00:21:46.400 --> 00:21:48.720
modeling, which works often on one.

00:21:48.720 --> 00:21:53.840
So if you do a REST API query and you modify one user ID, then you're transactional.

00:21:53.840 --> 00:21:56.160
And if you're doing OLAP, that's more analytical.

00:21:56.160 --> 00:21:59.840
And then you do large aggregations of large whole tables.

00:21:59.840 --> 00:22:03.200
And then you need to process all the data and those different databases,

00:22:03.200 --> 00:22:05.520
these eyes lead to different query optimizers.

00:22:05.520 --> 00:22:07.040
And OLAP is focused on OLAP.

00:22:07.040 --> 00:22:11.360
But yeah, we, so as you described, you've got two ways of programming things.

00:22:11.360 --> 00:22:14.400
One is procedural, which Python mostly is.

00:22:14.400 --> 00:22:19.360
So you tell exactly if you want to get a couple coffee, how many steps should take forward.

00:22:19.360 --> 00:22:21.600
They rotate 90 degrees, take three steps.

00:22:21.600 --> 00:22:23.040
They rotate 90 degrees.

00:22:23.040 --> 00:22:25.680
You can put, write down the whole algorithm, how to get a coffee.

00:22:25.680 --> 00:22:27.680
Or you could just say, get me a coffee.

00:22:27.680 --> 00:22:34.160
I'd like to sell sugar and then let the algorithm, let query engine decide how to best get.

00:22:34.160 --> 00:22:34.160
Right.

00:22:34.160 --> 00:22:35.840
And that's more declarative.

00:22:35.840 --> 00:22:37.760
You describe the end result.

00:22:37.760 --> 00:22:42.640
And as it turns out, this is also very readable because you declare what you want and the intent

00:22:42.640 --> 00:22:44.960
is readable in the query.

00:22:44.960 --> 00:22:48.880
And if you're doing more procedural programming, you describe what you're doing.

00:22:48.880 --> 00:22:51.760
And the intent often needs to come from comments.

00:22:51.760 --> 00:22:54.320
Like what are we trying to do when we follow this?

00:22:54.320 --> 00:22:54.800
Right.

00:22:54.800 --> 00:22:54.960
Yeah.

00:22:54.960 --> 00:22:55.840
That makes a lot of sense.

00:22:55.840 --> 00:22:57.200
And that's very interesting.

00:22:57.200 --> 00:22:57.440
Yeah.

00:22:57.440 --> 00:22:57.920
Sorry.

00:22:57.920 --> 00:23:02.720
And that's why the, so the first thing is we write from, we write a database engine,

00:23:02.720 --> 00:23:09.280
a premium engine scratch and really think about multiprocessing, about cache, caches, about also out

00:23:09.280 --> 00:23:12.320
of core, we think process data doesn't fit into memory.

00:23:12.320 --> 00:23:15.920
So we really built this web spread with all those things in mind.

00:23:15.920 --> 00:23:22.800
And then in, at first we wanted to expose Pondus API, and then we noticed how bad it was for

00:23:22.800 --> 00:23:23.920
writing fast data.

00:23:23.920 --> 00:23:29.360
It, Pondus API just isn't really neat for this declarative analyzing of what the user wants to

00:23:29.360 --> 00:23:29.680
do.

00:23:29.680 --> 00:23:34.560
So we just got it off and took the freedom to design an API that makes most sense.

00:23:34.560 --> 00:23:35.920
Oh, that's interesting.

00:23:35.920 --> 00:23:40.160
I didn't realize that you had started trying to be closer to pandas than you ended up.

00:23:40.160 --> 00:23:40.640
Yeah.

00:23:40.640 --> 00:23:42.480
Well, it was very short-lived, I must say.

00:23:42.480 --> 00:23:43.040
That was it.

00:23:43.040 --> 00:23:44.640
Most faithful.

00:23:44.640 --> 00:23:44.800
Yeah.

00:23:44.800 --> 00:23:48.640
And that's not necessarily saying pandas are bad, I don't think.

00:23:48.640 --> 00:23:51.760
It's approaching the problem differently and it has different goals, right?

00:23:51.760 --> 00:23:52.080
Yeah.

00:23:52.080 --> 00:23:56.960
So maybe we could look at an example of some of the code that we're talking about.

00:23:56.960 --> 00:24:03.360
I guess also one of the other differences there is much of this has to do with what you would call,

00:24:03.360 --> 00:24:08.560
I guess you refer to them as lazy APIs or streaming APIs, kind of like a generator.

00:24:08.560 --> 00:24:09.280
Yeah.

00:24:09.280 --> 00:24:14.480
So what you think about a join, for instance, in pandas, if you would write a join and then take

00:24:14.480 --> 00:24:20.080
only one to first 100 rows with that result, then it would first do the join.

00:24:20.080 --> 00:24:24.000
And then that might produce 1 million or 10 million rows.

00:24:24.000 --> 00:24:25.680
And then you take only 100 of them.

00:24:25.680 --> 00:24:29.120
And then you have materialized a million, but you take only a fraction of that.

00:24:29.120 --> 00:24:34.640
And by having that lazy, you can optimize for the whole query at a time and just see,

00:24:34.640 --> 00:24:36.800
yeah, we do this join, but we only need 100 rows.

00:24:36.800 --> 00:24:38.640
So that's how we materialize normally.

00:24:38.640 --> 00:24:40.480
It gets you more realistic.

00:24:40.480 --> 00:24:41.360
That's really cool.

00:24:41.360 --> 00:24:46.080
I didn't realize it had so many similarities to databases, but yeah, it makes a lot of sense.

00:24:46.080 --> 00:24:46.480
All right.

00:24:46.480 --> 00:24:53.600
Let's look at maybe a super simple example you've got on fuller.rs.

00:24:53.600 --> 00:24:54.880
What country is RS?

00:24:54.880 --> 00:25:00.240
I always love how different countries that often have nothing to do with domain names get grabbed

00:25:00.240 --> 00:25:04.640
because they have a cool ending like Libya that was .ly for a while.

00:25:04.640 --> 00:25:07.840
You know, it still is, but it was used frequently like bit.ly and stuff.

00:25:07.840 --> 00:25:08.800
Do you know what RS is?

00:25:08.800 --> 00:25:10.800
I believe it's Serbia.

00:25:10.800 --> 00:25:11.280
Serbia.

00:25:11.280 --> 00:25:11.680
Okay, cool.

00:25:11.680 --> 00:25:12.160
I'm not sure.

00:25:12.160 --> 00:25:12.480
Yeah.

00:25:12.480 --> 00:25:12.640
Yeah.

00:25:12.640 --> 00:25:13.120
Very cool.

00:25:13.120 --> 00:25:13.440
All right.

00:25:13.440 --> 00:25:17.280
So polar.rs, like polar.rs.

00:25:17.280 --> 00:25:22.240
Over here, you've got on the homepage here, the landing page, and then through the documentation

00:25:22.240 --> 00:25:26.480
as well, you've got a lot of places where you're like, show me the Rust API or show me the Python API.

00:25:26.480 --> 00:25:29.040
People can come and check out the Rust code.

00:25:29.040 --> 00:25:32.960
It's a little bit longer because it's, you know, that kind of language,

00:25:32.960 --> 00:25:35.360
but it's not terribly more complex.

00:25:35.360 --> 00:25:40.240
But maybe talk us through this little example here on the homepage in Python,

00:25:40.240 --> 00:25:42.480
just to give people a sense of what the API looks like.

00:25:42.480 --> 00:25:42.960
Yeah.

00:25:42.960 --> 00:25:49.280
So we start with a scan CSP, which is a lazy read, which is so read CSP.

00:25:49.280 --> 00:25:50.160
It tells what you do.

00:25:50.160 --> 00:25:53.040
And then it reads the CSP and you get the data frame.

00:25:53.040 --> 00:25:56.400
In a scan CSP, we start a computation graph.

00:25:56.400 --> 00:25:57.520
called this a lazy frame.

00:25:57.520 --> 00:26:02.080
And the lazy frame is actually just, it also, it remembers the steps of the operations you want to

00:26:02.080 --> 00:26:02.560
do.

00:26:02.560 --> 00:26:06.880
Then it tells it to the boiler, but it looks at this very plan and it will optimize it.

00:26:06.880 --> 00:26:09.040
And we'll think of how to execute it.

00:26:09.040 --> 00:26:10.160
And we have different engines.

00:26:10.160 --> 00:26:14.960
So you can have an engine that's more specialized for data that doesn't fit into memory and engine

00:26:14.960 --> 00:26:17.920
that's more specialized for data that does fit differently.

00:26:17.920 --> 00:26:24.080
So we start with a scan and then we do a dot filter and we want to use verbs.

00:26:24.080 --> 00:26:26.480
Verbs, that's the declarative part.

00:26:26.480 --> 00:26:32.800
If on us, we often do indexes or a, and those indexes are a big years in my opinion, because

00:26:32.800 --> 00:26:37.840
you can, you can pause in a new file array with booleans, but you can also pause in a new file array

00:26:37.840 --> 00:26:38.800
with integers.

00:26:38.800 --> 00:26:40.160
So you can do slicing.

00:26:40.160 --> 00:26:44.640
You can also pause in a new file, a list of strings and then you do column selection.

00:26:44.640 --> 00:26:46.320
So it has three, three functions.

00:26:46.320 --> 00:26:51.120
One thing that I find really interesting about pandas is it's so incredible.

00:26:51.120 --> 00:26:55.200
And people who are very good with pandas, they can just make it fly.

00:26:55.200 --> 00:27:00.160
They can make it really right expressions that are super powerful, but it's not obvious that

00:27:00.160 --> 00:27:02.880
you should have been able to do that before you see it.

00:27:02.880 --> 00:27:07.920
You know, there's a lot of not quite magic, but stuff that that doesn't seem to come really

00:27:07.920 --> 00:27:10.080
straight out of the API directly.

00:27:10.080 --> 00:27:15.440
You know, you pass in like some sort of like a boolean expression that involves a

00:27:16.160 --> 00:27:19.600
a vector and some other test into the brackets.

00:27:19.600 --> 00:27:21.360
Like, wait, how, how did I know I could do that?

00:27:21.360 --> 00:27:29.920
Whereas this, your API is a lot more of a fluent API where you say, you know, PD, you'd say PL, PL.scan,

00:27:29.920 --> 00:27:33.520
CSV.filter.groupby.aggregate.collect.

00:27:33.520 --> 00:27:35.520
And it kind of just flows together.

00:27:35.520 --> 00:27:41.440
Does that mean that the editors and IDEs can be more helpful suggesting what happens at each step?

00:27:41.440 --> 00:27:43.920
Yes, we are really strict on types.

00:27:43.920 --> 00:27:47.600
So we also only return a single type common from a method.

00:27:47.600 --> 00:27:54.320
And we only, a dot filter just expects a boolean expression that produces a boolean, not an integer, not a string.

00:27:54.320 --> 00:27:57.920
So we want our methods from reading or code.

00:27:57.920 --> 00:28:01.040
You should be able to understand what should go in there.

00:28:01.040 --> 00:28:02.560
That's really important to me.

00:28:02.560 --> 00:28:03.920
It should be unambiguous.

00:28:03.920 --> 00:28:04.480
It should be consistent.

00:28:04.480 --> 00:28:08.800
And you, your knowledge of the API should expand to different parts of the API.

00:28:08.800 --> 00:28:12.640
And that's where I think we're going to talk about this later, but that's where expressions

00:28:12.640 --> 00:28:14.480
may be coming over.

00:28:14.480 --> 00:28:20.240
So, this portion of Talk Python To Me is brought to you by User Interviews.

00:28:20.240 --> 00:28:25.920
As a developer, how often do you find yourself talking back to products and services that you use?

00:28:25.920 --> 00:28:29.280
Sometimes it may be frustration over how it's working poorly.

00:28:29.280 --> 00:28:33.200
And if they just did such and such, it would work better.

00:28:33.200 --> 00:28:34.880
And it's easy to do.

00:28:34.880 --> 00:28:36.640
Other times it might be delight.

00:28:36.640 --> 00:28:37.040
Wow.

00:28:37.040 --> 00:28:38.800
They auto-filled that section for me.

00:28:38.800 --> 00:28:40.000
How did they even do that?

00:28:40.000 --> 00:28:40.720
Wonderful.

00:28:40.720 --> 00:28:41.440
Thanks.

00:28:41.440 --> 00:28:45.600
While this verbalization might be great to get the thoughts out of your head, did you

00:28:45.600 --> 00:28:49.040
know that you can earn money for your feedback on real products?

00:28:49.040 --> 00:28:54.720
User Interviews connects researchers with professionals that want to participate in research studies.

00:28:54.720 --> 00:28:59.440
There is a high demand for developers to share their opinions on products being created for

00:28:59.440 --> 00:29:00.400
developers.

00:29:00.400 --> 00:29:04.720
Aside from the extra cash, you'll talk to people building products in your space.

00:29:04.720 --> 00:29:09.200
You will not only learn about new tools being created, but you'll also shape the future of

00:29:09.200 --> 00:29:11.120
the products that we all use.

00:29:11.120 --> 00:29:16.160
It's completely free to sign up and you can apply to your first study in under five minutes.

00:29:16.160 --> 00:29:18.480
The average study pays over $60.

00:29:18.480 --> 00:29:23.840
However, many studies specifically interested in developers pay several hundreds of dollars

00:29:23.840 --> 00:29:25.440
for a one-on-one interview.

00:29:25.440 --> 00:29:29.360
Are you ready to earn extra income from sharing your expert opinion?

00:29:29.360 --> 00:29:34.560
Head over to talkpython.fm/userinterviews to participate today.

00:29:34.560 --> 00:29:36.720
The link is in your podcast player show notes.

00:29:36.720 --> 00:29:39.440
Thank you to User Interviews for supporting the show.

00:29:39.440 --> 00:29:45.920
I just derailed you a little bit here as you were describing this.

00:29:45.920 --> 00:29:52.560
So you start out with scanning a CSV, which is sort of creating and kicking off a data frame

00:29:52.560 --> 00:29:53.280
equivalent here.

00:29:53.280 --> 00:29:53.920
Lazy frame.

00:29:53.920 --> 00:30:01.280
And then you say a dot filter and you give it an expression like this column is greater than five.

00:30:01.280 --> 00:30:01.520
Right.

00:30:01.520 --> 00:30:01.520
Right.

00:30:01.520 --> 00:30:04.400
Or some expression that we would understand in Python.

00:30:04.400 --> 00:30:05.840
And that's the filter statement, right?

00:30:05.840 --> 00:30:06.080
Yeah.

00:30:06.080 --> 00:30:12.240
And then we follow a group by argument and then an aggregation where we say, okay, take all columns

00:30:12.240 --> 00:30:13.040
and sum them.

00:30:13.040 --> 00:30:14.480
And this again is an expression.

00:30:14.480 --> 00:30:16.640
And these are really easy expressions.

00:30:16.640 --> 00:30:21.360
And then we take this lazy frame and we materialize it into a data frame called

00:30:21.360 --> 00:30:22.000
Comecton.

00:30:22.000 --> 00:30:25.040
And collect means, okay, all those steps you recorded.

00:30:25.040 --> 00:30:28.960
Now you can do your magic, query optimizer, get all the stuff.

00:30:28.960 --> 00:30:34.880
And what this will do here, it will recognize that, okay, we've taken the iris.csv, which got

00:30:34.880 --> 00:30:35.120
different columns.

00:30:35.120 --> 00:30:37.120
And now in this case, it won't.

00:30:37.120 --> 00:30:41.840
So if you would have finished with a CEMEC where we only selected two columns, it would

00:30:41.840 --> 00:30:45.840
have recognized, oh, we don't need all those columns in the, in the CSV file.

00:30:45.840 --> 00:30:47.200
We only take the ones we need.

00:30:47.200 --> 00:30:51.440
What it will do, it will push the filter, the predicate down to the scan.

00:30:51.440 --> 00:30:55.280
So during the reading of the CSV, we will take this predicate.

00:30:55.280 --> 00:31:00.080
We say, okay, where the sample length is larger than five, the rows that don't match the predicate

00:31:00.080 --> 00:31:01.680
will not be materialized.

00:31:01.680 --> 00:31:06.880
So if you wrap a really large CSV file, if we really, let's say you have a CSV file with

00:31:06.880 --> 00:31:10.960
a 10s of gigabytes, but your, your predicate only selects 5% of that.

00:31:10.960 --> 00:31:14.080
Then you only materialize 5% of the 10 gigabytes.

00:31:14.080 --> 00:31:14.320
Yeah.

00:31:14.320 --> 00:31:19.360
So 500 megs instead of 10 gigabytes or something like that, or 200, 200 megs, whatever it is,

00:31:19.360 --> 00:31:20.880
quite a bit less.

00:31:20.880 --> 00:31:22.240
That's really interesting.

00:31:22.240 --> 00:31:27.360
And this is all part of the benefits of what we were talking about with the lazy, lazy frames,

00:31:27.360 --> 00:31:33.120
lazy APIs, and, and building up all of the steps before you say go, because in pandas,

00:31:33.120 --> 00:31:34.160
you would say, read CSV.

00:31:34.160 --> 00:31:36.080
So, okay, it's going to read the CSV.

00:31:36.080 --> 00:31:36.560
Now what?

00:31:36.560 --> 00:31:37.040
Yes.

00:31:37.040 --> 00:31:37.520
Right.

00:31:37.520 --> 00:31:41.280
And then you apply your filter if that's the order you want to do it in, and then you group

00:31:41.280 --> 00:31:42.640
and then, and so on and so on.

00:31:42.640 --> 00:31:42.880
Right.

00:31:42.880 --> 00:31:43.120
Right.

00:31:43.120 --> 00:31:48.160
It's interesting in that it does allow more database like behavior behind the scenes.

00:31:48.160 --> 00:31:48.720
Yeah.

00:31:48.720 --> 00:31:49.200
Yeah.

00:31:49.200 --> 00:31:54.560
And yet, in my opinion, the data frame is, should be seen as a table in a, in a database.

00:31:54.560 --> 00:31:57.600
It's, it's the final view of computation.

00:31:57.600 --> 00:31:59.760
Like you can see it as a materialized view.

00:31:59.760 --> 00:32:06.240
It's, we have some data on this and we want to get it into another table, which we would feed into

00:32:06.240 --> 00:32:08.880
our machine learning models or whatever.

00:32:08.880 --> 00:32:12.720
And we do a lot of operations on them before we get there.

00:32:12.720 --> 00:32:15.760
So I wouldn't see a data frame as a, as a data.

00:32:15.760 --> 00:32:17.840
It's not, it's not only a data structure.

00:32:17.840 --> 00:32:20.160
It's not only a list or a dictionary.

00:32:20.160 --> 00:32:23.920
There are lots of steps before we get into those tables.

00:32:23.920 --> 00:32:24.880
And eventually.

00:32:24.880 --> 00:32:25.200
Right.

00:32:25.200 --> 00:32:28.160
So here's an interesting challenge.

00:32:28.160 --> 00:32:32.000
There's a lot of visualization libraries.

00:32:32.000 --> 00:32:37.680
There are a lot of other data science libraries that know and expect

00:32:37.680 --> 00:32:38.960
and as data frames.

00:32:38.960 --> 00:32:41.840
So like, okay, what you do is you send me the pandas data frame here,

00:32:41.840 --> 00:32:46.080
or we're going to patch pandas so that if you call this function on the data frame,

00:32:46.080 --> 00:32:47.280
it's going to do this thing.

00:32:47.280 --> 00:32:50.880
And they may say, Richie, fantastic job you've done here in Polars,

00:32:50.880 --> 00:32:53.440
but my stuff is already all built around pandas.

00:32:53.440 --> 00:32:55.200
So I'm not going to use this.

00:32:55.200 --> 00:32:55.600
Right.

00:32:55.600 --> 00:32:56.720
But it's worth pointing out.

00:32:56.720 --> 00:32:58.480
There's some cool pandas integration.

00:32:58.480 --> 00:32:58.800
Right.

00:32:58.800 --> 00:32:59.280
Yeah.

00:32:59.280 --> 00:32:59.600
Yeah.

00:32:59.600 --> 00:33:02.560
So he said, so Polars doesn't want to do plotting.

00:33:02.560 --> 00:33:05.440
I don't think it should be in a different line.

00:33:05.440 --> 00:33:08.400
Maybe another length, another library can do it on top of Polars.

00:33:08.400 --> 00:33:11.360
Just like it shouldn't be a Polars in my opinion.

00:33:11.360 --> 00:33:16.320
But often when you do plotting, you're plotting the number of rows will not be billions.

00:33:16.320 --> 00:33:19.040
I mean, there's no plotting engine that can deal with that.

00:33:19.040 --> 00:33:22.960
So you will be reducing your, your big data set to something small.

00:33:22.960 --> 00:33:24.720
And then you can send it to the plot.

00:33:24.720 --> 00:33:24.720
Yeah.

00:33:24.720 --> 00:33:29.600
There's hardly a monitor that has enough pixels to show you that anyway.

00:33:29.600 --> 00:33:29.920
Right.

00:33:29.920 --> 00:33:30.480
So yeah.

00:33:30.480 --> 00:33:30.480
Yeah.

00:33:30.480 --> 00:33:30.480
Yeah.

00:33:30.480 --> 00:33:31.040
Yeah.

00:33:31.040 --> 00:33:34.880
You can call to pandas and then we transform our polars data frame to pandas.

00:33:34.880 --> 00:33:41.440
And then you can integrate with I could learn with, and we often find that progressively rewriting

00:33:41.440 --> 00:33:45.280
things from pandas to polars already is cheaper than keeping it in pandas.

00:33:45.280 --> 00:33:50.320
If you do it, if you call from pandas, or let's do a join in polars and then back to pandas,

00:33:50.320 --> 00:33:52.720
we probably made up for those double copies.

00:33:52.720 --> 00:33:54.320
Pandas does a lot of internal copies.

00:33:54.320 --> 00:33:57.200
If you do a reset index copies all data.

00:33:57.200 --> 00:34:00.240
There are a lot of internal copies and pandas which aren't listed.

00:34:00.240 --> 00:34:06.560
So I wouldn't worry about an explicit copy in the end of your ETL to go to plotting when the data is

00:34:06.560 --> 00:34:06.960
already.

00:34:06.960 --> 00:34:07.360
Right.

00:34:07.360 --> 00:34:07.520
Right.

00:34:07.520 --> 00:34:07.520
Right.

00:34:07.520 --> 00:34:12.960
So let's look at the benchmarks because it sounds like to a large degree, even if you do have to do this

00:34:12.960 --> 00:34:17.280
conversion in the end, many times, it still might even be quicker.

00:34:17.280 --> 00:34:22.240
So you've got some benchmarks over here and you compared, I'm going to need some good vision for

00:34:22.240 --> 00:34:22.640
this one.

00:34:22.640 --> 00:34:28.480
You compared polars, pandas, Dask, and then two things which are too small for me to read.

00:34:28.480 --> 00:34:29.520
Tell us what you compared.

00:34:29.520 --> 00:34:30.240
Modding and facts.

00:34:30.240 --> 00:34:30.960
Modding and facts.

00:34:30.960 --> 00:34:31.520
Okay.

00:34:31.520 --> 00:34:35.520
And for people listening, you go out here and look at these benchmarks,

00:34:35.520 --> 00:34:38.160
like linked right off the homepage.

00:34:38.160 --> 00:34:42.800
There's like a little tiny purple thing and a whole bunch of really tall bar graphs.

00:34:42.800 --> 00:34:43.760
It's got the rest.

00:34:43.760 --> 00:34:44.080
Yes.

00:34:44.080 --> 00:34:47.920
And the little tiny thing that you can kind of miss if you don't look carefully,

00:34:47.920 --> 00:34:50.560
that's the time it takes for polars.

00:34:50.560 --> 00:34:55.040
And then all the others are up there in like 60 seconds, a hundred seconds.

00:34:55.040 --> 00:34:57.360
And then polars is like a quarter of a second.

00:34:57.360 --> 00:34:59.520
So, you know, it's easy to miss it in the graph.

00:34:59.520 --> 00:35:03.120
But the quick takeaway here, I think, is there's some fast stuff.

00:35:03.120 --> 00:35:03.520
Yeah.

00:35:03.520 --> 00:35:03.760
Yeah.

00:35:03.760 --> 00:35:06.400
We're often orders of magnitude faster than pandas.

00:35:06.400 --> 00:35:12.240
So it's not uncommon to hear it's 1020x times fast, especially if you do write proper

00:35:12.240 --> 00:35:14.000
pandas and for apropotos.

00:35:14.000 --> 00:35:17.600
It's probably 20 except if we deal with IO as well.

00:35:17.600 --> 00:35:20.320
So what we see here are the TPCH benchmarks.

00:35:20.320 --> 00:35:28.080
And TPCH is a database query benchmark standard, which this is used by every query engine to show

00:35:28.080 --> 00:35:29.200
how fast it is.

00:35:29.200 --> 00:35:34.800
And those are really our questions that really, really collect some muscles of the query engine.

00:35:34.800 --> 00:35:40.720
So you have joints on several tables, different group buys, different nested group buys, etc.

00:35:40.720 --> 00:35:44.320
And yeah, yeah, I really tried to make those other tools faster.

00:35:44.320 --> 00:35:47.440
But so in memory, it does then mode in.

00:35:47.440 --> 00:35:49.840
But it was really hard to make stuff faster than pandas.

00:35:49.840 --> 00:35:50.560
Except for polars.

00:35:50.560 --> 00:35:51.920
So I think that's a good idea.

00:35:51.920 --> 00:35:53.200
So I think that's a good idea.

00:35:53.200 --> 00:35:53.920
So I think that's a good idea.

00:35:53.920 --> 00:35:55.040
So I think that's a good idea.

00:35:55.040 --> 00:35:55.920
So I think that's a good idea.

00:35:55.920 --> 00:35:57.040
So I think that's a good idea.

00:35:57.040 --> 00:35:57.920
So I think that's a good idea.

00:35:57.920 --> 00:35:59.040
So I think that's a good idea.

00:35:59.040 --> 00:35:59.920
So I think that's a good idea.

00:35:59.920 --> 00:36:01.040
I think that's a good idea.

00:36:01.040 --> 00:36:01.920
So I think that's a good idea.

00:36:01.920 --> 00:36:03.040
I think that's a good idea.

00:36:03.040 --> 00:36:03.680
I think that's a good idea.

00:36:03.680 --> 00:36:05.680
I think that's a good idea.

00:36:05.680 --> 00:36:06.800
I think that's a good idea.

00:36:06.800 --> 00:36:07.680
I think that's a good idea.

00:36:07.680 --> 00:36:08.800
I think that's a good idea.

00:36:08.800 --> 00:36:09.680
I think that's a good idea.

00:36:09.680 --> 00:36:10.800
I think that's a good idea.

00:36:10.800 --> 00:36:11.680
I think that's a good idea.

00:36:11.680 --> 00:36:12.800
I think that's a good idea.

00:36:12.800 --> 00:36:13.680
I think that's a good idea.

00:36:13.680 --> 00:36:14.800
I think that's a good idea.

00:36:14.800 --> 00:36:15.680
I think that's a good idea.

00:36:15.680 --> 00:36:16.800
I think that's a good idea.

00:36:16.800 --> 00:36:17.680
I think that's a good idea.

00:36:17.680 --> 00:36:18.800
I think that's a good idea.

00:36:18.800 --> 00:36:19.680
I think that's a good idea.

00:36:19.680 --> 00:36:20.800
I think that's a good idea.

00:36:20.800 --> 00:36:21.680
I think that's a good idea.

00:36:21.680 --> 00:36:23.680
I think that's a good idea.

00:36:23.680 --> 00:36:24.880
I think that's a good idea.

00:36:24.880 --> 00:36:26.320
I think that's a good idea.

00:36:26.320 --> 00:36:27.440
I think that's a good idea.

00:36:27.440 --> 00:36:28.720
I think that's a good idea.

00:36:28.720 --> 00:36:30.000
I think that's a good idea.

00:36:30.000 --> 00:36:31.040
I think that's a good idea.

00:36:31.040 --> 00:36:32.000
I think that's a good idea.

00:36:32.000 --> 00:36:33.040
I think that's a good idea.

00:36:33.040 --> 00:36:34.000
I think that's a good idea.

00:36:34.000 --> 00:36:35.040
I think that's a good idea.

00:36:35.040 --> 00:36:36.000
I think that's a good idea.

00:36:36.000 --> 00:36:37.040
I think that's a good idea.

00:36:37.040 --> 00:36:38.000
I think that's a good idea.

00:36:38.000 --> 00:36:39.040
I think that's a good idea.

00:36:39.040 --> 00:36:40.000
I think that's a good idea.

00:36:40.000 --> 00:36:41.040
I think that's a good idea.

00:36:41.040 --> 00:36:42.000
I think that's a good idea.

00:36:42.000 --> 00:36:43.040
I think that's a good idea.

00:36:43.040 --> 00:36:44.320
I think that's a good idea.

00:36:44.320 --> 00:36:46.320
I think that's a good idea.

00:36:46.320 --> 00:36:48.320
I think that's a good idea.

00:36:48.320 --> 00:36:50.320
I think that's a good idea.

00:36:50.320 --> 00:36:52.080
And then you have problems.

00:36:52.080 --> 00:36:53.520
Or we need to do multiprocessing.

00:36:53.520 --> 00:36:55.280
Or we need to send those Python objects

00:36:55.280 --> 00:36:57.120
to to to another project.

00:36:57.120 --> 00:36:58.560
And we copy data, which is slow.

00:36:58.560 --> 00:37:00.000
Or we need to do multi-threading

00:37:00.000 --> 00:37:01.360
and we're bound by the gill

00:37:01.360 --> 00:37:02.400
and we're single thread.

00:37:02.400 --> 00:37:03.920
And then there are key defenses.

00:37:03.920 --> 00:37:05.200
Yeah, I think there's some

00:37:05.200 --> 00:37:09.520
interesting parallels for Dask and Polars.

00:37:09.520 --> 00:37:10.720
On these benchmarks, at least,

00:37:10.720 --> 00:37:11.920
you're showing much better

00:37:11.920 --> 00:37:13.760
than performance than Dask.

00:37:13.760 --> 00:37:15.680
I've had Matthew Rocklin on a couple

00:37:15.680 --> 00:37:17.200
of times to talk about Dask

00:37:17.200 --> 00:37:18.320
and some of the work they're doing

00:37:18.320 --> 00:37:19.120
there at Coiled.

00:37:19.120 --> 00:37:20.320
and it's very cool.

00:37:20.320 --> 00:37:22.000
And one of the things that

00:37:22.000 --> 00:37:23.760
I think Dask is interesting for

00:37:23.760 --> 00:37:26.160
is allowing you to scale your code out

00:37:26.160 --> 00:37:27.760
to multi cores on your machine

00:37:27.760 --> 00:37:30.400
or to even distributed grid computing

00:37:30.400 --> 00:37:33.040
or process data that doesn't fit in memory

00:37:33.040 --> 00:37:34.800
and they can behind the scenes

00:37:34.800 --> 00:37:36.320
juggle all that for you.

00:37:36.320 --> 00:37:38.240
I feel like Polars kind of

00:37:38.240 --> 00:37:39.280
has a different way,

00:37:39.280 --> 00:37:40.720
but attempts to solve

00:37:40.720 --> 00:37:42.320
some of those problems as well.

00:37:42.320 --> 00:37:44.320
Yeah, but Polars has full control

00:37:44.320 --> 00:37:45.600
over it over everything.

00:37:45.600 --> 00:37:47.360
So it's built from the ground up.

00:37:47.360 --> 00:37:49.600
It controls I/O, it controls their own memory,

00:37:49.600 --> 00:37:52.160
it controls which strap gets which data.

00:37:52.160 --> 00:37:54.240
And in Dask, it goes through,

00:37:54.240 --> 00:37:55.760
it takes this other tool

00:37:55.760 --> 00:37:57.280
and then parallelizes that.

00:37:57.280 --> 00:38:00.080
But it is limited by what this other tool

00:38:00.080 --> 00:38:01.200
also is limited by.

00:38:01.200 --> 00:38:03.680
But I think, so on a single machine,

00:38:03.680 --> 00:38:04.720
it has those challenges.

00:38:04.720 --> 00:38:06.080
I think Dask distributed

00:38:06.080 --> 00:38:07.680
and does that these challenges.

00:38:07.680 --> 00:38:09.200
And I think for distributed,

00:38:09.200 --> 00:38:10.480
it worked really well.

00:38:10.480 --> 00:38:12.320
Yeah, the interesting part with Dask,

00:38:12.320 --> 00:38:14.560
I think, is that it's kind of like Pandas,

00:38:14.560 --> 00:38:16.960
but it scales in all these interesting ways.

00:38:16.960 --> 00:38:18.160
Across cores,

00:38:18.160 --> 00:38:19.280
bigger memory,

00:38:19.280 --> 00:38:20.400
but also across machines

00:38:20.400 --> 00:38:20.960
and then, you know,

00:38:20.960 --> 00:38:22.160
across cores, across machines,

00:38:22.160 --> 00:38:23.840
like all that stuff.

00:38:23.840 --> 00:38:25.280
I feel like Dask is a little bit,

00:38:25.280 --> 00:38:26.320
maybe it's trying to solve

00:38:26.320 --> 00:38:28.480
like a little bit bigger computer problem.

00:38:28.480 --> 00:38:30.720
Like how can we use a cluster of computers

00:38:30.720 --> 00:38:32.160
to answer these questions?

00:38:32.160 --> 00:38:34.560
The documentation also says it themselves.

00:38:34.560 --> 00:38:36.960
they say that they're probably not faster

00:38:36.960 --> 00:38:38.880
than Pandas on the single machine.

00:38:38.880 --> 00:38:41.600
So they're more for the large, big data.

00:38:41.600 --> 00:38:41.920
Yeah.

00:38:41.920 --> 00:38:43.200
But Paulus wants to be

00:38:43.200 --> 00:38:44.720
and a lot faster on the single machine,

00:38:44.720 --> 00:38:46.800
but also wants to be able to do

00:38:46.800 --> 00:38:48.560
out of core processing on the single machine.

00:38:48.560 --> 00:38:49.840
So if you,

00:38:49.840 --> 00:38:51.440
we don't support all queries yet,

00:38:51.440 --> 00:38:52.400
but we want to,

00:38:52.400 --> 00:38:55.520
we already do basic device group by sorts,

00:38:55.520 --> 00:38:58.320
predicates, element wise operations.

00:38:58.320 --> 00:38:59.760
And then we can process,

00:38:59.760 --> 00:39:02.800
I process iPhone gigabytes on my laptop.

00:39:02.800 --> 00:39:04.320
Matt, that's pretty good.

00:39:04.320 --> 00:39:06.080
Your laptop probably doesn't have 500.

00:39:06.080 --> 00:39:06.720
No, no, no, no, no.

00:39:06.720 --> 00:39:07.760
It's 16 gigs.

00:39:07.760 --> 00:39:09.040
Yeah.

00:39:09.040 --> 00:39:09.680
Nice.

00:39:09.680 --> 00:39:11.840
It's probably actually a value to,

00:39:11.840 --> 00:39:13.280
as you develop this product,

00:39:13.280 --> 00:39:16.560
to not have too massive of a computer to work on.

00:39:16.560 --> 00:39:20.080
If you had a $5,000 workstation,

00:39:20.080 --> 00:39:22.560
you know, you might be a little out of touch

00:39:22.560 --> 00:39:24.080
with many people using your code.

00:39:24.080 --> 00:39:25.440
And, you know, it's so awesome.

00:39:25.440 --> 00:39:27.360
Although I think there,

00:39:27.360 --> 00:39:30.240
I think others like scaling on a single machine

00:39:30.240 --> 00:39:32.560
makes sense for different reasons as well.

00:39:32.560 --> 00:39:35.760
I think a lot of people talk about distributed,

00:39:35.760 --> 00:39:38.000
but if you think about complexity of distributed,

00:39:38.000 --> 00:39:39.280
you need to send data,

00:39:39.280 --> 00:39:41.600
shuffle data over the network to other machines.

00:39:41.600 --> 00:39:44.640
So there are a lot of people using polars in our discord

00:39:44.640 --> 00:39:47.680
who have one terabyte of red and say,

00:39:47.680 --> 00:39:49.760
it's cheaper and a lot faster than Spark,

00:39:49.760 --> 00:39:50.880
because they can,

00:39:50.880 --> 00:39:52.720
well, all this is faster on a single machine.

00:39:52.720 --> 00:39:54.000
And one too,

00:39:54.000 --> 00:39:58.240
they have a beefy machine with like 120 cores

00:39:58.240 --> 00:40:01.600
and they don't have to go over the network to parallelize.

00:40:01.600 --> 00:40:04.400
And yeah, so I think times are changing.

00:40:04.400 --> 00:40:07.760
I think also scaling out data on a single machine

00:40:07.760 --> 00:40:08.640
is getting more worried.

00:40:08.640 --> 00:40:09.360
It is.

00:40:09.360 --> 00:40:12.240
One of the areas in which it's interesting is GPUs.

00:40:12.240 --> 00:40:15.440
Do you have any integration with GPUs or any of those sorts of things?

00:40:15.440 --> 00:40:15.440
No.

00:40:15.440 --> 00:40:15.440
No.

00:40:15.440 --> 00:40:18.160
I'm suggesting that necessarily is even a good idea.

00:40:18.160 --> 00:40:19.040
I'm just wondering if it does.

00:40:19.040 --> 00:40:20.000
No, I get this question,

00:40:20.000 --> 00:40:23.440
but I'm not really convinced I can get memory.

00:40:23.440 --> 00:40:25.840
I can get the data fast enough into the memory.

00:40:25.840 --> 00:40:28.640
We want to process gigabytes of data.

00:40:28.640 --> 00:40:31.840
The challenge already on the CPU is,

00:40:31.840 --> 00:40:34.960
is getting the data or cache or memory fast enough

00:40:34.960 --> 00:40:36.160
on a CPU piece.

00:40:36.160 --> 00:40:37.280
Just, I don't know.

00:40:37.280 --> 00:40:38.240
Yeah.

00:40:38.240 --> 00:40:38.560
Yeah.

00:40:38.560 --> 00:40:42.480
So maybe we could talk really quickly about platforms that it runs on.

00:40:42.480 --> 00:40:43.600
You know, I just,

00:40:43.600 --> 00:40:48.320
this is the very first show that I'm doing on my M2 Pro processor,

00:40:48.320 --> 00:40:49.120
which is fun.

00:40:49.120 --> 00:40:51.360
I literally been using for like an hour and a half,

00:40:51.360 --> 00:40:53.600
so I don't really have much to say, but it looks neat.

00:40:53.600 --> 00:40:54.480
Anyway, you know,

00:40:54.480 --> 00:40:56.320
that's very different than an Intel machine,

00:40:56.320 --> 00:40:58.000
which is different than a Raspberry Pi,

00:40:58.000 --> 00:40:59.680
which is different than, you know,

00:40:59.680 --> 00:41:04.080
some version of Linux running on ARM or on AMD.

00:41:04.080 --> 00:41:05.840
So where, where do these,

00:41:05.840 --> 00:41:07.680
what's the reach?

00:41:07.680 --> 00:41:08.720
Well, we support it.

00:41:08.720 --> 00:41:09.440
We support it.

00:41:09.440 --> 00:41:10.320
We don't.

00:41:10.320 --> 00:41:13.280
So Poilers also has a lot of like SIMD optimizations.

00:41:13.280 --> 00:41:16.080
SIMD starts for a single instruction mock data,

00:41:16.080 --> 00:41:16.960
where for instance,

00:41:16.960 --> 00:41:18.480
if you do a floating point operation,

00:41:18.480 --> 00:41:21.600
it's doing a single floating point at a time,

00:41:21.600 --> 00:41:24.640
you can fill in those vector lanes into your CPU,

00:41:24.640 --> 00:41:26.400
which can fit eight floating points.

00:41:26.400 --> 00:41:27.920
And in a single operation,

00:41:27.920 --> 00:41:29.600
can include eight of the five.

00:41:29.600 --> 00:41:32.240
And they have eight times the parallelism on a single core.

00:41:32.240 --> 00:41:35.600
Those instructions are only activated for

00:41:35.600 --> 00:41:36.320
Intel.

00:41:36.320 --> 00:41:39.440
So we don't have these instructions activated for ARM,

00:41:39.440 --> 00:41:41.200
but we do compile to ARM.

00:41:41.200 --> 00:41:42.240
How it forms?

00:41:42.240 --> 00:41:43.840
I think it performs far.

00:41:43.840 --> 00:41:44.880
Yeah.

00:41:44.880 --> 00:41:45.280
Yeah.

00:41:45.280 --> 00:41:47.520
But so if the standard machines, right?

00:41:47.520 --> 00:41:48.960
macOS, Windows, Linux,

00:41:48.960 --> 00:41:50.720
or we're all good to go.

00:41:50.720 --> 00:41:50.960
Yeah.

00:41:50.960 --> 00:41:52.400
And it ships as a wheel.

00:41:52.400 --> 00:41:53.680
So you don't have to have any,

00:41:53.680 --> 00:41:55.280
you don't have to have rusty or anything like that.

00:41:55.280 --> 00:41:55.920
Hanging around.

00:41:55.920 --> 00:41:56.160
Yeah.

00:41:56.160 --> 00:41:56.720
Okay.

00:41:56.720 --> 00:41:57.760
We also have condo,

00:41:57.760 --> 00:42:00.880
but condo is always a bit lagging the eye.

00:42:00.880 --> 00:42:05.040
So I could try to answer a bit because we can control this.

00:42:05.040 --> 00:42:06.480
Yeah, exactly.

00:42:06.480 --> 00:42:10.000
You push it out to IPI and that's what pip sees.

00:42:10.000 --> 00:42:10.960
And it's going to go, right?

00:42:10.960 --> 00:42:12.240
Pretty much instantly.

00:42:12.240 --> 00:42:15.040
I guess it's worth pointing out while we're sitting here is,

00:42:15.040 --> 00:42:16.880
um, not that thing I highlighted this.

00:42:16.880 --> 00:42:19.520
You do have a whole section in your user guide,

00:42:19.520 --> 00:42:24.480
the Polar's book called coming from pandas that actually talks about the differences,

00:42:24.480 --> 00:42:29.760
not just how do I do this versus, you know, this operation and pandas versus Polar's,

00:42:29.760 --> 00:42:32.160
but it also talks about some of the philosophy,

00:42:32.160 --> 00:42:36.640
like this lazy concepts that we've spoken about and a query optimization.

00:42:36.640 --> 00:42:38.800
I feel like we covered it pretty well.

00:42:38.800 --> 00:42:39.200
Yeah.

00:42:39.200 --> 00:42:42.240
Unless there's maybe some other stuff that you want to throw in here really quick,

00:42:42.240 --> 00:42:44.720
but I mostly just want to throw this out as resource.

00:42:44.720 --> 00:42:48.480
Cause I know many people are coming from pandas and they may be interested in this,

00:42:48.480 --> 00:42:50.320
and this is probably a good place to start.

00:42:50.320 --> 00:42:51.360
I'll link to it in the show notes.

00:42:51.360 --> 00:42:55.440
I think the most controversial one is that we don't have the multi-index.

00:42:55.440 --> 00:42:59.360
You don't have anything other than zero base zero one, two, three,

00:42:59.360 --> 00:43:00.640
where is it in the array type of.

00:43:00.640 --> 00:43:00.960
Yeah.

00:43:00.960 --> 00:43:05.440
Well, we can, we will support data structures that make you cooks faster,

00:43:05.440 --> 00:43:09.680
like index in a database sense, but it will not involve the,

00:43:09.680 --> 00:43:10.960
it will not chase the cement.

00:43:10.960 --> 00:43:11.680
Great.

00:43:11.680 --> 00:43:12.880
That's important.

00:43:12.880 --> 00:43:13.600
Okay.

00:43:13.600 --> 00:43:14.320
Yeah.

00:43:14.320 --> 00:43:18.400
So I encourage people who are mostly pandas people that come down here and,

00:43:18.400 --> 00:43:19.280
you know, look through this.

00:43:19.280 --> 00:43:20.640
It's, it's pretty straightforward.

00:43:20.640 --> 00:43:26.400
Another thing that I think is interesting and we're talking about maybe is we could touch

00:43:26.400 --> 00:43:30.560
a little bit on some of the, how can I, and your user guide, you've got,

00:43:30.560 --> 00:43:32.000
how can I work with IO?

00:43:32.000 --> 00:43:33.680
How can I work with time series?

00:43:33.680 --> 00:43:36.400
How can I work with multiprocessing and so on?

00:43:36.400 --> 00:43:38.080
What do you think is good to highlight out of here?

00:43:38.080 --> 00:43:38.320
Yeah.

00:43:38.320 --> 00:43:39.200
How do you regard it?

00:43:39.200 --> 00:43:40.560
It's a bit outdated.

00:43:40.560 --> 00:43:42.080
So you can see your own.

00:43:42.080 --> 00:43:45.680
So the Francis IO is changing.

00:43:45.680 --> 00:43:49.360
All of this writes as its own IO readers.

00:43:49.360 --> 00:43:55.440
So we've written our own CSP reader, JSON reader, RK, IPC, Arrow.

00:43:55.440 --> 00:44:01.840
And that's all in our control, but for interaction with databases, often a bit more complicated.

00:44:01.840 --> 00:44:04.080
Deal with different drivers, different ways.

00:44:04.080 --> 00:44:09.280
And currently we do this with connector X, which is really great and allows us to read from a lot

00:44:09.280 --> 00:44:13.200
of different databases, but it doesn't allow us to write from databases yet.

00:44:13.200 --> 00:44:14.720
And this is happy.

00:44:14.720 --> 00:44:15.920
This is not changing.

00:44:15.920 --> 00:44:17.520
I want to play a bit why.

00:44:17.520 --> 00:44:23.280
So Parler is built upon the Arrow memory specification and the Arrow memory specification

00:44:23.280 --> 00:44:28.960
is sort of the standard of how memory or data, our memory for columnar data should look into,

00:44:28.960 --> 00:44:32.640
how columnar data should be for, should be represented in memory.

00:44:32.640 --> 00:44:38.400
And this is becoming a new standard and Bark is using it, Dremel, Pondos itself.

00:44:38.960 --> 00:44:43.840
For instance, if you read a parquet in Pondos, it reads in first into Arrow memory and then

00:44:43.840 --> 00:44:45.680
copies that into Pondos memory.

00:44:45.680 --> 00:44:52.240
So the Arrow memory specification coming in standard, and this is a way to share data to processes,

00:44:52.240 --> 00:44:57.120
to other, also to other libraries within a process without copying data.

00:44:57.120 --> 00:45:00.480
We can just swap our pointers if we know that we both support Arrow.

00:45:00.480 --> 00:45:05.520
Oh, so Arrow defines basically a, in memory, it looks like this.

00:45:05.520 --> 00:45:05.520
Yes.

00:45:05.520 --> 00:45:08.480
And if you both agree on that, we can just swap our pointers.

00:45:08.480 --> 00:45:09.520
Right.

00:45:09.520 --> 00:45:15.360
Because a .NET object, a C++ object and a Python object, those don't look like anything similar

00:45:15.360 --> 00:45:16.960
to any of them, right?

00:45:16.960 --> 00:45:18.080
In memory.

00:45:18.080 --> 00:45:19.360
And yeah.

00:45:19.360 --> 00:45:22.480
So, so this is from the Apache Arrow project.

00:45:22.480 --> 00:45:22.800
Yeah.

00:45:22.800 --> 00:45:28.160
And this is really, really used by a lot of different tools already.

00:45:28.160 --> 00:45:32.880
And currently there is coming the ADBC, which is the Apache Arrow database connector,

00:45:32.880 --> 00:45:36.000
which will solve all those problems because then we can write,

00:45:36.000 --> 00:45:41.600
read arrives from a lot of databases in Arrow and then it will be really fast and very easy for us to do.

00:45:41.600 --> 00:45:53.040
So luckily we, we, that's one of those foundations of folders I'm really happy about because supporting Arrow and using Arrow memory gives us a lot of interaction,

00:45:53.040 --> 00:45:54.640
which interwoven with other libraries.

00:45:54.640 --> 00:45:54.640
Yeah.

00:45:54.640 --> 00:45:56.080
That's interesting.

00:45:56.080 --> 00:46:01.280
And when you think of Pandas, you know, it's kind of built on top of NumPy as its core foundation,

00:46:01.280 --> 00:46:06.080
and it can exchange NumPy arrays with other things that do that.

00:46:06.080 --> 00:46:09.680
So Apache Arrow is kind of, kind of your, your base.

00:46:09.680 --> 00:46:09.680
Yeah.

00:46:09.680 --> 00:46:13.840
Well, it's kind of full circle because Apache Arrow is started by Wes McKinney.

00:46:13.840 --> 00:46:17.600
Wes McKinney being known as the creator of Pandas.

00:46:17.600 --> 00:46:24.480
And when he got out of Pandas, he thought, okay, the memory representation of NumPy is just not, we should not use it.

00:46:24.480 --> 00:46:29.680
And then he was inspired to build Apache Arrow, which made from our master.

00:46:29.680 --> 00:46:30.080
Yeah.

00:46:30.080 --> 00:46:32.800
So that's how you learn about these projects, right?

00:46:32.800 --> 00:46:36.560
This is how you realize, oh, we, we had put this thing in place.

00:46:36.560 --> 00:46:37.440
Maybe we'll work better, right?

00:46:37.440 --> 00:46:43.040
You, you work on a project for five years and you're like, if I got a chance to start over, but it's too late now.

00:46:43.040 --> 00:46:46.800
But every now and then you do actually get a chance to start over.

00:46:46.800 --> 00:46:47.120
Yeah.

00:46:47.120 --> 00:46:47.600
Interesting.

00:46:47.600 --> 00:46:50.480
I didn't realize that Wes was involved with both.

00:46:50.480 --> 00:46:52.640
I mean, I knew from Pandas, but I didn't realize it's.

00:46:52.640 --> 00:46:52.880
Yeah.

00:46:52.880 --> 00:46:55.840
He's a CEO of Holter and with Shiro.

00:46:55.840 --> 00:47:03.440
He started Pajero and that's, Pajero is sort of super big, like use everywhere, but sort of middleware.

00:47:03.440 --> 00:47:09.680
Like it's end users are developers and not end users are developers who build tools and not developers who use

00:47:09.680 --> 00:47:11.120
like that.

00:47:11.120 --> 00:47:11.600
Right.

00:47:11.600 --> 00:47:14.160
You might not even know that you're using it.

00:47:14.160 --> 00:47:16.000
You just use, I just use Polars.

00:47:16.000 --> 00:47:20.320
And oh, by the way, it happens to internally be better because of this.

00:47:20.320 --> 00:47:20.800
Yeah.

00:47:20.800 --> 00:47:20.960
Yeah.

00:47:20.960 --> 00:47:21.600
Very cool.

00:47:21.600 --> 00:47:22.080
Okay.

00:47:22.080 --> 00:47:22.640
Let's see.

00:47:22.640 --> 00:47:25.040
We've got a little bit of time left to talk about it.

00:47:25.040 --> 00:47:29.120
So for example, this, some of these, how can I let me just touch on a couple that are nice

00:47:29.120 --> 00:47:29.440
here.

00:47:29.440 --> 00:47:34.720
So you talked about connector X, you talked about the database, but it's like three lines of code to

00:47:34.720 --> 00:47:39.520
define a connection string, define a SQL query, and then just

00:47:39.520 --> 00:47:40.960
you can just say PL dot read SQL.

00:47:40.960 --> 00:47:41.600
Yeah.

00:47:41.600 --> 00:47:42.560
And there you go.

00:47:42.560 --> 00:47:45.840
You call it data frame or what do you call the thing you get back here?

00:47:45.840 --> 00:47:47.680
So reading is always a data frame.

00:47:47.680 --> 00:47:48.000
Okay.

00:47:48.000 --> 00:47:49.200
Scanning will be a base.

00:47:49.200 --> 00:47:49.440
Got it.

00:47:49.440 --> 00:47:49.760
Okay.

00:47:49.760 --> 00:47:52.240
Is there a scan SQL as well?

00:47:52.240 --> 00:47:55.120
We know this might happen in the future.

00:47:55.120 --> 00:47:59.120
The challenge is, are we going to push back our optimizations?

00:47:59.120 --> 00:48:05.760
Sorry, we write out others query, and then we must translate that into SQL into the SQL we send

00:48:05.760 --> 00:48:07.120
to the database.

00:48:07.120 --> 00:48:09.920
But that needs to be consistent over different databases.

00:48:09.920 --> 00:48:12.560
That's all the rabbit hole we might get into.

00:48:12.560 --> 00:48:18.960
I'm not sure it's worth it because you can already do many of these operations in the SQL

00:48:18.960 --> 00:48:20.480
query that you're sending over, right?

00:48:20.480 --> 00:48:25.040
You have sort of two layers of query engines and optimizers and query plans.

00:48:25.040 --> 00:48:32.320
And it's not like you can't add on additional filters, joins, sorts, and so on before it ever

00:48:32.320 --> 00:48:32.800
gets back to you.

00:48:32.800 --> 00:48:38.160
It would be terrible if someone writes select star from table and then writes the filters in

00:48:38.160 --> 00:48:42.480
polars and then the database has sent all those data over the network.

00:48:42.480 --> 00:48:47.840
So yeah, ideally we'd be able to push those predicates down into the SQL.

00:48:47.840 --> 00:48:48.160
Yeah.

00:48:48.160 --> 00:48:51.520
But you know somebody's going to do it because they're more comfortable writing

00:48:51.520 --> 00:48:54.640
polar API in Python than they are writing in T-SQL.

00:48:54.640 --> 00:48:55.200
Yeah.

00:48:55.200 --> 00:48:56.160
You will not.

00:48:56.160 --> 00:48:56.800
Yeah.

00:48:56.800 --> 00:48:58.560
If it's possible, someone will write it.

00:48:58.560 --> 00:48:59.360
It's not optimal.

00:48:59.360 --> 00:49:00.640
That's right.

00:49:00.640 --> 00:49:01.600
That is right.

00:49:01.600 --> 00:49:03.280
Let's see what else can you do here.

00:49:03.280 --> 00:49:08.240
So you can, we've already talked about the CSV files and this is the part of that I was talking

00:49:08.240 --> 00:49:12.560
about where you've got the toggle to see the rust code and the Python code.

00:49:12.560 --> 00:49:15.760
So I think people might appreciate that parquet files.

00:49:15.760 --> 00:49:19.440
So parquet files is a more efficient format.

00:49:19.440 --> 00:49:24.160
Maybe talk about using parquet files versus CSV and why you might want to get rid of your

00:49:24.160 --> 00:49:28.720
CSV and like store these intermediate files and then load them.

00:49:28.720 --> 00:49:31.120
But this is really policy here reader.

00:49:31.120 --> 00:49:32.800
I really did my best on that.

00:49:32.800 --> 00:49:39.680
But you can use parquet or arrow IPC because your data is typed.

00:49:39.680 --> 00:49:41.920
There's no ambiguity on reading.

00:49:41.920 --> 00:49:43.120
We know type it is.

00:49:43.120 --> 00:49:43.440
Right.

00:49:43.440 --> 00:49:47.520
Because CSV files, even though it might be representing a date, it's still a string.

00:49:47.520 --> 00:49:49.920
Yeah, we need to parse it.

00:49:49.920 --> 00:49:51.520
Yeah, it's slow to parse it.

00:49:51.520 --> 00:49:52.080
Yeah.

00:49:52.080 --> 00:49:58.320
But also we can just, so it parquet interacts really nicely with query optimization.

00:49:58.320 --> 00:50:02.320
So we can select just a single column from the file without touching any of the other

00:50:02.320 --> 00:50:03.040
columns.

00:50:03.040 --> 00:50:04.480
We can read statistics.

00:50:04.480 --> 00:50:10.240
So a parquet file can write statistics, which knows, okay, this page has got this maximum

00:50:10.240 --> 00:50:11.600
value, this minimum value.

00:50:11.600 --> 00:50:16.880
And if you have written a photo square, which says, oh, so I'll give me the result where the

00:50:16.880 --> 00:50:18.480
value is larger than this.

00:50:18.480 --> 00:50:22.800
And we see that the statistics say it cannot be in this file.

00:50:22.800 --> 00:50:24.640
We can just skip the whole column.

00:50:24.640 --> 00:50:25.600
We don't have to read.

00:50:25.600 --> 00:50:25.920
Yeah.

00:50:25.920 --> 00:50:26.640
Oh, interesting.

00:50:26.640 --> 00:50:26.640
Wow.

00:50:26.640 --> 00:50:26.640
Okay.

00:50:26.640 --> 00:50:32.640
So there are a lot of optimizations, which, so the best work is work you don't have to do

00:50:32.640 --> 00:50:33.520
and partake a lot.

00:50:33.520 --> 00:50:34.560
Exactly.

00:50:34.560 --> 00:50:39.360
Or you've done it when you created the file and you never do it again or something like that.

00:50:39.360 --> 00:50:39.600
Yeah.

00:50:39.600 --> 00:50:39.840
Yeah.

00:50:39.840 --> 00:50:46.000
So you've got a read parquet, a scan parquet, I suppose that's the data frame versus lazy frame.

00:50:46.000 --> 00:50:47.600
And then you also have the ability to write them.

00:50:47.600 --> 00:50:48.640
That's pretty interesting.

00:50:48.640 --> 00:50:50.400
JSON, multiple files.

00:50:50.400 --> 00:50:50.960
Yeah.

00:50:50.960 --> 00:50:51.280
Yeah.

00:50:51.280 --> 00:50:55.280
There's just a whole bunch of how do I, how can I rather, a bunch of neat things.

00:50:55.280 --> 00:50:57.200
What else would you like to highlight here in the next couple minutes?

00:50:57.200 --> 00:51:00.640
The most important thing I want to touch on is the expression API.

00:51:00.640 --> 00:51:02.800
So that's a bit, if you go a bit higher.

00:51:02.800 --> 00:51:05.200
So you swallow, photos expression.

00:51:05.200 --> 00:51:06.640
We got our own chip.

00:51:06.640 --> 00:51:07.200
There you go.

00:51:07.200 --> 00:51:13.760
One of the goals of the photos API is to keep the API so small, but give you a lot of things

00:51:13.760 --> 00:51:14.560
you can do.

00:51:14.560 --> 00:51:16.640
And this is where the photos expressions come in.

00:51:16.640 --> 00:51:20.880
So photos expressions are expressions of what you want to do, which are

00:51:20.880 --> 00:51:25.200
run and parallelized on a query in Japan and can combine them in depth.

00:51:25.200 --> 00:51:29.280
So an expression takes a series and produces a series and does the input.

00:51:29.280 --> 00:51:30.480
It's the same as the output.

00:51:30.480 --> 00:51:31.440
You can combine them.

00:51:31.440 --> 00:51:36.800
And as you can see, we can do pretty complicated stuff and you can keep chaining them.

00:51:36.800 --> 00:51:38.240
And this is the same.

00:51:38.240 --> 00:51:40.480
Like, I would, I'd like to see it.

00:51:40.480 --> 00:51:43.280
Transistor Python vocabulary is quite small.

00:51:43.280 --> 00:51:46.480
So we have a while we have a loop, we have a variable assignment.

00:51:46.480 --> 00:51:52.640
But if you, I think it fits into maybe two, two pieces of paper, but with this, you can write

00:51:52.640 --> 00:51:57.680
any program you want with the combination of all those, all those, yeah, this vocabulary.

00:51:57.680 --> 00:51:58.000
Yeah.

00:51:58.000 --> 00:52:00.800
And that's what we want to do with the photos expressions as well.

00:52:00.800 --> 00:52:06.080
So we, you've got a lot of small building blocks, which can be combined into.

00:52:06.080 --> 00:52:06.640
Yeah.

00:52:06.640 --> 00:52:11.600
So somebody could say, I want to select a column back, but then I don't want the actual

00:52:11.600 --> 00:52:12.160
values.

00:52:12.160 --> 00:52:15.200
I want the unique ones, a uniqueness.

00:52:15.200 --> 00:52:18.640
So if there's duplicate, remove those, and then you can do a dot account.

00:52:18.640 --> 00:52:23.440
Then you can add an alias, which gives it a new, which basically defines the column name.

00:52:23.440 --> 00:52:25.600
You could read it as a, well, it's not names.

00:52:25.600 --> 00:52:27.280
It's a, it's a, it's.

00:52:27.280 --> 00:52:32.560
So take column names as, you need names to, you see, but as is the keyword and pipe.

00:52:32.560 --> 00:52:33.760
So I'm allowed to use it.

00:52:33.760 --> 00:52:34.080
Right.

00:52:34.080 --> 00:52:35.760
It means something else.

00:52:35.760 --> 00:52:36.000
Yeah.

00:52:36.000 --> 00:52:37.920
That's, that's interesting.

00:52:37.920 --> 00:52:38.560
Okay.

00:52:38.560 --> 00:52:38.800
Yeah.

00:52:38.800 --> 00:52:45.600
So people, they use these expressions to do lots of transformations and filterings and, and things

00:52:45.600 --> 00:52:46.000
like that.

00:52:46.000 --> 00:52:46.400
Yeah.

00:52:46.400 --> 00:52:52.240
So these expressions can be used in a select on different places, but the knowledge of expressions

00:52:52.240 --> 00:52:54.000
extrapolates to different locations.

00:52:54.000 --> 00:52:57.120
So you can do it in a, in a select statement and then you select column.

00:52:57.120 --> 00:53:00.160
Then you select this expression and you get a result.

00:53:00.160 --> 00:53:02.480
But you can also do this in a group by aggregation.

00:53:02.480 --> 00:53:04.400
And then the same logic applies.

00:53:04.400 --> 00:53:07.840
It runs on the same engine and we make sure everything is possible.

00:53:07.840 --> 00:53:14.160
And this is really powerful because because it's so expressive, people don't have to use

00:53:14.160 --> 00:53:18.320
custom apply with Lambda because when you use a Lambda, it's like box to us.

00:53:18.320 --> 00:53:21.440
It will be slow because it's Python and we don't know what happened.

00:53:21.440 --> 00:53:23.520
So a Lambda is, it will be slow.

00:53:23.520 --> 00:53:25.840
It will kill parallelization because it builds.

00:53:25.840 --> 00:53:26.320
But yeah.

00:53:26.320 --> 00:53:28.640
A Lambda is three times a day.

00:53:28.640 --> 00:53:29.200
Right.

00:53:29.200 --> 00:53:34.000
It gets in the way of a lot of your optimizations and a lot of your, your speed ups there.

00:53:34.000 --> 00:53:37.840
That's why we want to make this expression API very complete.

00:53:37.840 --> 00:53:39.760
So you, you don't need that as much.

00:53:39.760 --> 00:53:40.000
Yeah.

00:53:40.000 --> 00:53:42.640
So people are wanting to get this, get seriously into this.

00:53:42.640 --> 00:53:45.360
They should check out chapter three expressions, right?

00:53:45.360 --> 00:53:46.320
And just go through there.

00:53:46.320 --> 00:53:50.800
And probably, especially, you know, sort of browse through the Python examples that they

00:53:50.800 --> 00:53:54.160
can see where, go back and see what they need to learn more about.

00:53:54.160 --> 00:53:56.640
But it's a very interesting API.

00:53:56.640 --> 00:53:59.200
The speed is very compelling thing.

00:53:59.200 --> 00:53:59.440
Yeah.

00:53:59.440 --> 00:54:00.400
I think it's a cool project.

00:54:00.400 --> 00:54:02.240
And like I said, how many people we got here?

00:54:02.240 --> 00:54:03.760
13,000 people using it already.

00:54:03.760 --> 00:54:06.080
So that's, that's a big community.

00:54:06.080 --> 00:54:06.320
Yeah.

00:54:06.320 --> 00:54:10.880
So if you're interested in project, we have a discord where, where you can chat with us

00:54:10.880 --> 00:54:14.400
and ask questions and see how you can best do things.

00:54:14.400 --> 00:54:15.200
Pretty active there.

00:54:15.200 --> 00:54:15.440
Cool.

00:54:15.440 --> 00:54:17.520
The discord's linked right off the homepage.

00:54:17.520 --> 00:54:18.560
So that's awesome.

00:54:18.560 --> 00:54:19.680
People can find it there.

00:54:19.680 --> 00:54:20.480
Contributions.

00:54:20.480 --> 00:54:22.160
People want to make contributions.

00:54:22.160 --> 00:54:25.200
I'm sure you're willing to accept PRs and other feedback.

00:54:25.200 --> 00:54:29.760
Or you put in a really large PR, please first open an issue with a, with a, with a,

00:54:31.760 --> 00:54:34.800
to start with discussion of business, this contribution is welcome.

00:54:34.800 --> 00:54:37.520
And we also have a few getting started.

00:54:37.520 --> 00:54:39.360
Good for the contributors.

00:54:39.360 --> 00:54:39.680
Okay.

00:54:39.680 --> 00:54:40.000
Yes.

00:54:40.000 --> 00:54:45.280
You've, you've tagged or labeled some of the issues as look here, if you want to get,

00:54:45.280 --> 00:54:46.000
get into this.

00:54:46.000 --> 00:54:46.240
Yeah.

00:54:46.240 --> 00:54:50.720
I must say, I think we're an interesting project to contribute to because we're,

00:54:50.720 --> 00:54:53.760
you can, it's not, not everything is set in stone.

00:54:53.760 --> 00:54:56.800
So there are still places where you can play.

00:54:56.800 --> 00:54:57.520
I'm not sure.

00:54:57.520 --> 00:54:59.840
And there, there's still interesting work to be done.

00:54:59.840 --> 00:55:04.320
It's not completely 100% polished and finalized.

00:55:04.320 --> 00:55:04.560
Yeah.

00:55:04.560 --> 00:55:05.360
On the periphery.

00:55:05.360 --> 00:55:05.600
Yeah.

00:55:05.600 --> 00:55:06.160
Yeah.

00:55:06.160 --> 00:55:06.880
Yeah.

00:55:06.880 --> 00:55:07.120
Yeah.

00:55:07.120 --> 00:55:07.200
Yeah.

00:55:07.200 --> 00:55:07.600
Very cool.

00:55:07.600 --> 00:55:09.920
Let's wrap it up with a comment from the audience here.

00:55:09.920 --> 00:55:12.160
Ajit says, excellent content guys.

00:55:12.160 --> 00:55:16.080
It certainly helps me kickstart my journey from pandas to pollers.

00:55:16.080 --> 00:55:16.560
Awesome.

00:55:16.560 --> 00:55:17.120
Awesome.

00:55:17.120 --> 00:55:18.480
Glad, glad to help.

00:55:18.480 --> 00:55:19.200
I'm sure it will.

00:55:19.200 --> 00:55:20.400
Many people do that.

00:55:20.400 --> 00:55:23.440
So Richie, let's close it out with final call action.

00:55:23.440 --> 00:55:25.200
People are interested in this project.

00:55:25.200 --> 00:55:27.520
They want to start playing and learning pollers.

00:55:27.520 --> 00:55:30.640
maybe try it out on some other code that is and is at the moment.

00:55:30.640 --> 00:55:31.200
What do they do?

00:55:31.200 --> 00:55:34.480
I'd recommend if you have a new project, just start in pollers.

00:55:34.480 --> 00:55:41.120
Because you can also rewrite some comments, but the most fun experience will just start a new

00:55:41.120 --> 00:55:42.160
project in pollers.

00:55:42.160 --> 00:55:46.000
And because then you can really enjoy what pollers offers.

00:55:46.000 --> 00:55:49.120
The only expression API, learn how you use it declaratively.

00:55:49.120 --> 00:55:52.480
And yeah, we'll be, then it will be most fun.

00:55:52.480 --> 00:55:53.040
Absolutely.

00:55:53.040 --> 00:55:53.680
Sounds great.

00:55:53.680 --> 00:55:58.640
And like we did point out, it has the to and from hand as data frame.

00:55:58.640 --> 00:56:02.560
So you can work on a section of your code and still have it consistent, right?

00:56:02.560 --> 00:56:04.080
With, with other parts that have to be.

00:56:04.080 --> 00:56:04.480
Yeah.

00:56:04.480 --> 00:56:08.720
You can progressively rewrite some performance heavy parts.

00:56:08.720 --> 00:56:13.680
Or I also think supporters have really strict on the, on the schema and the types.

00:56:13.680 --> 00:56:17.760
It's also, if you write any ETL, you will be really happy to do that.

00:56:17.760 --> 00:56:21.600
But also because you can check the scheme of lazy frame before executing it.

00:56:21.600 --> 00:56:26.880
But you know, the apps core running the query and if the data comes in and it doesn't

00:56:26.880 --> 00:56:30.400
oblige to this schema, you can fail fast.

00:56:30.400 --> 00:56:31.840
Instead of having strange outtakes.

00:56:31.840 --> 00:56:37.280
Oh, that's interesting because you definitely don't want zero when you expected something else

00:56:37.280 --> 00:56:40.160
because it could parse or other weird, you know, whatever, right?

00:56:40.160 --> 00:56:40.160
Yeah.

00:56:40.160 --> 00:56:40.160
Yeah.

00:56:40.160 --> 00:56:45.280
So this was my, for missing data and polar doesn't change the schema.

00:56:45.280 --> 00:56:45.760
Yeah.

00:56:45.760 --> 00:56:47.600
So polar is great.

00:56:47.600 --> 00:56:53.280
The schema is defined by the operations and the data and not by their values in data.

00:56:53.280 --> 00:56:55.200
So you can definitely check.

00:56:55.200 --> 00:56:55.920
Got it.

00:56:55.920 --> 00:56:56.720
Excellent.

00:56:56.720 --> 00:56:57.040
All right.

00:56:57.040 --> 00:56:59.520
Well, congratulations on a cool project.

00:56:59.520 --> 00:57:00.880
I'm glad we got to share with everybody.

00:57:00.880 --> 00:57:01.840
Thanks for coming on the show.

00:57:01.840 --> 00:57:02.240
Bye.

00:57:02.240 --> 00:57:02.720
You bet.

00:57:02.720 --> 00:57:03.040
Bye.

00:57:03.040 --> 00:57:04.240
Bye.

00:57:04.240 --> 00:57:06.720
This has been another episode of Talk Python To Me.

00:57:06.720 --> 00:57:08.720
Thank you to our sponsors.

00:57:08.720 --> 00:57:10.160
Be sure to check out what they're offering.

00:57:10.160 --> 00:57:11.600
It really helps support the show.

00:57:11.600 --> 00:57:16.560
Type I is here to take on the challenge of rapidly transforming a bare algorithm in Python

00:57:16.560 --> 00:57:20.080
into a full-fledged decision support system for end users.

00:57:20.080 --> 00:57:25.440
Get started with Type I core and GUI for free at talkpython.fm/Typy.

00:57:25.440 --> 00:57:27.440
T-A-I-P-Y.

00:57:27.440 --> 00:57:32.400
Earn extra income from sharing your software development opinion at user interviews.

00:57:32.400 --> 00:57:36.800
Head over to talkpython.fm/userinterviews to participate today.

00:57:36.800 --> 00:57:39.040
Want to level up your Python?

00:57:39.040 --> 00:57:43.120
We have one of the largest catalogs of Python video courses over at Talk Python.

00:57:43.120 --> 00:57:48.160
Our content ranges from true beginners to deeply advanced topics like memory and async.

00:57:48.160 --> 00:57:50.880
And best of all, there's not a subscription in sight.

00:57:50.880 --> 00:57:53.920
Check it out for yourself at training.talkpython.fm.

00:57:53.920 --> 00:57:58.480
Be sure to subscribe to the show, open your favorite podcast app, and search for Python.

00:57:58.480 --> 00:57:59.840
We should be right at the top.

00:57:59.840 --> 00:58:05.600
You can also find the iTunes feed at /itunes, the Google Play feed at /play, and the direct

00:58:05.600 --> 00:58:08.880
RSS feed at /rss on talkpython.fm.

00:58:08.880 --> 00:58:12.480
We're live streaming most of our recordings these days.

00:58:12.480 --> 00:58:15.840
If you want to be part of the show and have your comments featured on the air,

00:58:15.840 --> 00:58:20.240
be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

00:58:20.240 --> 00:58:22.240
This is your host, Michael Kennedy.

00:58:22.240 --> 00:58:23.360
Thanks so much for listening.

00:58:23.360 --> 00:58:24.560
I really appreciate it.

00:58:24.560 --> 00:58:26.400
Now get out there and write some Python code.

00:58:26.400 --> 00:58:41.760
I'll see you next time.

00:58:41.760 --> 00:58:43.520
I'll see you next time.

00:58:43.520 --> 00:58:45.520
Bye.

00:58:45.520 --> 00:58:47.520
Bye.

