WEBVTT

00:00:00.001 --> 00:00:04.960
What do you do when you're working with an amazing web application that, for whatever reason, doesn't have an API?

00:00:04.960 --> 00:00:09.500
One option is to say, gee, I wish that site had an API and just give up.

00:00:09.500 --> 00:00:17.860
Or you could use Scrapey, an open source web scraping framework from Pablo Hoffman and scrapinghub.com, and create your own API.

00:00:17.860 --> 00:00:24.580
On episode 50 of Talk Python to Me, we'll talk about how to do this, when it makes sense, and even when it's allowed.

00:00:24.580 --> 00:00:28.720
This episode has been recorded February 16th, 2016.

00:00:28.720 --> 00:00:56.540
Welcome to Talk Python to Me, a weekly podcast on Python.

00:00:56.540 --> 00:00:59.620
The language, the libraries, the ecosystem, and the personalities.

00:00:59.620 --> 00:01:01.740
This is your host, Michael Kennedy.

00:01:01.740 --> 00:01:03.720
Follow me on Twitter, where I'm @mkennedy.

00:01:03.720 --> 00:01:07.620
Keep up with the show and listen to past episodes at talkpython.fm.

00:01:07.620 --> 00:01:10.140
And follow the show on Twitter via at Talk Python.

00:01:10.140 --> 00:01:13.520
This episode is brought to you by Hired and SnapCI.

00:01:13.520 --> 00:01:20.300
Thank them for supporting the show on Twitter via at Hired underscore HQ and at Snap underscore CI.

00:01:20.960 --> 00:01:21.700
Hey, everyone.

00:01:21.700 --> 00:01:22.860
Thanks for joining me today.

00:01:22.860 --> 00:01:25.540
We have a great interview on tap with Pablo Hoffman.

00:01:25.540 --> 00:01:28.900
I want to give you a quick Kickstarter update before we get to that, though.

00:01:28.900 --> 00:01:31.840
There are just three days left to join the course via Kickstarter.

00:01:31.840 --> 00:01:33.580
Get a big discount while you're at it.

00:01:33.580 --> 00:01:36.780
Of course, it'll be for sale afterwards, but not at the Kickstarter prices.

00:01:36.780 --> 00:01:40.940
The students who have had early access have had really positive things to say.

00:01:40.940 --> 00:01:41.880
Here's just a taste.

00:01:42.620 --> 00:01:45.300
This is by far the best Python course I've done to date.

00:01:45.300 --> 00:01:49.760
Clear explanations, good apps that incorporate the concepts of particular sessions, and follow

00:01:49.760 --> 00:01:51.000
up on core concepts.

00:01:51.000 --> 00:01:55.520
The course is very engaging with a little humor, which makes it much easier to dedicate time

00:01:55.520 --> 00:01:56.360
to following along.

00:01:56.360 --> 00:02:00.440
If you're looking for an easy to follow, enjoyable Python course, back this project now.

00:02:00.440 --> 00:02:01.940
Get comfortable and enjoy.

00:02:01.940 --> 00:02:02.900
Thanks, Andy.

00:02:02.900 --> 00:02:06.740
You can check it out at talkpython.fm/course.

00:02:06.740 --> 00:02:08.780
Now let's talk about web scraping.

00:02:08.780 --> 00:02:10.700
Pablo, welcome to the show.

00:02:10.700 --> 00:02:11.540
Thank you, Michael.

00:02:11.540 --> 00:02:12.740
Thanks for having me here.

00:02:12.740 --> 00:02:14.860
Yeah, we got some really cool stuff to talk about.

00:02:14.860 --> 00:02:16.940
Web scraping, open source businesses.

00:02:16.940 --> 00:02:19.400
We'll talk a little bit of Python 3, maybe.

00:02:19.400 --> 00:02:20.660
All sorts of cool things.

00:02:20.660 --> 00:02:24.100
But before we dig into them, let's talk about your story.

00:02:24.100 --> 00:02:25.420
How did you get into Python and programming?

00:02:25.420 --> 00:02:26.180
All right.

00:02:26.180 --> 00:02:31.920
So I met Python when I was in college in 2004, and I immediately fell in love with it, with

00:02:31.920 --> 00:02:34.420
the simplicity and the structure of the syntax.

00:02:34.420 --> 00:02:40.160
Back then, I was doing a lot of subjects in college, but I always tried to find an opportunity

00:02:40.160 --> 00:02:41.700
to use Python for whatever.

00:02:41.700 --> 00:02:49.100
I would invent a lot of crazy, just useless stuff just to be able to use Python for it.

00:02:49.100 --> 00:02:51.380
Before Python, I used my past.

00:02:51.380 --> 00:02:56.420
I used PHP, but I don't regret anything I did.

00:02:56.980 --> 00:03:02.880
But yeah, I come from that background, more sysadmin, intranet web application background, sort

00:03:02.880 --> 00:03:04.160
of natural flow for me.

00:03:04.160 --> 00:03:10.700
I started working on Python in 2014, introduced by a colleague in college.

00:03:10.700 --> 00:03:12.500
And I never looked back.

00:03:12.500 --> 00:03:19.360
And yeah, it was only in 2007 that I was able to start a company working solely on Python.

00:03:19.360 --> 00:03:20.740
So I had to wait three years.

00:03:21.080 --> 00:03:23.680
But you've made it.

00:03:23.680 --> 00:03:24.540
You finally made it.

00:03:24.540 --> 00:03:25.360
Yeah, yeah, yeah.

00:03:25.360 --> 00:03:25.840
Absolutely.

00:03:25.840 --> 00:03:33.680
And I'm still here almost 10 years later working pretty much exclusively with Python and enjoying

00:03:33.680 --> 00:03:34.540
every moment of it.

00:03:34.540 --> 00:03:35.480
Yeah, that's really cool.

00:03:35.620 --> 00:03:40.120
What continues to surprise me about Python is here's a language in the ecosystem that's

00:03:40.120 --> 00:03:47.100
25 plus years old and seems to be becoming more popular and growing faster, even though

00:03:47.100 --> 00:03:49.480
it's not like something new that just sprung on the scene.

00:03:49.480 --> 00:03:51.780
So yeah, I think we're in a good place.

00:03:52.160 --> 00:03:52.500
Yeah, yeah.

00:03:52.500 --> 00:03:55.360
I believe it's a lot of combination of things, right?

00:03:55.360 --> 00:04:00.340
And the community certainly is one of the key aspects of Python, in addition to the language

00:04:00.340 --> 00:04:02.600
being beautiful and elegant by itself.

00:04:02.600 --> 00:04:07.500
So a lot of things that come together for that to happen.

00:04:07.500 --> 00:04:08.800
Yeah, I totally agree.

00:04:08.800 --> 00:04:10.780
So we're going to talk about web scraping.

00:04:10.780 --> 00:04:12.960
There's a whole variety of listeners.

00:04:12.960 --> 00:04:14.540
Some of them go, oh, yeah, web scraping.

00:04:14.540 --> 00:04:16.320
I did that back in, you know, whatever.

00:04:16.320 --> 00:04:19.820
But a lot of people maybe don't really understand or are familiar with this.

00:04:19.820 --> 00:04:21.900
So maybe give me the background on what is web scraping?

00:04:22.140 --> 00:04:22.880
Why do we do this?

00:04:22.880 --> 00:04:23.480
How does it work?

00:04:23.480 --> 00:04:24.080
Right.

00:04:24.080 --> 00:04:30.180
So there's a bunch of information everywhere in the web right now and has always been and

00:04:30.180 --> 00:04:31.680
continues to be and will.

00:04:31.680 --> 00:04:35.800
The information is growing in presenting amounts.

00:04:35.800 --> 00:04:42.560
And so there's a natural need for extracting and using that information for all kinds of purposes.

00:04:42.560 --> 00:04:46.980
But you can't just take what's on a web page and use it as is.

00:04:46.980 --> 00:04:52.120
You need to transform it, extract it and transform it in order to apply whatever.

00:04:52.120 --> 00:04:54.080
You need to apply to it.

00:04:54.080 --> 00:04:56.980
And that is where web scraping comes into play.

00:04:56.980 --> 00:04:58.220
And data is great.

00:04:58.220 --> 00:04:59.040
Data is beautiful.

00:04:59.040 --> 00:05:04.760
There's a lot of things that you can do with data once you have it in a format that you can manipulate.

00:05:05.260 --> 00:05:07.960
And so that's where web scraping discipline sits.

00:05:07.960 --> 00:05:13.720
And given the increasing and ever increasing amount of data in web pages in a structure form,

00:05:13.720 --> 00:05:18.000
web scraping is here to stay definitely and it's going to continue growing.

00:05:18.400 --> 00:05:22.140
My interest in web scraping came shortly after I met Python in college.

00:05:22.140 --> 00:05:31.060
So there was this new site ecosystem in my country here in Uruguay where newspapers had just came online but didn't really get it.

00:05:31.060 --> 00:05:34.200
So they posted the news there.

00:05:34.200 --> 00:05:37.060
However, they seemed too pleased.

00:05:37.380 --> 00:05:39.440
And the information there wasn't really accessible.

00:05:39.440 --> 00:05:41.420
There was no commenting possibilities.

00:05:41.420 --> 00:05:48.500
So I started off creating a new site that aggregated all the newspaper, the local media available then, which was really short.

00:05:48.500 --> 00:05:53.720
And sort of put it together in a single news aggregator with RSS feeds.

00:05:53.720 --> 00:05:54.160
Nice.

00:05:54.160 --> 00:05:57.940
So it was just like a very early simple version of Google News type thing?

00:05:57.940 --> 00:06:02.620
Yeah, Google News existed back then, but Uruguay was completely ignored by them, right?

00:06:02.620 --> 00:06:08.020
Even up to today, we don't have Uruguay in Google News supported country.

00:06:08.020 --> 00:06:12.840
But you can think of it, yeah, as a Google News with custom extractors for each site.

00:06:12.840 --> 00:06:15.020
So I ended up building this project.

00:06:15.020 --> 00:06:20.900
It was called NotiUI because Noticias and Uruguay, the two words, combined together.

00:06:20.900 --> 00:06:22.520
I built it in Python.

00:06:22.520 --> 00:06:26.360
The extractors, as I was live, started getting some attention.

00:06:26.360 --> 00:06:31.600
It was after that project that I run my extra time completely.

00:06:31.600 --> 00:06:41.920
It's a hobby project that I got noticed by a company here in Uruguay that introduced me to the company that I later joined to work full-time on Python.

00:06:42.220 --> 00:06:45.840
I eventually created Scrappy and open-sourced it.

00:06:45.840 --> 00:06:49.740
So, yeah, it's funny how you can connect the dots after everything.

00:06:49.740 --> 00:06:53.080
Yeah, you can connect the dots looking backwards, but not forwards, right?

00:06:53.080 --> 00:06:54.200
Yeah, forward now.

00:06:54.200 --> 00:06:56.460
That's really cool.

00:06:56.460 --> 00:07:01.640
Before we move on to Scrappy, ideally, everything, all the information should be free.

00:07:01.920 --> 00:07:09.900
Maybe there should be, like, an HTTP JSON service, we could access, or, God forbid, like an XML, so WCF service or something.

00:07:09.900 --> 00:07:12.800
But a lot of times, even that is not available, right?

00:07:12.800 --> 00:07:17.780
There's just, it's on the web, and technically you can get the text, but nobody's built an API, right?

00:07:17.780 --> 00:07:24.540
And that's really where web scraping kind of comes in, is when you know that it is accessible, but there's no structured API for it, right?

00:07:24.600 --> 00:07:33.860
Exactly. And as a future of web scraping, I see scraping evolving a lot with a lot more machine learning incorporated directly at the early extraction phase,

00:07:33.860 --> 00:07:43.260
so that you can turn a website page into an API as quick as possible without requiring manual coding or manual intervention.

00:07:43.500 --> 00:07:52.900
That's really interesting, because the web scraping that I know is, you say, I'm looking for this information, and maybe here's a CSS selector that I could go against the page, and then I'll grab it.

00:07:52.900 --> 00:07:54.180
But that's not machine learning, right?

00:07:54.180 --> 00:07:56.700
That's the web scraping that most people are familiar with.

00:07:56.700 --> 00:08:05.060
But once you need to scale to a lot of websites that simply doesn't work, it was the scraping that Scrappy was built for.

00:08:05.060 --> 00:08:16.840
And because of this need to scale to maintaining a lot of website instructors in a common way and more consistent, unified fashion is that this Scrappy idea came to light.

00:08:16.840 --> 00:08:27.220
Because otherwise, after you're maintaining a couple of dozens of sites, you're fighting with just your own code infrastructure rather than writing the expats or the CSS selectors.

00:08:27.220 --> 00:08:29.340
That's the easy part, right?

00:08:29.340 --> 00:08:33.500
That's one of the reasons why we came up with Scrappy.

00:08:33.620 --> 00:08:33.940
Very cool.

00:08:33.940 --> 00:08:35.340
So why don't you tell us what it is?

00:08:35.340 --> 00:08:44.720
Scrappy started as an initiative to build a lot of web scrapers more easily and relieve the pain of maintaining them.

00:08:44.720 --> 00:08:52.580
Because Scrappy was built in an environment where we had to maintain hundreds of spiders, as we call them.

00:08:52.580 --> 00:09:02.900
And you need to have a certain structure in the code, certain conventions in place, so that when someone else joins the team, a new developer joins the team,

00:09:03.260 --> 00:09:14.680
and starts developing new spiders, they don't need to go through learning a lot of intricate internals about how the spiders are supposed to work and the rest of the infrastructure.

00:09:14.680 --> 00:09:29.620
So what Scrappy does is it tries to factor out the common things that you do when you write web scrapers and separate them from the actual extraction rules or expats and CSS selectors that you will type for each website.

00:09:29.620 --> 00:09:32.960
So that you have all the rest of the site.

00:09:32.960 --> 00:09:38.260
So that's where Scrappy excels, really.

00:09:38.260 --> 00:09:47.260
And compared to other ad hoc solutions, like combining requests with beautiful soup, which have great libraries and they do a great job.

00:09:47.640 --> 00:09:54.080
And maybe if you're writing a single structure for a single site, you wouldn't find much difference between using one or the other.

00:09:54.080 --> 00:10:05.740
But if your project involves maintaining a spider for hundreds or even dozens of websites, you'll see these conventions that Scrappy proposes very welcome advantages.

00:10:05.740 --> 00:10:07.580
So in a way, it's a framework.

00:10:07.580 --> 00:10:08.620
It's not a library.

00:10:08.900 --> 00:10:10.580
So it's not that it doesn't get in the way.

00:10:10.580 --> 00:10:11.980
It does get in the way.

00:10:11.980 --> 00:10:13.680
But for good reasons.

00:10:13.680 --> 00:10:32.900
It tries to impose some common practice and good practices and mechanisms so that you don't have to go through the typical journey of first doing this simple stuff and then realizing that you needed something more complex only to end up implementing a smaller version of what Scrappy is.

00:10:32.900 --> 00:10:35.160
That's a really interesting comparison with the other ones.

00:10:35.160 --> 00:10:38.600
I want to have you maybe quickly walk people through the API.

00:10:38.600 --> 00:10:42.260
But before we kind of get off the beginning of it, can you just tell me the history?

00:10:42.260 --> 00:10:44.120
You said it came out of your work at this company.

00:10:44.120 --> 00:10:51.880
Like, did you actually get permission to kind of extract that framework out or did you leave that company and rebuild it with your knowledge from there?

00:10:51.880 --> 00:10:52.840
What year is that?

00:10:52.840 --> 00:10:56.960
This company, MyDeco, was a furniture aggregator in the UK.

00:10:56.960 --> 00:10:58.920
I joined them in 2007.

00:10:58.920 --> 00:11:03.820
And when I joined MyDeco, there was a lot of parts of Scrappy already built there.

00:11:03.820 --> 00:11:11.040
Like the core framework, the downloader, and the more important internals were already in place.

00:11:11.040 --> 00:11:12.940
It was working fine already.

00:11:12.940 --> 00:11:20.040
You really noticed that there was an improvement over how you will go about writing ad hoc web crawlers.

00:11:20.200 --> 00:11:24.240
So I started in this Scrappy department in MyDeco.

00:11:24.240 --> 00:11:32.680
And I noticed that there was a huge potential for releasing this to the wild and making other people benefit from it.

00:11:32.680 --> 00:11:35.380
By then, I was already a hardcore open source fan.

00:11:35.820 --> 00:11:40.680
And I've been following many open source projects and open source practices.

00:11:40.680 --> 00:11:46.720
And I was looking for an opportunity to get involved and to release my own open source project.

00:11:46.720 --> 00:11:52.740
Pretty much like I was waiting for opportunities to pop up on or to work on Python in 2004.

00:11:52.740 --> 00:11:58.620
I was looking for open source opportunities in 2007 as well.

00:11:59.160 --> 00:12:06.540
Yeah, fortunately, the people in charge of MyDeco, the technical part of MyDeco, were quite aligned with open source as well.

00:12:06.540 --> 00:12:08.340
Quite fans themselves.

00:12:08.340 --> 00:12:10.220
So that makes things a lot easier.

00:12:10.220 --> 00:12:13.500
So we didn't have much problems convincing the board.

00:12:13.500 --> 00:12:18.560
Well, they didn't have much problems convincing the board to allow us to open source that part.

00:12:18.560 --> 00:12:26.180
What happened afterwards is that there was a lot of work on my side and a couple of guys that worked in my company back then.

00:12:26.180 --> 00:12:35.700
They're working on ironing out, factoring out the common code and packaging in a way that makes sense for external developers to digest and use.

00:12:35.700 --> 00:12:36.200
That's cool.

00:12:36.200 --> 00:12:39.100
And when you did that, basically you released this open source.

00:12:39.100 --> 00:12:42.560
Did this company like take it back and sort of start using it?

00:12:42.560 --> 00:12:44.900
Or did they just kind of go along in a parallel path?

00:12:44.900 --> 00:12:47.140
We started from something already built, right?

00:12:47.140 --> 00:12:52.260
It's not like we decided to build an open source project and started from scratch there.

00:12:52.260 --> 00:12:59.260
We had a system already working and we wanted to take certain core parts of it and open source it.

00:12:59.260 --> 00:13:06.400
This was a challenge because there was a lot of dependencies in the code that took us some time to factor out.

00:13:06.400 --> 00:13:11.140
But at no point we wanted to diverge or fork into two separate projects.

00:13:11.140 --> 00:13:14.000
I never even considered it to go that path.

00:13:14.060 --> 00:13:24.780
But I think it was crucial and important that we remain always in sync because MyDeco was the main user of Scrappy and the feedback loop was crucial for Scrappy to succeed.

00:13:24.780 --> 00:13:29.180
If you look at it, it makes no sense to build an open source.

00:13:29.420 --> 00:13:37.360
At least if you want to build a successful open source project, for me, you need to have successful companies or users using it, right?

00:13:37.360 --> 00:13:40.920
Just don't build it in abstract unless it's an academic thing, of course.

00:13:40.920 --> 00:13:42.880
This allowed us to grow Scrappy.

00:13:42.880 --> 00:13:57.420
The beginning was a lot of de-integration work, to put it somehow, so that all the crawlers at this company remain running while we move stuff to a state where it could be open source.

00:13:57.420 --> 00:14:09.880
And sure, a few spiders broke here and there, but yeah, we learned along the process how to improve spider testing, if you will, for when you make changes in a framework.

00:14:09.880 --> 00:14:13.700
And that's also part of Scrappy as one of the features.

00:14:13.700 --> 00:14:18.360
One thing that's cool about the spiders is you can rerun them against the same source if you have to, right?

00:14:18.360 --> 00:14:24.080
You can check if they extract the same data, and that's how you actually check that the spider remains working.

00:14:24.080 --> 00:14:28.980
You need to find a way to do it fast enough if you have thousands of them.

00:14:28.980 --> 00:14:33.300
That's just one caveat to keep in mind, but yeah.

00:14:33.300 --> 00:14:34.040
Sure, sure.

00:14:34.040 --> 00:14:36.140
What was the version of Python that you started on?

00:14:36.140 --> 00:14:37.400
It was 2.5 back then.

00:14:37.400 --> 00:14:42.380
It was only a year or two ago that we dropped support for 2.5 in Scrappy.

00:14:42.380 --> 00:14:47.640
And yeah, right now we're 2.7 and almost finishing support for Python 3.

00:14:47.640 --> 00:14:53.300
Actually, support is finished, and it's in beta mode now in the last version of Scrappy, which makes me very proud.

00:14:53.300 --> 00:14:54.920
Yeah, I just saw an announcement.

00:14:54.920 --> 00:15:00.640
What was that, two weeks ago that you guys announced that you are now officially beta in Python 3?

00:15:01.060 --> 00:15:13.100
I had a project where I needed to do some web scraping, and I ended up going the request plus beautiful soup for direction because I didn't want to be painted into the corner of halving to do Python 2.

00:15:13.100 --> 00:15:15.900
And I'm like, oh, if I go down Scrappy, the path maybe.

00:15:15.900 --> 00:15:19.080
But so I was like, oh, yes, that's great when that came out.

00:15:19.080 --> 00:15:20.080
So congratulations.

00:15:20.080 --> 00:15:21.060
Was it a lot of work?

00:15:21.060 --> 00:15:23.620
It was because it's a moderately big project.

00:15:23.620 --> 00:15:36.800
But we decided to really, really prioritize this because we realized that maybe a bit late, if you will, that people were actually not using or leaving Scrappy because of the lack of Python 3 support.

00:15:36.800 --> 00:15:39.200
But I'm glad that we're on it now and we have it there.

00:15:39.200 --> 00:15:40.700
Yeah, that's fantastic.

00:15:40.700 --> 00:15:41.220
Congrats.

00:15:41.220 --> 00:15:41.660
Thank you.

00:15:41.660 --> 00:15:41.860
Cool.

00:15:41.860 --> 00:15:43.540
So let's talk about the API a little bit.

00:15:43.540 --> 00:15:50.520
If you go to Scrappy.org, right there you've got a couple of code snippets showing you how you can use it and so on.

00:15:50.760 --> 00:15:55.300
One of your examples is if I go to like a blog, I want to go grab all the links that are categories.

00:15:55.300 --> 00:15:57.440
Can you maybe just talk us through what that looks like?

00:15:57.440 --> 00:15:57.740
Yeah.

00:15:57.740 --> 00:16:01.400
You want me to go through the code that is on Scrappy.org?

00:16:01.400 --> 00:16:02.240
Not exactly.

00:16:02.240 --> 00:16:05.680
Just tell me what, like kind of how do I get started with Scrappy, basically.

00:16:05.680 --> 00:16:11.440
So Scrappy has a pretty good tutorial in the base documentation that I recommend as a starting point.

00:16:11.440 --> 00:16:13.000
I always recommend a starting point.

00:16:13.000 --> 00:16:19.440
The idea is that you should, unless you have complex needs, you should be enough writing a spider,

00:16:19.580 --> 00:16:23.200
which can be just a couple of lines as it's shown in Scrappy.org website.

00:16:23.200 --> 00:16:30.760
The spider API itself is really simple in the sense that you have this, well, class to group all things related to the spider,

00:16:30.760 --> 00:16:33.740
where the class has methods that are called.

00:16:33.740 --> 00:16:40.720
These are essentially callbacks that are called with responses or web pages that are retrieved from the web,

00:16:40.880 --> 00:16:42.820
that are delivered to these callbacks.

00:16:42.820 --> 00:16:51.500
And then these callbacks process the response in however they need, and then return items or subsequent requests to follow.

00:16:51.500 --> 00:16:56.340
That's the very basic idea of Scrappy API in its best.

00:16:56.340 --> 00:17:01.400
So using this simple API, you can build a lot of things on top of it.

00:17:01.500 --> 00:17:05.780
I love APIs that are really simple at the bottom and that allows you to do a lot.

00:17:05.780 --> 00:17:11.600
This is the most basic spider API, and all spiders in Scrappy will follow this one.

00:17:11.600 --> 00:17:17.120
But then on top of it, there's variations and improvements, like something called crawl spider,

00:17:17.120 --> 00:17:22.120
that will allow you to set up some rules for URLs that should be followed by the spider.

00:17:22.120 --> 00:17:28.520
So you set up some class attributes with these rules and start URLs that they should start crawling from.

00:17:28.520 --> 00:17:37.020
And the spider automatically follows them and calls certain callbacks when the URLs fit a certain pattern.

00:17:37.020 --> 00:17:43.160
This type of spider is very useful for, for example, crawling a retailer, an e-commerce site,

00:17:43.160 --> 00:17:46.600
where you want to extract the product data for certain URLs.

00:17:46.600 --> 00:17:52.840
And many sites will follow this pattern of having a certain pattern of URLs to follow

00:17:52.840 --> 00:17:57.520
and certain rules to extract the product data out of the pages.

00:17:57.520 --> 00:18:04.900
So all that internally is built with a simple API of receiving a response and returning requests to follow

00:18:04.900 --> 00:18:07.460
and eat items of escape data.

00:18:07.460 --> 00:18:11.500
That's basically one of the great things and beauty of it.

00:18:11.500 --> 00:18:12.540
That's really cool.

00:18:12.540 --> 00:18:19.760
And at the heart of it, you've got this CSS method that I can give sort of complex hierarchical CSS expressions

00:18:19.760 --> 00:18:21.880
to get a hold of the pieces I need, right?

00:18:21.880 --> 00:18:22.260
Yeah.

00:18:22.260 --> 00:18:30.000
And on the extraction side, we also provide some convenient methods for extracting the data using well-known selectors

00:18:30.000 --> 00:18:33.660
like CSS selectors or even XPaths.

00:18:33.660 --> 00:18:33.820
Yeah.

00:18:33.820 --> 00:18:38.800
This is like the most universal way to address regions of pages.

00:18:38.800 --> 00:18:40.480
So you can do anything with it.

00:18:40.480 --> 00:18:47.700
And with the requests to follow, you can generate further requests to make what you would say ASHA calls

00:18:47.700 --> 00:18:50.840
in order to retrieve extra data from the web pages.

00:18:50.840 --> 00:18:52.760
And here is something to note.

00:18:52.760 --> 00:19:00.600
If you want to use this low level, you need to view in your browser what requests the website is doing

00:19:00.600 --> 00:19:02.480
and replicate those in Scrappy.

00:19:02.480 --> 00:19:03.200
Oh, interesting.

00:19:03.200 --> 00:19:10.820
So suppose I want to go and I want to scrape like a single page app, spa type app that's written in AngularJS.

00:19:10.820 --> 00:19:11.400
Right?

00:19:11.400 --> 00:19:17.740
If I just go hit that with a direct request, I'm going to get curly this, curly that, meaningless, no data, right?

00:19:17.740 --> 00:19:18.640
Yeah, exactly.

00:19:18.640 --> 00:19:18.700
Exactly.

00:19:29.700 --> 00:19:32.100
This episode is brought to you by Hired.

00:19:32.100 --> 00:19:37.700
Hired is a two-sided, curated marketplace that connects the world's knowledge workers to the best opportunities.

00:19:37.700 --> 00:19:45.180
Each offer you receive has salary and equity presented right up front, and you can view the offers to accept or reject them before you even talk to the company.

00:19:45.180 --> 00:19:50.920
Typically, candidates receive five or more offers within the first week, and there are no obligations ever.

00:19:50.920 --> 00:19:52.500
Sounds awesome, doesn't it?

00:19:52.500 --> 00:19:54.160
Well, did I mention the signing bonus?

00:19:54.160 --> 00:19:57.560
Everyone who accepts a job from Hired gets a $1,000 signing bonus.

00:19:57.560 --> 00:20:00.340
And as Talk Python listeners, it gets way sweeter.

00:20:00.340 --> 00:20:06.040
Use the link Hired.com slash Talk Python To Me, and Hired will double the signing bonus to $2,000.

00:20:06.040 --> 00:20:08.200
Opportunity's knocking.

00:20:08.200 --> 00:20:11.620
Visit Hired.com slash Talk Python To Me and answer the call.

00:20:17.620 --> 00:20:21.060
Remember that Scrappy was built in a different world in 2007.

00:20:21.060 --> 00:20:22.600
There was a lot of static.

00:20:22.600 --> 00:20:25.080
There was not so much Ajax, not so much JavaScript.

00:20:25.080 --> 00:20:38.120
And now with this crazy world moving to more like apps running in websites rather than website websites, Scrappy still is able to do it because Scrappy is, to some extent, a very low level.

00:20:38.120 --> 00:20:44.840
Whatever you can do in a browser, you can do in Scrappy because it works at the HTTP response request level.

00:20:44.840 --> 00:20:54.020
You sometimes feel that you need something more digestible so that you have the data readily available, the data that is delivered to Scrappy.

00:20:54.020 --> 00:20:58.300
Based on this need, we're working on extending Scrappy JavaScript support.

00:20:58.960 --> 00:21:13.420
Being the hardcore reusable component funds that we are, we ended up creating a separate component called Splash, which is sort of a mini browser that runs with an HTTP API.

00:21:13.420 --> 00:21:30.160
And it interacts really well with Scrappy, which is one of the many libraries that integrates well with Scrappy, so that you can actually have rendered data, JavaScript rendered data available in your callbacks for it to use, as well as executing actions on the website.

00:21:30.160 --> 00:21:31.000
That's really cool.

00:21:31.000 --> 00:21:32.220
I didn't know it did that.

00:21:32.220 --> 00:21:34.000
So that's really excellent.

00:21:34.160 --> 00:21:39.260
Yeah, it's not one of the things that are most prominently shown when you came across Scrappy.

00:21:39.260 --> 00:21:39.800
Yeah, of course.

00:21:39.800 --> 00:21:46.700
But if you've got to go after a website that's sort of one of these front-end JavaScript data binding type frameworks, you have to have those things.

00:21:46.700 --> 00:21:47.420
That's really cool.

00:21:47.420 --> 00:22:00.120
So another thing that's in your sample that I think is worth bringing up just really quickly, although it's not specific to what you guys are doing, is you're using the yield keyword and your parse methods.

00:22:00.120 --> 00:22:05.780
And the bit that you do supports sort of these generator methods.

00:22:05.780 --> 00:22:06.360
Yeah.

00:22:06.360 --> 00:22:09.540
And these sort of progressive get-as-much-as-you-like type of methods.

00:22:09.540 --> 00:22:12.540
So do you want to maybe just talk really quickly about generator methods?

00:22:12.540 --> 00:22:16.260
Because I don't think we've talked about it on the show very often, if at all.

00:22:16.260 --> 00:22:16.960
Yeah, absolutely.

00:22:17.280 --> 00:22:24.820
So this spider callback that I was talking about actually can return a generator or an iterator in its most general form.

00:22:24.820 --> 00:22:30.900
So what Scrappy does internally, it calls this callback and starts iterating the output of it.

00:22:30.900 --> 00:22:34.320
So it can potentially return an infinite generator.

00:22:34.320 --> 00:22:37.200
And Scrappy sometimes is used that way.

00:22:37.200 --> 00:22:44.560
You start in a page and start monitoring the page and keep returning data and your requests to follow as they appear.

00:22:44.560 --> 00:22:49.420
Or you generate just incremental numbers or whatever of pages to check.

00:22:49.420 --> 00:22:55.640
And yeah, Scrappy uses memory efficiently in order not to consume the whole generator at once.

00:22:55.640 --> 00:22:58.740
It consumes its incisable, manageable parts.

00:22:59.260 --> 00:23:09.920
And it has a bunch of flow control directives in place to make sure that no place in the framework consumes too much and overflows memory.

00:23:09.920 --> 00:23:14.280
This has the result that in the spider code, you can use nine things like GIL,

00:23:14.280 --> 00:23:20.500
where you just send the data out into Scrappy framework side.

00:23:20.500 --> 00:23:28.660
And you'll be sure that it's going to get processed and you don't have to check yourself that if you're sending too much data or too little or whatever.

00:23:28.660 --> 00:23:33.500
So yeah, that's how you can make benefits of how Scrappy makes benefits.

00:23:33.500 --> 00:23:33.780
Yeah.

00:23:33.780 --> 00:23:36.160
So internally, it uses iterators everywhere.

00:23:36.160 --> 00:23:40.040
And you can just layer on your own iterators, right?

00:23:40.100 --> 00:23:44.320
Because if it would build like lists and, you know, fill them up, that would kind of be useless, right?

00:23:44.320 --> 00:23:44.720
Exactly.

00:23:44.720 --> 00:23:47.920
It's iterator and generator friendly by default.

00:23:47.920 --> 00:23:49.520
And you can take that advantage.

00:23:49.520 --> 00:23:50.500
Yeah, that's really awesome.

00:23:50.500 --> 00:23:57.000
If I want to go to a website, you know, maybe let's just take somewhere like the New York Times, say.

00:23:57.000 --> 00:23:59.720
So the New York Times, they're a well-known newspaper.

00:24:00.000 --> 00:24:01.380
Their articles are highly valued.

00:24:01.380 --> 00:24:03.720
But there's also usage restrictions.

00:24:03.720 --> 00:24:08.160
They have their so-called paywall about trying to make sure you have an account and stuff.

00:24:08.160 --> 00:24:12.940
Is it legal for me to go and turn Scrappy loose on the New York Times?

00:24:12.940 --> 00:24:17.260
Or what's the story sort of around like when I can and can't use the web scraping?

00:24:17.260 --> 00:24:19.060
We like to see scraping as a tool.

00:24:19.060 --> 00:24:25.940
And as with many other tools, you can use it for with legal purposes in mind and illegal purposes in mind.

00:24:26.160 --> 00:24:36.160
Even tools like Bitstorrent, which is a de facto example, can be used for completely authentic and genuine stuff, legal stuff.

00:24:36.160 --> 00:24:37.100
I'm not saying that.

00:24:37.100 --> 00:24:42.800
So they use what you're doing with the tool is ultimately what makes it illegal or not.

00:24:42.800 --> 00:24:48.420
In the case of the New York Times, you should probably obey whatever rules they have to access the content.

00:24:48.420 --> 00:24:52.640
I hope that sometimes they will realize that they are doing it wrong.

00:24:53.800 --> 00:25:01.360
You can't see, at least I don't see scraping as a way to get data, to steal data that isn't supposed to be taken.

00:25:01.360 --> 00:25:12.060
There's a lot of projects and stuff out there that uses scraping to just gather the data that is already available there in order to process it.

00:25:12.060 --> 00:25:12.260
Right.

00:25:12.260 --> 00:25:14.000
Like a really common example would be Google.

00:25:14.000 --> 00:25:14.400
Exactly.

00:25:14.400 --> 00:25:17.420
I don't know that they use Scrappy, but obviously they do that.

00:25:17.420 --> 00:25:18.700
Conceptually, they do that, right?

00:25:18.700 --> 00:25:19.100
Exactly.

00:25:19.320 --> 00:25:21.960
Google is the largest scraper in the world.

00:25:21.960 --> 00:25:28.020
And they get the data and add value to it by showing them their search results.

00:25:28.020 --> 00:25:34.500
Similar thing applies everywhere for us and how we tackle projects.

00:25:34.500 --> 00:25:42.340
We generally go with public information only when we work on scraping projects.

00:25:42.340 --> 00:25:48.080
We don't want to deal with all the legal stuff like this.

00:25:48.080 --> 00:25:55.340
And yeah, we always recommend customers to get proper assistance and possible consent from the websites to get the data.

00:25:55.600 --> 00:26:01.360
Because many cases, believe it or not, their websites don't care if you take the data from them.

00:26:01.360 --> 00:26:06.380
But they just don't want to go through the table of packaging the data and sending it to you or putting it from you.

00:26:06.380 --> 00:26:06.960
Yeah, right.

00:26:06.960 --> 00:26:10.060
Or maybe they would like to, but they're technically incapable.

00:26:10.060 --> 00:26:10.780
Exactly.

00:26:10.960 --> 00:26:13.100
Not everybody who has a website is a programmer.

00:26:13.100 --> 00:26:14.820
Yeah.

00:26:14.820 --> 00:26:22.700
I've heard so many times saying to website owners saying, okay, if you can take the data and you don't cause me any problems, then go for it.

00:26:22.700 --> 00:26:26.140
And you wouldn't believe how common that sentiment is.

00:26:26.140 --> 00:26:26.560
Yeah, yeah.

00:26:26.560 --> 00:26:27.120
That's really cool.

00:26:27.120 --> 00:26:27.680
That's really cool.

00:26:27.680 --> 00:26:30.440
Let's talk about large-scale web crawling.

00:26:30.440 --> 00:26:31.560
I mentioned Google.

00:26:31.560 --> 00:26:32.660
That's kind of large-scale.

00:26:32.660 --> 00:26:36.500
You had your experience with thousands of crawlers.

00:26:36.700 --> 00:26:43.880
What do you need to worry about when you're running stuff at that scale rather than I have my app that goes to one page and just get some information?

00:26:43.880 --> 00:26:44.300
Right.

00:26:44.300 --> 00:26:48.760
Scalping can kind of scale in two directions, I would say.

00:26:48.760 --> 00:26:55.800
One is the code direction and another is the infrastructure and volumes direction.

00:26:55.800 --> 00:27:03.640
Large-scale at the code side could be having to maintain a couple thousand spiders in your project.

00:27:03.640 --> 00:27:06.520
Let's call it more vertical scaling, if you will.

00:27:06.520 --> 00:27:22.700
Whereas scaling in the horizontal or infrastructure side would be having a single code, perhaps more automated or based on sophisticated extraction, that needs to grow a lot of web pages, like hundreds of millions of web pages.

00:27:22.700 --> 00:27:23.560
Okay.

00:27:23.560 --> 00:27:25.800
And that's more like what you're talking about with the machine learning.

00:27:25.800 --> 00:27:38.840
Maybe you would do Scrapey plus scikit-learn and a single code base to just go understand the web versus I know that these are the 27 retailers' websites and I want to give you the price of all of them.

00:27:38.840 --> 00:27:42.000
So I'll write specific code to get the price of these common objects.

00:27:42.000 --> 00:27:42.760
Something like this, right?

00:27:42.760 --> 00:27:43.520
Yeah, exactly.

00:27:43.700 --> 00:27:53.900
And you can only scale so much, vertically scale so much with numbers of spiders because the cost starts increasing quite a lot to keep these spiders running and well-maintained.

00:27:53.900 --> 00:28:03.640
So because spiders break, you need to monitor them, react when they break, and the size of the team that you need to have to maintain them goes up really quick.

00:28:03.640 --> 00:28:09.060
You get a little nice happy message from Target saying, we've just redesigned Target.com.

00:28:09.060 --> 00:28:11.340
And you're like, oh, no, there it goes, right?

00:28:11.340 --> 00:28:11.840
Yeah.

00:28:11.840 --> 00:28:12.400
Yeah.

00:28:12.400 --> 00:28:14.960
Don't get me started with Black Friday and holiday seasons.

00:28:14.960 --> 00:28:19.020
Challenges vary depending on which case we're talking.

00:28:19.020 --> 00:28:26.920
The code scalability is somehow minimized or addresses as much as possible by scrappy common conventions.

00:28:26.920 --> 00:28:28.580
It's as much as you can do.

00:28:28.580 --> 00:28:34.360
I mean, in the end, you have to write the XPaths or CSS selectors anyway at some point.

00:28:34.360 --> 00:28:43.040
But aside from that, anything else that can be automated, infrastructure or review all sides, we try to automate it.

00:28:43.040 --> 00:28:49.160
The other type of scalability problems are related with crawling a huge number of pages.

00:28:49.160 --> 00:29:07.260
There involves not only the massive amount of data that you need to go through and to digest, but things like revisiting policies for when you have to keep track of a huge number of pages in a very large website or group of websites.

00:29:07.260 --> 00:29:12.280
And you only want to revisit them depending on how often they get updated.

00:29:12.280 --> 00:29:16.160
So there's a few common algorithms used in this case.

00:29:16.160 --> 00:29:25.760
But the big challenge here is keeping this big queue of URLs that you need to keep control of and prioritize.

00:29:26.300 --> 00:29:29.720
Because the crawler just kind of takes URLs from a queue.

00:29:29.720 --> 00:29:33.480
Crawler just will ask a queue, okay, what's the next URL you want me to fetch?

00:29:33.480 --> 00:29:38.340
And it will take that URL, fetch it, return the process results.

00:29:38.340 --> 00:29:43.380
But keeping this queue is the challenging thing when doing it at scale.

00:29:43.380 --> 00:29:44.260
Yeah, yeah, of course.

00:29:44.260 --> 00:29:51.440
Because you want to, you know, cache, sort of conceptually cache as much of that as possible and not hit it if it's not going to have changed.

00:29:51.440 --> 00:29:59.380
Is that a place where you could plug in, like, machine learning and have it watch and sort of tell it, like, this time I learned something new, this time I didn't.

00:29:59.380 --> 00:30:01.340
And it could maybe be smart about this?

00:30:01.340 --> 00:30:02.300
Yeah, yeah, exactly.

00:30:02.300 --> 00:30:08.340
As I was saying, there's a few common algorithms used to revisit simply based on frequency.

00:30:08.340 --> 00:30:10.120
For example, frequency of updates.

00:30:10.120 --> 00:30:16.300
Like if you visit a page two times during a certain amount of period and the page is the same.

00:30:16.300 --> 00:30:18.460
Or actually the extracted data is the same.

00:30:18.460 --> 00:30:18.780
Exactly.

00:30:18.780 --> 00:30:19.700
That's what matters, right?

00:30:19.700 --> 00:30:23.280
Just to, yeah, to take out overhead changes.

00:30:23.280 --> 00:30:28.880
Then you perhaps double the time that you're going to check that page again.

00:30:28.880 --> 00:30:32.480
So if you revisit in a day, it didn't change.

00:30:32.480 --> 00:30:35.500
Then you're going to visit again at least in two days from now.

00:30:35.500 --> 00:30:41.440
And if the page changes, then for that page, you reduce to the half of the time that you waited.

00:30:41.440 --> 00:30:43.600
So you're going to revisit in a half day.

00:30:43.600 --> 00:30:48.320
And let the wait times adjust automatically.

00:30:48.320 --> 00:30:50.840
That works as a basic algorithm.

00:30:50.840 --> 00:30:52.220
It works really well.

00:30:52.220 --> 00:30:59.960
And Scrappy has its own internal scheduler, which serves as this in-memory queue for requests.

00:31:00.340 --> 00:31:05.980
It can only grow so much because Scrappy all runs in a process, in an operating system process.

00:31:05.980 --> 00:31:10.900
And there's memory limitations when running a single process.

00:31:10.900 --> 00:31:17.060
So when you need to scale to very large number of web pages, you need to use something external.

00:31:17.060 --> 00:31:25.540
And on this side of the equation is that we're working on for just over a year now in a new project called Frontera,

00:31:25.540 --> 00:31:29.840
which is an extension to Scrappy to deal with large crowds.

00:31:30.040 --> 00:31:33.460
And it essentially manages what is called the crawl frontier.

00:31:33.460 --> 00:31:43.020
I should have mentioned before, but this queue of requests to follow is called the crawl frontier in Scraping and web crawling terms.

00:31:43.020 --> 00:31:43.700
Yeah, very cool.

00:31:43.700 --> 00:31:51.560
You guys also built like a web crawling as a service or web crawling infrastructure as a service, if you will, right?

00:31:51.560 --> 00:31:52.760
Tell me about that.

00:31:52.760 --> 00:32:01.440
Yeah, so one of the things that are common to many Scraping, if not all Scraping projects, are the infrastructures required by it.

00:32:01.440 --> 00:32:08.720
As I was saying, writing the CSS selector expat is something that you may need to do separately for each website.

00:32:08.720 --> 00:32:20.440
But all the rest, like running the spider, getting the data, reviewing the data with your colleagues or customer, iterating over it, is pretty much the same.

00:32:20.440 --> 00:32:27.000
And it's surprisingly what ends up taking more time than writing the CSS selectors or the expats itself.

00:32:27.000 --> 00:32:37.760
Because when you're dealing with data, sometimes there are different expectations of how the customer needs the data and how you produce it from your spider.

00:32:37.760 --> 00:32:44.820
So being able to have those in sync with an efficient tool is of crucial importance.

00:32:45.280 --> 00:32:53.840
So we realized this around 2010 that there was a business opportunity to launch a tool for taking the next step after Scrappy.

00:32:53.840 --> 00:32:58.220
Because Scrappy kind of solves the problems at the developer level.

00:32:58.220 --> 00:33:02.260
You can run Scrappy Spider with the Scrappy crawl command.

00:33:02.260 --> 00:33:04.980
It all works the same in any machine, in any platform.

00:33:04.980 --> 00:33:06.740
But what after that?

00:33:06.740 --> 00:33:08.500
What if you need to collaborate with teams?

00:33:08.720 --> 00:33:11.680
Then it starts getting a bit complicated.

00:33:11.680 --> 00:33:21.220
We started Scraping Hub to deal with that, with running Scrappy Spiders in the cloud in the most friendly way possible.

00:33:21.220 --> 00:33:24.480
So that developers can benefit from it.

00:33:24.480 --> 00:33:25.080
Yeah, that's cool.

00:33:25.080 --> 00:33:27.200
And that's just at scrapinghub.com, right?

00:33:27.200 --> 00:33:27.880
Yeah, exactly.

00:33:27.880 --> 00:33:28.240
Yeah.

00:33:28.300 --> 00:33:29.460
So people can check it out there.

00:33:29.460 --> 00:33:32.360
Maybe tell us, like, what is the main use case?

00:33:32.360 --> 00:33:37.120
Like, why do people come and set up a system to run on your platform?

00:33:37.120 --> 00:33:45.600
The nice thing about Scraping Hub is that you can run any Scrappy Spider that runs in your machine already will run on Scraping Hub cloud.

00:33:45.600 --> 00:33:47.220
So that's the premise of the service.

00:33:47.220 --> 00:33:53.100
We don't require you to make any changes to your already working spiders.

00:33:53.560 --> 00:34:00.520
You can just run a command, Scraping Hub, deploy, and you have it already deployed in our cloud.

00:34:00.520 --> 00:34:04.140
And you can run it using the web analytics panel from there.

00:34:04.140 --> 00:34:06.360
Think of it similar to Heroku here.

00:34:06.360 --> 00:34:07.880
Not sure if you're familiar with it.

00:34:07.880 --> 00:34:10.180
But Heroku is the same for websites.

00:34:10.180 --> 00:34:12.840
You have a website running locally.

00:34:12.840 --> 00:34:18.340
And yes, you need to configure some manifest file for sure to indicate a few things.

00:34:18.340 --> 00:34:20.120
But that's all you need to do.

00:34:20.120 --> 00:34:23.400
And then with Heroku.deploy, you have the website running.

00:34:23.400 --> 00:34:25.920
It doesn't matter where you have the URL.

00:34:25.920 --> 00:34:28.180
And anyone can access it.

00:34:28.180 --> 00:34:31.040
We wanted to build the same for spiders.

00:34:31.040 --> 00:34:32.680
There wasn't anything like it.

00:34:32.680 --> 00:34:34.700
And still, there isn't.

00:34:34.700 --> 00:34:40.100
And we realized that we had kind of this, the best framework for writing with spiders.

00:34:40.100 --> 00:34:49.240
And kind of the next obvious thing to do was to give it the friendliest way possible to run this and collaborate with your team.

00:34:49.240 --> 00:34:57.240
You can run and you can see the stuff, the data that is being scraped with the images or all those things like nicely rendered.

00:34:57.240 --> 00:35:04.420
And you can add inline comments to the data to check and mention colleagues to check if it's okay.

00:35:04.420 --> 00:35:12.380
The other big piece of infrastructure is that you need to, it's very hard to have this infrastructure when you're small.

00:35:12.380 --> 00:35:16.720
So it doesn't make sense to build this if you're just crawling a few sites.

00:35:16.720 --> 00:35:24.860
Although big companies that do a lot of web crawling like Google have their own sophisticated crawling infrastructures already in place.

00:35:25.420 --> 00:35:33.000
And having a sophisticated crawling infrastructure in place is a pretty daunting task for a small startup or for a small company.

00:35:33.000 --> 00:35:48.580
So we wanted to make that accessible for small companies when we started escaping how all this or as much as this nice infrastructure perks that one has when you have a recently large crawling infrastructure.

00:35:48.580 --> 00:35:50.180
Make it available to everyone.

00:35:50.180 --> 00:35:52.140
That's really an interesting mission.

00:35:52.140 --> 00:36:03.740
I mean, if I was going to do a startup and a key component of that was to go out and just gather all this data through web scraping, you know, you could set up your own infrastructure on AWS or DigitalOcean or wherever.

00:36:03.740 --> 00:36:04.420
Right.

00:36:04.420 --> 00:36:05.500
But that's not your job.

00:36:05.500 --> 00:36:06.280
Why do you want to do that?

00:36:06.280 --> 00:36:06.460
Right.

00:36:06.460 --> 00:36:08.740
Just drop it over there and let you guys deal with it.

00:36:08.740 --> 00:36:25.840
Continuous delivery isn't just a buzzword.

00:36:25.840 --> 00:36:29.500
It's a shift in productivity that will help your whole team become more efficient.

00:36:29.500 --> 00:36:35.680
With SnapCI's continuous delivery tool, you can test, debug, and deploy your code quickly and reliably.

00:36:36.000 --> 00:36:41.560
Get your product in the hands of your users faster and deploy from just about anywhere at any time.

00:36:41.560 --> 00:36:46.780
And did you know that ThoughtWorks literally wrote the book on continuous integration and continuous delivery?

00:36:46.780 --> 00:36:51.920
Connect Snap to your GitHub repo and they'll build and run your first pipeline automagically.

00:36:51.920 --> 00:36:58.540
Thanks SnapCI for sponsoring this episode by trying them for free at snap.ci slash talkpython.

00:36:58.540 --> 00:37:09.100
That's what we try to convey.

00:37:09.100 --> 00:37:14.060
Like, we are the Heroku of, we're trying to become the Heroku of web scraping.

00:37:14.060 --> 00:37:21.180
You can always build an AWS server or DigitalOcean server and deploy everything there.

00:37:21.180 --> 00:37:28.200
And it will work because everything is based on open source tools and we have remained firmly connected.

00:37:28.200 --> 00:37:30.760
against any type of vendor lock-in.

00:37:30.760 --> 00:37:33.640
We release as much as we can on open source.

00:37:33.640 --> 00:37:36.780
And I'm not understating this in any way.

00:37:36.780 --> 00:37:43.600
The things that we haven't yet open source, if we haven't cleaned them enough to send them out there.

00:37:43.600 --> 00:37:46.280
And it will cause more harm than good being out there.

00:37:46.340 --> 00:37:54.260
But eventually we see ourselves like open source and everything and charging for infrastructure, essentially, to run those things.

00:37:54.260 --> 00:37:56.380
We don't want to charge for code licenses.

00:37:56.380 --> 00:37:56.920
Right.

00:37:56.920 --> 00:37:59.200
Do you see it as somewhat analogous to OpenStack?

00:37:59.200 --> 00:38:00.140
Yes, absolutely.

00:38:00.140 --> 00:38:01.340
From a business model.

00:38:01.340 --> 00:38:03.140
Yeah, absolutely.

00:38:03.560 --> 00:38:07.880
Because the infrastructure is something that you can never expect to open source, right?

00:38:07.880 --> 00:38:14.120
So there's, it's not like a major flaw that we could have at some point.

00:38:14.120 --> 00:38:18.080
And so, yeah, it's very similar in that regard to OpenStack.

00:38:18.080 --> 00:38:21.460
There's also two types of customers, right?

00:38:21.600 --> 00:38:30.300
as sometimes I say, like developers with no money, but very eager to learn and do stuff.

00:38:30.300 --> 00:38:38.120
And companies with big pockets and no time and they want things like working tomorrow.

00:38:38.120 --> 00:38:42.260
We're trying to fulfill both targets, right?

00:38:42.260 --> 00:38:48.380
We have a business division in Scape and have that where you can get the spider written and everything

00:38:48.380 --> 00:38:50.800
and you don't need to do anything, just get the data.

00:38:51.240 --> 00:38:56.560
But internally, this uses all of the rest of our platform that we share with developers

00:38:56.560 --> 00:38:57.740
in the world.

00:38:57.740 --> 00:39:00.240
And we both work on the platform.

00:39:00.240 --> 00:39:03.280
So if you're a developer, you can use the platform.

00:39:03.280 --> 00:39:08.660
And if you're a business, you can hire developers to using the same platform, get the job done

00:39:08.660 --> 00:39:09.100
for you.

00:39:09.100 --> 00:39:17.420
And I believe that trying to fulfill both audiences is key for business built around open source

00:39:17.420 --> 00:39:18.240
to succeed.

00:39:18.240 --> 00:39:20.880
Well, I guess you could extend that to any business.

00:39:21.020 --> 00:39:25.200
But it has been one of our best decisions.

00:39:25.200 --> 00:39:29.300
And I've always been a fan of building developer tools.

00:39:29.300 --> 00:39:34.060
Coming from a developer background, and I'm still a programmer first and foremost.

00:39:34.780 --> 00:39:41.620
I love tools that are well done that allow you to enjoy working and having fun while you

00:39:41.620 --> 00:39:41.920
work.

00:39:41.920 --> 00:39:44.360
So I love building these things.

00:39:44.360 --> 00:39:52.100
I always like when I wanted to work on something, I first try to build tools to make that something

00:39:52.100 --> 00:39:55.280
more efficient before actually going to work.

00:39:55.380 --> 00:39:56.000
Yeah, that's cool.

00:39:56.000 --> 00:39:59.400
And now you just get to enable other people to build their thing, right?

00:39:59.400 --> 00:40:00.920
And you get to just keep working on the tools.

00:40:00.920 --> 00:40:01.180
Yeah.

00:40:01.180 --> 00:40:05.080
So I want to come back to this idea of open source and business.

00:40:05.080 --> 00:40:08.880
These are very interesting mixes to me.

00:40:09.560 --> 00:40:11.960
Real quickly, what's the future of web scraping like?

00:40:11.960 --> 00:40:16.620
From where we are now, what do you see coming up in the next five or 10 years that's going

00:40:16.620 --> 00:40:17.200
to be different?

00:40:17.200 --> 00:40:20.580
Well, the web is going to be there still, that's for sure.

00:40:20.940 --> 00:40:26.800
Technologies will change quite a bit in five years if you look back at what happened in

00:40:26.800 --> 00:40:27.720
the last five years.

00:40:27.720 --> 00:40:36.020
And I think the world is a lot less proprietary now in terms of internet technologies.

00:40:36.020 --> 00:40:43.120
Perhaps the last big one was the fact that Flash lost to HTML5.

00:40:43.120 --> 00:40:49.880
It's kind of the last example of a big proprietary versus open technology.

00:40:49.880 --> 00:40:55.040
Yeah, that's a big technical wall coming crashing down that will let you into that area now,

00:40:55.040 --> 00:40:55.240
right?

00:40:55.240 --> 00:40:56.260
Yeah.

00:40:56.260 --> 00:41:00.800
So of course, we benefit from the fact that everything is in HTML.

00:41:00.800 --> 00:41:03.700
It's going to be more in HTML out there.

00:41:03.700 --> 00:41:05.500
And it will continue to be.

00:41:05.500 --> 00:41:11.740
But the point is that technologies may change a bit, but the information that is available

00:41:11.740 --> 00:41:15.940
out there will need to be retrieved and used by companies.

00:41:15.940 --> 00:41:20.860
So scraping in general, I don't think that it will change much the concept.

00:41:20.860 --> 00:41:27.060
The earlier concept of screen scraping, of extracting data from screens rather than from web pages,

00:41:27.060 --> 00:41:29.020
still remains to this day.

00:41:29.560 --> 00:41:35.500
And if anything, it will change a bit in terms of how you extract the data.

00:41:35.500 --> 00:41:43.460
And it's going to involve a lot more JavaScript processing and kind of tool automation for reproducing

00:41:43.460 --> 00:41:49.740
actions rather than following links to follow the pattern of how web applications are transforming.

00:41:50.300 --> 00:41:56.480
But I don't expect a lot of that to change aside from that, really.

00:41:56.480 --> 00:42:02.400
But there's going to be a lot of sophisticated technologies that will keep increasing on the

00:42:02.400 --> 00:42:08.200
side of keeping web crawlers away, detecting web crawlers and banning them for sure.

00:42:08.200 --> 00:42:10.120
There's a bit of an arms race there, right?

00:42:10.180 --> 00:42:11.500
Yeah, yeah, exactly.

00:42:11.500 --> 00:42:12.960
It's a...

00:42:12.960 --> 00:42:15.400
User agent equals MSIE 10.

00:42:15.400 --> 00:42:17.620
Yeah.

00:42:17.620 --> 00:42:18.740
Things like this, yes?

00:42:18.740 --> 00:42:21.040
That summarizes well.

00:42:21.040 --> 00:42:28.440
But yeah, the fact that the companies are going to be more protective of their data, or some

00:42:28.440 --> 00:42:33.920
of them, the ones that consider it a core business value, will mean that the scraping technologies

00:42:33.920 --> 00:42:38.740
will need to evolve and become more sophisticated as well to follow it.

00:42:38.740 --> 00:42:45.480
So there's going to be a large, very expensive, private market where you will be able to find,

00:42:45.480 --> 00:42:51.940
I don't know, mechanisms to still get the data out of web pages with an even bigger infrastructure.

00:42:51.940 --> 00:42:57.040
That's not like the focus of what we're trying to achieve, right?

00:42:57.040 --> 00:42:57.620
Right, of course.

00:42:57.620 --> 00:43:02.960
So two other things that are sort of not knowing anything about this come to mind for me is,

00:43:02.960 --> 00:43:08.860
one, you talked about the JavaScript front-end stuff, the fact that you guys support grabbing

00:43:08.860 --> 00:43:11.140
sort of AngularJS style apps.

00:43:11.140 --> 00:43:12.640
I think there's more of those, right?

00:43:12.640 --> 00:43:13.560
So that'll be interesting.

00:43:13.560 --> 00:43:15.360
But what about HTTP2?

00:43:15.360 --> 00:43:20.100
Will that have any effect on you guys, or better scalability, or no effect?

00:43:20.100 --> 00:43:20.780
What do you think?

00:43:20.780 --> 00:43:23.800
No, I don't see HTTP...

00:43:23.800 --> 00:43:26.900
I mean, it won't change really much.

00:43:26.900 --> 00:43:29.900
It's going to be part of...

00:43:29.900 --> 00:43:38.440
The framework will be adapted to take HTTP2 enhancement in hand, but the discipline of the scraping data,

00:43:38.440 --> 00:43:42.000
I don't see it will change much after that.

00:43:42.000 --> 00:43:42.400
Yeah.

00:43:42.400 --> 00:43:47.240
Web sockets may require, if you think of it, a more...

00:43:47.240 --> 00:43:49.080
It's a different approach to it.

00:43:49.080 --> 00:43:50.940
Yeah, when the data starts coming out of web sockets.

00:43:50.940 --> 00:43:51.240
Yeah.

00:43:51.240 --> 00:43:58.480
I guess that we haven't done pretty much any web sockets scraping recently, or at all.

00:43:58.480 --> 00:44:02.760
And I really haven't put a lot of thought to it.

00:44:02.760 --> 00:44:05.020
It hasn't crossed my mind a lot later.

00:44:05.020 --> 00:44:05.220
Sure.

00:44:05.220 --> 00:44:11.640
So let's talk a little bit about, while we have a few minutes left, let's talk about business, open source, making a living on open source.

00:44:11.640 --> 00:44:16.560
There's some really powerful examples of companies doing this and people doing it.

00:44:16.560 --> 00:44:20.960
But there's also a lot of people who dream of it, but don't see a path forward.

00:44:20.960 --> 00:44:21.640
Right?

00:44:21.640 --> 00:44:24.240
Notable examples of people making it work for companies.

00:44:24.240 --> 00:44:30.680
Continuum, the Anaconda guys, the Red Hat OpenStack, MongoDB.

00:44:31.100 --> 00:44:35.100
Those are all really companies that have somehow made open source work.

00:44:35.100 --> 00:44:36.360
Somehow made it work.

00:44:36.360 --> 00:44:37.520
Somehow.

00:44:37.520 --> 00:44:41.140
But there's got to be a ton of failed attempts, unfortunately.

00:44:41.140 --> 00:44:43.920
So maybe tell me, how is this going for you guys?

00:44:43.920 --> 00:44:52.580
What's it like to run sort of a distributed company that builds an open source product, but still somehow makes it a business?

00:44:52.580 --> 00:45:00.580
We started by doing a lot of professional, and we still do a lot of professional service work consulting using our own tools.

00:45:00.940 --> 00:45:10.900
So you need to keep this mind, this audience of companies and businesses that could benefit from your open source project if you want to make a living out of it.

00:45:10.900 --> 00:45:14.160
In our case, it wasn't too difficult, right?

00:45:14.160 --> 00:45:16.580
Because there's a natural need from data.

00:45:16.580 --> 00:45:20.240
Data is king, and many companies are after data.

00:45:20.240 --> 00:45:26.900
And the open source tool that we developed and we maintain is related to extracting data.

00:45:27.360 --> 00:45:35.940
So we started escaping here with no standard funding and still don't have any by providing solutions to these companies, first and foremost.

00:45:36.200 --> 00:45:45.700
And then we were able to evolve the open source products as the needs of our customers require, improving it along the way when we found the opportunity.

00:45:45.700 --> 00:45:49.520
And even up to this point, the modus operandi is very similar.

00:45:49.800 --> 00:46:07.320
So try to find, I would say, try to find where your paying customers will be, like companies that will benefit from your open source project.

00:46:07.860 --> 00:46:13.820
And try to build, like try to build a company that proposes, provides these offerings.

00:46:13.820 --> 00:46:19.780
In our case, we did it by providing development expertise to write web crawlers.

00:46:20.040 --> 00:46:25.560
But it's very similar how we did it with how continuous that's it, for example.

00:46:25.560 --> 00:46:27.700
And I'm a big fan of them.

00:46:27.700 --> 00:46:31.320
And it's a really nice model to follow.

00:46:31.320 --> 00:46:39.420
They do a lot of consulting, but they keep this Anaconda platform and everything that makes their jobs easier.

00:46:39.420 --> 00:46:43.780
And at the same time, allows them to spend working on open source a lot.

00:46:43.780 --> 00:46:52.300
And I always wanted to be able to just live in the open source world and keep coding on Scrappy.

00:46:52.300 --> 00:46:56.720
Unfortunately, I'm not able to code much on Scrappy these days.

00:46:56.720 --> 00:47:01.800
But I plan to return to it once the company runs in autopilot.

00:47:01.800 --> 00:47:07.760
And I'm able to retire to get back to Scrappy and size projects.

00:47:07.760 --> 00:47:18.960
But yeah, if you're just trying to connect the dots to where companies will make use of the open source project that you're working on.

00:47:18.960 --> 00:47:28.960
Like if you're working on something like Bootstrap, find a consulting company that builds websites quickly, prototype websites quickly, those type of things.

00:47:28.960 --> 00:47:34.360
So let me think through if I were trying to plan this out for Scrappy, how it might go.

00:47:34.360 --> 00:47:46.320
So if I were, you know, you guys five or eight years ago, whenever, before you started Scraping Hub, I'm thinking, okay, well, I've got this really successful web scraping open source project.

00:47:46.320 --> 00:47:47.500
And people are using it.

00:47:47.500 --> 00:47:48.380
The company is using it.

00:47:48.380 --> 00:47:50.940
They're really just, they're using it as a tool.

00:47:50.940 --> 00:47:55.760
But the thing they actually want is they want lots of data off web pages fast.

00:47:55.760 --> 00:47:57.640
And so then how do you build a business?

00:47:57.880 --> 00:48:09.180
I think it makes perfect sense to say, well, let's build like this infrastructure where you can take the thing you're already doing and just push a button, drop it onto it and deliver the data fast in a reliable way without all the other stuff, right?

00:48:09.180 --> 00:48:10.800
Without adding anything to it.

00:48:10.800 --> 00:48:11.480
Yeah, yeah, yeah.

00:48:11.480 --> 00:48:12.900
Is that kind of the thinking you guys went through?

00:48:12.900 --> 00:48:14.160
That's a perfect example.

00:48:14.160 --> 00:48:21.580
And in fact, we didn't focus early on just pushing a button and getting the data automatically.

00:48:21.580 --> 00:48:28.060
We focused more on the provides engineer hours and consulting and training for our tools.

00:48:28.260 --> 00:48:35.940
But we are very much focused on data marketplaces and automatic APS to do extraction right now.

00:48:35.940 --> 00:48:40.540
But the model is just what you just described is also perfectly possible.

00:48:40.540 --> 00:48:43.560
I mean, it's another way to go about doing that.

00:48:43.560 --> 00:48:58.260
And it's even better if you find a way to productize your open source project in a way that companies can be able to benefit from it without having to go through all the installation and configuration and customization phase.

00:48:58.260 --> 00:49:07.880
So if you find a way to productize your open source project and provide it in a hosted manner, it's another very common example of a few successful ones.

00:49:07.880 --> 00:49:12.320
Like GetSentry, for example, this tool for monitoring websites.

00:49:12.840 --> 00:49:17.960
They provide a nice hosted solution of the open source tool and they're doing awesome.

00:49:17.960 --> 00:49:22.020
Yeah, that's another model that works really well.

00:49:22.020 --> 00:49:41.860
It's in the end a matter of finding where your tool produces value to business and try to sort of connect the dots or try to offer it so that when you're working for your customer, you're implicitly forced to work in your open source project and remain connected to it and growing it.

00:49:42.220 --> 00:49:47.340
You need to find a way to put your open source project in your work agenda.

00:49:47.340 --> 00:49:52.460
Right.

00:49:52.460 --> 00:50:06.660
So if you do consulting, maybe you make sure that your contracts include clauses that if I'm helping you with consulting using my open source project and there's a natural enhancement that I could add to the project, but that would be kind of driven by your need.

00:50:06.660 --> 00:50:10.520
Make that possible for me to put that in my open source project without any conflict.

00:50:10.520 --> 00:50:11.360
Something like that, right?

00:50:11.580 --> 00:50:11.900
Exactly.

00:50:11.900 --> 00:50:14.580
In our cases, we include a clause like that.

00:50:14.580 --> 00:50:20.700
Sometimes we even get Scrappy enhancements sponsored by our customers, which is great.

00:50:20.700 --> 00:50:23.020
You can get that win-win situation.

00:50:23.020 --> 00:50:27.500
You need to be a bit creative, of course, and be patient as well.

00:50:27.700 --> 00:50:38.060
For a long time, at the early stages of Scrappy, I was working on it in my extra time and I wasn't really sure where the project was going or if it was going to be successful at all.

00:50:38.060 --> 00:50:40.200
But I just wanted to build something.

00:50:40.200 --> 00:50:42.720
That was my main motivation back then.

00:50:42.900 --> 00:50:51.860
Create something of value to developers because I was a developer full-time back then and something that made my show a little more fun.

00:50:51.860 --> 00:50:53.520
And that's what I did.

00:50:53.520 --> 00:51:01.520
And then it was later than I realized I had a nice thing going on, a nice open source project that the community has gathered around.

00:51:01.520 --> 00:51:08.400
First, it was just a couple of guys and I tried to help them and answer a few questions here and there.

00:51:08.400 --> 00:51:15.560
And yeah, there was this community that encouraged me to quit my show and focus solely on Scrappy.

00:51:15.560 --> 00:51:16.920
Was that a pretty happy day?

00:51:16.920 --> 00:51:17.340
Yeah.

00:51:17.340 --> 00:51:24.280
At the time, it was a bit crazy, like leaving my show and then just starting this.

00:51:24.280 --> 00:51:32.380
But to be honest, I already had a few customers that were going to become immediate scripting, have customers.

00:51:33.060 --> 00:51:43.080
So it was enough to sustain a small business because we kind of had the market proven before I started Scrappy.

00:51:43.080 --> 00:51:53.800
I know you can do even a lot more risky moves, but I constantly feel proud about the things that Scrappy has achieved.

00:51:54.180 --> 00:51:57.820
And a lot of things that are achieved lately are not thanks to me.

00:51:57.820 --> 00:52:02.360
Like I'm not super involved day to day in the project itself.

00:52:02.360 --> 00:52:05.540
But it's like when you see your kid, right?

00:52:05.540 --> 00:52:11.300
That just graduated from college and is doing great things out there.

00:52:11.300 --> 00:52:13.220
It really makes me very proud.

00:52:13.220 --> 00:52:17.700
And just last month, the first Scrappy book came live.

00:52:18.220 --> 00:52:22.700
And that was a really happy thing for me when I realized.

00:52:22.700 --> 00:52:24.020
I'm sure you're really proud of it.

00:52:24.020 --> 00:52:24.540
That's great.

00:52:24.540 --> 00:52:25.080
Congratulations.

00:52:25.080 --> 00:52:31.120
So the companies I named earlier, possibly with the exception of Continuum, were really large companies.

00:52:31.120 --> 00:52:32.800
But you're not a super large company, right?

00:52:32.800 --> 00:52:34.120
How many people work with you?

00:52:34.120 --> 00:52:41.420
At the moment, we are 130 people working fully distributed around the world, which to me is pretty big.

00:52:41.420 --> 00:52:42.800
But of course, not comfortable.

00:52:42.800 --> 00:52:44.640
Okay, that's bigger than I realized.

00:52:44.640 --> 00:52:45.200
That's awesome.

00:52:45.200 --> 00:52:46.160
Yeah, cool.

00:52:46.160 --> 00:52:50.720
So yeah, we were kind of growing and doubling in size every year, more or less.

00:52:50.720 --> 00:52:56.760
We never kind of set a goal of like, let's double in size or let's grow to this size.

00:52:56.760 --> 00:52:58.320
It kind of just naturally happened.

00:52:58.320 --> 00:53:04.160
Like customers were requesting more work and we keep adding people to the team as there was more work.

00:53:04.160 --> 00:53:10.660
And now we have a large part of the team working on the platform that already drives a lot of demand.

00:53:10.660 --> 00:53:15.460
And running a remote company comes with its challenges, right?

00:53:15.460 --> 00:53:18.420
I mean, you have a lot of times on issues.

00:53:18.420 --> 00:53:21.720
Coordination between teams, like communication is key.

00:53:21.720 --> 00:53:24.600
Self-management is very important.

00:53:25.140 --> 00:53:30.060
But in exchange, you have a global pool of talent available to you.

00:53:30.100 --> 00:53:38.640
And that's been one of the key, if not possibly the most important thing to be able to grow scraping up to the size that we've grown it so far.

00:53:38.640 --> 00:53:43.340
Like we've been focusing really hard on hiding the best talent out there.

00:53:43.340 --> 00:53:47.700
It took us a while, but I'm really proud of the team that we have assembled.

00:53:47.940 --> 00:53:50.980
And I'm sure that the best is still to come.

00:53:50.980 --> 00:54:00.140
I've always kind of worked remotely since I started working professionally in 2000 because I was sort of a sysadmin at the beginning.

00:54:00.740 --> 00:54:03.140
So, yeah, I managed the servers from home.

00:54:03.140 --> 00:54:07.780
Then I worked for this company in the UK that built this aggregator.

00:54:07.780 --> 00:54:09.300
And I was also remote there.

00:54:09.300 --> 00:54:13.480
And it kind of felt natural to continue working this way for me.

00:54:13.580 --> 00:54:21.820
And also, the things that I learned while managing the scrapping community, I tried to apply them as much as possible to manage the scraping hub as well.

00:54:21.820 --> 00:54:31.560
Like the whole culture of not asking for, it's better to ask for forgiveness and permission, those type of things.

00:54:31.560 --> 00:54:33.300
I really applied to the scraping hub.

00:54:33.300 --> 00:54:41.340
Sometimes I try to see it as a really big open source project with some commercial interest that we're trying to manage.

00:54:41.420 --> 00:54:43.180
Yeah, you must be super proud of what you built.

00:54:43.180 --> 00:54:43.620
That's awesome.

00:54:43.620 --> 00:54:50.320
And I think people will really enjoy hearing this sort of success story from the beginning until now.

00:54:50.320 --> 00:54:50.860
That's great.

00:54:50.860 --> 00:54:54.300
So, we just have like a couple minutes for final questions.

00:54:54.300 --> 00:54:56.040
Questions I always see at the end of the show.

00:54:56.040 --> 00:54:58.480
If you're going to write some Python code, what editor do you open?

00:54:58.480 --> 00:55:01.600
Well, I'm a BIM guy.

00:55:01.600 --> 00:55:02.360
What can I say?

00:55:02.360 --> 00:55:04.180
All right.

00:55:04.180 --> 00:55:04.460
Awesome.

00:55:04.460 --> 00:55:06.860
That may be the most common answer.

00:55:06.860 --> 00:55:08.180
I think it probably is.

00:55:08.180 --> 00:55:11.120
I'm quite fond of Sublime.

00:55:11.240 --> 00:55:11.400
Yeah.

00:55:11.400 --> 00:55:14.660
Because I'm not writing much code these days.

00:55:14.660 --> 00:55:17.100
I've tried Sublime a couple of times.

00:55:17.100 --> 00:55:20.240
And it's the first one that made me consider leaving.

00:55:20.240 --> 00:55:21.720
Leaving.

00:55:21.720 --> 00:55:22.360
Leaving.

00:55:22.360 --> 00:55:23.040
But.

00:55:23.040 --> 00:55:23.980
Yeah.

00:55:23.980 --> 00:55:25.840
It hasn't pulled you over to its side yet.

00:55:25.840 --> 00:55:26.420
No.

00:55:26.420 --> 00:55:27.440
Not yet.

00:55:27.440 --> 00:55:27.880
All right.

00:55:27.880 --> 00:55:32.840
And from all the packages on PyPI, there's 70 something thousand these days.

00:55:32.840 --> 00:55:35.760
There's so many that are awesome that maybe people don't know about.

00:55:35.760 --> 00:55:36.640
Like, what would you recommend?

00:55:36.640 --> 00:55:39.060
Like, obviously, pip install Scrappy.

00:55:39.060 --> 00:55:40.860
And then anything else?

00:55:40.860 --> 00:55:43.680
There's a whole bunch of interesting stuff there.

00:55:43.680 --> 00:55:46.940
It's hard to name, to pick one.

00:55:46.940 --> 00:55:51.980
I'll just try to think one that is not part of Scrappy ecosystem.

00:55:51.980 --> 00:55:55.400
Do definitely give a try to Sentry if you're running a website.

00:55:55.740 --> 00:55:58.360
It's really cool stuff to monitor websites.

00:55:58.360 --> 00:55:58.700
Okay.

00:55:58.700 --> 00:55:58.960
Yeah.

00:55:58.960 --> 00:55:59.560
Sentry's nice.

00:55:59.560 --> 00:55:59.920
Yeah.

00:55:59.920 --> 00:56:03.000
That's, I guess I will go with that one.

00:56:03.000 --> 00:56:03.800
Go with Sentry.

00:56:03.800 --> 00:56:04.100
Awesome.

00:56:04.100 --> 00:56:04.460
Okay.

00:56:04.460 --> 00:56:06.920
Before we let you go, any final calls to action?

00:56:06.920 --> 00:56:08.700
Things people should go try?

00:56:09.100 --> 00:56:09.300
Yeah.

00:56:09.300 --> 00:56:09.340
Yeah.

00:56:09.340 --> 00:56:09.780
Definitely.

00:56:09.780 --> 00:56:16.360
Come try Scraping Hub and you won't forget if you have any Scraping needs at any level.

00:56:16.360 --> 00:56:19.920
Like, we are building the most awesome tools to make your shop easier.

00:56:19.920 --> 00:56:25.700
So, yeah, I'm always happy to get in calls with people doing things right with Scraping.

00:56:25.700 --> 00:56:30.860
And we'd like to learn more about what your needs are and how we can get it easier.

00:56:30.860 --> 00:56:31.120
Awesome.

00:56:31.120 --> 00:56:32.500
So, everyone check it out.

00:56:32.500 --> 00:56:34.840
Pablo, it's been really fun to talk about web scraping.

00:56:34.840 --> 00:56:35.980
I've learned a ton from it.

00:56:35.980 --> 00:56:36.340
Thank you.

00:56:36.340 --> 00:56:36.800
Thank you.

00:56:36.800 --> 00:56:37.420
Likewise.

00:56:37.420 --> 00:56:39.320
It's a very interesting job.

00:56:39.320 --> 00:56:39.660
Yeah.

00:56:39.660 --> 00:56:40.360
Thanks for being on the show.

00:56:40.360 --> 00:56:40.760
Thank you.

00:56:40.760 --> 00:56:41.240
Bye-bye.

00:56:41.240 --> 00:56:41.680
Thanks.

00:56:41.680 --> 00:56:41.880
Bye.

00:56:41.880 --> 00:56:45.260
This has been another episode of Talk Python to Me.

00:56:45.260 --> 00:56:46.860
Today's guest was Pablo Hoffman.

00:56:46.860 --> 00:56:49.300
And this episode has been sponsored by Hired and SnapCI.

00:56:49.300 --> 00:56:51.360
Thank you guys for supporting the show.

00:56:51.360 --> 00:56:53.720
Hired wants to help you find your next big thing.

00:56:53.720 --> 00:56:58.180
Visit Hired.com slash Talk Python to Me to get five or more offers with salary and equity

00:56:58.180 --> 00:57:01.720
presented right up front and a special listener signing bonus of $2,000.

00:57:01.720 --> 00:57:05.800
SnapCI is modern, continuous integration, and delivery.

00:57:06.320 --> 00:57:10.080
Build, test, and deploy your code directly from GitHub, all in your browser with debugging,

00:57:10.080 --> 00:57:12.020
Docker, and parallelism included.

00:57:12.020 --> 00:57:15.260
Try them for free at snap.ci slash Talk Python.

00:57:15.260 --> 00:57:18.180
It's the final few days for my video course Kickstarter.

00:57:18.180 --> 00:57:23.920
The campaign is open until March 18th, and you'll find all the details at talkpython.fm slash

00:57:23.920 --> 00:57:24.260
course.

00:57:24.260 --> 00:57:26.280
Hurry on over there and sign up before it closes.

00:57:26.940 --> 00:57:32.400
You can find the links from today's show at talkpython.fm/episode slash show slash 50.

00:57:32.400 --> 00:57:34.420
Be sure to subscribe to the show.

00:57:34.420 --> 00:57:36.400
Open your favorite podcatcher and search for Python.

00:57:36.400 --> 00:57:37.520
We should be right near the top.

00:57:37.520 --> 00:57:41.300
You can also find the iTunes and direct RSS feeds in the footer of the website.

00:57:41.300 --> 00:57:45.600
Our theme music is Developers, Developers, Developers by Corey Smith, who goes by Smix.

00:57:45.980 --> 00:57:48.600
You can hear the entire song on talkpython.fm.

00:57:48.600 --> 00:57:50.820
This is your host, Michael Kennedy.

00:57:50.820 --> 00:57:52.420
Thank you so much for listening.

00:57:52.420 --> 00:57:54.460
Smix takes us out of here.

00:57:55.980 --> 00:58:02.980
Thank you.

00:58:02.980 --> 00:58:03.980
We'll be right back.

00:58:03.980 --> 00:58:14.980
We'll be right back.

00:58:14.980 --> 00:58:15.480
you

00:58:15.480 --> 00:58:15.980
Bye.

