WEBVTT

00:00:00.001 --> 00:00:03.300
Do you run any code that listens on an open port on the internet?

00:00:03.300 --> 00:00:08.320
This could be a website, a RESTful web service, or GASP even, a database endpoint.

00:00:08.320 --> 00:00:14.860
Troy Hunt, a renowned security expert, likes to say that you're doing free penetration testing for that product right there.

00:00:14.860 --> 00:00:17.620
Join Troy and me on this episode of Talk Python to Me.

00:00:17.620 --> 00:00:22.160
We discuss lessons learned from running the vulnerability monitoring website Have I Been Pwned,

00:00:22.160 --> 00:00:26.620
as well as other lessons for developers to keep your code safe while providing public services.

00:00:26.840 --> 00:00:32.080
This is episode 136, recorded October 26, 2017.

00:00:32.080 --> 00:00:52.780
Welcome to Talk Python to Me, a weekly podcast on Python, the language, the libraries, the ecosystem, and the personalities.

00:00:52.780 --> 00:00:54.920
This is your host, Michael Kennedy.

00:00:55.180 --> 00:00:56.920
Follow me on Twitter, where I'm @mkennedy.

00:00:56.920 --> 00:01:03.320
Keep up with the show and listen to past episodes at talkpython.fm, and follow the show on Twitter via at Talk Python.

00:01:03.320 --> 00:01:07.440
This episode has been sponsored by Rollbar and GoCD.

00:01:07.440 --> 00:01:13.140
Thank them both for supporting the podcast by checking out what they're offering during their segments.

00:01:13.140 --> 00:01:15.380
Troy, welcome to Talk Python.

00:01:15.380 --> 00:01:16.860
Hey, thank you very much for having me.

00:01:16.860 --> 00:01:19.520
Yeah, it's really great to have you on as a guest.

00:01:19.520 --> 00:01:28.420
I've respected the work you've done in the security space immensely, and I'm looking forward to sharing what you've learned about software security and developers with the audience.

00:01:28.420 --> 00:01:29.920
Cool. Awesome. Let's do it.

00:01:29.920 --> 00:01:30.560
Yeah, let's do it.

00:01:30.560 --> 00:01:34.460
Now, before we get into all the details, let's start with how you got into programming in the first place.

00:01:34.460 --> 00:01:34.960
What's your story?

00:01:34.960 --> 00:01:36.020
That's a good question.

00:01:36.220 --> 00:01:43.220
So I was a little bit anti-computer when I was a kid, probably up until about teenage years.

00:01:43.220 --> 00:01:46.860
And I was actually very frustrated by my friends who'd be inside on the computer.

00:01:46.860 --> 00:01:47.720
I'm like, come on, man.

00:01:47.720 --> 00:01:50.260
I want to go outside and play football or something like that.

00:01:50.260 --> 00:01:51.480
What are you nerds doing?

00:01:52.040 --> 00:01:57.600
And then I guess I got into it by, I moved overseas when I was a kid.

00:01:57.600 --> 00:01:59.320
So this was when I was almost 14.

00:01:59.320 --> 00:02:01.340
My family moved to the Netherlands.

00:02:01.340 --> 00:02:04.300
And it's kind of cold over there.

00:02:04.300 --> 00:02:05.920
And like you get into it.

00:02:05.920 --> 00:02:06.820
A lot of dark evenings.

00:02:06.820 --> 00:02:08.460
The sun goes down to what, like 2.30?

00:02:08.460 --> 00:02:10.280
Yeah, well, this is a little later, man.

00:02:10.280 --> 00:02:10.720
No, just kidding.

00:02:10.720 --> 00:02:11.840
Yeah, yeah.

00:02:11.840 --> 00:02:13.820
But it gets, it's pretty dark in the winter there, right?

00:02:13.820 --> 00:02:14.460
It is.

00:02:14.460 --> 00:02:17.180
I mean, for folks in the UK, it's basically like the UK.

00:02:17.180 --> 00:02:20.060
But, well, I won't say something derogatory about the UK.

00:02:20.060 --> 00:02:21.260
You can read my Twitter feed for that.

00:02:21.740 --> 00:02:22.620
No, we love them, honestly.

00:02:22.620 --> 00:02:25.160
So anyway, there's a lot of time spent indoors.

00:02:25.160 --> 00:02:26.480
And I just started getting involved.

00:02:26.480 --> 00:02:29.080
I think I must have started doing a bit of basic back then.

00:02:29.080 --> 00:02:31.900
So this would have been sort of early 90s as well.

00:02:31.900 --> 00:02:37.400
And I think really for me, though, so I was doing a bunch of sort of PC-related stuff in

00:02:37.400 --> 00:02:41.240
my teenage years and doing part-time jobs at PC repair stores and that sort of thing.

00:02:41.240 --> 00:02:44.580
But the thing that really hooked me was seeing the web.

00:02:44.580 --> 00:02:48.400
So I started using the web in 95 when I started uni.

00:02:48.400 --> 00:02:50.600
And it was just like, wow, this is awesome.

00:02:50.600 --> 00:02:51.140
This is amazing.

00:02:51.200 --> 00:02:52.180
I want to build stuff for this.

00:02:52.180 --> 00:02:56.940
And really, that was the start of modern day life as I know it in terms of what I do.

00:02:56.940 --> 00:02:57.620
That's awesome.

00:02:57.620 --> 00:02:59.100
Yeah, I had the same experience.

00:02:59.100 --> 00:03:04.140
I was a sophomore or junior at university when the Mozilla?

00:03:04.140 --> 00:03:06.160
What was the first before Netscape?

00:03:06.160 --> 00:03:07.100
Before Netscape?

00:03:07.100 --> 00:03:07.660
Yeah.

00:03:07.660 --> 00:03:08.620
Something very real.

00:03:08.620 --> 00:03:08.840
Mosaic.

00:03:08.840 --> 00:03:10.000
Oh, Mosaic.

00:03:10.000 --> 00:03:10.400
Yeah.

00:03:10.820 --> 00:03:11.160
Yes.

00:03:11.160 --> 00:03:12.440
When Mosaic came out.

00:03:12.440 --> 00:03:14.960
And I was just like, oh, my gosh.

00:03:14.960 --> 00:03:17.080
Like, the entire world opened up.

00:03:17.080 --> 00:03:18.680
It was such an amazing time.

00:03:18.680 --> 00:03:22.300
And it seemed like it's funny because technology was so much more limited.

00:03:22.300 --> 00:03:24.760
And what you can do on the web now is way more impressive.

00:03:24.760 --> 00:03:27.740
But the world seemed so open back then, right?

00:03:27.740 --> 00:03:28.660
Yeah, yeah.

00:03:28.660 --> 00:03:31.460
I don't want to reminisce in so far as, oh, so awesome, McMahon.

00:03:31.460 --> 00:03:32.980
Because frankly, it's so awesome now.

00:03:32.980 --> 00:03:35.420
And it's really a lot more awesome by any reasonable measure.

00:03:35.420 --> 00:03:36.020
Yeah, for sure.

00:03:36.220 --> 00:03:41.380
But it was just an exciting time where we're sort of seeing stuff that didn't even resemble

00:03:41.380 --> 00:03:42.620
the things that we'd done before.

00:03:42.620 --> 00:03:43.860
Yeah, I had the same feel.

00:03:43.860 --> 00:03:44.500
And that was awesome.

00:03:44.500 --> 00:03:47.020
So now, what do you do day to day?

00:03:47.020 --> 00:03:48.440
More stuff on the web?

00:03:48.440 --> 00:03:49.420
More stuff with security?

00:03:49.420 --> 00:03:50.200
Yeah.

00:03:50.200 --> 00:03:52.140
So look, it's a bit of both.

00:03:52.140 --> 00:03:52.800
It's funny.

00:03:52.800 --> 00:03:55.960
In fact, I've been thinking the last couple of days because I've had, I don't want to say

00:03:55.960 --> 00:03:57.900
downtime, but I haven't been rushing somewhere.

00:03:57.900 --> 00:04:00.560
And I was sort of going, what am I going to do today?

00:04:00.560 --> 00:04:05.680
Like, I'm entirely independent, so I can do pretty much whatever I want.

00:04:05.980 --> 00:04:08.700
And that consists of sort of several main things.

00:04:08.700 --> 00:04:11.660
So I still do a lot of traveling and speaking at conferences.

00:04:11.660 --> 00:04:15.580
I'm trying to do less of the traveling just because of how much I've been away.

00:04:15.580 --> 00:04:17.540
But I'm still doing a lot of that.

00:04:17.540 --> 00:04:19.020
I'm still doing a lot of workshops.

00:04:19.020 --> 00:04:22.320
I was down interstate last week doing a workshop.

00:04:22.320 --> 00:04:24.260
I think I've probably done this same workshop.

00:04:24.260 --> 00:04:26.540
I must have done it 25 times this year.

00:04:26.540 --> 00:04:28.200
25 two-day events.

00:04:28.200 --> 00:04:30.620
The workshop is Hack Yourself First.

00:04:31.160 --> 00:04:37.280
So I usually go into, say last week it was into a large financial institution and then

00:04:37.280 --> 00:04:38.980
another online retailer.

00:04:38.980 --> 00:04:44.960
And I'll go in there for two days and I'll sit down with usually developers, but also regularly

00:04:44.960 --> 00:04:51.120
project management or managers, QA people, DBA, security folks, and go, okay, let's spend

00:04:51.120 --> 00:04:56.020
two days figuring out the mechanics of things like SQL injection and cross-site scripting.

00:04:56.180 --> 00:04:57.100
Here's how they work.

00:04:57.100 --> 00:04:58.840
Here's what you do to be resilient to them.

00:04:58.840 --> 00:05:02.340
And it's very, very sort of hands-on, active sort of stuff.

00:05:02.340 --> 00:05:06.740
And it's just like, it's a huge amount of fun for everyone because we get to go in and break

00:05:06.740 --> 00:05:08.780
stuff in ways that they've never been able to before.

00:05:08.780 --> 00:05:12.900
And everyone leaves with this sort of new appreciation of the mechanics of how these

00:05:12.900 --> 00:05:13.780
attacks work.

00:05:14.240 --> 00:05:17.260
And they end up building better software for it, which is a good win.

00:05:17.260 --> 00:05:18.540
Yeah, it's definitely a great win.

00:05:18.540 --> 00:05:24.180
And I think sometimes you just have to see these things in action to really appreciate how your

00:05:24.180 --> 00:05:27.140
security and the code you're writing is not actually secure.

00:05:27.140 --> 00:05:32.440
I mean, even something as sort of old school as SQL injection attacks, like when you first

00:05:32.440 --> 00:05:35.000
start writing SQL strings, you're like, well, I need to put the variable here.

00:05:35.000 --> 00:05:38.640
So plus, plus quote thing, you know, how could that be wrong?

00:05:38.640 --> 00:05:38.860
Right?

00:05:38.860 --> 00:05:39.360
It just works.

00:05:39.360 --> 00:05:43.020
But I think totally seeing it go, oh my gosh, do we do that?

00:05:43.020 --> 00:05:45.740
Because if we do that, I just see how horrible this is going to be.

00:05:45.740 --> 00:05:46.220
That's it.

00:05:46.220 --> 00:05:47.500
It's the hands-on, you know.

00:05:47.500 --> 00:05:50.080
It's like the light bulb moments when people do that.

00:05:50.080 --> 00:05:54.240
And SQL injection is a great example because, you know, like you said, it's something that's

00:05:54.240 --> 00:05:55.520
been around for a long time.

00:05:55.520 --> 00:05:57.140
And we sort of think of this as an old thing.

00:05:57.140 --> 00:06:01.420
It's still number one in the OWASP top 10 most critical web app security risks.

00:06:01.420 --> 00:06:06.260
You know, it's still up there, even in their 2017 revised edition, it's still number one.

00:06:06.260 --> 00:06:12.380
And when I do the workshop and I demonstrate SQL injection, I show things like there's a blog post

00:06:12.380 --> 00:06:16.640
I refer to written by a guy who's trying to teach people how to do password resets in

00:06:16.640 --> 00:06:17.300
ASP.net.

00:06:17.300 --> 00:06:23.440
And literally within the one screen, he's got good, resilient, parameterized SQL statements.

00:06:23.440 --> 00:06:25.560
And we're looking at this going, is this okay?

00:06:25.560 --> 00:06:26.780
Yeah, yeah, it's okay.

00:06:26.780 --> 00:06:27.960
Because you've got parameterization.

00:06:27.960 --> 00:06:30.620
And then like the next line, there's an update statement.

00:06:30.620 --> 00:06:35.620
And it's just like inline concatenated SQL with untrusted data that you can totally own.

00:06:35.620 --> 00:06:37.880
And then he connects to the database with a privileged account.

00:06:37.880 --> 00:06:39.100
And it's like, yep, good night.

00:06:39.100 --> 00:06:39.900
Like the whole thing's over.

00:06:39.900 --> 00:06:43.900
And there's still material, new material out there teaching people that.

00:06:43.900 --> 00:06:45.440
And here's another fun one.

00:06:45.440 --> 00:06:48.360
You can go and do, and be careful with this, folks, if you go and do this.

00:06:48.360 --> 00:06:51.500
You can go to Google and do a Google Doc search.

00:06:51.800 --> 00:06:55.160
So one of these sort of searches that turn up things that aren't really meant to be there.

00:06:55.160 --> 00:07:00.080
And you can do a search for inurl.php?id equals.

00:07:00.080 --> 00:07:05.840
And I think about the first half a dozen results have got an integer somewhere in a query string.

00:07:05.840 --> 00:07:07.240
You load the page up.

00:07:07.240 --> 00:07:08.600
You put an apostrophe on the end of it.

00:07:08.600 --> 00:07:10.460
And bang, there's an internal SQL exception.

00:07:10.460 --> 00:07:14.100
So very, very high likelihood of having a SQL injection risk.

00:07:14.100 --> 00:07:16.940
And this sort of stuff is just absolutely rampant.

00:07:17.080 --> 00:07:18.060
Yeah, that's really scary.

00:07:18.060 --> 00:07:21.300
Why do you think it still is a problem?

00:07:21.300 --> 00:07:23.040
I mean, it's 2017.

00:07:23.040 --> 00:07:28.360
We've got ORMs and ODMs that generally protect us against these things.

00:07:28.360 --> 00:07:31.540
We've got an XKCD, Little Bobby Table.

00:07:31.540 --> 00:07:32.140
I know.

00:07:32.140 --> 00:07:34.860
If that doesn't teach people the lesson, I don't know what will.

00:07:34.860 --> 00:07:35.920
We've had it for years.

00:07:35.920 --> 00:07:37.560
I got him on a T-shirt, actually.

00:07:37.560 --> 00:07:38.480
I used to wear it at conferences.

00:07:38.480 --> 00:07:39.140
People love it.

00:07:39.140 --> 00:07:39.960
Oh, that's wonderful.

00:07:39.960 --> 00:07:41.580
I'll have to put a link to that in the show notes.

00:07:41.580 --> 00:07:44.420
So why does this all happen?

00:07:44.420 --> 00:07:46.340
I think there are multiple factors.

00:07:46.560 --> 00:07:48.440
So, you know, I just gave one example.

00:07:48.440 --> 00:07:51.000
People are still creating training material that's vulnerable.

00:07:51.000 --> 00:07:55.520
And in fact, one of the reasons why I show this blog post is that I actually left the

00:07:55.520 --> 00:07:56.940
guy a really nice comment.

00:07:56.940 --> 00:07:59.300
And I literally said, here's some friendly feedback.

00:07:59.300 --> 00:08:02.040
And being very constructive, I gave him this feedback.

00:08:02.040 --> 00:08:03.880
It's just like the comment sitting there.

00:08:03.880 --> 00:08:05.100
The guy's never replied.

00:08:05.100 --> 00:08:08.780
And then after that, there's been a whole bunch of people that have chimed in and said,

00:08:08.780 --> 00:08:09.180
thank you.

00:08:09.180 --> 00:08:09.960
This is very useful.

00:08:10.260 --> 00:08:14.700
And I'm going, didn't you just read like the big, long, friendly comment from the Australian

00:08:14.700 --> 00:08:17.180
guy saying like, this is really just full of holes.

00:08:17.180 --> 00:08:17.860
Don't do this.

00:08:17.860 --> 00:08:21.460
So we see this propagate over and over and over again.

00:08:22.220 --> 00:08:28.140
And ultimately, so many of these risks just fundamentally boil down to the people building the software

00:08:28.140 --> 00:08:31.140
and not being familiar with how something like SQL injection works.

00:08:31.140 --> 00:08:33.320
And that is just purely a competency issue.

00:08:33.320 --> 00:08:33.620
Yeah.

00:08:33.720 --> 00:08:37.380
I guess that part of it is like you kind of hinted at the copy and paste stack overflow

00:08:37.380 --> 00:08:38.680
type of thing.

00:08:38.680 --> 00:08:41.540
Although I suspect it would take a pretty good beating on stack overflow.

00:08:41.540 --> 00:08:44.260
But you know, this copy and paste sort of thing.

00:08:44.260 --> 00:08:50.780
And we have so, our technology and languages and libraries we use changes so quickly that

00:08:50.780 --> 00:08:54.200
people are, I think a lot of people are just scrambling to keep up to make things work,

00:08:54.200 --> 00:08:55.160
much less be secure.

00:08:55.160 --> 00:08:57.560
So that the copy and paste thing is funny.

00:08:57.560 --> 00:09:02.100
In January last year, I was running the workshop we just spoke about in Norway.

00:09:02.760 --> 00:09:06.800
And part of this workshop, there's a module on looking at mobile APIs.

00:09:06.800 --> 00:09:12.480
And one of the guys in my workshop says, all right, look, I want to look at the way the

00:09:12.480 --> 00:09:17.360
app for my Nissan Leaf, or in the US you'd say Nissan, the car.

00:09:17.360 --> 00:09:19.980
So the way my car.

00:09:19.980 --> 00:09:20.820
Yeah.

00:09:20.820 --> 00:09:21.480
So, okay.

00:09:21.480 --> 00:09:23.040
How does my app talk to my car?

00:09:23.040 --> 00:09:27.380
Because he could control features of the car from the app.

00:09:27.380 --> 00:09:31.220
Now, this is not something we have a problem with in Australia, but apparently in some parts

00:09:31.220 --> 00:09:35.340
of the world, it is so cold, you've got to turn your car on before you get in the car,

00:09:35.340 --> 00:09:36.560
you know, like turn the heating on.

00:09:36.560 --> 00:09:39.960
And this is one of the things I have to do in Norway because it's so freaking cold there.

00:09:39.960 --> 00:09:42.760
So he's like, all right, pulls out his app and he's figuring it out.

00:09:42.760 --> 00:09:48.040
And what he discovers is that the only thing that the app needs to know about the car is

00:09:48.040 --> 00:09:49.080
the car's VIN number.

00:09:49.720 --> 00:09:53.740
Now, for folks who may not be familiar with what a VIN number is, first of all, this was

00:09:53.740 --> 00:09:55.300
being used like an API key.

00:09:55.300 --> 00:09:55.920
It was a secret.

00:09:55.920 --> 00:09:58.540
Second of all, it was printed in the windscreen of every car.

00:09:58.540 --> 00:10:03.140
So you could literally walk past a car and it had its API key in the windscreen.

00:10:03.140 --> 00:10:05.960
And it was worse than that too, because they're innumerable.

00:10:05.960 --> 00:10:10.160
So you can just take the, in this case, I think we could take about the last five digits and

00:10:10.160 --> 00:10:12.480
just keep randomizing numbers and finding different cars.

00:10:13.000 --> 00:10:18.240
And based on what he found in the space of literally sort of single digit minutes, we

00:10:18.240 --> 00:10:21.380
discovered that you could control our climate control features of the car.

00:10:21.380 --> 00:10:24.100
You could pull back trip history, battery status, all sorts of things.

00:10:24.100 --> 00:10:24.860
It was just crazy.

00:10:24.860 --> 00:10:25.400
Shouldn't have happened.

00:10:25.400 --> 00:10:25.920
That's crazy.

00:10:25.920 --> 00:10:29.400
And if there's any vulnerabilities, you can, you're like, you can own the car basically,

00:10:29.400 --> 00:10:29.740
right?

00:10:29.740 --> 00:10:30.980
Because you're talking to it.

00:10:30.980 --> 00:10:32.140
Yeah, God knows.

00:10:32.140 --> 00:10:32.860
You know what I mean?

00:10:32.860 --> 00:10:35.140
And when we say you're talking to the car, let's be clear.

00:10:35.140 --> 00:10:39.340
You're talking to, you're basically talking to a web server and not a web server in a car.

00:10:39.340 --> 00:10:41.460
We can come back to the things people put web servers in later.

00:10:41.900 --> 00:10:44.820
You're talking to a web server on the web running an API.

00:10:44.820 --> 00:10:49.540
And then that goes back end over some proprietary GSM network or something to the vehicle itself.

00:10:49.540 --> 00:10:51.620
So there is a proxy in between.

00:10:51.620 --> 00:10:53.260
Yeah, that's not too bad then, I guess.

00:10:53.260 --> 00:10:55.400
Well, so here's where it kind of went worse.

00:10:55.400 --> 00:11:00.220
I disclosed it to Nissan and we had chats on the phone and, you know, they were like, yeah,

00:11:00.220 --> 00:11:01.540
we probably should fix this.

00:11:01.540 --> 00:11:02.680
Yes, you probably should fix this.

00:11:02.680 --> 00:11:05.440
And then time goes by and they're not fixing it.

00:11:05.440 --> 00:11:10.160
And eventually we get to like a month after disclosure and they've stopped replying to messages

00:11:10.160 --> 00:11:11.080
and nothing's happening.

00:11:11.600 --> 00:11:14.600
So I write about it and then suddenly they decide it's important.

00:11:14.600 --> 00:11:16.220
So they take the whole thing offline.

00:11:16.600 --> 00:11:21.960
And it's offline for about six weeks and eventually it comes back online and they've got a new app and everything.

00:11:21.960 --> 00:11:26.240
The funny thing about the app, and this is the relevancy to Stack Overflow and code reuse,

00:11:26.240 --> 00:11:31.020
is that down the bottom of one of the screens there's this really odd text.

00:11:31.020 --> 00:11:33.400
And the text is something like,

00:11:33.480 --> 00:11:36.060
the spirit of Stack Overflow is developers helping developers.

00:11:36.060 --> 00:11:38.500
And this isn't an app for your car.

00:11:38.700 --> 00:11:41.400
And we're looking at this going, what are you doing?

00:11:41.400 --> 00:11:42.600
Why would you put that in there?

00:11:42.600 --> 00:11:50.240
And then, of course, we found the Stack Overflow post where they've literally copied and pasted the text from the Stack Overflow post without understanding what it does.

00:11:50.240 --> 00:11:52.660
Put it in the app that controls features of your car.

00:11:52.660 --> 00:11:53.460
That's incredible.

00:11:53.700 --> 00:11:57.220
Honestly, folks, if you Google for this, Google like,

00:11:57.220 --> 00:12:00.720
Nissan Stack Overflow code reuse, and it's just like, this is insane.

00:12:00.720 --> 00:12:01.620
How does this happen?

00:12:01.620 --> 00:12:06.580
I mean, how does this happen at all, let alone in a car or software to control a car?

00:12:06.700 --> 00:12:18.260
Yeah, I feel like the companies that have these types of IoT-like things that are really valuable, you know, not a light bulb, but cars and humans travel in them at high dangerous speeds.

00:12:18.260 --> 00:12:19.920
They should really be careful about this, right?

00:12:19.920 --> 00:12:20.960
Yeah, they probably should.

00:12:20.960 --> 00:12:22.060
They probably should.

00:12:22.060 --> 00:12:25.500
Maybe you should send them an email about that or a bunch.

00:12:25.500 --> 00:12:30.100
Yeah, well, afterwards, they were like, if you find any other stuff, please do send us an email.

00:12:30.100 --> 00:12:30.940
I sent you an email.

00:12:30.940 --> 00:12:32.260
We had phone conversations.

00:12:32.260 --> 00:12:33.200
You were endorsed.

00:12:33.200 --> 00:12:33.840
What happened?

00:12:33.840 --> 00:12:34.700
Anyway.

00:12:34.700 --> 00:12:35.260
Wow.

00:12:36.100 --> 00:12:39.500
So, do you know if any of these car companies run bug bounty type things?

00:12:39.500 --> 00:12:40.500
Yeah, no, Tesla does.

00:12:40.500 --> 00:12:43.980
In fact, Tesla runs a bug bounty through Bug Crowd.

00:12:43.980 --> 00:12:54.280
Bug Crowd is a bug bounty as a service platform started by a mate of mine who's done extremely well moving over to the U.S. and getting funded and doing wonderful things.

00:12:54.280 --> 00:12:57.260
And bug bounties are becoming absolutely massive now.

00:12:57.260 --> 00:13:02.040
It's great to see him doing well, but it's great to see this being a big thing in the industry now as well.

00:13:02.040 --> 00:13:04.100
Yeah, I totally think it's a very positive thing.

00:13:04.180 --> 00:13:08.240
I suspect most people listening to know what bug bounties are, but maybe just define it for everyone.

00:13:08.240 --> 00:13:17.800
Yeah, so a bug bounty is effectively acknowledging that maybe the right way to put this is everyone who has anything online is continually getting free penetration tests.

00:13:18.520 --> 00:13:21.240
So, there are always people out there probing away at your things.

00:13:21.240 --> 00:13:31.640
And a bug bounty is a means of saying to people, look, if you find vulnerabilities in our software, if you find things that could be dangerous, like SQL injection, submit them here.

00:13:31.640 --> 00:13:34.160
And there's usually a formal process.

00:13:34.160 --> 00:13:36.280
You know, this is the email address to send it to.

00:13:36.280 --> 00:13:37.340
This is the information we need.

00:13:37.340 --> 00:13:38.780
Here's how to encrypt your communications.

00:13:39.580 --> 00:13:42.340
And then, depending on the vulnerability, you may be incentivized.

00:13:42.340 --> 00:13:45.380
So, you may actually get anything from a T-shirt to a large amount of money.

00:13:45.380 --> 00:13:51.220
And this is a really neat way of recognizing that we do have flaws in software.

00:13:51.220 --> 00:13:52.520
This is the nature of building software.

00:13:53.060 --> 00:13:55.920
And that if someone finds it, it is actually worth something.

00:13:55.920 --> 00:14:07.900
And the sort of value proposition of the bug bounty is that it incentivizes people to report these things responsibly and allows the organization to handle them and fix them before someone actually goes and exploits nasty things.

00:14:07.900 --> 00:14:11.920
And obviously, there's some incentivization for that in a monetary sense.

00:14:11.920 --> 00:14:13.460
And these are becoming really big.

00:14:13.460 --> 00:14:14.620
So, you know, we mentioned Tesla.

00:14:14.620 --> 00:14:16.620
The Pentagon has run a bug bounty.

00:14:16.620 --> 00:14:18.840
Like, these are going really, really mainstream.

00:14:19.500 --> 00:14:24.840
And there's a lot more to it than just some random hackers sitting in a basement on the other side of the world trying to break into your things as well.

00:14:24.840 --> 00:14:28.900
They can be very, very carefully managed programs with well-selected testers.

00:14:28.900 --> 00:14:29.140
Yeah.

00:14:29.140 --> 00:14:33.600
Like, the Pentagon one, you had to kind of interview and be approved to be part of it.

00:14:33.600 --> 00:14:36.520
It wasn't just, now everybody go forth and attack the site, right?

00:14:36.520 --> 00:14:40.580
You couldn't be Australian either or anything else that wasn't American, as I understand it.

00:14:40.580 --> 00:14:42.040
That's unfortunate.

00:14:42.040 --> 00:14:44.760
Well, you know, like, it's the freaking Pentagon.

00:14:44.760 --> 00:14:46.260
Like, I kind of get that.

00:14:46.260 --> 00:14:57.040
And frankly, just the fact that they ran that and it became such a more mainstream thing that entered into people's psyches in places it just wasn't before, I think is a very positive thing.

00:14:57.040 --> 00:14:57.960
Yeah, I totally agree.

00:14:57.960 --> 00:14:58.600
It's a positive thing.

00:14:58.600 --> 00:14:59.940
And this money can be pretty large.

00:14:59.940 --> 00:15:02.440
Like you said, it could be a t-shirt, but it could be $100,000.

00:15:02.440 --> 00:15:04.960
And this incentive has been there recently.

00:15:04.960 --> 00:15:11.500
Like, if there's not a bug bounty, there's probably some other bad actor who's willing to pay $50,000 for a good O-Day, right?

00:15:11.580 --> 00:15:12.120
That's the thing.

00:15:12.120 --> 00:15:16.960
And then we sort of get into this interesting space of who is competing for the dollars of the bugs.

00:15:16.960 --> 00:15:25.720
And look, some people will argue that there are still actors out there that will pay a lot more than what the organizations with the potential vulnerability will.

00:15:25.720 --> 00:15:31.620
But then at least you sort of get to have a little bit of weight on the other side of the scales in a monetary sense.

00:15:31.620 --> 00:15:41.040
And of course, from a legal and an ethical perspective as well, there's always the incentive to try and report things through the formal channels and get money.

00:15:41.040 --> 00:15:45.040
But yeah, look, there'll always be nefarious parties willing to pay for this stuff as well.

00:15:45.040 --> 00:15:50.560
Yeah, at least now there's some option to monetize that and make it part of your living and do the right thing.

00:15:50.560 --> 00:15:51.020
Exactly.

00:15:51.300 --> 00:15:51.460
Yeah.

00:15:51.460 --> 00:15:55.940
This portion of Talk Python to Me has been brought to you by Rollbar.

00:15:55.940 --> 00:15:59.620
One of the frustrating things about being a developer is dealing with errors.

00:15:59.620 --> 00:16:08.640
Relying on users to report errors, digging through log files, trying to debug issues, or getting millions of alerts just flooding your inbox and ruining your day.

00:16:08.640 --> 00:16:15.400
With Rollbar's full-stack error monitoring, you get the context, insight, and control you need to find and fix bugs faster.

00:16:15.400 --> 00:16:19.380
Adding Rollbar to your Python app is as easy as pip install Rollbar.

00:16:19.700 --> 00:16:23.620
You can start tracking production errors and deployments in eight minutes or less.

00:16:23.620 --> 00:16:27.800
Are you considering self-hosting tools for security or compliance reasons?

00:16:27.800 --> 00:16:31.380
Then you should really check out Rollbar's compliant SaaS option.

00:16:31.380 --> 00:16:40.740
Get advanced security features and meet compliance without the hassle of self-hosting, including HIPAA, ISO 27001, Privacy Shield, and more.

00:16:40.740 --> 00:16:42.120
They'd love to give you a demo.

00:16:42.120 --> 00:16:43.820
Give Rollbar a try today.

00:16:43.820 --> 00:16:47.500
Go to talkpython.fm/Rollbar and check them out.

00:16:48.500 --> 00:17:02.960
Speaking of looking at breaches and disclosure, you're running a pretty amazing website called Have I Been Pwned that has really grown in terms of awareness for when these breaches happen, right?

00:17:02.960 --> 00:17:04.360
Tell everyone about Have I Been Pwned.

00:17:04.360 --> 00:17:04.780
Yeah.

00:17:04.780 --> 00:17:08.020
Well, Have I Been Pwned has been running almost for four years now.

00:17:08.060 --> 00:17:14.300
I've got to think about a birthday thing to do, actually, because it will be, I think, late November, early December will be the four-year anniversary.

00:17:14.300 --> 00:17:18.380
And I started that after the Adobe data breach.

00:17:18.380 --> 00:17:21.500
So Adobe was about 150 million records from memory.

00:17:21.900 --> 00:17:28.280
And at that time, I'd been doing some analysis across sort of different data breaches, you know, looking for patterns.

00:17:28.280 --> 00:17:29.940
Are people using the same password?

00:17:29.940 --> 00:17:30.520
Well, guess what?

00:17:30.520 --> 00:17:31.000
Yeah, they are.

00:17:31.000 --> 00:17:34.780
Are people appearing in multiple different incidents, you know, like the same person?

00:17:34.780 --> 00:17:36.360
And I was sort of seeing stuff.

00:17:36.360 --> 00:17:38.600
I thought, like, I find this really interesting.

00:17:38.920 --> 00:17:47.000
I reckon other people would find it interesting if they could see where they are exposed as well and particularly see things like, you know, you're exposed in these multiple places.

00:17:47.000 --> 00:17:51.000
So that was sort of the genesis for the project.

00:17:51.000 --> 00:17:56.780
And I built that out with my 100 and I think it was a total of about 155 million records.

00:17:56.780 --> 00:18:00.020
So it was basically just about all Adobe when I launched it.

00:18:00.020 --> 00:18:01.420
And I was like, wow, this is massive.

00:18:01.420 --> 00:18:05.880
And I was actually, I built it all out on Microsoft's Azure platform as well.

00:18:05.880 --> 00:18:08.860
And I really wanted to spend time using Azure in anger.

00:18:08.860 --> 00:18:13.040
So, you know, how can I do something that actually uses a lot of storage?

00:18:13.040 --> 00:18:20.100
And how can I use things like the table storage construct instead of relational databases to save money and make it go faster and all the rest of it?

00:18:20.100 --> 00:18:23.020
And that was sort of a bit of a hobby project too.

00:18:23.020 --> 00:18:31.100
And now sort of fast forward to nearly four years later and there's 4.8 billion accounts in there, which I still find to be a bit of an unfathomable number.

00:18:31.100 --> 00:18:33.180
That's really incredible.

00:18:33.180 --> 00:18:34.540
Incredible.

00:18:34.540 --> 00:18:36.620
That's more than half the population of the earth.

00:18:36.620 --> 00:18:41.140
I mean, I realize it's not one-to-one person to account, but still, that's staggering.

00:18:41.140 --> 00:18:41.980
It is crazy.

00:18:41.980 --> 00:18:47.560
And there's just sort of all sorts of metrics about the project that just exceeded any sort of form of expectation.

00:18:47.560 --> 00:18:48.600
So that's one of them.

00:18:48.600 --> 00:18:54.200
The visitor stats, an average day is somewhere between 50,000 and 70,000 people come to the site.

00:18:54.200 --> 00:18:56.260
A big day is seven figures.

00:18:56.260 --> 00:18:59.880
So I think I had about 2.8 million in one day the other day.

00:18:59.880 --> 00:19:01.740
What was the driving factor behind that?

00:19:01.740 --> 00:19:04.100
That's a good question because there's always a trigger, right?

00:19:04.100 --> 00:19:08.380
So it's to sort of deviate from that baseline by an order of multiples.

00:19:08.740 --> 00:19:11.220
In that case, there was this spam bot.

00:19:11.220 --> 00:19:12.980
So it was called the online spam bot.

00:19:12.980 --> 00:19:26.800
And a security researcher in France discovered this spam bot where, unfortunately, or fortunately, depending on your perspective, whoever was running the spam bot didn't do a great job of actually securing their data and left 711 million email addresses exposed.

00:19:27.380 --> 00:19:31.240
So he managed to grab all this data and sent it over to me.

00:19:31.240 --> 00:19:34.380
And I went, OK, well, let's load 711 million records in here.

00:19:34.380 --> 00:19:40.560
It's not a breach in the traditional sense, but it is data about individuals redistributed over the Internet.

00:19:40.560 --> 00:19:41.980
Let's load this.

00:19:42.120 --> 00:19:44.780
And of course, a huge number of people were then interested in it.

00:19:44.780 --> 00:19:47.060
And that sort of made a lot of news headlines.

00:19:47.060 --> 00:19:49.380
And this is the thing that really drives the traffic.

00:19:49.380 --> 00:19:59.480
It's news headlines because every time there's a data breach, I see a spike because it's on CNN or the BBC or something that faces the masses and not just the tech audiences.

00:20:00.040 --> 00:20:05.440
So it's sort of really interesting to see how broadly appealing the project has been.

00:20:05.440 --> 00:20:06.860
Yeah, I think it's a great service.

00:20:06.860 --> 00:20:13.800
And I've gotten probably five or six emails from your service saying you've appeared in some breach or other.

00:20:13.800 --> 00:20:21.360
Usually it doesn't freak me out too much because I use one password and my passwords are 40 characters long and they're unique per site.

00:20:21.360 --> 00:20:24.340
So I hear a lot from people saying they hate getting email from me.

00:20:24.340 --> 00:20:27.760
So I apologize to everyone that's had an email from me.

00:20:27.760 --> 00:20:29.440
Don't hate the messenger, right?

00:20:29.440 --> 00:20:29.900
Come on.

00:20:29.900 --> 00:20:31.080
Well, exactly, right?

00:20:31.080 --> 00:20:31.720
Yeah, cool.

00:20:31.720 --> 00:20:44.280
So the one that really stands out to me that you were pretty well highlighted in, it actually had some interesting ethical components to it, was the Addison Mashley one.

00:20:44.280 --> 00:20:46.520
Just make it look like you don't know the name.

00:20:46.520 --> 00:20:47.480
That is a great defense.

00:20:47.480 --> 00:20:47.880
Well done.

00:20:47.880 --> 00:20:48.820
Thank you.

00:20:48.820 --> 00:20:50.500
I've heard of it.

00:20:50.500 --> 00:20:52.100
Honestly, darling, I've never heard of this site before.

00:20:52.100 --> 00:20:53.660
I have no idea why my email address is.

00:20:53.660 --> 00:20:57.800
So this is like an adult friend finder, let's have an affair type of thing.

00:20:57.800 --> 00:20:58.700
And it got hacked.

00:20:59.420 --> 00:21:07.640
Because not just people's passwords and possibly reused passwords were leaked, but just the fact that you even existed on the account was a pretty bad data breach.

00:21:07.640 --> 00:21:12.840
You had to even be a little careful about letting people know or searching for that data, right?

00:21:13.020 --> 00:21:16.180
Yeah, well, look, I mean, Ashley Madison, just for context for everyone.

00:21:16.180 --> 00:21:19.300
So this was a data breach that happened in July 2015.

00:21:19.400 --> 00:21:28.160
And hackers said, hackers, we still don't know who it was, said, look, we've got the Ashley Madison data.

00:21:28.160 --> 00:21:31.260
These guys, we don't agree with their business model.

00:21:31.260 --> 00:21:37.900
And OK, many people took an ethical stance on the whole context of this is not like a dating site.

00:21:37.900 --> 00:21:39.620
It's not like an outright adult site.

00:21:39.700 --> 00:21:43.940
It is literally their whole MO was helping people have affairs.

00:21:43.940 --> 00:21:46.800
You know, their strapline used to be life is short, have an affair.

00:21:46.800 --> 00:21:49.820
So they were, I guess, trying to mainstream adultery.

00:21:49.820 --> 00:21:53.360
And obviously, a lot of people took issue with that ethically.

00:21:53.860 --> 00:21:57.280
So whoever it was that got their data, said, look, we got the data.

00:21:57.280 --> 00:21:58.580
These guys have been bad.

00:21:58.580 --> 00:21:59.740
We don't like the business model.

00:21:59.740 --> 00:22:02.400
If they don't shut down in the next month, we're going to dump the data publicly.

00:22:02.400 --> 00:22:06.000
And this was kind of interesting on many levels.

00:22:06.000 --> 00:22:10.880
I mean, we do see threats like this, but often we see threats that are more financially motivated.

00:22:10.880 --> 00:22:15.080
We will see threats that say, you know, give us Bitcoin or we'll dump your data publicly.

00:22:15.080 --> 00:22:19.020
But in this case, they just obviously took an ethical dislike.

00:22:19.200 --> 00:22:24.520
And it was kind of interesting as well, because part of the original reasoning was what Ashley Madison did.

00:22:24.520 --> 00:22:30.360
And this is a this was a real dick movie in anyone's books is they said, look, you know, you sign up to this website.

00:22:30.360 --> 00:22:36.100
If you want to remove your data, you've got to pay for this full delete service.

00:22:36.100 --> 00:22:38.840
And from memory, it was about $19 to delete your data.

00:22:38.840 --> 00:22:46.560
So what would happen is there were a lot of people and I had I had literally hundreds of conversations with people on the site afterwards.

00:22:46.960 --> 00:22:55.960
And I was learning there are a lot of people who'd maybe have a couple too many red wines one night and go, hey, this might be fun and sign up and then, you know, this wasn't a good idea.

00:22:55.960 --> 00:22:57.280
And then they'd forget about it.

00:22:57.280 --> 00:22:59.220
There are a lot of people in there who were single.

00:22:59.220 --> 00:23:04.800
There are a lot of people who look again, regardless of your ethical position on it, were consenting adults.

00:23:04.800 --> 00:23:06.420
And they decided they wanted to use the site.

00:23:06.420 --> 00:23:11.800
And then if they wanted to get off there, they're having to pay money, which is just it's just a reprehensible move.

00:23:11.800 --> 00:23:13.660
But obviously, they thought they could monetize that.

00:23:13.980 --> 00:23:20.320
But anyway, one of the things the hacker said, as I said, when you pay for your full delete, you're not actually getting deleted.

00:23:20.320 --> 00:23:28.500
And what we discovered after we actually saw the data is paying the money would null out your record in the membership table.

00:23:28.500 --> 00:23:32.440
And then there would be a payment record with a foreign key back to the membership table.

00:23:32.440 --> 00:23:34.560
And the payment record would have your personal data on it.

00:23:35.060 --> 00:23:38.940
So it's like, yes, we removed you from the database.

00:23:38.940 --> 00:23:40.880
By the way, you just created a payment record.

00:23:40.880 --> 00:23:43.600
And the payment record has your personal info on it.

00:23:43.600 --> 00:23:47.340
So they took an ethical dislike to it.

00:23:47.340 --> 00:23:55.300
And in a way, it was kind of fortunate the way it panned out and that we had a month's notice between saying we're going to dump this and it actually happening.

00:23:55.300 --> 00:23:59.080
And I had time to think about how would I handle this and have I been pwned.

00:23:59.080 --> 00:24:00.940
Because it turned out to be more than 30 million records.

00:24:00.940 --> 00:24:02.040
So it was a very large breach.

00:24:02.040 --> 00:24:06.540
And I kind of went, look, this is going to be valuable data if it does turn up.

00:24:06.540 --> 00:24:08.820
And I want to make it searchable.

00:24:08.820 --> 00:24:18.020
But by the same token, this is the sort of thing where I don't want to be the vector through which, let's say, a rightly jealous wife discovers that her husband has been on the site.

00:24:18.460 --> 00:24:24.180
And incidentally, this was a very, very heavily male-dominated service, which probably comes as no surprise to anyone.

00:24:24.180 --> 00:24:30.640
And the women that were on there, a huge number of them signed up from IP address 127.0.0.1, which is a little bit suspicious.

00:24:30.640 --> 00:24:32.960
That is a tiny bit suspicious, yes.

00:24:32.960 --> 00:24:33.420
Yes.

00:24:33.420 --> 00:24:35.620
And actually, this is one of the things we learned, right?

00:24:35.620 --> 00:24:37.660
They're actually FEM bots, you know.

00:24:37.660 --> 00:24:40.100
So I always think Austin Powers, every time I hear FEM bots.

00:24:40.100 --> 00:24:41.480
I don't know if it was exactly like that.

00:24:42.280 --> 00:24:49.400
However, you've got these accounts on there, which are effectively bots, which are trying to engage with men.

00:24:49.400 --> 00:24:53.420
Because the more you can engage with them, the more you can get them to pay and all sorts of shady stuff.

00:24:53.420 --> 00:24:56.660
So anyway, eventually the data does get dumped.

00:24:56.660 --> 00:25:03.040
And by virtue of having had time to think about it, I had decided that I'd introduce the concept of a sensitive breach.

00:25:03.040 --> 00:25:06.720
And a sensitive breach means the data still goes in Have I Been Pwned?

00:25:06.720 --> 00:25:10.340
But you can't publicly, anonymously search for someone else.

00:25:10.700 --> 00:25:14.900
So you've got to actually subscribe to the notification service, which is free.

00:25:14.900 --> 00:25:19.820
But the reason I use that mechanism is because that sends you a verification email with a unique link.

00:25:19.820 --> 00:25:23.840
You click the unique link, and then it says, okay, now I know that you control this email address.

00:25:23.840 --> 00:25:25.500
We'll show you everything.

00:25:25.500 --> 00:25:27.520
So the public ones and the sensitive ones.

00:25:27.520 --> 00:25:29.420
Pretty legit way to handle it.

00:25:29.420 --> 00:25:34.080
Definitely the way the website handled the breach didn't sound like it went well.

00:25:34.080 --> 00:25:39.560
Let's talk about one more breach before we move on to some developer topics.

00:25:39.880 --> 00:25:51.880
One that you did write about was Disqus, the comment section, which actually is at the bottom of your show page, right at talkpython.fm/136.

00:25:51.880 --> 00:25:53.680
I suspect you could find Disqus right there.

00:25:53.680 --> 00:25:59.840
And you said that these guys, while something happened in terms of a data breach, they handled it right.

00:26:00.160 --> 00:26:02.680
Yeah, and you know, it's at the bottom of my blog as well.

00:26:02.680 --> 00:26:05.860
And it's also my data in the Disqus data breach.

00:26:06.520 --> 00:26:16.320
So I think that the macro picture here is that when data breaches happen, there's a real broad range of reactions from organizations involved.

00:26:16.320 --> 00:26:30.760
And they range from, on the one hand, being extremely difficult to get in touch with, sometimes denying it, often downplaying the severity, sometimes covering it up, like knowing there was a breach, but not telling people because they're worried about reputation damage.

00:26:30.760 --> 00:26:37.140
And doing all the sorts of crappy things that you might expect an organization at the receiving end of one of these things might do.

00:26:37.140 --> 00:26:41.500
And then on the other hand, there's sort of like, this is exactly the way to handle it.

00:26:41.500 --> 00:26:44.780
And Disqus was very much on that right-hand side.

00:26:44.780 --> 00:26:48.000
And there's really only a couple of organizations I've dealt with that have been down there.

00:26:48.460 --> 00:26:52.660
And the things that Disqus did really well, one of them is the speed.

00:26:52.660 --> 00:26:56.660
So everyone would have seen Equifax in the news only last month.

00:26:56.660 --> 00:27:02.360
And Equifax took about five weeks after learning of the data breach to advise everyone.

00:27:02.360 --> 00:27:03.540
They needed to sell their stock.

00:27:03.540 --> 00:27:04.400
Well, geez.

00:27:04.400 --> 00:27:06.280
Allegedly.

00:27:06.280 --> 00:27:08.440
Well, they did sell stock.

00:27:08.440 --> 00:27:10.340
Allegedly because of the breach.

00:27:10.340 --> 00:27:11.260
You've got to be careful.

00:27:11.260 --> 00:27:12.020
Right, right, right, right.

00:27:12.020 --> 00:27:13.740
People get very litigious in America.

00:27:13.740 --> 00:27:14.240
I know this.

00:27:14.240 --> 00:27:15.140
Yes, that's right.

00:27:16.880 --> 00:27:17.940
Actually, I've heard this.

00:27:17.940 --> 00:27:19.900
Fortunately, I don't know this through personal experience.

00:27:19.900 --> 00:27:24.740
But the thing with Disqus is that I got in touch with them.

00:27:24.740 --> 00:27:30.960
And actually, just to be clear as well, the context we've discussed is someone popped up and gave me seven different data breaches.

00:27:30.960 --> 00:27:33.920
And they're things that I had never seen anywhere before.

00:27:33.920 --> 00:27:37.140
It was things like Reverb Nation, Kickstarter, Bitly.

00:27:37.140 --> 00:27:43.220
And all of these three had previously disclosed where they'd said, hey, we've been hacked, never seen any data for it.

00:27:43.220 --> 00:27:45.600
And then suddenly they all turn up in this one place and they're all legit.

00:27:46.180 --> 00:27:49.120
And then one of them as well was this Disqus one.

00:27:49.120 --> 00:27:52.400
And I'm trying to find references to a data breach.

00:27:52.400 --> 00:27:53.180
I can't find anything.

00:27:53.180 --> 00:27:55.260
And there's 17 million email addresses in there.

00:27:55.260 --> 00:28:01.040
So fortunately, I had a contact there, someone I'd been chatting to only a couple of months earlier on another topic.

00:28:01.040 --> 00:28:06.300
And I was like, I think I have your data and you probably should know about this.

00:28:06.700 --> 00:28:21.100
And from the point where I sent that first email to when they had made a public statement and had already reset impacted passwords as well, I think it was 23 hours and 43 minutes.

00:28:21.100 --> 00:28:22.400
It was just under a day.

00:28:23.200 --> 00:28:23.960
And it's like, okay.

00:28:23.960 --> 00:28:24.560
That's awesome.

00:28:24.560 --> 00:28:25.200
That is awesome.

00:28:25.200 --> 00:28:27.020
Like you guys have turned this around less than a day.

00:28:27.660 --> 00:28:31.680
And when I spoke to them, so we jumped on the phone as well and had a good chat.

00:28:31.680 --> 00:28:35.580
They were sort of really, I mean, obviously they weren't happy about the situation.

00:28:35.580 --> 00:28:36.580
No one's going to be.

00:28:36.660 --> 00:28:42.460
But they sort of understood that this is the reality of operating online today where these things do happen very regularly.

00:28:42.460 --> 00:28:47.920
They prepared communications, which was transparent, candid, honest.

00:28:47.920 --> 00:28:50.520
They worked with the media as well.

00:28:50.520 --> 00:28:57.920
So one of the things I was sort of impressing on them is I think it's really important to engage with the media because there's going to be stories on this.

00:28:58.180 --> 00:29:07.940
And you can either ensure that those stories are representing your point of view and your version of events or you can say, look, we're too busy or we don't respond to the media and you can let them form their own opinions.

00:29:07.940 --> 00:29:10.900
So they just did all of that right.

00:29:10.900 --> 00:29:16.780
And the only negative feedback I saw out of any of this was people saying, well, how come it took you four years?

00:29:16.780 --> 00:29:18.980
Because apparently it actually happened in late 2013.

00:29:18.980 --> 00:29:20.640
Well, they didn't know.

00:29:20.640 --> 00:29:21.780
And we still don't know.

00:29:21.780 --> 00:29:27.100
Well, I haven't seen any press as to how it happened or why it took this long to know.

00:29:27.200 --> 00:29:33.020
But I guess based on the hand that they were dealt a couple of weeks ago, the way they handled it was just exemplary.

00:29:33.020 --> 00:29:40.260
One of the things that's always kind of in the back of my mind, I've run a number of websites, have a couple of servers, some backend servers.

00:29:40.260 --> 00:29:41.320
I talk to those servers.

00:29:41.320 --> 00:29:44.520
You know, how would I know if I've been hacked?

00:29:44.520 --> 00:29:47.320
Well, as we've just seen, you may not.

00:29:47.320 --> 00:29:49.440
And this is sort of part of the problem.

00:29:49.440 --> 00:29:54.520
We've got to remember as well that the discuss situation is far from being exceptional.

00:29:54.520 --> 00:30:06.420
So just off the top of my head last year was LinkedIn, Dropbox, MySpace, Last.fm, Tumblr, many others that were in similar boats where they had had breaches years ago and were only just discovering it.

00:30:06.420 --> 00:30:11.200
And incidentally, that big stash of data that had Bitly and Kickstarter and all the other ones in it, there was another one.

00:30:11.300 --> 00:30:15.420
I'm thinking very carefully before I say these words because two of them haven't been disclosed yet.

00:30:15.420 --> 00:30:19.580
But there was another one which was a service called We Heart It, which seems a little bit like Pinterest.

00:30:19.580 --> 00:30:20.540
That was in there.

00:30:20.540 --> 00:30:23.340
And there were two others which were still waiting.

00:30:23.340 --> 00:30:27.620
One of them should be sending their message out any moment now, which is millions of accounts again.

00:30:27.820 --> 00:30:31.760
And then a final one, which I'm still trying to get in touch with some people.

00:30:31.760 --> 00:30:34.500
Not everyone replies to an email when you say, hey, you've been breached.

00:30:34.500 --> 00:30:37.460
They might pretend it went to spam and just go dark, right?

00:30:37.460 --> 00:30:37.960
Yeah.

00:30:37.960 --> 00:30:40.280
Well, they won't be able to once I've published the data and have I been pwned.

00:30:40.280 --> 00:30:44.200
But I'd really like to give them the opportunity to control the messaging themselves first.

00:30:44.720 --> 00:30:52.820
But what we've got to remember with all these cases is they happened years ago, very often with entirely different people in the organization as well.

00:30:52.820 --> 00:30:56.160
Very often with different infrastructure, different code bases.

00:30:56.160 --> 00:31:00.140
Very often you're not even going to have logs that go back, say, four years.

00:31:00.140 --> 00:31:02.400
You know, do you really keep web server logs that long?

00:31:02.400 --> 00:31:03.760
A lot of organizations don't.

00:31:03.760 --> 00:31:09.960
So it can be enormously difficult to know when an incident has actually happened.

00:31:09.960 --> 00:31:20.360
And I guess your point about how do you know, one of the things that sort of continues to strike me is that there are all of these incidents that have already happened that we don't know about as the public.

00:31:20.360 --> 00:31:27.080
And then a subset of those, probably a very large subset, where the organization themselves doesn't know about it as well or don't know about it.

00:31:27.080 --> 00:31:33.420
So we're yet to see so much stuff actually come out of the woodwork that we just haven't even begun to conceive of yet.

00:31:33.420 --> 00:31:35.240
Yeah, that's kind of daunting.

00:31:35.240 --> 00:31:36.740
It's sobering, isn't it?

00:31:36.740 --> 00:31:37.540
Yeah, sure.

00:31:37.740 --> 00:31:44.400
So it sounds to me like a lot of the time this becomes something that we're aware of because the data is discovered.

00:31:44.400 --> 00:31:47.200
You're like, oh my gosh, these accounts are all coming from this one place.

00:31:47.200 --> 00:31:55.100
There might be people that use like their email address, like my email address plus LinkedIn at gmail.com.

00:31:55.100 --> 00:32:00.360
And you're like, well, I only use the plus LinkedIn at LinkedIn and it's out in this paste bin or something, right?

00:32:00.360 --> 00:32:11.220
Well, when people do that, so when they use that sort of aliasing pattern where they have this sort of plus after the alias or when they have their own domain and they create custom aliases for every service.

00:32:11.220 --> 00:32:11.600
Right.

00:32:11.600 --> 00:32:12.960
LinkedIn at my domain.

00:32:12.960 --> 00:32:13.160
Yeah.

00:32:13.160 --> 00:32:13.760
Yeah, right.

00:32:13.760 --> 00:32:14.660
That does actually help.

00:32:14.780 --> 00:32:21.240
In fact, one of the ones I'm going through a disclosure with now, and this is sort of the last out of the seven that was in the stash I just spoke about.

00:32:21.740 --> 00:32:24.160
It wasn't immediately clear where all the data was from.

00:32:24.160 --> 00:32:31.160
It's labeled one service, but I was actually checking with have I been pwned subscribers saying, hey, look, you're in here.

00:32:31.160 --> 00:32:32.120
Have you used this service?

00:32:32.120 --> 00:32:34.640
And a bunch of them were saying, you know, no, I've got no idea what it is.

00:32:35.300 --> 00:32:43.720
And it was only when I started going through looking at things like the aliases on email addresses where it's like, oh, okay, I can actually see this other thing now.

00:32:43.720 --> 00:32:49.700
And I was able to kind of join the dots and go, okay, well, I can see that this is actually from multiple different sources.

00:32:49.700 --> 00:32:51.260
I think it's actually from two different sources.

00:32:51.260 --> 00:32:53.620
It does seem like at least a little bit of a breadcrumb.

00:32:53.620 --> 00:32:56.040
I don't do it that often, but sometimes I do.

00:32:56.040 --> 00:32:56.360
Yeah.

00:32:56.360 --> 00:32:58.620
No, it can be really useful for that.

00:32:58.880 --> 00:33:08.600
The other thing that I hear people saying is I say, look, I use this aliasing pattern because then I can see when my data is, say, sold or redistributed or something like that.

00:33:08.600 --> 00:33:10.480
I can see where the source was.

00:33:10.480 --> 00:33:12.780
The problem then, of course, is what are you going to do about it?

00:33:12.780 --> 00:33:14.040
Like we know that this happens.

00:33:14.040 --> 00:33:21.560
And if you go back to the organization and say, hey, I signed up on your service and I signed up with my name plus your service name at gmail.com.

00:33:21.560 --> 00:33:24.300
And I've just seen I've just gotten spam trying to sell me Viagra.

00:33:24.300 --> 00:33:25.900
Okay, now what?

00:33:25.900 --> 00:33:28.780
You know, like there's not really anything that's actionable.

00:33:28.780 --> 00:33:30.160
Yeah, I've heard that.

00:33:30.160 --> 00:33:35.500
And actually, I've been on both sides of that story, not with stuff that I'm doing these days, but previous project.

00:33:35.500 --> 00:33:39.020
And, you know, how much weight should you put in that?

00:33:39.020 --> 00:33:44.380
Like on one hand, I feel like, yeah, okay, if somebody really got all the data out of the database, they would have those things that they could email people.

00:33:44.380 --> 00:33:47.580
But it seems like that's a weird thing to do is just spam folks.

00:33:47.580 --> 00:33:53.900
On the other hand, if somebody breaks into that person's email and just harvests every email they see and just start sending to it,

00:33:54.240 --> 00:33:59.020
maybe it's their account that got broken into and just happened to be they kind of loop back.

00:33:59.020 --> 00:34:01.200
Well, you know, this is also one of the great mysteries, right?

00:34:01.200 --> 00:34:04.760
Where there are so many different ways that these things can go down.

00:34:04.760 --> 00:34:08.820
And very, very often, you just simply can't get to the bottom of it.

00:34:08.820 --> 00:34:11.400
I mean, people say to me all the time, like, I'm getting spam.

00:34:11.400 --> 00:34:12.140
Where's it come from?

00:34:12.140 --> 00:34:12.640
I don't know.

00:34:12.640 --> 00:34:14.880
It's like you put your email address out all over the place.

00:34:14.880 --> 00:34:16.080
It could be from anywhere.

00:34:16.080 --> 00:34:16.860
Yeah, that's right.

00:34:16.960 --> 00:34:19.720
Get a good spam filter and just buck it up, I guess.

00:34:19.720 --> 00:34:20.520
Exactly.

00:34:22.600 --> 00:34:26.260
This portion of TalkPath in a Me is brought to you by GoCD from ThoughtWorks.

00:34:26.260 --> 00:34:29.280
These are the people that literally invented continuous integration.

00:34:29.280 --> 00:34:34.000
They have a great open source on-prem CDCI server called GoCD.

00:34:34.000 --> 00:34:39.700
But rather than tell you about the server this week, I want to share a course they created for people who are new to continuous integration.

00:34:39.700 --> 00:34:47.480
Check out their course called Continuous Delivery 101 from GoCD at gocd.org slash 101.

00:34:47.780 --> 00:34:54.180
This video series covers the history of continuous delivery, concepts, best practices, how to get started, and popular tools.

00:34:54.180 --> 00:35:01.020
You'll gain a holistic view of continuous delivery and a deeper understanding and an appreciation of the critical concepts.

00:35:01.020 --> 00:35:08.120
Be sure to try their course at gocd.org slash 101 and let them know that you appreciate their sponsorship of Talk Python.

00:35:08.120 --> 00:35:10.840
Speak to the web developers out there.

00:35:10.840 --> 00:35:17.480
What lessons have you learned from running Have I Been Pwned that you'd like to share with that audience?

00:35:17.640 --> 00:35:22.740
There are a lot of things, you know, speaking or sort of thinking about it from a pure dev perspective.

00:35:22.740 --> 00:35:31.900
Going back to what I was saying earlier, one of the main reasons I wanted to do this is because I really wanted to do something in anger on Azure.

00:35:31.900 --> 00:35:43.480
And I've had a heap of fun, to be honest, building this service out on Azure and sort of experiencing the whole cloud scale thing and commoditized pricing and all the other sort of promises of the cloud.

00:35:44.060 --> 00:35:46.040
And there's a heap of learning out of that, both good and bad.

00:35:46.040 --> 00:35:58.380
I mean, some of the good stuff has been things like the ability to have auto scale, you know, so actually provision more infrastructure on evidence of exceeding existing infrastructure resources is fantastic.

00:35:58.840 --> 00:36:04.240
And I've spoken many times at events about how to run a project like this on a coffee budget.

00:36:04.240 --> 00:36:10.940
So how do I run a service with 4.8 billion records and sometimes millions of visitors a day for what you'd spend on coffee?

00:36:10.940 --> 00:36:14.440
And I don't always manage to do that, but I usually get pretty close.

00:36:14.940 --> 00:36:19.380
And things like really managing your scale very carefully have been great.

00:36:19.380 --> 00:36:25.560
Things like choosing the right data storage constructs for your use case have been great as well.

00:36:25.560 --> 00:36:33.740
So particularly in sort of the Microsoft-y world, there's been this traditional view of you're going to store data, you've got to have a SQL database.

00:36:33.740 --> 00:36:36.820
And a SQL database is a behemoth of a thing, right?

00:36:36.840 --> 00:36:39.500
I mean, it is a big, big thing.

00:36:39.500 --> 00:36:41.500
And it's expensive and it's...

00:36:41.500 --> 00:36:43.700
Especially with four point something billion records.

00:36:43.700 --> 00:36:51.820
Well, hey, look, if I got to that point, if I was actually trying to put that data in a relational database like SQL Server, I mean, the cost would just be astronomical.

00:36:51.820 --> 00:36:59.980
But it would also be really unnecessary because the patterns with which the data is used just don't predispose it to needing a relational database.

00:36:59.980 --> 00:37:05.860
So one of the best things I ever did was to use Azure's table storage, which is basically just a key value pair.

00:37:05.860 --> 00:37:09.180
And I just petitioned it in a way that worked really well.

00:37:09.180 --> 00:37:12.160
So those 4.8 billion records are in there.

00:37:12.160 --> 00:37:15.260
You can create a petition and then a row key.

00:37:15.260 --> 00:37:17.440
So my petition keys are the domain.

00:37:17.440 --> 00:37:18.540
So say gmail.com.

00:37:18.540 --> 00:37:20.500
And then the row key is the alias.

00:37:20.500 --> 00:37:24.300
And what that allowed me to do is do super, super, super fast lookups.

00:37:24.300 --> 00:37:28.400
Because when you're searching, you're literally searching by domain and alias.

00:37:28.400 --> 00:37:31.220
So a very specific petition and a very specific row key.

00:37:31.220 --> 00:37:37.080
And it also made it really easy to do entire domain wide reports because I just pulled the entire petition.

00:37:37.080 --> 00:37:39.700
So that works awesomely.

00:37:39.700 --> 00:37:44.380
And that is still the single best decision I've ever made, I'd say, in terms of the architecture.

00:37:44.380 --> 00:37:52.120
So for those 4.8 billion records that on disk is some tens of gigabytes, they actually don't have good reporting about actual size.

00:37:52.620 --> 00:37:54.680
And it always scales infinitely.

00:37:54.680 --> 00:37:56.760
So it is platform as a service.

00:37:56.760 --> 00:37:59.940
I've never reached any scale capacity on the storage tier.

00:37:59.940 --> 00:38:04.600
And it usually returns records within sort of a single digit millisecond range.

00:38:04.600 --> 00:38:06.280
And that cost me about $40 a month.

00:38:06.280 --> 00:38:06.500
Awesome.

00:38:06.500 --> 00:38:08.680
Yeah, which is just like, how cool is this?

00:38:08.680 --> 00:38:09.640
It just rocks.

00:38:09.640 --> 00:38:17.520
One of the other really, really big things I've learned is I started using Cloudflare for Have I Been Pwned about a year ago.

00:38:17.520 --> 00:38:21.640
And look, I've been using them on things like my blog and a couple of other little projects.

00:38:22.160 --> 00:38:31.180
And it was kind of cool for that in that you get HTTPS for free and a few other little sort of bits and pieces that make things like my blog run a lot better.

00:38:31.180 --> 00:38:34.620
But it made a massive difference to Have I Been Pwned.

00:38:34.620 --> 00:38:38.940
And I originally did it because I was getting DDoSed and that sort of, that wasn't fun.

00:38:38.940 --> 00:38:41.600
And Cloudflare put a stop to that.

00:38:42.120 --> 00:38:47.980
But then it became really, really awesome because you have things like a firewall that you can control programmatically.

00:38:47.980 --> 00:38:56.260
So one of the reasons I put it in place was the API I have, I introduced a rate limit because I was seeing some fairly nefarious behavior to it.

00:38:56.740 --> 00:38:58.920
And I went, oh, look, I'll just put a rate limit in.

00:38:58.920 --> 00:39:04.900
And if you exceed the rate limit, I'll return a 429 too many requests and say you can retry after two seconds.

00:39:04.900 --> 00:39:07.060
And they'll see that and they'll stop.

00:39:07.060 --> 00:39:08.040
But no, they don't.

00:39:08.040 --> 00:39:09.900
They just kept hammering it.

00:39:10.580 --> 00:39:18.840
And one of the pennies that dropped was that when you expose that sort of origin server to the world, you have to deal with everything there.

00:39:18.840 --> 00:39:24.180
So you have to deal with every incoming request on that same infrastructure, which is actually trying to reserve legitimate requests.

00:39:24.180 --> 00:39:36.800
And when you put a service like Cloudflare in front, suddenly you have this other layer in front where you can start to programmatically exclude nasty stuff and actually free up the underlying resources to do the things they're meant to do.

00:39:37.260 --> 00:39:40.320
So I've got some great rate limiting implementations.

00:39:40.320 --> 00:39:56.660
I've got a really neat model I've written about before using Azure Functions where even when I see behavior that's slightly nefarious on the website itself, I just drop in a JavaScript challenge rule on the Cloudflare edge node so that if you go to the site, it just makes sure you're in a browser and you can't automate it with an API.

00:39:56.660 --> 00:40:00.360
And then you basically get a 24-hour timeout and then you can try again.

00:40:00.360 --> 00:40:02.260
So it's been awesome for that.

00:40:02.260 --> 00:40:07.540
That has also been super awesome for reducing my cost because I cached the bejesus out of this thing.

00:40:07.540 --> 00:40:20.000
So Cloudflare has got 118 edge nodes as of today around the world and everything from the front page to the FAQs to every single image in JavaScript file and CSS file is served from those edge nodes.

00:40:20.540 --> 00:40:27.080
So the actual traffic that comes through to the site is usually just API requests and a couple of other dynamic things.

00:40:27.080 --> 00:40:33.460
So it's really, really dramatically reduced my costs and the frequency with which I need to scale my infrastructure.

00:40:33.460 --> 00:40:34.420
Oh, that's really cool.

00:40:34.420 --> 00:40:41.360
And Cloudflare is actually in the news recently this week, a couple weeks ago, for announcing basically unlimited DDoS production.

00:40:41.360 --> 00:40:43.740
So yeah, that's cool.

00:40:43.940 --> 00:40:45.540
It's the world we live in today, isn't it?

00:40:45.540 --> 00:40:48.180
And look, that was sort of their traditional thing, right?

00:40:48.180 --> 00:40:50.200
Like they made their name out of DDoS protection.

00:40:50.200 --> 00:40:55.420
But they do so much more now because they can sit on the wire and intercept that traffic.

00:40:55.420 --> 00:40:57.220
And that spins some people out as well.

00:40:57.220 --> 00:41:02.500
And if you are listening to this and you get spun out by it, I've got a blog post about security absolutism.

00:41:02.500 --> 00:41:07.080
So if you Google my name, security absolutism, you sort of see me try and put things in perspective.

00:41:07.500 --> 00:41:14.080
But it means you can do stuff like all these websites that are now going HTTPS because they're being sort of forced down this route.

00:41:14.080 --> 00:41:19.480
You can go HTTPS for free via Cloudflare within about five minutes.

00:41:19.480 --> 00:41:25.600
And not only that, but they can also do things like add an HSTS header because when you can intercept the traffic, you can add headers.

00:41:25.600 --> 00:41:29.860
They can rewrite HTTP references to HTTPS.

00:41:29.860 --> 00:41:34.080
They can 301 all of your insecure requests over to secure requests.

00:41:34.400 --> 00:41:38.280
And they can do all of this stuff because they're sitting there controlling the traffic.

00:41:38.280 --> 00:41:41.580
And that just makes a huge amount of sense for many, many good reasons.

00:41:41.580 --> 00:41:42.220
Yeah, that's cool.

00:41:42.220 --> 00:41:43.260
And you don't have to worry about it.

00:41:43.260 --> 00:41:43.720
Yeah, yeah.

00:41:43.720 --> 00:41:45.360
It's like literally a turnkey thing.

00:41:45.360 --> 00:41:46.020
Yeah, that's cool.

00:41:46.020 --> 00:41:49.080
Yeah, I don't use it, but I've considered adding it.

00:41:49.080 --> 00:41:50.840
And it seems like it might be a good idea.

00:41:50.840 --> 00:41:51.760
I should be quiet.

00:41:51.760 --> 00:41:53.680
Somebody might try to attack my...

00:41:53.680 --> 00:41:55.060
No, you know what?

00:41:55.060 --> 00:41:57.060
And it is like a five-minute job too.

00:41:57.060 --> 00:41:59.820
And people go, yeah, but you can just go to Let's Encrypt and get a certificate.

00:41:59.820 --> 00:42:00.640
And they're right.

00:42:00.640 --> 00:42:02.360
And Let's Encrypt is absolutely awesome.

00:42:02.680 --> 00:42:04.020
But it is just certificates.

00:42:04.020 --> 00:42:08.500
And there is so much more that you can do with a reverse proxy like Cloudflare.

00:42:08.500 --> 00:42:14.200
Like once you actually use it in anger on a large-scale site, you go, wow, how would I ever not?

00:42:14.200 --> 00:42:14.860
You know?

00:42:14.860 --> 00:42:15.520
Right.

00:42:15.520 --> 00:42:21.540
And even if it's a small site or a static blog or something, it still makes an enormous amount of sense because of stuff like the caching.

00:42:21.540 --> 00:42:21.920
Cool.

00:42:21.920 --> 00:42:24.560
Yeah, it definitely sounds like it's worth checking out.

00:42:25.560 --> 00:42:37.140
So one thing I wanted to talk to you about is WannaCry, which was a real sad thing that sort of was a ransomware thing that went around just encrypting all the things.

00:42:37.140 --> 00:42:39.540
And it took out a bunch of places.

00:42:39.540 --> 00:42:44.420
The most notable one, I guess, was the National Health Service in the UK.

00:42:44.420 --> 00:42:44.840
That's right.

00:42:44.860 --> 00:42:49.020
But it also took out things like Maersk, FedEx, a bunch of places, right?

00:42:49.020 --> 00:42:57.620
WannaCry seems to me like a real lesson in just being vigilant and patching all of your stuff and keeping it up to date.

00:42:57.620 --> 00:43:00.460
But there's tons of things that were way out of date.

00:43:00.460 --> 00:43:02.420
What do you think some of the lessons are from WannaCry?

00:43:02.680 --> 00:43:03.900
Well, just one thing on that, actually.

00:43:03.900 --> 00:43:05.620
The Maersk one was NotPetya.

00:43:05.620 --> 00:43:10.340
So NotPetya came, I think it was about a month or six weeks after WannaCry.

00:43:10.340 --> 00:43:11.140
It wasn't that long.

00:43:11.140 --> 00:43:11.720
Right, right.

00:43:11.720 --> 00:43:12.700
There were similar times.

00:43:12.700 --> 00:43:13.580
Yeah, it was similar.

00:43:13.580 --> 00:43:15.640
But, yeah, yeah, there's similar times.

00:43:15.640 --> 00:43:16.800
And look, it's still ransomware.

00:43:16.800 --> 00:43:18.160
Look, you don't want either.

00:43:18.160 --> 00:43:18.960
All right?

00:43:18.960 --> 00:43:25.680
The thing that was really interesting about WannaCry is the sort of sequence of events leading up to that.

00:43:25.680 --> 00:43:29.880
Without having the exact dates in front of me, from memory, WannaCry hit us in May.

00:43:30.480 --> 00:43:34.720
And back in March, we had seen Microsoft say, look, there are some critical patches.

00:43:34.720 --> 00:43:36.840
You really should take these critical patches.

00:43:36.840 --> 00:43:40.700
And they didn't really elaborate why they knew they were so important or so timely.

00:43:40.700 --> 00:43:43.520
But, you know, maybe you should just really just do this right now.

00:43:43.520 --> 00:43:44.580
Exactly.

00:43:44.580 --> 00:43:49.320
And then a month later, we saw this sort of shadow brokers dump.

00:43:49.320 --> 00:43:54.080
So this collective that has collected themselves a bunch of zero days.

00:43:54.080 --> 00:43:59.660
And one of the vulnerabilities in there was this eternal blue vulnerability which exploited SMB.

00:44:00.260 --> 00:44:04.600
Which we'd normally use for sort of connecting and file systems and sharing information across them.

00:44:04.600 --> 00:44:13.360
And the problem there was that you could remotely connect to a machine with a vulnerable SMB implementation and have remote code exec on it, which is really nasty stuff.

00:44:13.360 --> 00:44:17.360
And, okay, now we're saying, all right, so a month ago, Microsoft said patch your things.

00:44:17.360 --> 00:44:19.060
Now we know why it was important.

00:44:19.260 --> 00:44:21.220
And then another whole month went by.

00:44:21.220 --> 00:44:22.720
And then WannaCry hit.

00:44:22.720 --> 00:44:29.620
So by the time someone got hit with WannaCry that was exploiting Eternal Blue, two months ago, they knew it was a big thing.

00:44:29.620 --> 00:44:31.540
A month ago, we knew why it was a big thing.

00:44:31.540 --> 00:44:33.340
And now you still haven't patched your things.

00:44:34.000 --> 00:44:38.300
And really, the lesson out of this was around patching.

00:44:38.300 --> 00:44:44.200
And this was just such a sort of, I guess, poignant example of why it was so important.

00:44:44.200 --> 00:44:45.440
Because this was devastating.

00:44:45.440 --> 00:44:47.520
I mean, it hit particularly the NHS.

00:44:47.780 --> 00:44:54.120
But it hit everything from German trains through to other services and other parts of Europe in particular really, really hard.

00:44:54.120 --> 00:45:05.600
And what sort of stunned me is after that, a lot of the stuff I was writing and talking to people about was this whole patching cycle where there were still people going, I disable Windows updates.

00:45:05.600 --> 00:45:07.580
And there's literally tutorials out there.

00:45:07.580 --> 00:45:10.540
How to disable Windows updates because maybe you'll get a bad one.

00:45:10.540 --> 00:45:13.660
And I would have people sort of justifying turning it off.

00:45:13.660 --> 00:45:19.740
They're like, well, I don't like it because sometimes you go down, you got to shut down your PC and it says you've got to wait while updates install.

00:45:19.740 --> 00:45:22.700
And once I was about to get on an airplane and I couldn't close my laptop.

00:45:22.700 --> 00:45:25.760
That's your reason.

00:45:25.760 --> 00:45:30.660
But actually, my favorite one was a bunch of people would say, I keep installing all these stupid updates.

00:45:30.660 --> 00:45:31.920
I've never even had a virus.

00:45:31.920 --> 00:45:36.120
Yeah, but that's why you haven't because you install the updates.

00:45:36.120 --> 00:45:38.260
You know, it's certainly an important part of it.

00:45:38.620 --> 00:45:45.960
So to have that happen and then have NotPetya occur just after that, which again was exploiting a number of exploits.

00:45:45.960 --> 00:45:51.260
But one of them was an unpatched or rather a patched vulnerability, which people had left unpatched and then got exploited.

00:45:51.260 --> 00:45:51.580
Right.

00:45:51.580 --> 00:45:54.960
If they didn't learn from the first two times around, they should have really learned from WannaCry.

00:45:54.960 --> 00:45:55.640
I know.

00:45:55.640 --> 00:45:56.240
Exactly.

00:45:56.240 --> 00:45:57.320
Anyway.

00:45:57.320 --> 00:45:57.660
Yeah.

00:45:57.660 --> 00:46:01.600
The reason I bring this up is not to just talk about these crazy viruses and patching.

00:46:01.600 --> 00:46:15.480
But I think there's a real tension in organizations and the larger the organization, the greater the dissolution of responsibility in this is to say like, look, there's this system that's running our invoices.

00:46:15.480 --> 00:46:18.660
Nobody knows how really to upgrade it.

00:46:18.660 --> 00:46:24.640
And nobody wants to be the one to take the responsibility of patching it because if it goes down, their weekend is toast.

00:46:24.640 --> 00:46:25.860
We're just going to leave it.

00:46:25.860 --> 00:46:26.680
Not my problem.

00:46:26.680 --> 00:46:27.980
Someone else's problem.

00:46:27.980 --> 00:46:28.280
Right.

00:46:28.280 --> 00:46:30.280
How do you think we address that?

00:46:30.280 --> 00:46:31.940
Look, it's a good observation.

00:46:31.940 --> 00:46:39.700
And we've got to be fair here that we don't sort of overly trivialize the complexity that can be involved in actually patching these things.

00:46:39.700 --> 00:46:45.940
I mean, think about the NHS for a moment and think about what hospitals run and some of the systems they have.

00:46:45.940 --> 00:46:47.240
Think about an MRI machine.

00:46:47.240 --> 00:46:50.920
You know, imagine actually trying to patch that thing.

00:46:50.920 --> 00:46:54.240
And look, there's a bunch of them probably sitting out there still running Windows XP.

00:46:54.240 --> 00:46:57.940
You know, you don't just like whack in the DVD and upgrade to Windows 10.

00:46:57.940 --> 00:47:00.080
This is not a simple process.

00:47:00.080 --> 00:47:01.660
So I'm sympathetic to that.

00:47:01.660 --> 00:47:07.440
And I suspect that what happened in cases like the NHS is there's budget constraints.

00:47:07.440 --> 00:47:10.060
In fact, we know there's going to be budget constraints in a hospital.

00:47:10.060 --> 00:47:11.320
There's budget constraints.

00:47:11.320 --> 00:47:19.000
The IT managers had to make a call between where that budget gets spent, what's patched when, how much money they allocate into different areas.

00:47:19.340 --> 00:47:21.140
And it would have been a very hard problem.

00:47:21.140 --> 00:47:26.280
And large enterprises are not exactly just running Windows update automatically across everything either.

00:47:26.280 --> 00:47:29.640
I mean, these things are tested and rolled out through standard operating environments.

00:47:29.640 --> 00:47:31.200
And they're a big thing.

00:47:31.480 --> 00:47:40.600
And I think, to be honest, like the bigger picture here is that when there is a high friction of updates, it makes the uptake very difficult.

00:47:40.600 --> 00:47:45.060
I mean, one of the things that Apple's done really well is they've made it such a low friction process.

00:47:45.060 --> 00:47:47.700
You know, like, hey, iOS 11 landed the other day.

00:47:47.700 --> 00:47:49.700
A thing popped up on my screen and I said yes.

00:47:49.700 --> 00:47:51.460
And I never had to worry that it wouldn't work.

00:47:51.840 --> 00:47:56.840
And if it didn't work, I would have restored from iCloud while I went out and, you know, kicked the ball with the kids or something.

00:47:56.840 --> 00:47:57.980
And I'd come back inside and I'd be done.

00:47:57.980 --> 00:48:04.920
So, yeah, that's really the model that we'd love to move towards where these things are low friction and automated.

00:48:04.920 --> 00:48:09.160
And unfortunately, that's just not the reality with a lot of systems today.

00:48:09.160 --> 00:48:10.240
Yeah, absolutely.

00:48:10.240 --> 00:48:17.560
The more complicated they get, like it could be like a library that's compiled into your code that you've got to upgrade.

00:48:17.560 --> 00:48:20.600
For example, with the Equifax one, right?

00:48:20.600 --> 00:48:21.820
Yeah, the struts in Equifax.

00:48:21.820 --> 00:48:22.200
That's right.

00:48:22.200 --> 00:48:28.140
And like they had to like recompile and, you know, who knows what got deprecated, what had to have been changed.

00:48:28.140 --> 00:48:29.760
Granted, that doesn't excuse them.

00:48:29.760 --> 00:48:31.060
They really messed that up.

00:48:31.060 --> 00:48:33.060
But still, it's not that.

00:48:33.060 --> 00:48:36.880
It's a little bit hard trying to throw Equifax a bone.

00:48:36.880 --> 00:48:41.700
But look, I mean, that is something where it's like, okay, let's just be objective about it.

00:48:41.700 --> 00:48:46.520
I can see where when it struts and you have to like go and recompile some pretty serious stuff as well.

00:48:46.700 --> 00:48:48.580
It comes back to that point about the friction.

00:48:48.580 --> 00:48:54.380
And when there is this high friction of change, well, yeah, it's going to be hard to get this stuff done in a timely manner.

00:48:54.380 --> 00:48:55.200
Yeah, absolutely.

00:48:55.200 --> 00:48:55.680
Absolutely.

00:48:55.680 --> 00:48:57.380
So we don't have a lot of time left.

00:48:57.380 --> 00:48:58.920
Let's talk a little bit about IoT.

00:48:58.920 --> 00:49:03.060
You wrote an interesting blog post.

00:49:03.060 --> 00:49:04.360
I think it's a blog post called,

00:49:04.360 --> 00:49:08.960
What It Would Look Like If We Put Warnings on IoT Devices Like We Do on Cigarette Packs.

00:49:08.960 --> 00:49:10.560
Tell us about that.

00:49:10.560 --> 00:49:12.000
Yeah, yeah.

00:49:12.000 --> 00:49:12.680
That was fun.

00:49:13.360 --> 00:49:18.940
So the sort of premise of this is a lot of IoT stuff's got some pretty crazy vulnerabilities in it.

00:49:18.940 --> 00:49:24.520
Now, one of the ones I saw a couple of years ago is that there's these kids' tablets.

00:49:24.520 --> 00:49:27.160
So imagine if Fisher Price made an iPad.

00:49:27.160 --> 00:49:28.960
This is sort of my vision of it.

00:49:28.960 --> 00:49:32.380
So it's plasticky and colorful and all this sort of thing.

00:49:32.580 --> 00:49:35.480
And these were made by a company called VTech, Hong Kong-based toy maker.

00:49:35.480 --> 00:49:41.960
And VTech had a vulnerability that someone exploited, sucked out millions of parents' and children's data.

00:49:41.960 --> 00:49:50.520
And the kids' data included things like their names, their birthdates, their genders, their photos, foreign keys to the parents with the parents' physical addresses as well.

00:49:50.520 --> 00:49:55.660
So it was like from a stalker perspective, it was kind of the worst possible thing you can imagine.

00:49:56.320 --> 00:49:58.420
I know, it was like super, super creepy.

00:49:58.420 --> 00:50:01.700
And, you know, like this was really bad news.

00:50:01.700 --> 00:50:08.420
And someone did break into their systems, but they had some shockingly bad aspects of their security there.

00:50:08.420 --> 00:50:10.400
I mean, stuff like, I wrote a blog post at the time.

00:50:10.400 --> 00:50:11.820
You would log in.

00:50:11.820 --> 00:50:16.660
And when you log in via, they had a little flash emulator for the tablets.

00:50:16.660 --> 00:50:17.820
Okay, there's another hint.

00:50:17.820 --> 00:50:18.700
Some things are going to be wrong.

00:50:18.700 --> 00:50:20.340
So there's a little flash emulator.

00:50:20.340 --> 00:50:20.980
It calls an API.

00:50:21.200 --> 00:50:26.840
The API returns a JSON response, which contains the actual SQL query executed in the database.

00:50:26.840 --> 00:50:29.640
And it's just really, really weird stuff like that.

00:50:29.640 --> 00:50:33.960
And it wouldn't surprise me at all if it was just classic SQL injection that the guy got in with.

00:50:33.960 --> 00:50:37.020
Anyway, they had a bad time out of it.

00:50:37.020 --> 00:50:44.840
And they were in the news only a couple of weeks ago because a class action against them didn't succeed, which frankly, I agree with.

00:50:44.840 --> 00:50:51.560
I think people trying to mount a class action against a company where the data was exposed but contained.

00:50:51.560 --> 00:50:52.380
It was never spread.

00:50:52.380 --> 00:50:53.280
It was never abused.

00:50:53.280 --> 00:50:54.820
It was just a bit of a money grab.

00:50:54.820 --> 00:50:56.800
I think the regulators should be pinging them.

00:50:56.800 --> 00:50:57.600
That's a different story.

00:50:57.600 --> 00:50:59.720
But anyway, so they're in the news.

00:50:59.720 --> 00:51:05.360
And this one story pointed out that their terms and conditions today effectively say you could get hacked.

00:51:05.360 --> 00:51:06.660
Someone could get your data.

00:51:06.660 --> 00:51:08.060
That's your problem, not ours.

00:51:08.460 --> 00:51:13.400
I kind of replied to them or quote tweeted them and said, look, how about you put this on the front of the pack?

00:51:13.400 --> 00:51:15.240
And it was a little bit tongue in cheek, right?

00:51:15.240 --> 00:51:17.980
Because if you put that in the front of the pack, no one's going to buy your freaking tablet.

00:51:17.980 --> 00:51:20.200
And I was thinking about it later.

00:51:20.200 --> 00:51:22.900
I was like, it's almost like it's a dangerous good, you know.

00:51:22.900 --> 00:51:28.560
And in Australia, what we do with dangerous goods like cigarettes is we put great big warning signs on the front of the packs.

00:51:28.560 --> 00:51:29.960
And they're very graphic.

00:51:29.960 --> 00:51:32.560
And they tell you how bad stuff can be when it goes wrong.

00:51:32.560 --> 00:51:34.680
This can kill you in slow and horrible ways.

00:51:34.680 --> 00:51:35.680
Let us list them.

00:51:35.680 --> 00:51:36.120
I know.

00:51:36.320 --> 00:51:38.560
And they're super, super graphic in Australia as well.

00:51:38.560 --> 00:51:41.340
So I thought, all right, well, look, why don't we do this with IoT?

00:51:41.340 --> 00:51:47.400
We'll literally just put these things on the front of these IoT devices like we would a cigarette pack.

00:51:47.400 --> 00:51:51.640
So I just did this blog post with a bunch of mock-ups of what would it look like.

00:51:51.640 --> 00:51:58.080
And it's, you know, there's sort of warnings on everything from the VTech tablets to teddy bears to automated dog feeders.

00:51:58.080 --> 00:52:00.680
And that was a rather popular post.

00:52:00.680 --> 00:52:03.040
Yes.

00:52:03.040 --> 00:52:03.380
Warning.

00:52:03.660 --> 00:52:09.420
You acknowledge and agree that your child's intimate voice recordings may be placed in an unsecure Amazon S3 bucket.

00:52:09.420 --> 00:52:10.600
Oh, cloud pencil.

00:52:10.600 --> 00:52:11.080
Oh, my God.

00:52:11.080 --> 00:52:11.420
Oh, yeah.

00:52:11.420 --> 00:52:12.420
That's pretty funny.

00:52:12.420 --> 00:52:14.240
I think it makes a good point.

00:52:14.240 --> 00:52:17.200
And I guess maybe just a final thought on IoT.

00:52:17.200 --> 00:52:18.660
Like, do you think things are going to get better?

00:52:18.660 --> 00:52:21.680
Or is it just going to continue to be a bunch of unpatched badness?

00:52:21.680 --> 00:52:22.960
No, of course they're not going to get better.

00:52:22.960 --> 00:52:27.260
I mean, there's just nothing that predisposes it to getting better.

00:52:27.780 --> 00:52:31.880
When you look at the factors that are driving the growth of IoT, you know, we want to be first to market.

00:52:31.880 --> 00:52:35.580
We want to put internet in things that were never meant to have internet.

00:52:35.580 --> 00:52:39.800
And if you want to know what I mean by that, just Google WeVibe and we'll leave it at that.

00:52:39.800 --> 00:52:42.600
So there are all sorts of things.

00:52:42.600 --> 00:52:42.740
Wait, wait, wait.

00:52:42.740 --> 00:52:44.620
Do that in an incognito window.

00:52:44.620 --> 00:52:45.920
Look, you know what?

00:52:45.940 --> 00:52:47.160
You'll find news stories on it.

00:52:47.160 --> 00:52:51.120
And let's touch on that in a mature, responsible adult fashion.

00:52:51.120 --> 00:52:55.520
So these are toys for adults and they are internet connected.

00:52:55.520 --> 00:53:03.860
And what I find fascinating from this in a very kind of mature way is that this is digitizing data we never had before.

00:53:03.860 --> 00:53:06.040
So it actually stored usage data.

00:53:06.280 --> 00:53:19.340
And whilst these devices have been around for eons, that the concept of actually recording the use, everything from the modes that was used into the times of day, reference to the identity using it, we never had that before.

00:53:19.340 --> 00:53:26.540
And now we have this new class of data, which is enormously sensitive by virtue of the fact that we've internet connected the things.

00:53:26.540 --> 00:53:35.460
And the reason you'll find them in the news is that they recently got fined up to $10,000 per owner of the device because they were collecting this data without consent.

00:53:35.620 --> 00:53:43.800
And you can imagine for the owners of them, I mean, that must be absolutely gut-wrenching to think that this sort of data about them now exists on a server somewhere.

00:53:43.800 --> 00:53:44.380
Definitely.

00:53:44.380 --> 00:53:46.620
It's quite troubling.

00:53:46.620 --> 00:53:47.520
Yeah.

00:53:47.520 --> 00:53:48.280
So I don't know.

00:53:48.280 --> 00:53:49.260
I think you're probably right.

00:53:49.260 --> 00:53:52.260
I think that there's going to be a lot of trouble with these types of things.

00:53:52.260 --> 00:53:55.980
Just the incentive to keep things updated is not very good.

00:53:55.980 --> 00:54:02.920
I did recently get an electric car and I have a charger for the car that's on the internet, which makes me a little nervous.

00:54:02.920 --> 00:54:07.080
But I logged into it the other day and it had updated itself within the last two weeks.

00:54:07.080 --> 00:54:10.980
So maybe the higher end devices will be a little bit better off.

00:54:10.980 --> 00:54:11.320
Yeah.

00:54:11.320 --> 00:54:15.560
Look, I think the devices that are doing these auto updates are the way we're going, right?

00:54:15.560 --> 00:54:16.960
And some of them do it very well.

00:54:16.960 --> 00:54:20.960
Some of them do it still with requiring user action, but sort of very low friction.

00:54:21.560 --> 00:54:24.200
I hope that's going in the right direction.

00:54:24.200 --> 00:54:31.040
But geez, there's so many things out there where something has gone fundamentally wrong in order to require the update in the first place as well.

00:54:31.040 --> 00:54:31.340
Yeah.

00:54:31.340 --> 00:54:32.240
Yeah, absolutely.

00:54:32.240 --> 00:54:32.640
Yeah.

00:54:32.640 --> 00:54:35.040
I mean, there's the danger that you could brick the device.

00:54:35.040 --> 00:54:42.080
So there's that same type of hesitation to actually put that security patch in like that you have with the big companies, right?

00:54:42.080 --> 00:54:44.640
Like no one wants to brick everything they've sold.

00:54:44.800 --> 00:54:45.360
I know.

00:54:45.360 --> 00:54:47.260
There's also that.

00:54:48.040 --> 00:54:54.860
So, all right, I think we're just, I have so many questions I'd love to chat with you about and maybe someday we'll do a follow up and I can ask the other ones.

00:54:54.860 --> 00:55:03.560
But I do want to give you a chance to talk about your courses because you've written a ton of PlutalSite courses and they're actually really, really valuable, I think.

00:55:03.560 --> 00:55:07.760
So maybe touch on some of the ones you feel are notable for my audience.

00:55:07.760 --> 00:55:13.600
I think the one which is most notable at the moment is the one that's still pinned to my Twitter timeline.

00:55:13.600 --> 00:55:18.020
I do intend to leave it there for a little bit, which is what every developer must know about HTTPS.

00:55:18.020 --> 00:55:24.360
And I love the HTTPS discussion at the moment because there are so many, so many angles to it.

00:55:24.360 --> 00:55:26.360
And it's such an important time as well.

00:55:26.360 --> 00:55:39.600
So, for example, back in January this year, we saw Chrome and Firefox start to warn anyone if they went to a login form or a credit card form over HTTP, even if it publishes or posts rather to HTTPS.

00:55:39.600 --> 00:55:41.540
And that was an important change.

00:55:41.900 --> 00:55:46.380
We're here recording in October and Chrome 62 has just hit.

00:55:46.380 --> 00:55:54.960
And in 62, they are gradually enabling the feature, which now does the same thing for any page with an input form as soon as you type a character.

00:55:54.960 --> 00:56:03.480
So you can go to like CNN, click on the search link, type any character into the search box, and then suddenly you get a big not secure warning.

00:56:03.480 --> 00:56:05.720
So that makes things really, really interesting.

00:56:05.720 --> 00:56:07.400
Like that's really starting to push HTTPS.

00:56:07.840 --> 00:56:16.500
And the sorts of stuff I talk about in the course are things like, I think a lot of people know, you're meant to make sure that all of your embedded resources on the page are done so over a secure connection.

00:56:16.500 --> 00:56:20.680
Otherwise, you lose your padlock and your green text and your secure in Chrome.

00:56:21.240 --> 00:56:23.320
But there are tricks to help you do this.

00:56:23.320 --> 00:56:27.280
So there are things like there is an upgrade in secure requests content security policy.

00:56:27.500 --> 00:56:30.740
So you can add a response header or you can put it in a meta tag.

00:56:30.740 --> 00:56:36.440
And if you accidentally put anything in securely on the page, then the request automatically gets upgraded to a secure one.

00:56:36.440 --> 00:56:37.000
Oh, that's nice.

00:56:37.080 --> 00:56:44.720
So there are all these neat little tricks like that you can do, which make HTTPS so much more easily accessible.

00:56:44.720 --> 00:56:57.300
And a lot of time when I hear people saying I'm having problems because of this or that or whatever, it's like there are usually things like response headers, the CSP, also HSTS to enforce HTTPS connections for all requests.

00:56:57.300 --> 00:56:59.460
You know, those sorts of things are just fantastic.

00:56:59.460 --> 00:57:01.440
And that's what a lot of the course is about.

00:57:01.440 --> 00:57:02.000
Okay, cool.

00:57:02.000 --> 00:57:06.280
I'll definitely link to that and a couple of other ones in the show notes that I thought were pretty cool.

00:57:06.280 --> 00:57:11.600
You also have a nice article on getting started in ethical hacking as a career.

00:57:11.600 --> 00:57:13.040
So that's cool as well.

00:57:13.040 --> 00:57:15.740
Yeah, I guess we're going to have to leave it there for the topics.

00:57:15.740 --> 00:57:17.540
I always have two questions I ask at the end.

00:57:17.540 --> 00:57:19.500
So let me ask them to you now.

00:57:19.500 --> 00:57:23.120
The one of them is if you're going to write some code, what editor do you open up?

00:57:23.120 --> 00:57:24.020
Usually Visual Studio.

00:57:24.020 --> 00:57:25.460
Okay, right on.

00:57:25.460 --> 00:57:29.360
And now normally I ask about libraries and Python.

00:57:29.360 --> 00:57:30.440
You don't do that much Python.

00:57:30.440 --> 00:57:32.140
So I'll give you a pass on that one.

00:57:32.140 --> 00:57:33.580
I'm going to give you a variation on this.

00:57:33.580 --> 00:57:36.920
So what password manager do you use?

00:57:36.920 --> 00:57:37.960
So I'm like you.

00:57:37.960 --> 00:57:40.820
I use the password manager called 1Password.

00:57:40.820 --> 00:57:45.980
Now, I said the password manager called that because if you just say I use 1Password, people are like, what is wrong with you?

00:57:45.980 --> 00:57:46.860
Well, people should be like, what is wrong with you?

00:57:46.860 --> 00:57:47.800
You're using it everywhere.

00:57:47.800 --> 00:57:48.500
What are you thinking?

00:57:48.500 --> 00:57:48.840
I know.

00:57:48.840 --> 00:57:49.440
I know.

00:57:49.440 --> 00:57:51.200
So look, I still use that.

00:57:51.540 --> 00:57:59.160
I'm a little bit agnostic in so far as I think, frankly, so long as you're using a mainstream professional password manager like that or LastPass.

00:57:59.160 --> 00:58:02.400
Honestly, that's going to make your life so much better in so many ways.

00:58:02.400 --> 00:58:04.340
So use one of those things.

00:58:04.340 --> 00:58:04.640
Yeah.

00:58:04.640 --> 00:58:11.180
It definitely makes your blood pressure stay pretty cool when you find that there's been a password breach.

00:58:11.480 --> 00:58:13.300
Yeah, that was 40 characters random.

00:58:13.300 --> 00:58:14.120
I'm going to reset it.

00:58:14.120 --> 00:58:14.740
Not a big deal.

00:58:14.740 --> 00:58:15.280
That's it.

00:58:15.280 --> 00:58:15.600
Yeah.

00:58:15.600 --> 00:58:16.080
Right on.

00:58:16.080 --> 00:58:16.380
Okay.

00:58:16.380 --> 00:58:18.620
Well, Troy, thank you so much for being on the show.

00:58:18.620 --> 00:58:20.800
Any final call to action for everyone listening?

00:58:20.800 --> 00:58:21.100
No.

00:58:21.100 --> 00:58:26.100
Look, I mean, if you want to learn any more, go to TroyHunt.com or find me on the Twitters as Troy Hunt.

00:58:26.100 --> 00:58:26.560
All right.

00:58:26.560 --> 00:58:26.820
Awesome.

00:58:26.820 --> 00:58:28.440
I'll be sure to put those links in the show notes.

00:58:28.440 --> 00:58:29.280
And thanks for being here.

00:58:29.280 --> 00:58:29.800
Good on you.

00:58:29.800 --> 00:58:30.320
Thanks, mate.

00:58:30.320 --> 00:58:30.660
Yeah.

00:58:30.660 --> 00:58:30.780
Bye.

00:58:30.780 --> 00:58:34.320
This has been another episode of Talk Python to Me.

00:58:34.320 --> 00:58:36.840
Today's guest has been Troy Hunt.

00:58:36.840 --> 00:58:39.720
And this episode has been brought to you by Rollbar and GoCD.

00:58:40.640 --> 00:58:42.660
Rollbar takes the pain out of errors.

00:58:42.660 --> 00:58:50.360
They give you the context and insight you need to quickly locate and fix errors that might have gone unnoticed until your users complain, of course.

00:58:50.360 --> 00:58:57.520
As Talk Python to Me listeners, track a ridiculous number of errors for free at rollbar.com slash Talk Python to Me.

00:58:57.520 --> 00:59:02.260
GoCD is the on-premise, open-source, continuous delivery server.

00:59:02.260 --> 00:59:06.420
Want to improve your deployment workflow but keep your code and builds in-house?

00:59:06.420 --> 00:59:12.780
Check out GoCD at talkpython.fm/G-O-C-D and take control over your process.

00:59:12.780 --> 00:59:15.160
Are you or a colleague trying to learn Python?

00:59:15.160 --> 00:59:19.840
Have you tried books and videos that just left you bored by covering topics point by point?

00:59:19.840 --> 00:59:28.400
Well, check out my online course, Python Jumpstart, by building 10 apps at talkpython.fm/course to experience a more engaging way to learn Python.

00:59:28.400 --> 00:59:35.800
And if you're looking for something a little more advanced, try my WritePythonic code course at talkpython.fm/Pythonic.

00:59:36.800 --> 00:59:38.520
Be sure to subscribe to the show.

00:59:38.520 --> 00:59:40.720
Open your favorite podcatcher and search for Python.

00:59:40.720 --> 00:59:41.960
We should be right at the top.

00:59:41.960 --> 00:59:51.260
You can also find the iTunes feed at /itunes, Google Play feed at /play, and direct RSS feed at /rss on talkpython.fm.

00:59:51.260 --> 00:59:53.140
This is your host, Michael Kennedy.

00:59:53.140 --> 00:59:54.500
Thanks so much for listening.

00:59:54.500 --> 00:59:55.580
I really appreciate it.

00:59:55.580 --> 00:59:57.520
Now get out there and write some Python code.

00:59:57.520 --> 01:00:18.240
I really appreciate it.

