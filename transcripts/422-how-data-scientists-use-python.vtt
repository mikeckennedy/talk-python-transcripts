WEBVTT

00:00:00.001 --> 00:00:04.160
Regardless of which side of Python you sit on, software developer or data scientist,

00:00:04.160 --> 00:00:09.700
you surely know that data scientists and software devs seem to have different styles and priorities.

00:00:09.700 --> 00:00:14.480
Why is that? And what are the benefits as well as the pitfalls of this separation?

00:00:14.480 --> 00:00:19.280
That's the topic of this conversation with our guest, Dr. Jody Birchall,

00:00:19.280 --> 00:00:21.620
data science developer advocate at JetBrains.

00:00:21.620 --> 00:00:27.120
This is Talk Python in Me, episode 422, recorded May 31st, 2023.

00:00:27.120 --> 00:00:43.400
Welcome to Talk Python in Me, a weekly podcast on Python.

00:00:43.400 --> 00:00:45.140
This is your host, Michael Kennedy.

00:00:45.140 --> 00:00:52.620
Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython, both on fosstodon.org.

00:00:52.620 --> 00:00:56.200
Be careful with impersonating accounts on other instances, there are many.

00:00:56.640 --> 00:01:01.260
Keep up with the show and listen to over seven years of past episodes at talkpython.fm.

00:01:01.260 --> 00:01:05.300
We've started streaming most of our episodes live on YouTube.

00:01:05.300 --> 00:01:09.020
Subscribe to our YouTube channel over at talkpython.fm/youtube

00:01:09.020 --> 00:01:12.840
to get notified about upcoming shows and be part of that episode.

00:01:12.840 --> 00:01:19.320
This episode is brought to you by JetBrains, who encourage you to get work done with PyCharm.

00:01:19.320 --> 00:01:26.220
Download your free trial of PyCharm Professional at talkpython.fm/done-with-PyCharm.

00:01:26.220 --> 00:01:29.620
And it's brought to you by Prodigy from Explosion AI.

00:01:29.620 --> 00:01:35.600
Spend better time with your data and build better ML-based applications with Prodigy, a radically

00:01:35.600 --> 00:01:37.480
efficient data annotation tool.

00:01:37.480 --> 00:01:43.760
Get it at talkpython.fm/Prodigy and use our code TALKPYTHON, all caps, to save 25%

00:01:43.760 --> 00:01:44.900
off a personal license.

00:01:44.900 --> 00:01:48.020
Jodi, welcome to Talk Python in Me.

00:01:48.020 --> 00:01:48.680
Thank you.

00:01:48.680 --> 00:01:50.660
I am so thrilled to be on the show.

00:01:50.780 --> 00:01:52.320
I'm so thrilled to have you on the show.

00:01:52.320 --> 00:01:56.100
I've been a fan of your work for a while and we got a chance to get to know each other at

00:01:56.100 --> 00:01:57.380
this year's PyCon.

00:01:57.380 --> 00:02:00.320
And so here you are on the podcast as well.

00:02:00.320 --> 00:02:00.820
Thank you.

00:02:00.820 --> 00:02:05.620
We had some very nice Mexican food, actually, or maybe Utah Mexican.

00:02:05.620 --> 00:02:07.920
I don't know quite how I would interpret it.

00:02:07.920 --> 00:02:08.980
It was very good, though.

00:02:08.980 --> 00:02:10.180
It was very good.

00:02:10.180 --> 00:02:11.740
Yeah, the food was excellent.

00:02:11.740 --> 00:02:18.220
I thought the parties were great at the conference and people who are maybe still holding out on

00:02:18.220 --> 00:02:18.560
going.

00:02:18.560 --> 00:02:20.500
Personally, I really enjoyed being there.

00:02:20.500 --> 00:02:24.260
I think it's probably the best conference that I go to yearly.

00:02:24.260 --> 00:02:26.740
And it's like the vibe is so nice.

00:02:26.740 --> 00:02:33.180
On this show, we're going to talk about how data scientists use Python, which is somewhat

00:02:33.180 --> 00:02:38.720
different than maybe a software developer we have, which I guess I'll put myself solidly

00:02:38.720 --> 00:02:39.900
into that camp.

00:02:39.900 --> 00:02:43.640
I do a bunch of web development, make APIs, I build apps and ship them.

00:02:43.640 --> 00:02:45.040
That's quite a different story.

00:02:45.040 --> 00:02:48.280
And I think we're going to have a really great time talking about those things.

00:02:48.480 --> 00:02:51.480
But before we do, let's get a little bit.

00:02:51.480 --> 00:02:54.240
How did you get into programming, Python, data science?

00:02:54.240 --> 00:02:54.640
Yeah.

00:02:54.640 --> 00:02:59.480
So I'm probably going hand in hand with maybe not being a developer.

00:02:59.480 --> 00:03:01.940
My story is perhaps a little unconventional.

00:03:01.940 --> 00:03:04.900
So my background is academic, like a lot of data scientists.

00:03:05.560 --> 00:03:10.760
And unsurprisingly, the first language that I learned was R because I was doing psychology

00:03:10.760 --> 00:03:13.260
and health sciences and a lot of statistics.

00:03:13.260 --> 00:03:17.080
And I was procrastinating once during my PhD.

00:03:17.080 --> 00:03:20.100
You will find any excuse to not work on your thesis.

00:03:20.100 --> 00:03:25.560
And I think I was reading, oh, you know, people who are into statistics, you should really learn

00:03:25.560 --> 00:03:25.980
Python.

00:03:25.980 --> 00:03:26.680
It's the future.

00:03:26.800 --> 00:03:28.180
And I was like, I should learn Python.

00:03:28.180 --> 00:03:29.800
So I sat down and...

00:03:29.800 --> 00:03:32.000
Because I really don't want to write that next chapter.

00:03:32.000 --> 00:03:32.540
I just don't.

00:03:32.540 --> 00:03:32.720
Exactly.

00:03:32.720 --> 00:03:33.420
Exactly.

00:03:33.420 --> 00:03:34.580
So I remember it.

00:03:34.580 --> 00:03:40.000
Like I actually had this long weekend and I worked my way through, I think it was Zed Shaw's

00:03:40.000 --> 00:03:41.180
Learn Python the Hard Way.

00:03:41.180 --> 00:03:43.000
This is showing my age, I think.

00:03:43.440 --> 00:03:44.280
I loved it.

00:03:44.280 --> 00:03:49.060
Like I completed the course in three days and then I didn't know what to do with Python

00:03:49.060 --> 00:03:51.660
because the stats libraries weren't as developed back then.

00:03:51.660 --> 00:03:57.440
So I just put it aside for a couple of years and ended up picking it up again when I started

00:03:57.440 --> 00:04:00.120
working in industry because obviously I've left academia.

00:04:00.120 --> 00:04:05.260
And you sort of fairly quickly, once you start in data science, move away from more sort of

00:04:05.260 --> 00:04:07.120
statistical stuff to machine learning.

00:04:07.120 --> 00:04:09.540
And Python really has libraries for that.

00:04:09.540 --> 00:04:11.240
So that's my journey.

00:04:11.400 --> 00:04:15.180
It's a little bit bibs and bobs and stops and starts.

00:04:15.180 --> 00:04:18.600
But once I kind of picked up Python, it really was love at first sight.

00:04:18.600 --> 00:04:19.680
Oh, that's excellent.

00:04:19.680 --> 00:04:20.860
What's your PhD in?

00:04:20.860 --> 00:04:21.740
It's actually...

00:04:21.740 --> 00:04:22.640
Computer science, of course, right?

00:04:22.640 --> 00:04:24.020
Of course, of course.

00:04:24.020 --> 00:04:25.340
You know, it's so funny.

00:04:25.340 --> 00:04:29.960
You are the third person to ask me in two weeks and no one has asked me this question for like

00:04:29.960 --> 00:04:30.520
two years.

00:04:30.520 --> 00:04:32.380
My PhD was in hurt feelings.

00:04:32.380 --> 00:04:33.180
Hurt feelings?

00:04:33.180 --> 00:04:33.660
Yeah.

00:04:33.660 --> 00:04:34.260
Okay.

00:04:34.260 --> 00:04:35.700
I say this a little bit blithely.

00:04:35.700 --> 00:04:41.220
So my PhD being in psychology, I was really interested in emotions research and

00:04:41.220 --> 00:04:42.080
relationships research.

00:04:42.080 --> 00:04:47.760
So I kind of wanted to see what happens to people emotionally when close relationships

00:04:47.760 --> 00:04:48.880
go bad.

00:04:48.880 --> 00:04:55.120
And it's hurt feelings like things like, you know, infidelities, rejections, all of that.

00:04:55.120 --> 00:04:55.680
It's hurt.

00:04:55.680 --> 00:05:01.520
So I was just studying what generates like and regulates the intensity of hurt and studied

00:05:01.520 --> 00:05:03.900
that for four and a half years.

00:05:03.900 --> 00:05:05.420
Sounds interesting.

00:05:05.420 --> 00:05:07.520
I'm sure there was a lot of data to process.

00:05:07.780 --> 00:05:12.920
There was a lot of data to process and a lot of very interesting statistics.

00:05:12.920 --> 00:05:15.060
That was sort of how I got into data science.

00:05:15.060 --> 00:05:18.880
I fell in love with stats in undergrad and just kept going down that path.

00:05:18.880 --> 00:05:25.160
I think a lot of people are drawn to data science, not with the intent of waking up one

00:05:25.160 --> 00:05:29.680
day and saying, I'm going to be a data scientist, but they're excited or inspired about something

00:05:29.680 --> 00:05:30.440
tangential.

00:05:30.440 --> 00:05:34.920
And they're like, well, I really need to get something better than Excel to work on

00:05:34.920 --> 00:05:35.180
this.

00:05:35.180 --> 00:05:36.000
Absolutely.

00:05:36.000 --> 00:05:36.740
Yeah.

00:05:36.740 --> 00:05:37.000
Yeah.

00:05:37.000 --> 00:05:42.180
And we'll probably talk about this a little bit later about why data scientists use programming.

00:05:42.180 --> 00:05:48.060
And it kind of is like in some ways that need to jump from something way more powerful and

00:05:48.060 --> 00:05:49.300
reproducible than Excel.

00:05:49.300 --> 00:05:49.700
Yeah.

00:05:49.700 --> 00:05:50.020
Yeah.

00:05:50.020 --> 00:05:50.560
For sure.

00:05:50.560 --> 00:05:51.760
So how about now?

00:05:51.760 --> 00:05:53.880
You said you've left academics.

00:05:53.880 --> 00:05:56.120
And what are you doing these days?

00:05:56.260 --> 00:05:56.500
Yeah.

00:05:56.500 --> 00:05:59.740
So that leap from academia was a long time ago now.

00:05:59.740 --> 00:06:02.820
I think that was like seven years again, showing my age.

00:06:02.820 --> 00:06:06.200
So for six of those years, I was a data scientist.

00:06:06.200 --> 00:06:13.180
So day to day was, you know, pretty varied, but the job I have now is very different.

00:06:13.180 --> 00:06:16.280
So I currently work as a developer advocate at JetBrains.

00:06:16.280 --> 00:06:23.340
And the way I would describe my job is I'm a liaison between data scientists and JetBrains.

00:06:23.640 --> 00:06:27.500
So I try and advocate for our tools to be as good as they can be.

00:06:27.500 --> 00:06:31.900
And I try to recommend ways that people can use our tools if I think it's useful.

00:06:31.900 --> 00:06:33.700
But I'm definitely not marketing or sales.

00:06:33.700 --> 00:06:36.380
It's more if I think this is the right fit for you, I'll do it.

00:06:36.380 --> 00:06:41.120
So it's like the way I achieve that is really up to me.

00:06:41.120 --> 00:06:46.740
For me, I really kind of, I like to do a mixture of what I call internal and external

00:06:46.740 --> 00:06:47.320
activities.

00:06:47.320 --> 00:06:51.640
So external activities are actually kind of only tangentially related to the products.

00:06:52.040 --> 00:06:54.160
So this would be an example of an external activity.

00:06:54.160 --> 00:06:58.980
It's just getting out there and educating people about data science or educating data scientists

00:06:58.980 --> 00:07:05.000
about technical topics, things like conference talks or webinars, you know, all this sort of

00:07:05.000 --> 00:07:05.240
stuff.

00:07:05.240 --> 00:07:10.420
And then internal stuff is more focused on maybe things a bit more related to the product.

00:07:10.420 --> 00:07:13.940
So if I think there's a feature that people would be really interested in, I might make a

00:07:13.940 --> 00:07:16.660
video about it or create, you know, a blog post.

00:07:17.180 --> 00:07:19.140
So yeah, it's a real hodgepodge.

00:07:19.140 --> 00:07:26.180
So this week, for example, I've been working on actually a materials for a free workshop that

00:07:26.180 --> 00:07:27.600
they're organizing at Europython.

00:07:27.600 --> 00:07:29.740
So I'm going to be volunteering to help out that.

00:07:29.740 --> 00:07:32.180
It's completely unrelated to anything I'm doing at JetBrains.

00:07:32.180 --> 00:07:33.580
It's just a volunteer activity.

00:07:33.580 --> 00:07:36.580
But last week, I was at a conference, week before that.

00:07:36.580 --> 00:07:38.780
So you can see the job's pretty varied.

00:07:38.780 --> 00:07:40.320
Think developer evangelist.

00:07:40.320 --> 00:07:41.860
It seems like such a fun job.

00:07:42.160 --> 00:07:46.880
You know, I had your colleague, Paul Everett on and we actually talked, it's quite a while

00:07:46.880 --> 00:07:48.260
ago, a couple of years ago, three, four.

00:07:48.260 --> 00:07:54.420
And we had a whole episode on like a panel on what is the developer advocate, developer

00:07:54.420 --> 00:07:56.020
relations job.

00:07:56.020 --> 00:08:01.100
But it's, it just seems like such a great mix of you still get to travel a little bit,

00:08:01.100 --> 00:08:08.120
see people, but you also get to write code and work on, on influencing technology and products

00:08:08.120 --> 00:08:08.580
and stuff.

00:08:08.920 --> 00:08:08.960
Yeah.

00:08:08.960 --> 00:08:13.720
And I think the thing that I started to appreciate about the job a few months in is you have a

00:08:13.720 --> 00:08:18.560
platform with this job and that means you can choose to promote the message that you want.

00:08:18.560 --> 00:08:24.700
And a message that no surprise is very meaningful for me is data science is for everyone.

00:08:24.700 --> 00:08:28.140
Like I hate the gatekeeping that can happen in tech communities.

00:08:28.140 --> 00:08:33.960
I think it's quite bad in terms of like people being intimidated by math in data science.

00:08:33.960 --> 00:08:37.400
And like, I'm here to say to you, if you want it, it's for you.

00:08:37.640 --> 00:08:41.240
It is a very cool field and yeah, it doesn't matter what background you come from.

00:08:41.240 --> 00:08:42.120
I absolutely agree.

00:08:42.120 --> 00:08:46.020
And I was kind of hinting at that saying like a lot of people who don't see themselves as

00:08:46.020 --> 00:08:51.780
developers or programmers, like still find really great places, really great fits in data

00:08:51.780 --> 00:08:53.500
science and in programming as well.

00:08:53.500 --> 00:08:57.320
And I also want to second that I don't think you really need that much math.

00:08:57.320 --> 00:09:03.100
Maybe if you're trying to build the next machine learning model platform, then yes.

00:09:03.200 --> 00:09:03.440
Okay.

00:09:03.440 --> 00:09:05.380
But that's not what most people do.

00:09:05.380 --> 00:09:07.940
They take the data, they clean it up.

00:09:07.940 --> 00:09:11.480
They do interesting visualizations and maybe put it into some framework for production.

00:09:11.480 --> 00:09:11.740
Right?

00:09:11.740 --> 00:09:12.120
Yeah.

00:09:12.120 --> 00:09:17.680
And the nice thing is the field is in such a point where you have so many frameworks or

00:09:17.680 --> 00:09:20.840
tools that will handle a lot of this stuff for you.

00:09:20.840 --> 00:09:24.820
Like I'm not saying you don't need any understanding of what's going on under the hood, but you can

00:09:24.820 --> 00:09:25.920
learn it incrementally.

00:09:26.260 --> 00:09:30.760
A lot of it is like with software development where you develop that smell or that instinct

00:09:30.760 --> 00:09:32.840
for when something is not right.

00:09:32.840 --> 00:09:38.860
That will benefit you more than, you know, knowing how backpropagation works from a calculus

00:09:38.860 --> 00:09:39.380
perspective.

00:09:39.380 --> 00:09:41.480
Like that stuff is maybe a bit too much.

00:09:41.480 --> 00:09:42.720
You don't necessarily need it.

00:09:42.720 --> 00:09:43.020
Yeah.

00:09:43.020 --> 00:09:48.360
Let's get into the main topic and talk a little bit about how does programming and Python differ

00:09:48.360 --> 00:09:52.640
on the data science side than say me as somebody who builds web apps.

00:09:52.640 --> 00:09:53.000
Yeah.

00:09:53.000 --> 00:09:57.940
And maybe we can start by doing an orientation to like, what does a data scientist do?

00:09:57.940 --> 00:09:59.720
Because I think this confuses a lot of people.

00:09:59.720 --> 00:09:59.960
Yeah.

00:09:59.960 --> 00:10:00.200
Yeah.

00:10:00.200 --> 00:10:06.880
So basically the role of a data scientist is to sort of like, like the reason you would

00:10:06.880 --> 00:10:11.860
hire a data scientist is you have a bunch of data and you have an instinct that you can

00:10:11.860 --> 00:10:17.540
use that data to either improve your internal processes or sell some sort of IP.

00:10:17.540 --> 00:10:24.900
So the reason, you know, we differ from BI analysts is BI analysts are doing analysis, but it's

00:10:24.900 --> 00:10:27.220
more about business as usual, which is really important.

00:10:27.220 --> 00:10:29.520
You absolutely need BI analysts.

00:10:29.520 --> 00:10:30.640
How are sales going?

00:10:30.640 --> 00:10:32.800
How much have we made versus last year?

00:10:32.800 --> 00:10:34.460
Like those kinds of charts, right?

00:10:34.460 --> 00:10:36.360
Like absolutely fundamental questions.

00:10:36.360 --> 00:10:40.720
So you need to be a analyst before you need a data scientist, but your data scientists are

00:10:40.720 --> 00:10:44.160
more there to push the envelope in a data driven way.

00:10:44.160 --> 00:10:47.700
So we have two main outputs.

00:10:47.700 --> 00:10:53.140
I would say you can either create an analysis and do a report, or you can build some sort of

00:10:53.140 --> 00:10:54.900
model that will go into production.

00:10:54.900 --> 00:11:01.180
So an example might be as a business, I have an instinct that I can get my customers to buy

00:11:01.180 --> 00:11:05.700
more things based on people like them also buying those things.

00:11:05.700 --> 00:11:10.220
So in that case, your data scientists might be able to build you a recommendation engine

00:11:10.220 --> 00:11:13.580
and this, you know, will have a business outcome.

00:11:13.580 --> 00:11:16.900
Developers obviously, on the other hand, have a very different goal.

00:11:16.900 --> 00:11:20.080
Their goal is to create robust software systems.

00:11:20.080 --> 00:11:27.440
So the concerns that they have are things like latency, server load, downtime, things like that.

00:11:27.440 --> 00:11:29.280
And it's very interesting.

00:11:29.280 --> 00:11:32.680
And we'll talk about it a bit more when we talk about code, if we kind of get into that

00:11:32.680 --> 00:11:38.200
topic, but basically data scientists are not really interested in creating code for the

00:11:38.200 --> 00:11:38.820
long term.

00:11:38.820 --> 00:11:43.720
Whereas the code becomes the product that software developers write often.

00:11:43.720 --> 00:11:48.520
And you have to think about things like legacy systems, because eventually every Greenfield

00:11:48.520 --> 00:11:51.320
project becomes a legacy system if it lasts for long enough.

00:11:51.320 --> 00:11:51.640
Yeah.

00:11:51.640 --> 00:11:52.800
If you're lucky, right?

00:11:52.800 --> 00:11:56.940
Because the alternative is it never really got used or it didn't add that much value and

00:11:56.940 --> 00:11:58.660
got discarded, shut down.

00:11:58.660 --> 00:11:59.080
All right.

00:11:59.080 --> 00:12:03.660
So even though people talk about how much they don't want legacy code or how they kind

00:12:03.660 --> 00:12:07.440
of, you know, don't necessarily want to work on it because they want to work on something

00:12:07.440 --> 00:12:08.420
new and shiny.

00:12:08.420 --> 00:12:12.800
That's kind of the success side of software development, right?

00:12:12.800 --> 00:12:13.840
Yeah, exactly.

00:12:13.840 --> 00:12:20.060
I do think it's super interesting to different life cycle of code on the data science side,

00:12:20.060 --> 00:12:26.860
because you might be just looking to explore a concept or understand an idea better and not

00:12:26.860 --> 00:12:32.340
necessarily ever intend to put it into production in the traditional software sense, right?

00:12:32.340 --> 00:12:40.100
So I've seen some pretty interesting code written that people would look at and go, oh my goodness,

00:12:40.100 --> 00:12:43.660
what is there's not even a function here?

00:12:43.660 --> 00:12:44.580
How is this possible?

00:12:44.760 --> 00:12:46.880
You know, it's like copy and paste, reuse almost.

00:12:46.880 --> 00:12:54.340
And yet it really does go from having no idea to pictures and understanding and then maybe

00:12:54.340 --> 00:12:58.820
handing that off potentially to be written more robustly or better.

00:12:58.820 --> 00:12:59.220
Exactly.

00:12:59.220 --> 00:13:01.400
And it's really interesting.

00:13:01.400 --> 00:13:04.600
Like they're kind of two very different processes.

00:13:04.600 --> 00:13:11.120
And that point actually where engineering and research meets is a very, very interesting one.

00:13:11.180 --> 00:13:14.640
And I've seen it work in multiple different ways in multiple different workplaces.

00:13:14.640 --> 00:13:20.120
So for example, I've worked in places where the data scientists were completely sequestered

00:13:20.120 --> 00:13:21.160
away from the engineers.

00:13:21.160 --> 00:13:25.180
And there really wouldn't be that much discussion between the engineers and the data scientists

00:13:25.180 --> 00:13:29.080
during the research phase, which I do not advise.

00:13:29.080 --> 00:13:33.680
So what that means is the data scientists will come at the end and hand over this chunk of

00:13:33.680 --> 00:13:39.700
perhaps very difficult to read code to an engineer and be like, hey, so we need to implement

00:13:39.700 --> 00:13:40.100
this.

00:13:40.100 --> 00:13:41.680
And the engineer is like, what is this?

00:13:41.680 --> 00:13:42.340
Okay.

00:13:42.340 --> 00:13:44.720
I will schedule that for the next six months.

00:13:44.720 --> 00:13:50.700
And then I've seen, you know, or I've been a data scientist embedded within a software

00:13:50.700 --> 00:13:51.540
development team.

00:13:51.540 --> 00:13:56.300
And in that case, your project is marching in lockstep with what the engineers are doing.

00:13:56.300 --> 00:14:00.960
And from the very start, you know, you've been discussing important things like you need

00:14:00.960 --> 00:14:02.960
to build a model that has latency constraints.

00:14:02.960 --> 00:14:07.280
You need to think about this as the data scientist in terms of like the model that you run,

00:14:07.280 --> 00:14:08.640
but also how it's implemented.

00:14:09.080 --> 00:14:09.140
Right.

00:14:09.140 --> 00:14:10.540
How much memory does it use?

00:14:10.540 --> 00:14:14.240
Because if you run it on your own machine by yourself, then, well, it's kind of the limit

00:14:14.240 --> 00:14:15.780
of your computer sort of sometimes.

00:14:15.780 --> 00:14:20.360
But if you're running a thousand of them concurrently because people are interacting at the website,

00:14:20.360 --> 00:14:22.600
all of a sudden that might make a difference.

00:14:22.600 --> 00:14:23.080
Exactly.

00:14:23.080 --> 00:14:28.240
Or like one of the most interesting problems I think I ever solved in my career was basically

00:14:28.240 --> 00:14:29.900
I was working at a job board.

00:14:29.900 --> 00:14:33.700
We're trying to improve the search using natural language processing.

00:14:34.260 --> 00:14:40.300
So we had this idea that we could build a model that found out the probabilistic relations between

00:14:40.300 --> 00:14:41.840
skills and job titles.

00:14:41.840 --> 00:14:46.300
So if someone typed a skill into the search, we could expand it with job titles and then

00:14:46.300 --> 00:14:51.220
find all of the jobs that we indexed with that at search time and vice versa with job titles

00:14:51.220 --> 00:14:51.720
and skills.

00:14:51.960 --> 00:14:55.160
The thing is we need to find those relations at search time.

00:14:55.160 --> 00:14:57.220
Like that is a very low latency system.

00:14:57.220 --> 00:15:02.180
And it was super interesting because we had to think about how we could search that vector

00:15:02.180 --> 00:15:05.560
space in really, really like quickly.

00:15:05.740 --> 00:15:10.260
Like instead of having to calculate the distance between that and every single vector, we had

00:15:10.260 --> 00:15:12.060
to work out how to do that more efficiently.

00:15:12.060 --> 00:15:17.260
That sort of stuff I really like because it's so applied and it's so like it's this really

00:15:17.260 --> 00:15:20.300
nice intersection between computer science and data science.

00:15:20.300 --> 00:15:21.600
I think it's super cool.

00:15:21.600 --> 00:15:22.100
That is neat.

00:15:22.100 --> 00:15:28.560
One of the things I really like about working with programming broadly is how concrete it is,

00:15:28.560 --> 00:15:28.920
right?

00:15:28.920 --> 00:15:30.040
You came from academics.

00:15:30.040 --> 00:15:33.060
You know, I was in grad school for a while as well.

00:15:33.060 --> 00:15:37.420
And it's, you could debate on and on about a certain idea or concept.

00:15:37.420 --> 00:15:39.240
And it's like, well, you might be right.

00:15:39.240 --> 00:15:44.920
Or I'm here and you push a button and you get the answer or it runs or like, there's a really

00:15:44.920 --> 00:15:49.840
nice feedback of like, I built this thing and it's look, it's really connecting these people.

00:15:49.840 --> 00:15:54.340
And, you know, then it comes down to, can you do it in real time and other things like

00:15:54.340 --> 00:15:54.620
that.

00:15:54.620 --> 00:15:57.140
But that's a really cool aspect of programming.

00:15:57.140 --> 00:16:02.840
This portion of Talk Python To Me is brought to you by

00:16:02.840 --> 00:16:04.440
JetBrains and PyCharm.

00:16:04.440 --> 00:16:09.220
Are you a data scientist or a web developer looking to take your projects to the next level?

00:16:09.220 --> 00:16:11.920
Well, I have the perfect tool for you, PyCharm.

00:16:11.920 --> 00:16:17.000
PyCharm is a powerful integrated development environment that empowers developers and data

00:16:17.000 --> 00:16:21.200
scientists like us to write clean and efficient code with ease.

00:16:21.200 --> 00:16:26.900
Whether you're analyzing complex data sets or building dynamic web applications, PyCharm has

00:16:26.900 --> 00:16:27.540
got you covered.

00:16:27.540 --> 00:16:32.540
With its intuitive interface and robust features, you can boost your productivity and bring your

00:16:32.540 --> 00:16:34.680
ideas to life faster than ever before.

00:16:34.680 --> 00:16:39.500
For data scientists, PyCharm offers seamless integration with popular libraries like NumPy,

00:16:39.500 --> 00:16:40.900
Pandas, and Matplotlib.

00:16:40.900 --> 00:16:46.760
You can explore, visualize, and manipulate data effortlessly, unlocking valuable insights with

00:16:46.760 --> 00:16:48.000
just a few lines of code.

00:16:48.540 --> 00:16:52.680
And for us web developers, PyCharm provides a rich set of tools to streamline your workflow.

00:16:52.680 --> 00:16:58.900
From intelligent code completion to advanced debugging capabilities, PyCharm helps you write clean, scalable

00:16:58.900 --> 00:17:01.660
code that powers stunning web applications.

00:17:01.660 --> 00:17:09.940
Plus, PyCharm's support for popular frameworks like Django, FastAPI, and React make it a breeze to build and deploy your web projects.

00:17:10.500 --> 00:17:14.880
It's time to say goodbye to tedious configuration and hello to rapid development.

00:17:14.880 --> 00:17:16.460
But wait, there's more.

00:17:16.460 --> 00:17:25.120
With PyCharm, you get even more advanced features like remote development, database integration, and version control, ensuring your projects stay organized and secure.

00:17:25.640 --> 00:17:30.580
So whether you're diving into data science or shaping the future of the web, PyCharm is your go-to tool.

00:17:30.580 --> 00:17:32.660
Join me and try PyCharm today.

00:17:32.660 --> 00:17:43.300
Just visit talkpython.fm/done-with-pycharm, links in your show notes, and experience the power of PyCharm firsthand for three months free.

00:17:43.300 --> 00:17:44.460
PyCharm.

00:17:44.460 --> 00:17:46.220
It's how I get work done.

00:17:49.660 --> 00:17:56.700
I think it's kind of a shame that a lot of places do set up their engineering and data science teams so separately.

00:17:56.700 --> 00:18:01.440
Sure, we have quite different roles and we have quite different backgrounds sometimes.

00:18:01.440 --> 00:18:10.660
But I really think that having the two teams at least planning things together, you can really actually learn a lot from each other about how to approach problems.

00:18:10.860 --> 00:18:25.540
When you were describing either having those groups really separated or working really closely together, maybe an analogous relationship that people could relate with is maybe front-end developers and people building the APIs and the back-end, right?

00:18:25.540 --> 00:18:32.340
Like the people doing React or Angular or Vue or whatever it is, you know, and the web design.

00:18:32.340 --> 00:18:36.200
Having those completely separated as well is also, you know, it's terrible.

00:18:36.200 --> 00:18:36.800
Not a good idea.

00:18:36.800 --> 00:18:37.940
It doesn't make any sense.

00:18:38.020 --> 00:18:47.260
And like, I can totally understand it from the point of view of like team composition, because it is, I think, better to have all your data scientists together because they can learn from each other.

00:18:47.260 --> 00:18:52.200
But then I think having, I don't want to use the squad term because I know it's become a little bit unpopular to use it.

00:18:52.200 --> 00:18:56.340
But, you know, this idea of project-oriented teams, I think are quite important.

00:18:56.340 --> 00:19:00.760
Let's dive a little bit more into the research side of things that I want to ask you about.

00:19:00.760 --> 00:19:01.920
Why Python?

00:19:02.300 --> 00:19:10.800
Let's talk about how the research process works and maybe why that results in different priorities and styles of code and styles of engineering.

00:19:10.800 --> 00:19:17.760
It starts at a similar point to all software projects, which is business comes to you and they have some sort of goal.

00:19:17.760 --> 00:19:23.980
Sometimes it's very vague and you need to interpret that and turn it into an executable project.

00:19:23.980 --> 00:19:31.340
But where the sort of uncertainty starts and like where it sort of becomes a research project rather than a project.

00:19:31.340 --> 00:19:45.900
And I don't know if I described that very well, but where it becomes research versus something you're building concretely is even know at the start of a research project, whether it's even possible to answer the question that you're being asked or build the internal product that you're being asked for.

00:19:46.060 --> 00:19:48.460
You might not understand the domain entirely, right?

00:19:48.460 --> 00:19:50.420
You're trying to gain understanding even.

00:19:50.420 --> 00:19:56.960
In the very worst cases, you won't even know if you have the data because maybe your company has so much data and it's so poorly organized.

00:19:56.960 --> 00:20:01.660
Again, something I've seen that you don't even know if the data exists to answer this question.

00:20:01.660 --> 00:20:04.620
So first is going and getting your data.

00:20:04.620 --> 00:20:09.660
And you spend quite a lot of time with the data because the data will be the one that tells you the story.

00:20:09.660 --> 00:20:12.920
It'll tell you whether what you even want is possible.

00:20:12.920 --> 00:20:18.320
And you probably like heard data scientists hammering on about, you know, garbage in, garbage out.

00:20:18.320 --> 00:20:21.620
Like you can build the most beautiful, sophisticated model you want.

00:20:21.620 --> 00:20:27.540
But if you have crap data where there's no signal, you're not going to get anything because it's just not there.

00:20:27.540 --> 00:20:30.060
Like the relationship you're looking for is not there.

00:20:30.060 --> 00:20:30.360
Yeah.

00:20:30.360 --> 00:20:41.000
The side of that I've heard is 80% of the work is actually the data cleanup, data wrinkling, data gathering before you just magically hit it with a plot or something, right?

00:20:41.000 --> 00:20:41.400
Absolutely.

00:20:42.160 --> 00:20:48.660
And it's interesting because that data cleaning, data wrangling step also doesn't happen in one go, especially if you're building a model.

00:20:48.660 --> 00:20:52.860
So what will happen is you'll try something out and you'll be like, okay, it didn't quite work.

00:20:52.860 --> 00:20:56.740
Maybe I need to manipulate the data in a different way or I need to create this new variable.

00:20:56.740 --> 00:20:58.220
And then you'll go again.

00:20:58.220 --> 00:21:05.560
And it's this super iterative process where you have this tightly coupled relationship between both the models and the data.

00:21:06.020 --> 00:21:09.480
So it really is sort of, you know, how I was talking about the instinct.

00:21:09.480 --> 00:21:14.640
This is sort of where that comes in because you're going to spend like 80% of your time honing your skills.

00:21:14.640 --> 00:21:17.440
But it's the most, I think, valuable part of the process.

00:21:17.940 --> 00:21:23.820
And if the signal's there, you can usually get away with using really dumb and simple models.

00:21:23.820 --> 00:21:28.260
You know, things that are unfashionable now like decision trees or linear regression.

00:21:28.260 --> 00:21:33.260
You can get away with them because you've just got such good data that just go with a simpler model.

00:21:33.260 --> 00:21:34.700
It's got all the advantages.

00:21:35.240 --> 00:21:42.480
This is sort of, I think, what makes it different that you're sort of moving towards a goal, but you don't know what that goal is.

00:21:42.480 --> 00:21:44.160
Estimation's always hard, right?

00:21:44.160 --> 00:21:51.920
What I found is best is really just time boxing each step, seeing if you are up to where you thought you'd be up to by a certain point.

00:21:51.920 --> 00:22:02.600
And if not, you need to just keep having those discussions with the business stakeholders because otherwise they're going to not be very happy if you've spent six months just looking at something and you have nothing.

00:22:02.600 --> 00:22:03.620
Or what have you built?

00:22:03.760 --> 00:22:06.660
Well, I have some notebooks I could show you.

00:22:06.660 --> 00:22:09.840
I have 40,000 notebooks and they're all terrible.

00:22:09.840 --> 00:22:11.120
Yeah.

00:22:11.120 --> 00:22:15.060
Speaking of the data, Diego out in the audience has an interesting question.

00:22:15.060 --> 00:22:17.980
How big are the data sets businesses will bring to you typically?

00:22:17.980 --> 00:22:20.920
Enough that you don't need to go out and find more data.

00:22:20.920 --> 00:22:21.880
This is a good question.

00:22:21.880 --> 00:22:25.200
So I hate to be, it depends, you know.

00:22:25.200 --> 00:22:31.260
I get to say that though because, you know, I was a lead data scientist, so I earned that rank.

00:22:31.580 --> 00:22:33.700
It really does depend on the problem you need to solve.

00:22:33.700 --> 00:22:40.180
So typically business will have enough data to cover at least some of the use cases.

00:22:40.180 --> 00:22:48.920
So to give you a really concrete example, this job board that I was talking about that I worked at, we actually had like a bunch of different job boards across Europe.

00:22:49.140 --> 00:22:53.040
So we had some that were a lot bigger like Germany or the UK.

00:22:53.040 --> 00:22:56.480
And we had some that were really small like Poland or Spain.

00:22:56.480 --> 00:23:02.760
And we wanted to build these multi-language models or models maybe for different languages.

00:23:02.760 --> 00:23:03.980
We played around with both.

00:23:03.980 --> 00:23:09.480
And I don't think we really had enough data to support the models in these smaller languages.

00:23:09.480 --> 00:23:12.880
So the models were just not as good quality because we didn't have enough data.

00:23:12.880 --> 00:23:14.580
But for the bigger languages, we did.

00:23:14.580 --> 00:23:22.040
And then it sort of becomes a case of, okay, well, we have more data for these particular websites because they're the ones that are bringing the most revenue.

00:23:22.040 --> 00:23:27.620
So then it sort of became like, well, okay, maybe it's good enough that we improve the search on the most important ones.

00:23:27.620 --> 00:23:30.720
And for the smaller ones, we just wait until we accumulate more data.

00:23:31.140 --> 00:23:37.060
So yeah, most of the time I found that there's a way to make it work for at least part of the solution.

00:23:37.060 --> 00:23:44.480
And then sometimes, like in the case of my last job, we had something like 170 billion auctions per day.

00:23:44.480 --> 00:23:48.200
So we had so much data.

00:23:48.200 --> 00:23:50.440
We even had problems like processing it.

00:23:50.440 --> 00:23:52.100
So sometimes, you know.

00:23:52.100 --> 00:23:56.740
That's the other side of the story is when you've got too much and then how do you throw it away, right?

00:23:56.840 --> 00:24:02.180
I mean, you've got this auction story, like another one that comes to mind is the Large Hadron Collider.

00:24:02.180 --> 00:24:04.000
Oh, yes, yes.

00:24:04.000 --> 00:24:18.080
They've got layers and layers of like chips on hardware and then chips or machines right next to the collectors and then on it, where it's all about how do we throw away, you know, terabytes of data down to get it to megabytes per second, right?

00:24:18.080 --> 00:24:25.640
Yeah, and it's interesting because what you can end up in this within those situations is even then you can have underrepresented groups.

00:24:26.000 --> 00:24:31.460
So, for example, we had, we're working with advertisers and apps, you know, basically trading ads.

00:24:31.460 --> 00:24:42.300
And we ended up with some apps that were just so small that you were like, even with all this data, I really don't have enough to represent this particular combination in this country.

00:24:42.300 --> 00:24:43.260
Interesting.

00:24:43.260 --> 00:24:44.320
Very interesting.

00:24:44.320 --> 00:24:46.520
So, why Python?

00:24:46.520 --> 00:24:53.120
You know, you started out in R and, of course, any distraction from writing a PhD is a good distraction.

00:24:53.120 --> 00:24:57.720
But I do think there's been a really interesting sort of graph.

00:24:57.720 --> 00:25:02.200
Like, if people go and look at, what is it, Stack Overflow Insights.

00:25:02.200 --> 00:25:12.440
If you go look at Stack Overflow Insights, they had a really great graph that shows you the popularity of Python over time.

00:25:12.840 --> 00:25:15.640
And there's just this huge inflection point around 2012.

00:25:15.640 --> 00:25:21.440
And I feel like that's when a lot of the data science libraries really came around and took off.

00:25:21.440 --> 00:25:25.140
It seems like there was a big inflection at one point, but, you know, why?

00:25:25.780 --> 00:25:31.200
To be honest, I can talk about why I like Python from my background.

00:25:31.200 --> 00:25:34.380
I couldn't really tell you exactly what caused that takeoff.

00:25:34.380 --> 00:25:38.420
But, you know, apart from, you know, this idea that the libraries were maturing enough.

00:25:38.420 --> 00:25:47.120
But the thing is, looking even at current surveys, around 60% of data scientists do not have a software development or a software engineering background.

00:25:47.720 --> 00:25:52.200
So, for people like us, we don't really understand, like, it sounds terrible.

00:25:52.200 --> 00:25:56.940
We don't really understand basic constructs in how a programming language works.

00:25:56.940 --> 00:26:03.660
And that can actually mean that going to some sort of compiled language even can be quite a steep learning curve.

00:26:03.660 --> 00:26:03.960
Sure.

00:26:03.960 --> 00:26:05.760
Pointers to pointers, for example.

00:26:05.760 --> 00:26:07.380
Like, oh, no things.

00:26:07.380 --> 00:26:08.020
Yeah.

00:26:08.020 --> 00:26:12.660
Or having to deal with the fact that in Java, everything is a class.

00:26:12.660 --> 00:26:15.020
You're just like, what is this?

00:26:15.020 --> 00:26:18.160
But of course, you understand why if you have that background.

00:26:18.160 --> 00:26:22.740
But if you're trying to learn it yourself, you then have a lot of background you need to cover.

00:26:22.740 --> 00:26:26.480
But in Python and in R as well, you don't need to cover those things.

00:26:26.480 --> 00:26:28.360
It's super easy to prototype.

00:26:28.360 --> 00:26:29.740
It's super easy to script.

00:26:29.740 --> 00:26:34.400
The flexibility of Python is what makes it, I think, the perfect prototyping language.

00:26:34.400 --> 00:26:36.060
And that's essentially what you're doing.

00:26:36.060 --> 00:26:36.920
You're prototyping.

00:26:36.920 --> 00:26:41.640
So, we talked a little bit earlier about, like, why not just Excel?

00:26:41.640 --> 00:26:42.720
We didn't quite say that.

00:26:42.720 --> 00:26:45.380
But this was sort of what we were maybe getting at.

00:26:45.380 --> 00:26:48.440
And, yeah, we could do some of our work in Excel.

00:26:48.440 --> 00:26:49.780
I've tried this.

00:26:49.780 --> 00:26:56.300
And first, I can tell you Excel really starts to struggle when you have too many calculations going on under the hood.

00:26:56.300 --> 00:26:57.800
It gets very, very slow.

00:26:58.120 --> 00:27:02.780
But to be honest, it's sort of just cleaner to code this sort of logic.

00:27:02.780 --> 00:27:06.620
It's much more reproducible when you need to do this iterative sort of stuff.

00:27:06.620 --> 00:27:10.880
And it also means that you can use much more powerful tools.

00:27:10.880 --> 00:27:16.500
So, you can, say, use APIs that developers have made to process your data.

00:27:16.500 --> 00:27:18.160
You can use powerful data.

00:27:18.160 --> 00:27:18.760
Right?

00:27:18.760 --> 00:27:21.960
Like, I need live currency conversion data.

00:27:21.960 --> 00:27:22.500
Right?

00:27:22.500 --> 00:27:23.720
So much easier than in Excel.

00:27:24.400 --> 00:27:25.420
Yes, yes, yes, exactly.

00:27:25.420 --> 00:27:27.880
Like, you can, like, scrape data.

00:27:27.880 --> 00:27:30.740
Or you can, yeah, pull data in off an API.

00:27:30.740 --> 00:27:40.540
Or you can use powerful tools like Spark to process 170 billion auctions per day in order to reduce it down to something manageable.

00:27:41.060 --> 00:27:42.980
So, yeah, it just gives you a lot more power.

00:27:42.980 --> 00:27:49.760
But at the same time, why we use programming languages is just such a different focus.

00:27:50.220 --> 00:27:52.040
It's a bit overkill to use something like Java.

00:27:52.040 --> 00:27:54.960
I know some people do do natural language processing in Java.

00:27:54.960 --> 00:27:58.700
But that's more on the engineering side to build maintainable systems.

00:27:58.700 --> 00:28:08.540
One of the things I like to say when thinking about how people who are coming from a tangential interest like biology or whatever is you can be really effective with Python.

00:28:08.540 --> 00:28:14.200
And I suspect R as well with a really partial understanding of what Python is and what it does.

00:28:14.200 --> 00:28:14.580
Right?

00:28:14.580 --> 00:28:19.480
Like you pointed out, you don't even have to know what a class is or even really how to create a function.

00:28:19.480 --> 00:28:26.520
You just, I can put these six magical incantations in a file and then I can do way more than I otherwise could.

00:28:26.520 --> 00:28:26.760
Right?

00:28:27.100 --> 00:28:28.040
Then you learn one more.

00:28:28.040 --> 00:28:30.600
You make it better and better as you kind of gain experience.

00:28:30.600 --> 00:28:31.060
Pretty much.

00:28:31.060 --> 00:28:31.980
And this is where I started.

00:28:31.980 --> 00:28:36.300
Like, obviously, I learned what functions and classes were when I first started programming.

00:28:36.300 --> 00:28:40.340
But in the end, you will just, maybe it's not the best thing.

00:28:40.340 --> 00:28:42.640
And we can sort of maybe get into this.

00:28:42.640 --> 00:28:50.760
I suppose part of the confusion or not confusion, but internal debate I've had over the years is how good does data science code really need to be?

00:28:50.760 --> 00:28:56.800
Like, how much would data scientists benefit from knowing more about computer science topics?

00:28:56.800 --> 00:28:59.800
Or software engineering topics maybe more to the point?

00:28:59.800 --> 00:29:03.520
And, you know, because like the thing is, every field has so much to learn.

00:29:03.520 --> 00:29:06.560
Don't even get started on what's happening with large language models at the moment.

00:29:06.560 --> 00:29:07.540
Like, it's just overwhelming.

00:29:07.540 --> 00:29:11.980
Should we take some of our precious time and learn software engineering concepts?

00:29:12.100 --> 00:29:12.660
I'm not sure.

00:29:12.660 --> 00:29:14.280
Like, I'm not sure if I have the answer to that.

00:29:16.700 --> 00:29:22.220
This portion of Talk Python Army is brought to you by Prodigy, a data annotation tool from Explosion AI.

00:29:22.220 --> 00:29:26.200
Prodigy is created by Ines Montani and her team at Explosion.

00:29:26.200 --> 00:29:30.000
And she's been doing machine learning and NLP for a long time.

00:29:30.000 --> 00:29:32.520
Ines is a friend and frequent guest on the show.

00:29:32.520 --> 00:29:36.760
And if you've listened to any of her episodes, you know that she knows her ML tools.

00:29:37.300 --> 00:29:38.580
So what is Prodigy?

00:29:38.580 --> 00:29:46.980
It's a modern, scriptable annotation tool for machine learning developers made by the team behind the popular NLP library, spaCy.

00:29:46.980 --> 00:29:57.380
You can easily and visually annotate and develop data for named entity recognition, text classification, span, categorization, computer vision, audio, video, and more,

00:29:57.740 --> 00:30:00.680
and put your model in the loop for even faster results.

00:30:00.680 --> 00:30:09.240
After collecting data, you can quickly train and export a custom spaCy model or download annotations to use it with any other library or framework.

00:30:09.240 --> 00:30:13.640
Prodigy is entirely scriptable in Python, of course, the language we all love.

00:30:13.640 --> 00:30:16.700
And it seamlessly integrates with your favorite libraries and tools.

00:30:16.700 --> 00:30:23.620
Plus, the new alpha version they just released also introduces a built-in support for large language models,

00:30:23.700 --> 00:30:29.520
such as OpenAI's GPT models, and new tools for dividing up your data between multiple annotators.

00:30:29.520 --> 00:30:33.520
Talk Python listeners get a massive discount on a lifetime license.

00:30:33.520 --> 00:30:38.740
You'll get 25% off using the discount code Talk Python, but don't wait too long.

00:30:38.740 --> 00:30:40.160
This offer does expire.

00:30:40.160 --> 00:30:49.780
Get Prodigy at talkpython.fm/Prodigy and use our code Talk Python, all caps, to save 25% off a personal license.

00:30:49.780 --> 00:30:52.320
This link is in your podcast player show notes.

00:30:52.960 --> 00:30:55.160
Thank you to Explosion AI for sponsoring the show.

00:30:55.160 --> 00:31:02.300
I think it really depends on what kind of data scientist you are.

00:31:02.300 --> 00:31:06.200
If what you are is someone doing research, as you described before,

00:31:06.200 --> 00:31:13.040
and you're like, is there a trend between the type of device that they use to buy their thing at our store

00:31:13.040 --> 00:31:17.660
and how much they're buying on the second, you know, how much are likely to come back?

00:31:17.660 --> 00:31:22.460
Like, if they're using an iPhone, do they tend to spend more than if they're using an Android?

00:31:22.600 --> 00:31:24.700
And is that a thing that we should consider?

00:31:24.700 --> 00:31:31.560
Or, you know, is there any, like that kind of exploration, which you can judge whether or not you should make that exploration.

00:31:31.560 --> 00:31:33.580
But just put that aside for a minute.

00:31:33.580 --> 00:31:38.180
That kind of stuff, like once you know that answer, maybe you don't need to run that code again.

00:31:38.180 --> 00:31:38.840
Maybe you don't care.

00:31:38.920 --> 00:31:41.880
You just want to kind of discover if there is a trend.

00:31:41.880 --> 00:31:46.960
And there, maybe you need to know software engineering techniques, but should you be writing unit tests for that?

00:31:46.960 --> 00:31:48.600
I'd say maybe not, honestly.

00:31:48.600 --> 00:31:56.240
On the other hand, if your job is to create a model that's going to go into production, that's going to run behind a Flask or FastAPI endpoint,

00:31:56.740 --> 00:32:02.380
then you're kind of in the realm of continuously running for many people over a long time.

00:32:02.380 --> 00:32:04.840
And I think that really is a different situation.

00:32:04.840 --> 00:32:08.540
I think this is where you actually move from data science to machine learning engineering.

00:32:08.540 --> 00:32:10.880
This term has a lot of different definitions.

00:32:11.380 --> 00:32:15.620
For me, I base my definition of ML engineers on the two people that I've worked with,

00:32:15.620 --> 00:32:23.140
who were like true full stack kind of people who could go from research and prototyping to deployment.

00:32:23.140 --> 00:32:30.620
And they were data scientists who really cared enough to actually learn how to do proper engineering,

00:32:30.620 --> 00:32:32.720
and they could actually deploy their own things.

00:32:32.720 --> 00:32:39.080
But then this leads to another one of my very favorite topics, which is who is responsible for apps in production?

00:32:39.780 --> 00:32:41.840
And here's the thing.

00:32:41.840 --> 00:32:47.000
So I think as good as your data scientist is going to be, or your ML engineer, let's say an ML engineer,

00:32:47.000 --> 00:32:49.400
let's say that they can actually deploy their own code.

00:32:49.400 --> 00:32:56.240
If they're then responsible for that code in production, that then eats up the time that they can be prototyping

00:32:56.240 --> 00:32:58.040
and researching new things for you.

00:32:58.040 --> 00:33:02.560
So the conclusion I've come to over time, and again, this is a matter of a debate.

00:33:02.560 --> 00:33:04.080
This is just my opinion.

00:33:04.080 --> 00:33:09.600
Basically, I think if your company is above any sort of level of size or complexity,

00:33:09.600 --> 00:33:16.100
in terms of the data products it has, I think you really do need dedicated data science and engineering teams.

00:33:16.100 --> 00:33:20.420
Because in the end, no matter how good your data scientist code is going to be,

00:33:20.420 --> 00:33:23.540
it needs to be implemented by the person who's going to maintain it.

00:33:23.540 --> 00:33:26.620
And maybe they're not the ones writing the code from scratch.

00:33:26.620 --> 00:33:29.500
Maybe they can adapt the data scientist code if it's good enough.

00:33:29.500 --> 00:33:33.800
But in the end, they need to be comfortable and familiar enough with that code to be like,

00:33:33.880 --> 00:33:38.960
yeah, if I get pinged at three in the morning, I'm okay knowing what to do with this code.

00:33:38.960 --> 00:33:40.120
Yeah, that's a good point.

00:33:40.120 --> 00:33:40.380
Yeah.

00:33:40.380 --> 00:33:45.160
So I think it's just easier to scale these teams in parallel rather than trying to hire this like

00:33:45.160 --> 00:33:47.480
wall in one person who can do everything.

00:33:47.480 --> 00:33:48.500
They're impossible to hire.

00:33:48.500 --> 00:33:51.020
Like I've only ever met two over the course of my career.

00:33:51.020 --> 00:33:55.480
And quickly, they become overwhelmed by having to maintain projects.

00:33:55.480 --> 00:33:55.760
Right.

00:33:55.820 --> 00:33:57.160
Is that the best use of their time?

00:33:57.160 --> 00:33:57.580
Yeah.

00:33:57.580 --> 00:34:01.740
And like, it's maybe it's not necessarily even if it's the best use of their time.

00:34:01.740 --> 00:34:05.100
It's more like, then who's going to do your research?

00:34:05.100 --> 00:34:09.860
Because now you've used up that resource on maintaining two or three projects.

00:34:09.860 --> 00:34:10.220
Right.

00:34:10.220 --> 00:34:10.700
Absolutely.

00:34:10.700 --> 00:34:13.880
Chris May has got an interesting question out here in the audience.

00:34:13.880 --> 00:34:15.720
This kind of turns us on its head a little bit.

00:34:15.720 --> 00:34:19.420
It says, development teams tend to work better when they focus on writing and refactoring code

00:34:19.420 --> 00:34:21.480
to make it testable and understandable.

00:34:21.960 --> 00:34:26.800
And we've talked a little bit about maybe stuff that data scientists shouldn't care about

00:34:26.800 --> 00:34:27.060
or whatever.

00:34:27.060 --> 00:34:33.520
So he has other ideas that are like good practices for data scientists and teams of them.

00:34:33.520 --> 00:34:35.000
This is actually a really great question.

00:34:35.000 --> 00:34:40.660
So basically, it's an interesting thing with data scientists that unlike software developers,

00:34:40.660 --> 00:34:47.680
we often tend to work alone on projects or maybe in very, very small teams, like maybe two

00:34:47.680 --> 00:34:48.540
or three people.

00:34:48.540 --> 00:34:53.300
And I think it's probably a hangover from the fact a lot of us are ex-academics.

00:34:53.300 --> 00:34:57.980
We're just used to having like, it's not great, but it's...

00:34:57.980 --> 00:35:01.520
A whiteboard, an office in the corner, and no one knows what you're doing.

00:35:01.520 --> 00:35:02.160
Exactly.

00:35:02.160 --> 00:35:03.500
And no one cares.

00:35:03.500 --> 00:35:07.080
That paper that three people read took me three years.

00:35:07.460 --> 00:35:13.600
So what I think has been neglected, you know, aside from learning software engineering best

00:35:13.600 --> 00:35:18.200
practices is more fundamental things, which is like writing maintainable code.

00:35:18.200 --> 00:35:23.420
And I don't mean maintainable in the sense of it's a system that needs to be able to run

00:35:23.420 --> 00:35:23.820
regularly.

00:35:23.820 --> 00:35:29.240
It's more like this is a piece of code that I can come back to in six months and understand

00:35:29.240 --> 00:35:30.320
what I was doing.

00:35:30.560 --> 00:35:34.540
Because, you know, research projects can be shelved forever, but maybe they need to

00:35:34.540 --> 00:35:37.040
be revisited and, you know, built upon.

00:35:37.040 --> 00:35:43.120
So this was actually a topic I got really interested when I first moved to industry, like this idea

00:35:43.120 --> 00:35:45.840
of reproducibility with data science projects.

00:35:45.840 --> 00:35:50.500
It's about the code, but it's also about things like dependency management, which is notoriously...

00:35:50.500 --> 00:35:51.640
Oh, yes.

00:35:51.640 --> 00:35:56.220
...difficult in Python to get reproducible environments later.

00:35:56.420 --> 00:36:01.700
And even the operating system, if Linux is really dramatically changed over time, then

00:36:01.700 --> 00:36:07.000
maybe your old dependency, you want to keep that one, but it won't run on the new operating

00:36:07.000 --> 00:36:09.880
system or there's a whole spectrum of challenges there.

00:36:09.880 --> 00:36:10.440
Exactly.

00:36:10.440 --> 00:36:11.040
Exactly.

00:36:11.040 --> 00:36:16.680
And it's sort of something that can be solved with using poetry, which is a little bit more

00:36:16.680 --> 00:36:17.240
robust.

00:36:17.240 --> 00:36:19.620
But even then, you've still got it.

00:36:19.620 --> 00:36:23.760
Like it runs on my machine effect where your machine will not be the same machine.

00:36:24.400 --> 00:36:28.820
Recently, there's actually a move towards doing more sort of cloud-based stuff for data

00:36:28.820 --> 00:36:31.180
science, which solves a few of these problems.

00:36:31.180 --> 00:36:35.340
And it also solves the additional problem where data scientists often need to do remote development

00:36:35.340 --> 00:36:36.260
for various reasons.

00:36:36.260 --> 00:36:39.920
Like you need access to GPUs in order to train models.

00:36:39.920 --> 00:36:45.700
So, you know, obviously if you have a server, you have a Docker container, which has, you know,

00:36:45.700 --> 00:36:48.680
environment specifications, you can power up that exact same environment.

00:36:48.680 --> 00:36:51.100
And that actually helps with that reproducibility a lot.

00:36:51.100 --> 00:36:57.200
And then another point, which I think is really important for data scientists and can be neglected

00:36:57.200 --> 00:36:58.820
is literate programming.

00:36:58.820 --> 00:37:00.960
So this is an idea from Donald Knuth.

00:37:00.960 --> 00:37:06.680
And it's this idea that, you know, you should write your code in such a way that it's actually

00:37:06.680 --> 00:37:07.800
understandable later.

00:37:07.800 --> 00:37:14.700
With data science work, it's also that you really need to document a lot of the implicit kind of

00:37:14.700 --> 00:37:18.100
assumptions that you make or decisions that you make as part of the research process.

00:37:18.100 --> 00:37:23.140
And this is one of the reasons, probably a good segue, why Jupyter is so important.

00:37:23.140 --> 00:37:23.680
Yeah.

00:37:23.680 --> 00:37:26.460
Jupyter notebooks are designed to be research documents.

00:37:26.460 --> 00:37:31.160
So this is why you have the markdown cells if you've seen a Jupyter notebook, because it's

00:37:31.160 --> 00:37:36.440
this idea that you really, really need to like document along with the code, the decisions

00:37:36.440 --> 00:37:36.920
that you made.

00:37:36.920 --> 00:37:38.580
Like, why did you choose this sample?

00:37:38.580 --> 00:37:43.040
Why did you decide to create the inputs to your models the way that you did it?

00:37:43.040 --> 00:37:44.440
You need to document all this stuff.

00:37:44.440 --> 00:37:47.060
So yeah, reproducibility is a super interesting topic.

00:37:47.060 --> 00:37:53.240
And I think it's, yeah, something that really needs to be thought about carefully, even if

00:37:53.240 --> 00:37:57.000
you're not collaborating with anyone else, because otherwise your piece of research is going to

00:37:57.000 --> 00:37:59.480
be worthless in three months because you're not going to remember what you did.

00:37:59.480 --> 00:38:01.400
I think notebooks are quite interesting.

00:38:01.400 --> 00:38:02.800
They go a long ways to solving that.

00:38:02.800 --> 00:38:07.520
When used in the right way, you can just jam a bunch of non-understandable stuff in there.

00:38:07.520 --> 00:38:11.620
And it's just, well, now it's not understandable, but it's in a web page instead of in an editor.

00:38:11.620 --> 00:38:18.620
But I think as in, you know, not just programmers, but tech in general, we're just bad at thinking

00:38:18.620 --> 00:38:24.460
about the long-term life cycle of information and compute.

00:38:24.460 --> 00:38:29.000
For example, I got a new heat pump to replace the furnace at my house.

00:38:29.000 --> 00:38:31.540
The manual for it came on a CD drive.

00:38:31.540 --> 00:38:33.520
And I'm like, I don't think I have a CD.

00:38:33.520 --> 00:38:34.760
Where did I put that?

00:38:34.760 --> 00:38:37.520
I went to go dig through closet full of electronics.

00:38:37.520 --> 00:38:39.680
I'm not sure I can read that, right?

00:38:39.680 --> 00:38:43.200
And, you know, CD seems so ubiquitous for so long, right?

00:38:43.200 --> 00:38:48.120
And just simple little mismatches like that just get worse over time.

00:38:48.120 --> 00:38:53.560
It's going to be tough to keep some of this older research and reproducibility around.

00:38:53.560 --> 00:38:54.000
Yeah.

00:38:54.000 --> 00:38:59.260
Like it's super interesting that there are packages I used to use, you know, back when I first

00:38:59.260 --> 00:39:00.780
started in natural language processing.

00:39:01.080 --> 00:39:03.360
Some of them haven't been updated from Python 2.

00:39:03.360 --> 00:39:08.160
So I can't use them anymore because they were just some, probably like a PhD project and

00:39:08.160 --> 00:39:12.180
no one really had the time or energy to maintain it after that person graduated.

00:39:12.180 --> 00:39:16.040
And the person graduated, got a job and doesn't really care that much anymore, potentially.

00:39:16.040 --> 00:39:16.620
Exactly.

00:39:16.620 --> 00:39:17.740
Not enough to keep it going.

00:39:17.740 --> 00:39:18.000
Yeah.

00:39:18.000 --> 00:39:19.600
It's not even necessarily their fault.

00:39:19.600 --> 00:39:20.700
It's just life.

00:39:20.700 --> 00:39:21.520
Yeah.

00:39:21.600 --> 00:39:24.300
Let's talk about some of the libraries and tools.

00:39:24.300 --> 00:39:25.560
You mentioned Jupyter.

00:39:25.560 --> 00:39:29.160
I think Jupyter is one of the absolute cornerstones, right?

00:39:29.160 --> 00:39:33.260
So Jupyter or JupyterLab, what are your thoughts here?

00:39:33.260 --> 00:39:38.440
It's funny, actually, for years, I was just working in Jupyter, playing Jupyter on my computer.

00:39:38.440 --> 00:39:42.080
Maybe give people a quick summary of the differences so who don't know.

00:39:42.080 --> 00:39:42.840
Very good idea.

00:39:43.080 --> 00:39:46.380
So basically, Jupyter is, I suppose you could call it an editor.

00:39:46.380 --> 00:39:52.280
It's basically an interactive document, which you run against a Python kernel, or you can

00:39:52.280 --> 00:39:53.660
run it against different language kernels.

00:39:53.660 --> 00:39:56.560
There are R, there are Julia, there are Kotlin notebooks.

00:39:56.560 --> 00:39:58.860
Should I give my little advertisement for JetBrains?

00:39:58.860 --> 00:40:02.740
Basically, what you can do is you can run code in cell blocks.

00:40:02.740 --> 00:40:05.580
Then you can also create markdown cells in between them.

00:40:05.580 --> 00:40:09.920
And this allows you to basically have, you know, markdown chunks and then cell chunks.

00:40:10.360 --> 00:40:13.480
JupyterLab is hosted remotely.

00:40:13.480 --> 00:40:16.460
So you have basically a bunch of other functionality built in.

00:40:16.460 --> 00:40:20.140
So you can open terminals, you can create scripts, things like that.

00:40:20.140 --> 00:40:25.400
But basically, it's like a little Jupyter ecosystem, which is designed to be remotely hosted.

00:40:25.400 --> 00:40:28.380
And it can be accessed simultaneously by several people.

00:40:28.380 --> 00:40:35.540
So I would say Jupyter is good if you are just starting out and you're dealing with small

00:40:35.540 --> 00:40:36.380
data sets.

00:40:36.900 --> 00:40:42.300
Maybe you're even retrieving things from databases, but you're not saving anything too heavy locally.

00:40:42.300 --> 00:40:46.920
You're not using a huge amount of memory, like maybe unless you got one of those new M2 Macs

00:40:46.920 --> 00:40:48.800
and server in your office.

00:40:48.800 --> 00:40:49.480
So go for it.

00:40:49.480 --> 00:40:49.860
Exactly.

00:40:49.860 --> 00:40:50.400
Yeah.

00:40:50.400 --> 00:40:56.140
JupyterLab, I think, is good if basically you need to access different types of machines.

00:40:56.140 --> 00:40:59.220
So maybe you need to be able to access GPU machines easily.

00:40:59.220 --> 00:41:03.500
You kind of want that remote first experience where you don't have to then connect to a remote

00:41:03.500 --> 00:41:03.960
machine.

00:41:04.360 --> 00:41:07.640
And I have found JupyterLab helpful in the past for sharing.

00:41:07.640 --> 00:41:10.880
But the thing you can't do with JupyterLab is real-time collaboration.

00:41:10.880 --> 00:41:12.540
And that's a bit of a pain in the butt.

00:41:12.540 --> 00:41:17.720
Obviously, since I started at JetBrains, I've kind of, you know, like I'm using our tools

00:41:17.720 --> 00:41:18.820
and I like them a lot.

00:41:18.820 --> 00:41:20.240
Obviously, I wouldn't advocate them.

00:41:20.240 --> 00:41:20.520
Yeah.

00:41:20.520 --> 00:41:25.380
I was going to ask, is this PyCharm, Dataspel, like when you actually do that, are you using

00:41:25.380 --> 00:41:26.660
some of those type of tools?

00:41:26.660 --> 00:41:27.160
I am.

00:41:27.160 --> 00:41:31.720
So I won't turn this into too much of an advertisement for our tools because it's not

00:41:31.720 --> 00:41:32.960
really the point of me being here.

00:41:32.960 --> 00:41:39.060
But we've kind of tried or my teams have tried to solve some of these problems that you might

00:41:39.060 --> 00:41:43.760
have with just using plain Jupyter Notebooks or even working with JupyterLab, maybe a bit

00:41:43.760 --> 00:41:45.840
more like robustly.

00:41:46.300 --> 00:41:49.920
So we have actually three data science projects, products.

00:41:49.920 --> 00:41:52.260
We have PyCharm and Dataspel, which you've mentioned.

00:41:52.260 --> 00:41:57.680
They're desktop IDEs with the ability to connect to remote machines, but they're not really

00:41:57.680 --> 00:42:02.980
collaborative, but they do give you like really like nice experience with using Jupyter, debugging

00:42:02.980 --> 00:42:05.040
and co-completion and all those sort of things.

00:42:05.040 --> 00:42:07.040
We have another one, which is DataLaw.

00:42:07.040 --> 00:42:10.280
And this falls into those managed notebooks that I was talking about.

00:42:10.280 --> 00:42:11.920
It's cloud hosted.

00:42:11.920 --> 00:42:15.660
And the nice thing about DataLaw actually is you can do real-time collaboration.

00:42:16.280 --> 00:42:18.000
So it sort of helps overcome-

00:42:18.000 --> 00:42:18.820
Google Docs style, sort of.

00:42:18.820 --> 00:42:20.560
Yes, it's the same technology actually.

00:42:20.560 --> 00:42:27.360
So yeah, so it's kind of a very interesting thing because there will be times where, you

00:42:27.360 --> 00:42:31.040
know, maybe you're not working on a project with a data scientist, but you need them to

00:42:31.040 --> 00:42:32.240
have a look at your work.

00:42:32.240 --> 00:42:37.640
And when I was working with JupyterLab, what we would do is we would clone the notebook to

00:42:37.640 --> 00:42:40.560
our own folder and then we were in the same environment.

00:42:40.560 --> 00:42:41.360
So it was okay.

00:42:41.360 --> 00:42:43.700
But then you would rerun the whole thing again.

00:42:43.700 --> 00:42:45.780
And sometimes it would be pretty time consuming.

00:42:46.260 --> 00:42:47.880
DataLaw is an alternative to that.

00:42:47.880 --> 00:42:53.080
It may or may not be kind of your style, but it's pretty cool because you can actually

00:42:53.080 --> 00:42:58.360
just invite someone to the same notebook instance that you're in and you're basically hosting

00:42:58.360 --> 00:43:01.720
them and they have access to everything that you've already run.

00:43:01.720 --> 00:43:04.100
So it's like true kind of real time.

00:43:04.100 --> 00:43:04.920
Yeah, that's nice.

00:43:04.920 --> 00:43:09.640
Because sometimes a cell has to run for 30 minutes, but then it has this nice little answer

00:43:09.640 --> 00:43:11.320
and you can work with that afterwards, right?

00:43:11.420 --> 00:43:11.860
Exactly.

00:43:11.860 --> 00:43:17.080
Or you want a model to be available and maybe you haven't saved it or something like this

00:43:17.080 --> 00:43:19.540
is just a way around some of these friction points.

00:43:19.540 --> 00:43:23.940
I want to circle back just really quickly for a sort of testimonial, I guess, out in the

00:43:23.940 --> 00:43:24.260
audience.

00:43:24.260 --> 00:43:29.880
Michael says, I started teaching basic Git, Docker and Python packaging to bioinformatics

00:43:29.880 --> 00:43:30.880
students at UCLA.

00:43:31.420 --> 00:43:33.820
And it's made a huge difference in the handoff.

00:43:33.820 --> 00:43:38.620
But I think for actual projects, you know, I just think as we were talking about what should

00:43:38.620 --> 00:43:40.560
people learn as data science and what they shouldn't.

00:43:40.560 --> 00:43:40.800
Yeah.

00:43:40.800 --> 00:43:44.300
A little bit of the fluency with some of these tools is really helpful.

00:43:44.680 --> 00:43:45.780
I absolutely agree.

00:43:45.780 --> 00:43:50.960
Like I know it can be really overwhelming, especially Git initially for students.

00:43:50.960 --> 00:43:51.720
Git is overwhelming.

00:43:51.720 --> 00:43:52.640
Yeah.

00:43:52.640 --> 00:43:52.980
At first.

00:43:52.980 --> 00:43:57.180
I would say I'm because I tend to work on things by myself.

00:43:57.180 --> 00:43:57.780
Yeah, yeah.

00:43:57.780 --> 00:44:02.300
This sort of falls into the reproducibility stuff that I was talking about earlier.

00:44:02.300 --> 00:44:04.980
And it's super, super important.

00:44:04.980 --> 00:44:08.780
Like and once you get comfortable with like just basic use of these tools, you can get really

00:44:08.780 --> 00:44:09.180
far.

00:44:09.180 --> 00:44:09.500
Okay.

00:44:09.500 --> 00:44:12.720
Back to some of the tools, Jupyter, JupyterLab.

00:44:12.720 --> 00:44:14.380
What about JupyterLite?

00:44:14.460 --> 00:44:16.180
Have you played with JupyterLite any?

00:44:16.180 --> 00:44:21.520
Only a teeny tiny bit because of this workshop that I'm going to be helping out with at Europython.

00:44:21.520 --> 00:44:24.940
So they're going to be running the whole thing in JupyterLite, hopefully.

00:44:24.940 --> 00:44:27.980
A couple of bugs to solve, but I think they're overcomable.

00:44:27.980 --> 00:44:33.060
But yeah, it's a really interesting alternative to Google Colab, actually.

00:44:33.060 --> 00:44:33.460
Yeah.

00:44:33.460 --> 00:44:40.800
JupyterLite, take Pyodide, which is CPython running a WebAssembly, and then build a bunch

00:44:40.800 --> 00:44:44.240
of the data science libraries like Matplotlib and stuff in web

00:44:44.240 --> 00:44:44.620
WebAssembly.

00:44:44.620 --> 00:44:50.740
And then the benefit is you don't need a complex server to handle the compute and run arbitrary

00:44:50.740 --> 00:44:52.420
Python code, which is a little sketchy.

00:44:52.420 --> 00:44:55.540
You just run it on the front end in WebAssembly, which is pretty cool.

00:44:55.640 --> 00:44:58.880
I interviewed the folks at PySport a little while ago.

00:44:58.880 --> 00:45:08.280
And it's just the ability to just take code and run all these different pieces on your front end without worrying about a server, I think is super cool.

00:45:08.280 --> 00:45:09.840
If I get that right or not.

00:45:09.840 --> 00:45:19.880
But anyway, just I think running it on top of people using it on top of the browsers like you do JavaScript is it's an interesting thing to throw into the mix for notebooks.

00:45:20.380 --> 00:45:24.080
Actually, a lot of these projects coming out using Pyodide are really interesting.

00:45:24.080 --> 00:45:26.800
Obviously, PyScript is the big one from last year.

00:45:26.960 --> 00:45:33.000
Yeah, I think PyScript actually has really lots of interesting possibilities beyond just the data science side, right?

00:45:33.000 --> 00:45:39.820
Whereas Pyodide is a little more focused on just, I think, really providing the data science tools on the client side.

00:45:39.960 --> 00:45:41.360
We'll see where PyScript goes.

00:45:41.360 --> 00:45:52.840
If they can make an equivalent of Vue.js or something like that, where people can start building legitimate front end interactive web apps like Airbnb or Google Maps or something.

00:45:52.840 --> 00:45:57.500
But with Python, that's going to unlock something that has been locked away for a really long time.

00:45:57.500 --> 00:46:01.220
With Pyodide, that's like a nine or 10 meg download.

00:46:01.220 --> 00:46:06.300
That's too much for the front end, just for like a public facing site generally at the start of time.

00:46:06.300 --> 00:46:09.480
But they're moving it to MicroPython as an option.

00:46:09.480 --> 00:46:12.860
And that's a couple hundred K, which is like these other front end frameworks.

00:46:12.860 --> 00:46:13.980
So it's very exciting.

00:46:13.980 --> 00:46:17.200
I think that's going to be that's definitely the most exciting thing in that area.

00:46:17.200 --> 00:46:18.660
But all right, back to data science.

00:46:18.660 --> 00:46:19.120
Let's see.

00:46:19.120 --> 00:46:20.160
Where do you want to go next?

00:46:20.160 --> 00:46:21.240
You want to talk Pandas maybe?

00:46:21.240 --> 00:46:26.040
Yeah, let's jump into Pandas, which is the other biggie when you're talking about data science.

00:46:26.040 --> 00:46:32.040
So what Pandas is really important for is it's basically the entry point of you working with your data.

00:46:32.320 --> 00:46:37.600
So it's a library, which basically allows you to work with data frames.

00:46:37.600 --> 00:46:39.240
Data frames are basically tables.

00:46:39.240 --> 00:46:41.820
And from there, you can do data manipulation.

00:46:41.820 --> 00:46:44.780
You can explore your data and visualize it.

00:46:44.780 --> 00:46:49.320
And it also is an entry point to passing your data into models.

00:46:49.320 --> 00:46:51.560
Sometimes it'll need additional transformations.

00:46:51.560 --> 00:46:59.260
But say scikit-learn, which we can talk about in a sec, you can basically pass Pandas data frames directly into scikit-learn models.

00:46:59.260 --> 00:47:14.060
Pandas also, because of its popularity, has kind of opened up this easy access to like grid computing and other types of processing database stuff that you don't really need to learn those tools, but you get to take advantage of.

00:47:14.120 --> 00:47:16.240
And so two things that come to mind for me are Dask.

00:47:16.240 --> 00:47:16.660
Yes.

00:47:16.660 --> 00:47:34.640
It's kind of like a Pandas code, but instead you can actually run this across this cluster of machines or larger than memory or stuff on my personal computer or even just take advantage of all 10 cores on my M2 instead of the one.

00:47:34.640 --> 00:47:35.380
Yes.

00:47:35.380 --> 00:47:36.520
Have you done anything with Dask?

00:47:36.520 --> 00:47:37.240
Are you a fan of it?

00:47:37.240 --> 00:47:42.760
I was kind of there when Dask was new and let's just say they find out a lot of the bugs.

00:47:42.760 --> 00:47:43.800
Yeah.

00:47:43.800 --> 00:47:46.620
So what ended up happening was I ended up learning PySpark instead.

00:47:46.620 --> 00:47:52.380
So I went down a different kind of route, but I think, you know, they solve very similar problems.

00:47:52.380 --> 00:47:55.080
It's just Dask is much more similar to Pandas.

00:47:55.080 --> 00:47:57.680
And so you don't really need to deal with learning.

00:47:57.680 --> 00:48:00.200
It's similar, but it's a new API.

00:48:00.200 --> 00:48:00.700
Yeah.

00:48:00.700 --> 00:48:04.880
Another one that I was thinking of, I just had these guys on the show sort of is Ponder.

00:48:05.180 --> 00:48:07.300
Oh, I have not heard of this.

00:48:07.300 --> 00:48:16.100
So Ponder, they were at startup row at PyCon and they basically built on top of Moden, which is important, Moden.pandas as PD.

00:48:16.100 --> 00:48:23.600
And what it does is it, instead of pulling all the data back and executing the commands on your machine in memory, which maybe that data transfer is huge.

00:48:23.600 --> 00:48:26.460
It actually runs it inside of Postgres and other data.

00:48:26.460 --> 00:48:28.600
And I think PySpark as well.

00:48:28.600 --> 00:48:37.220
Like it translates all these Pandas commands to SQL commands to run inside the database where the data is, which is also a pretty interesting thing.

00:48:37.220 --> 00:48:38.180
That is amazing.

00:48:38.180 --> 00:48:41.180
So, yeah, it's just interpreting the code in a completely different way.

00:48:41.280 --> 00:48:44.400
You can do like query planning and optimize the code.

00:48:44.400 --> 00:48:44.820
Yeah, exactly.

00:48:44.820 --> 00:48:50.260
They said that df.describe is like 300 lines of SQL.

00:48:50.260 --> 00:48:51.460
It's really, really tough.

00:48:51.460 --> 00:48:53.300
But once this thing writes it, then it's good to go.

00:48:53.300 --> 00:48:56.340
And I think the reason I bring this up is like you don't have to write that code.

00:48:56.340 --> 00:48:57.700
You just have to know Pandas.

00:48:58.040 --> 00:49:04.980
And then all of a sudden there's these libraries that will do either grid computing or really complex SQL queries that you don't care about.

00:49:04.980 --> 00:49:05.300
Yes.

00:49:05.300 --> 00:49:06.700
You don't care to write or so on.

00:49:06.700 --> 00:49:12.160
So I think it's Pandas is interesting on its own, but it's almost like a gateway to the broader data science community.

00:49:12.160 --> 00:49:12.880
Agreed.

00:49:12.880 --> 00:49:13.300
Agreed.

00:49:13.300 --> 00:49:20.000
And it's such a de facto, I think, for data analysis now or data manipulation transformation.

00:49:20.780 --> 00:49:23.200
Yeah, like I don't see it going away anytime soon.

00:49:23.200 --> 00:49:26.800
And actually, Pandas 2.0 just came out.

00:49:26.800 --> 00:49:38.200
And instead of being, yeah, instead of Pandas is NumPy under the hood, which is fast, but it's not really equipped to deal with certain kinds of structures like strings.

00:49:38.200 --> 00:49:41.100
Because, you know, it's not really what NumPy is about.

00:49:41.100 --> 00:49:42.460
And also missing values.

00:49:42.460 --> 00:49:44.760
The way that it handles it is pretty janky.

00:49:45.200 --> 00:49:48.260
So, yeah, it's been rewritten with PyArrow under the hood.

00:49:48.260 --> 00:49:48.580
Right.

00:49:48.580 --> 00:49:48.860
Yeah.

00:49:48.860 --> 00:49:51.720
Apparently, the performance is so much better.

00:49:51.720 --> 00:49:54.420
Something I need to sit down and actually try.

00:49:54.420 --> 00:49:57.060
It's been out for like a month and I'm feeling a bit bad.

00:49:57.060 --> 00:49:57.740
But, yeah.

00:49:57.740 --> 00:49:58.760
Yeah, that's cool.

00:49:58.760 --> 00:50:04.560
It probably has support for some of the serialization formats for the back of the term, like, is it Parquet?

00:50:04.560 --> 00:50:06.360
And some of those types of things.

00:50:06.360 --> 00:50:08.540
I think that comes straight out of PyArrow.

00:50:08.540 --> 00:50:09.100
Yeah.

00:50:09.100 --> 00:50:09.400
Yeah.

00:50:09.400 --> 00:50:09.760
Excellent.

00:50:09.760 --> 00:50:14.480
So that kind of brings me to a trade-off I wanted to talk to you about before we get off of Pandas.

00:50:14.780 --> 00:50:18.640
Although it sounds like Pandas 2.0, it makes this less important.

00:50:18.640 --> 00:50:26.320
But, you know, another sort of competitor that came out is Polars, which is a data frame library for Python written in Rust.

00:50:26.320 --> 00:50:30.120
Many of the things are written in Rust these days when they care about performance.

00:50:30.120 --> 00:50:31.180
It's like a big trend.

00:50:31.180 --> 00:50:33.500
It's the new C extensions of Python.

00:50:33.500 --> 00:50:37.500
But this one is supposed to also be way faster than Pandas 1.

00:50:37.500 --> 00:50:41.720
And I think it's also based on PyArrow, amongst other things.

00:50:41.720 --> 00:50:43.180
The details are not super important.

00:50:43.180 --> 00:50:45.540
But more what I wanted to ask you is like, well, here's another way.

00:50:45.540 --> 00:50:47.000
This is a totally different API.

00:50:47.000 --> 00:50:48.580
It doesn't try to be compatible.

00:50:48.580 --> 00:50:49.700
So you've got to learn it.

00:50:49.700 --> 00:50:58.760
So the question is, as a data scientist, as a data science team leader, how should you think about, you know, do we keep chasing the shiny new thing?

00:50:59.040 --> 00:51:07.700
Or do we stick with stuff that one, people know like Pandas, but two, also extends into this broader space as a gateway, as we described?

00:51:07.700 --> 00:51:08.720
Like, what are your thoughts here?

00:51:08.860 --> 00:51:10.100
This is a super interesting question.

00:51:10.100 --> 00:51:18.800
So data scientists in some ways have the luxury of being able to maybe use newer packages faster.

00:51:18.800 --> 00:51:27.280
Because we build these small kind of atomic projects that we can just update to the next library that we feel like using in the next project.

00:51:27.340 --> 00:51:29.140
And maybe we're the only ones who ever look at that code.

00:51:29.140 --> 00:51:29.820
So it's cool.

00:51:29.820 --> 00:51:39.960
The problem is, though, of course, is if someone else needs to look at your code, they are going to need to be able to read it, which is not maybe the biggest problem.

00:51:39.960 --> 00:51:42.880
The biggest problem, of course, is any new library.

00:51:43.080 --> 00:51:47.140
You have less documentation and you have less entries on Stack Overflow.

00:51:47.140 --> 00:51:54.420
So I would say you need to make a tradeoff between the time you're not only going to spend learning it, but also debugging it because it's going to be slower.

00:51:54.420 --> 00:51:57.100
But your ChatGPT doesn't know much about Polis.

00:51:57.100 --> 00:52:03.300
Basically, you're essentially going to need to trade that off against, are you going to see a benefit from that?

00:52:03.300 --> 00:52:09.000
So do you actually have problems with processing your data fast enough?

00:52:09.000 --> 00:52:10.920
If you're working on small data sets, probably not.

00:52:10.920 --> 00:52:14.940
If you're not, then maybe try something pandas or pandas adjacent.

00:52:14.940 --> 00:52:18.460
Yeah, that sort of community support side is important.

00:52:18.460 --> 00:52:25.020
And I'm pretty sure there are a lot of data scientists out there who are the one data scientist at their organization.

00:52:25.020 --> 00:52:31.420
And so it's not like, oh, we'll go ask the other expert down the hall because if it's not you, there's no answer, right?

00:52:31.420 --> 00:52:31.820
Exactly.

00:52:31.820 --> 00:52:34.700
I do think, though, like it's good to be curious.

00:52:34.700 --> 00:52:37.380
It's good to try out new things as well.

00:52:37.380 --> 00:52:41.180
And again, part of being a data scientist is you can experiment a bit more.

00:52:41.180 --> 00:52:51.440
So, you know, 2017, 18, sort of the peak Python 2 versus 3 tension, I guess, maybe one year before then.

00:52:51.660 --> 00:52:55.300
I noticed that the data scientists were like, I don't know what y'all are arguing about.

00:52:55.300 --> 00:52:56.860
We're done with this.

00:52:56.860 --> 00:53:04.120
What we're arguing about is when can we take the Python 2 code out to absolutely 100% drop support for it, not when are we moving over?

00:53:04.120 --> 00:53:12.980
Whereas people running, you know, that Django site that's been around for eight years that's still on Python 2, they're starting to get nervous because they don't want to rewrite it because it works.

00:53:13.320 --> 00:53:14.900
But they know they're going to have to.

00:53:14.900 --> 00:53:23.620
And I feel like, you know, we talked about the legacy code is sort of the success story that is dreaded of software on the computer science side.

00:53:23.620 --> 00:53:31.220
Because that is less of a thing in data science, it's easier to go, well, this next project that we're starting in a couple of months, we can start with newer tools.

00:53:31.220 --> 00:53:36.880
Yep. And I actually remember the point where I decided, okay, this is the last project I'm doing into.

00:53:36.880 --> 00:53:45.100
Because the thing that was keeping me into was actually one of those libraries that I mentioned, which built by a university.

00:53:45.100 --> 00:53:48.060
And I was like, you know what, I'm just going to go find some alternative tool.

00:53:48.060 --> 00:53:56.100
I think at that time, spaCy, which is a very well-known NLP library, actually, based here in Berlin, the company.

00:53:56.100 --> 00:53:58.400
Yeah, exactly. Basically a neighbor of yours.

00:53:58.400 --> 00:54:03.020
That's right. But I think spaCy was really getting off its feet in that time.

00:54:03.020 --> 00:54:06.540
So I was like, you know, I'm just going to switch over to this new library and try that.

00:54:06.540 --> 00:54:09.260
And it's excellent. So I didn't look back.

00:54:09.260 --> 00:54:13.080
Yeah. spaCy's cool. Ines Montani is doing really great work.

00:54:13.080 --> 00:54:14.580
And everyone over at Explosion AI.

00:54:14.580 --> 00:54:18.340
And that's the thing. Sometimes it seems like a hassle, right?

00:54:18.340 --> 00:54:23.860
But if it forces you out of your comfort zone to pick stuff that's being actively developed, maybe it's worth it, right?

00:54:23.860 --> 00:54:24.460
Exactly.

00:54:24.460 --> 00:54:31.600
All right. We're getting short on time. So you want to give us a lightning round and the other important libraries you think data scientists should pay attention to?

00:54:31.600 --> 00:54:35.720
Yeah. So let's just quickly go through the visualization side of things.

00:54:35.720 --> 00:54:37.360
So visualization is massive.

00:54:37.360 --> 00:54:40.300
So matplotlib is really the biggie.

00:54:40.300 --> 00:54:44.220
And it's what a lot of libraries are actually built on top of in Python.

00:54:44.480 --> 00:54:48.960
So the syntax is not that friendly. So there's a lot of alternatives.

00:54:48.960 --> 00:54:51.620
So Seaborn is a very popular one.

00:54:51.620 --> 00:54:56.020
We actually have an internal one called Let's Plot, which is a port of ggplot2.

00:54:56.020 --> 00:54:59.620
And there's another one called Plot9. And I think there actually may even be one called ggplot.

00:54:59.620 --> 00:55:00.520
Plotlib.

00:55:00.520 --> 00:55:06.760
Some of the fancy new ones that people hear about, they're actually internally just controlling matplotlib and a cleaner API, right?

00:55:06.840 --> 00:55:13.180
Pretty much. And let me tell you, matplotlib needs a clean API. It's a bit, let's say, okay.

00:55:13.180 --> 00:55:17.280
Although give it some props for its XKCD graph style.

00:55:17.280 --> 00:55:21.720
I mean, that is pretty cool that you can get it to do that.

00:55:21.720 --> 00:55:27.000
I actually have done, I've done XKCD graphs in Python as well.

00:55:27.000 --> 00:55:31.580
It's a goal that you aim for to do like elite visualizations.

00:55:31.580 --> 00:55:35.520
It's fun. And XKCD is amazing in a lot of ways.

00:55:35.520 --> 00:55:44.400
However, I think it also can serve an important role when you're presenting to like leaders of an organization, non-technical people.

00:55:44.500 --> 00:55:50.380
Because if they look and see a beautiful, pristine production ready, sort of like, we're done.

00:55:50.380 --> 00:55:53.460
Like, no, no, no, this is a product. No, we're done. Look, you already got it.

00:55:53.460 --> 00:56:01.080
But if it comes out in sort of cartoony, kind of like wireframing for UI design, you're like, oh, there are no expectations it's done.

00:56:01.080 --> 00:56:03.860
It's XKCD. We're going to get you the real graphs later, right?

00:56:03.860 --> 00:56:04.580
Yeah, yeah.

00:56:04.580 --> 00:56:05.860
There may be some value there.

00:56:05.860 --> 00:56:10.020
Like a psychological effect where you make it look like a hand-drawn prototype.

00:56:10.020 --> 00:56:12.580
Exactly. It looks just hand-drawn. It's barely done.

00:56:12.580 --> 00:56:13.360
That's right.

00:56:13.360 --> 00:56:14.400
It's really just theme equals.

00:56:14.400 --> 00:56:15.820
It didn't take me two days.

00:56:15.820 --> 00:56:19.400
scikit-learn, you mentioned that before.

00:56:19.400 --> 00:56:24.100
Yes. So there are a whole bunch of libraries for doing machine learning.

00:56:24.100 --> 00:56:27.780
scikit-learn is kind of your all-in-one for classic machine learning.

00:56:27.780 --> 00:56:33.140
But then, you know, you have this whole other branch of data science, which is around neural nets or deep learning.

00:56:33.140 --> 00:56:39.320
So you have Keras, TensorFlow, you have PyTorch.

00:56:39.400 --> 00:56:49.960
And then you have a package for working with a lot of like these generative AI models or large language models called Transformers from a company called Hugging Face.

00:56:50.700 --> 00:56:53.500
So all of these are actually super accessible.

00:56:53.500 --> 00:56:57.900
I wouldn't say TensorFlow and PyTorch can be tricky, but Keras is like a friendly front end for them.

00:56:57.900 --> 00:57:07.760
Actually, if anyone is interested in getting into this side of things, there's a book called Deep Learning in Python by an AI researcher at Google called Francois Cholet.

00:57:08.100 --> 00:57:13.040
It is actually, I think, the most popular book ever on Manning.

00:57:13.040 --> 00:57:15.820
So it's an amazing book.

00:57:15.820 --> 00:57:17.420
I can only recommend it.

00:57:17.420 --> 00:57:20.920
And it's very gentle for beginners who have no background in the area.

00:57:20.920 --> 00:57:22.100
Okay. Yeah, cool.

00:57:22.100 --> 00:57:23.460
I'll put that in the show notes.

00:57:23.460 --> 00:57:23.780
Awesome.

00:57:23.780 --> 00:57:24.240
Yeah.

00:57:24.240 --> 00:57:25.260
All right.

00:57:25.260 --> 00:57:28.320
Well, there are many other things we can talk about.

00:57:28.320 --> 00:57:33.560
Maybe just let's close this out with a quick shout out to your PyCon talk.

00:57:34.160 --> 00:57:40.920
Eventually, someday, I'm sure that the talks for PyCon will be on YouTube.

00:57:40.920 --> 00:57:45.740
They were last year, but I looked back and I was so excited near the end of the conference.

00:57:45.740 --> 00:57:46.820
I'm like, look, the talks are up.

00:57:46.820 --> 00:57:49.080
And I was talking to someone like, look, here's your talk.

00:57:49.080 --> 00:57:50.940
They're like, no, that's my talk from last year.

00:57:50.940 --> 00:57:51.580
I'm like, oh.

00:57:51.580 --> 00:57:52.260
Yeah.

00:57:52.260 --> 00:57:56.120
So it was maybe three or four months delayed until it actually came out.

00:57:56.120 --> 00:58:00.460
So maybe this midsummer or the video Virginia talk will be out.

00:58:00.460 --> 00:58:03.660
But maybe just give people a quick elevator pitch of your talk here.

00:58:03.820 --> 00:58:04.000
Yeah.

00:58:04.000 --> 00:58:10.080
So I decided to give this talk because I kind of had to learn things the hard way in terms

00:58:10.080 --> 00:58:11.300
of performance with Python.

00:58:11.300 --> 00:58:15.180
So basically, I used to do everything with loops.

00:58:15.180 --> 00:58:18.080
And then I had to start working with larger amounts of data.

00:58:18.080 --> 00:58:19.420
And it just doesn't scale.

00:58:19.420 --> 00:58:25.220
So over time, as I got better with Python, I learned more about NumPy, which is another

00:58:25.220 --> 00:58:26.640
important data science library.

00:58:26.640 --> 00:58:30.260
And it basically allows you to do what's called vectorized operations.

00:58:30.620 --> 00:58:36.640
So in this talk, I basically talk about the math behind why vectorized operations work.

00:58:36.640 --> 00:58:38.380
You don't need any math background to understand.

00:58:38.380 --> 00:58:39.160
It's very gentle.

00:58:39.160 --> 00:58:46.760
And then just show why some of these operations work in NumPy and how you can implement it yourself

00:58:46.760 --> 00:58:50.880
to get really massive gains in performance speed.

00:58:50.880 --> 00:58:51.920
Yeah, that's incredible.

00:58:51.920 --> 00:58:56.900
Move a lot of that stuff down into like a C or a Rust layer and just let it do its magic

00:58:56.900 --> 00:58:58.560
instead of looping in Python.

00:58:58.560 --> 00:58:58.960
Yeah.

00:58:58.960 --> 00:58:59.660
Exactly.

00:58:59.660 --> 00:59:00.160
Yeah.

00:59:00.160 --> 00:59:00.540
Very cool.

00:59:00.540 --> 00:59:03.580
So I don't know when, but eventually this will be out as a video.

00:59:03.580 --> 00:59:06.060
People can check out for me now.

00:59:06.060 --> 00:59:07.700
They know to go look for it.

00:59:07.780 --> 00:59:10.980
Yeah, I think the poor team is still recovering so much work.

00:59:10.980 --> 00:59:11.660
I know.

00:59:11.660 --> 00:59:12.840
All right.

00:59:12.840 --> 00:59:15.360
Well, Jodi, it's been great to have you on the show.

00:59:15.360 --> 00:59:17.800
Before you get out of here, final two questions.

00:59:17.800 --> 00:59:20.880
If you're going to write some Python code, what editor are you using these days?

00:59:20.880 --> 00:59:23.860
So I'm actually using all three that I talked about.

00:59:23.860 --> 00:59:29.160
I use PyCharm if I need to do something like a bit more on the engineering side, which is

00:59:29.160 --> 00:59:30.040
not that often for me.

00:59:30.040 --> 00:59:35.740
DataSpell if I'm doing sort of very local development and doing more of the research side.

00:59:35.740 --> 00:59:39.180
And then if I need some GPUs, I'm using DataLaw.

00:59:39.180 --> 00:59:42.580
So a bit boring, but using all of our tools.

00:59:42.580 --> 00:59:43.760
And I really like them.

00:59:43.760 --> 00:59:45.260
Yeah, they are good.

00:59:45.260 --> 00:59:45.860
All right.

00:59:45.860 --> 00:59:51.780
And then notable PyPI package, something you want to give a shout out to, or if you prefer

00:59:51.780 --> 00:59:54.140
a Conda package, there's a lot of intersection there.

00:59:54.140 --> 00:59:56.580
I think my favorite package at the moment is Transformers.

00:59:56.580 --> 00:59:58.220
It is amazing.

00:59:58.220 --> 01:00:01.280
And the documentation that Hugging Face have put together is so good.

01:00:01.280 --> 01:00:05.960
And just the work they're doing in open data science is so, so important.

01:00:05.960 --> 01:00:08.320
So like big props to Hugging Face.

01:00:08.320 --> 01:00:10.360
We should really support the work that they're doing.

01:00:10.360 --> 01:00:10.740
Excellent.

01:00:10.740 --> 01:00:11.360
All right.

01:00:11.360 --> 01:00:15.020
Well, thanks for being on the show and sharing your experience.

01:00:15.020 --> 01:00:16.200
Thank you so much for having me.

01:00:16.200 --> 01:00:17.480
I had an absolute blast.

01:00:17.480 --> 01:00:18.020
Yeah, same.

01:00:18.020 --> 01:00:18.480
Bye.

01:00:18.480 --> 01:00:18.680
Bye.

01:00:19.880 --> 01:00:22.600
This has been another episode of Talk Python To Me.

01:00:22.600 --> 01:00:24.420
Thank you to our sponsors.

01:00:24.420 --> 01:00:26.020
Be sure to check out what they're offering.

01:00:26.020 --> 01:00:27.440
It really helps support the show.

01:00:27.440 --> 01:00:32.540
The folks over at JetBrains encourage you to get work done with PyCharm.

01:00:32.540 --> 01:00:38.080
PyCharm Professional understands complex projects across multiple languages and technologies,

01:00:38.080 --> 01:00:43.740
so you can stay productive while you're writing Python code and other code like HTML or SQL.

01:00:43.740 --> 01:00:48.880
Download your free trial at talkpython.fm/done with PyCharm.

01:00:48.880 --> 01:00:53.180
Spend better time with your data and build better ML-based applications.

01:00:53.180 --> 01:00:57.820
Use Prodigy from Explosion AI, a radically efficient data annotation tool.

01:00:57.820 --> 01:01:05.380
Get it at talkpython.fm/prodigy and use our code TALKPYTHON, all caps, to save 25% off a personal license.

01:01:06.020 --> 01:01:07.280
Want to level up your Python?

01:01:07.280 --> 01:01:11.320
We have one of the largest catalogs of Python video courses over at Talk Python.

01:01:11.320 --> 01:01:16.500
Our content ranges from true beginners to deeply advanced topics like memory and async.

01:01:16.500 --> 01:01:19.160
And best of all, there's not a subscription in sight.

01:01:19.160 --> 01:01:22.080
Check it out for yourself at training.talkpython.fm.

01:01:22.080 --> 01:01:26.760
Be sure to subscribe to the show, open your favorite podcast app, and search for Python.

01:01:26.760 --> 01:01:28.060
We should be right at the top.

01:01:28.060 --> 01:01:37.440
You can also find the iTunes feed at /itunes, the Google Play feed at /play, and the direct RSS feed at /rss on talkpython.fm.

01:01:37.440 --> 01:01:40.860
We're live streaming most of our recordings these days.

01:01:40.860 --> 01:01:48.640
If you want to be part of the show and have your comments featured on the air, be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

01:01:48.640 --> 01:01:50.540
This is your host, Michael Kennedy.

01:01:50.540 --> 01:01:51.840
Thanks so much for listening.

01:01:51.840 --> 01:01:53.020
I really appreciate it.

01:01:53.020 --> 01:01:54.920
Now get out there and write some Python code.

01:01:54.920 --> 01:02:15.800
I'll see you next time.

