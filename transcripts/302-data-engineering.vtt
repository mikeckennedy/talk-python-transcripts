WEBVTT

00:00:00.001 --> 00:00:03.580
I'm sure you're familiar with data science, but what about data engineering?

00:00:03.580 --> 00:00:06.160
Are these the same thing, or how are they related?

00:00:06.160 --> 00:00:10.120
Data engineering is dedicated to overcoming data processing bottlenecks,

00:00:10.120 --> 00:00:15.120
data cleanup, data flow, and data handling problems for applications that utilize a lot of data.

00:00:15.120 --> 00:00:18.040
On this episode, we welcome back Tobias Macy,

00:00:18.040 --> 00:00:22.400
give us a 30,000-foot view of the data engineering landscape in 2021.

00:00:22.400 --> 00:00:27.960
This is Talk Python To Me, episode 302, recorded January 29, 2021.

00:00:27.960 --> 00:00:47.140
Welcome to Talk Python To Me, a weekly podcast on Python, the language, the libraries, the ecosystem, and the personalities.

00:00:47.140 --> 00:00:51.140
This is your host, Michael Kennedy. Follow me on Twitter, where I'm @mkennedy,

00:00:51.140 --> 00:00:54.860
and keep up with the show and listen to past episodes at talkpython.fm,

00:00:54.860 --> 00:00:58.020
and follow the show on Twitter via at talkpython.

00:00:58.020 --> 00:01:01.600
This episode is brought to you by Datadog and Retool.

00:01:01.600 --> 00:01:05.380
Please check out what they're offering during their segments. It really helps support the show.

00:01:05.380 --> 00:01:07.180
Tobias, you ready to kick it off?

00:01:07.180 --> 00:01:08.960
Yeah, sounds good. Thanks for having me on, Mike.

00:01:08.960 --> 00:01:11.420
Yeah, great to have you here. Good to have you back.

00:01:11.420 --> 00:01:18.320
I was recently looking at my podcast page here, and it says you were on the show 68, which is a lot of fun.

00:01:18.480 --> 00:01:22.680
That was when Chris Paddy was with you as well around Podcasts and Knit.

00:01:22.680 --> 00:01:24.820
But boy, that was 2016.

00:01:24.820 --> 00:01:26.020
Yeah, it's been a while.

00:01:26.020 --> 00:01:31.040
We've been at this a while. I mean, ironically, we started within a week of each other, but yeah, we're still going, both of us.

00:01:31.040 --> 00:01:38.400
Yeah, it's definitely been a fun journey and a lot of great sort of unexpected benefits and great people that I've been able to meet as a result of it.

00:01:38.540 --> 00:01:41.640
So definitely glad to be on the journey with you.

00:01:41.640 --> 00:01:45.900
Yeah, yeah. Same here. Podcasting opened doors like nothing else. It's crazy.

00:01:45.900 --> 00:01:48.380
People who wouldn't normally want to talk to you are like, hey, you want to be on the show?

00:01:48.380 --> 00:01:51.020
Yeah, let's spend an hour together all of a sudden, right? It's fantastic.

00:01:51.020 --> 00:01:54.100
What's new since 2016? What have you been up to?

00:01:54.100 --> 00:01:59.600
Definitely a number of things. I mean, one being that I actually ended up going solo as the host.

00:01:59.600 --> 00:02:07.460
So I've been running the Podcast in It show by myself. I don't remember exactly when it happened, but I think probably sometime around 2017.

00:02:07.460 --> 00:02:12.980
I know around the same time that I was on your show, you were on mine. So we kind of flip-flopped.

00:02:12.980 --> 00:02:18.340
And then you've been on the show again since then, talking about your experience working with MongoDB and Python.

00:02:18.340 --> 00:02:18.820
Yeah.

00:02:18.820 --> 00:02:25.340
Beyond that, I also ended up starting a second podcast. So I've got Podcast in It, which focuses on Python and its community.

00:02:25.700 --> 00:02:31.120
So a lot of stuff about DevOps, data science, machine learning, web development, you name it.

00:02:31.120 --> 00:02:33.600
Anything that people are doing with Python, I've had them on.

00:02:33.600 --> 00:02:37.180
But I've also started a second show focused on data engineering.

00:02:37.180 --> 00:02:41.960
So going beyond just the constraints of Python into a separate niche.

00:02:41.960 --> 00:02:46.160
So more languages, but more tightly focused problem domain.

00:02:46.160 --> 00:02:50.420
And so I've been enjoying learning a lot more about the area of data engineering.

00:02:50.980 --> 00:02:57.220
And so it's actually been a good companion to the two where there's a lot of data science that happens in Python.

00:02:57.220 --> 00:02:59.900
So I'm able to cover that side of things on podcast.init.

00:02:59.900 --> 00:03:05.160
And then data engineering is all of the prep work that makes data scientists' lives easier.

00:03:05.160 --> 00:03:09.960
And so just learning a lot about the technologies and challenges that happen on that side of things.

00:03:09.960 --> 00:03:10.920
Yeah, that's super cool.

00:03:10.920 --> 00:03:15.860
And to be honest, one of the reasons I invite you on the show is because I know people talk about data engineering.

00:03:15.860 --> 00:03:18.320
And I know there's neat tools in there.

00:03:18.320 --> 00:03:21.700
They feel like they come out of the data science space, but not exactly.

00:03:21.700 --> 00:03:25.600
And so I'm really looking forward to learning about them along with everyone else listening.

00:03:25.600 --> 00:03:26.680
So it's going to be a lot of fun.

00:03:26.680 --> 00:03:27.520
Yeah.

00:03:27.520 --> 00:03:31.120
Before we dive into that, let people maybe know, what are you doing day to day these days?

00:03:31.120 --> 00:03:32.060
Are you doing consulting?

00:03:32.060 --> 00:03:33.520
Are you got a full time job?

00:03:33.520 --> 00:03:34.580
Oh, what's the plan?

00:03:34.580 --> 00:03:35.020
Yes.

00:03:35.560 --> 00:03:36.500
Yes to all of that.

00:03:36.500 --> 00:03:41.640
So, yeah, I mean, I run the podcast as a side, just sort of hobby.

00:03:41.640 --> 00:03:51.120
And for my day to day, I actually work full time at MIT in the open learning department and help run the platform engineering and data engineering team there.

00:03:51.120 --> 00:03:59.040
So responsible for making sure that all the cloud environments are set up and secured and servers are up and running and applications stay available.

00:03:59.640 --> 00:04:13.900
And working through building out a data platform to provide a lot of means for analytics and gaining insights into the learning habits and the behaviors that global learners have and how they interact with all of the different platforms that we run.

00:04:13.900 --> 00:04:14.780
That's fantastic.

00:04:14.780 --> 00:04:16.580
Yeah, it's definitely a great place to work.

00:04:16.580 --> 00:04:18.820
I've been happy to be there for a number of years now.

00:04:18.820 --> 00:04:20.720
And then, you know, I run the podcast.

00:04:20.720 --> 00:04:22.160
So those go out every week.

00:04:22.160 --> 00:04:24.440
So a lot of stuff that happens behind the scenes there.

00:04:24.440 --> 00:04:34.720
And then I also do some consulting where lately it's been more of the advisory type where it used to be I'd be hands on keyboard, but I've been able to level up beyond that.

00:04:34.720 --> 00:04:40.780
And so I've been working with a couple of venture capital firms to help them understand the data ecosystem.

00:04:40.780 --> 00:04:42.640
So data engineering, data science.

00:04:42.640 --> 00:05:04.900
I've also worked a little bit with a couple of businesses, just helping them understand sort of what are the challenges and what's the potential in the data marketplace and data ecosystem to be able to go beyond just having an application and then being able to use the information and the data that they gather from that to be able to build more interesting insights into their business, but also products for their customers.

00:05:04.900 --> 00:05:06.460
Oh, yeah, that sounds really fun.

00:05:06.460 --> 00:05:08.340
I mean, working for MIT sounds amazing.

00:05:08.900 --> 00:05:19.560
And then those advisory roles are really neat because you kind of get a take, especially as a podcaster, you get this broad view because you talk to so many people and, you know, they've got different situations and different contexts.

00:05:19.560 --> 00:05:21.780
And so you can say, all right, look, here's kind of what I see.

00:05:21.780 --> 00:05:23.420
You seem to fit into this realm.

00:05:23.420 --> 00:05:25.180
And so this might be the right path.

00:05:25.180 --> 00:05:25.740
Absolutely.

00:05:25.740 --> 00:05:26.180
Yeah.

00:05:26.180 --> 00:05:29.960
I mean, the data ecosystem in particular is very fast moving.

00:05:29.960 --> 00:05:35.600
So it's definitely very difficult to be able to keep abreast with all the different aspects of it.

00:05:35.600 --> 00:06:04.360
And so because that's kind of my job as a podcaster, I'm able to dig deep into various areas of it and be able to learn from and take advantage of all of the expertise and insight that these various innovators and leaders in the space have and kind of synthesize that because I'm talking to people across the storage layer to the data processing layer and orchestration and analytics and machine learning and operationalizing all of that.

00:06:04.360 --> 00:06:14.380
Whereas if I were one of the people who's deep in the trenches, it's, you know, you get a very detailed but narrow view, whereas I've got a very shallow and broad view across the whole ecosystem.

00:06:14.380 --> 00:06:15.500
So I'm able to.

00:06:15.500 --> 00:06:17.880
Which is the perfect match for the high level view, right?

00:06:17.880 --> 00:06:18.660
Exactly.

00:06:18.660 --> 00:06:19.200
Nice.

00:06:19.200 --> 00:06:19.460
All right.

00:06:19.460 --> 00:06:21.320
Well, let's jump into our main topic.

00:06:21.320 --> 00:06:25.180
And we touched on it a little bit, but I know what data science is, I think.

00:06:25.180 --> 00:06:30.200
And there's a really interesting interview I did with Emily and Jacqueline.

00:06:30.200 --> 00:06:34.420
I don't remember both their last names recently, but about building a career in data science.

00:06:34.420 --> 00:06:43.140
And they talked about basically three areas of data science that you might be in, like production and machine learning versus making predictions and so on.

00:06:43.140 --> 00:06:48.240
And data engineering, it feels like it's kind of in that data science realm, but it's not exactly that.

00:06:48.240 --> 00:06:51.480
Like it could kind of be databases and other stuff too, right?

00:06:51.480 --> 00:06:57.660
Like what is this data engineering thing, maybe compare and contrast against data science as people probably know that pretty well.

00:06:57.660 --> 00:06:58.000
Yeah.

00:06:58.000 --> 00:07:05.260
So it's one of those kind of all encompassing terms that, you know, the role depends on the organization that you're in.

00:07:05.260 --> 00:07:12.660
So in some places, data engineer might just be the person who, you know, used to be the DBA or the database administrator.

00:07:12.920 --> 00:07:20.380
And other places, they might be responsible for maintaining streaming systems.

00:07:20.380 --> 00:07:32.400
One way that I've seen it broken down is kind of two sort of broad classifications of data engineering is there's the SQL focused data engineer where they might have a background as a database administrator.

00:07:32.960 --> 00:07:35.740
And so they do a lot of work in managing the data warehouse.

00:07:35.740 --> 00:07:51.280
They work with SQL oriented tools where there are a lot of them coming out now where you can actually use SQL for being able to pull data from source systems into the data warehouse and then provide, you know, build transformations to provide to analysts and data scientists.

00:07:51.280 --> 00:07:58.660
And then there is the more engineering oriented data engineer, which is somebody who writes a lot of software.

00:07:58.660 --> 00:08:04.820
They're building complex infrastructure and architectures using things like Kafka or Flink or Spark.

00:08:04.820 --> 00:08:06.200
They're working with the database.

00:08:06.200 --> 00:08:11.120
They're working with data orchestration tools like Airflow or Dagster or Prefect.

00:08:11.120 --> 00:08:12.540
They might be using Dask.

00:08:12.540 --> 00:08:18.500
And so they're much more focused on actually writing software and delivering code as the output of their efforts.

00:08:18.500 --> 00:08:18.760
Right.

00:08:18.760 --> 00:08:19.020
Okay.

00:08:19.020 --> 00:08:40.000
But the shared context across however you define data engineering, the shared aspect of it is that they're all working to bring data from multiple locations into a place that is accessible for various end users where the end users might be analysts or data scientists or the business intelligence tools.

00:08:40.000 --> 00:08:53.180
And they're tasked with making sure that those workflows are repeatable and maintainable and that the data is clean and organized so that it's useful because, you know, everybody knows the whole garbage in garbage out principle.

00:08:53.180 --> 00:08:53.660
Yeah.

00:08:53.660 --> 00:09:02.140
If you're a data scientist and you don't have all the context of where the data is coming from, you just have a small narrow scope of what you need to work with.

00:09:02.200 --> 00:09:04.680
You're kind of struggling with that garbage in garbage out principle.

00:09:04.680 --> 00:09:09.860
And so the data engineer's job is to get rid of all the garbage and give you something clean that you can work from.

00:09:09.860 --> 00:09:13.300
I think that's really a tricky problem in the data science side of things.

00:09:13.300 --> 00:09:19.560
You take your data, you run it through a model or through some analysis graphing layer, and it gives you a picture.

00:09:19.560 --> 00:09:20.580
And you're like, well, that's the answer.

00:09:20.580 --> 00:09:21.300
Maybe.

00:09:21.300 --> 00:09:22.140
Maybe it is.

00:09:22.140 --> 00:09:22.420
Right.

00:09:22.580 --> 00:09:23.640
Did you give it the right input?

00:09:23.640 --> 00:09:26.080
Did you train the models in the right data?

00:09:26.080 --> 00:09:27.080
Who knows, right?

00:09:27.080 --> 00:09:27.440
Right.

00:09:27.440 --> 00:09:29.720
That's, you know, definitely a big challenge.

00:09:29.720 --> 00:09:38.540
And that's one of the reasons why data engineering has become so multifaceted is because what you're doing with the data informs the ways that you prepare the data.

00:09:38.540 --> 00:09:47.680
You know, you need to make sure that you have a lot of the contextual information as well to make sure that the data scientists and data analysts are able to answer the questions accurately.

00:09:47.680 --> 00:09:52.740
Because data in isolation, if you just give somebody the number five, it's completely meaningless.

00:09:52.740 --> 00:09:58.960
But if you tell them that a customer ordered five of this unit, well, then now you can actually do something with it.

00:09:58.960 --> 00:10:07.060
So the context helps to provide the information about the isolated number and understanding where it came from and why it's important.

00:10:07.060 --> 00:10:07.800
Yeah, absolutely.

00:10:07.800 --> 00:10:12.900
You know, two things come to mind when I hear data engineering for me is like one is like pipelines of data.

00:10:12.900 --> 00:10:16.440
You know, maybe you've got to bring in data and do transformations to it to get it ready.

00:10:16.440 --> 00:10:24.180
This is part of that data cleanup, maybe, and taking disparate sources and unifying them under one canonical model or something in representation.

00:10:24.180 --> 00:10:30.160
And then ETL, I kind of like we get something terrible like FTP uploads of CSV files.

00:10:30.160 --> 00:10:33.680
And we've got to turn those into databases like overnight jobs, right?

00:10:33.680 --> 00:10:36.100
Or things like that, which probably still exist.

00:10:36.100 --> 00:10:37.520
They existed not too long ago.

00:10:37.520 --> 00:10:37.880
Yeah.

00:10:37.880 --> 00:10:46.920
Every sort of legacy technology that you think has gone away because you're not working with it anymore is still in existence somewhere, which is why we still have COBOL.

00:10:47.940 --> 00:10:48.340
Exactly.

00:10:48.340 --> 00:10:49.460
Oh, my gosh.

00:10:49.460 --> 00:10:54.480
I've got some crazy, crazy COBOL stories for you that probably shouldn't go out public.

00:10:54.480 --> 00:10:56.180
But ask me over the next conference.

00:10:56.180 --> 00:10:57.800
The next time we get to travel somewhere, you know?

00:10:57.800 --> 00:10:58.260
All right.

00:10:58.260 --> 00:10:58.780
Sounds good.

00:10:58.780 --> 00:10:59.240
For sure.

00:10:59.240 --> 00:11:01.140
So let's talk about trends.

00:11:01.140 --> 00:11:02.340
I made that joke, right?

00:11:02.420 --> 00:11:10.080
Like, well, maybe it used to be CSV files or text files and FTP and then a job that would put that into a SQL database or some kind of relational database.

00:11:10.080 --> 00:11:11.200
What is it now?

00:11:11.200 --> 00:11:12.660
It's got to be better than that, right?

00:11:12.660 --> 00:11:14.600
I mean, again, it depends where you are.

00:11:14.600 --> 00:11:16.640
I mean, CSV files are still a thing.

00:11:16.640 --> 00:11:18.740
You know, it might not be FTP anymore.

00:11:18.740 --> 00:11:22.880
It's probably going to be living in object storage like S3 or Google Cloud Storage.

00:11:22.880 --> 00:11:25.840
But, you know, you're still working with individual files in some places.

00:11:26.500 --> 00:11:34.820
A lot of it is coming from APIs or databases where you might need to pull all of the information from Salesforce to get your CRM data.

00:11:34.820 --> 00:11:38.360
Or you might be pulling data out of Google Analytics via their API.

00:11:38.360 --> 00:11:42.020
You know, there are a lot of evolutionary trends that have happened.

00:11:42.020 --> 00:11:49.300
Sort of first big movement in data engineering beyond just the sort of, well, there have been a few generations.

00:11:49.520 --> 00:11:57.960
So the first generation was the data warehouse where you took a database appliance, whether that was Oracle or Microsoft SQL Server or Postgres.

00:11:57.960 --> 00:11:59.860
You put all of your data into it.

00:11:59.860 --> 00:12:05.740
And then you had to do a lot of work to model it so that you could answer questions about that data.

00:12:05.740 --> 00:12:10.840
So in an application database, you're liable to just overwrite a record when something changes,

00:12:10.840 --> 00:12:16.820
where in a data warehouse, you want that historical information about what changed and the evolution of that data.

00:12:17.160 --> 00:12:20.800
What about like normalization in operational databases?

00:12:20.800 --> 00:12:23.100
It's all about one source of truth.

00:12:23.100 --> 00:12:24.780
We better not have any duplication.

00:12:24.780 --> 00:12:26.980
It's fine if there's four joins to get there.

00:12:26.980 --> 00:12:33.740
Whereas in warehousing, it's maybe better to have that duplication so you can run different types of reports real quickly and easily.

00:12:33.740 --> 00:12:34.320
Exactly.

00:12:34.320 --> 00:12:34.620
Yeah.

00:12:34.660 --> 00:12:41.320
I mean, you still need to have one source of truth, but you will model the tables differently than in an application database.

00:12:41.320 --> 00:12:47.320
So there are things like the star schema or the snowflake schema that became popular in the initial phase of data warehousing.

00:12:47.320 --> 00:12:53.200
So Ralph Kimball is famous for building out the sort of star schema approach with facts and dimensions.

00:12:53.200 --> 00:12:53.640
Yeah.

00:12:53.640 --> 00:12:57.600
Maybe describe that a little bit for people because maybe they don't know these terms.

00:12:57.800 --> 00:12:57.960
Sure.

00:12:57.960 --> 00:13:04.820
So facts are things like, you know, a fact is Tobias Macy works at MIT.

00:13:05.220 --> 00:13:11.560
And then a dimension might be he was hired in 2016 or whatever year it was.

00:13:11.560 --> 00:13:17.080
And another dimension of it is he, you know, his work anniversary is X date.

00:13:17.080 --> 00:13:25.120
And so the way that you model it makes it so a fact is something that's immutable and then a dimension or things that might evolve over time.

00:13:25.120 --> 00:13:35.860
And then in sort of the next iteration of data engineering and data management was the sort of, quote unquote, big data craze where Google released their paper about MapReduce.

00:13:35.860 --> 00:13:39.460
And so Hadoop came out as a open source option for that.

00:13:39.600 --> 00:13:40.980
And so everybody said, oh, I've got to get.

00:13:40.980 --> 00:13:41.280
Yeah.

00:13:41.280 --> 00:13:43.240
MapReduce was going to take over the world.

00:13:43.240 --> 00:13:43.480
Right.

00:13:43.480 --> 00:13:45.500
Like that was the only way you could do anything.

00:13:45.500 --> 00:13:47.520
If you had big data, then you had to MapReduce it.

00:13:47.520 --> 00:13:51.620
And then maybe it had to do with one of these large scaled out databases.

00:13:51.620 --> 00:13:52.380
Right.

00:13:52.380 --> 00:13:54.940
Spark or Cassandra or who knows something like that.

00:13:54.940 --> 00:13:55.120
Yeah.

00:13:55.120 --> 00:13:57.140
I mean, Spark and Cassandra came after Hadoop.

00:13:57.140 --> 00:14:01.580
So, I mean, Hadoop was your option in the early 2000s.

00:14:01.580 --> 00:14:03.700
And so everybody said, oh, big data is the answer.

00:14:03.700 --> 00:14:06.540
If I just throw big data at everything, it'll solve all my problems.

00:14:07.140 --> 00:14:14.020
And so people built these massive data lakes using Hadoop and built these MapReduce jobs and then realized that what are we actually doing with all this data?

00:14:14.020 --> 00:14:15.620
It's costing us more money than it's worth.

00:14:15.620 --> 00:14:18.100
MapReduce jobs are difficult to scale.

00:14:18.100 --> 00:14:21.400
They're difficult to understand the order of dependencies.

00:14:21.400 --> 00:14:29.540
And so that's when things like Spark came out to use the data that you were already collecting, but be able to parallelize the operations and run it a little faster.

00:14:29.540 --> 00:14:33.340
And so, you know, that was sort of the era of batch oriented workflows.

00:14:34.000 --> 00:14:42.180
And then with the advent of things like Spark streaming and Kafka and, you know, there are a whole number of other tools out there now, like Flink and Pulsar.

00:14:42.480 --> 00:14:50.660
The sort of real time revolution is where we're at now, where it's not enough to be able to understand what happened the next day.

00:14:50.660 --> 00:14:53.380
You have to understand what's happening, you know, within five minutes.

00:14:53.380 --> 00:15:05.000
And so there are principles like change data capture, where every time I write a new record into a database, it goes into a Kafka queue, which then gets replicated out to an Elasticsearch cluster and to my data warehouse.

00:15:05.000 --> 00:15:15.220
And so within five minutes, my business intelligence dashboard is updated with the fact that customer A bought product B rather than having to wait in 24 hours to get that insight.

00:15:15.440 --> 00:15:16.540
I think that makes tons of sense.

00:15:16.540 --> 00:15:25.360
So instead of going like, we're just going to pile the data into this, you know, some sort of data lake type thing, then we'll grab it and we'll do our reports nightly or hourly or whatever.

00:15:25.360 --> 00:15:29.960
You just keep pushing it down the road as it comes in or as it's generated, right?

00:15:29.960 --> 00:15:30.300
Right.

00:15:30.620 --> 00:15:30.760
Yeah.

00:15:30.760 --> 00:15:33.000
So, I mean, there are still use cases for batch.

00:15:33.000 --> 00:15:35.620
I mean, and there are different ways of looking at it.

00:15:35.620 --> 00:15:43.660
So, I mean, a lot of people view batch as just a special case of streaming where, you know, streaming is sort of micro batches where as a record comes in, you operate on it.

00:15:43.660 --> 00:15:48.700
And then for large batch jobs, you're just doing the same thing, but multiple times for a number of records.

00:15:48.700 --> 00:15:49.020
Yeah.

00:15:49.020 --> 00:15:50.960
I mean, there are a lot of paradigms that are building up.

00:15:50.960 --> 00:15:52.180
People are getting used to the idea.

00:15:52.180 --> 00:15:54.400
I mean, batch is still the easier thing to implement.

00:15:54.400 --> 00:16:04.820
It requires fewer moving pieces, but streaming the sort of availability of different technologies is making it more feasible for more people to be able to actually take advantage of that.

00:16:04.820 --> 00:16:08.780
And so there are managed platforms that help you with that problem.

00:16:08.780 --> 00:16:10.980
There are a lot of open source projects that approach it.

00:16:10.980 --> 00:16:11.240
Yeah.

00:16:11.240 --> 00:16:15.860
There's a whole platforms that are just around to just do data streaming for you, right?

00:16:15.860 --> 00:16:18.000
To just like sort of manage that and keep that alive.

00:16:18.000 --> 00:16:26.260
And with the popularization of web hooks, right, it's easy to say if something changes here, you know, notify this other thing and that thing can call it things.

00:16:26.260 --> 00:16:28.000
And it seems like it's coming along.

00:16:28.000 --> 00:16:28.160
Yeah.

00:16:28.160 --> 00:16:28.480
Yeah.

00:16:28.480 --> 00:16:40.300
One of the interesting aspects, too, of a lot of the work that's been going into the data engineering space is that you're starting to see some of the architectural patterns and technologies move back into the application development domain.

00:16:40.300 --> 00:16:54.620
Where a lot of applications, particularly if you're working with microservices, will use something like a Kafka or a Pulsar queue as the communication layer for being able to propagate information across all the different decoupled applications.

00:16:54.620 --> 00:17:00.280
And that's the same technology and same architectural approaches that are being used for these real-time data pipelines.

00:17:00.280 --> 00:17:00.700
Yeah.

00:17:00.700 --> 00:17:04.640
Man, aren't queues amazing for adding scale to systems, right?

00:17:04.640 --> 00:17:09.020
And if it's going to take too long, throw it in a queue and let the thing crank on it for 30 seconds.

00:17:09.020 --> 00:17:09.520
It'll be good.

00:17:09.520 --> 00:17:09.880
Absolutely.

00:17:10.200 --> 00:17:12.840
I mean, Celery is, you know, the same idea.

00:17:12.840 --> 00:17:14.200
It's just a smaller scale.

00:17:14.200 --> 00:17:16.980
And so, you know, RabbitMQ, it's more ephemeral.

00:17:16.980 --> 00:17:25.600
Whereas when you're putting it into these durable queues, you can do more with the information where you can rewind time to be able to say, okay, I changed my logic.

00:17:25.600 --> 00:17:29.280
I now want to reprocess all of these records from the past three months.

00:17:29.280 --> 00:17:33.580
Whereas if you had that on RabbitMQ, all of those records are gone unless you wrote them out somewhere else.

00:17:40.100 --> 00:17:41.100
You're not going to have to go to the next one.

00:17:41.100 --> 00:17:41.100
You're not going to have to go to the next one.

00:17:41.100 --> 00:17:42.100
You're not going to have to go to the next one.

00:17:42.100 --> 00:17:43.100
You're not going to have to go to the next one.

00:17:43.100 --> 00:17:43.100
You're not going to have to go to the next one.

00:17:43.100 --> 00:17:44.100
You're not going to have to go to the next one.

00:17:44.100 --> 00:17:49.260
where the issue is coming from or how to solve it? Datadog seamlessly correlates logs and traces

00:17:49.260 --> 00:17:53.920
at the level of individual requests, allowing you to quickly troubleshoot your Python application.

00:17:53.920 --> 00:17:58.700
Plus, their continuous profiler allows you to find the most resource-consuming parts of your

00:17:58.700 --> 00:18:04.000
production code all the time at any scale with minimal overhead. Be the hero that got that app

00:18:04.000 --> 00:18:10.040
back on track at your company. Get started today with a free trial at talkpython.fm/datadog,

00:18:10.320 --> 00:18:14.380
or just click the link in your podcast player's show notes. Get the insight you've been missing

00:18:14.380 --> 00:18:20.340
with Datadog. A couple of comments from the live stream. Defria says Airflow, Apache Airflow is

00:18:20.340 --> 00:18:23.680
really cool for sure. We're going to talk about that. But I did want to ask you about the cloud.

00:18:23.680 --> 00:18:29.760
Stefan says, I'm a little bit skeptical about the privacy and security on the cloud. So kind of

00:18:29.760 --> 00:18:34.260
want to use the zone server more often. So maybe that's a trend that you could speak to that you've

00:18:34.260 --> 00:18:39.500
seen with folks you've interviewed. This kind of data is really sensitive sometimes and people are

00:18:39.500 --> 00:18:45.440
very protective of it or whatever. Right. So what is the cloud story versus, oh, we got to do this all

00:18:45.440 --> 00:18:50.980
on prem or maybe even some hybrid thereof? Right. So I mean, it's definitely an important question and

00:18:50.980 --> 00:18:55.900
something that is, it's a complicated problem. There are ways to solve it. I mean, data governance

00:18:55.900 --> 00:19:01.700
is kind of the umbrella term that's used for saying, I want to keep control of my data and make sure that I

00:19:01.700 --> 00:19:09.020
am using the appropriate regulatory aspects and making sure that I am filtering out private information or

00:19:09.020 --> 00:19:14.220
encrypting data at rest, encrypting data in transit. And so there are definitely ways that you can

00:19:14.220 --> 00:19:19.540
keep tight control over your data, even when you're in the cloud. And a lot of the cloud platforms have

00:19:19.540 --> 00:19:23.920
been building out capabilities to make it easier for you. So for instance, if you're on Amazon,

00:19:23.920 --> 00:19:28.940
they have their key management service that you can use to encrypt all of your storage at rest.

00:19:28.940 --> 00:19:34.460
You can provide your own keys if you don't trust them to hold the keys to the kingdom there so that you

00:19:34.460 --> 00:19:37.760
are the person who's in control of being able to encrypt and decrypt your data.

00:19:38.180 --> 00:19:42.500
You know, there are a class of technologies used in data warehouses called privacy enhancing

00:19:42.500 --> 00:19:48.540
technologies, where you can actually have all of the rows in your database fully encrypted.

00:19:48.540 --> 00:19:56.340
And then you can encrypt the predicate of a SQL query to be able to see if the data matches the

00:19:56.340 --> 00:20:00.440
values in the database without ever actually having to decrypt anything so that you could do some

00:20:00.440 --> 00:20:05.860
rudimentary analytics like aggregates on that information so that it all stays safe.

00:20:05.860 --> 00:20:11.000
There's also a class of technologies that are still a little bit in the experimental phase called

00:20:11.000 --> 00:20:17.920
homomorphic encryption, where it actually the data is never actually decrypted. So it lives in this

00:20:17.920 --> 00:20:24.740
encrypted enclave, your data processing job operates within that encrypted space. And so there's never any

00:20:24.740 --> 00:20:29.580
actual clear text information stored anywhere, not even in your computer's RAM.

00:20:29.580 --> 00:20:35.540
Wow. So if one of those like weird CPU bugs that lets you jump through the memory of like different

00:20:35.540 --> 00:20:39.160
VMs or something like that, even then you're probably okay, right?

00:20:39.160 --> 00:20:43.280
Absolutely. Yeah. I mean, the homomorphic encryption, there are some companies out there that are

00:20:43.280 --> 00:20:49.420
offering that as a managed service. And, you know, it's becoming more viable. It's been something that's

00:20:49.420 --> 00:20:54.140
been discussed and theorized about for a long time. But because of the computational cost, it was something

00:20:54.140 --> 00:20:59.400
that was never really commercialized. But there are a lot of algorithms that have been discovered to help

00:20:59.400 --> 00:21:02.160
make it more feasible to actually use in production contexts.

00:21:02.160 --> 00:21:06.700
Yeah. I don't know about the other databases. I know MongoDB, they added some feature where you

00:21:06.700 --> 00:21:11.840
can encrypt just certain fields, right? So maybe here's a field that is sensitive, but you don't

00:21:11.840 --> 00:21:16.480
necessarily need to query by for your reports, but it needs to be in with, say, with a user or an order

00:21:16.480 --> 00:21:21.240
or something like that. So even going to that part might be a pretty good step. But yeah, the clouds are

00:21:21.240 --> 00:21:23.360
both amazing and scary, I suppose.

00:21:23.360 --> 00:21:27.940
Yeah. Yeah. I mean, there's definitely a lot of options. It's something that requires a bit of

00:21:27.940 --> 00:21:32.460
understanding and legwork, but it's definitely possible to make sure that all your data stays

00:21:32.460 --> 00:21:37.000
secured and that you are in full control over where it's being used.

00:21:37.000 --> 00:21:43.560
Yeah. So one of the next things I wanted to ask you about is languages. So you're probably familiar

00:21:43.560 --> 00:21:49.760
with this chart here, right? Which if people are not watching the stream, this is the Stack Overflow

00:21:49.760 --> 00:21:56.460
trend showing Python just trouncing the other languages, including Java. But I know Java had been

00:21:56.460 --> 00:22:00.720
maybe one of the main ways that probably has to do with Spark and whatnot to some degree.

00:22:00.720 --> 00:22:04.500
What do you see Python's role relative to other technologies here?

00:22:04.500 --> 00:22:08.460
So Python has definitely been growing a lot in the data engineering space,

00:22:08.460 --> 00:22:14.360
largely because of the fact that it's so popular in data science. And so there are data scientists

00:22:14.360 --> 00:22:19.600
who have been moving further down the stack into data engineering as a requirement of their job.

00:22:19.600 --> 00:22:25.700
And so they are bringing Python into those layers of the stack. It's also being used as just a unifying

00:22:25.700 --> 00:22:31.660
language so that data engineers and data scientists can work on the same code bases. As you mentioned,

00:22:31.660 --> 00:22:37.540
Java has been popular for a long time in the data ecosystem because of things like Hadoop and Spark.

00:22:37.540 --> 00:22:43.120
And looking at the trend graph, I'd be interested to see what it looks like if you actually combine the

00:22:43.120 --> 00:22:49.300
popularities of Java and Scala because Scala has become the strong contender in that space as well

00:22:49.300 --> 00:22:55.520
because of things like Spark and Flink that have native support for Scala. It's a bit more of an

00:22:55.520 --> 00:23:00.060
esoteric language, but it's used a lot in data processing. But Python has definitely gained a lot

00:23:00.060 --> 00:23:07.000
of ground. And also because of tools like Airflow, which was kind of the first generation tool built for

00:23:07.000 --> 00:23:12.760
data engineers by data engineers to be able to manage these dependency graphs of operations so

00:23:12.760 --> 00:23:18.660
that you can have these pipelines to say, you know, I need to pull data out of Salesforce and then land

00:23:18.660 --> 00:23:22.820
it into S3. And then I need to have another job that takes that data out of S3 and puts it into the

00:23:22.820 --> 00:23:29.600
database. And then also that same S3 data needs to go into an analytics job. Then once those two jobs

00:23:29.600 --> 00:23:34.520
are complete, I need to kick off another job that then runs a SQL query against the data warehouse to be

00:23:34.520 --> 00:23:40.100
able to provide some aggregate information to my sales and marketing team to say, this is what your

00:23:40.100 --> 00:23:44.060
customer engagement is looking like or whatever it might be. Yeah. And that was all written in Python.

00:23:44.060 --> 00:23:49.540
And also just because of the massive ecosystem of libraries that Python has for being able to

00:23:49.540 --> 00:23:55.940
interconnect across all these different systems and data engineering at a certain level is really just

00:23:55.940 --> 00:24:01.060
a systems integration task where you need to be able to have information flowing across all of these

00:24:01.060 --> 00:24:05.560
different layers and all these different systems and get good control over it. Some of the interesting tools

00:24:05.560 --> 00:24:10.900
that have come out as a sort of generational improvement over Airflow are Dagster and Prefect.

00:24:10.900 --> 00:24:16.760
I've actually been using Dagster for my own work at MIT and been enjoying that tool. I'm always happy to dig into

00:24:16.760 --> 00:24:17.040
that.

00:24:17.040 --> 00:24:21.880
Let's sort of focus on those things. And one of the themes I wanted to cover is maybe the five most important

00:24:21.880 --> 00:24:27.280
packages or libraries for data engineering. And you kind of hit the first one that will group together as a

00:24:27.280 --> 00:24:33.540
trifecta, right? So Airflow, Dagster, and Prefect. You want to maybe tell us about those three?

00:24:33.540 --> 00:24:33.940
Yeah.

00:24:33.940 --> 00:24:35.140
Which one do you prefer?

00:24:35.140 --> 00:24:40.080
So I personally use Dagster. I like a lot of the abstractions and the interface design that they

00:24:40.080 --> 00:24:45.980
provide, but they're all three grouped into a category of tools called sort of workflow management

00:24:45.980 --> 00:24:52.800
or data orchestration. And so the responsibility there is that you need to have a way to build these

00:24:52.800 --> 00:24:59.980
pipelines, build these DAGs or directed acyclic graphs of operations where the vertices of the graph

00:24:59.980 --> 00:25:05.520
are the data and the nodes are the jobs or the operations being performed on them. And so you

00:25:05.520 --> 00:25:10.220
need to be able to build up this dependency chain because you need to get information out of a source

00:25:10.220 --> 00:25:14.400
system. You need to get it into a target system. You might need to perform some transformations either

00:25:14.400 --> 00:25:20.740
on route or after it's been landed. You know, one of the common trends that's happening is it used to

00:25:20.740 --> 00:25:25.860
be extract, transform, and then load because you needed to have all of the information in that

00:25:25.860 --> 00:25:28.640
specialized schema for the data warehouse that we were mentioning earlier.

00:25:28.640 --> 00:25:33.220
Right. Right. And all the relational database, database, it's got to have these columns in this.

00:25:33.220 --> 00:25:37.300
It can't be a long character. It's got to be a VAR, VAR car 10 or whatever.

00:25:37.300 --> 00:25:43.400
Right. And then with the advent of the cloud data warehouses that have been happening in the past few

00:25:43.400 --> 00:25:48.420
years that was kicked off by Redshift from Amazon and then carried on by things like Google BigQuery,

00:25:48.680 --> 00:25:53.260
Snowflake that a lot of people will probably be aware of. You know, there are a number of other

00:25:53.260 --> 00:25:58.340
systems and platforms out there. Presto out of Facebook that is now an open source project,

00:25:58.340 --> 00:26:02.940
actually renamed it to Trino. Those systems are allowing people to be very SQL oriented,

00:26:02.940 --> 00:26:07.000
but because of the fact that they're scalable and they provide more flexible data models,

00:26:07.000 --> 00:26:12.940
the trend has gone to extract, load, and then transform because you can just replicate the schema

00:26:12.940 --> 00:26:18.000
as is into these destination systems. And then you can perform all of your transformations in SQL.

00:26:18.000 --> 00:26:23.100
And so that brings us into another tool that is in the Python ecosystem that's been gaining a lot of

00:26:23.100 --> 00:26:30.620
ground called DBT or data build tool. And so this is a tool that actually brings data analysts and

00:26:30.620 --> 00:26:37.180
improves their skill set, makes them more self-sufficient within the organization and provides a lot of,

00:26:37.400 --> 00:26:43.620
provides a great framework for them to operate in an engineering mindset where it helps to build up a

00:26:43.620 --> 00:26:49.560
specialized DAG within the context of the data warehouse to take those source data sets that are

00:26:49.560 --> 00:26:54.620
landed into the data warehouse from the extract and load jobs and build these transformations.

00:26:54.620 --> 00:27:00.820
So you might have the user table from your application database and the orders table.

00:27:00.820 --> 00:27:05.740
And then you also have the Salesforce information that's landed in a separate table.

00:27:05.740 --> 00:27:10.220
And you want to be able to combine all of those to be able to understand your customer order,

00:27:10.220 --> 00:27:16.520
customer buying patterns. And so you use SQL to build either a view or build a new table out of that

00:27:16.520 --> 00:27:21.820
source information in the data warehouse and DBT will handle that workflow.

00:27:21.820 --> 00:27:27.120
It also has support for being able to build unit tests in SQL into your workflow.

00:27:27.120 --> 00:27:31.580
Oh, how interesting. Yeah. That's something that you hadn't really heard very much of

00:27:31.580 --> 00:27:37.120
10 years ago was testing and databases. It was usually, how do I get the database out of the

00:27:37.120 --> 00:27:41.100
picture so I can test without depending upon it or something like that? That was the story.

00:27:41.100 --> 00:27:47.880
Yeah. That's another real growing trend is the overall aspect of data quality and confidence in your

00:27:47.880 --> 00:27:54.080
data flows. So things like in Dagster and Prefect and Airflow, they have support for being able to

00:27:54.080 --> 00:27:58.400
unit test your pipelines, which is another great aspect of the Python ecosystem is you can just

00:27:58.400 --> 00:28:03.740
write pytest code to ensure that all the operations on your data match your expectations and you don't

00:28:03.740 --> 00:28:06.320
have regressions and bugs. Right. Right. Absolutely.

00:28:06.560 --> 00:28:11.620
The complicating aspect of data engineering is that it's not just the code that you need to make sure

00:28:11.620 --> 00:28:16.080
is right, but you also need to make sure that the data is right. And so another tool that is helping

00:28:16.080 --> 00:28:19.520
in that aspect, again, from the Python ecosystem is great expectations.

00:28:19.520 --> 00:28:23.120
Right. And that's right in the realm of this testing your data.

00:28:23.120 --> 00:28:24.500
Yeah. Exactly. Absolutely.

00:28:24.740 --> 00:28:30.340
So you can say, you know, I'm pulling data out of my application database. I expect the schema to have

00:28:30.340 --> 00:28:36.500
these columns in it. I expect the data distribution within this column to, you know, the values are only

00:28:36.500 --> 00:28:41.380
going to range from zero to five. And then if I get a value outside of that range, then I can,

00:28:41.380 --> 00:28:45.640
you know, it will fail the test and it will notify me that something's off. So you can build these very

00:28:45.640 --> 00:28:51.500
expressive and flexible expectations of what your data looks like, what your data pipeline is going to do

00:28:51.500 --> 00:28:57.340
so that you can gain visibility and confidence into what's actually happening as you are propagating

00:28:57.340 --> 00:29:01.040
information across all these different systems. So do you make this part of your continuous

00:29:01.040 --> 00:29:05.660
integration tests? Absolutely. Yeah. So it would be part of your continuous integration as you're

00:29:05.660 --> 00:29:10.900
delivering new versions of your pipeline, but it's also something that executes in the context of the

00:29:10.900 --> 00:29:16.560
nightly batch job or of your streaming pipeline. So it's both a build time and a runtime expectation.

00:29:16.560 --> 00:29:21.480
Yeah, yeah, yeah. So it's like a pre test. It's like an if test for your function.

00:29:21.480 --> 00:29:25.440
But for your data, right? Like, let's make sure everything's good before we run through this and

00:29:25.440 --> 00:29:29.240
actually drop the answer on to the dashboard for the morning or something like that.

00:29:29.240 --> 00:29:33.900
Okay, right. Yeah, it helps to build up that confidence because anybody who's been working

00:29:33.900 --> 00:29:40.120
in data has had the experience of I delivered this report, I feel great about it. I'm happy that I was

00:29:40.120 --> 00:29:45.740
able to get this thing to run through and then you hand it off to your CEO or your CTO and they look at it

00:29:45.740 --> 00:29:49.520
and they say, well, this doesn't quite look right. And then you go back and realize, oh, crud,

00:29:49.520 --> 00:29:53.760
that's because I forgot to pull in this other column or whatever it is. And so this way you

00:29:53.760 --> 00:29:57.520
can not have to have that sinking feeling in your gut when you hand off the report.

00:29:57.520 --> 00:30:03.240
That would be bad. What would be worse is we decided to invest by buying a significant position

00:30:03.240 --> 00:30:07.860
in this other company. Oh, but it turned out, whoops, it was actually we had a negative sign.

00:30:07.860 --> 00:30:09.720
It wasn't really good for you to invest in this.

00:30:09.720 --> 00:30:10.900
Absolutely. Yep.

00:30:10.940 --> 00:30:12.500
If it's actions have been taken.

00:30:15.180 --> 00:30:19.420
The next question of Talk Python To Me is brought to you by Retool. Do you really need a full dev team to build

00:30:19.420 --> 00:30:23.620
that simple internal app at your company? I'm talking about those back office apps,

00:30:23.620 --> 00:30:29.980
the tool your customer service team uses to access your database, that S3 uploader you built last year for the marketing team,

00:30:29.980 --> 00:30:35.800
the quick admin panel that lets you monitor key KPIs, or maybe even the tool your data science team hacked together

00:30:35.800 --> 00:30:38.320
so they could provide custom ad spend insights.

00:30:38.980 --> 00:30:45.100
Literally every type of business relies on these internal tools, but not many engineers love building these tools,

00:30:45.100 --> 00:30:48.300
let alone get excited about maintaining or supporting them over time.

00:30:48.300 --> 00:30:52.740
They eventually fall into the please don't touch it. It's working category of apps.

00:30:52.740 --> 00:31:01.220
And here's where Retool comes in. Companies like DoorDash, Brex, Plaid, and even Amazon use Retool to build internal tools super fast.

00:31:01.220 --> 00:31:04.400
The idea is that almost all internal tools look the same.

00:31:04.400 --> 00:31:08.420
Forms over data. They're made up of tables, dropdowns, buttons, text input,

00:31:08.800 --> 00:31:09.300
and so on.

00:31:09.300 --> 00:31:18.080
Retool gives you a point, click, and drag and drop interface that makes it super simple to build internal UIs like this in hours, not days.

00:31:18.080 --> 00:31:20.900
Retool can connect to any database or API.

00:31:20.900 --> 00:31:25.500
Want to pull data from Postgres? Just write a SQL query and drag the table onto your canvas.

00:31:25.500 --> 00:31:29.860
Search across those fields, add a search input bar and update your query.

00:31:29.860 --> 00:31:31.920
Save it, share it, super easy.

00:31:31.920 --> 00:31:35.820
Retool is built by engineers, explicitly for engineers.

00:31:36.240 --> 00:31:41.040
It can be set up to run on-prem in about 15 minutes using Docker, Kubernetes, or Heroku.

00:31:41.040 --> 00:31:42.880
Get started with Retool today.

00:31:42.880 --> 00:31:48.740
Just visit talkpython.fm/retool or click the Retool link in your podcast player show notes.

00:31:51.540 --> 00:31:54.860
Hey, let me jump you back really quick to that language trends question real quick.

00:31:54.860 --> 00:32:02.000
So Anthony Lister asks if R is still widely used and sort of a strong competitor, let's say, to Python.

00:32:02.000 --> 00:32:03.200
What's your thoughts these days?

00:32:03.200 --> 00:32:06.460
I kind of honestly hear a little bit less of it in my world for some reason.

00:32:06.460 --> 00:32:09.620
Yeah, so there are definitely a lot of languages.

00:32:09.620 --> 00:32:12.680
R is definitely one of them that's still popular in the data space.

00:32:12.680 --> 00:32:15.880
I don't really see R in the data engineering context.

00:32:15.880 --> 00:32:21.380
It's definitely still used for a lot of statistical modeling, machine learning, data science workloads.

00:32:21.920 --> 00:32:41.760
There's a lot of great interoperability between R and Python now, especially with the Arrow project, which is a in-memory columnar representation that provides an interoperable, provides an in-memory space where you can actually exchange data between R and Python and Java without having to do any IO copying between them.

00:32:41.760 --> 00:32:46.240
So it helps to reduce a lot of the impedance mismatch between those languages.

00:32:46.240 --> 00:32:57.780
Another language that's been gaining a lot of ground in the data ecosystem is Julia, and they're actually under the NumFocus organization that supports a lot of the Python data ecosystem.

00:32:57.780 --> 00:32:58.460
Yeah.

00:32:58.460 --> 00:33:03.820
So Julia has been gaining a lot of ground, but Python, just because of its broad use, is still very popular.

00:33:03.820 --> 00:33:12.660
And there's an anecdote that I've heard a number of times, I don't remember where I first came across it, that Python isn't the best language for anything, but it's the second best language for everything.

00:33:12.660 --> 00:33:14.440
Yeah, that's a good quote.

00:33:14.440 --> 00:33:16.900
I think it does put a lot of perspective on it.

00:33:16.900 --> 00:33:19.480
I feel like it's just so approachable, right?

00:33:19.480 --> 00:33:19.900
Exactly.

00:33:19.900 --> 00:33:32.260
And there's a lot of these languages that might make slightly more sense for a certain use case like R and statistics, but you better not want to have to build some other thing that reaches outside of what's easily possible, right?

00:33:32.260 --> 00:33:34.320
Like, right, you want to make that an API now?

00:33:34.320 --> 00:33:37.160
Well, all of a sudden, it's not so easy or whatever, right?

00:33:37.160 --> 00:33:38.080
Something along those lines.

00:33:38.080 --> 00:33:38.540
Exactly.

00:33:38.540 --> 00:33:39.040
All right.

00:33:39.040 --> 00:33:41.740
Next in our list here is Dask.

00:33:41.740 --> 00:33:42.140
Yeah.

00:33:42.140 --> 00:33:44.580
So Dask is a great tool.

00:33:44.580 --> 00:33:48.640
I kind of think about it as the Python version of Spark.

00:33:48.640 --> 00:33:58.340
There are a number of reasons that's not exactly accurate, but it's a tool that lets you parallelize your Python operations, scale it out into clusters.

00:33:58.720 --> 00:34:09.480
It also has a library called Dask.distributed that's used a lot for just scaling out Python independent of actually building the directed acyclic graphs in Dask.

00:34:09.480 --> 00:34:13.280
So one of the main ways that Spark is used is as an ETL engine.

00:34:13.280 --> 00:34:15.880
So you can build these graphs of tasks in Spark.

00:34:15.880 --> 00:34:17.380
You can do the same thing with Dask.

00:34:17.380 --> 00:34:24.440
It was actually built originally more for the hard sciences and for scientific workloads and not just for data science.

00:34:24.440 --> 00:34:24.760
Yeah.

00:34:24.860 --> 00:34:31.580
But Dask is actually also used as a foundational layer for a number of the data orchestration tools out there.

00:34:31.580 --> 00:34:35.180
So Dask is the foundational layer for Prefect.

00:34:35.180 --> 00:34:40.820
You can use it as an execution substrate for the Dagster library, the Dagster framework.

00:34:40.820 --> 00:34:44.880
It's also supported in Airflow as a execution layer.

00:34:45.280 --> 00:34:54.280
And there are also a number of people who are using it as a replacement for things like Celery as just a means of running asynchronous tasks outside of the bounds of a request response cycle.

00:34:54.280 --> 00:34:59.500
So it's just growing a lot in the data ecosystem, both for data engineering and data science.

00:34:59.500 --> 00:35:11.020
And so it just provides that unified layer of being able to build your data engineering workflows and then hand that directly off into machine learning so that you don't have to jump between different systems.

00:35:11.020 --> 00:35:12.600
You can do it all in one layer.

00:35:12.600 --> 00:35:13.600
Yeah, that's super neat.

00:35:13.600 --> 00:35:15.840
And Dask, I never really appreciated it.

00:35:15.840 --> 00:35:19.320
Sort of it's different levels at which you can use it, I guess I should say.

00:35:19.320 --> 00:35:25.700
You know, when I thought about it, OK, well, this is like parallel computing for Pandas or for NumPy or something like that.

00:35:25.700 --> 00:35:25.980
Right.

00:35:25.980 --> 00:35:29.420
But it's also it works well on just your single laptop.

00:35:29.420 --> 00:35:29.840
Right.

00:35:29.840 --> 00:35:34.780
It'll let you run multi-core stuff locally because Python doesn't always do that super well.

00:35:34.780 --> 00:35:43.100
And it'll even think it'll even do caching and stuff so it can actually work with more data than you have RAM, which is hard, which is straight NumPy.

00:35:43.100 --> 00:35:45.720
But then, of course, you can point it at a cluster and go crazy.

00:35:45.720 --> 00:35:46.160
Exactly.

00:35:46.160 --> 00:35:46.520
Yeah.

00:35:46.880 --> 00:36:02.260
And because of the fact that it has those transparent API layers for being able to swap out the upstream Pandas with the Dask Pandas library and NumPy, it's easy to go from working on your laptop to just changing an import statement.

00:36:02.260 --> 00:36:04.800
And now you're scaling out across a cluster of hundreds of machines.

00:36:04.800 --> 00:36:06.600
Yeah, that's pretty awesome, actually.

00:36:06.600 --> 00:36:06.920
Yeah.

00:36:06.920 --> 00:36:10.340
Maybe that has something as well to do with the batch to real time.

00:36:10.340 --> 00:36:10.700
Right.

00:36:10.700 --> 00:36:14.840
If you've got to run it in one on one core on one machine, it's a batch job.

00:36:14.840 --> 00:36:18.840
If you can run it on the entire cluster at, you know, that's sitting around idle.

00:36:18.840 --> 00:36:20.720
Well, then all of a sudden it's real time.

00:36:20.720 --> 00:36:21.120
Right.

00:36:21.360 --> 00:36:23.440
Yeah, there's a lot of interesting real time stuff.

00:36:23.440 --> 00:36:32.420
There's an interesting project, sort of a side note here called Wallaroo that's built for building stateful stream processing jobs using Python.

00:36:32.420 --> 00:36:35.700
And interestingly, it's actually implemented in a language called Pony.

00:36:35.700 --> 00:36:37.160
But Pony?

00:36:37.160 --> 00:36:37.760
Yeah.

00:36:37.760 --> 00:36:50.300
An interesting project, you know, levels up your ability to scale out the speed of execution and the sort of just being able to build these complex pipelines real time jobs.

00:36:50.300 --> 00:36:53.780
without having to build all of the foundational layers of it.

00:36:53.780 --> 00:36:54.060
Yeah.

00:36:54.060 --> 00:36:54.640
Okay.

00:36:54.640 --> 00:36:55.200
Interesting.

00:36:55.200 --> 00:36:56.280
I have not heard of this one.

00:36:56.280 --> 00:36:57.140
That sounds fun.

00:36:57.140 --> 00:36:57.500
Yeah.

00:36:57.500 --> 00:36:58.880
It's not as widely known.

00:36:58.880 --> 00:37:05.520
I interviewed the creator of it on the data engineering podcast a while back, but it's a tool that comes up every now and then.

00:37:05.520 --> 00:37:06.840
Interesting approach to it.

00:37:06.840 --> 00:37:07.120
Yeah.

00:37:07.120 --> 00:37:09.920
Right in that stream processing real time world.

00:37:09.920 --> 00:37:10.260
Right.

00:37:10.260 --> 00:37:12.260
The next one that you put on our list here is...

00:37:12.260 --> 00:37:13.220
Meltano.

00:37:13.220 --> 00:37:13.900
Meltano.

00:37:13.900 --> 00:37:14.720
Meltano.

00:37:14.720 --> 00:37:15.420
I got to say it right.

00:37:15.420 --> 00:37:15.620
Yeah.

00:37:15.620 --> 00:37:16.420
Yeah.

00:37:16.420 --> 00:37:18.520
So that one is an interesting project.

00:37:18.840 --> 00:37:20.660
It came from the GitLab folks.

00:37:20.660 --> 00:37:21.980
It's still supported by them.

00:37:21.980 --> 00:37:31.580
And in its earliest stage, they actually wanted it to be the full end-to-end solution for data analytics for startups.

00:37:32.040 --> 00:37:41.900
Meltano is actually an acronym for, if I can remember correctly, model, extract, load, transform, analyze, notebook, and orchestrate.

00:37:41.900 --> 00:37:42.900
Okay.

00:37:42.900 --> 00:37:43.360
Yeah.

00:37:43.860 --> 00:37:47.360
That's quite a wild one to put into something you can say well.

00:37:47.360 --> 00:37:47.860
Exactly.

00:37:47.860 --> 00:38:02.140
And, you know, about a year, year and a half ago now, they actually decided that they were being a little too ambitious and trying to boil the ocean and scoped it down to doing the extract and load portions of the workflow really well.

00:38:02.140 --> 00:38:11.320
Because it's a very underserved market where you would think that given the amount of data we're all working with, point-to-point data integration and extract and load would be a solved problem, easy to do.

00:38:11.320 --> 00:38:13.080
But there's a lot of nuance to it.

00:38:13.080 --> 00:38:18.700
And there isn't really one easy thing to say, yes, that's the tool you want to use all the time.

00:38:18.880 --> 00:38:21.860
And so there are some paid options out there that are good.

00:38:21.860 --> 00:38:26.340
Meltano is aiming to be the default open source answer for data integration.

00:38:26.340 --> 00:38:35.960
And so it's building on top of the Singer specification, which is sort of an ecosystem of libraries that was built by a company called Stitch Data.

00:38:35.960 --> 00:38:47.300
But the idea is that you have what they call taps and targets, where a tap will tap into a source system, pull data out of it, and then the targets will load that data into a target system.

00:38:47.300 --> 00:38:57.580
And they have this interoperable specification that's JSON-based so that you can just wire together any two taps and targets to be able to pull data from a source into a destination system.

00:38:57.580 --> 00:38:58.240
Nice.

00:38:58.240 --> 00:39:00.900
Yeah, it's definitely a well-designed specification.

00:39:00.900 --> 00:39:02.060
A lot of people like it.

00:39:02.280 --> 00:39:07.160
There are some issues with the way that the ecosystem was sort of created and fostered.

00:39:07.160 --> 00:39:13.420
So there's a lot of uncertainty or variability in terms of the quality of the implementations of these taps and targets.

00:39:13.420 --> 00:39:23.220
And there was never really one cohesive answer to this is how you run these in a production context, partially because Stitch Data was the answer to that.

00:39:23.220 --> 00:39:29.060
So they wanted you to buy into this open source ecosystem so that you would then use them as the actual execution layer.

00:39:29.180 --> 00:39:39.760
And so Meltano is working to build an open source option for you to be able to wire together these taps and targets and be able to just have an easy out-of-the-box data integration solution.

00:39:39.760 --> 00:39:45.260
So it's a small team from GitLab, but there's a large and growing community helping to support it.

00:39:45.260 --> 00:40:06.600
And they've actually been doing a lot to help push forward the state-of-the-art for the Singer ecosystem, building things like a starter template for people building taps and targets so that there's a common baseline of quality built into these different implementations without having to wonder about, you know, is this tap going to support all of the features of the specification that I need?

00:40:06.600 --> 00:40:06.860
Nice.

00:40:06.860 --> 00:40:08.320
Is this actually from GitLab?

00:40:08.500 --> 00:40:10.380
Yeah, so it's sponsored by GitLab.

00:40:10.380 --> 00:40:18.040
It's the source code is within the GitLab organization on GitLab.com, but it's definitely a very community-driven project.

00:40:18.040 --> 00:40:23.340
Yeah, Stefan is quite excited about the open source and default open source choice.

00:40:23.340 --> 00:40:23.640
Yeah.

00:40:23.640 --> 00:40:24.780
Well, I think there's two things.

00:40:24.780 --> 00:40:29.040
One, open source is amazing, but two, you get this paradox of choice, right?

00:40:29.040 --> 00:40:30.360
It's like, well, it's great.

00:40:30.360 --> 00:40:33.500
You can have anything, but there's so many things and I'm new to this.

00:40:33.500 --> 00:40:34.060
What do I do?

00:40:34.220 --> 00:40:34.360
Right.

00:40:34.360 --> 00:41:00.580
And so, yeah, Meltano is trying to be the answer to, you know, you just Meltano in it, you have a project, you say, I want these sources and destinations, and then it will help you handle things like making sure that the jobs run on a schedule, handling, tracking the state of the operations, because you can do either full extracts and loads every time, or you can do incremental, because you don't necessarily want to dump a 4 million line source table every single time it runs.

00:41:00.580 --> 00:41:03.920
You just want to pull the 15 lines that changed since the last operation.

00:41:03.920 --> 00:41:06.060
So it will help track that state for you.

00:41:06.060 --> 00:41:06.720
Oh, that's cool.

00:41:06.720 --> 00:41:09.360
And try to be real efficient and just get what it needs.

00:41:09.360 --> 00:41:09.860
Yeah.

00:41:09.860 --> 00:41:16.680
And it builds in some of the monitoring information that you want to be able to see as far as like execution time, performance of these jobs.

00:41:17.120 --> 00:41:24.800
And it actually, out of the box, will use Airflow as the orchestration engine for being able to manage these schedules, but everything is pluggable.

00:41:24.800 --> 00:41:30.120
So if you wanted to write your own implementation that will use Dagster as the orchestrator instead, then they'll do that.

00:41:30.120 --> 00:41:33.600
There's actually a ticket in their tracker for doing that work.

00:41:33.860 --> 00:41:40.580
So it's very pluggable, very flexible, but gives you a lot of out of the box answers to being able to just get something up and running quickly.

00:41:40.580 --> 00:41:40.840
Yeah.

00:41:40.840 --> 00:41:43.640
And it looks like you can build custom loaders and custom extractors.

00:41:43.640 --> 00:41:51.060
So if you've got some internal API, that's who knows, maybe it's a SOAP XML endpoint or some random thing, right?

00:41:51.060 --> 00:41:51.640
You could do that.

00:41:51.820 --> 00:41:52.120
Exactly.

00:41:52.120 --> 00:41:52.760
Yeah.

00:41:52.760 --> 00:41:58.780
And they actually lean on DBT, another tool that we were just talking about, as the transformation layer.

00:41:58.780 --> 00:42:06.200
So they hook directly into that so that you can very easily do the extract and load and then jump into DBT for doing the transformations.

00:42:06.200 --> 00:42:06.740
Yeah.

00:42:06.740 --> 00:42:09.940
Now, you didn't put this one on the list, but I do want to ask you about it.

00:42:09.940 --> 00:42:15.460
What's the story of something like Zapier in this whole, you know, get notified about these changes, push stuff here?

00:42:15.460 --> 00:42:21.180
I mean, it feels like if you were trying to wire things together, I've seen more than one Python developer reach for Zapier.

00:42:21.340 --> 00:42:27.060
Yeah. So Zapier is definitely a great platform, particularly for doing these event-based workflows.

00:42:27.060 --> 00:42:33.600
You can use it as a data engineering tool if you want, but it's not really what it's designed for.

00:42:33.600 --> 00:42:39.700
It's more just for business automation aspects or maybe automation of my application did this thing,

00:42:39.700 --> 00:42:43.100
and now I want to have it replicate some of that state out to a third-party system.

00:42:43.100 --> 00:42:50.400
Zapier isn't really meant for the sort of full-scale data engineering workflows, maintaining visibility.

00:42:50.860 --> 00:42:53.420
It's more just for this evented IO kind of thing.

00:42:53.420 --> 00:42:55.700
Yeah. So here on the Meltano, it says,

00:42:55.700 --> 00:43:00.760
Pipelanger code ready to be version controlled and containerized and deployed continuously.

00:43:00.760 --> 00:43:04.380
The CI-CD side sounds pretty interesting, right?

00:43:04.380 --> 00:43:08.580
Especially with these workflows that might be in flight while you're making changes.

00:43:08.580 --> 00:43:09.420
How does that work?

00:43:09.420 --> 00:43:10.000
How does that work? Do you know?

00:43:10.000 --> 00:43:15.200
It's basically the point with Meltano is that everything is versioned in Git.

00:43:15.200 --> 00:43:19.920
So that's another movement that's been happening in the data engineering ecosystem where early on,

00:43:19.920 --> 00:43:24.080
a lot of the people coming to it were systems administrators, database administrators,

00:43:24.080 --> 00:43:28.280
maybe data scientists who had a lot of the domain knowledge,

00:43:28.420 --> 00:43:34.000
but not as much of the engineering expertise to be able to build these workflows in a highly engineered,

00:43:34.000 --> 00:43:35.040
highly repeatable way.

00:43:35.040 --> 00:43:41.980
And the past few years has been seeing a lot of movement of moving to data ops and ML ops to

00:43:41.980 --> 00:43:46.580
make sure that all of these workflows are well-engineered, well-managed,

00:43:46.580 --> 00:43:48.820
you know, version controlled, tested.

00:43:49.480 --> 00:43:55.400
And so having this DevOps oriented approach to data integration is what Meltano is focusing on,

00:43:55.400 --> 00:43:58.500
saying all of your configuration, all of your workflows, it lives in Git.

00:43:58.500 --> 00:44:01.620
You can run it through your CI-CD pipeline to make sure that it's tested.

00:44:01.620 --> 00:44:06.380
And then when you deliver it, you know that you can trust that it's going to do what you want it to do

00:44:06.380 --> 00:44:11.740
rather than I just push this config from my laptop and hopefully it doesn't blow up.

00:44:11.740 --> 00:44:12.560
Right.

00:44:12.560 --> 00:44:18.140
It also sounds like there's a lot of interplay between these things like Meltano might be leveraging Airflow

00:44:18.140 --> 00:44:26.300
and DBT and maybe you want to test this through CI with great expectations before it goes through its CD side,

00:44:26.300 --> 00:44:27.180
like continuous deployment.

00:44:27.180 --> 00:44:29.520
Seems like there's just a lot of interflow here.

00:44:29.520 --> 00:44:30.120
Definitely.

00:44:30.120 --> 00:44:34.360
And there have been a few times where I've been talking to people and they've asked me to kind of

00:44:34.360 --> 00:44:40.060
categorize different tools or like draw nice lines about what are the dividing layers of the different

00:44:40.060 --> 00:44:40.960
of the data stack.

00:44:40.960 --> 00:44:45.600
And it's not an easy answer because so many of these tools fit into a lot of different boxes.

00:44:46.540 --> 00:44:52.180
So, you know, Spark is a streaming engine, but it's also an ELT tool.

00:44:52.180 --> 00:45:00.820
And, you know, Dagster is a data orchestration tool, but it can also be used for managing delivery.

00:45:00.820 --> 00:45:02.940
You can write it to do arbitrary tasks.

00:45:02.940 --> 00:45:04.280
So you can build up these chains of tasks.

00:45:04.280 --> 00:45:06.860
So if you wanted to use it for UCI-CD, you could.

00:45:07.700 --> 00:45:08.820
Quite what it's built for.

00:45:08.820 --> 00:45:14.080
But, you know, and then different databases have been growing a lot of different capabilities

00:45:14.080 --> 00:45:18.680
where, you know, it used to be you had your SQL database or you had your document database

00:45:18.680 --> 00:45:20.020
or you had your graph database.

00:45:20.020 --> 00:45:26.060
And then you have things like Arango DB, which can be a graph database and a document database

00:45:26.060 --> 00:45:28.880
and a SQL database all on the same engine.

00:45:28.880 --> 00:45:31.140
So, you know, there's a lot of multimodal databases.

00:45:31.660 --> 00:45:34.040
It's all of the SQL and all the NoSQL, all in one.

00:45:34.040 --> 00:45:34.840
Right.

00:45:34.840 --> 00:45:39.220
And, you know, JSON is being pushed into relational databases and data warehouses.

00:45:39.220 --> 00:45:43.520
So there's a lot of crossover between the different aspects of the data stack.

00:45:43.520 --> 00:45:48.080
Yeah, there probably is more of that, I would say, in this like data warehousing stuff.

00:45:48.080 --> 00:45:52.840
You know, in an operational database, it doesn't necessarily make a ton of sense to jam JSON

00:45:52.840 --> 00:45:54.360
blobs all over the place.

00:45:54.420 --> 00:45:55.940
You might as well just make tables and columns.

00:45:55.940 --> 00:45:56.200
Yeah.

00:45:56.200 --> 00:45:57.580
No, it makes some sense, but not that much.

00:45:57.580 --> 00:46:00.620
But in this space, you might get a bunch of things you don't really know what their shape

00:46:00.620 --> 00:46:02.900
is or exactly you're not ready to process it.

00:46:02.900 --> 00:46:05.140
You just want to save it and then try to deal with it later.

00:46:05.140 --> 00:46:09.500
So do you see more of that, those kind of JSON columns or more NoSQL stuff?

00:46:09.500 --> 00:46:10.060
Absolutely.

00:46:10.060 --> 00:46:15.520
Basically, any data warehouse worth its salt these days has to have some sort of support for nested

00:46:15.520 --> 00:46:15.840
data.

00:46:15.840 --> 00:46:20.520
A lot of that, too, comes out of the outgrowth of, you know, we had the first generation data

00:46:20.520 --> 00:46:21.060
warehouses.

00:46:21.920 --> 00:46:24.680
They did their thing, but they were difficult to scale and they were very expensive.

00:46:24.680 --> 00:46:29.320
And you had to buy these beefy machines so that you were planning for the maximum capacity

00:46:29.320 --> 00:46:30.420
that you're going to have.

00:46:30.420 --> 00:46:34.520
And then came things like Hadoop, where you said, oh, you can scale out as much as you

00:46:34.520 --> 00:46:35.580
want, just add more machines.

00:46:35.580 --> 00:46:36.280
They're all commodity.

00:46:36.280 --> 00:46:40.380
And so that brought in the era of the data lake.

00:46:40.380 --> 00:46:45.680
And then things like S3 became inexpensive enough that you could put all of your data storage

00:46:45.680 --> 00:46:49.680
in S3, but then still use the rest of the Hadoop ecosystem for doing MapReduce jobs

00:46:49.680 --> 00:46:50.180
on that.

00:46:50.480 --> 00:46:53.240
And then that became the next generation data lake.

00:46:53.240 --> 00:46:58.560
And then things like Presto came along to be able to build a data warehouse interface on

00:46:58.560 --> 00:47:01.520
top of this distributed data and these various data sources.

00:47:01.520 --> 00:47:07.860
And then you had the dedicated data warehouses built for the cloud, where they were designed

00:47:07.860 --> 00:47:12.100
to be able to ingest data from S3, where you might have a lot of unstructured information.

00:47:12.100 --> 00:47:17.300
And then you can clean it up using things like DBT to build these transformations, have these

00:47:17.300 --> 00:47:23.120
nicely structured tables built off of this nested or messy data that you're pulling in from various

00:47:23.120 --> 00:47:23.700
data sources.

00:47:23.700 --> 00:47:24.160
Yeah.

00:47:24.160 --> 00:47:24.780
Interesting.

00:47:24.780 --> 00:47:29.880
When you see the story of versioning of this, the data itself, I'm thinking.

00:47:30.080 --> 00:47:35.800
So I've got this huge pile of data I've built up and we're using to drive these pipelines.

00:47:35.800 --> 00:47:38.280
But it seems like the kind of data that could change.

00:47:38.280 --> 00:47:43.140
Or I brought in a new source now that we've switched credit card providers or we're now screen

00:47:43.140 --> 00:47:44.200
scraping extra data.

00:47:44.200 --> 00:47:46.020
Do you see anything interesting happen there?

00:47:46.020 --> 00:47:46.340
Yeah.

00:47:46.340 --> 00:47:49.580
So there's definitely a lot of interesting stuff happening in the data versioning space.

00:47:49.580 --> 00:47:55.420
So I mean, one tool that was kind of early to the party is a platform called Packaderm.

00:47:55.420 --> 00:48:01.340
They're designed as a end-to-end solution built on top of Kubernetes for being able to do data

00:48:01.340 --> 00:48:03.660
science and data engineering and data versioning.

00:48:03.660 --> 00:48:06.320
So your code and your data all gets versioned together.

00:48:06.320 --> 00:48:13.860
There's a system called LakeFS that was released recently that provides a Git-like workflow on top

00:48:13.860 --> 00:48:15.940
of your data that lives in S3.

00:48:15.940 --> 00:48:21.020
And so they act as a proxy to S3, but it lets you branch your data to say,

00:48:21.020 --> 00:48:23.420
I want to bring in this new data source.

00:48:23.420 --> 00:48:27.240
And as long as everything is using LakeFS as the interface,

00:48:27.240 --> 00:48:32.700
then your main branch won't see any of this new data source until you are happy with it.

00:48:32.700 --> 00:48:36.380
And then you can commit it and merge it back into the main branch and then it becomes live.

00:48:36.380 --> 00:48:40.560
And so this is a way to be able to experiment with different processing workflows to say,

00:48:40.560 --> 00:48:43.640
I want to try out this new transformation job or this new batch job,

00:48:43.640 --> 00:48:47.300
or I want to bring in this new data source, but I'm not quite confident about it yet.

00:48:47.300 --> 00:48:49.180
And so it brings in this versioning workflow.

00:48:49.180 --> 00:48:53.440
There's another system combination of two tools called Iceberg,

00:48:53.440 --> 00:48:58.000
which is a table format for use in these large-scale data lakes,

00:48:58.000 --> 00:49:01.560
data warehouses that hooks into things like Spark and Presto.

00:49:01.560 --> 00:49:06.820
And there's another accompanying project called Nessie that is inspired by Git for being able

00:49:06.820 --> 00:49:10.500
to do the same type of branching and merging workflow for bringing in new data sources

00:49:10.500 --> 00:49:12.960
or changing table schemas and things like that.

00:49:12.960 --> 00:49:13.280
Wow.

00:49:13.280 --> 00:49:17.500
These all sound like such fun tools to learn and they're all solving painful problems.

00:49:17.500 --> 00:49:17.840
Right.

00:49:17.840 --> 00:49:22.860
And then another one actually from the Python ecosystem is DVC or data version control that's

00:49:22.860 --> 00:49:30.100
built for machine learning and data science workflows that actually integrates with your source code

00:49:30.100 --> 00:49:33.000
management so that you get commit and get push.

00:49:33.260 --> 00:49:37.480
There's some additional commands, but they're modeled after Git where you commit your code

00:49:37.480 --> 00:49:42.100
and then you also push your data and it lives in S3 and it will version the data assets so that

00:49:42.100 --> 00:49:46.060
as you make different versions of your experiment with different versions of your data,

00:49:46.060 --> 00:49:51.940
it all lives together so that it's repeatable and easier for multiple data scientists or data engineers

00:49:51.940 --> 00:49:53.180
to be able to collaborate on it.

00:49:53.180 --> 00:49:59.640
Yeah, the version control story around data has always been interesting, right?

00:49:59.640 --> 00:50:00.760
It's super tricky.

00:50:00.760 --> 00:50:01.300
Absolutely.

00:50:01.300 --> 00:50:04.900
I mean, on one hand, your schemas might have to evolve over time.

00:50:04.900 --> 00:50:11.020
If you've got a SQLAlchemy model trying to talk to a database, it really hates it if there's a mismatch at all, right?

00:50:11.020 --> 00:50:17.200
And so you want those things to go, the database schema maybe to change along with your code with like migrations or something.

00:50:17.320 --> 00:50:19.680
But then the data itself, yeah, that's tricky.

00:50:19.680 --> 00:50:20.000
Yeah.

00:50:20.000 --> 00:50:24.640
And so there's actually a tool called Avro and another one called Parquet.

00:50:24.640 --> 00:50:27.520
Well, they're tools, they're data serialization formats.

00:50:27.520 --> 00:50:36.440
And Avro in particular has a concept of schema evolution for, you know, what are compatible evolutions of a given schema.

00:50:36.440 --> 00:50:40.940
So each record in an Avro file has the schema co-located with it.

00:50:40.940 --> 00:50:44.300
So it's kind of like a binary version of JSON, but the schema is embedded with it.

00:50:44.300 --> 00:50:44.660
Oh, wow.

00:50:44.660 --> 00:50:45.520
Okay, that's interesting.

00:50:45.640 --> 00:50:54.640
Yeah. So if you say, I want to change the type of this column from an int to a float, then, you know, maybe that's a supported conversion.

00:50:54.640 --> 00:50:58.160
And so it will let you change the schemas or add columns.

00:50:58.160 --> 00:51:04.400
But if you try to change the schema in a method that is not backwards compatible, it will actually throw an error.

00:51:04.400 --> 00:51:08.540
I see. Like a float to an int might drop data, but an int to a float probably wouldn't.

00:51:08.620 --> 00:51:11.260
Exactly. So it will let you evolve your schemas.

00:51:11.260 --> 00:51:20.620
And Parquet is actually built to be interoperable with Avro for being able to handle those schema evolutions as well, where Avro is a row or record oriented format.

00:51:20.620 --> 00:51:25.660
And Parquet is column oriented, which is more powerful for being able to do aggregate analytics.

00:51:25.660 --> 00:51:29.280
And it's more efficient so that you're not pulling all of the data for every row.

00:51:29.280 --> 00:51:31.320
You're just pulling all of the data for a given column.

00:51:31.540 --> 00:51:32.680
So it's also more compressible.

00:51:32.680 --> 00:51:37.640
Yeah. I think I need to do more thinking to really fully grok the column oriented data stores.

00:51:37.640 --> 00:51:37.980
Yeah.

00:51:37.980 --> 00:51:38.980
It's a different way of thinking.

00:51:38.980 --> 00:51:49.600
Yeah. The column oriented aspect is also a major revolution in how data warehousing has come about, where, you know, the first generation was all built on the same databases that we were using for our application.

00:51:49.600 --> 00:51:55.000
So it was all row oriented. And that was one of the inherent limits to how well they could scale their compute.

00:51:55.000 --> 00:52:01.840
Whereas all of the modern cloud data warehouses or all the modern, even non-cloud data warehouses are column oriented.

00:52:02.380 --> 00:52:14.020
And so if you have, you know, one column that is street addresses and another column that's integers and another column that is, you know, bare care 15, all of those are the same data type.

00:52:14.020 --> 00:52:23.720
And so they can compress them down a lot more than if you have one row that is a street address and a text field and an integer and a float and a JSON array.

00:52:23.720 --> 00:52:27.020
If you try to compress all of those together, they're not compatible data types.

00:52:27.160 --> 00:52:31.060
So you have a lot more inefficiency in terms of how well you can compress it.

00:52:31.060 --> 00:52:39.220
And then also as you're scanning, you know, a lot of analytics jobs are operating more on aggregates of information than on individual records.

00:52:39.220 --> 00:52:49.720
And so if you want to say, I want to find out what is the most common street name across all the street addresses that I have in my database, all I have to do is pull all the information out of that street address column.

00:52:49.720 --> 00:52:51.520
It's all co-located on disk.

00:52:51.520 --> 00:52:55.060
So it's a faster seek time and it's all compressed the same.

00:52:55.060 --> 00:53:02.840
And that way you don't have to read all of the values for all of the rows to get all of the street addresses, which is what you would do in a relational database.

00:53:02.840 --> 00:53:03.220
Right.

00:53:03.220 --> 00:53:06.240
Because probably those are co-located on disk by row.

00:53:06.240 --> 00:53:06.760
Exactly.

00:53:06.760 --> 00:53:14.800
Whereas if you're going to ask, so all about the streets across everyone, then it's better to put all the streets and then all the cities or whatever.

00:53:14.800 --> 00:53:15.100
Right.

00:53:15.100 --> 00:53:15.540
Exactly.

00:53:15.540 --> 00:53:16.040
Interesting.

00:53:16.040 --> 00:53:16.460
Cool.

00:53:16.460 --> 00:53:18.180
Well, I think I actually understand a little bit better now.

00:53:18.180 --> 00:53:18.460
Thanks.

00:53:18.460 --> 00:53:23.900
The final one that you put on the list that just maybe to put a pen in it as a very, very popular pandas.

00:53:24.020 --> 00:53:26.820
I never cease to be amazed with what you can do with pandas.

00:53:26.820 --> 00:53:27.100
Yeah.

00:53:27.100 --> 00:53:31.400
So, I mean, pandas, it's one of the most flexible tools in the Python toolbox.

00:53:31.400 --> 00:53:34.480
I've used it in web development contexts.

00:53:34.480 --> 00:53:36.280
I've used it for data engineering.

00:53:36.280 --> 00:53:37.740
I've used it for data analysis.

00:53:37.740 --> 00:53:40.740
And it's definitely the Swiss army knife of data.

00:53:40.940 --> 00:53:47.140
So it's absolutely one of the more critical tools in the toolbox of anybody who's working with data, regardless of the context.

00:53:47.140 --> 00:53:50.840
And so it's absolutely no surprise that data engineers reach for it a lot as well.

00:53:51.400 --> 00:53:59.980
So pandas is supported natively in things like Dagster, where it will give you a lot of rich metadata information about the column layouts and the data distributions.

00:53:59.980 --> 00:54:02.520
But yeah, it's just absolutely indispensable.

00:54:02.520 --> 00:54:05.540
You know, it's been covered enough times in both your show and mine.

00:54:05.540 --> 00:54:06.980
We don't need to go too deep into it.

00:54:07.080 --> 00:54:11.000
But yeah, if you're working with data, absolutely get at least a little bit familiar with pandas.

00:54:11.000 --> 00:54:19.200
Well, just to give people a sense, like one of the things I learned yesterday, I think it was, Chris Moffitt was showing off some things with pandas.

00:54:19.200 --> 00:54:24.160
And he's like, oh, over on this Wikipedia page, three-fourths of the way down, there's a table.

00:54:24.160 --> 00:54:26.580
The table has a header that has a name.

00:54:26.580 --> 00:54:33.940
And you can just say, load HTML, give me the table called this as a data frame from screen scraping as part of the page.

00:54:33.940 --> 00:54:34.460
It's amazing.

00:54:34.460 --> 00:54:35.160
Yeah.

00:54:35.160 --> 00:54:45.160
Another interesting aspect of the pandas ecosystem is the pandas extension arrays library that lets you create plugins for pandas to support custom data types.

00:54:45.160 --> 00:54:56.540
So I know that they have support for things like geo.json and IP addresses so that you can do more interesting things out of the box in terms of aggregates and group buys and things like that.

00:54:56.540 --> 00:55:06.100
So, you know, if you have the IP address pandas extension, then you can say, give me all of the rows that are grouped by this network prefix and things like that.

00:55:06.100 --> 00:55:09.160
Whereas just pandas out of the box, we'll just treat it as an object.

00:55:09.160 --> 00:55:13.540
And so you have to do a lot more additional coding around it and it's not as efficient.

00:55:13.760 --> 00:55:18.080
So that was an interesting aspect to the pandas ecosystem as well.

00:55:18.080 --> 00:55:18.480
Nice.

00:55:18.480 --> 00:55:19.540
One quick question.

00:55:19.540 --> 00:55:26.080
And then I think we should probably wrap this up, Stefan, throughout some stuff about graph databases, particularly GraphQL.

00:55:26.080 --> 00:55:27.720
Or that's actually the API, right?

00:55:27.720 --> 00:55:29.860
It's efficient, but what about its maturity?

00:55:29.860 --> 00:55:33.220
Like, what do you think about some of these new API endpoints?

00:55:33.220 --> 00:55:36.240
GraphQL is definitely gaining a lot of popularity.

00:55:36.240 --> 00:55:41.060
I mean, so as you mentioned, there's sometimes a little bit of confusion about they both have the word graph in the name.

00:55:41.060 --> 00:55:42.400
So GraphQL and GraphDB.

00:55:42.620 --> 00:55:43.380
I read it too quickly.

00:55:43.380 --> 00:55:43.660
Yeah.

00:55:43.660 --> 00:55:45.320
I'm like, oh, yeah, like Neo4j.

00:55:45.320 --> 00:55:46.520
Wait, no, it has nothing to do with that.

00:55:46.520 --> 00:55:46.840
Right.

00:55:46.840 --> 00:55:51.020
So, you know, GraphQL is definitely a popular API design.

00:55:51.020 --> 00:55:57.440
Interesting side note is that the guy who created Dagster is also one of the co-creators of GraphQL.

00:55:57.940 --> 00:56:07.340
And Dagster has a really nice web UI that comes out of the box that has a GraphQL API to it so that you can do things like trigger jobs or introspect information about the running system.

00:56:07.340 --> 00:56:16.120
Another interesting use case, use of GraphQL is there's a database engine called GraphQL that uses GraphQL as its query language.

00:56:16.420 --> 00:56:18.800
So it's a native graph storage engine.

00:56:18.800 --> 00:56:21.060
It's scalable, horizontally distributable.

00:56:21.060 --> 00:56:26.080
And so you can actually model your data as a graph and then query it using GraphQL.

00:56:26.080 --> 00:56:31.000
So certainly seeing a lot of interesting use cases within the data ecosystem as well.

00:56:31.200 --> 00:56:31.280
Yeah.

00:56:31.280 --> 00:56:37.060
For the right hyperdata, a graph database seems like it would really light up the speed of accessing certain things.

00:56:37.060 --> 00:56:37.420
Absolutely.

00:56:37.420 --> 00:56:38.020
Yeah.

00:56:38.020 --> 00:56:45.500
So the funny thing is you have this concept of a relational database, but it's actually not very good at storing information about relationships.

00:56:45.500 --> 00:56:46.660
It is.

00:56:46.660 --> 00:56:48.800
The joins make them so slow and so on.

00:56:48.800 --> 00:56:49.060
Exactly.

00:56:49.060 --> 00:56:50.600
The lazy loading or whatever.

00:56:50.600 --> 00:56:50.900
Yeah.

00:56:50.900 --> 00:56:51.280
Right.

00:56:51.280 --> 00:57:01.080
So graph databases are entirely optimized for storing information about relationships so that you can do things like network traversals or understanding within this,

00:57:01.080 --> 00:57:02.640
structure of relations.

00:57:02.640 --> 00:57:11.540
You know, things like social networks are kind of the natural example of a graph problem where I want to understand what are the degrees of separation between these people.

00:57:11.540 --> 00:57:13.660
So, you know, the six degrees of Kevin Bacon kind of thing.

00:57:13.660 --> 00:57:13.940
Yeah.

00:57:13.940 --> 00:57:14.440
Yeah.

00:57:14.440 --> 00:57:21.780
Seems like you could also model a lot of interesting things like the, I don't know how real it is, but, you know, the bananas are at the back or the milk is at the back of the store.

00:57:21.780 --> 00:57:27.000
So you have to walk all the way through the store and you can find those kind of traversing those like behaviors and relationships.

00:57:27.000 --> 00:57:27.160
Yeah.

00:57:27.160 --> 00:57:29.200
The traveling salesman problem, stuff like that.

00:57:29.200 --> 00:57:29.520
Yeah.

00:57:29.520 --> 00:57:30.400
Yeah, exactly.

00:57:30.800 --> 00:57:31.100
All right.

00:57:31.100 --> 00:57:42.460
Well, so many tools, way more than five that we actually made our way through, but very, very interesting because I think there's just so much out there and it sounds like a really fun place to work, like a technical space to work.

00:57:42.460 --> 00:57:42.820
Absolutely.

00:57:42.820 --> 00:57:53.840
You know, a lot of these ideas also seem like they're probably really ripe for people who have programming skills and software engineering mindsets like CICD testing and so on.

00:57:53.840 --> 00:57:54.180
Absolutely.

00:57:54.180 --> 00:57:56.460
They'll come in and say, I could make a huge impact.

00:57:56.640 --> 00:58:26.620
We have this organization that's going to be able to do that.

00:58:26.620 --> 00:58:30.240
You know, the first step is, you know, just kind of start to take a look at it.

00:58:30.240 --> 00:58:39.280
You know, you probably have data problems in your applications that you're working with that maybe you're just using a sequence of salary jobs and hoping that they complete in the right order.

00:58:39.280 --> 00:58:45.000
You know, maybe take a look at something like Dagster or Prefect to build a more structured graph of execution.

00:58:45.640 --> 00:58:54.040
If you don't want to go for a full fledged framework like that, there are also tools like Bonobo that are just command line oriented that help you build up that same structured graph of execution.

00:58:54.040 --> 00:58:59.620
So, you know, definitely just start to take a look and try and understand, like, what are the data flows in your system?

00:58:59.620 --> 00:59:09.360
If you think about it more than just flows of logic and think about it in flows of data, then it starts to become a more natural space to solve it with some of these different tools and practices.

00:59:09.740 --> 00:59:12.140
So getting familiar with thinking about it in that way.

00:59:12.140 --> 00:59:20.580
Another really great book, if you're definitely interested in data engineering and want to kind of get deep behind the scenes, is Designing Data Intensive Applications.

00:59:20.580 --> 00:59:28.960
I read that book recently and learned a whole lot more than I thought I would about just the entire space of building applications oriented around data.

00:59:28.960 --> 00:59:30.560
So great resource there.

00:59:30.560 --> 00:59:31.760
Nice. We'll put those in the show notes.

00:59:31.760 --> 00:59:38.760
Yeah. And also just kind of raise your hand, say to your management or your team to say, hey, it looks like we have some data problems.

00:59:38.760 --> 00:59:40.240
I'm interested in digging into it.

00:59:40.240 --> 00:59:42.280
And chances are they'll welcome the help.

00:59:42.280 --> 00:59:46.480
You know, lots of great resources out there if you want to get if you want to learn more about it.

00:59:46.480 --> 00:59:47.800
You know, shameless plug.

00:59:47.800 --> 00:59:49.640
The Data Engineering Podcast is one of them.

00:59:49.640 --> 00:59:52.520
I'm always happy to help answer questions.

00:59:52.520 --> 00:59:59.980
Yeah. I mean, basically just start to dig into the space, take a look at some of the tools and frameworks and just try to implement them in your day to day work.

00:59:59.980 --> 01:00:02.980
You know, a lot of data engineers come from software engineering backgrounds.

01:00:02.980 --> 01:00:09.500
A lot of data engineers might come from database administrator positions because they're familiar with the problem domain of the data.

01:00:09.500 --> 01:00:13.300
And then it's a matter of learning the actual engineering aspects of it.

01:00:13.300 --> 01:00:24.000
A lot of people come from data analyst or data scientist backgrounds where they actually decide that they enjoy working more with getting the data clean and well managed than doing the actual analysis on it.

01:00:24.000 --> 01:00:27.760
So there's not really any one concrete background to come from.

01:00:27.760 --> 01:00:34.120
It's more just a matter of being interested in making the data reproducible, helping make it valuable.

01:00:34.120 --> 01:00:50.700
Interesting note is that if you look at some of the statistics around it, there are actually more data engineering positions open, at least in the U.S., than there are data scientist positions because of the fact that it is such a necessary step in the overall lifecycle of data.

01:00:51.320 --> 01:01:21.300
How interesting.

01:01:21.300 --> 01:01:22.840
solid results because of solid output.

01:01:22.840 --> 01:01:25.220
Those are extremely marketable skills.

01:01:25.220 --> 01:01:25.860
That's awesome.

01:01:25.860 --> 01:01:26.040
Exactly.

01:01:26.040 --> 01:01:26.740
All right.

01:01:26.740 --> 01:01:29.280
Well, Tobias, thanks so much for covering that.

01:01:29.280 --> 01:01:32.400
Before we get out of here, though, final two questions.

01:01:32.400 --> 01:01:36.020
So if you're going to write some Python code, what editor do you use these days?

01:01:36.020 --> 01:01:38.460
So I've been using Emacs for a number of years now.

01:01:38.580 --> 01:01:46.020
I've tried out things like PyTarm and VS Code here and there, but it just never feels quite right just because my fingers have gotten so used to Emacs.

01:01:46.020 --> 01:01:50.300
You just want to have an entire operating system as your editor, not just a piece of software.

01:01:50.300 --> 01:01:51.040
Exactly.

01:01:51.040 --> 01:01:51.140
Got you.

01:01:52.040 --> 01:01:54.800
And it has that ML background with Lisp as its language.

01:01:54.800 --> 01:01:55.100
Right.

01:01:55.640 --> 01:01:58.400
And then notable PyPI package or packages.

01:01:58.400 --> 01:01:58.940
Yeah.

01:01:58.940 --> 01:01:59.420
You should check out.

01:01:59.420 --> 01:02:00.600
I mean, we kind of touched on some, right?

01:02:00.600 --> 01:02:01.140
Yeah, exactly.

01:02:01.140 --> 01:02:03.020
I mean, a lot of them in the list here.

01:02:03.020 --> 01:02:07.020
I'll just mention again, Dagster, DBT, and Great Expectations.

01:02:07.020 --> 01:02:07.480
Yeah.

01:02:07.480 --> 01:02:08.000
Very nice.

01:02:08.000 --> 01:02:08.600
All right.

01:02:08.600 --> 01:02:09.440
Final call to action.

01:02:09.820 --> 01:02:11.220
People are excited about this.

01:02:11.220 --> 01:02:11.980
What should they do?

01:02:11.980 --> 01:02:13.380
Listen to the Data Engineering Podcast.

01:02:13.380 --> 01:02:18.020
Listen to podcast.init if you want to understand a little bit more about the whole ecosystem.

01:02:18.020 --> 01:02:29.340
Because since I do spend so much time in the data engineering space, I sometimes have crossover where if there's a data engineering tool that's implemented in Python, I'll have them on podcast.init just to make sure that I can get everybody out there.

01:02:29.340 --> 01:02:32.860
And yeah, feel free to send questions my way.

01:02:32.860 --> 01:02:36.200
I'll add the information about the podcast in the show notes.

01:02:36.200 --> 01:02:38.280
And yeah, just be curious.

01:02:38.280 --> 01:02:39.240
Yeah, absolutely.

01:02:39.500 --> 01:02:45.020
Well, like I said, it looks like a really interesting and growing space that has got a lot of low-hanging fruit.

01:02:45.020 --> 01:02:46.380
So it sounds like a lot of fun.

01:02:46.380 --> 01:02:46.720
Absolutely.

01:02:46.720 --> 01:02:47.080
Yeah.

01:02:47.080 --> 01:02:47.480
All right.

01:02:47.480 --> 01:02:48.200
Well, thanks for being here.

01:02:48.200 --> 01:02:49.360
And thanks, everyone, for listening.

01:02:49.360 --> 01:02:50.080
Thanks for having me.

01:02:50.080 --> 01:02:53.960
This has been another episode of Talk Python To Me.

01:02:53.960 --> 01:02:58.820
Our guest in this episode was Tobias Macy, and it's been brought to you by Datadog and Retool.

01:02:58.820 --> 01:03:02.920
Datadog gives you visibility into the whole system running your code.

01:03:02.920 --> 01:03:06.740
Visit talkpython.fm/datadog and see what you've been missing.

01:03:06.740 --> 01:03:08.900
We'll throw in a free t-shirt with your free trial.

01:03:09.180 --> 01:03:12.300
Supercharge your developers and power users.

01:03:12.300 --> 01:03:16.960
Let them build and maintain their internal tools quickly and easily with Retool.

01:03:16.960 --> 01:03:21.300
Just visit talkpython.fm/retool and get started today.

01:03:21.300 --> 01:03:23.300
Want to level up your Python?

01:03:23.300 --> 01:03:27.340
We have one of the largest catalogs of Python video courses over at Talk Python.

01:03:27.680 --> 01:03:32.520
Our content ranges from true beginners to deeply advanced topics like memory and async.

01:03:32.520 --> 01:03:35.200
And best of all, there's not a subscription in sight.

01:03:35.200 --> 01:03:38.100
Check it out for yourself at training.talkpython.fm.

01:03:38.460 --> 01:03:40.200
Be sure to subscribe to the show.

01:03:40.200 --> 01:03:42.980
Open your favorite podcast app and search for Python.

01:03:42.980 --> 01:03:44.300
We should be right at the top.

01:03:44.300 --> 01:03:50.380
You can also find the iTunes feed at /itunes, the Google Play feed at /play, and the direct

01:03:50.380 --> 01:03:53.640
RSS feed at /rss on talkpython.fm.

01:03:54.520 --> 01:03:57.080
We're live streaming most of our recordings these days.

01:03:57.080 --> 01:04:01.560
If you want to be part of the show and have your comments featured on the air, be sure to subscribe

01:04:01.560 --> 01:04:04.920
to our YouTube channel at talkpython.fm/youtube.

01:04:05.400 --> 01:04:06.760
This is your host, Michael Kennedy.

01:04:06.760 --> 01:04:08.060
Thanks so much for listening.

01:04:08.060 --> 01:04:09.220
I really appreciate it.

01:04:09.220 --> 01:04:11.120
Now get out there and write some Python code.

01:04:11.120 --> 01:04:11.740
Bye.

01:04:11.740 --> 01:04:12.740
Bye.

01:04:12.740 --> 01:04:13.740
Bye.

01:04:13.740 --> 01:04:13.740
Bye.

01:04:13.740 --> 01:04:14.740
Bye.

01:04:14.740 --> 01:04:15.740
Bye.

01:04:15.740 --> 01:04:16.740
Bye.

01:04:16.740 --> 01:04:17.740
Bye.

01:04:17.740 --> 01:04:18.740
Bye.

01:04:18.740 --> 01:04:19.740
Bye.

01:04:19.740 --> 01:04:20.740
Bye.

01:04:20.740 --> 01:04:21.740
Bye.

01:04:21.740 --> 01:04:22.740
Bye.

01:04:22.740 --> 01:04:23.740
Bye.

01:04:23.740 --> 01:04:24.740
Bye.

01:04:24.740 --> 01:04:25.740
Bye.

01:04:25.740 --> 01:04:26.740
Bye.

01:04:26.740 --> 01:04:27.740
Bye.

01:04:27.740 --> 01:04:28.740
Bye.

01:04:28.740 --> 01:04:29.240
you

01:04:29.240 --> 01:04:29.740
you

01:04:29.740 --> 01:04:31.740
Thank you.

01:04:31.740 --> 01:05:01.720
Thank you.

