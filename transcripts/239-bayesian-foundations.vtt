WEBVTT

00:00:00.001 --> 00:00:03.800
In this episode, we'll dive deep into one of the foundations of modern data science,

00:00:03.800 --> 00:00:09.240
Bayesian algorithms and Bayesian thinking. Join me along with guest Max Sklar as we look at the

00:00:09.240 --> 00:00:16.280
algorithmic side of data science. This is Talk Python To Me, episode 239, recorded November 10th, 2019.

00:00:29.400 --> 00:00:34.660
Welcome to Talk Python To Me, a weekly podcast on Python, the language, the libraries, the ecosystem,

00:00:34.660 --> 00:00:39.740
and the personalities. This is your host, Michael Kennedy. Follow me on Twitter where I'm @mkennedy.

00:00:39.740 --> 00:00:44.860
Keep up with the show and listen to past episodes at talkpython.fm and follow the show on Twitter

00:00:44.860 --> 00:00:50.580
via at Talk Python. This episode is brought to you by Linode and Tidelift. Please check out what

00:00:50.580 --> 00:00:55.120
they're offering during their segments. It really helps support the show. Max, welcome to Talk Python

00:00:55.120 --> 00:00:59.000
to Me. Thanks for having me, Michael. It's very great to be on. It's great to have you on as well.

00:00:59.000 --> 00:01:02.880
You've been on Python Bytes before, but never Talk Python To Me.

00:01:02.880 --> 00:01:06.980
That was a lot of fun. I actually got someone reached out to me on Twitter the other day saying,

00:01:06.980 --> 00:01:09.760
hey, I saw you on Python Bytes. So that was really exciting.

00:01:09.760 --> 00:01:11.660
Right on, right on. That's super cool.

00:01:11.660 --> 00:01:16.240
I heard you on Python Bytes. I always say saw you when it's really heard you, but anyway.

00:01:16.240 --> 00:01:20.680
It's all good. So now they can say they saw you on Talk Python To Me as well.

00:01:20.680 --> 00:01:26.640
Now, we're going to talk about some of the foundational ideas behind data science,

00:01:26.640 --> 00:01:31.160
machine learning. That's going to be a lot of fun. But before we get to them, let's set the stage and

00:01:31.160 --> 00:01:34.480
give people a sense of where you're coming from. How do you get into programming in Python?

00:01:34.480 --> 00:01:40.080
That is a really interesting question because I think I started in Python a very long time ago,

00:01:40.080 --> 00:01:47.980
like 10 years ago maybe. I was working on kind of a side project called stickymap.com. The website's

00:01:47.980 --> 00:01:53.740
still up. It barely works. But it was basically a, it was, it was like my senior project as an

00:01:53.740 --> 00:01:59.800
undergrad. So I really, I started this in 2005. And what it was, was it was, you know, Google Maps

00:01:59.800 --> 00:02:05.620
had just come out with their API where you can like, you know, include a Google map on your site.

00:02:05.620 --> 00:02:10.940
And so I was like, okay, this is cool. What can I do with this? Let's add markers all over the map

00:02:10.940 --> 00:02:15.900
and it could be user generated. We would call them emojis now. And people could leave little messages

00:02:15.900 --> 00:02:19.940
and little locations and things like that. This was before there was Foursquare, which is where I

00:02:19.940 --> 00:02:24.380
worked, which is location intelligence. This was just me messing around, trying to make something cool

00:02:24.380 --> 00:02:30.880
and being inspired by the whole host of like, you know, social media startups that were happening at

00:02:30.880 --> 00:02:38.220
the time. And I was using, what was I using at the time? I was using PHP and MySQL to put that

00:02:38.220 --> 00:02:42.300
together. I knew nothing about web development. So I went to the Barnes and Noble. I got that book,

00:02:42.300 --> 00:02:48.560
PHP, MySQL. I got it. But then sometime around like 2008, 2009, I realized, you know, a lot of people

00:02:48.560 --> 00:02:52.780
were talking about Python at work. And I realized like, sometimes I need, this is kind of when I was

00:02:52.780 --> 00:02:59.440
winding down on the project, but I realized, you know, I had all this data and I realized I needed

00:02:59.440 --> 00:03:05.340
a way to like clean the data. I needed a way to like write good scripts that would clear up certain

00:03:05.340 --> 00:03:11.300
like if I have a flat file of like, here's the latitude and longitude, they're separated by

00:03:11.300 --> 00:03:16.320
tabs. And here's a, you know, here's some text that someone wrote that needs to be cleaned up,

00:03:16.320 --> 00:03:23.240
et cetera, et cetera. Yeah, I can write some scripts in like Python or Java, believe it or not, which I

00:03:23.240 --> 00:03:28.320
knew at the time, but then, or sorry, a PHP or Python, which I knew at the time, but like, wait, wait,

00:03:28.760 --> 00:03:34.180
not job. Sorry. I was trying to do it in PHP and Java, which is really bad idea.

00:03:34.180 --> 00:03:39.640
Yeah. Especially PHP sounds tricky. Yeah. Yes, yes, yes. And then I was like, well,

00:03:39.640 --> 00:03:44.220
I'm just learning this Python. I need something. So let me try to do it with Python. And it worked

00:03:44.220 --> 00:03:52.380
really well. And then I had, you know, to deal a lot more with CSVs and stuff like that tab separated

00:03:52.380 --> 00:03:57.980
files. And it really was just a way to like save time at work. And it was like a trick to say,

00:03:57.980 --> 00:04:03.820
hey, that thing that you're doing manually, I can do that in like 10 minutes. And it's not 10 minutes,

00:04:03.820 --> 00:04:08.100
maybe a couple hours and write a script. And it's going to take you like one week. Like I saw someone

00:04:08.100 --> 00:04:12.340
at work trying to change something manually. And so this is all a very long time ago. So I don't

00:04:12.340 --> 00:04:17.000
remember exactly what it was, but it was kind of like a good trick to save time. And it had nothing

00:04:17.000 --> 00:04:20.900
to do with data science or machine learning at the time. It was more like writing scripts to clean up

00:04:20.900 --> 00:04:25.720
files. Well, that's perfect for Python, right? Like it's really one of the things that's super good at.

00:04:25.720 --> 00:04:31.880
It's so easy to read CSV files, JavaScript files, XML, whatever, right? It's just they're all like a

00:04:31.880 --> 00:04:34.400
handful of lines of code and you know, magic happens.

00:04:34.400 --> 00:04:39.680
Yeah. The one thing that I was really impressed with was like, how easy at the time now, when I

00:04:39.680 --> 00:04:44.640
wanted to do more complicated Python packages in like 2012, 2013, I realized, oh, actually,

00:04:45.020 --> 00:04:50.960
some of these packages are complicated to install. But like, I was so impressed with how easy it was

00:04:50.960 --> 00:04:57.000
to just import the CSV package and just be like, okay, now we understand your CSV. If you have some

00:04:57.000 --> 00:05:01.480
stuff in quotes, no problem. If you want to clean up the quotes, no problem. Like it was all just like,

00:05:01.480 --> 00:05:02.700
it just happened very fast.

00:05:02.700 --> 00:05:07.940
Yeah. You don't have to statically link to some library or add a reference to some other thing or

00:05:07.940 --> 00:05:09.860
none of that, right? It's all good. It's all right there.

00:05:09.860 --> 00:05:15.960
Yeah. I mean, that was, those were the days when like, I was still programming in C++ for work. So

00:05:15.960 --> 00:05:21.940
you could imagine what, how big of a jump that was. I mean, that seems so ancient. I used to have to

00:05:21.940 --> 00:05:26.880
program in C++ for the Palm Pilot. That was my first job out of school, which is crazy.

00:05:26.880 --> 00:05:28.480
Oh, wow. That sounds interesting. Yeah.

00:05:28.480 --> 00:05:28.800
Yeah.

00:05:28.860 --> 00:05:33.680
Yeah. Coming from C++, I think people have two different reactions. One, like, wow, this is so

00:05:33.680 --> 00:05:40.020
easy. I can't believe I did this in so few lines. Or this is cheating. It's not real programming.

00:05:40.020 --> 00:05:46.400
It's not for me, you know? But I think people go, who even disagree, like, oh, this is not for me,

00:05:46.400 --> 00:05:48.960
eventually like find their way over. They're pulled in.

00:05:48.960 --> 00:05:54.280
I never had a phase where it was like, oh, this is not for me. But I did have a phase where it was like,

00:05:54.720 --> 00:05:59.360
I don't see, this is just another language. And I don't see why it's better or worse than any other.

00:05:59.360 --> 00:06:02.700
I think that's the phase that you go through when you learn any new language where it's like, okay,

00:06:02.700 --> 00:06:07.240
I see all the features. I don't see what this brings me. It was only through doing those specific

00:06:07.240 --> 00:06:10.400
projects where it was like, aha, no one could have convinced me.

00:06:10.400 --> 00:06:14.360
Yeah. Also, you know, if you come from another language, right, if you come from C++, you come

00:06:14.360 --> 00:06:20.540
from Java, whatever, you know how to solve problems super well in that language. And you're comfortable.

00:06:20.820 --> 00:06:26.080
And when you sit down to work, you say, file a new project and file, new files, start typing.

00:06:26.080 --> 00:06:30.700
And it's like, okay, well, what do I want to do? I want to call this website or talk to this database.

00:06:30.700 --> 00:06:35.700
I'm going to create this and I'll do this. And bam, like, you can just do it. You don't have to just

00:06:35.700 --> 00:06:41.560
pound on every little step. Like, how do I run the code? How do I use another library?

00:06:41.560 --> 00:06:46.760
What libraries are there? Is there like, there's every, you know, it's just that transition is always

00:06:46.760 --> 00:06:52.480
tricky. And it takes a while before you, you get over that and you feel like, okay,

00:06:52.480 --> 00:06:57.660
I really actually do like it over here. I'm going to put the effort into learn it properly because

00:06:57.660 --> 00:07:03.100
I don't care how amazing it is. You're still going to feel incompetent at first.

00:07:03.100 --> 00:07:07.300
The switching costs are so tough. And that's why they say, oh, if you're going to build a new

00:07:07.300 --> 00:07:11.960
product, it has to be like 10 X better than the one that exists or something like that. I don't know

00:07:11.960 --> 00:07:18.040
if that's, you know, literally true, but like it's true with languages too, because it's really hard to

00:07:18.040 --> 00:07:21.860
like pick up a new language and everyone's busy at work and busy doing all the tasks they need to do

00:07:21.860 --> 00:07:27.720
every day. For me, frankly, it was helpful to take that time off in quotes, time off. When I was going to

00:07:27.720 --> 00:07:33.000
grad school, time off from working full-time as a software engineer to actually pick some of this

00:07:33.000 --> 00:07:38.540
stuff up. Absolutely. All right. So you had mentioned earlier that you do stuff at Foursquare and it

00:07:38.540 --> 00:07:44.080
sounds like your early programming experience with sticky maps is not that different than Foursquare,

00:07:44.080 --> 00:07:48.960
honestly. Tell people about what you do. Maybe, I'm pretty sure everyone knows what Foursquare is,

00:07:48.960 --> 00:07:54.320
what you guys do, but tell them what you do there. People might not be aware of where Foursquare is

00:07:54.320 --> 00:08:01.660
today. You know, there is Foursquare is kind of known as that quirky check-in app, find good places

00:08:01.660 --> 00:08:08.320
to go with your friends and eat app, you know, share where you are. And that's where we were in 2011,

00:08:08.320 --> 00:08:14.380
where, when I joined up to, you know, a few years ago, but ultimately, you know, the company kind of

00:08:14.380 --> 00:08:20.060
pivoted business models and sort of said, Hey, we have this really cool technology that we built for the

00:08:20.060 --> 00:08:25.380
consumer apps, which is called Pilgrim, which essentially takes the data from your phone and

00:08:25.380 --> 00:08:30.380
translates that into stops. You know, you'd stopped at Starbucks in the morning, and then you stopped at

00:08:30.380 --> 00:08:35.840
this other place, and then you stopped at work, et cetera, et cetera. And then, you know, that goes into,

00:08:35.840 --> 00:08:41.420
that finds use cases like, you know, across the apposphere, I don't even know what to call it,

00:08:41.420 --> 00:08:47.500
but many apps would like that technology. And so we have this panel and, you know, so for a few years,

00:08:47.500 --> 00:08:53.080
I was working on a product at Foursquare called Attribution, where companies, our clients would say,

00:08:53.080 --> 00:08:57.420
Hey, we want to know if our ads are working, our ads across the internet, not just on Foursquare.

00:08:57.420 --> 00:09:03.000
And we would say, well, we could tell you whether your ads are actually causing people to go into your

00:09:03.000 --> 00:09:09.080
stores more than they otherwise would. And I worked on that for a few years, which is a really cool

00:09:09.080 --> 00:09:12.560
problem to solve, a really cool data science problem to solve, because it's a causality problem.

00:09:12.760 --> 00:09:18.520
It's not just, you know, you can't just say, well, the people who saw the ads visited 10% more,

00:09:18.520 --> 00:09:21.860
because maybe you targeted people who would have visited 10% more.

00:09:21.860 --> 00:09:26.080
Exactly. I'm targeting my demographic, so they better visit more. I got it wrong.

00:09:26.080 --> 00:09:31.460
That industry is a struggle, because the people that you're selling to often don't have the

00:09:31.460 --> 00:09:36.080
backgrounds to understand the difference, and sometimes don't have the incentives to understand

00:09:36.080 --> 00:09:42.420
the difference. But we did the best we could. And so that led to kind of an acquisition that

00:09:42.420 --> 00:09:49.440
Foursquare did earlier this year of Placed, which was an attribution company owned by Snap,

00:09:49.440 --> 00:09:54.020
but they sold it to us through this big deal. You can read about it online.

00:09:54.020 --> 00:09:56.340
Giant tech company trade.

00:09:56.340 --> 00:10:05.100
Yeah. And so I had left Foursquare in the interim, but then I recently went back to work with the

00:10:05.100 --> 00:10:10.320
founder, Dennis Crowley, and just kind of building new apps and trying to build cool apps based on

00:10:10.320 --> 00:10:15.040
location technology, which is really why I got into Foursquare, why I get into Sticky Map,

00:10:15.040 --> 00:10:21.200
and I'm just having so much fun. So that's, and we have some products coming along the way where

00:10:21.200 --> 00:10:26.600
it's not enterprise. It's not, you know, measuring ads. It's not ad retargeting. It's just

00:10:26.600 --> 00:10:32.060
building cool stuff for people. And I don't know how long this will last, but I couldn't be happier.

00:10:32.060 --> 00:10:36.320
Sounds really fun. I'm sure Squarespace is, sorry, Squarespace.

00:10:36.320 --> 00:10:43.500
You're not the first fan. Squarespace is around here. Foursquare is in New York where you are.

00:10:43.500 --> 00:10:48.660
Now, I'm sure that that's a great place to be, and they're doing a lot of stuff. They used

00:10:48.660 --> 00:10:52.540
something like Scala. There's some functional programming language that primarily there,

00:10:52.540 --> 00:10:53.120
right? Is it Scala?

00:10:53.120 --> 00:10:57.820
Yeah, it's primarily Scala. I've actually done a lot of data science and machine learning in Scala. And

00:10:57.820 --> 00:11:03.500
sometimes I'm kind of envious of Python because there's better tools in Python. And we do some of

00:11:03.500 --> 00:11:10.980
our, we do some of our initial testing on data sets in Python sometimes, but there is a lot of momentum

00:11:10.980 --> 00:11:15.380
to go with Scala because all of our backend jobs are written in Scala. And so we often have to

00:11:15.380 --> 00:11:19.120
translate it into Scala, which has good tools, but not as good as Python.

00:11:19.120 --> 00:11:24.780
Yeah. Yeah. So I was going to ask, what's the Python story there? Do you guys get to do much

00:11:24.780 --> 00:11:25.280
Python there?

00:11:25.280 --> 00:11:33.600
Yeah. So I have done, if I can take you back in the, to the olden days of 2014, if that's,

00:11:33.600 --> 00:11:38.500
if that's allowed, because one of the things that I did at Foursquare that I'm pretty proud of

00:11:38.500 --> 00:11:45.440
is building a sentiment model, which is trying to take a Foursquare tip, which were like three

00:11:45.440 --> 00:11:50.960
sentences that people wrote in Foursquare on the Foursquare City Guide app. And that gets surfaced

00:11:50.960 --> 00:11:57.520
later. It was sort of compared to the Yelp reviews, but except they're short and helpful and not as

00:11:57.520 --> 00:12:01.580
negative. What we want to do is we want to take those tips and try to come up with the rating of

00:12:01.580 --> 00:12:07.640
the venue because we have this one to 10 rating that every venue receives. And so using the likes

00:12:07.640 --> 00:12:12.100
and dislikes explicitly wasn't good enough because there were so many people who would just click like

00:12:12.100 --> 00:12:19.980
very casually. And so we realized at some point, Hey, we have a labeled training set here. We can say,

00:12:19.980 --> 00:12:25.360
Hey, the person who explicitly liked a place and also left a text tip, that is a label of positive.

00:12:25.360 --> 00:12:29.660
And someone who explicitly disliked a place, that's a label of negative. And someone who left the

00:12:29.660 --> 00:12:35.740
middle option, which we called a meh or a mixed review, their tip is probably mixed. And so we have

00:12:35.740 --> 00:12:41.140
this tremendous data set on tips and that allowed us to build a model, a pretty good model. And it

00:12:41.140 --> 00:12:46.920
wasn't very sophisticated. It was multi-logistic regression based on sparse data, which was like

00:12:46.920 --> 00:12:53.640
what phrases are included in the tip. Right. Trying to understand the sentiment of the actual words,

00:12:53.640 --> 00:12:58.080
right? Yeah. There was logistic regression available in Python at the time, which is great,

00:12:58.080 --> 00:13:03.220
but I wanted something a little custom, which is now available in Python. But back then it was kind

00:13:03.220 --> 00:13:08.560
of hard to find these packages and not just that there, even when there were packages, sometimes

00:13:08.560 --> 00:13:13.200
it's difficult to say, okay, is this working? How do I test what's going on into the hood? It's not very,

00:13:13.200 --> 00:13:20.640
so I decided to build my own in Python, which was a multi-logistic regression means we're trying to find

00:13:20.640 --> 00:13:27.860
out three categories like positive review, negative review, or mixed review based on the label data.

00:13:27.860 --> 00:13:34.320
And we were going to have a sparse data set, which means it's not like there are 20 words that we're

00:13:34.320 --> 00:13:39.040
looking for. No, there are like tens of thousands. I don't know the exact number, tens of thousands,

00:13:39.040 --> 00:13:43.840
hundreds of thousands of phrases that we're going to look for. And for most of the tips, most of the

00:13:43.840 --> 00:13:47.800
phrases are going to be zero. Didn't see it, didn't see it, didn't see it. But every once in a while,

00:13:47.800 --> 00:13:50.820
you're going to have a one, didn't see it. So that's when you have that matrix where most of

00:13:50.820 --> 00:13:56.220
them are zero, that's sparse. And then thirdly, we wanted to use elastic net, which meant that

00:13:56.220 --> 00:14:01.360
most of the weights are going to be set to exactly zero. So when we store our model,

00:14:01.360 --> 00:14:06.400
most words, it's going to say, hey, these words aren't sentiment. So we're just going to,

00:14:06.400 --> 00:14:10.980
these don't really affect it. We want to have it exactly zero, except what a traditional logistic

00:14:10.980 --> 00:14:18.360
regression would do is it would say, okay, we are going to come up with the optimal,

00:14:18.360 --> 00:14:21.940
but everything will be close to zero. And so you have to kind of store it. You have to store the

00:14:21.940 --> 00:14:29.080
like 0.0001. So that's a problem too. So I actually built that kind of open source and put that on my

00:14:29.080 --> 00:14:34.980
GitHub on base pi back in 2014. I don't think anyone uses it, but it was a lot of fun. I use Cython to make

00:14:34.980 --> 00:14:39.340
go really fast. It's kind of a problem at Foursquare because it's the only thing that

00:14:39.340 --> 00:14:42.620
runs in Python. And every once in a while, someone asks me like, what's this doing here?

00:14:42.620 --> 00:14:45.620
Exactly. How do I run this? I don't know. This doesn't fit to our world, right?

00:14:45.620 --> 00:14:45.940
Yeah.

00:14:45.940 --> 00:14:50.160
Cool. All right. Well, Foursquare sounds really fun. Another thing that you do

00:14:50.160 --> 00:14:54.580
that I know you from, I don't know you through the Foursquare work that you're doing. I know you

00:14:54.580 --> 00:15:00.360
through your podcast, The Local Maximum, which is pretty cool. You had me on back on episode 73.

00:15:00.360 --> 00:15:01.660
So thanks for that. That was cool.

00:15:01.660 --> 00:15:04.840
That is our most downloaded episode right now.

00:15:04.920 --> 00:15:06.100
Really? Wow. Awesome.

00:15:06.100 --> 00:15:06.440
Yeah.

00:15:06.440 --> 00:15:07.540
That's super cool to hear.

00:15:07.540 --> 00:15:07.760
Yeah.

00:15:07.760 --> 00:15:14.280
More relevant for today's conversation, though, would be episode 78, which is all about Bayesian

00:15:14.280 --> 00:15:20.080
thinking and Bayesian analysis and those types of things. So people can check that out for a more

00:15:20.080 --> 00:15:26.560
high level, less technical, more philosophical view, I think, on what we're going to talk about

00:15:26.560 --> 00:15:27.580
if they want to go deeper, right?

00:15:27.580 --> 00:15:31.860
Absolutely. You could also ask me questions directly because I ramble a little bit in that, but I cover

00:15:31.860 --> 00:15:37.540
some pretty cool ideas, some pretty deep ideas there that I've been thinking about for many years.

00:15:37.540 --> 00:15:42.220
Yeah, for sure. So maybe tell people just really quickly what The Local Maximum is, just to give you

00:15:42.220 --> 00:15:43.380
a chance to tell them about it.

00:15:43.380 --> 00:15:47.260
Yeah. So I started this podcast about a year and a half ago in 2018.

00:15:48.120 --> 00:15:53.800
And it started with, you know, I started basically interviewing my friends at Foursquare being like,

00:15:53.800 --> 00:15:57.500
hey, this person's working on something cool, that person's working on something cool, but they never

00:15:57.500 --> 00:16:02.940
get to tell their story. So why not let these engineers tell their story about what they're

00:16:02.940 --> 00:16:09.320
working on? And since then, I've kind of expanded it to cover, you know, current events and interesting

00:16:09.320 --> 00:16:14.720
topics in math and machine learning that people can kind of apply to their everyday life. Some episodes

00:16:14.720 --> 00:16:18.720
get more technical, but I kind of want to bring it back to the more general audience that it's like,

00:16:18.720 --> 00:16:23.980
hey, my guests and I, we have this expertise. We don't just want to talk amongst ourselves. We want

00:16:23.980 --> 00:16:29.720
to actually engage with the current events, engage with the tech news and try to think, okay, how do we

00:16:29.720 --> 00:16:36.380
apply these ideas? And so that's sort of the direction that I've been going in. And it's been a lot of fun.

00:16:36.380 --> 00:16:42.260
I've expanded beyond tech several times. I've had a few historians on, I've had a few journalists on.

00:16:42.260 --> 00:16:45.880
That's cool. I like the intersection of tech and those things as well. Yeah, it's pretty nice.

00:16:45.880 --> 00:16:53.020
This portion of Talk Python To Me is brought to you by Linode. Are you looking for hosting that's fast,

00:16:53.020 --> 00:16:57.740
simple, and incredibly affordable? Well, look past that bookstore and check out Linode at

00:16:57.740 --> 00:17:04.620
 talkpython.fm/Linode. That's L-I-N-O-D-E. Plans start at just $5 a month for a dedicated

00:17:04.620 --> 00:17:09.720
server with a gig of RAM. They have 10 data centers across the globe. So no matter where you are or

00:17:09.720 --> 00:17:13.940
where your users are, there's a data center for you. Whether you want to run a Python web app,

00:17:13.940 --> 00:17:19.120
host a private Git server, or just a file server, you'll get native SSDs on all the machines,

00:17:19.120 --> 00:17:26.120
a newly upgraded 200 gigabit network, 24-7 friendly support, even on holidays, and a seven-day money-back

00:17:26.120 --> 00:17:30.100
guarantee. Need a little help with your infrastructure? They even offer professional

00:17:30.100 --> 00:17:34.820
services to help you with architecture, migrations, and more. Do you want a dedicated server for free

00:17:34.820 --> 00:17:43.780
for the next four months? Just visit talkpython.fm/Linode. Let's talk about general data science

00:17:43.780 --> 00:17:52.480
before we get into the Bayesian stuff. So I think one of the misconceptions in general is that you have to be

00:17:52.480 --> 00:17:59.340
a mathematician or be very good at math to be a programmer. I think that's a false statement.

00:17:59.340 --> 00:18:00.620
To be a programmer.

00:18:00.620 --> 00:18:05.780
Yes, yes. Software developer. Straight up, I built the checkout page on this e-commerce site,

00:18:05.780 --> 00:18:06.340
for example.

00:18:06.340 --> 00:18:12.760
I would agree. I think you need some abstract thinking. You can't escape letters and stuff and

00:18:12.900 --> 00:18:18.320
variables, but you don't need, well, in the case of data science to compare, like you don't need,

00:18:18.320 --> 00:18:23.720
you don't need algebra or you don't need maybe a little bit, but you don't really need calculus and

00:18:23.720 --> 00:18:29.000
you don't need geometry, linear algebra and geometry. Yeah. Sometimes it's a UI engineer. You might need a

00:18:29.000 --> 00:18:29.600
little geometry.

00:18:29.600 --> 00:18:33.840
I mean, there's certain parts that you need that kind of stuff. Like video game development, for example,

00:18:33.840 --> 00:18:39.600
everything is about multiplying something by a matrix, right? You put all your stuff on the screen,

00:18:39.600 --> 00:18:44.660
even arrange it and rotate it by multiplying by matrices. There's some stuff happening there you

00:18:44.660 --> 00:18:48.840
got to know about, but generally speaking, you don't. However, I feel like in data science,

00:18:48.840 --> 00:18:55.060
you do get a little bit closer to statistics and you do need to maybe understand some of these

00:18:55.060 --> 00:19:00.660
algorithms. And I think that's where we can focus our conversation for this show is like,

00:19:01.000 --> 00:19:06.980
what do we need to know in general? And then the idea of Bayesian Bay's theorem and things like that.

00:19:06.980 --> 00:19:12.160
What do we need to know if I wanted to go into say data science? Because like I said, I don't really

00:19:12.160 --> 00:19:16.880
think you know that need to know that to do like, you know, connecting to a database and like saving a

00:19:16.880 --> 00:19:23.380
user. And you absolutely need logical thinking, but not like stats, but for data science, what do you

00:19:23.380 --> 00:19:24.000
think you need to know?

00:19:24.240 --> 00:19:29.200
Well, for data science, it really depends on what you're doing and how far down the rabbit hole you

00:19:29.200 --> 00:19:34.780
really want to go. You don't necessarily need all of the philosophical background that I talk about.

00:19:34.780 --> 00:19:41.280
I just love thinking about it. And it sort of helps me focus my thoughts when I do work on it

00:19:41.280 --> 00:19:46.480
to kind of go back and think about the first principles. So I get a lot of value out of that,

00:19:46.480 --> 00:19:53.220
but maybe not everyone does. There is sort of a surface level data science that or machine learning

00:19:53.220 --> 00:19:58.000
that you can get away with. If you want to do simple things, which is like, hey, I want to

00:19:58.000 --> 00:20:04.720
understand the idea that I have a training set, you know what a training set is, and this is what I want

00:20:04.720 --> 00:20:12.900
to predict. And here is roughly my mathematical function of how I know whether I'm predicting it well

00:20:12.900 --> 00:20:16.880
or not, but it could be something simple like the square distance, but already you're introducing

00:20:16.880 --> 00:20:23.460
some math there. And basically, I'm going to take a look at some libraries and I'm going to

00:20:23.460 --> 00:20:28.700
see if something works out of the box and gives me what I need. And if you do it that way,

00:20:28.700 --> 00:20:33.920
you need a little bit of understanding, but you don't need everything that like I would say kind of a

00:20:33.920 --> 00:20:39.280
true data science or machine learning engineer needs. But if you want to go deeper and kind of

00:20:39.280 --> 00:20:45.920
make it your profession, I would say you need kind of a background in calculus and linear algebra.

00:20:45.920 --> 00:20:52.340
And again, like, look, if I went back to grad school and I like if I went to a linear algebra

00:20:52.340 --> 00:20:57.980
final and I took it right now, would I be able to get every question right? Probably not. But I know

00:20:57.980 --> 00:21:03.540
the basics and I have a great understanding of how it works. And if I look at the equations, I can kind of

00:21:03.540 --> 00:21:06.820
break it down, you know, maybe with a little help from Google and all that.

00:21:06.960 --> 00:21:13.440
I think there's a danger of using these libraries to make predictions and other stuff when you're

00:21:13.440 --> 00:21:18.320
like, well, the data goes in here to this function and then I call it and then out comes the answer.

00:21:18.320 --> 00:21:24.280
Maybe there's some conditionality versus independence requirement that you didn't understand and it's

00:21:24.280 --> 00:21:26.440
not met or, you know, whatever, right?

00:21:26.440 --> 00:21:30.220
That's why I said it's really surface level and you can get away with it sometimes, but

00:21:30.220 --> 00:21:37.460
only for so long. And I think understanding where these things go wrong outside the, you know, when you

00:21:37.460 --> 00:21:43.660
take these black box functions requires both kind of a theoretical understanding of how they work and

00:21:43.660 --> 00:21:46.920
then also just like experience of seeing things going wrong in the past.

00:21:46.920 --> 00:21:50.660
Yeah. That experience sounds hard to get, but it seems like I'm an experience, right?

00:21:51.100 --> 00:21:52.500
You just, you got to get out there and do it.

00:21:52.500 --> 00:21:57.560
Right. Well, here's a good example. One time I was trying to predict how likely someone is to

00:21:57.560 --> 00:22:03.900
visit a store. This was part of working on Foursquare's attribution product, right? And

00:22:03.900 --> 00:22:09.700
someone was using random forest algorithm, or maybe it was just a simple decision tree. I'm not sure,

00:22:09.700 --> 00:22:18.020
but basically it creates a tree structure and puts people into buckets and determines whether or not,

00:22:18.020 --> 00:22:21.960
you know, and for each bucket, it says, okay, in this bucket, everyone visited and in this bucket,

00:22:21.960 --> 00:22:26.940
everyone didn't, or maybe this bucket is 90, 10 and this bucket is 10, 90. And so I can give good

00:22:26.940 --> 00:22:32.460
predictions on the probability someone will visit based on where they fall on the leaves of the tree.

00:22:32.460 --> 00:22:39.340
And we were using it and something just wasn't making sense to me. Somehow the numbers were just,

00:22:39.340 --> 00:22:44.440
something was wrong. And then I said, okay, let's make, let's make more leaves. And then I made more

00:22:44.440 --> 00:22:49.560
leaves. Like I made, I made the tree deeper, right? And then they're like, see, when you make the tree

00:22:49.560 --> 00:22:54.740
deeper, it gets better. That makes sense because it's, it's more fine graining. I'm like, yeah,

00:22:54.740 --> 00:22:59.120
but something doesn't make sense. It shouldn't be getting this good. And then as I realized what was

00:22:59.120 --> 00:23:04.840
happening, what was it, what was happening was some of the leaves had nobody visited in this leaf.

00:23:04.840 --> 00:23:08.280
That makes a lot of sense because most days you don't visit any particular chain.

00:23:08.980 --> 00:23:15.660
And when it went to zero and then it saw someone visited, well, the log likelihood loss,

00:23:15.660 --> 00:23:22.620
it basically predicted 0% of an event that didn't happen. And so log, when you do log likelihood

00:23:22.620 --> 00:23:27.720
loss or negative log likelihood loss, the score is like the negative log of that. So essentially you

00:23:27.720 --> 00:23:33.880
should be penalized infinitely for that because there was no smoothing. But the language we were using,

00:23:33.880 --> 00:23:40.460
which I think was spark or something like that. And it was probably some library and spark. I probably

00:23:40.460 --> 00:23:43.800
shouldn't throw a spark under the bus. It was probably some library or something was changing

00:23:43.800 --> 00:23:48.900
that infinity to a zero. So the thing that was infinitely bad, it was saying was infinitely good.

00:23:49.560 --> 00:23:56.160
And so the worst thing. And that took, oh God, that took us so long to figure out. Like it's embarrassing

00:23:56.160 --> 00:24:02.020
how long that one took to figure out, but that's, that's a good example of when experience will get

00:24:02.020 --> 00:24:05.320
you in something. I don't think I've ever talked about this one publicly.

00:24:05.320 --> 00:24:10.100
Yeah. Well, you just got to know that, you know, that's not what we're expecting, right?

00:24:10.240 --> 00:24:15.360
Yeah. But you know, theoretically, Hey, if I more fine grained my tree, if I, you know,

00:24:15.360 --> 00:24:21.800
make my groups smaller, maybe it works better. But I was like something, I was like, something's not

00:24:21.800 --> 00:24:26.080
right. It's working a little too good. There was nothing specifically that got me, but it was just

00:24:26.080 --> 00:24:30.600
like, there's probably a lot of stuff out there. That's actually people are taking actions on and

00:24:30.600 --> 00:24:35.520
spending money on, but it's, it's like that, right? Yeah. Yeah. So let's see. So we talked about

00:24:35.520 --> 00:24:39.560
some of the math stuff. If you really want to understand the algorithms, you know, statistics,

00:24:39.880 --> 00:24:44.860
calculus, linear algebra, you obviously need calculus to understand like real statistics,

00:24:44.860 --> 00:24:50.440
right? Continuous statistics and stuff. What else though? Like, do you need to know machine learning?

00:24:50.440 --> 00:24:55.660
What kind of algorithms do you need to know? Like what, what in the computer science-y side of things

00:24:55.660 --> 00:25:00.300
do you think you got to know? Bread and butter of the data scientists that I work with is machine

00:25:00.300 --> 00:25:06.320
learning algorithms. So I think that is very helpful to know. And I think that, you know, some of the

00:25:06.320 --> 00:25:10.900
basic algorithms in machine learning are good to know, which is like the K nearest neighbor,

00:25:10.900 --> 00:25:17.680
K means, logistic regression, decision trees, and then some kind of random forest algorithm,

00:25:17.680 --> 00:25:22.820
whether it's just random forest, which is a mixture of trees or gradient boosted trees we've had a lot

00:25:22.820 --> 00:25:29.160
of luck with. And then a lot of this deep learning stuff is, well, neural networks is one of them.

00:25:29.460 --> 00:25:33.000
Maybe you don't need to be an expert in neural networks, but it's certainly one to be aware of.

00:25:33.000 --> 00:25:40.180
And based on these neural networks, deep learning is becoming very popular. And I've been hearing and

00:25:40.180 --> 00:25:45.420
kind of looking into reading about deep learning for many years, but I have to say, I haven't actually

00:25:45.420 --> 00:25:51.800
implemented one of these algorithms myself. But I just interviewed a guy on my show, Mark Ryan,

00:25:51.800 --> 00:25:57.660
and he came out with a book called machine learning for structured data, which means, hey, you don't

00:25:57.660 --> 00:26:01.940
just, this doesn't just work for like images or audio recognition, you could actually use it for

00:26:01.940 --> 00:26:06.000
regular marketing data, like use everything else for. So I was like, all right, that's interesting. Maybe

00:26:06.000 --> 00:26:11.500
I'll work on that now. But I don't think at this point, you need to know machine learning to be a good

00:26:11.500 --> 00:26:16.140
or deep learning to be a good data scientist or machine learning engineer. I think the basics are really

00:26:16.140 --> 00:26:20.740
good to know, because in many problems, you know, the basics will get you very far. And there's a lot

00:26:20.740 --> 00:26:22.120
less that can go wrong.

00:26:22.120 --> 00:26:26.240
Yeah, a lot of those algorithms you talked about as well, like K-Nearest Neighbor and so on.

00:26:26.240 --> 00:26:30.740
There are several books that seem to cover all of those. I can't think of any off the top of my

00:26:30.740 --> 00:26:33.780
head, but I feel like I've looked through a couple and they all seem to have like, here are the main

00:26:33.780 --> 00:26:38.580
algorithms you need to know to kind of learn data science. So not too hard to pick them up.

00:26:38.580 --> 00:26:42.000
Slash names Bishop, the book that I read for grad school, but that's already 10 years old,

00:26:42.000 --> 00:26:46.320
certainly had all that stuff. That was very deep on math. I can send you a link if I want.

00:26:46.320 --> 00:26:46.560
Sure.

00:26:46.560 --> 00:26:50.680
I think kind of any intro book to machine learning will have all of that stuff.

00:26:50.680 --> 00:26:55.700
And basically, it's not in order of like hard to easy. It's just sort of, hey,

00:26:55.700 --> 00:27:02.180
these are things that have helped in the past and that statisticians and machine learning engineers

00:27:02.180 --> 00:27:06.780
have relied on in the past to get started and it's worked for them. So maybe it'll work for you.

00:27:06.780 --> 00:27:12.760
Cool. Well, a lot of machine learning and data science is about making predictions. We have some

00:27:12.760 --> 00:27:16.280
data. What does that tell us about the future, right?

00:27:16.280 --> 00:27:16.580
Right.

00:27:16.580 --> 00:27:20.720
That's where the Bayesian inference comes from in that world, right?

00:27:20.720 --> 00:27:26.360
Yeah. It's trying to form beliefs, which could be a belief about something that already happened that

00:27:26.360 --> 00:27:30.420
you don't know about, but you'll find out in the future or be affected by in the future, or it could

00:27:30.420 --> 00:27:35.640
be a belief about something that will happen in the future. So something that either will happen in

00:27:35.640 --> 00:27:39.540
the future or you'll learn about in the future. But Bayesian inference is more about, you know,

00:27:39.600 --> 00:27:47.960
forming beliefs and I kind of call it like it's a quantification of the scientific method. So in the

00:27:47.960 --> 00:27:53.780
basic form, the Bayes rule is very easy. You start with your current beliefs and you codify that in a

00:27:53.780 --> 00:27:59.680
special mathematical way. And then you say, okay, here's some new data I received on this topic. And then it

00:27:59.680 --> 00:28:04.340
gives you a framework to update your beliefs within the same framework that you've began with.

00:28:04.560 --> 00:28:09.160
Right. And so like an example that you gave would be say a fire alarm, right?

00:28:09.160 --> 00:28:16.020
We know from like life experience that most fire alarms are false alarms. You know, one example is

00:28:16.020 --> 00:28:23.100
what is your prior belief that there is a fire right now without seeing the alarm? The alarm is the data.

00:28:23.100 --> 00:28:29.160
The prior is what's the probability that, you know, my building is on fire and I need to

00:28:29.160 --> 00:28:34.000
get the F out right now. You know, it's very low actually. Yeah. I mean,

00:28:34.000 --> 00:28:39.860
yeah, for most of us, it hasn't really happened in our life. Maybe we've seen one or two fires,

00:28:39.860 --> 00:28:44.040
but they weren't that big of a deal. I'm sure there are some people in the audience who have seen

00:28:44.040 --> 00:28:47.060
bad fires and for them, maybe their prior is a little higher.

00:28:47.060 --> 00:28:49.720
I once in my entire life have had to escape a fire.

00:28:49.720 --> 00:28:50.340
Yeah.

00:28:50.340 --> 00:28:51.620
Only once, right?

00:28:51.620 --> 00:28:53.800
Were you in like real danger or?

00:28:53.800 --> 00:28:56.860
Oh yeah, probably. It was a car and the car actually caught on fire.

00:28:57.040 --> 00:28:58.360
Oh yeah. That sounds pretty bad.

00:28:58.360 --> 00:29:02.000
It had been worked on by some mechanics and they put it back together wrong. It like shot

00:29:02.000 --> 00:29:05.900
oil over something and it caught fire. And so we're like, Oh, the car's on fire. We should get out of

00:29:05.900 --> 00:29:06.020
it.

00:29:06.020 --> 00:29:11.880
Yeah. But yeah, sitting in your building at work, your prior is going to be much lower than in a car that

00:29:11.880 --> 00:29:18.600
you just worked on. So when the alarm goes off, okay, that's your data. The data is that we received

00:29:18.600 --> 00:29:26.320
an alarm today. And so then you have to think about, okay, I still have two hypotheses, right? Hypothesis one

00:29:26.320 --> 00:29:32.100
is that there is a fire and I have to escape. And hypothesis two is that there is no fire.

00:29:32.100 --> 00:29:37.600
And so once you hear the alarm, you still have those two hypotheses. One is that the alarm is

00:29:37.600 --> 00:29:42.300
going off and there's a real fire. And two is that there is no fire, but this is a false alarm.

00:29:42.300 --> 00:29:48.600
And so what ends up happening is that because there's a significant probability of a false alarm.

00:29:48.600 --> 00:29:54.000
So at the beginning, there is a very low probability of a fire. After you hear the alarm,

00:29:54.000 --> 00:29:58.980
there's still a pretty low probability of a fire, but the probability of a false alarm still overwhelms

00:29:58.980 --> 00:30:03.940
that. Now I'm not saying that you should ignore fire alarms all the time, but because in that case,

00:30:03.940 --> 00:30:10.660
that's a, that's a case where the action that you take is important regardless of the belief. So,

00:30:10.880 --> 00:30:17.140
you know, Hey, there is a very low cost to checking into it, at least checking into it or leaving the

00:30:17.140 --> 00:30:21.800
building in, if you have a fire alarm, but there's a very high consequence of failure. So high.

00:30:21.800 --> 00:30:26.980
Exactly. Exactly. But in terms of just forming beliefs, which is a good reason not to panic,

00:30:26.980 --> 00:30:31.180
you shouldn't put a lot of probability on the idea that there's definitely a fire.

00:30:31.220 --> 00:30:37.040
Okay. Yeah. So that's basically Bayesian inference, right? I know how likely a fire is. I have all

00:30:37.040 --> 00:30:43.720
of a sudden, I have this piece of data that now there is a fire. I have a set, a space of hypotheses

00:30:43.720 --> 00:30:50.320
that could apply, try to figure out which hypothesis, start testing and figure out which one is the right

00:30:50.320 --> 00:30:56.340
one. Maybe. Yeah. So you take your prior. So let's say there's like a, I don't know, one in 10,

00:30:56.340 --> 00:31:03.480
a hundred thousand chance that there's a fire in the building today and a 99,999 chance there isn't.

00:31:03.480 --> 00:31:10.760
Then you take that, that's your prior. Then you multiply it by your likelihood, which is okay.

00:31:10.760 --> 00:31:17.460
What is the likelihood of seeing the data given that the hypothesis is true? So what's the likelihood

00:31:17.460 --> 00:31:21.500
that the alarm would go off if there is a fire? Maybe that's pretty high. Maybe that's close to one

00:31:21.500 --> 00:31:26.440
or a little bit lower than one. And then on the second hypothesis that there's no fire,

00:31:26.440 --> 00:31:32.000
what's the likelihood of a false alarm today, which could actually be pretty high. Could be like one

00:31:32.000 --> 00:31:36.380
in a thousand or even one in a hundred in some buildings. And then you multiply those together and

00:31:36.380 --> 00:31:40.520
then you get an unnormalized posterior and that is your answer. So it's really just multiplication.

00:31:40.520 --> 00:31:45.060
Yeah. It's like simple fractions once you have all the pieces, right? So it's a pretty simple

00:31:45.060 --> 00:31:51.060
algorithm. It's very hard to describe through audio, but it's much better visually if you want to check it

00:31:51.060 --> 00:31:55.780
out. I've been struggling to describe it through audio for, you know, for the last year and a half,

00:31:55.780 --> 00:31:57.560
but I do the best I can.

00:31:57.560 --> 00:32:00.100
This is like describing code. You can only take it so precisely.

00:32:00.100 --> 00:32:00.440
Yeah.

00:32:00.440 --> 00:32:07.640
This portion of Talk Python To Me is brought to you by Tidelift. Tidelift is the first managed

00:32:07.640 --> 00:32:12.060
open source subscription, giving you commercial support and maintenance for the open source

00:32:12.060 --> 00:32:17.760
dependencies you use to build your applications. And with Tidelift, you not only get more dependable

00:32:17.760 --> 00:32:22.440
software, but you pay the maintainers of the exact packages you're using, which means your software

00:32:22.440 --> 00:32:27.520
will keep getting better. The Tidelift subscription covers millions of open source projects across

00:32:27.520 --> 00:32:33.380
Python, JavaScript, Java, PHP, Ruby, .NET, and more. And the subscription includes security updates,

00:32:33.380 --> 00:32:38.880
licensing, verification, and indemnification, maintenance and code improvements, package selection,

00:32:38.880 --> 00:32:44.720
and version guidance, roadmap input, and tooling and cloud integration. The bottom line is you get the

00:32:44.720 --> 00:32:50.720
capabilities you'd expect and require from commercial software. But now for all the key open source

00:32:50.720 --> 00:32:56.540
software you depend upon. Just visit talkpython.fm/Tidelift to get started today.

00:32:56.540 --> 00:33:07.620
This comes from a reverend, Reverend Bays, who came up with this idea in the 1700s, but for a long time,

00:33:07.800 --> 00:33:12.680
it wasn't really respected, right? And then it actually found some pretty powerful,

00:33:12.680 --> 00:33:17.620
it solved some pretty powerful problems that matters a lot to people recently.

00:33:17.620 --> 00:33:22.440
Yeah. I mean, I can't go through the whole, do the whole history justice in just a few minutes, but

00:33:22.440 --> 00:33:28.580
I'll try to give my highlights, which was this reverend who was sort of, he was a, you know,

00:33:28.580 --> 00:33:34.520
he was into theology and he was also into mathematics. So he was probably like pondering big questions and

00:33:34.520 --> 00:33:39.560
he wrote down notes and he was trying to figure out the validity of various arguments.

00:33:39.560 --> 00:33:46.200
His notes were found after he died, so he'd never published that. And so this was taken by

00:33:46.200 --> 00:33:52.780
Pierre Laplace, who was a more well-known mathematician and kind of formalized. But when the basis of

00:33:52.780 --> 00:34:00.260
statistical thinking was built in the late 20th, early 19th century, or late 19th, early 20th century,

00:34:00.520 --> 00:34:08.980
it really went in a more frequentist direction where it's like, no, a probability is actually a

00:34:08.980 --> 00:34:15.860
fraction of a repeatable experiment that kind of like over time, what fraction does it, does it end up

00:34:15.860 --> 00:34:22.260
as? And so they consider probability as sort of a, an objective property of the system. So for example,

00:34:22.260 --> 00:34:27.280
a dice flip, well, each side is one sixth. That's like kind of an objective property of the,

00:34:27.280 --> 00:34:33.540
of the die. Whereas no Bayesian statistics is called sort of based on belief. And because belief kind of

00:34:33.540 --> 00:34:40.440
seemed unscientific and the frequentists had very good methods for coming up with, with answers and

00:34:40.440 --> 00:34:48.740
more, more objective ways of doing it, they sort of had the upper hand. But as kind of the focus got into

00:34:48.740 --> 00:34:55.940
more complex issues and we had the rise of computers and that sort of thing, and the rise of more data and

00:34:55.940 --> 00:35:02.100
that sort of thing, Bayesian inference started taking a bigger and bigger role until now, I think most

00:35:02.100 --> 00:35:07.040
machine learning engineers and most data science scientists think as a Bayesian. And so it's like

00:35:07.040 --> 00:35:14.060
some examples in history, most people are probably aware of Alan Turing at Bletchley Park, along with

00:35:14.060 --> 00:35:19.500
many other people, you know, building these machines that broke the German codes during World War II.

00:35:19.500 --> 00:35:21.200
It's all movie about it.

00:35:21.320 --> 00:35:26.660
Right. That's trying to break the Enigma machine and the Enigma code. And that, those were some

00:35:26.660 --> 00:35:31.620
important problems to solve, but also highly challenging.

00:35:31.620 --> 00:35:37.640
Yeah. And so they incorporated a form of Bayes rule into this. Well, what are my relative beliefs

00:35:37.640 --> 00:35:42.660
as to the setting of the machine? Because, you know, the machine could have had quadrillions of settings and

00:35:42.660 --> 00:35:48.000
they're trying to distinguish between which one is likely to have and which one's not likely to have.

00:35:48.360 --> 00:35:55.600
But after the war, that stuff was classified. So nobody could say, oh yeah, Bayesian inference was

00:35:55.600 --> 00:36:00.800
used in that problem. And one interesting application that I found, even as it wasn't accepted by academia

00:36:00.800 --> 00:36:07.920
for many years, was life insurance. Because they're kind of on the hook for determining if the actuaries

00:36:07.920 --> 00:36:13.180
get the answer wrong as to how likely people are to live and die, then they're on the hook for lots and

00:36:13.180 --> 00:36:17.080
lots of money or like the continuation of their company if they get it wrong.

00:36:17.080 --> 00:36:18.180
And so-

00:36:18.180 --> 00:36:20.500
Right. Right. Or how likely is it to flood here?

00:36:20.500 --> 00:36:25.420
How likely is it for there to be a hurricane that wipes this part of the world off the map?

00:36:25.420 --> 00:36:25.700
Right.

00:36:25.760 --> 00:36:29.300
And a lot of these were one-off problems. You know, one problem is, you know, what's the

00:36:29.300 --> 00:36:34.000
likelihood of two commercial planes flying into each other? It hadn't happened, but they wanted to

00:36:34.000 --> 00:36:38.620
estimate the probability of that. And you can't do repeated experiments on that. So they really had to

00:36:38.620 --> 00:36:45.940
use a priors, which was sort of like expert data. And then, you know, more recently, as we had the rise of

00:36:45.940 --> 00:36:52.440
kind of machine learning algorithms and big data, you know, Bayesian methods have become more and more

00:36:52.440 --> 00:36:58.360
relevant. But also a big problem was, you know, the problems that we just mentioned, which are, you

00:36:58.360 --> 00:37:02.640
know, fire alarms and figuring out whether or not you have a disease and things like that. That's the

00:37:02.640 --> 00:37:08.560
two hypothesis problem. But a lot of times you have an infinite space, you have an infinite hypothesis

00:37:08.560 --> 00:37:14.420
problem that you're trying to determine between an infinite set of possible hypotheses. And that becomes

00:37:14.420 --> 00:37:19.340
very difficult to do, becomes extremely difficult without a computer, even with a computer becomes

00:37:19.340 --> 00:37:24.320
difficult to do. And so, you know, there's been a lot of research into how do you search that space

00:37:24.320 --> 00:37:29.560
of hypotheses to find the ones that are most likely. And so if you've heard the term Markov chain Monte

00:37:29.560 --> 00:37:36.220
Carlo, that is the most common algorithm used. And for that purpose, there is even current research

00:37:36.220 --> 00:37:41.960
into that, to making that faster and finding the hypothesis you want more quickly. Andrew Gellman at

00:37:41.960 --> 00:37:48.700
Columbia has some, a lot of stuff out about this. And he has like a new thing that's called like the

00:37:48.700 --> 00:37:55.080
nuts, which is like the no U-turn sampler, which is based off a very complicated version of MCMC.

00:37:55.080 --> 00:38:02.740
And so that's what's used in a framework that Python has called PyMC3 to come up with your

00:38:02.740 --> 00:38:04.860
most likely hypothesis very, very quickly.

00:38:04.860 --> 00:38:10.320
So let's take this over to the Python world. Yeah. Like, yeah, there's a lot of stuff that works

00:38:10.320 --> 00:38:15.440
with it. And obviously, like you said, the machine learning deep down uses some of these techniques,

00:38:15.440 --> 00:38:23.840
but this PyMC3 library is pretty interesting. Let's talk about it. So its subtitle is probabilistic

00:38:23.840 --> 00:38:25.040
programming in Python.

00:38:25.040 --> 00:38:30.500
If I could start with some alternatives, which I've used because I haven't, I've been diving into

00:38:30.500 --> 00:38:36.340
reading about PyMC3, but I haven't used it personally. So even when I was doing things in 2014,

00:38:36.340 --> 00:38:42.280
just on my own, basically without libraries, I was able to use Python very, very easily to

00:38:42.280 --> 00:38:50.400
kind of put in these equations for Bayesian inference on whether it's multi-logistic regression,

00:38:50.400 --> 00:38:56.060
or another one I did was Dirichlet prior calculator, which if I can kind of describe that, it's sort

00:38:56.060 --> 00:39:00.400
of thinking, well, how, what should I believe about a place before I've seen any reviews? Should I

00:39:00.400 --> 00:39:04.800
believe it's good? Should I believe it's bad? You know, if I have very few reviews, what should I

00:39:04.800 --> 00:39:10.260
believe about it? Which was an important question to ask for something like four square city guide in

00:39:10.260 --> 00:39:15.180
many cases, because we didn't have a lot of data. And so that was a good application of Bayesian

00:39:15.180 --> 00:39:21.600
inference. And I was able to just use the equations straight up and kind of from first principles,

00:39:21.600 --> 00:39:29.500
apply algorithms directly in Python. And it actually was not that hard to do because when searching the

00:39:29.500 --> 00:39:34.320
space, there was a single global maximum, didn't have to worry about the local maximum in these

00:39:34.320 --> 00:39:39.320
equations. So it was just a hill climbing. Hey, I'm going to start with this hypothesis in this

00:39:39.320 --> 00:39:43.260
n dimensional space, and I'm going to find the gradient, I'm going to go a little higher,

00:39:43.260 --> 00:39:47.500
a little higher, a little higher gradient ascent is what I described, although it's usually called

00:39:47.500 --> 00:39:54.240
gradient descent. So that's sort of an easy one to understand. Then if you want to do MCMC directly,

00:39:54.240 --> 00:40:00.580
because you have some space that you want to search, and you have the equations of the probability

00:40:00.580 --> 00:40:09.640
on each of the points in that space, I used pi MC, which is spelled E M C E E, which is a simple

00:40:09.640 --> 00:40:18.320
program that only does MCMC. And so I had a lot of success with that when I wanted to do some one off

00:40:18.320 --> 00:40:24.820
sampling of, you know, non standard probability distributions. So those are ones that I've actually

00:40:24.820 --> 00:40:31.100
used and had success with in the past. But pi MC three seems to be like the full, you know, we do

00:40:31.100 --> 00:40:38.300
everything sort of a thing. And basically, what you do is you program probabilistically. So you say,

00:40:38.300 --> 00:40:44.720
hey, I imagine that this is how the data is generated. So I'm just going to basically put

00:40:44.720 --> 00:40:50.900
that in code. And then I'm going to let you, the algorithm work backwards and tell me what the

00:40:50.900 --> 00:40:57.400
parameters originally were. So if I could do a specific here, let's say I'm doing logistic regression,

00:40:57.400 --> 00:41:02.960
which is like, every item has a score, or, you know, in the case that I was working on,

00:41:02.960 --> 00:41:08.600
every word has a score, the scores get added up, that's then a real number, then it's transformed

00:41:08.600 --> 00:41:13.700
using a sigmoid into a number between zero and one. And that's the probability that's a positive review.

00:41:13.700 --> 00:41:20.240
And so basically, you'll just say, hey, I have this vector that describes the words this has,

00:41:20.240 --> 00:41:24.300
then I'm going to add these parameters, which I'm not going to tell you what they are.

00:41:24.560 --> 00:41:28.620
And then I'm going to get this result. And then I'm going to give you the final data set at the end.

00:41:28.620 --> 00:41:33.420
And it kind of works backwards and tells you, okay, this is what I think the parameters were.

00:41:33.420 --> 00:41:40.100
And what's really interesting about something like pi MC3, which I would like to use in the future is

00:41:40.100 --> 00:41:44.440
when you do a linear regression or logistic regression, in kind of standard practice,

00:41:44.440 --> 00:41:50.600
you get one model at the end, right? This is the model that we think is best. And this is the model

00:41:50.600 --> 00:41:55.180
that has the highest probability. And this is the model that we're going to use. Great. You know,

00:41:55.180 --> 00:42:02.360
that that works for a lot of cases. But what pi MC3 does is that instead of picking a model at the end,

00:42:02.360 --> 00:42:08.500
it says, well, we still don't know exactly which model produced this data. But because we have the

00:42:08.500 --> 00:42:13.140
data set, we have a better idea of which models are now more likely and less likely. So we now have

00:42:13.140 --> 00:42:17.060
a probability distribution over models. And we're going to let you pull from that.

00:42:17.060 --> 00:42:21.800
So it kind of gives you a better sense of what the uncertainty is over the model. So

00:42:21.800 --> 00:42:28.560
for example, if you have a word in your data set, let's say the word's delicious, and it's a pod,

00:42:28.560 --> 00:42:32.320
we know it's a positive word. But let's say for some reason, there's not a lot of data on it,

00:42:32.320 --> 00:42:38.020
then it can say, well, I don't really know what the weight of delicious should be.

00:42:38.260 --> 00:42:41.140
It's being used at rock concerts. We don't know why. What does it mean?

00:42:41.140 --> 00:42:45.900
Yeah, yeah, yeah. And so we're going to give you a few possible models. And, you know, and you can

00:42:45.900 --> 00:42:52.100
keep sampling from that. And you'll see that the deviation, the discrepancy, the variance of that

00:42:52.100 --> 00:42:56.660
model is going to be very high of that weight is going to be very high, because we just don't have a

00:42:56.660 --> 00:43:00.300
lot of data on it. And that's something that standard regressions just don't do.

00:43:00.300 --> 00:43:06.680
That's pretty cool. And the way you work with it is, you basically code out the model and like a

00:43:06.680 --> 00:43:13.060
really nice Python language API. You kind of say, well, this, I think it's a linear model,

00:43:13.060 --> 00:43:18.200
I think it's this type of thing. And then like you said, it'll go back and solve it for you. That's

00:43:18.200 --> 00:43:19.820
pretty awesome. I think it's nice.

00:43:19.820 --> 00:43:24.520
Right. A good thing to think about it is in terms of just a standard linear regression, like,

00:43:24.520 --> 00:43:29.700
what's the easiest example I can think of? Try to find someone's weight from their height,

00:43:29.840 --> 00:43:36.220
for example. And so you think there might be an optimal coefficient on there given the data.

00:43:36.220 --> 00:43:40.860
But if you use PyMC3, it will say, no, we don't know exactly what the coefficient is given your data.

00:43:40.860 --> 00:43:44.180
You don't have a lot of data, but we're going to give you several possibilities. We're going to give

00:43:44.180 --> 00:43:50.460
you a probability distribution over it. And as I say, in the local maximum, you shouldn't make everything

00:43:50.460 --> 00:43:56.080
probabilistic because there is a cost in that. But oftentimes you can, by considering something to be,

00:43:56.440 --> 00:44:00.620
rather than considering one single truth by considering multiple truths probabilistically,

00:44:00.620 --> 00:44:05.600
you can unlock a lot of value. In this case, you can kind of determine your variance a little better.

00:44:05.600 --> 00:44:10.380
Yeah, that's super cool. I hadn't really thought about it. And like I said, the API is super clean

00:44:10.380 --> 00:44:12.640
for doing this. So it's great. Yeah.

00:44:12.640 --> 00:44:18.760
Where does this Bayesian inference, like, where do you see this solving problems today? Where do you see

00:44:18.760 --> 00:44:21.480
like stuff going? What's the world look like now?

00:44:21.660 --> 00:44:26.920
I've been using it to solve problems basically as soon as I started working as a machine learning engineer

00:44:26.920 --> 00:44:32.100
at Foursquare, basically using Bayes' rule as kind of my first principles whenever I approach a problem.

00:44:32.100 --> 00:44:38.140
And it's never driven me in the wrong direction. So I think it's one of those timeless things that

00:44:38.140 --> 00:44:44.880
you can always use. For me, especially after working with our attribution product a lot,

00:44:44.880 --> 00:44:51.220
I think that the future is trying to figure out causality a lot better. And I think that's where

00:44:51.220 --> 00:44:56.700
some of these more sophisticated ideas come in. Because it's one thing to say, this variable is

00:44:56.700 --> 00:45:00.960
correlated with that and I can have a model. But it's like, well, what's the probability that this

00:45:00.960 --> 00:45:06.340
variable, changing this variable actually causes this other variable to change? In the case of ads,

00:45:06.340 --> 00:45:10.240
where you could see where it's going to unlock a lot of value for companies where, you know,

00:45:10.240 --> 00:45:16.300
there might be a lot of investment in this, is what is the probability that this ad affects

00:45:16.300 --> 00:45:23.320
someone's likelihood to visit my place or to buy something from me more generally? Or what is my

00:45:23.320 --> 00:45:31.140
probability distribution over that? And so can I estimate that? And I think that that whole industry

00:45:31.140 --> 00:45:37.400
of online ads is, it's very frustrating for an engineer because it's so inefficient. And there's so

00:45:37.400 --> 00:45:40.840
many people in there that don't know what they're doing. And it could be very frustrating at times.

00:45:40.840 --> 00:45:46.000
But I think that means also that there's a lot of opportunity to like unlock value if you have

00:45:46.000 --> 00:45:51.720
a lot of patience. Sure. Well, so much of it is just they looked for this keyword, so they must be

00:45:51.720 --> 00:45:56.260
interested, right? It doesn't take very much into account. Yeah, but the question is, okay, maybe they

00:45:56.260 --> 00:46:00.320
look for that keyword and now they're going to buy it no matter what I do. So don't send them the ad,

00:46:00.320 --> 00:46:04.980
send the ad to someone who didn't search the keyword. Or maybe they need that extra push and that extra

00:46:04.980 --> 00:46:10.700
push is very valuable. It's hard to know unless you measure it. And you measure it, you don't get a

00:46:10.700 --> 00:46:17.660
whole lot of data. So you really, it really has to be a Bayesian model. Whoever uses these Bayesian

00:46:17.660 --> 00:46:23.580
models is going to get way ahead. But right now it goes through several layers. I kept saying when we

00:46:23.580 --> 00:46:29.120
were working on this problem and people weren't getting what we were doing, I was like, I wish the

00:46:29.120 --> 00:46:33.760
people who are writing the check for these ads could get in touch with us because I know they care.

00:46:33.760 --> 00:46:39.220
But, you know, oftentimes you're working through sales and someone on the other side.

00:46:39.220 --> 00:46:41.220
It was just too many layers between, right?

00:46:41.220 --> 00:46:41.800
Yeah.

00:46:41.800 --> 00:46:42.380
Yeah, for sure.

00:46:42.380 --> 00:46:48.180
Earlier, you spoke about having your code go fast and you talked about Cython.

00:46:48.180 --> 00:46:48.720
Oh yeah.

00:46:48.720 --> 00:46:49.640
What's your experience with Cython?

00:46:49.840 --> 00:46:57.220
I used that for the multi-logistic regression. And all I can say is it took a little getting used

00:46:57.220 --> 00:47:02.680
to, but, you know, I got an order of magnitude speed up, which we needed to launch that thing

00:47:02.680 --> 00:47:12.800
in our one-off Python job at Foursquare. So it took only a few hours versus all day. So it was kind

00:47:12.800 --> 00:47:18.880
of a helpful tool to get that thing launched. And I haven't used it too much since, but I kind of keep

00:47:18.880 --> 00:47:21.280
that in the back of my mind as a part of my toolkit.

00:47:21.280 --> 00:47:25.420
Yeah. It's great to have in the toolkit. I feel like it doesn't get that much love, but

00:47:25.420 --> 00:47:30.660
I know people talk about Python speed and, oh, it's fast here. It's slow there.

00:47:30.660 --> 00:47:31.020
Yeah.

00:47:31.020 --> 00:47:34.660
First people just think it's slow because it's not compiled, but then you're like, oh,

00:47:34.660 --> 00:47:39.720
but wait about the C extensions. You go, actually, yeah, that's actually faster than Java or something

00:47:39.720 --> 00:47:40.720
like that. So interesting.

00:47:40.720 --> 00:47:47.940
Yeah. I've also had a big speed up just by taking, you know, a dictionary or matrix I was using and then

00:47:47.940 --> 00:47:53.660
using NumPy instead of the, or NumPy, I don't know how you pronounce it, but instead of using-

00:47:53.660 --> 00:47:54.180
I go with NumPy, but yeah.

00:47:54.180 --> 00:47:54.480
Okay.

00:47:54.480 --> 00:48:01.280
NumPy instead of the standard, like, you know, Python tools, you could also get a big speed

00:48:01.280 --> 00:48:01.660
up there.

00:48:01.660 --> 00:48:04.100
Yeah, for sure. And that's pushing it down into the C layer, right?

00:48:04.100 --> 00:48:04.420
Yeah.

00:48:04.500 --> 00:48:10.380
But a lot of times you have your algorithm and Python, and one option is to go write that C

00:48:10.380 --> 00:48:13.860
layer because you're like, well, we kind of need it. So here we go down the rabbit hole of writing

00:48:13.860 --> 00:48:18.560
C code instead of Python. But Cython is sweet, right? Especially the latest one, you can just put

00:48:18.560 --> 00:48:21.500
the regular type annotations, the Python three type annotations.

00:48:21.500 --> 00:48:21.940
Oh, yeah.

00:48:21.940 --> 00:48:24.580
On the types. And then, you know, magic happens.

00:48:24.580 --> 00:48:28.040
I definitely, I just started with Python and it was like, you know, we're in this,

00:48:28.040 --> 00:48:31.000
these three functions 90% of the time, just fix that.

00:48:31.000 --> 00:48:35.980
It's usually the slow part is like really focused. Most of your code, it doesn't even matter what

00:48:35.980 --> 00:48:39.460
happens to it, right? It's just, there's like that little bit where you loop around a lot

00:48:39.460 --> 00:48:40.740
and that matters.

00:48:40.740 --> 00:48:41.080
Yeah.

00:48:41.080 --> 00:48:41.280
Yeah.

00:48:41.820 --> 00:48:46.520
It's funny how we over optimize and you can't escape it. Like even when I'm creating,

00:48:46.520 --> 00:48:50.720
you know, I see like a bunch of doubles. I'm like, oh, but these are only one and zero. Can

00:48:50.720 --> 00:48:54.140
we like change them to Boolean? But like in the end, it doesn't care. It doesn't matter.

00:48:54.140 --> 00:48:56.680
For most of the code, it really has no effect.

00:48:56.680 --> 00:48:57.080
For sure.

00:48:57.080 --> 00:48:58.760
Except in that one targeted place.

00:48:58.760 --> 00:49:01.900
Yeah. So the trick is to use the tools to find it, right?

00:49:01.900 --> 00:49:02.220
Yeah.

00:49:02.220 --> 00:49:07.460
Like C profiler or something like that. The other major thing, you know, one thing you can do to

00:49:07.460 --> 00:49:11.800
speed up stuff like this, these algorithms is just to say, well, I wrote it.

00:49:11.800 --> 00:49:17.440
I wrote it in Python or I use this data structure and maybe if I rewrote it differently or I wrote

00:49:17.440 --> 00:49:23.400
it in C or I applied Cython, it'll go faster. But it could be that you're speeding up the execution

00:49:23.400 --> 00:49:28.160
of a bad algorithm. And if you had a better algorithm, it might go a hundred times faster

00:49:28.160 --> 00:49:31.980
or something, right? Like, so how do you think about that with your problems?

00:49:31.980 --> 00:49:38.740
That's what I did for the, back in 2014 with the Dirichlet prior calculator. And that was an

00:49:38.740 --> 00:49:44.100
interesting problem to solve because to recap on that, it's one of the use cases we had.

00:49:44.100 --> 00:49:49.140
Okay. What's my prior on a venue before I've gotten any reviews? What's my prior on a restaurant

00:49:49.140 --> 00:49:53.200
before I've gotten any reviews? And I'm using the experience of the data on all the other restaurants

00:49:53.200 --> 00:49:59.020
I've seen. So we know what the variance is. And let me try to come up with an equation that can

00:49:59.020 --> 00:50:04.400
calculate that value from the data. And it turned out there were some algorithms available,

00:50:04.400 --> 00:50:12.660
but as I dug into the math, I noticed that there was like a math trick that I could make use of.

00:50:12.660 --> 00:50:17.060
In other words, it was something like certain logs were being taken of the same number,

00:50:17.060 --> 00:50:23.380
were being taken over and over again. And it's like, okay, just store how many times we took the

00:50:23.380 --> 00:50:28.260
log. And then when I dug into the math, they kind of combined into one term and multiply that together.

00:50:28.260 --> 00:50:33.540
So essentially I used a bunch of factoring and refactoring, whether you think of it as factoring

00:50:33.540 --> 00:50:41.180
code or factoring math to get kind of an exponential speed up in that algorithm. And so that's why I

00:50:41.180 --> 00:50:45.500
published a paper on it. I was very proud of that. It was a, it was very satisfying thing to do.

00:50:45.500 --> 00:50:49.160
It might not have mattered in terms of our product, but I think a lot of people used it though,

00:50:49.240 --> 00:50:53.560
to be like, I want rather than just taking an average of what I've seen in the past. No,

00:50:53.560 --> 00:51:00.420
I want to do something that is based on good principles. And so I want to use the Dirichlet

00:51:00.420 --> 00:51:07.480
prior calculator. And so some people have used that. It's my Python code online. And the algorithm has

00:51:07.480 --> 00:51:13.520
proven very fast and like almost instantaneous. Basically, as soon as you load all the data in,

00:51:13.520 --> 00:51:18.900
it gives you the answer, which I like. Now, my next step to that is to use PyMC3,

00:51:19.020 --> 00:51:22.340
rather than giving you an answer, it should give you a probability distribution over answers.

00:51:22.340 --> 00:51:23.140
Yeah, that's right.

00:51:23.140 --> 00:51:26.860
I haven't done that yet. Didn't know about that. Yeah. Didn't know about that at the time. I think

00:51:26.860 --> 00:51:28.380
my speed up would still apply.

00:51:28.380 --> 00:51:32.780
Yeah, that's cool. Well, that definitely takes it up a notch. What about learning more about

00:51:32.780 --> 00:51:37.180
Bayesian analysis and inference and like, where should people go for more resources?

00:51:37.180 --> 00:51:42.900
Oh, okay. Well, a kind of a history book that I read that I really like on Bayesian inference

00:51:42.900 --> 00:51:50.620
is one called The Theory That Should Not Die by Sharon McGrane, a few years old, but it's really good

00:51:50.620 --> 00:51:56.380
if you're interested in the history on that. I have a book about PyMC3, kind of a tech book that does go

00:51:56.380 --> 00:52:02.640
into the basics of Bayesian inference that has a really good title. It's called Bayesian analysis

00:52:02.640 --> 00:52:04.080
with Python. Oh, yeah.

00:52:04.080 --> 00:52:10.420
Yeah, yeah. So that's a good one to look at. And then I have a bunch of episodes on my show

00:52:10.420 --> 00:52:18.820
that are related to Bayesian analysis. So episode zero and one on my show were basically just starting

00:52:18.820 --> 00:52:24.580
out trying to describe Bayes' role to everyone. I sort of attempted to do the description in episode

00:52:24.580 --> 00:52:28.500
zero. And then in episode one, I applied it to the news story that was happening that day,

00:52:28.580 --> 00:52:33.740
which was kind of the fire alarm at the bigger scale, which was everyone in Hawaii getting this

00:52:33.740 --> 00:52:39.820
message that there's an ICBM missile coming their way because of a mistake someone made.

00:52:39.820 --> 00:52:41.040
And then-

00:52:41.040 --> 00:52:45.140
Yeah, because of some terrible UI decision on like the tooling.

00:52:45.140 --> 00:52:46.440
Yeah, is that what it was?

00:52:46.440 --> 00:52:47.240
Yeah, yeah.

00:52:47.240 --> 00:52:47.440
Yeah.

00:52:47.440 --> 00:52:52.360
There was some analysis about what had happened and not probabilistically, but there was some,

00:52:52.360 --> 00:52:58.520
there's some really old crummy UI and they have to press some button to like acknowledge a test.

00:52:58.520 --> 00:53:03.700
Or treat it as real and somehow they look like almost identical or there's some weird thing

00:53:03.700 --> 00:53:07.240
about the UI that had like tricked the operator into saying, oh, it's real.

00:53:07.240 --> 00:53:13.100
Yeah, yeah. And then another couple episodes I want to highlight is episode 21 and 22,

00:53:13.100 --> 00:53:18.540
which is sort of kind of 21 is the philosophy of probability. In 22, we talk about the problem

00:53:18.540 --> 00:53:24.020
of p-hacking, which is when people try their experiments over and over and until they get

00:53:24.020 --> 00:53:29.620
something that works with p-values, which is a frequentist idea, which works if you're using

00:53:29.620 --> 00:53:33.840
it properly. But the problem is most people don't. And then we did an episode, I think it

00:53:33.840 --> 00:53:39.080
was 65 on probability, how to estimate the probability of something that's never happened. And then

00:53:39.080 --> 00:53:45.320
78, the one that you mentioned, which was on the history of Bayes and a little more philosophy.

00:53:45.320 --> 00:53:49.400
So I've talked about that a lot. You could probably go to localmaxradio.com or

00:53:49.400 --> 00:53:52.640
localmaxradio.com slash archive and find the ones that you want.

00:53:52.640 --> 00:53:58.660
That's really cool. So yeah, I guess we'll leave it there for now. That's quite interesting. And yeah,

00:53:58.660 --> 00:54:02.460
it gives us a look into some of the algorithms and math we got to know for our data science.

00:54:02.460 --> 00:54:06.620
Now, before you get out of here, though, I got the two questions I always ask everyone.

00:54:06.620 --> 00:54:09.880
You're going to write some Python code. What editor do you use?

00:54:09.880 --> 00:54:16.300
I just use Sublime or TextMate also on Mac. But I'm sure I could do something a little better

00:54:16.300 --> 00:54:19.140
than that. I just picked one and never really looked back.

00:54:19.140 --> 00:54:23.080
Sounds good. And then notable PyPI package?

00:54:23.080 --> 00:54:24.380
Notable.

00:54:24.380 --> 00:54:28.140
Maybe not the most popular, but like, oh, you should totally know about this. I mean,

00:54:28.140 --> 00:54:32.760
you already threw out there PyMC3, if you want to claim that one, or if there's something else. Yeah,

00:54:32.760 --> 00:54:33.380
pick that.

00:54:33.380 --> 00:54:40.660
Yeah. Well, I have BayesPy, which is the one that's like in GitHub slash max slash BayesPy,

00:54:40.660 --> 00:54:44.980
which has all the stuff I talked about. It's not actively developed, but it does have my kind of

00:54:44.980 --> 00:54:52.740
one-off algorithms, which if you're in the market for multinomial models or Dirichlet,

00:54:52.740 --> 00:55:00.760
or you want some kind of interesting new way to do multi-logistic regression, I could certainly give

00:55:00.760 --> 00:55:06.800
that a try. But most people probably want to use kind of the standard toolings. Yeah. Why don't I go

00:55:06.800 --> 00:55:09.480
with that? Why don't I go with the one I wrote a long time ago?

00:55:09.680 --> 00:55:13.940
Yeah. Right on. Sounds good. All right. Final call to action. People are excited about this stuff.

00:55:13.940 --> 00:55:15.760
What do you tell them? What do they do?

00:55:15.760 --> 00:55:19.600
Check out the books I mentioned and check out my website,

00:55:19.600 --> 00:55:26.860
localmaxradio.com. And also subscribe to the Local Maximum. It should be on all of your podcatchers.

00:55:26.860 --> 00:55:30.640
If it's not on one, please let me know. But it should be on all of your podcatchers.

00:55:31.440 --> 00:55:35.780
localmaxradio.com. It's just every week. And we have a lot of fun. So definitely check it out.

00:55:35.780 --> 00:55:37.900
Yeah, it's cool. You spend a lot of time talking about these types of things.

00:55:37.900 --> 00:55:40.620
Super. All right. Well, Max, thanks for being on the show.

00:55:40.620 --> 00:55:43.360
Michael, thank you so much. I really enjoy this conversation.

00:55:43.360 --> 00:55:44.380
Yeah, same here. Bye-bye.

00:55:44.380 --> 00:55:44.580
Bye.

00:55:45.400 --> 00:55:50.300
This has been another episode of Talk Python To Me. Our guest on this episode was Max Sklar,

00:55:50.300 --> 00:55:56.180
and it's been brought to you by Linode and Tidelift. Linode is your go-to hosting for whatever you're

00:55:56.180 --> 00:56:02.320
building with Python. Get four months free at talkpython.fm/linode. That's L-I-N-O-D-E.

00:56:02.320 --> 00:56:08.000
If you run an open source project, Tidelift wants to help you get paid for keeping it going strong.

00:56:08.000 --> 00:56:13.620
Just visit talkpython.fm/Tidelift, search for your package, and get started today.

00:56:14.400 --> 00:56:19.160
Want to level up your Python? If you're just getting started, try my Python Jumpstart by

00:56:19.160 --> 00:56:23.940
Building 10 Apps course. Or if you're looking for something more advanced, check out our new

00:56:23.940 --> 00:56:28.840
async course that digs into all the different types of async programming you can do in Python.

00:56:28.840 --> 00:56:32.800
And of course, if you're interested in more than one of these, be sure to check out our

00:56:32.800 --> 00:56:37.540
Everything Bundle. It's like a subscription that never expires. Be sure to subscribe to the show.

00:56:37.540 --> 00:56:41.180
Open your favorite podcatcher and search for Python. We should be right at the top.

00:56:41.180 --> 00:56:46.000
You can also find the iTunes feed at /itunes, the Google Play feed at /play,

00:56:46.000 --> 00:56:50.160
and the direct RSS feed at /rss on talkpython.fm.

00:56:50.620 --> 00:56:54.800
This is your host, Michael Kennedy. Thanks so much for listening. I really appreciate it.

00:56:54.800 --> 00:56:56.580
Now get out there and write some Python code.

00:56:56.580 --> 00:57:10.380
We'll see you next time.

00:57:10.380 --> 00:57:40.360
We'll see you next time.

